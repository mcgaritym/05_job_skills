job_title,company,location,date_posted,applicants,job_text,seniority_level,employment_type,job_function,industries,date_scraped
Data Engineer (100% Remote),UNICOM Technologies Inc,"Chicago, IL",5 hours ago,Be among the first 25 applicants,"['', 'Required Skills', 'Responsibilities']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-01-19 09:25:31
Data Engineer,Workday,"Pleasanton, CA",18 hours ago,Be among the first 25 applicants,"['', ' Team player who can collaborate and communicate effectively with all stakeholders; i.e. developers, technical operations, and customers ', ' 4+ years working with BI Tools\xa0 (Tableau, Qlikview, Cognos, Business Objects) ', ' Experience in writing complex SQLs to transform and aggregate data into metrics and KPIs ', ' Possess good verbal and written communication skills ', 'Job Description', ' 2+ years Data Modeling Experience. ', 'Required Skills:', ' Design and develop analytic and reporting solutions for Workday customers ', ' 2+ years Data Modeling Experience.  4+ years working with data integration tools for Extraction, Transformation and Loads (ETL).  4+ years working with BI Tools\xa0 (Tableau, Qlikview, Cognos, Business Objects)  2+ years reporting on Human Resource, Payroll or Financial software applications  Experience working with APIs to collect or ingest data.  Experience in languages like Python, R, Java etc.  Experience in writing complex SQLs to transform and aggregate data into metrics and KPIs  General understanding of Workday Security/Data Governance  Strong technical problem solving skills, with an ability to troubleshoot complex issues  Possess good verbal and written communication skills  Quick learner, motivated to understand various technologies used at Workday  Strong planning, scheduling, and organization skills  Able to work in a fast paced, fast-growth, high-energy environment and deal with multiple high priority activities concurrently  Team player who can collaborate and communicate effectively with all stakeholders; i.e. developers, technical operations, and customers ', ' Strong planning, scheduling, and organization skills ', 'About You:', ' Experience using Workday Reporting Tools, including Prism Analytics and Workday Studio  Workday, Salesforce, PeopleSoft, SAP, Oracle or other SaaS or On-Premise ERP systems desirable', ' Quick learner, motivated to understand various technologies used at Workday ', ' 2+ years reporting on Human Resource, Payroll or Financial software applications ', ' Workday, Salesforce, PeopleSoft, SAP, Oracle or other SaaS or On-Premise ERP systems desirable', ' Design and develop analytic and reporting solutions for Workday customers  Support the business and/or analytic analysts discover and scope use case requirements  Passionate about innovation and thinking outside the box  Analytic and reporting experience in Human Resources, Financial Management or Student Information Systems related domains  Ability to contribute to multiple projects and initiatives simultaneously  Deliver on assigned schedules to ensure project timelines are met  Ability to work in a diverse, fast paced environment and effectively collaborate across teams  Team player who will work across the organization to continue improving the way we serve our internal customers  Capability to mentor and share best practices within Workday’s internal analytic ecosystem ', ' Deliver on assigned schedules to ensure project timelines are met ', ' Experience working with APIs to collect or ingest data. ', ' Capability to mentor and share best practices within Workday’s internal analytic ecosystem ', ' Experience using Workday Reporting Tools, including Prism Analytics and Workday Studio ', ' Ability to work in a diverse, fast paced environment and effectively collaborate across teams ', ' Experience in languages like Python, R, Java etc. ', ' Analytic and reporting experience in Human Resources, Financial Management or Student Information Systems related domains ', ' Passionate about innovation and thinking outside the box ', 'Preferred Skills:', 'Do what you love. Love what you do. ', ' General understanding of Workday Security/Data Governance ', ' 4+ years working with data integration tools for Extraction, Transformation and Loads (ETL). ', ' Strong technical problem solving skills, with an ability to troubleshoot complex issues ', ' Support the business and/or analytic analysts discover and scope use case requirements ', ' Able to work in a fast paced, fast-growth, high-energy environment and deal with multiple high priority activities concurrently ', ' Team player who will work across the organization to continue improving the way we serve our internal customers ', ' Ability to contribute to multiple projects and initiatives simultaneously ']",Not Applicable,Full-time,Information Technology,Computer Software,2021-01-19 09:25:31
Data Engineer,doxo,"Seattle, WA",11 hours ago,Be among the first 25 applicants,"['', 'Experience with high scale, distributed, 24x7 systems and applications', 'Strong analytical and design skills', '3+ years of industry experienceExperience with high scale, distributed, 24x7 systems and applications5+ years of experience with SQL in both transactional and analytical applicationsExperience with Linux platformsExperience with AWS data technologiesExperience with Ruby, Python, or a similar programming languageStrong analytical and design skillsCapacity for learning quickly in a fast paced environment and handling multiple tasks simultaneouslyStrong written and oral communication skills', 'Define the architecture, technologies, and processes used to create a high scale data processing pipeline', '3+ years of industry experience', '5+ years of experience with SQL in both transactional and analytical applications', 'Strong written and oral communication skills', 'Define the architecture, technologies, and processes used to create a high scale data processing pipelineWork with engineers to create the tools and infrastructure used to collect, transform, and store data to be used for analyticsIdentify and implement analytics tools used by internal stakeholders to engage with dataEnsure proper security and access control to analytics dataWork collaboratively with the team to deploy, support and operate your services and applications.', 'Identify and implement analytics tools used by internal stakeholders to engage with data', 'Ensure proper security and access control to analytics data', 'Experience with Linux platforms', 'Skills And Qualifications', 'Experience with Ruby, Python, or a similar programming language', 'Work collaboratively with the team to deploy, support and operate your services and applications.', 'Capacity for learning quickly in a fast paced environment and handling multiple tasks simultaneously', 'Work with engineers to create the tools and infrastructure used to collect, transform, and store data to be used for analytics', 'BS/MS in Computer Science or Engineering', 'Experience with AWS data technologies']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-01-19 09:25:31
Data Engineer,"Sagence, Inc.","Chicago, IL",18 hours ago,44 applicants,"['', 'MUST BE HANDS ON. Candidates should have system development experience and proficiency with system development methodologies', 'Database Management Tools (preferred with cloud):', 'Database Management Tools (preferred with cloud):Relational – e.g.Oracle,\xa0MySQL,\xa0Microsoft SQL Server,\xa0PostgreSQL or similar NoSQL – e.g. MongoDB, Apache Cassandra, Couchbase or similarCloud-based – e.g. AWS (Redshift or DynamoDB), Azure (SQL Server or Cosmos DB)or similarETL Tools - e.g. Informatica, Talend, Microsoft SSIS, or similarPython Data Manipulation/Analysis – e.g., Pandas, NumPyFamiliar with at least one Industry Leading BI tools - e.g., Microsoft PowerBI, Tableau, Qlik, Looker, or similar', 'Data Engineer', 'Passionate, diverse, creative, genuine, curious, hands-on…these are just a few of the words that describe who we are. We have a high-energy workplace with a focus on producing high-quality, impactful results for our clients. We build teamwork through small, dedicated teams who constantly communicate and continuously teach each other and learn from one another. We promote an entrepreneurial spirit by encouraging individual initiative and foster a collaborative culture and work environment which includes open communication and on-going learning. We are committed to equality of opportunity, fairness, work and lifestyle balance, and mutual respect. We strongly believe these characteristics enable our employees to develop to their fullest potential.', 'Strong strategic business acumen and a passion for solving problems with data-driven solutions', 'Client centric focus with strong relationship management skills and the ability to influence business decisions', 'Prior professional experience in a management consulting, an IT management, or client facing role strongly preferred', 'Cloud-based – e.g. AWS (Redshift or DynamoDB), Azure (SQL Server or Cosmos DB)or similar', 'Skills & Requirements:', 'Hands on experience with cloud-based data solutions (AWS, Azure) and integrating with data sources', 'hands-on', 'NoSQL – e.g. MongoDB, Apache Cassandra, Couchbase or similar', 'Our People:', 'Deep knowledge of data architecture principles/approaches, data environment infrastructure considerations. Additional knowledge in data modeling principles/approaches preferred', '\xa0', 'Python Data Manipulation/Analysis – e.g., Pandas, NumPy', 'Must be currently authorized to work in the US on a full-time basis.\xa0Sagence does not provide sponsorship for work visas2+ years of professional experience working in a related roleStrong strategic business acumen and a passion for solving problems with data-driven solutionsClient centric focus with strong relationship management skills and the ability to influence business decisionsIntellectually curious and independently resourceful with the ability to prioritize and execute in a fast-paced and dynamic environmentWillingness to travel to client sites as needed', 'Familiar with at least one Industry Leading BI tools - e.g., Microsoft PowerBI, Tableau, Qlik, Looker, or similar', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Chicago or New York area candidates preferred, but will consider candidates in other parts of U.S.\xa0', 'Company Description: ', 'Relational – e.g.Oracle,\xa0MySQL,\xa0Microsoft SQL Server,\xa0PostgreSQL or similar ', 'Must have significant hands-on experience with various IT concepts of data management/engineering (ETL, data modeling, data warehousing, etc.)Must have significant hands-on experience with SQL, data profiling, and data discoveryMust have a strong understanding of data analytics and data wranglingHands on experience with cloud-based data solutions (AWS, Azure) and integrating with data sourcesExperience with at least one data manipulating programming languages (e.g. Python)Familiar with building business intelligence, analytics, or reporting solutions - either front-end consumption tools (e.g., Microsoft Power BI, Tableau & Qlik) or supply of data for these purposesDeep knowledge of data architecture principles/approaches, data environment infrastructure considerations. Additional knowledge in data modeling principles/approaches preferredAbility to drive out technical requirements with business and IT stakeholders for implementations of data solutionsHands-on experience with Agile delivery methodologyDemonstrated ability in effectively engaging, communicating, and presenting to stakeholders across both business and technology functionsConceptual and analytical thinker with the ability to define business requirements and extract, analyze, and synthesize complex business insightsPrior professional experience in a management consulting, an IT management, or client facing role strongly preferredAbility to perform knowledge transfer to support a teamKnowledge of the Financial Services, Insurance, Healthcare or Retail industries is preferred', 'Demonstrated ability in effectively engaging, communicating, and presenting to stakeholders across both business and technology functions', 'Hands-on experience with Agile delivery methodology', '2+ years of professional experience working in a related role', 'Knowledge of the Financial Services, Insurance, Healthcare or Retail industries is preferred', 'Proficient at leveraging tools and technology to drive value for clients.\xa0Examples include the following; ', 'Ability to drive out technical requirements with business and IT stakeholders for implementations of data solutions', 'Ability to perform knowledge transfer to support a team', 'Must have', 'Willingness to travel to client sites as needed', '***PLEASE NOTE***:\xa0You must be currently authorized to work in the US on a full-time basis. Sagence DOES NOT provide sponsorship for work visas at the time of hire or while employed with us.', 'Experience with at least one data manipulating programming languages (e.g. Python)', 'Tools and Technology:', 'Sagence is a management advisory firm dedicated to helping our clients optimize the value of their data assets. From thinking to doing, Sagence works with leading institutions in the acquisition, evaluation, development and management of their critical data assets and in the application of analytics to discover new insights, shorten time-to-value, and drive competitive advantage.', 'Conceptual and analytical thinker with the ability to define business requirements and extract, analyze, and synthesize complex business insights', 'ETL Tools - e.g. Informatica, Talend, Microsoft SSIS, or similar', 'Must have significant hands-on experience with various IT concepts of data management/engineering (ETL, data modeling, data warehousing, etc.)', 'Must have significant hands-on experience with SQL, data profiling, and data discovery', 'Familiar with building business intelligence, analytics, or reporting solutions - either front-end consumption tools (e.g., Microsoft Power BI, Tableau & Qlik) or supply of data for these purposes', 'Must be currently authorized to work in the US on a full-time basis.\xa0Sagence does not provide sponsorship for work visas', 'MUST BE HANDS ON. Candidates should have system development experience and proficiency with system development methodologies and possess the following skills and knowledge: ', 'and possess the following skills and knowledge: ', 'Intellectually curious and independently resourceful with the ability to prioritize and execute in a fast-paced and dynamic environment', ' ', 'Must have a strong understanding of data analytics and data wrangling', ' Sagence is looking for a Data Engineer to help us build and sustain our client’s data capabilities and our competitive advantage. Our Data Engineers are responsible for implementing repositories and building and maintaining data pipeline systems that are used in Data Analytics. Along with the requirements below, candidates must possess deep, practical experience in one or more of the following Data Management competencies; data warehousing, data lakes, data architecture. Additionally, and equally as important, as a Sagence Data Consultant, you must have the skills and experience necessary to effectively communicate with our clients', 'Job Overview', 'General:']",Entry level,Full-time,Information Technology,Management Consulting,2021-01-19 09:25:31
Data Engineer,North Highland,"Portland, OR",36 minutes ago,Be among the first 25 applicants,"['', ' AWS – S3, Snowflake, Airflow, PySpark, Python, Redshift ', ' COLLABORATE WITH AMAZING PEOPLE. ', ' We started as three leaders and a kitchen table. ', ' Codes complex programs and derives logical processes on technical platforms. ', 'LEAVE YOUR MARK ON A BETTER WORLD. ', 'What You Will Need', ' Adapts business requirements, developed by modeling/development staff and systems engineers, and develops the data, database specifications, and table and element attributes for an application. ', ' Participates in quality assurance and develops test application code in client server environment. ', ' MAKE CHANGE HAPPEN. ', ' An Agile background is important. ', ' Experience ingesting data in the past is preferred ', ' 5+ years of data engineering experience ', '  Looking to build foundation so feeling comfortable ingesting data from different master sources,   AWS – S3, Snowflake, Airflow, PySpark, Python, Redshift   Knowledge of industry standards on data pipelines   Experience ingesting data in the past is preferred   Engineering skillset and understanding on how to break down the strategy and solutions with the product owner to design an implementation.   An Agile background is important.   The ability to collaborate and work with team members and business partners is important.   Demonstrated success in a matrix environment   5+ years of data engineering experience  ', "" At more experienced levels, helps to develop an understanding of client's original data and storage mechanisms. "", ' Demonstrated success in a matrix environment ', ""  Establishes database management systems, standards, guidelines and quality assurance for database deliverables, such as conceptual design, logical database, capacity planning, external data interface specification, data loading plan, data maintenance plan and security policy.   Documents and communicates database design.   Evaluates and installs database management systems.   Codes complex programs and derives logical processes on technical platforms.   Builds windows, screens and reports.   Assists in the design of user interface and business application prototypes.   Participates in quality assurance and develops test application code in client server environment.   Provides expertise in devising, negotiating and defending the tables and fields provided in the database.   Adapts business requirements, developed by modeling/development staff and systems engineers, and develops the data, database specifications, and table and element attributes for an application.   At more experienced levels, helps to develop an understanding of client's original data and storage mechanisms.   Determines appropriateness of data for storage and optimum storage organization.   Determines how tables relate to each other and how fields interact within the tables for a relational model.  "", ' HERE ', ' to apply ', ' …Click ', ' Determines how tables relate to each other and how fields interact within the tables for a relational model. ', ' Why North Highland? ', ' Evaluates and installs database management systems. ', ' The ability to collaborate and work with team members and business partners is important. ', ' At North Highland, you’re never a number. ', ' Looking to build foundation so feeling comfortable ingesting data from different master sources, ', ' Documents and communicates database design. ', ' Engineering skillset and understanding on how to break down the strategy and solutions with the product owner to design an implementation. ', ' Builds windows, screens and reports. ', ' Assists in the design of user interface and business application prototypes. ', ' Exciting work you will do:… ', ' Provides expertise in devising, negotiating and defending the tables and fields provided in the database. ', ' Determines appropriateness of data for storage and optimum storage organization. ', ' Knowledge of industry standards on data pipelines ', ' Establishes database management systems, standards, guidelines and quality assurance for database deliverables, such as conceptual design, logical database, capacity planning, external data interface specification, data loading plan, data maintenance plan and security policy. ']",Entry level,Contract,Information Technology,Information Technology and Services,2021-01-19 09:25:31
21100158 - Data Engineer,Total Quality Logistics,"Cincinnati, OH",9 hours ago,Be among the first 25 applicants,"['', 'What You’ll Be Doing', 'Support the maintenance of an on-premise Enterprise Data Warehouse', ' Competitive compensation, benefits package and company perks Advancement opportunities with aggressive and structured career paths Flexible work hours and work-from-home options Access to the latest emerging technologies Reimbursement for continuous education and technical training ', 'Advancement opportunities with aggressive and structured career paths', 'Execute and coordinate requirements management and change management processes', 'Total Quality Logistics is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, age, national origin, genetic information, disability or protected veteran status.', '3+ year of experience in T-SQL with ability to write complex logic using SQL as part of ETL and effectively perform complex data analysis and discovery', 'Experience in API data integrations into Azure or on-premise databases', 'Build cubes, data marts and data models to support internal and external customers', '2+ years of experience developing solutions in Microsoft Azure ', 'Assist in developing large-scale data structures and pipelines to ingest, move, transform and integrate data to help generate insights and meet reporting needs', 'Bachelor’s degree in Mathematics, Computer Science, Economics, Statistics, or related', 'Ability to self-motivate and meet deadlines', 'What You Need', '2+ years of development experience in Microsoft SQL Server Integration Services (SSIS) or other ETL delivery solutions', 'What’s In It For You', 'Flexible work hours and work-from-home options', ' Bachelor’s degree in Mathematics, Computer Science, Economics, Statistics, or related 3-5 years of experience with SQL Server (Versions – 2012, 2016 or 2017) 3+ year of experience in T-SQL with ability to write complex logic using SQL as part of ETL and effectively perform complex data analysis and discovery 2+ years of experience developing solutions in Microsoft Azure  2+ years of development experience in Microsoft SQL Server Integration Services (SSIS) or other ETL delivery solutions 2+ years of experience building reports with SSRS Demonstrate strong organization skills and attention to detail  Experience in multiple programming languages (R, Java, Python, etc.) Ability to multi-task in a fast-paced, changing environment Experience in API data integrations into Azure or on-premise databases Ability to self-motivate and meet deadlines Intense desire to learn ', ' Assist in developing large-scale data structures and pipelines to ingest, move, transform and integrate data to help generate insights and meet reporting needs Build cubes, data marts and data models to support internal and external customers Document requirements and translate into proper system specifications Execute and coordinate requirements management and change management processes Support the maintenance of an on-premise Enterprise Data Warehouse Lead initiatives of limited scope, providing guidance and direction to level I Data Engineers ', 'Document requirements and translate into proper system specifications', 'Ability to multi-task in a fast-paced, changing environment', '3-5 years of experience with SQL Server (Versions – 2012, 2016 or 2017)', 'Experience in multiple programming languages (R, Java, Python, etc.)', 'If you are unable to apply online due to a disability, contact recruiting at .', 'Category:', 'Intense desire to learn', 'Demonstrate strong organization skills and attention to detail ', 'Competitive compensation, benefits package and company perks', 'Access to the latest emerging technologies', 'Job ID:', 'About The Role', '2+ years of experience building reports with SSRS', 'Reimbursement for continuous education and technical training', 'Lead initiatives of limited scope, providing guidance and direction to level I Data Engineers', 'Where you’ll be: ']",Not Applicable,Full-time,Information Technology,Logistics and Supply Chain,2021-01-19 09:25:31
Data Engineer,Blue Rose Technologies,"San Francisco, CA",9 hours ago,Be among the first 25 applicants,[''],Entry level,Full-time,Information Technology,Information Technology and Services,2021-01-19 09:25:31
Data Engineer,"McCarthy Building Companies, Inc.","St Louis, MO",12 hours ago,Be among the first 25 applicants,"['', 'Mature understanding of data warehouse and data lake concepts and design (including star schemas)', 'Design, implement, and support modern data solutions with Azure Data Factory, Azure Data Lake, and Azure SQL', 'Entrepreneurial attitude with a passion to deliver value for the organization and a desire to foster and develop team members ', 'Azure Data Factory (including pipelines and mapping & wrangling data flows)', 'Excellent analytical, conceptual, and problem-solving abilities', 'Design, implement, and support modern data solutions with Azure Data Factory, Azure Data Lake, and Azure SQLDesign, implement, and support ETL and ELT pipelines using Azure Data Factory pipelines and dataflows (both mapping and wrangling)Create and run testing protocols for data solutionsPromote ADF and database objects through environments using Azure DevOps and Visual StudioAssure development work accords with best practices including security and data qualityParticipate in code and design review to ensure alignment to standards and best practicesResearch, analyze, recommend and select technical approaches for solving challenging and complex development and integration problems', 'Mature understanding of data warehouse and data lake concepts and design (including star schemas) 5+ years of data solution delivery experience with 1+ years on the Microsoft Azure platform:Azure Data Factory (including pipelines and mapping & wrangling data flows)Azure SQL (nice to have: Azure Data Warehouse/Synapse)Azure Data Lake Gen2Azure DevOpsExtensive experience with T-SQL (queries and DDL)Extensive experience querying API endpointsExperience M and Power Query data wranglingExperience developing and promoting work through devops pipelines and utilizing source control, preferably GIT, and Visual Studio’s database toolsExperience with Agile/Scrum methodology preferredExcellent analytical, conceptual, and problem-solving abilitiesEntrepreneurial attitude with a passion to deliver value for the organization and a desire to foster and develop team members ', 'Experience with master data management', 'Participate in code and design review to ensure alignment to standards and best practices', 'Extensive experience querying API endpoints', 'Key Responsibilities', 'Promote ADF and database objects through environments using Azure DevOps and Visual Studio', 'Experience preparing data for Data Science and Machine Learning use cases', 'Preferred', 'Create and run testing protocols for data solutions', 'Azure Data Lake Gen2', 'Required', 'Position at McCarthy', ' 5+ years of data solution delivery experience with 1+ years on the Microsoft Azure platform:Azure Data Factory (including pipelines and mapping & wrangling data flows)Azure SQL (nice to have: Azure Data Warehouse/Synapse)Azure Data Lake Gen2Azure DevOps', 'Assure development work accords with best practices including security and data quality', 'Experience building data pipelines to ingest unstructured and streaming dataExperience preparing data for Data Science and Machine Learning use casesExperience with master data managementExperience designing reports using Power BI, Power Query, and DAX', 'Experience building data pipelines to ingest unstructured and streaming data', 'Extensive experience with T-SQL (queries and DDL)', 'Description', 'Experience M and Power Query data wrangling', 'Research, analyze, recommend and select technical approaches for solving challenging and complex development and integration problems', 'Skills & QualificationsMature understanding of data warehouse and data lake concepts and design (including star schemas) 5+ years of data solution delivery experience with 1+ years on the Microsoft Azure platform:Azure Data Factory (including pipelines and mapping & wrangling data flows)Azure SQL (nice to have: Azure Data Warehouse/Synapse)Azure Data Lake Gen2Azure DevOpsExtensive experience with T-SQL (queries and DDL)Extensive experience querying API endpointsExperience M and Power Query data wranglingExperience developing and promoting work through devops pipelines and utilizing source control, preferably GIT, and Visual Studio’s database toolsExperience with Agile/Scrum methodology preferredExcellent analytical, conceptual, and problem-solving abilitiesEntrepreneurial attitude with a passion to deliver value for the organization and a desire to foster and develop team members PreferredExperience building data pipelines to ingest unstructured and streaming dataExperience preparing data for Data Science and Machine Learning use casesExperience with master data managementExperience designing reports using Power BI, Power Query, and DAXMcCarthy is proud to be an equal opportunity and affirmative action employer regardless of race, color, gender, age, sexual orientation, gender identity, religious beliefs, marital status, genetic information, national origin, disability or protected veteran status.', 'Azure Data Factory (including pipelines and mapping & wrangling data flows)Azure SQL (nice to have: Azure Data Warehouse/Synapse)Azure Data Lake Gen2Azure DevOps', 'Azure DevOps', 'Design, implement, and support ETL and ELT pipelines using Azure Data Factory pipelines and dataflows (both mapping and wrangling)', 'Azure SQL (nice to have: Azure Data Warehouse/Synapse)', 'Experience with Agile/Scrum methodology preferred', 'Experience designing reports using Power BI, Power Query, and DAX', 'McCarthy is proud to be an equal opportunity and affirmative action employer regardless of race, color, gender, age, sexual orientation, gender identity, religious beliefs, marital status, genetic information, national origin, disability or protected veteran status.', 'Experience developing and promoting work through devops pipelines and utilizing source control, preferably GIT, and Visual Studio’s database tools']",Entry level,Full-time,Information Technology,Construction,2021-01-19 09:25:31
Data Engineer,OneMagnify,"Dearborn, MI",19 hours ago,Be among the first 25 applicants,"['', 'Presence demonstrating confidence, ability to learn quickly, influence, and shape ideas\xa0', 'Ability to establish and maintain cooperative and effective working relationships with application implementation teams, IT project teams, business customers, and end users.', '3+ years of experience with R, Python, SAS, MATLAB, Java, etc.', 'Possess excellent oral and written communication skills, as well as facilitation and presentation skills, and engaging presentation style.\xa0', 'Experience with agile/lean methodologiesExperience working independently and with minimal supervisionExperience with a global teamExperience with Test Driven Development and Software Craftsmanship\xa0', 'Other', 'Nice to Have', 'Work with data scientists and software engineers to support data acquisition activities, data solution ideation, and implementation\xa0', 'Experience with BI tools, such as Informatica, Data Stage, QlikView, Tableau, etc.', 'Provide extensive technical, strategic advice and guidance to key stakeholders around the data transformation efforts', 'Work with data scientists and software engineers to support data acquisition activities, data solution ideation, and implementation\xa0Work with technical and business leads to transfer global business requirements into sound solutions and implementation\xa0Share support responsibilities for implemented components\xa0', '3+ years of experience with PCF cloud services', 'Strong Communications skills', '3+ years of Hive, Spark, JavaScript, SQL, HTML', 'Redesign data flows to prevent recurring data issues', 'Work with technical and business leads to transfer global business requirements into sound solutions and implementation\xa0', 'Design data pipelines and data robots, take a vision and bring it to life', 'OneMagnify is an equal opportunity employer.', '\xa0', 'Experience with agile/lean methodologies', 'Strong analytical and problem-solving skills', '3+ years of experience with R, Python, SAS, MATLAB, Java, etc.3+ years of Hive, Spark, JavaScript, SQL, HTML3+ years of experience with PCF cloud servicesExperience with Hadoop, Spark, KafkaFamiliar with big data and machine learning tools and platformsExperience with BI tools, such as Informatica, Data Stage, QlikView, Tableau, etc.Design data pipelines and data robots, take a vision and bring it to lifeMaster data engineer; teaches others; works closely with IT architects to set strategy and design projectsLead a team of Associate Data Engineers and Data EngineersProvide extensive technical, strategic advice and guidance to key stakeholders around the data transformation effortsRedesign data flows to prevent recurring data issuesStrong analytical and problem-solving skillsPossess excellent oral and written communication skills, as well as facilitation and presentation skills, and engaging presentation style.\xa0Ability to work as a global team member, as well as independently, in a changing environment and managing multiple priorities.Ability to establish and maintain cooperative and effective working relationships with application implementation teams, IT project teams, business customers, and end users.Ability to deliver work within deadlines.\xa0', 'Experience with Hadoop, Spark, Kafka', 'This role is for experienced Data Engineer that will be responsible for designing and building the foundational components required for our customer. This includes bringing disparate sources into our Hadoop data lake and creating data products that will be used by the analytics and reporting teams. Data engineers will work in small, cross-functional teams. They will collaborate directly and continuously with product managers, designers, and product owners to release early and often.\xa0\xa0\xa0\\', 'OneMagnify powers business performance for its clients with actionable analytics, compelling marketing communications through brand strategy, and technology solutions for companies here and around the world. We think that’s pretty cool, making us an exciting place to work. But there is so much more to this employee-centric company. For example, it’s all the fun and important things we do—like our charitable giving team program, summer poker walks, quarterly cake day and, of course, the creative client solutions we deliver daily. It all stems from a culture of caring—for each other, our clients and the world around us.\xa0', 'Ability to illustrate and convey ideas and prototypes effectively with team and partners\xa0', 'Technical Skills Required ', 'Ability to work as a global team member, as well as independently, in a changing environment and managing multiple priorities.', 'Share support responsibilities for implemented components\xa0', 'Familiar with big data and machine learning tools and platforms', 'Experience with a global team', 'Experience working independently and with minimal supervision', 'Master data engineer; teaches others; works closely with IT architects to set strategy and design projects', 'Experience with Test Driven Development and Software Craftsmanship\xa0', ' Other', 'Lead a team of Associate Data Engineers and Data Engineers', 'Strong Communications skillsAbility to illustrate and convey ideas and prototypes effectively with team and partners\xa0Presence demonstrating confidence, ability to learn quickly, influence, and shape ideas\xa0', 'Ability to deliver work within deadlines.\xa0']",Mid-Senior level,Full-time,Information Technology,Marketing and Advertising,2021-01-19 09:25:31
Data Engineer,LendingClub,"San Francisco, CA",23 hours ago,26 applicants,"['', ' Experience with BigData technologies stack such as HBase, Hadoop, Hive, Oozie, MapReduce is a plus ', ' Assemble large, complex data sets that meet functional / non-functional business requirements and fostering data-driven decision making across the organization ', ' Experience with schema design and dimensional data modeling is a plus ', ' Performs root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement ', 'Current Employees of LendingClub: Please apply via your internal Workday Account  ', ' Passionate with good understanding of with a focus on having fun while delivering incredible business results ', 'Current Employees of LendingClub: Please apply via your internal Workday Account ', ' BS or MS degree in Computer Science or a related technical field ', 'LendingClub is an equal opportunity employer and dedicated to diversity and inclusion in the workplace. We do not discriminate on the basis of race, religion, color, national origin, sex, gender, gender identity, sexual orientation, age, marital status, pregnancy status, veteran status, or disability status. We believe that a variety of perspectives will make our teams and business stronger as we work together to transform the traditional banking system .', "" What You'll Do "", ' A seasoned professional with 5+ 3+ years of relevant experience who is excited to apply their current skills and to grow their knowledge base. ', ' Experience with SQL/No-SQL , schema design and dimensional data modeling ', ' About the Role ', ' Experience in AWS/Spark/Java/Python development is a plus ', ' Familiar with Agile methodology, test-driven development, source control management and automated testing. ', "" Develops and maintains scalable data pipelines and builds out new integrations and processes required for optimal extraction, transformation, and loading of data from a wide variety of data sources using HQL and 'Big Data' technologies "", ' A seasoned professional with 5+ 3+ years of relevant experience who is excited to apply their current skills and to grow their knowledge base.  Passionate with good understanding of with a focus on having fun while delivering incredible business results  BS or MS degree in Computer Science or a related technical field  Experience with data pipeline, data analytics, data warehousing and big data  Experience with SQL/No-SQL , schema design and dimensional data modeling  Experience with schema design and dimensional data modeling is a plus  Experience with BigData technologies stack such as HBase, Hadoop, Hive, Oozie, MapReduce is a plus  Experience in AWS/Spark/Java/Python development is a plus  Familiar with Agile methodology, test-driven development, source control management and automated testing. ', ' Implements processes and systems to validate data, monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it. ', 'About You', ' Writes unit/integration tests, contributes to engineering wiki, and documents work. ', "" Develops and maintains scalable data pipelines and builds out new integrations and processes required for optimal extraction, transformation, and loading of data from a wide variety of data sources using HQL and 'Big Data' technologies  Assemble large, complex data sets that meet functional / non-functional business requirements and fostering data-driven decision making across the organization  Implements processes and systems to validate data, monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it.  Writes unit/integration tests, contributes to engineering wiki, and documents work.  Performs root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement "", ' Experience with data pipeline, data analytics, data warehousing and big data ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-01-19 09:25:31
Data Engineer,Data Bridge Consultants,"Kansas City, MO",58 minutes ago,Be among the first 25 applicants,[''],Entry level,Full-time,Information Technology,Staffing and Recruiting,2021-01-19 09:25:31
Data & Visualization Engineer,Varian Medical Systems,"Atlanta, GA",9 hours ago,Be among the first 25 applicants,"['', 'Implement database modules for SQL Server, including database schemas and complex & optimized stored procedures, and develop unit test-drivers and conduct appropriate testing.', 'Perform necessary duties to support Varian quality policy and compliance needs including detailed documentation.', 'Should be able to support development using the SCRUM framework', 'Perform other project activities as required for team/organization support', 'Technologies Required', 'Tableau', 'Design, create, build and maintain data pipelines', 'PowerBI /PowerBI reporting', 'Knowledge of one programming language- preferably Python.', 'Communicate with cross functional teams to capture requirements and convert them to support system architecture', 'Azure Cloud', 'Fighting cancer calls for big ideas.', 'PowerBI /PowerBI reportingAzure SQLTableauAzure CloudAzureSQL Server,Datalake, NoSqlLanguages: Java, J2EE, Python (preferable)ETL Technology: SSIS, Databricks, Dremio, Azure datafactoryBI Technology- Powerbi, Tableau', 'Qualifications Required:', 'Strong proficiency in one or more Relational Database systems preferably SQL Server in theareas of Report/Dashboard building, Query optimization and performance improvements', 'Analyze business requirements from product management and develop reports and dashboards for self-service reporting needs.', 'Familiarity with Microsoft Azure technologies', 'Detect report, investigate, analyze and fix product defects.', '#TogetherWeFight', 'Coordinate with other team member.', 'Experience in report/dashboard development preferably in delivery of healthcare products is plus.Should be able to support development using the SCRUM frameworkKnowledge of one programming language- preferably Python.Familiarity with Microsoft Azure technologiesFamiliarity of Big Data Concepts and Hands-on is plus.Hands on experience with Erwin data modeling tools or other equivalent ETL tool is plus.Machine learning model integration with BI', 'Work as Scrum Master as and when required.', 'Translates business needs into Business Intelligence solutions for VarianDevelop database design specification and detailed design for the feature/modules being implemented as part of the project.Implement database modules for SQL Server, including database schemas and complex & optimized stored procedures, and develop unit test-drivers and conduct appropriate testing.Analyze business requirements from product management and develop reports and dashboards for self-service reporting needs.Communicate with cross functional teams to capture requirements and convert them to support system architectureSupport DB change request process and manage the associated impact.Support project by providing detailed effort estimates for assigned modules.Perform reviews (Code/Design) for the team.Conduct unit testing and participates in quality assurance testingDetect report, investigate, analyze and fix product defects.Perform other project activities as required for team/organization supportPerform necessary duties to support Varian quality policy and compliance needs including detailed documentation.Coordinate with other team member.Work as Scrum Master as and when required.', 'Experience with Big Data tools: Hadoop, Spark, Kafka, etc.', 'Together, we can beat cancer.', 'Perform reviews (Code/Design) for the team.', 'Hands on experience with Erwin data modeling tools or other equivalent ETL tool is plus.', 'Languages: Java, J2EE, Python (preferable)', 'Familiarity of Big Data Concepts and Hands-on is plus.', 'Support project by providing detailed effort estimates for assigned modules.', 'Conduct unit testing and participates in quality assurance testing', 'Skills', 'Develop database design specification and detailed design for the feature/modules being implemented as part of the project.', 'Experience (Years): 3-5 years', 'Experience (Years):', 'ETL Technology: SSIS, Databricks, Dremio, Azure datafactory', 'Azure SQL', 'AzureSQL Server,Datalake, NoSql', 'Building a cloud-based platform that allows easy development of new BI applications', 'Minimum 3 years of experience in IT with at least 1 years in database design and optimization along with 1 years of experience in BI and Data WarehousingStrong proficiency in one or more Relational Database systems preferably SQL Server in theareas of Report/Dashboard building, Query optimization and performance improvementsShould have hands on experience in the Azure SQL,Demonstrates solid understanding of database schema design, data warehousing principles and design considerations for multidimensional databases.Experience writing and tuning complex SQL, MDX queries.Strong business analysis skills are required.Design, create, build and maintain data pipelinesBuilding a cloud-based platform that allows easy development of new BI applicationsExperience with Big Data tools: Hadoop, Spark, Kafka, etc.', 'Translates business needs into Business Intelligence solutions for Varian', 'BI Technology- Powerbi, Tableau', 'Experience in report/dashboard development preferably in delivery of healthcare products is plus.', 'Nice To Have Experiences', 'Demonstrates solid understanding of database schema design, data warehousing principles and design considerations for multidimensional databases.', 'Experience writing and tuning complex SQL, MDX queries.', 'BE, MSc, MCA, Computer Science or related discipline. ', 'Support DB change request process and manage the associated impact.', 'Should have hands on experience in the Azure SQL,', 'Minimum 3 years of experience in IT with at least 1 years in database design and optimization along with 1 years of experience in BI and Data Warehousing', 'Machine learning model integration with BI', 'Strong business analysis skills are required.', 'Responsibilities']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-01-19 09:25:31
Data and Business Intelligence Engineer,Vendr,"Boston, MA",6 hours ago,Be among the first 25 applicants,[],Entry level,Full-time,Business Development,Information Technology and Services,2021-01-19 09:25:31
Data Engineer - Richmond,Lorvenk Technologies,"Richmond, VA",7 hours ago,Be among the first 25 applicants,"['', 'Client', 'You will participate in detailed technical design, development and implementation of applications used by our data scientists and business analysts to build and launch models, analyze data, and make decisions.', 'Min 7+ years.', 'Visa', ""As a Data Engineer, you'll be part of a team that’s building new analytical and machine learning tools and frameworks to exploit advantages in the latest developments in cloud computing - EMR, Airflow, SageMaker, etc.You will participate in detailed technical design, development and implementation of applications used by our data scientists and business analysts to build and launch models, analyze data, and make decisions."", ""As a Data Engineer, you'll be part of a team that’s building new analytical and machine learning tools and frameworks to exploit advantages in the latest developments in cloud computing - EMR, Airflow, SageMaker, etc."", '2+ years of data modeling experience.', '5+ years of experience in Python, Scala, or R for large scale data-analysis.', '2+ years of experience with Cloud computing (AWS).', 'Duration', ' Richmond, VA / McLean, VA', 'Role: ', '2+ years of experience with Spark.', ""5+ years' experience with Relational Database Systems and SQL (PostgreSQL or Redshift)."", 'Qualifications', '5+ years of UNIX/Linux experience.', 'Overall IT Experience of 8+ Years.', ""Overall IT Experience of 8+ Years.5+ years of experience in Python, Scala, or R for large scale data-analysis.5+ years' experience with Relational Database Systems and SQL (PostgreSQL or Redshift).5+ years of UNIX/Linux experience.2+ years of data modeling experience.2+ years of experience with Cloud computing (AWS).2+ years of experience with Spark."", 'Location:', 'Experience', 'Responsibilities', '2+ years.']",Entry level,Full-time,Information Technology,Staffing and Recruiting,2021-01-19 09:25:31
Data Engineer,Athletica_AI,United States,30 minutes ago,Be among the first 25 applicants,"['Have you ever wondered how technology can help athletes? In this position, your job will be to take part in the development of data science- and machine learning-based softwares which will help athletes by providing an adaptive coaching plan that\xa0adapts to the individual’s fitness levels, goals, training sessions and lifestyles, based on first principle exercise science. You will get to work in a small team and take part in all aspects of the algorithm developing process. However, your workload will mostly include cloud service related operations, database design and maintenance. You will focus on bringing the development version of the algorithms into production, working on our backend, mostly in Python.', '\xa0Required skills:', 'You’d work within our small, passionate team. Your work will be valued, your voice will be heard, and you will be supported and contribute daily to the development of Athletica. This is a remote position that you will be able to perform from anywhere in the world that has good internet connection, with flexible working hours, while sometimes having conference calls or one-to-one calls with team members in different time zones.', 'Knowledge of data science and machine learning tools and techniques', 'Ability to think outside the box with a growth mindset', 'Knowledge of the Python language and libraries such as Pandas and Sklearn', 'A genuine interest and experience in sports/sports science, coaching and statistics', 'Heroku and/or Docker container experienceFamiliarity with web based technologies (HTML, PHP, JavaScript) or other programming languagesA genuine interest and experience in sports/sports science, coaching and statistics', 'BSc or MSc in computer science or related field (or demonstrated equivalent level, such as through industry experience)Experience with cloud services and containerization technologiesKnowledge of data science and machine learning tools and techniquesKnowledge of the Python language and libraries such as Pandas and SklearnAbility to prioritise and work remotely by yourself, with people with different backgrounds and within a team with an international profileAbility to think outside the box with a growth mindset', 'Familiarity with web based technologies (HTML, PHP, JavaScript) or other programming languages', 'Ability to prioritise and work remotely by yourself, with people with different backgrounds and within a team with an international profile', 'Experience with cloud services and containerization technologies', 'You’d move to the top of our list if you also have any of the following skills, knowledge or experience:', 'Heroku and/or Docker container experience', 'BSc or MSc in computer science or related field (or demonstrated equivalent level, such as through industry experience)']",Entry level,Part-time,Information Technology,Sports,2021-01-19 09:25:31
Data Engineer,Gridiron IT,"Chantilly, VA",9 hours ago,Be among the first 25 applicants,[''],Entry level,Full-time,Information Technology,Information Technology and Services,2021-01-19 09:25:31
Data Engineer,Brightwing,Dallas-Fort Worth Metroplex,21 hours ago,32 applicants,"['', 'Translate business requirements into technology solutions', 'Effectively communicate with Business users and customers.', 'Best Practices in leading a development team providing guidance / training and taking care of the work reviews.', 'Examine and identify database structural necessities by evaluating client operations, applications, and programming.', 'o\xa0\xa0\xa0Define Different Data Framework and Standards around those.', 'o\xa0\xa0\xa0Examine and identify database structural necessities by evaluating client operations, applications, and programming.', 'o\xa0\xa0\xa0Review the designs and code created by others and provides constructive feedback.', 'Identify and assess risks due to changes in scope (business decisions/changes), resources, or timeline', 'Define Different Data Framework and Standards around those.', 'o\xa0\xa0\xa0Build and Design end to end data pipeline using Amazon AWS, Spark and Snowflake data warehouse', 'o\xa0\xa0\xa0Work with product owners to identify areas requiring decisions/clarity in order to define project scope and objectives/requirements.\xa0', 'o\xa0\xa0\xa0Align architecture diagrams and process flows to project plan to identify and remediate gaps', 'o\xa0\xa0\xa0Effectively communicate with Business users and customers.', 'Perform problem determination (root cause analysis), document the source of problems and their resolution. Recognize and analyze trends in errors so as to identify and install long term solutions to problems.', 'o\xa0\xa0\xa0ETL performance tunings using optimization, code quality and standards.', 'Align architecture diagrams and process flows to project plan to identify and remediate gaps', 'o\xa0\xa0\xa0Integrate the data pipelines with various upstream and downstream applications', 'ETL performance tunings using optimization, code quality and standards.', 'Review the designs and code created by others and provides constructive feedback.', 'o\xa0\xa0\xa0Translate business requirements into technology solutions', 'o\xa0\xa0\xa0Best Practices in leading a development team providing guidance / training and taking care of the work reviews.', 'Recognize and adopt best practices in reporting and analysis: data integrity, test design,\xa0analysis, validation, and documentation.', 'Work with product owners to identify areas requiring decisions/clarity in order to define project scope and objectives/requirements.\xa0', 'o\xa0\xa0\xa0Perform problem determination (root cause analysis), document the source of problems and their resolution. Recognize and analyze trends in errors so as to identify and install long term solutions to problems.', 'o\xa0\xa0\xa0Understanding of Conceptual /Logical and Physical Data Models; define logical views and physical data security structures.', 'Development Expertise with Spark Using Python', 'Aid in gathering Requirements, conducting Business Analysis, and writing technical design specifications.', 'o\xa0\xa0\xa0Aid in gathering Requirements, conducting Business Analysis, and writing technical design specifications.', 'o\xa0\xa0\xa0Recognize and adopt best practices in reporting and analysis: data integrity, test design,\xa0analysis, validation, and documentation.', 'In depth knowledge about data warehousing (data acquisition, data management and data consumption)\xa0and Cloud data Platform ETL Design on assigned projects.', 'o\xa0\xa0\xa0Adhere to Development Standards, Best Practices and Guidelines to ensure compatibility, scalability, and integration with other data platforms.', 'Build and Design end to end data pipeline using Amazon AWS, Spark and Snowflake data warehouse', 'Adhere to Development Standards, Best Practices and Guidelines to ensure compatibility, scalability, and integration with other data platforms.', 'Understanding of Conceptual /Logical and Physical Data Models; define logical views and physical data security structures.', 'o\xa0\xa0\xa0Development Expertise with Spark Using Python', 'Integrate the data pipelines with various upstream and downstream applications', 'o\xa0\xa0\xa0Identify and assess risks due to changes in scope (business decisions/changes), resources, or timeline']",Mid-Senior level,Full-time,Information Technology,Automotive,2021-01-19 09:25:31
Data Engineer,Ingrain Systems Inc,"Groveport, OH",21 hours ago,Be among the first 25 applicants,"['Client - GAP Inc:', '• High Problem-Solving Skills with analytical thinking and logical reasoning • Multiple project execution experience with PYSpark – RDDs, SparkSQL, Spark streaming, Structured Streaming • Hands-on knowledge on Azure SQL • Knowledge on Hive-Spark, HBase-Spark integrations • Working experience on cloud-based spark environments such as Databricks on Azure or AWS. • Hands on experience writing notebooks using Python/Scala. • Automation using Linux shell & Python scripting. • Proficient in streaming tech stack based on Kafka – Heterogeneous producers/consumers, Kafka Connectors, Schema registry, KSQL, KStreams. (Knowledge on Confluent platform would be an advantage). • Development experience on consuming Kafka streams from Spark & Java applications. • Hands on experience working with huge datasets in HDFS & Hive/HQL. • Development experience with Enterprise integration patterns. • Experience building cross platform data integration. • Working knowledge of REST Web Services. • Proficient working knowledge of at least one external caching tools (Memcache, Redis) • Working knowledge of at least one No-SQL database like MongoDB, Cassandra, HBase. • Experience working on Azure stack would be preferred such as – EventHubs/EventGrid/ServiceBus, Stream Analytics REQUIRED EXPERIENCE AND EDUCATION: • 5 – 8years of experience with BigData & Cloud Applications in software industry • CS degree or equivalent experience', 'Note: This is full time with Ingrain Systems. Corp-Corp candidates and H1B candidates please excuse.', 'Note: This is full time with Ingrain Systems. Corp-Corp candidates and H1B candidates please excuse. ', 'This is remote as of now. After COVID, candidate reports to Groveport,Ohio location of Gap Inc.', 'Client - GAP Inc:GAP Inc, a leader in the Retail domain employs cutting edge solutions to deliver a world class e-commerce experience. You will get to work amidst the best in the industry. ', ' ', 'Requirements: ', 'This is remote as of now. After COVID, candidate reports to Groveport,Ohio location of Gap Inc. ']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-01-19 09:25:31
Data Engineer,General Mills,"Minneapolis, MN",4 hours ago,42 applicants,"['', 'Minimum 2 years of IT experience, 3+ preferredCloud data experience', 'Development experience using Hive and/or Spark', 'Python, Scala or Java development experience', 'Experience with agile techniques or methods', 'Partner with business analysts and solutions architects to deliver business initiatives.', 'Familiarity with the Linux operating system', 'Minimum Qualifications', 'Key Responsibilities', 'Strong understanding of Hadoop fundamentals', 'Job Scheduling experience', 'Participate in the evaluation, implementation and deployment of emerging tools & process in the big data space.', 'Python, Scala or Java development experienceFamiliarity with Kafka Familiarity with the Linux operating systemExperience with agile techniques or methods', 'Act as a key technical leader within General Mills', 'Effective verbal and written communication and influencing skills.', 'Preferred Qualifications', 'Bachelor’s Degree preferred; Computer Science, MIS, or Engineering preferred', 'Effective analytical and technical skills.', 'Familiarity with Kafka ', 'Act as a key technical leader within General MillsDesign, create, code, and support a variety of ETL solutions (potentially including but not limited to: Talend Studio, Python, Scala, Kafka, SAP Data Services, or others)Generate and implement your own ideas on how to improve the operational and strategic health of big data ecosystemParticipate in the evaluation, implementation and deployment of emerging tools & process in the big data space.Partner with business analysts and solutions architects to deliver business initiatives.Collaboratively troubleshoot technical and performance issues in the big data ecosystem', 'Generate and implement your own ideas on how to improve the operational and strategic health of big data ecosystem', 'Ability to research, plan, organize, lead, and implement new processes or technology', 'Collaboratively troubleshoot technical and performance issues in the big data ecosystem', 'Ability to work in a team environment', 'Database development experience using Oracle, SQL Server, SAP BW or SAP HANA', 'Design, create, code, and support a variety of ETL solutions (potentially including but not limited to: Talend Studio, Python, Scala, Kafka, SAP Data Services, or others)', 'Bachelor’s Degree preferred; Computer Science, MIS, or Engineering preferredMinimum 2 years of IT experience, 3+ preferredCloud data experienceStrong understanding of Hadoop fundamentalsDatabase development experience using Oracle, SQL Server, SAP BW or SAP HANAJob Scheduling experienceProcess mindset with experience creating, documenting and implementing standard processesDevelopment experience using Hive and/or SparkEffective verbal and written communication and influencing skills.Effective analytical and technical skills.Ability to work in a team environmentAbility to research, plan, organize, lead, and implement new processes or technology', 'Overview', 'Process mindset with experience creating, documenting and implementing standard processes', 'Company Overview']",Entry level,Full-time,Information Technology,Food & Beverages,2021-01-19 09:25:31
Tableau Engineer / Data Analyst,SignalPath,"Raleigh, NC",12 hours ago,Be among the first 25 applicants,"['', 'Essential Experience', 'Expertise in developing, maintaining and managing Tableau driven dashboards & analytics, preferably with working knowledge of Tableau administration/architecture', 'Degree in Statistics, Mathematics, Computer Science, Information Systems or Accounting preferred. Or relevant work experience may be considered.', 'Provide application analysis and data modeling design to collect data for centralized data warehouse.', 'Great location in downtown Raleigh.', 'Minimum 2+ years of experience with Tableau Business Intelligence', 'In-depth knowledge and hands-on experience of RDBMS and NoSQL systems, specifically Postgres, MongoDB, etc.', 'Analytics experience manipulating large data sets to formulate insights and drive solutions', 'Flexible work schedule', 'Experience with data warehousing tools such as Redshift or Snowflake', 'Strong verbal and writing skills', 'Deep understanding of SQL, relational databases & normalization.', 'Write and edit custom SQL queries to generate customer-facing dashboards and reports in Tableau for both sites and pharma sponsors designed to meet their clinical trial operational needs', 'Analyze reporting requests to develop solutions addressing common needs across sites and pharma sponsors practices', 'Proficient in the use of query and reporting analysis tools.', 'Requirements', 'Developing, maintaining, and managing reporting, analytics, and dashboards.', '401k Match', 'Build, support, and manage analytical requirements for our customers on the AWS data mgmt suite', ' Degree in Statistics, Mathematics, Computer Science, Information Systems or Accounting preferred. Or relevant work experience may be considered. Minimum 2+ years of experience with Tableau Business Intelligence Deep understanding of SQL, relational databases & normalization. Analytics experience manipulating large data sets to formulate insights and drive solutions Experience with data warehousing tools such as Redshift or Snowflake Expertise in developing, maintaining and managing Tableau driven dashboards & analytics, preferably with working knowledge of Tableau administration/architecture Proficient in the use of query and reporting analysis tools. Standardize data collection by developing methods for database design and validation reports. Provide application analysis and data modeling design to collect data for centralized data warehouse. Strong verbal and writing skills Extract data from databases and data warehouses for reporting and to facilitate sharing between multiple data systems. ', 'Manage the Tableau solution architecture, administration and actively manage the performance and usability. Align/standardize team members on best practices around Tableau driven implementations', 'Maintain data catalogue / data dictionary for both internal and external data sets using AWS Glue; Oversee API Catalogue and drive API Roadmap and vision', 'Reviewing and improving existing systems and collaborating with teams to integrate with new systems', 'Benefits', 'Some travel may be required ', 'Standardize data collection by developing methods for database design and validation reports.', '20 days of vacation, 1 personal floating holiday, and 9 company holidays', 'Extract data from databases and data warehouses for reporting and to facilitate sharing between multiple data systems.', 'Ability to work with ETL tools in an RDBMs and NoSQL environment', ' Build, support, and manage analytical requirements for our customers on the AWS data mgmt suite Developing, maintaining, and managing reporting, analytics, and dashboards. Performing and documenting data analysis, data validation, and data mapping/design Reviewing and improving existing systems and collaborating with teams to integrate with new systems Write and edit custom SQL queries to generate customer-facing dashboards and reports in Tableau for both sites and pharma sponsors designed to meet their clinical trial operational needs Analyze reporting requests to develop solutions addressing common needs across sites and pharma sponsors practices Maintain data catalogue / data dictionary for both internal and external data sets using AWS Glue; Oversee API Catalogue and drive API Roadmap and vision Develop database queries to analyze the results and troubleshoot any issues Ability to work with ETL tools in an RDBMs and NoSQL environment Manage the Tableau solution architecture, administration and actively manage the performance and usability. Align/standardize team members on best practices around Tableau driven implementations Some travel may be required  ', 'Develop database queries to analyze the results and troubleshoot any issues', ' Competitive health, dental, life and disability insurance plans 401k Match Great location in downtown Raleigh. 20 days of vacation, 1 personal floating holiday, and 9 company holidays Flexible work schedule', ' In-depth knowledge and hands-on experience of RDBMS and NoSQL systems, specifically Postgres, MongoDB, etc. Experience with R development a big plus Experience with stored procedures and views ', 'Experience with R development a big plus', 'Performing and documenting data analysis, data validation, and data mapping/design', 'Experience with stored procedures and views', 'Competitive health, dental, life and disability insurance plans']",Mid-Senior level,Full-time,Analyst,Information Technology and Services,2021-01-19 09:25:31
Data Science Engineer,"BioIntelliSense, Inc","Golden, CO",2 hours ago,Be among the first 25 applicants,"['', 'Requirements', 'Description']",Entry level,Full-time,Information Technology,Medical Devices,2021-01-19 09:25:31
Big Data Engineer,Mindlance,"Beaverton, OR",13 hours ago,Be among the first 25 applicants,[''],Entry level,Contract,Engineering,Staffing and Recruiting,2021-01-19 09:25:31
Data Engineer,The Select Group,"Denver, CO",22 hours ago,35 applicants,"['', 'Extensive experience writing complex SQL queries', 'Strong communication with a collaborative, team-player mindset', 'Data reporting for all video products and applications in addition to online streaming services', 'DATA ENGINEER RESPONSIBILITIES:', '2+ years of experience with TableauExtensive experience writing complex SQL queriesHands-on data warehousing experience with HiveGood experience with Amazon Web Services (AWS) - S3 buckets and creating lambdasStrong communication with a collaborative, team-player mindset', 'Heavy development focus and this resource will spend roughly 70% of their time in SQL, 20% in AWS and 10% in Tableau', 'Hands-on data warehousing experience with Hive', '\xa0', 'The Select Groun a teamp is looking for a Data Engineer to join one of our largest clients in the communications space. This person will be joining the team responsible for data reporting on all video products and applications. This opportunity will be remote during COVID and will eventually transition into being onsite in Denver, CO. If you are looking to get your foot in the door with a large enterprise leader in the communications industry, then this is the perfect opportunity for you!', 'DATA ENGINEER - DENVER, CO.', 'Creating tables on new proucts to pull data analytics and monitor user activity', '·\xa0', 'Good experience with Amazon Web Services (AWS) - S3 buckets and creating lambdas', 'Creating tables on new proucts to pull data analytics and monitor user activityData reporting for all video products and applications in addition to online streaming servicesHeavy development focus and this resource will spend roughly 70% of their time in SQL, 20% in AWS and 10% in Tableau', '2+ years of experience with Tableau', 'DATA ENGINEER\xa0REQUIREMENTS:']",Mid-Senior level,Contract,Information Technology,Telecommunications,2021-01-19 09:25:31
Data Engineer,"Munich Reinsurance America, Inc.","New York, NY",4 minutes ago,Be among the first 25 applicants,"['Experience working on data projects involving data engineers, architects and analysts.', 'Adhere and enhance ETL and architecture standards and procedures to ensure consistency and accuracy of development, execution, knowledge sharing and documentation.', 'Adhere and enhance the existing data governance framework and personally ensure compliance with regulatory and privacy requirements.', 'Knowledgeable with agile development methodologies and tools (i.e. Jira, AzureDevOps)', 'Commercial : Ability to understand complex business priorities and translate them into clearly defined technical/data specifications for implementation', 'In depth knowledge of database structure principles, data modelling concepts, ETL procedures, Cloud data practices etc.', 'Exposure of working with Big data tools and technologies like Data Bricks, HIVE and SPARK.', '3 + years hands on experience in SQL, HSQL, Python or R, and Azure Web services (Azure SQL, Azure Data Factory, Logic Apps etc.) and other web development application', 'Experience/knowledge of insurance preferably commercial specialty', 'Experience with writing advanced SQL queries and performing query optimization.', ""3+ years' experience in a data engineering role with exposure to system design, data modelling, data-curation and validation proceduresExperience in creating scalable and robust Data Pipelines and working with data lakes.Experience with writing advanced SQL queries and performing query optimization.Exposure of working with Big data tools and technologies like Data Bricks, HIVE and SPARK.3 + years hands on experience in SQL, HSQL, Python or R, and Azure Web services (Azure SQL, Azure Data Factory, Logic Apps etc.) and other web development applicationBachelor's Degree in Computer Science, Statistics, Math or equivalent combination of education and experienceIn depth knowledge of database structure principles, data modelling concepts, ETL procedures, Cloud data practices etc.Knowledge of data provisions requirements for web based business intelligence tools including Cognos, PowerBI and Tableau.Experience/knowledge of insurance preferably commercial specialtyExperience working on data projects involving data engineers, architects and analysts.Knowledgeable with agile development methodologies and tools (i.e. Jira, AzureDevOps)"", 'Communication: Ability to communicate clearly, effectively, and concisely, both verbally and in written form.', 'Knowledge of data provisions requirements for web based business intelligence tools including Cognos, PowerBI and Tableau.', 'Collaborate with technical and business experts to improve our existing data landscape and drive better data driven decisions within the business.', 'Act as the owner and developer of a predefined subset of processes and data pipelines, ensuring highly accurate and reliable delivery of outputsCollaborate with technical and business experts to improve our existing data landscape and drive better data driven decisions within the business.Adhere and enhance ETL and architecture standards and procedures to ensure consistency and accuracy of development, execution, knowledge sharing and documentation.Work closely with business architecture, IT and analytic teams help implement an end to end data strategy from data ingest to output into business tools.Coordinate closely with delivery lead and data team lead to ensure work is scoped, planned and delivered on-time and budget.Adhere and enhance the existing data governance framework and personally ensure compliance with regulatory and privacy requirements.Maintain a close working relationship with other group data teams within Munich RE to understand existing solutions and resources available for use within MRSI.', 'Act as the owner and developer of a predefined subset of processes and data pipelines, ensuring highly accurate and reliable delivery of outputs', ""Bachelor's Degree in Computer Science, Statistics, Math or equivalent combination of education and experience"", 'Work closely with business architecture, IT and analytic teams help implement an end to end data strategy from data ingest to output into business tools.', 'Maintain a close working relationship with other group data teams within Munich RE to understand existing solutions and resources available for use within MRSI.', 'Experience in creating scalable and robust Data Pipelines and working with data lakes.', ' This position requires sound technical skills, good judgement, strong operational focus and ability to organize and manage workflow in fast paced environment with focus on service and business value. Successful candidates will be possess the following skills/capabilities:', 'Coordinate closely with delivery lead and data team lead to ensure work is scoped, planned and delivered on-time and budget.', 'Organization: Ability to deal with ambiguity and operate in a fast-paced and rapidly changing environment with competing project priorities', 'At Munich Re, we see Diversity and Inclusion as a solution to the challenges and opportunities all around us. Our goal is to foster an inclusive culture and build a workforce that reflects the communities in which we live and work. We strive to provide a workplace where all of our colleagues feel respected, valued and empowered to achieve their very best every day. We recruit and develop talent with a focus on providing our customers the most innovative products and services.  We are an equal opportunity employer. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.', ""We're adding to our diverse team of experts and are looking to hire those who are committed to building a culture that enables the creation of innovative solutions for our business units and clients. The Insurance OperationsMunich Re US is launching new insurance operations described as Munich Re Specialty Insurance(MRSI) that will unite the strengths and expertise of Munich Re's specialty commercial teams in North America under a new management structure. This will allow for the development of an overarching sales strategy through which the specialty risk appetite and product offerings can be profitably optimized and expanded.  As a member of Munich Re's US operations, we offer the financial strength and stability that comes with being part of the world's preeminent insurance and reinsurance brand. Our risk experts work together to assemble the right mix of products and services to help our clients stay competitive - from traditional reinsurance coverages, to niche and specialty reinsurance and insurance products.The OpportunityFuture focused and always one step ahead The person in the role will execute on the technical development of modern data landscape for the company, building the requirement to deliver data in good quality for use in analytics and reporting. The role will sit on the Data Team, a group of data architects and data engineers charged with acquisition, organization, integration and secure storage of mission-critical and value added enterprise data. A successful individual will have very strong data engineering experience and can demonstrate mastery of data warehousing concepts using both current and emerging best practices and technologies. They will enjoy connecting the dots between process and have an aversion to slow development cycles and the same old ways of doing things. Speed and agility in developing and delivering data driven solutions will be critical. This role reports to the Head of Data & Analytics as part of a function that is accountable and responsible for the reliable delivery of data and insights that the business needs to understand performance and adjust strategy and operate smoothly in the face of ambitious growth and changing technology and operating models internally, and a shifting risk and competitive landscape. In this position you will: "", 'Technical: Strong coding skills and experience developing data pipelines for warehouses/lakes (academic project or work setting)Commercial : Ability to understand complex business priorities and translate them into clearly defined technical/data specifications for implementationInnovation: Forward thinking and passionate about solving problems and has a strong drive to constantly learn and keep up to speed with the new technologiesOrganization: Ability to deal with ambiguity and operate in a fast-paced and rapidly changing environment with competing project prioritiesCommunication: Ability to communicate clearly, effectively, and concisely, both verbally and in written form.', 'QualificationsSuccessful candidates will be possess the following skills/capabilities:', ""3+ years' experience in a data engineering role with exposure to system design, data modelling, data-curation and validation procedures"", 'Innovation: Forward thinking and passionate about solving problems and has a strong drive to constantly learn and keep up to speed with the new technologies', 'Technical: Strong coding skills and experience developing data pipelines for warehouses/lakes (academic project or work setting)']",Entry level,Full-time,Information Technology,Insurance,2021-01-19 09:25:31
Business Intelligence and Data Engineer,"INOC, LLC","Madison, WI",9 hours ago,Be among the first 25 applicants,[],Entry level,Full-time,Business Development,Information Technology and Services,2021-01-19 09:25:31
Data Engineer,FanDuel,New York City Metropolitan Area,12 minutes ago,Over 200 applicants,"['', 'What we’re looking for in our next teammate', '●\xa0\xa0\xa0\xa0\xa0Opportunities to build really cool products that fans love', 'FanDuel Sportsbook', 'TVG', 'We treat our team right', 'FanDuel Group is looking for an experienced Data Engineer with deep understanding of large-scale data handling and processing best practices in a cloud environment to help us build scalable systems. As our data is a key component of the business used by almost every facet of the company, including product development, marketing, operations, and finance. It is vital that we deliver robust solutions that ensure reliable access to data with a focus on quality and availability.', 'Delivering test plans, monitoring, debugging and technical documents as a part of development cycle', 'Comfortable writing Python scripts', 'Our competitive edge comes from making decisions based on accurate and timely data and your work will provide access to that data across the whole company. Looking ahead to the next phase of our data platform we are keen to do more with real time data processing and working with our data scientists to create machine learning pipelines.', 'PokerStars', 'Proficiency with agile or lean development practices', 'Designing and deploying data models and views with large datasets that meet functional / non-functional business requirements', '●\xa0\xa0\xa0\xa0\xa0Hall of Fame benefit programs and platforms', 'Advanced working SQL knowledge and experience working with relational databases', 'Demonstrate the ability to optimize processes (ram vs io)', 'Identifying, designing, and implementing internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability', 'Creating and maintain optimal data pipeline architectureDesigning and implementing data pipelines required in the data warehouse and data lake in batch or real-time using data transformation technologies', 'Understanding of AWS and Google Cloud', 'FanDuel Group is based in New York, with offices in California, New Jersey, Florida, Oregon and Scotland. Our brands include:', 'Our roster has an opening with your name on it', 'Creating and maintain optimal data pipeline architectureDesigning and implementing data pipelines required in the data warehouse and data lake in batch or real-time using data transformation technologiesIdentifying, designing, and implementing internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalabilityDesigning and deploying data models and views with large datasets that meet functional / non-functional business requirementsDelivering data integration solutions to downstream marketing and campaign softwareDelivering quality production-ready code in an agile environmentDelivering test plans, monitoring, debugging and technical documents as a part of development cycleCreating data tools for analytics and working with stakeholders across all departments to assist with data-related technical issues and supporting their data infrastructure needs', 'PokerStars\xa0US —\xa0The premier online poker product\xa0and affiliate of FanDuel Group', '●\xa0\xa0\xa0\xa0\xa0Flexible vacation allowance to let you refuel', 'Ability to quickly learn new technologies is critical', 'Show proficiency understanding complex ETL processes', 'Advanced working SQL knowledge and experience working with relational databasesBuild processes supporting data transformation, data structures, metadata, dependency, and workload managementShow proficiency understanding complex ETL processesDemonstrate the ability to optimize processes (ram vs io)Knowledge of data integrity and relational rulesUnderstanding of AWS and Google CloudAbility to quickly learn new technologies is criticalProficiency with agile or lean development practicesComfortable writing Python scripts', 'Creating data tools for analytics and working with stakeholders across all departments to assist with data-related technical issues and supporting their data infrastructure needs', 'FOXBet\xa0— A world-class betting platform and affiliate of FanDuel Group\xa0', 'FanDuel', 'TVG\xa0—\xa0The best-in-class horse racing TV/media network and betting platform\xa0', 'Build processes supporting data transformation, data structures, metadata, dependency, and workload management', 'Knowledge of data integrity and relational rules', 'Delivering data integration solutions to downstream marketing and campaign software', 'FanDuel Casino & Betfair Casino', 'Competitive compensation is just the beginning. As part of our team, you can expect:', 'THE POSITION', 'FanDuel Sportsbook\xa0—\xa0America’s #1 sports betting app\xa0', 'FanDuel Group is a world-class team of brands and products all built with one goal in mind —\xa0to give fans new and innovative ways to interact with their favorite games, sports, teams, and leagues. That’s no easy task, which is why we’re so dedicated to building a winning team. And make no mistake, we are here to win, but we believe in winning right. That means we’ll never compromise when it comes to looking out for our teammates. From our many opportunities for professional development to our generous insurance and paid leave policies, we’re committed to making sure our employees get as much out of FanDuel as we ask them to give.', 'THE STATS', 'FanDuel\xa0—\xa0A game-changing real-money fantasy sports app\xa0', 'FOXBet\xa0', 'THE CONTRACT', 'Everyone on our team has a part to play', 'FanDuel Racing\xa0—\xa0A horse racing app built for the average sports fan\xa0', 'THE GAME PLAN', '●\xa0\xa0\xa0\xa0\xa0An exciting and fun environment committed to driving real growth', ""FanDuel Group is an equal opportunities employer. Diversity and inclusion in FanDuel means that we respect and value everyone as individuals. We don't tolerate bias, judgement or harassment.\xa0Our focus is on developing employees so that they reach their full potential."", '●\xa0\xa0\xa0\xa0\xa0Mentorship and professional development resources to help you refine your game', 'FanDuel Casino & Betfair Casino\xa0—\xa0Fan-favorite online casino apps\xa0', 'ABOUT FANDUEL GROUP', 'Delivering quality production-ready code in an agile environment', 'FanDuel Racing']",Mid-Senior level,Full-time,Information Technology,Sports,2021-01-19 09:25:31
Big Data Engineer,Mindlance,"Beaverton, OR",13 hours ago,Be among the first 25 applicants,[''],Entry level,Contract,Engineering,Staffing and Recruiting,2021-01-19 09:25:31
Data Engineer ,"IDR, Inc.",Atlanta Metropolitan Area,19 hours ago,36 applicants,"['', 'What’s in it for you?', 'Help build thriving businesses and solve complex problems that can make digital transformation a reality for clients. ', 'Collaborate within a team of technologists to build Tableau dashboards. Help build thriving businesses and solve complex problems that can make digital transformation a reality for clients. Working with stakeholders to ensure business decisions align with their needs. ', '3+ years of experience with Adobe Analytics ', 'Requirements for the Data Engineer:', 'Data Engineer ', 'Enjoy true work/life balance', 'SQL experience is preferred\xa0', 'Collaborate within a team of technologists to build Tableau dashboards. ', '*This contract will be fully remote* ', 'One of IDR’s largest clients is looking to hire a Data Engineer for a 4–5-week contract assignment. ', 'Working with stakeholders to ensure business decisions align with their needs. ', 'Join a flexible friendly laid-back environmentEnjoy extremely competitive compensation and benefits packageEnjoy true work/life balanceJoin an extremely secure organization that offers job stability', 'Cloud ETL exposure ', 'Join an extremely secure organization that offers job stability', '5+ years of experience managing Tableau dashboards. ', 'Join a flexible friendly laid-back environment', 'Enjoy extremely competitive compensation and benefits package', 'Hands on Data Engineer background', 'Responsibilities of the Data Engineer:', '2+ years of management or analytics/business experience', '5+ years of experience managing Tableau dashboards. Hands on Data Engineer background3+ years of experience with Adobe Analytics 2+ years of management or analytics/business experienceCloud ETL exposure SQL experience is preferred\xa0']",Mid-Senior level,Full-time,Information Technology,Staffing and Recruiting,2021-01-19 09:25:31
Data Engineer (Python),TalentPartners,United States,,N/A,"[' Bachelor’s Degree in Computer Science or a related discipline', ' Nice to have:', ' Ability to write complex SQL to perform common types of analysis and aggregations\xa0', ' Experience with Docker containerization', ' Experience with Apache Airflow or Google Composer', ' knowledge of Application Programming Interfaces', ' Ability to work with others from diverse skill-sets and backgrounds', ' 5+ years of applicable engineering experience', ' Strong proficiency in Python\xa0with an emphasis in building data pipelines', ' Detail-oriented and document all the work', 'Experience with version control systems (Git and Bitbucket)', 'Nice to have:', ' ', ' Experience with Atlassian products Jira and Confluence', 'Required Qualifications:']",Mid-Senior level,Full-time,Information Technology,Staffing and Recruiting,2021-01-19 09:25:31
Data Integration Engineer,LeanDNA,"Austin, TX",10 hours ago,Be among the first 25 applicants,"['', 'Good understanding of data design principles', 'Act as the primary technical point of contact for a group of customers', ' Ability to learn systems and business processes quickly Demonstrated experience working with ERP systems and other business systems Proficiency with SQL Experience with data pipeline engineering Good understanding of data design principles Great communication skills ', ' 2+ years experience as a data engineer (working with databases and data pipelines) Proven ability to effectively interface with customers Due to the nature of our business, we only accept U.S. Citizens.  ', 'Perform initial data integration with new customers, and maintain the data extraction and transformation processes', 'Qualifications', 'Analyze customer data to ensure accuracy and to help the customer achieve their goals (inventory reduction, shortage prevention)', 'The Data Integration Engineer', 'Proven ability to effectively interface with customers', 'Proficiency with SQL', 'Great communication skills', 'Demonstrated experience working with ERP systems and other business systems', 'Ability to learn systems and business processes quickly', 'Due to the nature of our business, we only accept U.S. Citizens. ', ' Perform initial data integration with new customers, and maintain the data extraction and transformation processes Act as the primary technical point of contact for a group of customers Analyze customer data to ensure accuracy and to help the customer achieve their goals (inventory reduction, shortage prevention) ', 'Good understanding of supply chain processes', 'Experience with data pipeline engineering', 'Required Skills', '2+ years experience as a data engineer (working with databases and data pipelines)', 'Nice To Have', 'Responsibilities']",Mid-Senior level,Full-time,Information Technology,Marketing and Advertising,2021-01-19 09:25:31
Data Analytics Engineer,Three Deep Marketing,Greater Minneapolis-St. Paul Area,17 hours ago,Be among the first 25 applicants,"['', 'Work with stakeholders to collect, interpret and translate business requirements into technical requirements ', 'Maintain an extensive reading list of industry and non-industry sources to understand upcoming changes to Google Analytics and Google Tag Manager landscape', 'Proficient in basic HTML and JavaScript a plus', 'Three Deep, Inc. does not and will not discriminate against employees, prospective employees, clients or vendors.', 'Ability to work in a fast-paced, collaborative environment with tight deadlines', 'Continued Learning:', 'You are a hard-working, solution-minded person that enjoys working across multiple channels, teams and keeping up with best practices. \xa0', 'Strong understanding of technical aspects of digital tracking ', 'Provide subject matter expertise to other teams', 'Position Overview', 'A natural curiosity for how things work and the ability to adapt ', 'The successful candidate will translate measurement and data collection needs to real tagging solutions, along with assisting our front-end development team from time to time.\xa0Candidate will leverage expertise in areas including requirements management, analytics tagging, quality assurance, and front-end web development.', 'Interfaces with business clients and vendor partners', 'Ability to review measurement plans and translate to tracking implementations', 'Excellent communication and team skills – verbal and written', '·\xa0\xa0\xa0\xa0\xa0\xa0Continued Learning:\xa0We encourage all of our employees to continue learning and refining their skills, so we will work with you to make sure that you’re continuing to further your expertise through certifications, local conferences and events.', 'A natural curiosity for how things work and the ability to adapt 2+ years’ experience in a front-end development role (agency experience preferred)Ability to work in a fast-paced, collaborative environment with tight deadlinesStrong understanding of technical aspects of digital tracking Proficient in basic HTML and JavaScript a plusAbility to review measurement plans and translate to tracking implementationsStrong project management skills and ability to prioritize and make progress on multiple concurrent projects and share progress with your teamKnowledge of Agile methodologies a plusMaintain an extensive reading list of industry and non-industry sources to understand upcoming changes to Google Analytics and Google Tag Manager landscapeAbility to communicate results and provide transparency to functional and executive teamsExcellent communication and team skills – verbal and written', 'Identify analytics opportunities and communicate with stakeholders ', 'We’re looking for someone with demonstrated ability to identify issues across a wide range of areas, develop and implement impactful solutions, and analyze results and relay those back to the team.', '\xa0', 'Three Deep, Inc. Equal Employment Opportunity\xa0', 'Work with stakeholders to collect, interpret and translate business requirements into technical requirements Construct and code standard and custom tag solutions Deploy standard and custom tags Identify analytics opportunities and communicate with stakeholders Create, run and aggregate data from various reporting tools such as Google Analytics Marketing Platform and potentially other internal data warehousing systems or tools Provide subject matter expertise to other teamsManage or co-manage GACP (Google Analytics Certified Partner) relationshipProduce, modify, and maintain web solutions using a variety of technologies and development languagesMonitor the progress of the technical workflow and adjust as necessary to ensure the successful completion of deliverablesAnalyze change requests to determine feasibility in relation to existing business requirements, processes, and data modelsWorks with project and operations leadership in determining design approach, estimating/planning, setting technical direction, enhancing business processes, and identifying required project resourcesInterfaces with business clients and vendor partners', 'Manage or co-manage GACP (Google Analytics Certified Partner) relationship', 'Team Collaboration:', '2+ years’ experience in a front-end development role (agency experience preferred)', 'Construct and code standard and custom tag solutions ', 'Ability to communicate results and provide transparency to functional and executive teams', 'We are an equal opportunity employer, dedicated to a policy of nondiscrimination in employment on any basis including race, color, creed, gender, sexual orientation, age, disability, religion, national origin, marital status, familial status, ancestry, status as a veteran, status with regard to public assistance and any other characteristic protected by law.', 'Strong project management skills and ability to prioritize and make progress on multiple concurrent projects and share progress with your team', 'Analyze change requests to determine feasibility in relation to existing business requirements, processes, and data models', 'Monitor the progress of the technical workflow and adjust as necessary to ensure the successful completion of deliverables', 'Produce, modify, and maintain web solutions using a variety of technologies and development languages', 'Skills and Qualifications', '·\xa0\xa0\xa0\xa0\xa0\xa0Team Collaboration:\xa0You’ll often engage in teamwork with analysts, developers as well as other members of the Three Deep team to ensure that we are delivering the most effective services to our clients.', 'Deploy standard and custom tags ', 'Knowledge of Agile methodologies a plus', 'Three Deep Marketing is seeking a highly skilled Digital Analytics Engineer to manage and support the tag management, privacy and digital analytics solutions across a variety of clients.\xa0', 'Create, run and aggregate data from various reporting tools such as Google Analytics Marketing Platform and potentially other internal data warehousing systems or tools ', 'Responsibilities', 'Works with project and operations leadership in determining design approach, estimating/planning, setting technical direction, enhancing business processes, and identifying required project resources']",Mid-Senior level,Full-time,Marketing,Marketing and Advertising,2021-01-19 09:25:31
Software Data Operations Engineer,MAQ Software,"Redmond, WA",7 hours ago,Be among the first 25 applicants,[],Entry level,Full-time,Engineering,Information Technology and Services,2021-01-19 09:25:31
Data Engineer,"York Solutions, LLC","Minneapolis, MN",21 hours ago,Be among the first 25 applicants,"['Contract-to-Hire, after 6 months (only looking at job seekers who can convert wo sponsorship)', '100 remote telecommuterSDL2017', '2+ years of writing Python 3 for production', '2+ years of unit and integration testing', '3+ years writing SQL ( Oracle PL SQL is preferred)', '1+ years of Docker, Jenkins, and Kubernetes OR OpenShift. Logistics', 'Understanding of the SDLC process design patterns, algorithms, data structures, schemas and queries, systems design, unit testing, code reviews', '5+ years hands on experience with Java, C, and C++']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-01-19 09:25:31
Data Engineer Sr,PNC,"Pittsburgh, PA",14 hours ago,Be among the first 25 applicants,"['', 'Disability Accommodations Statement', 'Work Experience', 'Job Description', ""Customer Focused - Knowledgeable of the values and practices that align customer needs and satisfaction as primary considerations in all business decisions and able to leverage that information in creating customized customer solutions.Managing Risk - Assessing and effectively managing all of the risks associated with their business objectives and activities to ensure they adhere to and support PNC's Enterprise Risk Management Framework."", 'Leads in developing, supporting and implementing data solutions for multiple applications in order to meet business objectives and user requirements. Leverages technical knowledge and industry experience to design, build and maintain technology solutions.Leads data requirement analysis and the data preparation process development for targeted data solutions.Leads in designing and building data service infrastructure on multiple data platforms, according the workflow.Oversees the development and implementation of data solutions for multiple applications to ensure its scalability, availability and maintainability.Consults on data migration and transformation to ensure the accuracy and security of data solutions.', 'Competencies', 'Position Overview', 'Customer Focused', 'Education', 'Consults on data migration and transformation to ensure the accuracy and security of data solutions.', 'Leads data requirement analysis and the data preparation process development for targeted data solutions.', 'Managing Risk', 'Leads in developing, supporting and implementing data solutions for multiple applications in order to meet business objectives and user requirements. Leverages technical knowledge and industry experience to design, build and maintain technology solutions.', 'Job Profile', 'California Residents ', ""Managing Risk - Assessing and effectively managing all of the risks associated with their business objectives and activities to ensure they adhere to and support PNC's Enterprise Risk Management Framework."", 'Customer Focused - Knowledgeable of the values and practices that align customer needs and satisfaction as primary considerations in all business decisions and able to leverage that information in creating customized customer solutions.', 'Leads in designing and building data service infrastructure on multiple data platforms, according the workflow.', 'Equal Employment Opportunity (EEO)', 'Oversees the development and implementation of data solutions for multiple applications to ensure its scalability, availability and maintainability.']",Not Applicable,Full-time,Information Technology,Financial Services,2021-01-19 09:25:31
Data Systems Engineer,KIPP,"New York, NY",3 hours ago,Be among the first 25 applicants,"['', 'Impact And InfluenceDrive Results', 'Student FocusDrive Results']",Entry level,Full-time,Information Technology,Machinery,2021-01-19 09:25:31
Data Analytics Engineer,"BDO USA, LLP","Oak Brook, IL",9 hours ago,Be among the first 25 applicants,"['', ' Must be open to local travel to client sites, if needed', ' Microsoft Azure SQL or SQL Data Warehouse', 'Other Knowledge, Skills & Abilities', ' Strong written and verbal communication skills', ' Experience with the following technologies, preferred:', ' Integration Services (SSIS)', ' Designs, develops, tests, and implements data analytics or business intelligence solutions', ' Strong SQL Server skills is a must along with SQL queries and stored procedures Excellent organizational and time management skills Strong written and verbal communication skills Experience with the following technologies, preferred: Semantic Models Integration Services (SSIS) Analysis Services (SSAS – Multidimensional and Tabular cubes) Reporting Services (SSRS) Machine Learning Stream Analytics or Data Lake Analytics Microsoft Azure SQL or SQL Data Warehouse Azure Data Factory PowerBI or Tableau Amazon Web Services (AWS) with Redshift Must be open to local travel to client sites, if needed', ' SSIS', ' Analysis Services (SSAS – Multidimensional and Tabular cubes)', ' Data visualization using PowerBI or Tableau for data analysis and reporting', ' Two (2) or more years of Software Development experience within Data Analytics / Business Intelligence development using Microsoft technologies, required', ' Bachelor’s degree in Computer Science or a related field from an accredited university, preferred', ' SSIS SSAS SSRS Stream Analytics or Data Lake Analytics Microsoft Azure SQL or SQL Data Warehouse Azure Data Factory PowerBI or Tableau AWS with Redshift', ' Machine Learning', ' Interacts with customers and project leaders to define and document project specifications', 'Education', 'Qualifications', ' Other duties as required', ' SSRS', ' Stream Analytics or Data Lake Analytics', ' Strong SQL Server skills is a must along with SQL queries and stored procedures', ' Azure Data Factory', ' Reporting Services (SSRS)', ' PowerBI or Tableau', ' Semantic Models', ' Excellent organizational and time management skills', ' Participates in support activities for existing software', ' Partakes in technology training to learn various Data Analytics / Business Intelligence technologies', ' AWS with Redshift', ' Amazon Web Services (AWS) with Redshift', 'Software', ' SSAS', ' Interacts with customers and project leaders to define and document project specifications Designs, develops, tests, and implements data analytics or business intelligence solutions Participates in support activities for existing software Data visualization using PowerBI or Tableau for data analysis and reporting Partakes in technology training to learn various Data Analytics / Business Intelligence technologies Other duties as required', "" Bachelor's degree from an accredited university, required"", 'Experience', 'Responsibilities', "" Bachelor's degree from an accredited university, required Bachelor’s degree in Computer Science or a related field from an accredited university, preferred""]",Not Applicable,Full-time,Finance,Accounting,2021-01-19 09:25:31
Data Engineer,KellyOCG,"Horsham, PA",,N/A,"['', 'Amazon Web Services: EC2, RDS, S3, IAM, CLIRed Hat Enterprise LinuxOracle Technologies: Database, ORDS, APEX, MySQL, Cloud Infrastructure', 'Our early level Full Stack Data Engineer will have 2 years minimum of demonstrated hands-on experience in software development to work between Medical Decision makers and Data Scientists; we need an engineer for the Data models:', 'Main Focus: 1) Oracle database (not SQL Server) since it’s an Oracle shop, 2) heavy Linux, and the must have of 3) AWS experience.', 'Kelly Functional Service Provider (FSP)', 'Pharmacovigilance experience preferred but not required', ' FULL BENEFITS', 'Oracle Technologies: Database, ORDS, APEX, MySQL, Cloud Infrastructure', 'Full Stack Data Engineer – REQUIRED Core Platforms and Technologies', 'For immediate consideration please APPLY ONLINE to this posting directly, or directly on our MyKellyJobs page', 'Version Control Systems and Source Code Repositories', 'Configuration and administration of virtualized platforms and operating systems', 'A data engineer who can understand the problem with/alongside the business partner, understands their (data) requirements, perform target mapping, architecture mapping, get the data out, go to a middle layer.', 'Linux configuration and administration, including shell scripting', 'Amazon Web Services: EC2, RDS, S3, IAM, CLI', 'There is no ‘per diem’ with this position and must be able to work on W-2 hourly basis.', 'Applicants must be legally permitted to work in the United States.', 'Full Stack Data Engineer – Knowledge and Skill Overview', 'The Engineer will work with Data Scientists, Enterprise IT Professionals, and Subject Matter Experts to develop, maintain, and troubleshoot back end information systems and solutions to be consumed by a diverse array of front-end technologies.This position will require participation in a rotating on-call schedule, including weekend and holiday hours.', 'Full Stack Data Engineer OVERVIEW:', 'Source Data Introspection', 'This role is Fully Remote, and ongoing, long term in duration!', 'Kelly Functional Service Provider (FSP) is happy to exclusively present a Long-Term contract opportunity for a role as a Full Stack Data Engineer for our Global Pharmaceutical Company client, a name brand Fortune 50 company.', 'Full Stack Data Engineer – Experience Required', 'Linux', 'Our client is creating a future where disease is a thing of the past, by bringing together the most creative minds and cutting-edge technology to help treat, cure, stop, and prevent some of the most complex diseases of our time.', 'Experience with Oracle Cloud Infrastructure', 'Oracle Database administration and configuration', 'This position is recruited by a remote Kelly Centralized Recruiting, not your local Kelly branch.Only qualified candidates can be contacted due to the volume of responses.', 'Applicants must be legally permitted to work in the United States.We appreciate all our applicants, but we are not able to sponsor now; no Corp2Corp or H-1B.There is no ‘per diem’ with this position and must be able to work on W-2 hourly basis.For immediate consideration please APPLY ONLINE to this posting directly, or directly on our MyKellyJobs page', 'Red Hat Enterprise Linux', 'Extract-Transform-Load and Extract-Load-Transform workflows', 'Full Stack Data Engineer - Preferred Qualifications:', 'AWS', 'We appreciate all our applicants, but we are not able to sponsor now; no Corp2Corp or H-1B.', 'AWS ', 'This position is recruited by a remote Kelly Centralized Recruiting, not your local Kelly branch.', 'The Full Stack Data Engineer will support a specialized analytics team in procuring, transforming, cleaning, and exposing multiple data pipelines within a large heterogeneous enterprise architecture.', 'Important information:', ' Oracle database', 'Main Focus:', 'Fully Remote', 'Configuration, administration, and troubleshooting of AWS-deployed compute, database, and storage', 'The Engineer will work with Data Scientists, Enterprise IT Professionals, and Subject Matter Experts to develop, maintain, and troubleshoot back end information systems and solutions to be consumed by a diverse array of front-end technologies.', 'Ability to spin up a server, work in oracle database, perform extractions…all so data scientists can get to the data easily and they can visualize it from there.', 'Source-to-Target Mapping', 'Creation of CI/CD workflows', 'Source Data IntrospectionSource-to-Target MappingConfiguration and administration of virtualized platforms and operating systemsConfiguration and administration of virtualized server instancesExtract-Transform-Load and Extract-Load-Transform workflowsData Cleaning and Post-ProcessingRESTful web servicesVersion Control Systems and Source Code Repositories', '2+ years of demonstrated hands-on experience in:', 'This role is Fully Remote, and ongoing, long term in duration!This position has FULL BENEFITS with PTO, Holidays, and Health Insurance as part of KellyFSP.PAY commensurate to skill experience! Let’s talk about that!', '1) Oracle database (not SQL Server) since it’s an Oracle shop, 2) heavy Linux, and the must have of 3) AWS experience', '2 years minimum of demonstrated hands-on experience ', 'Configuration and administration of virtualized server instances', 'Integration, cleaning, and exposing of data from structured and unstructured sources and legacy systems', 'PAY commensurate to skill experience! Let’s talk about that!', 'This position has FULL BENEFITS with PTO, Holidays, and Health Insurance as part of KellyFSP.', 'Global Pharmaceutical Company client, a name brand Fortune 50 company.', 'A data engineer who can understand the problem with/alongside the business partner, understands their (data) requirements, perform target mapping, architecture mapping, get the data out, go to a middle layer.Ability to spin up a server, work in oracle database, perform extractions…all so data scientists can get to the data easily and they can visualize it from there.Main Focus: 1) Oracle database (not SQL Server) since it’s an Oracle shop, 2) heavy Linux, and the must have of 3) AWS experience.', 'This position will require participation in a rotating on-call schedule, including weekend and holiday hours.', '1) Oracle database (not SQL Server) since it’s an Oracle shop, 2) heavy Linux, and the must have of 3) AWS experienceIntegration, cleaning, and exposing of data from structured and unstructured sources and legacy systemsConfiguration, administration, and troubleshooting of AWS-deployed compute, database, and storageLinux configuration and administration, including shell scriptingOracle Database administration and configuration', 'Creation of CI/CD workflowsExperience with Oracle Cloud InfrastructurePharmacovigilance experience preferred but not required', 'Only qualified candidates can be contacted due to the volume of responses.', 'Full Stack Data Engineer', ' ', 'RESTful web services', 'Oracle database', 'Data Cleaning and Post-Processing', 'OVERVIEW', 'We appreciate all our applicants, but we are not able to sponsor now; no Corp2Corp or H-1B']",Entry level,Full-time,Information Technology,Pharmaceuticals,2021-01-19 09:25:31
Data Analytics Engineer,"KCF Technologies, Inc.","Pittsburgh, PA",6 hours ago,Be among the first 25 applicants,"['', 'Integrate analytics solutions within the KCF SMARTdiagnostics platform', 'Our ideal candidate exemplifies our cultural values of Smarts, Grit, and Drive, and considers him or herself to be:', 'Design and implement algorithms tailored to industrial machine health diagnostics, prognostics, and root-cause analysisProduce models for edge and/or cloud (AWS) executionIntegrate analytics solutions within the KCF SMARTdiagnostics platformConstruct data mining tools for acquiring training and validation data sets', 'Competitive compensation plus quarterly bonus opportunities', 'Why KCF Technologies?', 'Programming experience with Python (Numpy, Scipy, Scikit-Learn, Matplotlib, TensorFlow, etc.), MATLAB, R, or similar languages', 'KCF Technologies is looking for a talented and motivated Data Analytics Engineer to help develop computer algorithms and models trained to ingest sensor and machine process data and diagnose fault conditions, prognose time-to-failure, provide recommended actions, and perform root-cause analysis.', ""Bachelor's degree in Computational Science; Statistics; Computer, Mechanical, or Industrial Engineering; or similar field of study"", 'Self-motivated', 'Annual Fitness Reimbursement program and generous travel stipends', 'Perks & Benefits', 'Strong algorithm design skills', 'KCF Technologies is an exciting and rapidly growing technology company dedicated to putting innovative solutions to work in the Industrial World. SmartDiagnostics, our IIOT Technology, supports our mission to Transform American Industry by helping our customers eliminate downtime, eradicate waste, increase safety, and elevate manufacturing jobs across the United States. To accomplish this epic technical revolution, we integrate our core values into our everyday actions. Learn more at www.kcftech.com ', 'We know it takes competitive benefits and development opportunities to fuel a team that exhibits our values of Smarts, Grit, Drive, Autonomy, and Responsibility. At KCF, we provide perks that are focused on bringing out the best in you:', '#PM19', 'Should be able to deal with difficult, sensitive, and confidential issues ', 'Strong organizational, time management, and prioritization abilities ', 'Strong mathematical background (linear algebra, calculus, probability and statistics)', 'Detail-oriented', 'Where You Come In', 'At KCF, we are an equal opportunity employer. The only things we require for employment, compensation, advancement and benefits are performance and a good team attitude. No one will be denied opportunities or benefits, and no employment decisions will be made, on the basis of race, religion/creed, national origin, ancestry, sex, sexual orientation, gender, gender identity, age, disability that does not prohibit performance of essential job functions, protected veteran status, medical condition, marital status, pregnancy, genetic information, possession of a general education development certificate (GED) as compared to a high school diploma, or any other characteristic protected by applicable federal or state laws. KCF complies with applicable state and local laws governing nondiscrimination in employment in every location in which KCF has facilities.', 'Hard-working', 'Communicates verbally and in writing in a clear and professional manner', 'Able to work in a rapid-paced environment, managing and tracking multiple tasks with speed and accuracy', 'Qualifications', 'At least 2 years of experience applying data analytics techniques to solve real-world problems', 'Design and implement algorithms tailored to industrial machine health diagnostics, prognostics, and root-cause analysis', 'For the purposes of determining compliance with U.S. export control regulations, all applicants will be required to answer yes or no to the following question: Are you a U.S. Citizen, a lawful permanent resident of the U.S. (i.e., a green card holder), an asylee, or a refugee?', ""Bachelor's degree in Computational Science; Statistics; Computer, Mechanical, or Industrial Engineering; or similar field of studyStrong mathematical background (linear algebra, calculus, probability and statistics)Machine Learning experience (regression and classification, supervised, and unsupervised learning)At least 2 years of experience applying data analytics techniques to solve real-world problemsStrong algorithm design skillsProgramming experience with Python (Numpy, Scipy, Scikit-Learn, Matplotlib, TensorFlow, etc.), MATLAB, R, or similar languagesCommunicates verbally and in writing in a clear and professional mannerAble to work in a rapid-paced environment, managing and tracking multiple tasks with speed and accuracyHighly service-oriented disposition with aptitude in problem-solving Must exemplify the following KCF cultural values: Smarts, Grit, Drive Strong organizational, time management, and prioritization abilities Should be able to deal with difficult, sensitive, and confidential issues For the purposes of determining compliance with U.S. export control regulations, all applicants will be required to answer yes or no to the following question: Are you a U.S. Citizen, a lawful permanent resident of the U.S. (i.e., a green card holder), an asylee, or a refugee?"", 'Construct data mining tools for acquiring training and validation data sets', '*Position has ability to be fully remote', 'Accepting of the unknown', 'Self-motivatedPassionate for solving real-world problemsHard-workingAdaptableDetail-orientedAccepting of the unknownIf this sounds like you, we encourage you to keep reading!', 'Machine Learning experience (regression and classification, supervised, and unsupervised learning)', 'Four weeks Paid Time Off, in addition to paid Holidays', 'Highly service-oriented disposition with aptitude in problem-solving ', 'Exciting opportunities for growth and professional development', 'If this sounds like you, we encourage you to keep reading!', 'Must exemplify the following KCF cultural values: Smarts, Grit, Drive ', 'Hybrid Remote Work Model - Work from Home, Work from Anywhere ', 'Industry leading Medical Insurance - 100% Company Paid for you and your family', '401(k) retirement plan with up to 4% KCF match', 'Ability to work in one of the most exciting fields in technology', ""We are looking for an individual who not only has an aptitude for data analytics, statistical analysis, and pattern recognition, but also has a passion for learning and understanding how things work. You consider yourself to be Independently motivated and an analytically deductive problem solver, with a factual and to-the-point communication style. You're a case matter expert on things that draw your interest, in addition to being extremely disciplined with perfectionist tendencies."", 'Industry leading Medical Insurance - 100% Company Paid for you and your familyCompany provided Vision and Dental planCompetitive compensation plus quarterly bonus opportunities401(k) retirement plan with up to 4% KCF matchFour weeks Paid Time Off, in addition to paid HolidaysAnnual Fitness Reimbursement program and generous travel stipendsExciting opportunities for growth and professional developmentHybrid Remote Work Model - Work from Home, Work from Anywhere Ability to work in one of the most exciting fields in technology', 'Essential Functions', 'Passionate for solving real-world problems', 'Produce models for edge and/or cloud (AWS) execution', 'Company provided Vision and Dental plan', 'Adaptable']",Entry level,Full-time,Information Technology,Computer Hardware,2021-01-19 09:25:31
Data Engineer,Method Media Intelligence,New York City Metropolitan Area,,N/A,"['', 'Salary is $120,000-$180,000 base depending on experience, with health and medical insurance coverage, above standard parental leave, and certain perks. ', 'Diligence in proper labeling and process documentation. ', '7+ years of back end engineering experience ', 'Reach out to info@methodmi.com with your resume/CV, to learn more.', 'This position will be a senior engineer working with our current team on updating and improving efficiencies in our data pipeline. Clients are advertisers and agencies, and the data we collect and manage is ad transactions. ', '4+ years of experience with AWS', 'Comfortable with large data sets and creating appropriate tables for team of analysts. ']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-01-19 09:25:31
Data Engineer,Vivid Resourcing Ltd,"Austin, Texas Metropolitan Area",19 hours ago,Be among the first 25 applicants,"['', 'Experienced in AWS but okay with GCP or Azure', 'With a growing team, you will make a big impact on the direction they are looking to move forward and ultimately have the opportunity to grow with the company! ', 'Send me your resume at carlos.rueda@vividresourcing.com', 'Location: Austin, TX (Remote)', '3-5+ years in Data Engineering3+ years in SQL 3+ years in Python, Scala, or Spark. Experienced in AWS but okay with GCP or AzureBonus if you have experience with Snowflake or KafkaExperience building ETL pipelines', 'Fulltime Direct Hire\xa0', 'Skills:', 'Experience building ETL pipelines', 'Competitive salary + bonus structureRemote flexibilityFully paid medical benefitsUnlimited PTOAmazing office culture; hosted lunches, happy hours, and special events!', 'Currently recruiting for an excellent Data Engineer in the Austin area for an exciting tech company! They are looking to grow their Data platform team extensively this year and are looking to bring on two Data Engineers. ', '**Unfortunately this position can not sponsor at this time. If you are authorized to work in the US you are encouraged to apply (US citizen or GC holder)**\xa0', 'What they offer:', '3+ years in SQL ', 'Please reach out if you are interested.', 'Senior Data Engineer ', 'Remote flexibility', 'Fully paid medical benefits', '3-5+ years in Data Engineering', '3+ years in Python, Scala, or Spark. ', 'Competitive salary + bonus structure', 'Unlimited PTO', 'Amazing office culture; hosted lunches, happy hours, and special events!', 'Bonus if you have experience with Snowflake or Kafka']",Mid-Senior level,Full-time,Information Technology,Staffing and Recruiting,2021-01-19 09:25:31
Data Engineer,Experis,"The Woodlands, TX",19 hours ago,Be among the first 25 applicants,"['Job Description:', 'Cloud Data Architect * Specialized in deploying large scale data analytics solutions in cloud leveraging variety of technologies and frameworks * Expert with architecting data and analytics solutions in Microsoft Azure stack * Strong experience with building end to end solutions within Snowflake  o ELT data pipelines o API integrations o DevOps / CICD deployments o Data Modeling o Platform Management (Account configurations/parameters, performance tuning, warehouse management, snowflake metadata) o Experience with Snowflake data sharing  * Expert mapping business needs and translating to technical architectures * Expert in data modeling (conceptual, physical, and logical), ER diagrams, data dictionary, data map, normalize/denormalize, agile data modeling * Solid understanding of continuous integration, deployment, and monitoring best practices * Demonstrate critical thinking, analytical skills, and employ judgment to offer thoughtful, concise input toward resolutions of problems. * Comprehension of DevOps and Agile development and application to data centric architecture and solutions * Strong communication and interpersonal skills with strong English proficiency', 'Length: 14 weeks', 'Length: 14 weeks  Location: Hughes Landing - does not anticipate on-campus during at this time - team is currently working remotely due to COVID.  Senior Cloud Data Engineer * Expertise with data modeling in SQL * Proficient with various tools in Azure Stack (ADF, ADLS, Azure Functions) * Expert in python for data engineering, including data ingestion, manipulation, and database structuring * Knowledge of data modeling (physical and logical), data dictionary, data map, normalize/denormalize, agile data modeling * Solid understanding of continuous integration, deployment, and monitoring best practices * Demonstrate critical thinking, analytical skills, and employ judgment to offer thoughtful, concise input toward resolutions of problems. * Be able to translate data requirements into business processes and reverse engineer business processes into data requirements * Comprehension of DevOps and Agile development and application to data centric architecture and solutions * Strong communication and interpersonal skills with strong English proficiency  Location: Hughes Landing - does not anticipate on-campus during at this time - team is currently working remotely due to COVID.', 'Senior Cloud Data Engineer', ' ', 'Location - Woodlands, TX -Remote', 'Cloud Data Architect']",Entry level,Contract,Information Technology,Oil & Energy,2021-01-19 09:25:31
Senior Engineer - Data Science,Donaldson,"Bloomington, MN",15 hours ago,Be among the first 25 applicants,"['', 'Offer consulting and service of data analytics for various organizations in Donaldson; ', 'Bachelors Degree in Applied Data Science, Computer Science, Machine Learning, Engineering, or similar from an accredited university; Advanced degree preferred.', 'Create clear data visualization that tell compelling stories and insight', 'Design and build modular and reusable data analytics models and algorithms that will be deployed as a service in a variety of industrial and business environment', 'Lead global projects, adopt and utilize Donaldson processes to drive project execution on time, on budget and on quality', 'Use machine learning tools and statistical techniques to produce solutions to problems', '3 plus years of relevant work experience ', 'Stay up to date with the latest technology in data engineering and data science, either on premise or in the cloud', 'Assess and improve the effectiveness of data sources and data-gathering techniques and improve data collection method', 'Conduct data analysis on tons of data generated by IoT sensors ingested into Big Data infrastructure', 'Qualifications', 'Validate data mining models and assess the model quality ', 'Bachelors Degree in Applied Data Science, Computer Science, Machine Learning, Engineering, or similar from an accredited university; Advanced degree preferred.3 plus years of relevant work experience ', 'Proficient in managing various data sources, and merge, manage, interrogate and extract data to address business questions ', 'Work with business partners to identify opportunities of data driven predictive solution for better decision making ', 'Work with business partners to identify opportunities of data driven predictive solution for better decision making Proficient in managing various data sources, and merge, manage, interrogate and extract data to address business questions Use machine learning tools and statistical techniques to produce solutions to problemsValidate data mining models and assess the model quality Create clear data visualization that tell compelling stories and insightAssess and improve the effectiveness of data sources and data-gathering techniques and improve data collection methodConduct data analysis on tons of data generated by IoT sensors ingested into Big Data infrastructureDesign and build modular and reusable data analytics models and algorithms that will be deployed as a service in a variety of industrial and business environmentLead global projects, adopt and utilize Donaldson processes to drive project execution on time, on budget and on qualityOffer consulting and service of data analytics for various organizations in Donaldson; Stay up to date with the latest technology in data engineering and data science, either on premise or in the cloud']",Associate,Full-time,Other,Mechanical or Industrial Engineering,2021-01-19 09:25:31
Data Engineer,Kforce Inc,"Beaverton, OR",21 hours ago,Be among the first 25 applicants,"['', '  Must be self-directed and comfortable supporting the data needs of the product roadmap; The right candidate will be excited by the prospect of optimizing and building integrated and aggregated data objects to architect and support our next generation of products and data initiatives  Create and maintain optimal data pipeline architecture  Assemble large, complex data sets that meet functional/non-functional business requirements  Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing for greater scalability  Comprehensive documentation and knowledge transfer to Production Support  Work with Production Support to analyze and fix Production issues  Participate in an Agile/Scrum methodology to deliver high -quality software releases every 2 weeks through Sprint  Refine, plan stories and deliver timely  Analyze requirement documents and Source to target mapping ', ' Strong understanding of Hadoop, MPP systems and data structures', ' Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing for greater scalability', ' 5+ years of experience in Big Data stack environments like AWS EMR, Clourdera, Hortonworks', ' Aware of Datawarehouse concepts', ' Strong problem solving and analytical mindset', ' Strong understanding of solution and technical design', ' Work with Production Support to analyze and fix Production issues', ' 3+ years of SPARK in batch mode', ' Create and maintain optimal data pipeline architecture', ' 3+ years of experience in scripting using Python ', ' Experience with source control tools such as GitHub and related dev processes', '  5+ years of experience in Big Data stack environments like AWS EMR, Clourdera, Hortonworks  3+ years of SPARK in batch mode  3+ years of experience in scripting using Python   3+ years of experience working on AWS Cloud environment  In-depth knowledge of Hive and S3  Strong understanding of Hadoop, MPP systems and data structures  Strong understanding of solution and technical design  Experience building cloud scalable high-performance data lake solutions  Experience with relational SQL & tools like Snowflake  Aware of Datawarehouse concepts  Performance tuning with large datasets  Experience with source control tools such as GitHub and related dev processes  Experience with workflow scheduling tools like Airflow  Strong problem solving and analytical mindset  Able to influence and communicate effectively, both verbally and written, with team members and business stakeholders ', 'Requirements', ' Must be self-directed and comfortable supporting the data needs of the product roadmap; The right candidate will be excited by the prospect of optimizing and building integrated and aggregated data objects to architect and support our next generation of products and data initiatives', ' Able to influence and communicate effectively, both verbally and written, with team members and business stakeholders', ' Experience building cloud scalable high-performance data lake solutions', ' Participate in an Agile/Scrum methodology to deliver high -quality software releases every 2 weeks through Sprint', ' Refine, plan stories and deliver timely', ' In-depth knowledge of Hive and S3', ' Experience with workflow scheduling tools like Airflow', ' Performance tuning with large datasets', ' Assemble large, complex data sets that meet functional/non-functional business requirements', ' Comprehensive documentation and knowledge transfer to Production Support', ' 3+ years of experience working on AWS Cloud environment', ' Experience with relational SQL & tools like Snowflake', 'Responsibilities', ' Analyze requirement documents and Source to target mapping']",Associate,Contract,Information Technology,Consumer Electronics,2021-01-19 09:25:31
Junior Data Engineer,"TalentBurst, an Inc 5000 company","San Jose, CA",14 hours ago,Be among the first 25 applicants,"['', 'Title: Junior Data EngineerDuration: 12 Months (Possible Extensions)', 'Develop, test and deploy the design solution with new data pipelines to support customer requirements; fix defects and implement enhancements in existing pipelines', 'Has significant experience in Big Data environment (Cloudera, Horton Works) ', 'Continue to improve system architecture for scalability as the company and data grow', 'Ability to explain complex data issues to senior leadership. ', 'Work closely with Engineering and IT teams to drive data instrumentation and ingestion to Hadoop.', 'Proven track in building SOA-based systems', 'Enjoys the challenge of building a system from the ground up.', 'Agile (Scrum, Kanban, Lean) and Test Driven Development with Python', 'Build the system from scratch with use of Python, SQL (HiveQL), Shell scripting, oozie', 'Location: San Jose, CA', 'Big Data - Hadoop (HDFS, YARN) with Hive on Map-Reduce or Tez', 'BS, MS, or PhD in Computer Science, Electrical Engineering, or a related field', 'Architect sophisticated backend data management solutions that will enable the mobile web analytics group to drive business insights and growth.Help define and drive execution of the data engineering roadmap mobile web analytics program.Develop, test and deploy the design solution with new data pipelines to support customer requirements; fix defects and implement enhancements in existing pipelinesDevelop processes and techniques for practicing good data hygiene to ensure data is always up-to-date, accurate, and stored efficientlyDesign and develop ETL solution with reliable error/exception handling and rollback framework with Python and shell scripting. Partner with IT, platform, application and support teams on support issues, process issues, bug fixes and delivery of enhancements to existing project and new projects\xa0Apply expertise in aggregating multiple data sources to creating complex SQL queriesBuild the system from scratch with use of Python, SQL (HiveQL), Shell scripting, oozieContinue to improve system architecture for scalability as the company and data growWork closely with Engineering and IT teams to drive data instrumentation and ingestion to Hadoop.Manage documentation and process definition of the data engineering solutions.\xa0 ', 'Excellent written and verbal communication skills. ', 'Manage documentation and process definition of the data engineering solutions.\xa0 ', 'Design and develop ETL solution with reliable error/exception handling and rollback framework with Python and shell scripting. ', 'Partner with IT, platform, application and support teams on support issues, process issues, bug fixes and delivery of enhancements to existing project and new projects\xa0', 'Linux (RHEL) with experience in interfacing with APIs from Salesforce, SAP, Tableau', 'Architect sophisticated backend data management solutions that will enable the mobile web analytics group to drive business insights and growth.', 'What you need to succeed ', 'Thrives in a start up environment of moving fast and failing fast but continuing to provide growth. ', 'Develop processes and techniques for practicing good data hygiene to ensure data is always up-to-date, accurate, and stored efficiently', 'Quick learner and proactive in solving problems. ', 'Help define and drive execution of the data engineering roadmap mobile web analytics program.', 'Building and leveraging CI/CD pipelines with use of such solutions as Jenkins', 'Advanced knowledge of computer algorithms to build and/ or leverage distributed systems', ""Willingness and ability to tackle problems outside the candidate's areas of expertise"", 'Languages: Python, SQL, Shell scripting. C with gcc compiler is a plus but not required .', 'Apply expertise in aggregating multiple data sources to creating complex SQL queries', 'Possesses expertise in the following areas :', ""What You'll Do"", ""Enjoys the challenge of building a system from the ground up.Thrives in a start up environment of moving fast and failing fast but continuing to provide growth. Quick learner and proactive in solving problems. Excellent written and verbal communication skills. Ability to explain complex data issues to senior leadership. Has significant experience in Big Data environment (Cloudera, Horton Works) Possesses expertise in the following areas :Agile (Scrum, Kanban, Lean) and Test Driven Development with PythonAdvanced knowledge of computer algorithms to build and/ or leverage distributed systemsProven track in building SOA-based systemsLanguages: Python, SQL, Shell scripting. C with gcc compiler is a plus but not required .Big Data - Hadoop (HDFS, YARN) with Hive on Map-Reduce or TezLinux (RHEL) with experience in interfacing with APIs from Salesforce, SAP, TableauBuilding and leveraging CI/CD pipelines with use of such solutions as JenkinsWillingness and ability to tackle problems outside the candidate's areas of expertiseBS, MS, or PhD in Computer Science, Electrical Engineering, or a related field""]",Associate,Contract,Information Technology,Information Technology and Services,2021-01-19 09:25:31
Data Science Engineer,CDPHP,"Troy, IL",48 minutes ago,Be among the first 25 applicants,"['', 'Experience Developing Qlik/SHINY-R/Bokeh/Plotly Dashboards Preferred.', 'Experience Writing Analytical Python And/or R Scripts Required.', 'Minimum One (1) Year Of Relevant Experience Required.', 'Qualifications']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-01-19 09:25:31
Data Developer,TIAG®,"Mount Pleasant, SC",14 hours ago,Be among the first 25 applicants,"['', 'Excellent verbal and written communication skills ', 'Providing support to application and database design, development, implementation and testing requirements in support of mission critical applications for the DHA', 'Reviewing management manipulation of complex high volume health data from a variety of sources in support of application database requirements', 'Thorough understanding of ETL and API Integration', 'Tableau, Qlik, or Microsoft reporting services experience ', 'Strong MS and Oracle SQL programming skills', 'Experience in Data Analytics or Business Intelligence applications for health care', 'Partnering with project and requirement leads to translate functional specifications into technical specifications for projects utilizing complex high volume data ', 'Required Experience: ', ' Developing dashboards, reports, and dynamic web pages using Tableau, Qlik and other leading BI tools. Providing technical support to data and analytic strategies, practices and standards. Working collaboratively with teammates and customers and other vendors as needed.  ', ' Tableau, Qlik, or Microsoft reporting services experience  Experience in Data Analytics or Business Intelligence applications for health care Strong MS and Oracle SQL programming skills Thorough understanding of ETL and API Integration Strong experience with HTML, JavaScript, C#, .NET, JQuery, SharePoint Designer, SQL, CAML, InfoPath Excellent verbal and written communication skills  Bachelor’s degree in Computer Science or a related discipline and 5 additional years of technical or functional experience', 'Working collaboratively with teammates and customers and other vendors as needed. ', 'Developing dashboards, reports, and dynamic web pages using Tableau, Qlik and other leading BI tools.', 'Data Developer', 'Bachelor’s degree in Computer Science or a related discipline and 5 additional years of technical or functional experience', 'Additional Responsibilities: ', 'Performing complex sophisticated analysis, synthesis of analytic results, development and implementation of techniques and procedures', 'Strong experience with HTML, JavaScript, C#, .NET, JQuery, SharePoint Designer, SQL, CAML, InfoPath', ' Providing support to application and database design, development, implementation and testing requirements in support of mission critical applications for the DHA Performing complex sophisticated analysis, synthesis of analytic results, development and implementation of techniques and procedures Reviewing management manipulation of complex high volume health data from a variety of sources in support of application database requirements Partnering with project and requirement leads to translate functional specifications into technical specifications for projects utilizing complex high volume data  ', 'Providing technical support to data and analytic strategies, practices and standards.']",Not Applicable,Full-time,Information Technology,Information Technology and Services,2021-01-19 09:25:31
Data Engineer - Remote ,Brooksource,"Tampa, FL",50 minutes ago,Be among the first 25 applicants,"['', 'Experience working with Cloud environments and databases', 'Hands-on experience in designing data solutions using Snowflake as an extension of existing on-premise or cloud data warehouses.Experience working with Cloud environments and databasesExperience developing and tuning ETL processes and utilizing industry standard ETL toolsExperience with modern Data Ingestion/Data Integration/Data Pipeline/Data Orchestration tools & streaming servicesAdvanced working SQL knowledge and experience working with relational databases, as well as a variety of databasesSpecific technologies: Snowflake, Azure, SSIS, dbt', 'Tampa, FL (100% Remote)', 'Designing optimal cloud-hybrid Big Data/ Data Warehouse data pipelines & orchestration architectures to migrate existing solutions from on-premise to cloud, and to initiate new cloud data solutions with an emphasis on Snowflake.Constructing new data platforms and developing pipelines, ETL and analytical processes to support products and solutions.Technical oversight for advanced analytical solutions to deliver real-time insights in a citizen developer model.Developing strategies, policies and best practices regarding Big Data and Cloud data acquisition, integration and analytics.Preparing technical design documents and estimation including storage, resource, and operational details.Work with product manager/product owner and development teams to ensure all project requirements are met.', 'Contract-to-Hire', 'Advanced working SQL knowledge and experience working with relational databases, as well as a variety of databases', 'Work with product manager/product owner and development teams to ensure all project requirements are met.', 'Designing optimal cloud-hybrid Big Data/ Data Warehouse data pipelines & orchestration architectures to migrate existing solutions from on-premise to cloud, and to initiate new cloud data solutions with an emphasis on Snowflake.', 'Qualifications', ""Our enterprise manufacturing and distribution client headquartered here in Tampa, FL is searching for a Data Engineer focused on cloud migration and Snowflake implementation. As a Data Engineer focused in Snowflake you will be responsible for designing, developing, and optimizing data warehouses, data pipelines, and cloud infrastructure to drive intelligent applications and products as part of the client's regional data improvement process!"", 'Technical oversight for advanced analytical solutions to deliver real-time insights in a citizen developer model.', 'Data Engineer (Snowflake)', 'Constructing new data platforms and developing pipelines, ETL and analytical processes to support products and solutions.', 'Hands-on experience in designing data solutions using Snowflake as an extension of existing on-premise or cloud data warehouses.', 'Specific technologies: Snowflake, Azure, SSIS, dbt', 'Experience with modern Data Ingestion/Data Integration/Data Pipeline/Data Orchestration tools & streaming services', 'Preparing technical design documents and estimation including storage, resource, and operational details.', 'Experience developing and tuning ETL processes and utilizing industry standard ETL tools', 'Developing strategies, policies and best practices regarding Big Data and Cloud data acquisition, integration and analytics.', 'Responsibilities']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-01-19 09:25:31
Data Engineer II,Novant Health,"Fort Mill, SC",14 hours ago,Be among the first 25 applicants,"['', ' Understanding of Healthcare data is desired. ', ' Our team members are part of an environment that fosters team work, team member engagement and community involvement.  The successful team member has a commitment to leveraging diversity and inclusion in support of quality care.  All Novant Health team members are responsible for fostering a safe patient environment driven by the principles of ""First Do No Harm"". ', ' Strong verbal and written communication skills. ', ' Understanding of Healthcare data is desired.  Working knowledge of and experience with ETL tools such as SSIS, Informatica, or Attunity Compose.  Working knowledge of and experience with data orchestration tools such as Control-M or ADF. ', "" Education: Associate's degree in computer science or related field required. Bachelor's degree in computer science or related field preferred.  Experience: A minimum of five years in Information Technology required. A minimum of one year with at least one contemporary data transformation tool preferred.  Additional skills required:Capable of handling multiple tasks with minimal supervision. Possess strong organizational, communication, problem solving/analytical skills.  Experienced with standard query language (SQL), knowledge of and experience with Data Warehouse concepts and design.  Must be able to build processes supporting data transformations, data structures, metadata, dependency and workload management.  Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores, and big data pipeline.  Strong verbal and written communication skills.  Ability to read, understand, and develop various data structures.  Able to work independently and take ownership of tasks.  Also must be able to work well within a team.  Ability to drive/travel to multiple facilities/locations as needed.  Ability to be on-call and respond to production issues in a timely manner. Additional skills preferred : "", ' Ability to read, understand, and develop various data structures.  Able to work independently and take ownership of tasks.  Also must be able to work well within a team.  Ability to drive/travel to multiple facilities/locations as needed.  Ability to be on-call and respond to production issues in a timely manner. ', 'Capable of handling multiple tasks with minimal supervision.', ' Possess strong organizational, communication, problem solving/analytical skills. ', 'Additional skills required', ' Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores, and big data pipeline. ', 'Additional skills preferred', ' Experienced with standard query language (SQL), knowledge of and experience with Data Warehouse concepts and design. ', 'Qualifications', 'Education', ' Also must be able to work well within a team. ', ' Ability to be on-call and respond to production issues in a timely manner. ', ' Experience: A minimum of five years in Information Technology required. A minimum of one year with at least one contemporary data transformation tool preferred. ', ' The successful team member has a commitment to leveraging diversity and inclusion in support of quality care. ', "" Education: Associate's degree in computer science or related field required. Bachelor's degree in computer science or related field preferred. "", ' Additional skills required:Capable of handling multiple tasks with minimal supervision. Possess strong organizational, communication, problem solving/analytical skills.  Experienced with standard query language (SQL), knowledge of and experience with Data Warehouse concepts and design.  Must be able to build processes supporting data transformations, data structures, metadata, dependency and workload management.  Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores, and big data pipeline.  Strong verbal and written communication skills.  Ability to read, understand, and develop various data structures.  Able to work independently and take ownership of tasks.  Also must be able to work well within a team.  Ability to drive/travel to multiple facilities/locations as needed.  Ability to be on-call and respond to production issues in a timely manner. ', 'Capable of handling multiple tasks with minimal supervision. Possess strong organizational, communication, problem solving/analytical skills.  Experienced with standard query language (SQL), knowledge of and experience with Data Warehouse concepts and design.  Must be able to build processes supporting data transformations, data structures, metadata, dependency and workload management.  Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores, and big data pipeline.  Strong verbal and written communication skills. ', ' Need help finding the right job? ', 'Additional skills preferred : ', ' Working knowledge of and experience with ETL tools such as SSIS, Informatica, or Attunity Compose. ', ' Working knowledge of and experience with data orchestration tools such as Control-M or ADF. ', ' Our team members are part of an environment that fosters team work, team member engagement and community involvement. ', ' Able to work independently and take ownership of tasks. ', 'Overview', ' Ability to drive/travel to multiple facilities/locations as needed. ', ' Must be able to build processes supporting data transformations, data structures, metadata, dependency and workload management. ', 'Experience', ' All Novant Health team members are responsible for fostering a safe patient environment driven by the principles of ""First Do No Harm"". ', ' Working knowledge of and experience with one or more of ADLS, ADW, SQL PaaS, Azure Data Bricks, and No SQL databases. ', 'Responsibilities', ' Ability to read, understand, and develop various data structures. ']",Entry level,Full-time,Information Technology,"Health, Wellness and Fitness",2021-01-19 09:25:31
Data Engineer,"Tech Providers, Inc","Plano, TX",21 hours ago,Be among the first 25 applicants,[],Entry level,Full-time,Information Technology,Staffing and Recruiting,2021-01-19 09:25:31
