job_title,company,location,date_posted,applicants,job_text,seniority_level,employment_type,job_function,industries,date_scraped
Data Engineer,Seattle Children's,"Seattle, WA",4 hours ago,Be among the first 25 applicants,"['', ' Design, implement, test, and deploy data processing infrastructure.', ' Perform work in an Agile team setting.', ' Develop highly scalable and reliable data engineering solutions for moving data efficiently across systems. Design, implement, test, and deploy data processing infrastructure. Break down, estimate, and provide just-in-time design for small increments of work. Perform work in an Agile team setting.', ' Develop highly scalable and reliable data engineering solutions for moving data efficiently across systems.', ' Competitive Benefits: We support a healthy work-life balance. Our benefits include employee care, paid time off, health insurance and retirement savings.', "" Leader in Pediatric Care: Because of our people, Seattle Children's is recognized as a leading teaching, research and specialty care center at the forefront of pediatric care. Competitive Benefits: We support a healthy work-life balance. Our benefits include employee care, paid time off, health insurance and retirement savings. Diversity/Inclusion: We strive to maintain an atmosphere that reflects our values of inclusion by providing effective and respectful care compatible with each patient and family's beliefs, values, and heritage."", "" Diversity/Inclusion: We strive to maintain an atmosphere that reflects our values of inclusion by providing effective and respectful care compatible with each patient and family's beliefs, values, and heritage."", 'Requirements', 'Responsibilities', ' Break down, estimate, and provide just-in-time design for small increments of work.', ""Seattle Children's Employer Highlights"", "" Leader in Pediatric Care: Because of our people, Seattle Children's is recognized as a leading teaching, research and specialty care center at the forefront of pediatric care."", 'Overview']",Entry level,Full-time,Legal,Nonprofit Organization Management,2021-02-05 10:31:41
Data Engineer I,CSX,"Jacksonville, FL",9 hours ago,Be among the first 25 applicants,"['', 'Primary Location', 'High School Diploma/GED', "" Bachelor's Degree/4-year Degree "", 'Minimum Qualifications', 'Provide impact analysis for changes to programs and systems', 'Preferred Qualifications', 'Translate functional requirements into working solutions', 'Time management and organizational planning skills', 'About CSX', 'Knowledge of analytical and visualization tools (eg. Tableau, Power BI, SAS, Alteryx) ; Knowledge of reporting technologies (eg. Business Objects)', 'Develop information solutions for data-related problems using a variety of technical and analytical skills', 'Experience in translating data into information that delivers business value through creative data processing and visualization techniques (e.g., Tableau)', 'Experience writing SQL Queries for Relational Databases', ' High School Diploma/GED 2 or more years of experience in required in Software Application Design and Development ', 'Collaborate with other IT partners within the company', 'Tax Status', 'Overtime Status', 'Travel', 'Debugging and problem solving skills', 'Miscellaneous activities and responsibilities as assigned by manager', '2 or more years of experience in Experience in Transportation/Intermodal Operations, Information Systems, or Logistics', 'Excellent customer service skills and strong problem solving skills', 'Job Summary', 'Knowledge of Agile Scrum methodologies and their application', 'Experience with Big Data and data processing technologies including Spark, Data Lakes, NoSQL, In-memory, SQL, PL/SQL, R, and/or Python', 'Interact with internal customers to identify business requirements and develop the best course of actions to meet their needs', 'Actively engaged with immediate team members to ensure alignment with specified technical deliverables', 'Closing Date', 'Provide ad-hoc analyses with large data sets', 'Collaborate with team members, architects, data scientists and software developers to gather requirements and work towards delivering complete analytics/reporting solutions that help improve organizational decision making', 'Ability to handle multiple tasks and priorities', 'Safety Commitment', 'Relocation Available', 'Ability to analyze complex issues and to assist in their resolution', 'May be required to complete a background check', 'Work flexible days/hours as dictated by business needs', ""Bachelor's Degree/4-year Degree"", 'Meticulous in approach to work and outstanding attention to detail', 'Willingness to learn new technologies', '2 or more years of experience in required in Software Application Design and Development', ' Graduate Degree 2 or more years of experience in Experience in Transportation/Intermodal Operations, Information Systems, or Logistics ', 'Number of Openings', 'Assist in the definition of best practices and strategies for the team', 'Job Requirements', 'Graduate Degree', 'Knowledge of change management processes and the full Software Development Lifecycle (SDLC) concepts', ' Collaborate with other IT partners within the company Interact with internal customers to identify business requirements and develop the best course of actions to meet their needs Translate functional requirements into working solutions Develop information solutions for data-related problems using a variety of technical and analytical skills Provide impact analysis for changes to programs and systems Provide production support responsibilities for one or more reporting and analytical solutions Collaborate with team members, architects, data scientists and software developers to gather requirements and work towards delivering complete analytics/reporting solutions that help improve organizational decision making Provide ad-hoc analyses with large data sets Actively engaged with immediate team members to ensure alignment with specified technical deliverables Design and build technical solutions for typical business problems based on standards and guidance from more experienced team members Assist in the definition of best practices and strategies for the team Miscellaneous activities and responsibilities as assigned by manager ', 'Data Analytical skills', 'CSX Company', ' Knowledge of Agile Scrum methodologies and their application Debugging and problem solving skills Knowledge of change management processes and the full Software Development Lifecycle (SDLC) concepts Meticulous in approach to work and outstanding attention to detail Experience with Big Data and data processing technologies including Spark, Data Lakes, NoSQL, In-memory, SQL, PL/SQL, R, and/or Python Time management and organizational planning skills Ability to handle multiple tasks and priorities Willingness to learn new technologies Data Analytical skills Excellent customer service skills and strong problem solving skills Experience in translating data into information that delivers business value through creative data processing and visualization techniques (e.g., Tableau) Ability to communicate and collaborate effectively with team members and peers (both written and orally) Experience writing SQL Queries for Relational Databases Knowledge of analytical and visualization tools (eg. Tableau, Power BI, SAS, Alteryx) ; Knowledge of reporting technologies (eg. Business Objects) Ability to analyze complex issues and to assist in their resolution ', 'Equivalent Minimum Qualifications', 'Primary Activities And Responsibilities', 'Knowledge And Skills', 'Leadership', 'Work hours may vary in length and schedule (may include a non-standard work week)', 'Provide production support responsibilities for one or more reporting and analytical solutions', 'Design and build technical solutions for typical business problems based on standards and guidance from more experienced team members', 'Ability to communicate and collaborate effectively with team members and peers (both written and orally)', 'Closing Statement', ' May be required to complete a background check Work hours may vary in length and schedule (may include a non-standard work week) Work flexible days/hours as dictated by business needs ']",Not Applicable,Full-time,Information Technology,Financial Services,2021-02-05 10:31:41
Data Engineer II,Grubhub,"New Philadelphia, PA",4 hours ago,Be among the first 25 applicants,"['', 'What You Bring To The Table', ' Excellent communication skills, including the ability to crystallize and broadly socialize insights ', ' Experience big data processing with Spark and other big data tools a plus  Excellent communication skills, including the ability to crystallize and broadly socialize insights  Problem analysis and problem-solving skills  Rigorous attention to detail and accuracy  Exposure to Amazon AWS or another cloud provider  Adaptability and collaborative skills ', ' Fun. Every Grubhub office has an employee-led Culture Crew that connects people through fun, meaningful events and initiatives. Some of our popular past events include: Wing-eating contests, Grubtoberfest, 5k Runs, Bring Your Child to Work Day, regular happy hours, and more! ', ' Social Impact. We believe in the importance of serving the communities that support our business. In addition, employees are given paid time off each year to support the causes that are important to them. ', ' Analyze data to measure impacts of data schemas and use it to iterate on improvements ', ' Health and Wellness. We provide programs that support your overall well-being such as generous medical benefits, employee network groups, company-wide fitness challenges, and a comfortable and casual workplace! We also support our parents by offering 8 weeks of paid parent bonding time, a 4-week returnship program, and 6-8 weeks paid medical leave. ', 'Got These? Even Better: ', ' Translate from technical to business, and vice versa. You need to be able to speak with the least technically-minded client (internal or external) and make technology make sense to them. Then turn around and do it the other way ', "" A bachelor's degree, preferably in a computer-related discipline. "", ' Understand our stakeholder (Finance, Marketing, Product) requirements and write complex and efficient code to transform raw data into an easy to approach data marts. ', ' Working with high volumes of data to efficiently process and expose for analysis ', "" Excellent knowledge on SQL, data modeling and patterns.  5-7 years experience with Python or another general purpose programming language  Background in writing ETL jobs within a Business Intelligence context  A bachelor's degree, preferably in a computer-related discipline.  Enthusiasm for the job. Are you excited about data? Do you love your users? Good, the same goes for us "", ' Rigorous attention to detail and accuracy ', ' Learning and Career Growth. Your personal and professional development is a priority at Grubhub. From day one, we empower you to lead and be an active participant in your career growth. We provide continuous learning opportunities, training, and coaching and mentorship programs. ', ' Why Work For Us ', ' Experience big data processing with Spark and other big data tools a plus ', ' Excellent knowledge on SQL, data modeling and patterns. ', 'Options', ' Problem analysis and problem-solving skills ', ' Exposure to Amazon AWS or another cloud provider ', ' Connect With Us! ', ' Background in writing ETL jobs within a Business Intelligence context ', ' Enthusiasm for the job. Are you excited about data? Do you love your users? Good, the same goes for us ', ' Doing deep dives on business verticals where you become one of the foremost experts on that vertical in the company ', ' Flexible PTO. Grubhub employees are provided a generous amount of time to recharge their batteries.  Health and Wellness. We provide programs that support your overall well-being such as generous medical benefits, employee network groups, company-wide fitness challenges, and a comfortable and casual workplace! We also support our parents by offering 8 weeks of paid parent bonding time, a 4-week returnship program, and 6-8 weeks paid medical leave.  Learning and Career Growth. Your personal and professional development is a priority at Grubhub. From day one, we empower you to lead and be an active participant in your career growth. We provide continuous learning opportunities, training, and coaching and mentorship programs.  MealPerks. Who’s ready for some lunch? We provide our employees with a weekly Grubhub credit to enjoy and support local restaurants. We also offer company-wide meals several times a year to bring our Grubhub family together.  Fun. Every Grubhub office has an employee-led Culture Crew that connects people through fun, meaningful events and initiatives. Some of our popular past events include: Wing-eating contests, Grubtoberfest, 5k Runs, Bring Your Child to Work Day, regular happy hours, and more!  Social Impact. We believe in the importance of serving the communities that support our business. In addition, employees are given paid time off each year to support the causes that are important to them. ', 'More About The Role', ' Work with cutting edge data processing technologies ', 'About The Opportunity', ' Adaptability and collaborative skills ', ' Flexible PTO. Grubhub employees are provided a generous amount of time to recharge their batteries. ', ' And Of Course, Perks! ', ' The Impact You Will Make ', ' MealPerks. Who’s ready for some lunch? We provide our employees with a weekly Grubhub credit to enjoy and support local restaurants. We also offer company-wide meals several times a year to bring our Grubhub family together. ', ' 5-7 years experience with Python or another general purpose programming language ', ' Collaborating with other engineering teams on strategies for data ', ' Working with high volumes of data to efficiently process and expose for analysis  Collaborating with other engineering teams on strategies for data  Work with cutting edge data processing technologies  Understand our stakeholder (Finance, Marketing, Product) requirements and write complex and efficient code to transform raw data into an easy to approach data marts.  Doing deep dives on business verticals where you become one of the foremost experts on that vertical in the company  Analyze data to measure impacts of data schemas and use it to iterate on improvements  Translate from technical to business, and vice versa. You need to be able to speak with the least technically-minded client (internal or external) and make technology make sense to them. Then turn around and do it the other way ']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-02-05 10:31:41
Data Engineer,Evolent Health,"Arlington, VA",12 hours ago,Be among the first 25 applicants,"['', 'What You’ll Be Doing', 'Experience or qualifications within Azure Synapse or relative components (High Value)', ' Microsoft SSIS processing engine', 'Your Future Evolves Here', 'Ability to work in a fast-paced entrepreneurial environment with small focused team of engineers to deliver solutions', 'Experience in medical, pharmacy, provider and laboratory domains (High Value)', ' Ability to work both independently, and as part of a globally distributed team of technical and non-technical colleagues ', 'Local candidates (Chicago, IL or Arlington, VA) are preferred for this role. We ask that candidates located outside of the Chicago or Arlington areas be willing to work the normal business hours that align with our local teams.', "" Bachelor's degree in Computer Science or related field Strong attention to detail, analytical thinking, and outstanding problem-solving skills  Ability to work both independently, and as part of a globally distributed team of technical and non-technical colleagues  Strong written and verbal communication skills  Three or more years of experience with: Software and application development  Microsoft Data Storage Technologies - specifically SQL Server 2017+  Microsoft SSIS processing engine Ability to partner collaboratively with stakeholders"", "" Bachelor's degree in Computer Science or related field"", ' Strong written and verbal communication skills ', 'Experience with MicroStrategy', ' Software and application development ', 'Technical Requirements', 'Experience creating system documentation for both data and business processes flows', ' Ability to partner collaboratively with stakeholders', 'It’s Time For A Change…', 'Finishing Touches (Preferred)', ' Software and application development  Microsoft Data Storage Technologies - specifically SQL Server 2017+  Microsoft SSIS processing engine', ' Three or more years of experience with: Software and application development  Microsoft Data Storage Technologies - specifically SQL Server 2017+  Microsoft SSIS processing engine', 'Experience or qualifications within Azure Synapse or relative components (High Value)Experience in medical, pharmacy, provider and laboratory domains (High Value)Experience in various data formats used in healthcare - NCPDP, HL7, X12 and XMLExperience with MicroStrategyExperience creating system documentation for both data and business processes flowsAbility to work in a fast-paced entrepreneurial environment with small focused team of engineers to deliver solutions', 'The Experience You’ll Need (Required)', ' Strong attention to detail, analytical thinking, and outstanding problem-solving skills ', ' Microsoft Data Storage Technologies - specifically SQL Server 2017+ ', 'Experience in various data formats used in healthcare - NCPDP, HL7, X12 and XML', 'Evolent Health is an equal opportunity employer and considers all qualified applicants equally without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran status, or disability status.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-02-05 10:31:41
Data Engineer,"Surya Systems, Inc","San Francisco, CA",56 minutes ago,Be among the first 25 applicants,"['', 'Thanks & Regards...\xa0', 'Experience working in Agile environment', 'G Naveen Kumar', 'Background with various scripting and coding languages (Python)\xa0\xa0\xa0', 'Location: San Francisco, California', 'Experience working in in SQL and have ability to build queries on complex datasets', 'Exposure to big data platforms a plus (Hadoop, Spark, etc)', 'Ability to analyze and troubleshoot complex data issues\xa0\xa0\xa0', 'Position: Data Engineer ', 'Duration: Long Term', 'Surya Systems, Inc', 'Expertise working in Python\xa0', 'Email : GavvalaN@suryasys.com', '...\xa0', 'Exton, PA 19341', 'Expertise working in Python\xa0Experience working in in SQL and have ability to build queries on complex datasetsExposure to big data platforms a plus (Hadoop, Spark, etc)Ability to analyze and troubleshoot complex data issues\xa0\xa0\xa0Background with various scripting and coding languages (Python)\xa0\xa0\xa0Experience working in Agile environment', 'Role Qualifications:', '\xa0', '120 E Uwchlan Avenue Suite 203']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-02-05 10:31:41
"Data Engineer, Marketing",Square,"San Francisco, CA",11 hours ago,Be among the first 25 applicants,"['', 'Employee Stock Purchase Program', 'Company Description', 'SQL and PythonLooker, or other data visualizations technologiesETL scheduling technologies with dependency checking such as AirflowLinux/OSX command line, version control software (git)', 'Technologies We Use And Teach', 'Develop data foundation and reporting infrastructure to ensure accurate and reliable business reporting', 'Healthcare coverage', 'Develop data foundation and reporting infrastructure to ensure accurate and reliable business reportingPartner with business leads and cross-functional teams to understand their data and reporting requirements and translate them into Product Requirement Definition (PRD), technical specifications and scalable implementationBe the expert on end-to-end data flow for MarketingImplement data model and ETL code improvements to improve pipeline efficiency and data qualityMentor Data Engineers and Data Analysts, and promote data engineering best practices', 'Learning and Development resources', '2+ years experience in Data Engineering or similar roleExpert knowledge in writing complex SQL and ETL development with experience processing extremely large datasets within cloud-based data warehouses such as Snowflake, Google BigQuery, and Amazon RedshiftExperience working with business teams on complex problems and can translate them to efficient, scalable and easy to maintain data engineering solutions and data visualizationExpert knowledge in data modeling concepts and implementationExpertise in visualization technologies such as Tableau and/or LookerExperience with PythonExperience with Linux/OSX command line, version control software (git), and general software developmentBS degree in Engineering, Computer Science, Math or a related technical field', 'Linux/OSX command line, version control software (git)', 'Implement data model and ETL code improvements to improve pipeline efficiency and data quality', 'Experience working with business teams on complex problems and can translate them to efficient, scalable and easy to maintain data engineering solutions and data visualization', 'Experience with Python', 'SQL and Python', 'Perks', 'Be the expert on end-to-end data flow for Marketing', '2+ years experience in Data Engineering or similar role', 'Wellness perks', 'Job Description', 'Mentor Data Engineers and Data Analysts, and promote data engineering best practices', 'Expert knowledge in writing complex SQL and ETL development with experience processing extremely large datasets within cloud-based data warehouses such as Snowflake, Google BigQuery, and Amazon Redshift', 'Paid time off', 'ETL scheduling technologies with dependency checking such as Airflow', 'You Will', 'You Have', 'Experience with Linux/OSX command line, version control software (git), and general software development', 'Healthcare coverageRetirement PlansEmployee Stock Purchase ProgramWellness perksPaid parental leavePaid time offLearning and Development resources', 'Retirement Plans', 'Expertise in visualization technologies such as Tableau and/or Looker', 'Expert knowledge in data modeling concepts and implementation', 'BS degree in Engineering, Computer Science, Math or a related technical field', 'Partner with business leads and cross-functional teams to understand their data and reporting requirements and translate them into Product Requirement Definition (PRD), technical specifications and scalable implementation', 'Paid parental leave', 'Looker, or other data visualizations technologies']",Associate,Full-time,Information Technology,Computer Software,2021-02-05 10:31:41
Data Engineer,Team Cymru,"Lake Mary, FL",9 hours ago,Be among the first 25 applicants,"['', 'Contributes across whole project lifecycle, utilizing peers for guidance where necessary.', 'Actively contributes to cross-functional team efforts', 'Identifies risks to projects, communicates and formulates mitigation plans', 'Education and Experience:', 'Articulate in oral and written communication', 'Ability to recognize trends and patterns in the data that can be exploited into a repeatable analysis process', 'Proficiency in one or more core languages: Golang, Python, SQL, Bash, Perl', ""Familiarity with design patterns and industry best practicesExperience with one or alternative database technologies like: ElasticSearch, Apache Cassandra, Mongo DB, SparkExperience with Cloud technologies like: AWS, Google Cloud, AzureAbility to effectively create and utilize REST APIsProactively creates automated analytics solutions to push team's capabilities and increased situational awarenessKnowledge of MVC frameworksAbility to execute complex queries and design relational databases in PostgreSQL using referential integrity, views, stored procedures and proper indicesAbility to create visualizations from resultant analytic resultsExperience creating and distributing Jupyter Notebooks for repeatable data analysis"", 'Experience creating and distributing Jupyter Notebooks for repeatable data analysis', 'Location:', 'Required Skills/Abilities:', 'Solid oral and written communications skills', 'Performs triage of product support requests, problem determination and assists with escalation when appropriate', 'Competent with Linux', 'Able to break-down complex requirements into workflows and identify key performance indicators.', 'Supervisory Responsibilities:', 'Prolonged periods of sitting at a desk and working on a computer.Must be able to travel up to 5% of the time.', ""Proactively creates automated analytics solutions to push team's capabilities and increased situational awareness"", 'High school diploma or equivalent requiredTypically has two to four years combined industry / education experienceSome specialized training or education beyond high school is preferred', 'Embodies and demonstrates maturity, professionalism, and ethicsArticulate in oral and written communicationWorking-level knowledge of algorithmsDemonstrates sound coding techniquesAble to break-down complex requirements into workflows and identify key performance indicators.Proficient in the use of databases: query and data definitionProficiency in one or more core languages: Golang, Python, SQL, Bash, Perl', 'Proficient in the use of databases: query and data definition', 'Working knowledge of networking protocols', 'Knowledge of MVC frameworks', 'Experience with Cloud technologies like: AWS, Google Cloud, Azure', 'Typically has two to four years combined industry / education experience', ""Proficiency in designing and developing innovative data analytics software and methods.Contributes across whole project lifecycle, utilizing peers for guidance where necessary.Operate independently and seeks assistance or guidance when required.Ability to recognize trends and patterns in the data that can be exploited into a repeatable analysis processPerforms triage of product support requests, problem determination and assists with escalation when appropriateDemonstrates a complete understanding of a core-product or service offering's features, construction and operating characteristicsIncorporates effective test procedures, logging and monitoring in software with minimal oversightParticipates in regular review of individual output to ensure it conforms to department and company standardsContributes to efforts in maintaining and improving product qualityidentification and submission of product improvement when appropriateCreates quality product and support documentationIdentifies risks to projects, communicates and formulates mitigation plansActively contributes to cross-functional team effortsConducts self-assessments by comparing required skills with existing knowledge to develop, present and execute plans for improvementConsistently delivers to deadlines at the required quality standards"", 'Participates in regular review of individual output to ensure it conforms to department and company standards', 'Ability to execute complex queries and design relational databases in PostgreSQL using referential integrity, views, stored procedures and proper indices', 'Consistently adheres to commitments with respect to delivery and timeframe', 'Job Summary:', 'Consistently delivers to deadlines at the required quality standards', 'Duties/Responsibilities:', 'Virtual', 'Embodies and demonstrates maturity, professionalism, and ethics', ' ', 'High school diploma or equivalent required', 'Additional Desired Skills/Abilities', 'Ability to create visualizations from resultant analytic results', ""Demonstrates a complete understanding of a core-product or service offering's features, construction and operating characteristics"", 'Demonstrates sound coding techniques', 'Incorporates effective test procedures, logging and monitoring in software with minimal oversight', 'Data Engineer', 'Experience with one or alternative database technologies like: ElasticSearch, Apache Cassandra, Mongo DB, Spark', 'Proficiency in designing and developing innovative data analytics software and methods.', 'Must be able to travel up to 5% of the time.', 'Contributes to efforts in maintaining and improving product quality', 'Prolonged periods of sitting at a desk and working on a computer.', 'The Data Engineer utilizes a wide range of technologies to design, develop, and deploy innovative programming and technical solutions to data analytics and data processing. The Data Engineer is expected to demonstrate increased proficiency in newly acquired industry-related skills. This person can work independently and produce work according to clear-cut and complete specifications.', 'Creates quality product and support documentation', 'Operate independently and seeks assistance or guidance when required.', 'Working-level knowledge of algorithms', 'Physical Requirements:', 'None.', 'Conducts self-assessments by comparing required skills with existing knowledge to develop, present and execute plans for improvement', 'Some specialized training or education beyond high school is preferred', 'Proficient in the use of industry standard tooling (i.e. the Atlassian Stack, etc.)', 'Ability to effectively create and utilize REST APIs', 'Competent with LinuxSolid oral and written communications skillsConsistently adheres to commitments with respect to delivery and timeframeWorking knowledge of networking protocols', 'identification and submission of product improvement when appropriate', 'Familiarity with design patterns and industry best practices']",Entry level,Full-time,Engineering,Information Technology and Services,2021-02-05 10:31:41
Data Engineer,Eventbrite,"Nashville, TN",2 hours ago,Be among the first 25 applicants,"['', 'Ability to teach and mentor engineers with a variety of skill levels and backgrounds', ' TECH STACK ', 'Bachelor’s degree or higher in a technical field (CS/Math/Stats/Engineering)', '3+ years of working experience in rapid product development building data infrastructure, ETL, or MapReduce jobs', 'Bachelor’s degree or higher in a technical field (CS/Math/Stats/Engineering)Passionate about live entertainment, and eager to help build Eventbrite into the world’s leading event technology platform', 'Understanding of Data Engineering, Data Science, Machine Learning, Business Analytics, and the relevant technologies that support them', 'Excellent customer service skillsOutstanding verbal, written, presentation, and facilitation skills. ', ' THE ROLE ', 'Familiarity with a server-side framework, such as Django, Express, Rails, or .Net', 'In particular, a demonstrated ability to effectively communicate technical and business issues and solutions to multiple organizational levels', 'Bonus ', 'The Skill Set ', '5 years of experience building high quality software in Python, Java, or Scala.', 'About Eventbrite', 'Strong analytical and problem solving skills and attention to detail', 'THE CHALLENGE ', 'Passionate about live entertainment, and eager to help build Eventbrite into the world’s leading event technology platform', 'Expert knowledge of SQL and relational database design and modeling, approaches and techniques for extracting, transforming, loading and integrating data', 'IS THIS ROLE NOT AN EXACT FIT?', ' THE TEAM ', '5 years of experience building high quality software in Python, Java, or Scala.3+ years of working experience in rapid product development building data infrastructure, ETL, or MapReduce jobsExpert knowledge of SQL and relational database design and modeling, approaches and techniques for extracting, transforming, loading and integrating dataUnderstanding of Data Engineering, Data Science, Machine Learning, Business Analytics, and the relevant technologies that support themFamiliarity with a server-side framework, such as Django, Express, Rails, or .NetExcellent customer service skillsOutstanding verbal, written, presentation, and facilitation skills. In particular, a demonstrated ability to effectively communicate technical and business issues and solutions to multiple organizational levelsAbility to teach and mentor engineers with a variety of skill levels and backgroundsStrong analytical and problem solving skills and attention to detail']",Entry level,Full-time,Information Technology,Computer Software,2021-02-05 10:31:41
Data Engineer,Imetris Corporation,United States,22 hours ago,28 applicants,"['', 'Spark, MySQL', 'Data engineer.', 'Good to Have:', 'We have an immediate need for a Data engineer. If interested in the long-term opportunity, please email your resume & contact details as soon as possible.\xa0', 'Airflow and spark is highly preferred ', 'Airflow', 'Airflow and spark is highly preferred Data oriented background candidates like Big data or data warehousing', 'Duration: Full time', 'Data oriented background candidates like Big data or data warehousing', 'Job Description ', '100% Remote', 'CloudSql', 'Big query', 'Location: 100% Remote', 'Location:', 'AirflowSpark, MySQLBig queryCloudSql', 'No third party candidates considered for this position']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-02-05 10:31:41
Data Engineer,Intel Corporation,"Hillsboro, OR",22 hours ago,Be among the first 25 applicants,"['', '2+ years of experience in Data Engineering role (or experience with RDBMS or Oracle or SQL or ETL)', 'Qualifications', ""Master's Degree in degree in Computer Science, Mathematics, Machine Learning, Operation Research, Statistics, Electrical Engineering, Computer Engineering or a related discipline2+ years of experience in Data Engineering role (or experience with RDBMS or Oracle or SQL or ETL)"", 'Experience in Hadoop and/or Python', ""Bachelor's degree in Computer Science, Mathematics, Machine Learning, Operation Research, Statistics, Electrical Engineering, Computer Engineering or related"", 'Job Description', ""Master's Degree in degree in Computer Science, Mathematics, Machine Learning, Operation Research, Statistics, Electrical Engineering, Computer Engineering or a related discipline"", 'Preferred Qualifications', '6+ months of work or internship experience as an engineer or data scientist.', 'Inside this Business Group', 'Posting Statement', ""Bachelor's degree in Computer Science, Mathematics, Machine Learning, Operation Research, Statistics, Electrical Engineering, Computer Engineering or related6+ months of work or internship experience as an engineer or data scientist.Experience in Hadoop and/or Python""]",Entry level,Full-time,Information Technology,Electrical/Electronic Manufacturing,2021-02-05 10:31:41
Data Engineer,Amazon,"Seattle, WA",18 hours ago,Be among the first 25 applicants,"['', ' Coding proficiency in at least one modern programming language (, , , etc)', ' Effective troubleshooting and problem solving skills', 'Basic Qualifications', ' Degree in Computer Science, Engineering, Mathematics, or a related field and 5+ years of professional experience', 'Description', 'Preferred Qualifications', ' Data warehousing experience with Redshift, Teradata, etc.', ' Excellent verbal and written communication', 'Company', ' Excellent verbal and written communication skills and the ability to work well in a team.', ' Degree in Computer Science, Engineering, Mathematics, or a related field and 5+ years of professional experience Experience in data modeling, ETL development and data warehousing Data warehousing experience with Redshift, Teradata, etc. Experience with Big Data Technologies (Hadoop, Hive, Hbase, Pig, Spark, etc.) Excellent verbal and written communication Effective troubleshooting and problem solving skills Strong customer focus, ownership, urgency and drive. Excellent verbal and written communication skills and the ability to work well in a team.', ' Experience in data modeling, ETL development and data warehousing', ' Experience with multiple database platforms', ' Knowledge in using OLAP technologies and BI Analytics.', ' Strong customer focus, ownership, urgency and drive.', ' Experience with Big Data Technologies (Hadoop, Hive, Hbase, Pig, Spark, etc.)', ' Professional experience as a Data Engineer or related specialty (e.g., Software Engineer, Business Intelligence Engineer, Data Scientist) with a track record of manipulating, processing, and extracting value from large datasets', ' Familiar with computer science fundamentals including object-oriented design, data structures, algorithm design, problem solving, and complexity analysis', ' Knowledge of AWS Technologies', ' Professional experience as a Data Engineer or related specialty (e.g., Software Engineer, Business Intelligence Engineer, Data Scientist) with a track record of manipulating, processing, and extracting value from large datasets Knowledge of AWS Technologies Knowledge in using OLAP technologies and BI Analytics. Oracle, Redshift, Linux, OBIEE experience Experience with multiple database platforms Familiar with computer science fundamentals including object-oriented design, data structures, algorithm design, problem solving, and complexity analysis Coding proficiency in at least one modern programming language (, , , etc) Query tuning skills', ' Oracle, Redshift, Linux, OBIEE experience', ' Query tuning skills']",Not Applicable,Full-time,Strategy/Planning,Computer Software,2021-02-05 10:31:41
Data Engineer III,Walmart,"Dallas, TX",15 hours ago,Be among the first 25 applicants,"['', ""What You'll Do"", 'Creates training documentation and trains end-users on data modeling. Oversees the tasks of less experienced programmers and stipulates system troubleshooting supports.', 'Provides and supports the implementation of business solutions by building relationships and partnerships with key stakeholders; identifying business needs; determining and carrying out necessary processes and practices; monitoring progress and results; recognizing and capitalizing on improvement opportunities; and adapting to competing demands, organizational changes, and new responsibilities.', ""Problem Formulation: Identifies possible options to address the business problems within one's discipline through analytics, big data analytics, and automation."", 'Outlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications. ', 'Models compliance with company policies and procedures and supports company mission, values, and standards of ethics and integrity by incorporating these into the development and implementation of business plans; using the Open Door Policy; and demonstrating and assisting others with how to apply these in executing business processes and practices.', 'Data Strategy: Understands, articulates, and applies principles of the defined strategy to routine business problems that involve a single function.', 'Data Governance: Supports the documentation of data governance processes. Supports the implementation of data governance practices.', ' Outlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.  ', 'Applied Business Acumen: Supports the development of business cases and recommendations. Owns delivery of project activity and tasks assigned by others. Supports process updates and changes. Solves business issues.', 'Demonstrates up-to-date expertise and applies this to the development, execution, and improvement of action plans by providing expert advice and guidance to others in the application of information and best practices; supporting and aligning efforts to meet customer and business needs; and building commitment for perspectives and rationales.', 'Data Transformation and Integration: Extracts data from identified databases. Creates data pipelines and transform data to a structure that is relevant to the problem by selecting appropriate techniques. Develops knowledge of current data science and analytics trends.', 'Code Development and Testing: Writes code to develop the required solution and application features by determining the appropriate programming language and leveraging business, technical, and data requirements. Creates test cases to review and validate the proposed solution design. Creates proofs of concept. Tests the code using the appropriate testing approach. Deploys software to production servers. Contributes code documentation, maintains playbooks, and provides timely progress updates.', 'Minimum Qualifications...', ""Problem Formulation: Identifies possible options to address the business problems within one's discipline through analytics, big data analytics, and automation.Applied Business Acumen: Supports the development of business cases and recommendations. Owns delivery of project activity and tasks assigned by others. Supports process updates and changes. Solves business issues.Data Governance: Supports the documentation of data governance processes. Supports the implementation of data governance practices.Data Strategy: Understands, articulates, and applies principles of the defined strategy to routine business problems that involve a single function.Data Transformation and Integration: Extracts data from identified databases. Creates data pipelines and transform data to a structure that is relevant to the problem by selecting appropriate techniques. Develops knowledge of current data science and analytics trends.Data Source Identification: Supports the understanding of the priority order of requirements and service level agreements. Helps identify the most suitable source for data that is fit for purpose. Performs initial data quality checks on extracted data.Data Modeling: Analyzes complex data elements, systems, data flows, dependencies, and relationships to contribute to conceptual, physical, and logical data models. Develops the Logical Data Model and Physical Data Models including data warehouse and data mart designs. Defines relational tables, primary and foreign keys, and stored procedures to create a data model structure. Evaluates existing data models and physical databases for variances and discrepancies. Develops efficient data flows. Analyzes data-related system integration challenges and proposes appropriate solutions.Creates training documentation and trains end-users on data modeling. Oversees the tasks of less experienced programmers and stipulates system troubleshooting supports.Code Development and Testing: Writes code to develop the required solution and application features by determining the appropriate programming language and leveraging business, technical, and data requirements. Creates test cases to review and validate the proposed solution design. Creates proofs of concept. Tests the code using the appropriate testing approach. Deploys software to production servers. Contributes code documentation, maintains playbooks, and provides timely progress updates.Demonstrates up-to-date expertise and applies this to the development, execution, and improvement of action plans by providing expert advice and guidance to others in the application of information and best practices; supporting and aligning efforts to meet customer and business needs; and building commitment for perspectives and rationales.Provides and supports the implementation of business solutions by building relationships and partnerships with key stakeholders; identifying business needs; determining and carrying out necessary processes and practices; monitoring progress and results; recognizing and capitalizing on improvement opportunities; and adapting to competing demands, organizational changes, and new responsibilities.Models compliance with company policies and procedures and supports company mission, values, and standards of ethics and integrity by incorporating these into the development and implementation of business plans; using the Open Door Policy; and demonstrating and assisting others with how to apply these in executing business processes and practices."", 'Outlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications. ', 'Data Modeling: Analyzes complex data elements, systems, data flows, dependencies, and relationships to contribute to conceptual, physical, and logical data models. Develops the Logical Data Model and Physical Data Models including data warehouse and data mart designs. Defines relational tables, primary and foreign keys, and stored procedures to create a data model structure. Evaluates existing data models and physical databases for variances and discrepancies. Develops efficient data flows. Analyzes data-related system integration challenges and proposes appropriate solutions.', 'Data Source Identification: Supports the understanding of the priority order of requirements and service level agreements. Helps identify the most suitable source for data that is fit for purpose. Performs initial data quality checks on extracted data.']",Associate,Full-time,Information Technology,Information Technology and Services,2021-02-05 10:31:41
Data Engineer,Peak-Ryzex,"Columbia, MD",51 minutes ago,Be among the first 25 applicants,"['', 'Achieves operational goals set by the Company.', 'Degree in Computer Science, IT, or similar field; a Masters is a plus', 'Works closely with all business units and engineering teams to develop strategy for long term data platform architecture.', 'Great numerical and analytical skills', 'Knowledge of programming languages(e.g. Java, Python)', 'Performs data analysis required to troubleshoot data related issues and assist in the resolution of data issues.', 'Designs and evaluates open source and vendor tools for data lineage.', 'Performs other duties/special projects as assigned.', 'Supports corporate initiatives and decisions.', 'Hands-on experience with SQL database design', 'Data engineering certification (e.g IBM Certified Data Engineer) is a plus', 'Previous experience with Salesforce and SAP data structures.', 'Works closely with a team of frontend and backend engineers, product managers, and analysts.', 'Designs data integrations and data quality framework.', 'Collaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.', 'Writes unit/integration tests, contributes to engineering wiki, and documents work.', 'Job Requirements', 'Implements processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it.', 'Responsibilities Include, But Are Not Limited To', 'Previous experience in software architecture background.', 'Assists in driving a continuous improvement strategy within the business to result in on going incremental gains in quality and efficiency.', ' Previous experience in software architecture background. Technical expertise with data models, data mining, and segmentation techniques Knowledge of programming languages(e.g. Java, Python) Previous experience with Salesforce and SAP data structures. Hands-on experience with SQL database design Knowledge of reporting tool (e.g. Power BI, Tableau) Great numerical and analytical skills Degree in Computer Science, IT, or similar field; a Masters is a plus Data engineering certification (e.g IBM Certified Data Engineer) is a plus', 'Technical expertise with data models, data mining, and segmentation techniques', 'Knowledge of reporting tool (e.g. Power BI, Tableau)', 'Develops and maintains scalable data pipelines and builds out new API integrations to support continuing increases in data volume and complexity.', ' Develops and maintains scalable data pipelines and builds out new API integrations to support continuing increases in data volume and complexity. Collaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization. Implements processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it. Writes unit/integration tests, contributes to engineering wiki, and documents work. Performs data analysis required to troubleshoot data related issues and assist in the resolution of data issues. Works closely with a team of frontend and backend engineers, product managers, and analysts. Designs data integrations and data quality framework. Designs and evaluates open source and vendor tools for data lineage. Works closely with all business units and engineering teams to develop strategy for long term data platform architecture. Assists in driving a continuous improvement strategy within the business to result in on going incremental gains in quality and efficiency. Achieves operational goals set by the Company. Supports corporate initiatives and decisions. Performs other duties/special projects as assigned.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-02-05 10:31:41
Data Engineer,Beghou Consulting,San Francisco Bay Area,23 hours ago,Be among the first 25 applicants,"['', 'Develop best practice guidance and supporting materials for data management and advanced analytics pipelines', 'We’d love to see:', 'All applicants must be currently authorized to work in the United States on a full-time basis.\xa0Beghou\xa0Consulting will not sponsor applicants for work visas.', 'Experience configuring AzureAD/SAML/Okta/Oauth and administering AWS or Azure security best practices', 'You’ll need to have:', '3+ years’ experience in data engineering or application development using Python, including use of pandas or PySpark', 'Software development fundamentals, including participating in Agile development, use of version control systems such as Git or DevOps, code reviews, emphasis on testing, and dedication to documentation', 'Knowledge and experience configuring AWS or Azure cloud infrastructure and managed services', 'As a Data Engineer, your goal is to develop and enhance an in-house enterprise data platform and tools used to ingest, transform, and analyze data to yield value for our clients. You will build data integrations, connectors, and analytics applications using tools like Python and Spark and support deployment to client cloud environments. You will partner with our internal consulting teams to develop data integration and analytics pipelines and will help deliver uncovered insights to our clients. You will collaborate with our data platform and technology team to craft and evolve the tools and proprietary systems we offer.\xa0', 'Experience configuring AzureAD/SAML/Okta/Oauth and administering AWS or Azure security best practicesWeb application development experience using Flask, Django, JavaScript, Ajax, or CSS/HTMLContainer orchestration systems experience using Docker, Kubernetes, AWS ECS)Experience with WYSIWYG ETL tools (Azure Data Factory, Informatica, SnapLogic, Boomi)Experience with building systems in event-driven or streaming architecturesLife Sciences industry experience', 'Experience with relational database technologies, such as PostgreSQL, Oracle, MySQL, Redshift, Snowflake', 'You will have the opportunity to work with and learn from senior staff and partners, allowing everyone to work together to develop, achieve, and succeed with every project. ', 'We treat our employees with respect and appreciation, not only for what you do but who you are. ', 'Emphasize and ensure reliability and stability of our enterprise data platform and tools used by internal teams and life sciences clients', 'We treat our employees with respect and appreciation, not only for what you do but who you are. We value the many talents and abilities of our employees and promote a supportive, collaborative and dynamic work environment that encourages both professional and personal growth. You will have the opportunity to work with and learn from senior staff and partners, allowing everyone to work together to develop, achieve, and succeed with every project. We have had steady growth throughout our history because the people we hire are committed not only to delivering quality results for our clients, but also to becoming leaders in sales and marketing analytics.', 'Experience with WYSIWYG ETL tools (Azure Data Factory, Informatica, SnapLogic, Boomi)', 'Experience with building systems in event-driven or streaming architectures', 'Emphasize and ensure reliability and stability of our enterprise data platform and tools used by internal teams and life sciences clientsBuild and enhance data integration, management, and analytics tools and pipelines using pandas and SparkDevelop best practice guidance and supporting materials for data management and advanced analytics pipelines', 'Container orchestration systems experience using Docker, Kubernetes, AWS ECS)', 'We’ll trust you to:', 'Web application development experience using Flask, Django, JavaScript, Ajax, or CSS/HTML', 'As a result of our tremendous growth, Beghou Consulting is seeking experienced individuals to join our team of skilled data practitioners. We seek creative, adaptable, and analytical candidates who are committed to delivering innovative solutions to life sciences companies that exceed expectations. ', 'All applicants must be currently authorized to work in the United States on a full-time basis.\xa0Beghou\xa0Consulting will not sponsor applicants for work visas. We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.', '3+ years’ experience in data engineering or application development using Python, including use of pandas or PySparkExperience with relational database technologies, such as PostgreSQL, Oracle, MySQL, Redshift, SnowflakeKnowledge and experience configuring AWS or Azure cloud infrastructure and managed servicesSoftware development fundamentals, including participating in Agile development, use of version control systems such as Git or DevOps, code reviews, emphasis on testing, and dedication to documentation', '\xa0', 'What you should know: ', 'We value the many talents and abilities of our employees and promote a supportive, collaborative and dynamic work environment that encourages both professional and personal growth. ', 'Life Sciences industry experience', ' We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.', 'Build and enhance data integration, management, and analytics tools and pipelines using pandas and Spark', 'We have had steady growth throughout our history because the people we hire are committed not only to delivering quality results for our clients, but also to becoming leaders in sales and marketing analytics.']",Associate,Full-time,Information Technology,Management Consulting,2021-02-05 10:31:41
Data Engineer,Nomi Health,"Austin, TX",23 hours ago,35 applicants,"['', 'Must have MongoDB experience.', 'Must have ELT/ETL experience (Talend, MDM, other ETL/ELT tools)', 'Good written and verbal communication skills.', 'Outstanding problem solver.', 'Contribute to the development of the ML capabilities for the Nomi HealthDevelop and implement data models to guide business decisionsMapping data sources, including descriptions of the business meaning of the data, its uses, its quality, the applications that maintain it and the database technology in which it is stored. Documentation of a data source must describe the semantics of the data so that the occasional subtle differences in meaning are understood.Documenting interfaces and data movement by recording how mapped data is moved around the virtual enterprise. This includes the frequency of movement, the source and destination of each step, how the data is transformed as it moves, and any aggregation or calculations.Designing the movement of data through the enterprise, including sources of data and how the data is moved around in order to be improved.Defining integrative views of data to draw together data from across the enterprise. Some views will use a database of extracted data and others will bring together data in near real time, considering data currency, availability, response times and data volumes. Designing canonical data views to limit technical debt as data flows from point-to-point transformation.Defining technical standards and guidelines. Assess and document when and how to use the architected producers and consumers, the technologies to be used for various purposes, and models of selected entities, objects and processes. The guidelines should encourage reuse of existing data stores, as well as address issues of security, timeliness and quality.Investigate and participate in emerging technologies and new release Proofs of Concept (PoCs).Leveraging existing [core] data assets.Managing related metadata to include business descriptions of the data, details of any calculations or summaries, descriptions of the sources of the data, and indications of data quality and currency.Communicating the data architecture across the enterprise.Ensuring a focus on data quality by working effectively with data stewards so they can understand data semantics and identify opportunities for improving data quality.', 'Develop and implement data models to guide business decisions', 'Since we are early in our journey and we will be soon dealing with a lot of data, this person will set the stage for future work of data scientists and data engineers.\xa0', '3-4 years experience in a similar role.', 'Must have AWS or Azure experience. (Snowflake, Databricks, S3 desirable)', 'Contribute to the development of the ML capabilities for the Nomi Health', 'Documenting interfaces and data movement by recording how mapped data is moved around the virtual enterprise. This includes the frequency of movement, the source and destination of each step, how the data is transformed as it moves, and any aggregation or calculations.', 'Communicating the data architecture across the enterprise.', 'Must have coding experience (python, JAVA, R),\xa0', 'Familiarity of system concepts and tools within an enterprise architecture framework.', 'As a Data Science Engineer you will provide technical and domain subject knowledge to the company and future customers. You should be able to know how to examine new data systems requirements and implement migration models. You will also spend a good deal of time problem solving, analyzing architecture and assessing architect models, reviewing data migrations, selecting platforms and on-boarding of data management solutions that meet the technical and operational needs of the business.\xa0You must be hands-on with tools and code.', ""Bachelor's degree in Computer Science, Computer Engineering or relevant field.3-4 years experience in a similar role.Must have MongoDB experience.Must have AWS or Azure experience. (Snowflake, Databricks, S3 desirable)Must have ELT/ETL experience (Talend, MDM, other ETL/ELT tools)Must have coding experience (python, JAVA, R),\xa0Familiarity of system concepts and tools within an enterprise architecture framework.Knowledge of various modern data formats, tools, and methodologies. (Infomatics desirable)Excellent organizational and analytical abilities.Outstanding problem solver.Good written and verbal communication skills."", 'Excellent organizational and analytical abilities.', 'Defining integrative views of data to draw together data from across the enterprise. Some views will use a database of extracted data and others will bring together data in near real time, considering data currency, availability, response times and data volumes. Designing canonical data views to limit technical debt as data flows from point-to-point transformation.', 'Investigate and participate in emerging technologies and new release Proofs of Concept (PoCs).', 'Ensuring a focus on data quality by working effectively with data stewards so they can understand data semantics and identify opportunities for improving data quality.', 'Health, Dental, Vision, 401k with match, Commuter benefits with Great Pay, plus Equity.', 'Defining technical standards and guidelines. Assess and document when and how to use the architected producers and consumers, the technologies to be used for various purposes, and models of selected entities, objects and processes. The guidelines should encourage reuse of existing data stores, as well as address issues of security, timeliness and quality.', 'We are looking for a Data Science Engineer who can help pave the way for more work in the data field and start by analyzing our current needs and use data to generate value with Nomi Health’s end goal in mind. From this goal, you will design the architecture and the analytics pipelines while taking into account appropriate time frames, and costs.', 'Managing related metadata to include business descriptions of the data, details of any calculations or summaries, descriptions of the sources of the data, and indications of data quality and currency.', 'Data Science Engineer requirements are:', 'Leveraging existing [core] data assets.', 'Knowledge of various modern data formats, tools, and methodologies. (Infomatics desirable)', 'OK, HOW ABOUT A FEW SPECIFIC RESPONSIBILITIES?', 'Mapping data sources, including descriptions of the business meaning of the data, its uses, its quality, the applications that maintain it and the database technology in which it is stored. Documentation of a data source must describe the semantics of the data so that the occasional subtle differences in meaning are understood.', '\ufeffNomi Health is offering a highly competitive compensation package with an attractive base salary as well as a significant equity stake into the company at an early stage.', 'Nomi Health is the modern payment system for employee healthcare. We sidestep the middlemen, and connect employers providers and families directly at scale to cut healthcare costs by 30% across America. We will do this by eliminating high cost “out of network” charges, making the process all digital with easy access to real data and understandable actions.\xa0', ""Bachelor's degree in Computer Science, Computer Engineering or relevant field."", 'Benefits', 'Requirements', 'Designing the movement of data through the enterprise, including sources of data and how the data is moved around in order to be improved.', 'WHAT IS A DAY IN THE LIFE?', 'WHY IS THIS ROLE CRITICAL?', 'WHO ARE WE?\xa0']",Mid-Senior level,Full-time,Engineering,Computer Software,2021-02-05 10:31:41
Data Engineer,N/A,"Bellevue, WA",11 hours ago,Be among the first 25 applicants,"['', 'Ensure development and testing follow industry standards', 'Be able to translate technical specifications into finished programs and systems and also have a thorough understanding of developing solutions to handle large volume data sets that are typical with BI solutions.', 'Conduct appropriate performance testing to ensure all solutions will meet SLAs and performance criteria.', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0401(k) Savings Plan- Matched 150% up to 6%. (Our 401k is in the top 1% of 401(k) plans offered in the US!)', 'Create Technical Design Specification documentation that clearly articulates the design and code being implemented.', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Personal Time Off (PTO) and Paid Holidays', 'Work on complex BI ecosystems and design / develop solutions based on SQL SSIS, Azure Databricks, SQL SSASDesign and develop PL / SQL procedures / ETL jobs with optimized processing timeCommunicate design, requirements, feature set, functionality, usability, and limitations of subsystem to team and/or development lead or manager.Participate in Business Requirements and Functional Requirements meetings, identify gaps in requirements and drive discussion around appropriate solutions.Design and code high quality database solutions within a fast-paced sprint release cycleProficiency in creating visualizations / reports / dashboards based on business requirementsManage errors gracefully. Document code and work completed.Conduct thorough unit testing of code and document the unit test cases.Conduct appropriate performance testing to ensure all solutions will meet SLAs and performance criteria.Provide support as needed throughout Test and User Acceptance Testing phases.Create Technical Design Specification documentation that clearly articulates the design and code being implemented.Provide client communication as appropriate to project.Be able to translate technical specifications into finished programs and systems and also have a thorough understanding of developing solutions to handle large volume data sets that are typical with BI solutions.Assist in architectural design reviews and ensure technology stack selection is relevantEnsure development and testing follow industry standards', 'Excellent coding and debugging skills.', 'Design and code high quality database solutions within a fast-paced sprint release cycle', '7+ of SQL Server development', 'Proficiency in creating visualizations / reports / dashboards based on business requirements', 'Location: Bellevue, WA', 'Conduct thorough unit testing of code and document the unit test cases.', 'Provide client communication as appropriate to project.', 'Please share your resume and work authorization status for US via email to me at harneet.sapra@sogeti.com', 'Strong communication skills in both written and spoken English.', 'Strong understanding of BI areas.\xa0Ability to work in large, complex development BI projects including the proactive identification of issues and coordination of resolutions.', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Medical and Dental Coverage ', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0100% Company-paid mobile phone plan', 'Manage errors gracefully. Document code and work completed.', 'Participate in Business Requirements and Functional Requirements meetings, identify gaps in requirements and drive discussion around appropriate solutions.', 'Excellent communication skills and ability to work under continual deadline constraints are necessary', 'Responsibilities:', '7+ of SQL Server development experience writing complex stored procedures, triggers, views.', 'Data Engineer', 'The benefits our employees enjoy:', 'Experience with Power BI', 'FTE w/ Benefits', 'Strong Analytical and troubleshooting skills', 'Work Experience:', 'Communicate design, requirements, feature set, functionality, usability, and limitations of subsystem to team and/or development lead or manager.', 'Able to work independently to implement a solution with minimal guidance.', '\xa0', 'Ability to communicate with Business and developers accordingly.', 'Work on complex BI ecosystems and design / develop solutions based on SQL SSIS, Azure Databricks, SQL SSAS', 'Must have strong experience with Azure – Azure Data Lake / Azure Data Factory.', '7+ of SQL Server development experience writing complex stored procedures, triggers, views.Strong understanding of BI areas.\xa0Ability to work in large, complex development BI projects including the proactive identification of issues and coordination of resolutions.Expertise in T-SQL, DW Concepts, Tabular Cube.Experience with Power BIMust have strong experience with Azure – Azure Data Lake / Azure Data Factory.\xa0Strong Analytical and troubleshooting skillsExcellent coding and debugging skills.Able to work independently to implement a solution with minimal guidance.Ability to communicate with Business and developers accordingly.Strong communication skills in both written and spoken English.Working knowledge on Cosmos / Big Data Platforms (Azure, Databricks) is recommended.Excellent communication skills and ability to work under continual deadline constraints are necessary', 'Education Requirements: Bachelor’s Degree', 'Must have strong experience with Azure – Azure Data Lake / Azure Data Factory.\xa0', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Competitive Base Salary', 'Design and develop PL / SQL procedures / ETL jobs with optimized processing time', 'Provide support as needed throughout Test and User Acceptance Testing phases.', 'Assist in architectural design reviews and ensure technology stack selection is relevant', 'Working knowledge on Cosmos / Big Data Platforms (Azure, Databricks) is recommended.', 'Expertise in T-SQL, DW Concepts, Tabular Cube.']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-02-05 10:31:41
Data Engineer,Daugherty Business Solutions,"Columbus, Ohio Metropolitan Area",,N/A,"['', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Interest in Python, R, sh/bash and JVM-based languages including Scala and Java.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Proven ability to pick up new languages and technologies quickly.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Knowledge of cloud and distributed systems principles, including load balancing, networks, scaling, and in-memory versus disk.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Life, disability and long-term care insurance.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Work with team members and functional leads to understand existing data requirements and validation rules to support moving existing data warehouse workloads into a distributed data platform.', 'When you are a Daugherty employee, your job doesn’t end when a contract is up. You stay on as an indispensable member of the team with career growth opportunities tailored to your interests and talents. We want you to be eager to take on a new challenge. We are always 100% honest about what to expect, because we don’t want Daugherty to be just another job; we want Daugherty to be your dream job.\xa0', 'As a Data Engineer you will have the opportunity to:', 'Due to COVID-19, most of our employees are working remotely. We’ve implemented a virtual hiring process and continue to interview candidates by phone or video and are onboarding new hires remotely. We value the safety of each member of our community because we know we’re all in this together.', 'Daugherty Business Solutions is an inclusive Equal Employment Opportunity employer that considers applicants without regard to gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Little to no travel.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Robust career development and training.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Understanding of DevOps and CI/CD toolset, such as Jenkins, GitLab CI, Buildbot, Drone and Bamboo.', 'We offer members of Team Daugherty:', 'Daugherty Business Solutions, a leading advisory services and technology consulting firm will be expanding its operations to open a new, world-class Software Development Center in Columbus, Ohio to support its rapid growth and to engage the region’s diverse talent pool and thriving business community.', 'We are looking for motivated people with:', 'Data & Analytics', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Revenue sharing and a 401(k) retirement savings plan.', 'Interested? Apply today to take your career to the next level!', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Interest working with AWS technologies such as Redshift, RDS, S3, EMR, ADP, Hive, Kinesis,', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Contribute to the creation and maintenance of optimal data pipeline architectures.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Collaborate and work closely with team to build data platforms.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Interest in Hadoop family languages including Pig and Hive.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Assemble large, complex data sets that meet functional/non-functional business requirements.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Some experience with programming Languages, such as Scala, Java, R and Python.', 'In addition to existing Software Development Centers across the organization; in Minneapolis, Atlanta, Dallas, Chicago, NY and the St. Louis Headquarters, the team in Columbus will strategically support Daugherty’s growth strategy and commitment to delivering high quality software fast and effectively for its Fortune 500 clients.\xa0', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Recommend ways to improve data reliability, efficiency and quality.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Employ a variety of languages and tools to marry systems together.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Exposure to stream-processing and messaging, such as Storm, Spark-Streaming, Kafka and MQ.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Create custom software components (e.g. specialized UDFs) and analytics applications.', '\xa0', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Familiarity with high performance data libraries including Spark, NumPy and TensorFlow.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Excellent health, dental and vision insurance.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Implement & automate high-performance algorithms, prototypes and predictive models.', 'From Ron Daugherty, founder and CEO of Daugherty Business Solutions:\xa0“I am excited to announce that Daugherty is expanding to Columbus, OH.\xa0This expansion is a direct result of the hard work of our consultants during this pandemic.\xa0Daugherty teammates continue to find ways to add even more value for our clients and to increase the demand for our consulting expertise.\xa0Columbus is a growing metro area with a strong base of talent, excellent universities, and interesting perspective clients.\xa0That’s why I’m proud to add Columbus to our existing development centers across the country.”', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience building data pipelines to connect analytics stacks, client data visualization tools and external data sources.', 'SNS/SQS and QuickSight.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Intermediate level of SQL programming and query performance tuning techniques for data integration and consumption using design for optimum performance against large data assets within an OLTP, OLAP and MPP architecture.', 'Daugherty is hiring experienced Data Engineers to join our Columbus-based team.\xa0The ideal employee is a problem solver with the ability to utilize insights, creativity and perspective to drive business success for our clients.', 'If you require accommodations or assistance to complete the online application process, please inform any recruiter you are working with (or send an email to careers@daugherty.com) and identify the type of accommodation or assistance you are requesting. Do not include any medical or health information in this email. The recruiting team will respond to your email promptly.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Maintain and manage Hadoop clusters in development and production environments.']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-02-05 10:31:41
Senior Data Engineer,Delta Air Lines,"Atlanta, GA",5 hours ago,Be among the first 25 applicants,"['', 'Bachelor of Science degree in Computer Science or equivalent experience ', 'Continuously improve quality, efficiency, and scalability of data pipelines', 'Embraces diverse people, thinking and stylesConsistently makes safety and security, of self and others, the priorityBachelor of Science degree in Computer Science or equivalent experience 7 years of post-degree professional experienceExperience in mentoring junior team members through code reviews and recommend adherence to best practicesDeep understanding of writing test cases to ensure data quality, reliability and high level of confidenceTrack record of advancing new technologies to improve data quality and reliabilityContinuously improve quality, efficiency, and scalability of data pipelinesExpert skills working with queries/applications, including performance tuning, utilizing indexes, and materialized views to improve query performanceIdentify necessary business rules for extracting data along with functional or technical risks related to data sources (e.g. data latency, frequency, etc.)Develop initial queries for profiling data, validating analysis, testing assumptions, driving data quality assessment specifications, and define a path to deploymentFamiliar with best practices for data ingestion and data design', '5+ years of Airline industry experience', 'Qualifications', 'Track record of advancing new technologies to improve data quality and reliability', '3+ years of experience working with database technologies and data development such as Python, PLSQL, etc.', '7 years of post-degree professional experience', 'Deep understanding of writing test cases to ensure data quality, reliability and high level of confidence', '5+ years of Airline industry experience4+ years development experience building and maintaining ETL pipelines3+ years of experience working with database technologies and data development such as Python, PLSQL, etc.', 'Embraces diverse people, thinking and styles', 'Familiar with best practices for data ingestion and data design', 'Develop initial queries for profiling data, validating analysis, testing assumptions, driving data quality assessment specifications, and define a path to deployment', 'Expert skills working with queries/applications, including performance tuning, utilizing indexes, and materialized views to improve query performance', 'Experience in mentoring junior team members through code reviews and recommend adherence to best practices', '4+ years development experience building and maintaining ETL pipelines', 'Consistently makes safety and security, of self and others, the priority', 'Identify necessary business rules for extracting data along with functional or technical risks related to data sources (e.g. data latency, frequency, etc.)']",Associate,Full-time,Information Technology,Information Technology and Services,2021-02-05 10:31:41
Data Engineer III,Expedia Group,"Chicago, IL",21 hours ago,Be among the first 25 applicants,"['', 'You have knowledge of cloud infrastructures automation tools such as CloudFormation and Docker.', 'Breakdown requirements, simplify architectures and collaborate with other business teams to deliver intelligent data solutions in line with defined objectives and results.', 'Implement the tools and processes to handle performance, scale, availability, accuracy, and monitoring of data crucial to internal and external partners ensuring that set SLAs are met.', 'Identify performance and data challenges, suggesting code and architecture improvements.', 'Seek opportunities to bring sophisticated analytical techniques and solutions to new data products.', 'You know about domain and business event patterns, streaming pipelines such as Kafka or Kinesis.', 'Experience producing tested, secure, resilient and well-documented applications.', 'You are fully hands-on coding with solid experience in SQL and in one or more key software languages (e.g. Python, Scala, Java, Javascript).', 'Participate in a DevOps environment (you build it, you run it) by managing and operating real-time and batch data solutions at scale.', 'Your Responsibilities', 'You deeply understand of SQL, all its facets, and have experience working with traditional (e.g. Teradata) and cloud-based data lakes (e.g. Redshift, Snowflake).', 'Cooperate in an agile environment to lead change and keep us current with the latest technologies.', 'You have excellent interpersonal skills and verbal and written communication skills when working with both business and technical teams.', 'You have knowledge of dimensional modeling concepts and other efficient data representations for optimizing SQL queries.', 'You have experience with the AWS cloud ecosystem, such as EMR, Lambda, S3, EC2, Cloud Formation, VPC, etc.', 'Proven background in a variety of data technologies, such as Teradata, Hadoop, Spark, HBase, Hive, Presto and/or ETL frameworks. Experience with Dremio a plus.', 'Your Background', 'Develop, build scalable and high-performant data enrichment processes, using the right development patterns and continuous deployment/integration practices.', 'You develop sophisticated SQL queries while breaking down complexity and identifying modular parts.', 'Develop, build scalable and high-performant data enrichment processes, using the right development patterns and continuous deployment/integration practices.Collaborate with upstream system architects, data technical specialists, developers, data product managers, and end-users to support and resolve data issues.Participate in a DevOps environment (you build it, you run it) by managing and operating real-time and batch data solutions at scale.Implement the tools and processes to handle performance, scale, availability, accuracy, and monitoring of data crucial to internal and external partners ensuring that set SLAs are met.Breakdown requirements, simplify architectures and collaborate with other business teams to deliver intelligent data solutions in line with defined objectives and results.Identify performance and data challenges, suggesting code and architecture improvements.Seek opportunities to bring sophisticated analytical techniques and solutions to new data products.Cooperate in an agile environment to lead change and keep us current with the latest technologies.', 'Collaborate with upstream system architects, data technical specialists, developers, data product managers, and end-users to support and resolve data issues.', 'About The Position', 'You know and use the best fit data technologies and approaches to addressing performance, scalability, governance challenges.', 'You are fully hands-on coding with solid experience in SQL and in one or more key software languages (e.g. Python, Scala, Java, Javascript).You deeply understand of SQL, all its facets, and have experience working with traditional (e.g. Teradata) and cloud-based data lakes (e.g. Redshift, Snowflake).You know and use the best fit data technologies and approaches to addressing performance, scalability, governance challenges.You have experience with the AWS cloud ecosystem, such as EMR, Lambda, S3, EC2, Cloud Formation, VPC, etc.You have knowledge of dimensional modeling concepts and other efficient data representations for optimizing SQL queries.You develop sophisticated SQL queries while breaking down complexity and identifying modular parts.Proven background in a variety of data technologies, such as Teradata, Hadoop, Spark, HBase, Hive, Presto and/or ETL frameworks. Experience with Dremio a plus.You have knowledge of cloud infrastructures automation tools such as CloudFormation and Docker.You know about domain and business event patterns, streaming pipelines such as Kafka or Kinesis.Experience producing tested, secure, resilient and well-documented applications.You have excellent interpersonal skills and verbal and written communication skills when working with both business and technical teams.']",Associate,Full-time,Information Technology,Computer Software,2021-02-05 10:31:41
Data Integration Engineer,Ford Motor Company,"Dearborn, MI",13 hours ago,Be among the first 25 applicants,"['', ""Bachelor's degree in Engineering, Computer Science, Management Information Systems, or AnalyticsMinimum of 1 year of experience delivering products or managing quality in a technical functionMinimum of 1 year of experience performing Data Analytics or Data Engineering to create data products/ visualizations by writing codes/queries/scriptsMinimum of 1 year of experience transforming, visualizing data and scheduling workflows with Hive, PySpark, Apache Pig, Hadoop, SQL, Tableau/QlikView, Oozie, and ETL tools like Alteryx"", 'Minimum of 1 year of experience performing Data Analytics or Data Engineering to create data products/ visualizations by writing codes/queries/scripts', 'Experience building web based analytical dashboards', 'Work with partner organizations to develop and manage metrics for Enterprise Connectivity.', ""Drive Ford's data strategy and align it with a continuously changing business landscape"", 'Build scalable and robust data workflows and visualization dashboards using data from various databases, application and files.', 'Experience working with big data tools to visualize data level KPIs via dashboards', ""Drive Ford's data strategy and align it with a continuously changing business landscapeWork with partner organizations to develop and manage metrics for Enterprise Connectivity.Collaborate with data analytics stakeholders to streamline the data acquisition, processing, and presentation process.Perform data mapping, data lineage activities and document information flowsProvide analyses of connected vehicle data to support decisions on new product developments and production vehicle improvementsPerform descriptive/exploratory/trend data analysis to derive KPI insights using tools like Hive, Apache Pig, Hadoop, HDFS, MapReduce, Spark, PySpark, SQL and Alteryx.Use connected data trends and special studies to understand customer and vehicle behavior.Provide visibility to data quality/vehicle/feature issues and work with the business owners to fix the issuesImplement an enterprise data governance model and actively promote the concept of data - protection, sharing, reuse, quality and standardsBuild scalable and robust data workflows and visualization dashboards using data from various databases, application and files."", 'Ability to manage deliverables according to a robust project plan. ', 'Experience with vehicle connectivity and vehicle launches.', 'Perform data mapping, data lineage activities and document information flows', 'Minimum of 1 year of experience transforming, visualizing data and scheduling workflows with Hive, PySpark, Apache Pig, Hadoop, SQL, Tableau/QlikView, Oozie, and ETL tools like Alteryx', 'Collaborate with data analytics stakeholders to streamline the data acquisition, processing, and presentation process.', 'Experience working in a complex business environment, including at least 3 years in a single function, with deep understanding of the information constructs of that business. ', ""Bachelor's degree in Engineering, Computer Science, Management Information Systems, or Analytics"", 'Perform descriptive/exploratory/trend data analysis to derive KPI insights using tools like Hive, Apache Pig, Hadoop, HDFS, MapReduce, Spark, PySpark, SQL and Alteryx.', 'Proven analytics capability to robustly examine large data sets and highlight patterns, anomalies, relationships and trends. ', 'About Us', ""Master's degree in Engineering/CS/MIS/Analytics or related fieldExperience with vehicle connectivity and vehicle launches.Prior experience within PD, Powertrain, Chassis and/or Mobility GroupsExperience working with big data tools to visualize data level KPIs via dashboardsExperience leading the delivery of projects using Hadoop and Big Data technology like HDFS, Hive, Python, PySpark, Scala, Oozie, Tableau, QlikView, Alteryx, Informatica etc.Experience building web based analytical dashboardsExperience working in a complex business environment, including at least 3 years in a single function, with deep understanding of the information constructs of that business. Demonstrated expertise in conceptual thinking of how to apply information solutions to a business challenge. Demonstrated problem solving abilities. Proven analytics capability to robustly examine large data sets and highlight patterns, anomalies, relationships and trends. Excellent data and statistical analysis skills in Excel. Ability to manage deliverables according to a robust project plan. Self-starter, demonstrating the ability to work independently but remain a team playerAn interest in automotive electronics and how customers interact with our products."", 'Experience leading the delivery of projects using Hadoop and Big Data technology like HDFS, Hive, Python, PySpark, Scala, Oozie, Tableau, QlikView, Alteryx, Informatica etc.', 'Implement an enterprise data governance model and actively promote the concept of data - protection, sharing, reuse, quality and standards', 'Minimum of 1 year of experience delivering products or managing quality in a technical function', 'Demonstrated expertise in conceptual thinking of how to apply information solutions to a business challenge. Demonstrated problem solving abilities. ', ""What You'll Receive In Return"", ""Master's degree in Engineering/CS/MIS/Analytics or related field"", 'Prior experience within PD, Powertrain, Chassis and/or Mobility Groups', 'Our Preferred Qualifications', 'Provide analyses of connected vehicle data to support decisions on new product developments and production vehicle improvements', 'An interest in automotive electronics and how customers interact with our products.', 'Provide visibility to data quality/vehicle/feature issues and work with the business owners to fix the issues', 'Requirements', 'Self-starter, demonstrating the ability to work independently but remain a team player', 'Excellent data and statistical analysis skills in Excel. ', ""What You'll Be Able To Do"", 'Use connected data trends and special studies to understand customer and vehicle behavior.']",Entry level,Full-time,Information Technology,Automotive,2021-02-05 10:31:41
Cloud Data Engineer,Applied Systems,"Remote, OR",2 hours ago,Be among the first 25 applicants,"['', 'WHO WE ARE', ' CLOUD SOLUTIONS & PROFESSIONAL SERVICES - We offer cloud solutions, 24x7 technical support, consulting, implementation, and education services. ', ' AWARD WINNING TECHNOLOGY - We have been voted 2020 Company of the Year (Stevie Award)  2020 New Product or Service of the Year- 2 awards (Stevie Award)  2019 Best Cloud-Based Software Solutions Provider in the insurance industry (2019 Corporate Excellence Awards)  2019 Digital Service Provider of the Year (Business Excellence Awards)  2019 Best Broker Software Management House (Insurance Times) ', ' Develop cloud data pipelines to transform and process data between systems ', ' 2019 Best Cloud-Based Software Solutions Provider in the insurance industry (2019 Corporate Excellence Awards) ', ' GOOGLE’S INVESTMENT IN APPLIED - Google/CapitalG made a minority investment in Applied that will spur AI, machine learning, and digital marketing innovation in the global insurance industry. ', ' Drive efficiency gains through improved reliability and stakeholder adoption of self-serve tools ', ' 2019 Digital Service Provider of the Year (Business Excellence Awards) ', ' WORKLIFE BALANCE - There is more to life than work: that is why Applied offers benefits to help balance your work and home life. We offer competitive paid vacation time, personal/sick time, paid holidays, summer hours, paid parental leave, volunteer time off, and a free day off for your birthday! ', ' JOIN A GREAT TEAM - We believe that success comes from a dynamic working environment that offers professionals an opportunity to grow and succeed alongside extraordinary people. We encourage idea sharing, problem solving, and teamwork in our environment.  DIVERSITY MATTERS- We strive to create a positive workplace culture for those of different thinking, backgrounds, experiences, expertise, and individual qualities across our organization. We want the best and the brightest to be a part of a growing culture that embraces a sense of belonging.  RELAXED DRESS CODE - Applied allows for a relaxed dress code where jeans are permitted; we call this “Dress for your Day”.  FUN PARTIES & PERKS - Fun perks are a staple at Applied, including holiday parties with games and contests, summer celebrations employee appreciation events, art contests, employee discount programs, and more!  OPPORTUNITIES FOR ADVANCEMENT - We are a growing company that offers career opportunities, and not just “another job”. Applied believes in growing our employees and promoting from within, offering many opportunities for professional advancement along the way!  CAREER STABILITY & LONGEVITY - Our average employee tenure is 9 years.  CULTURE OF RECOGNITION - Applied provides a culture of employee recognition with our Circle of Excellence program, and our internal social network recognition program.  APPLIED CARES - We have a culture that embraces and promotes volunteerism. Applied encourages our employees to help local charities and communities through the ‘Applied Cares’ program ', ' Design, build and work with dispersed engineering teams and business users to implement data pipelines into our centralized data platform ', 'QUALIFICATIONS FOR THIS JOB', ' FUN PARTIES & PERKS - Fun perks are a staple at Applied, including holiday parties with games and contests, summer celebrations employee appreciation events, art contests, employee discount programs, and more! ', 'RESPONSIBILITIES', ' 2020 Company of the Year (Stevie Award)  2020 New Product or Service of the Year- 2 awards (Stevie Award)  2019 Best Cloud-Based Software Solutions Provider in the insurance industry (2019 Corporate Excellence Awards)  2019 Digital Service Provider of the Year (Business Excellence Awards)  2019 Best Broker Software Management House (Insurance Times) ', ' Experience with Google Cloud Platform, BigQuery, Google DataFlow, Apache Spark, Airflow, Kafka, Scala, and Python preferred ', ' Take problems from inception to completion - own the building, testing, deployment, and maintenance of the code that you work on ', ' TO LEARN MORE ', ' Cloud Data Engineer ', ' Provide technical expertise to the Data Engineering team in all phases of work including analysis, design, and development of architecture ', ' Candidates with experience with AWS RedShift, Kinesis, and Data Pipeline will also be considered ', ' Experience with Google Cloud Platform, BigQuery, Google DataFlow, Apache Spark, Airflow, Kafka, Scala, and Python preferred  Candidates with experience with AWS RedShift, Kinesis, and Data Pipeline will also be considered ', ' FINANCIAL PEACE OF MIND - In addition to wellness benefits, Applied offers traditional and Roth 401k options, with employer match. Accidental Death & Dismemberment, Short and Long Term Disability, and Business Travel Accident insurance are also offered. ', ' OPPORTUNITIES FOR ADVANCEMENT - We are a growing company that offers career opportunities, and not just “another job”. Applied believes in growing our employees and promoting from within, offering many opportunities for professional advancement along the way! ', ' Please visit AppliedSystems.com ', ' 3+ years of development experience building large scale data solutions Experience with Google Cloud Platform, BigQuery, Google DataFlow, Apache Spark, Airflow, Kafka, Scala, and Python preferred  Candidates with experience with AWS RedShift, Kinesis, and Data Pipeline will also be considered ', ' CULTURE OF RECOGNITION - Applied provides a culture of employee recognition with our Circle of Excellence program, and our internal social network recognition program. ', ' 2019 Best Broker Software Management House (Insurance Times) ', ' APPLIED CARES - We have a culture that embraces and promotes volunteerism. Applied encourages our employees to help local charities and communities through the ‘Applied Cares’ program ', ' Identify gaps and weaknesses in our data stack and continues to drive learning advancements for the team ', 'Job Description', ' Drive innovation within Data Engineering by playing a key role in technology decisions for the future of our data science, analysis, and reporting needs  Work with business partners and software engineers to gather, understand, and bridge definitions and requirements  Design and develop highly complex and critical data projects with strict timelines  Drive efficiency gains through improved reliability and stakeholder adoption of self-serve tools  Leverage research and previous experience to ensure we’re up to date and continuously exploring  Identify gaps and weaknesses in our data stack and continues to drive learning advancements for the team  Provide technical expertise to the Data Engineering team in all phases of work including analysis, design, and development of architecture  Design, build and work with dispersed engineering teams and business users to implement data pipelines into our centralized data platform  Developing in Python leveraging a wide range of technologies, notably: GCP, BigQuery, Google DataFlow, Kubernetes, and Docker  Develop cloud data pipelines to transform and process data between systems ', ' BENEFITS FROM DAY ONE - Applied offers Medical, Rx, Dental, Vision, Virtual Doctors’ Appointments, Health Savings Account, Flexible Spending Accounts, Critical Illness, Group Accident, and Wellness Incentives to ensure employees are covered from day one.  FINANCIAL PEACE OF MIND - In addition to wellness benefits, Applied offers traditional and Roth 401k options, with employer match. Accidental Death & Dismemberment, Short and Long Term Disability, and Business Travel Accident insurance are also offered.  WORKLIFE BALANCE - There is more to life than work: that is why Applied offers benefits to help balance your work and home life. We offer competitive paid vacation time, personal/sick time, paid holidays, summer hours, paid parental leave, volunteer time off, and a free day off for your birthday! ', 'Options', ' 2020 New Product or Service of the Year- 2 awards (Stevie Award) ', ' Experience with reporting schema designs including data modeling, denormalization, data warehousing, and data lakes ', ' RELAXED DRESS CODE - Applied allows for a relaxed dress code where jeans are permitted; we call this “Dress for your Day”. ', ' Proven ability to work closely with business and product teams to ensure data solutions are aligned with business initiatives and are of high quality ', ' LEADING GLOBAL PROVIDER OF CLOUD-BASED INSURANCE SOFTWARE - Applied Systems develops the top two Insurance Agency/Broker Management software products in the world. In addition, we also provide innovative mobile apps, Data Analytics, Customer Self-Service, Insurer Connectivity & Rating, eServicing, Benefits Design, and CRM software products. By automating the insurance lifecycle, Applied’s people and products enable millions of people around the world to safeguard and protect what matters most. ', ' Developing in Python leveraging a wide range of technologies, notably: GCP, BigQuery, Google DataFlow, Kubernetes, and Docker ', ' JOIN A GREAT TEAM - We believe that success comes from a dynamic working environment that offers professionals an opportunity to grow and succeed alongside extraordinary people. We encourage idea sharing, problem solving, and teamwork in our environment. ', ' 3+ years of development experience building large scale data solutions Experience with Google Cloud Platform, BigQuery, Google DataFlow, Apache Spark, Airflow, Kafka, Scala, and Python preferred  Candidates with experience with AWS RedShift, Kinesis, and Data Pipeline will also be considered  Proven ability to work closely with business and product teams to ensure data solutions are aligned with business initiatives and are of high quality  Ability to communicate technical hurdles and challenges clearly and succinctly  Take problems from inception to completion - own the building, testing, deployment, and maintenance of the code that you work on  Experience with reporting schema designs including data modeling, denormalization, data warehousing, and data lakes ', ' Work with business partners and software engineers to gather, understand, and bridge definitions and requirements ', ' Design and develop highly complex and critical data projects with strict timelines ', ' Leverage research and previous experience to ensure we’re up to date and continuously exploring ', ' EMPLOYEES - Applied currently has 1,800+ employees across the US, Canada, the UK, and Ireland. ', ' CAREER STABILITY & LONGEVITY - Our average employee tenure is 9 years. ', ' CLIENTS - We provide technology to over 160k users within insurance agencies, brokerages, and carriers throughout the US, Canada, the UK, and Ireland. ', ' 2020 Company of the Year (Stevie Award) ', ' BENEFITS FROM DAY ONE - Applied offers Medical, Rx, Dental, Vision, Virtual Doctors’ Appointments, Health Savings Account, Flexible Spending Accounts, Critical Illness, Group Accident, and Wellness Incentives to ensure employees are covered from day one. ', 'COMPANY CULTURE & PERKS', ' DIVERSITY MATTERS- We strive to create a positive workplace culture for those of different thinking, backgrounds, experiences, expertise, and individual qualities across our organization. We want the best and the brightest to be a part of a growing culture that embraces a sense of belonging. ', 'BENEFITS & REWARDS', ' Drive innovation within Data Engineering by playing a key role in technology decisions for the future of our data science, analysis, and reporting needs ', ' Ability to communicate technical hurdles and challenges clearly and succinctly ', ' LEADING GLOBAL PROVIDER OF CLOUD-BASED INSURANCE SOFTWARE - Applied Systems develops the top two Insurance Agency/Broker Management software products in the world. In addition, we also provide innovative mobile apps, Data Analytics, Customer Self-Service, Insurer Connectivity & Rating, eServicing, Benefits Design, and CRM software products. By automating the insurance lifecycle, Applied’s people and products enable millions of people around the world to safeguard and protect what matters most.  CLOUD SOLUTIONS & PROFESSIONAL SERVICES - We offer cloud solutions, 24x7 technical support, consulting, implementation, and education services.  AWARD WINNING TECHNOLOGY - We have been voted 2020 Company of the Year (Stevie Award)  2020 New Product or Service of the Year- 2 awards (Stevie Award)  2019 Best Cloud-Based Software Solutions Provider in the insurance industry (2019 Corporate Excellence Awards)  2019 Digital Service Provider of the Year (Business Excellence Awards)  2019 Best Broker Software Management House (Insurance Times)  GOOGLE’S INVESTMENT IN APPLIED - Google/CapitalG made a minority investment in Applied that will spur AI, machine learning, and digital marketing innovation in the global insurance industry.  CLIENTS - We provide technology to over 160k users within insurance agencies, brokerages, and carriers throughout the US, Canada, the UK, and Ireland.  EMPLOYEES - Applied currently has 1,800+ employees across the US, Canada, the UK, and Ireland. ']",Mid-Senior level,Full-time,Information Technology,Computer Software,2021-02-05 10:31:41
Data Engineer,Flexton Inc.,"Sunnyvale, CA",10 hours ago,Be among the first 25 applicants,"['', ' Experience in engineering large-scale distributed systems in a production environment', ' Experience with an OO programming language like Java a bonus', '4+ years of relevant work experience in data Ability to write, analyze, and debug SQL queries Working experience with Hadoop projects/infrastructure Experience in the Big Data space (Hadoop Stack like Spark, HDFS, Pig, Hive, etc.) Experience with Data Warehouse design, ETL (Extract, Transform, Load), architecting efficient software solutions Knowledge of data modeling for both data warehousing and Big Data Experience working extensively in multi-petabyte data environment Experience in engineering large-scale distributed systems in a production environment Experience with at least one scripting language (Shell, Python, Perl etc.) a bonus Experience with an OO programming language like Java a bonus', ' Working experience with Hadoop projects/infrastructure', '4+ years of relevant work experience in data', 'Data Engineer', 'Sunnyvale, CA (Remote)', 'Contract- 1 year', ' Ability to write, analyze, and debug SQL queries', ' Knowledge of data modeling for both data warehousing and Big Data', ' Experience working extensively in multi-petabyte data environment', ' Experience with at least one scripting language (Shell, Python, Perl etc.) a bonus', ' Experience in the Big Data space (Hadoop Stack like Spark, HDFS, Pig, Hive, etc.)', ' Experience with Data Warehouse design, ETL (Extract, Transform, Load), architecting efficient software solutions']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-02-05 10:31:41
Data Engineer,"IDR, Inc.","Sandy Springs, GA",2 hours ago,Be among the first 25 applicants,"['', 'What’s in it for you?', 'Support and troubleshoot the data environment', 'Enjoy extremely competitive compensation and benefits package', '5-8+ years of Data Engineer experience', '5+ years of hands-on design and development experience in data space: data processing/data transformation using ETL tools, data warehouse (data modeling/programming), RDBMS', 'Knowledgeable in Agile practices and methodologies', 'IDR’s largest client is seeking a Data Engineer to join its team in Sandy Springs, GA!', '4+ years’ data engineering experience focusing on batch and real time data pipelines using Spark, PySpark, Python, SQL, or Java', 'Experienced in Microsoft technologies such as SSIS, SQL Server, SSRS', 'Develop enhanced data processing components, such as data ingest, data transformation, data store, data management, data quality', 'Bachelor’s in Information Systems, Finance/Mathematics, Computer Science, or similar field5-8+ years of Data Engineer experience4+ years’ data engineering experience focusing on batch and real time data pipelines using Spark, PySpark, Python, SQL, or Java5+ years of hands-on design and development experience in data space: data processing/data transformation using ETL tools, data warehouse (data modeling/programming), RDBMSExperienced in Microsoft technologies such as SSIS, SQL Server, SSRSExperience with a DevOps model using a CI/CD toolKnowledgeable in Agile practices and methodologies', 'Build and manage data pipelines for Azure cloud-based data platformBuild and maintain high available data pipelines and processing frameworks that handle the business’s growing databaseDesign, develop, and maintain data models, database objects, stored procedures, and viewsDevelop enhanced data processing components, such as data ingest, data transformation, data store, data management, data qualitySupport and troubleshoot the data environment', 'Design, develop, and maintain data models, database objects, stored procedures, and views', 'Bachelor’s in Information Systems, Finance/Mathematics, Computer Science, or similar field', 'Experience with a DevOps model using a CI/CD tool', 'Required skills/experience of the Data Engineer include:', 'Data Engineer', 'Build and maintain high available data pipelines and processing frameworks that handle the business’s growing database', 'Join an extremely secure organization that offers job stability', 'Join a flexible friendly laid-back environment', 'Responsibilities of the Data Engineer include:', 'Build and manage data pipelines for Azure cloud-based data platform', 'Enjoy true work/life balance', 'Join a flexible friendly laid-back environmentEnjoy extremely competitive compensation and benefits packageEnjoy true work/life balanceJoin an extremely secure organization that offers job stability']",Mid-Senior level,Full-time,Information Technology,Staffing and Recruiting,2021-02-05 10:31:41
Data Engineer,William Hill,"Jersey City, NJ",8 hours ago,Be among the first 25 applicants,"['', 'You’re an expert in development and data quality', ""You're experienced in working within an integration environment with testers to ensure end to end performance and resilience SLA’s can be achieved."", 'Assist in building a world-class Big Data platform which will give us power to process streams of data , as well as, enable machine learning and advanced analytics capabilities. Everything cloud-based for scalability and speed to market.', 'Regular attendance in the office', 'It’s essential that you’ve got experience of the full SDLC in an equivalent environment. ', 'Experience with supporting Data Scientist (Machine Learning) is a plus', 'Highly experienced in writing well designed, testable, efficient code which follows good coding standards', 'You have experience mentoring other engineers ', 'Keep NFRs as priority by maintaining code, supporting, restoring, monitoring and performance for any delivery. ', 'Essential Functions/Exposures', 'Handle large volumes of data and integrate our platform with a range of internal and external systems.', ' Assist in building a world-class Big Data platform which will give us power to process streams of data , as well as, enable machine learning and advanced analytics capabilities. Everything cloud-based for scalability and speed to market. Handle large volumes of data and integrate our platform with a range of internal and external systems. Understand new tech and how it can be applied to data management. Work with an agile team alongside business, testers, architects and project managers. Focus on the development of complex logic integrations Maintain and evaluate quality of documentation, code, and business logic and non-functional. Keep NFRs as priority by maintaining code, supporting, restoring, monitoring and performance for any delivery.  ', 'Focus on the development of complex logic integrations', "" Highly experienced in writing well designed, testable, efficient code which follows good coding standards You’re an expert in development and data quality You’re a strong developer, and you have expertise in SQL to handle large data sets and complex data transformations.  You have experience with Python It’s essential that you’ve got experience of the full SDLC in an equivalent environment.  You have worked in small focused scrum teams delivering events driven integrations across multiple teams. You're experienced in working within an integration environment with testers to ensure end to end performance and resilience SLA’s can be achieved. Agile mindset and practice in software development process e.g. Scrum, Kanban, TDD, BDD Experience with Big data platforms: Apache Spark / Hadoop and Scala is a plus Experience with cloud solutions for Big Data (Snowflake, GCP BigQuery, AWS Redshift) Experience with data pipelines (e.g. Airflow) and streaming processing (Kafka, Kinesis, Spark Streaming, Flink) is a plus Experience with supporting Data Scientist (Machine Learning) is a plus You have experience mentoring other engineers  "", 'Experience with data pipelines (e.g. Airflow) and streaming processing (Kafka, Kinesis, Spark Streaming, Flink) is a plus', 'Must be able to sit for extended periods of time ', 'You have experience with Python', ' Must be able to sit for extended periods of time  Must be able to type and talk on the phone for extended periods of time  Regular attendance in the office ', 'You’re a strong developer, and you have expertise in SQL to handle large data sets and complex data transformations. ', 'If you join us as a Senior, you will be a mentor to a team, and will need to bring some previous experience of this.', 'Must be able to type and talk on the phone for extended periods of time ', 'Agile mindset and practice in software development process e.g. Scrum, Kanban, TDD, BDD', 'Work with an agile team alongside business, testers, architects and project managers.', 'Maintain and evaluate quality of documentation, code, and business logic and non-functional.', 'When we say cutting edge, we mean it. Here, you can work on highly reliable systems with low latency, much like the transactional systems of the best financial institutions, but…with the fun included. ', 'What You Will Do', 'Experience with cloud solutions for Big Data (Snowflake, GCP BigQuery, AWS Redshift)', 'What You Need', 'You have worked in small focused scrum teams delivering events driven integrations across multiple teams.', 'Experience with Big data platforms: Apache Spark / Hadoop and Scala is a plus', 'Understand new tech and how it can be applied to data management.', 'You will have access to ']",Entry level,Full-time,Information Technology,Gambling & Casinos,2021-02-05 10:31:41
Data Engineer,Billtrust,"Lawrenceville, NJ",22 hours ago,Be among the first 25 applicants,"['', 'Recognition: From Billtrust Bucks and Gongings to Culture Champion and Presidents Awards, our employees are recognized for hard work and outcomes achieved.', 'Work with Infrastructure, security and operations/support teams to ensure processes run with high availability and in accordance with internal architectural standards. Establish performance SLAs and L1/L2/L3 support matrices.', 'Results driven, with strong analytical and problem-solving skills', 'Ensure proper technical environments are established and populated with data to facilitate development, testing, QA and production', 'Ability to quickly demonstrate validity of approaches through POCs', 'Opportunities for Growth: Professional development can take many shapes. From ERGs like Women in Tech and DE&I, to Mentor-Mentee, Leadership, and High-Potential Programs, we foster an environment where all employees can grow.', 'Oversee and validate deliverables from internal and external parties. Ensure high standards of technical work quality from all contributing parties.', '5-7 years of proven success coding, developing and deploying complex Finance and Data Warehousing solutions in a heavily transactional environment', ' Competitive Salary, Bonus, Stock Options and 401(k) Match: We appreciate our employees and we make sure they know it. Open PTO: Work-life balance is important. We believe in giving our employees time to truly relax and recharge. Paid Parental Leave: To keep our employees and their families healthy. Opportunities for Growth: Professional development can take many shapes. From ERGs like Women in Tech and DE&I, to Mentor-Mentee, Leadership, and High-Potential Programs, we foster an environment where all employees can grow. Recognition: From Billtrust Bucks and Gongings to Culture Champion and Presidents Awards, our employees are recognized for hard work and outcomes achieved. Minimal Bureaucracy: An entrepreneurial environment of ownership and accountability allows you to get work done. A Culture that Lives its Values: Our values are not just words or window dressing, they guide our decisions - big and small - each and every day. ', 'Experience with cloud ERP and integrating with HR systems is highly desirable', 'Perform timely evaluation of new projects initiated within and outside the domain', 'Find, qualify and technically rank multiple vendors for any outsourced work ', 'Design and deliver cost effective solutions aligned to the architecture roadmap, using input from key business and technology stakeholders. ', 'Works items are delivered on time and in accordance with the backlog co-developed with the business owners', 'Effective with time management and self-prioritization of tasks', 'Functionally decompose complex problems into simple, straight-forward, cost effective solutions', 'Minimal Bureaucracy: An entrepreneurial environment of ownership and accountability allows you to get work done.', 'Work with upstream and downstream system/platform owners to ensure interfaces and data models are optimized, and providing the necessary information for a complete solution', 'Experience with Waterfall/SDLC, Agile/Scrum methodologies', 'Experience in FinTech, Telecom, Software, Media or industry w/specific knowledge of financial transactions, high volume data/transactions and processing environments, security, testing, and reporting', 'Ability to work across large teams, prioritize, collaborate, make/meet commitments.', 'Ability to effectively work in a fast-paced, energetic team environment', ' 5-7 years of proven success coding, developing and deploying complex Finance and Data Warehousing solutions in a heavily transactional environment Experience with Waterfall/SDLC, Agile/Scrum methodologies PowerBI/PowerQuery, Python, Node JS, MySQL, MS-SQL/SSIS, Rest APIs, GIT, and other database technologies to create relevant solutions Experience building and enhancing a Cloud Data Warehouse Technologies (Front end and back end), ETL and on prem/hybrid environments Experience in FinTech, Telecom, Software, Media or industry w/specific knowledge of financial transactions, high volume data/transactions and processing environments, security, testing, and reporting Able to communicate effectively with technical and business personal Effective with time management and self-prioritization of tasks Ability to work across large teams, prioritize, collaborate, make/meet commitments. Results driven, with strong analytical and problem-solving skills Ability to effectively work in a fast-paced, energetic team environment Ability to quickly demonstrate validity of approaches through POCs Experience with cloud ERP and integrating with HR systems is highly desirable BS Degree in Software Engineering/Computer Science required, MS Degree desirable ', 'Proactively plan for new platform demands including but not limited to organic growth, new products, acquisitions of new companies, new product features and new billing users features. Ensure that new products/feature created by other teams are supported.', 'Able to communicate effectively with technical and business personal', 'Paid Parental Leave: To keep our employees and their families healthy.', 'Document all current and proposed solutions', 'Drive regular release of deliverables across the full system lifecycle from inception, design, development, deployment and support. Coordinate solutions with project managers and other teams performing technical work', 'Quickly grasp and implement complex user business rules with minimal oversight', 'A Culture that Lives its Values: Our values are not just words or window dressing, they guide our decisions - big and small - each and every day.', 'BS Degree in Software Engineering/Computer Science required, MS Degree desirable', 'Competitive Salary, Bonus, Stock Options and 401(k) Match: We appreciate our employees and we make sure they know it.', 'Open PTO: Work-life balance is important. We believe in giving our employees time to truly relax and recharge.', ' Design and deliver cost effective solutions aligned to the architecture roadmap, using input from key business and technology stakeholders.  Functionally decompose complex problems into simple, straight-forward, cost effective solutions Works items are delivered on time and in accordance with the backlog co-developed with the business owners Oversee and validate deliverables from internal and external parties. Ensure high standards of technical work quality from all contributing parties. Drive regular release of deliverables across the full system lifecycle from inception, design, development, deployment and support. Coordinate solutions with project managers and other teams performing technical work Work with upstream and downstream system/platform owners to ensure interfaces and data models are optimized, and providing the necessary information for a complete solution Work with Infrastructure, security and operations/support teams to ensure processes run with high availability and in accordance with internal architectural standards. Establish performance SLAs and L1/L2/L3 support matrices. Ensure proper technical environments are established and populated with data to facilitate development, testing, QA and production Ensure solution architecture, design, application and integration development follow best practices/principles and standards for enterprise architecture, source code management, environment management and testing. Find, qualify and technically rank multiple vendors for any outsourced work  Proactively plan for new platform demands including but not limited to organic growth, new products, acquisitions of new companies, new product features and new billing users features. Ensure that new products/feature created by other teams are supported. Perform timely evaluation of new projects initiated within and outside the domain Quickly grasp and implement complex user business rules with minimal oversight Document all current and proposed solutions ', 'PowerBI/PowerQuery, Python, Node JS, MySQL, MS-SQL/SSIS, Rest APIs, GIT, and other database technologies to create relevant solutions', 'Ensure solution architecture, design, application and integration development follow best practices/principles and standards for enterprise architecture, source code management, environment management and testing.', 'Experience building and enhancing a Cloud Data Warehouse Technologies (Front end and back end), ETL and on prem/hybrid environments']",Entry level,Full-time,Information Technology,Financial Services,2021-02-05 10:31:41
Data Engineer,Conexess Group,"St Louis, MO",2 hours ago,Be among the first 25 applicants,"['', 'Solid understanding of Agile Project Management methodologies', 'Experience with analytical tools working with central repository data sources', 'Experience with Linux a plus', 'Coding skills in languages such as SQL and Python', 'Understanding of data warehousing and ETL techniques', 'Demonstrated experience in handling large data sets and relational databases (SQL Server, Oracle, Teradata, MySQL, or Hadoop Hive).', 'Strong understanding of Software development lifecycle Excellent communication, documentation and presentation skills', ' Demonstrated experience in handling large data sets and relational databases (SQL Server, Oracle, Teradata, MySQL, or Hadoop Hive). Coding skills in languages such as SQL and Python Understanding of data warehousing and ETL techniques Experience with analytical tools working with central repository data sources Experience with Linux a plus Strong understanding of Software development lifecycle Excellent communication, documentation and presentation skills Solid understanding of Agile Project Management methodologies']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-02-05 10:31:41
Data Engineer,Alignment Healthcare,"Orange, CA",12 hours ago,Be among the first 25 applicants,"['', 'careers@ahcusa.com', 'Experience leading large-scale data warehousing and analytics projects, including using Azure or AWS technologies – SQL Server, Redshift, S3, EC2, Data-pipeline, Data Lake, Data Factory and other big data technologies', 'Position Title:', 'Healthcare domain and data experience', 'If you require any reasonable accommodation under the Americans with Disabilities Act (ADA) in completing the online application, interviewing, completing any pre-employment testing or otherwise participating in the employee selection process, please contact ', 'Interfacing with business customers, gathering requirements and developing new datasets in data platform', 'Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets', 'Servant leadership', 'Knowledge and experience of SQL Sever and SSIS.Excellent communication, analytical and collaborative problem-solving skills', 'Socially responsible', 'Linux/UNIX including to process large data sets.Experience with Azure, AWS or GCP is a plusMicrosoft Azure Certification is a plus', 'The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. ', '(May include but are not limited to) ', 'Other:Knowledge and experience of SQL Sever and SSIS.Excellent communication, analytical and collaborative problem-solving skills', 'Socially responsibleTechnologically enabledConcierge careTransformationServant leadership', 'Preferred:', 'Excellent communication, analytical and collaborative problem-solving skills', 'Minimum Experience:2+ years relevant experience in cloud based data engineering.Demonstrated ability in data modeling, ETL development, and data warehousing.Data Warehousing Experience with SQL Server, Oracle, Redshift, Teradata, etc.Experience with Big Data Technologies (NoSQL databases, Hadoop, Hive, Hbase, Pig, Spark, Elasticsearch etc.)Experience in using Python, .net, Java and/or other data engineering languages', 'Experience providing technical leadership and mentor other engineers for the best practices on the data engineering space', 'Experience with Azure, AWS or GCP is a plus', 'Experience with Big Data Technologies (NoSQL databases, Hadoop, Hive, Hbase, Pig, Spark, Elasticsearch etc.)', 'Industry experience as a Data Engineer or related specialty (e.g., Software Engineer, Business Intelligence Engineer, Data Scientist) with a track record of manipulating, processing, and extracting value from large datasets.', 'Healthcare domain and data experienceHealthcare EDI experience is a plus', 'Job Number:', 'EEO Employer Verbiage:', 'Designing, implementing and supporting a platform that can provide ad-hoc access to large datasets', 'Minimum Requirements', 'Alignment Healthcare, LLC is proud to practice Equal Employment Opportunity and Affirmative Action. We are looking for diversity in qualified candidates for employment: Minority/Female/Disable/Protected Veteran.', 'Minimum Experience:2+ years relevant experience in cloud based data engineering.Demonstrated ability in data modeling, ETL development, and data warehousing.Data Warehousing Experience with SQL Server, Oracle, Redshift, Teradata, etc.Experience with Big Data Technologies (NoSQL databases, Hadoop, Hive, Hbase, Pig, Spark, Elasticsearch etc.)Experience in using Python, .net, Java and/or other data engineering languagesEducation/Licensure:Bachelors or Masters in Computer Science, Engineering, Mathematics, Statistics, or related fieldOther:Knowledge and experience of SQL Sever and SSIS.Excellent communication, analytical and collaborative problem-solving skills', 'While performing the duties of this job, the employee is regularly required to talk or hear. The employee regularly is required to stand, walk, sit, use hand to finger, handle or feel objects, tools, or controls; and reach with hands and arms. The employee frequently lifts and/or moves up to 10 pounds. Specific vision abilities required by this job include close vision and the ability to adjust focus.', 'Location City:', 'Location State:', 'Identifying the data quality issues to address them immediately to provide great user experience', 'City:', 'General Duties/Responsibilities', 'Essential Physical Functions', 'Experience in using Python, .net, Java and/or other data engineering languages', 'Problem solving skills and Ability to meet deadlines are a must', 'Bachelors or Masters in Computer Science, Engineering, Mathematics, Statistics, or related field', 'Healthcare EDI experience is a plus', 'Modelling data and metadata to support machine learning and AI', '.', 'Data Engineer', 'Education/Licensure:Bachelors or Masters in Computer Science, Engineering, Mathematics, Statistics, or related field', 'Knowledge and experience of SQL Sever and SSIS.', 'Microsoft Azure Certification is a plus', 'API development experience is a plus', 'Technologically enabled', 'Linux/UNIX including to process large data sets.', 'Concierge care', 'Experience building data products incrementally and integrating and managing datasets from multiple sources', 'Position Summary', 'External Description', 'Demonstrable track record dealing well with ambiguity, prioritizing needs, and delivering results in an agile, dynamic startup environmentProblem solving skills and Ability to meet deadlines are a mustMicrosoft Azure Certification is a plus', 'Demonstrable track record dealing well with ambiguity, prioritizing needs, and delivering results in an agile, dynamic startup environment', 'Work Environment:', 'Demonstrated ability in data modeling, ETL development, and data warehousing.', '2+ years relevant experience in cloud based data engineering.Demonstrated ability in data modeling, ETL development, and data warehousing.Data Warehousing Experience with SQL Server, Oracle, Redshift, Teradata, etc.Experience with Big Data Technologies (NoSQL databases, Hadoop, Hive, Hbase, Pig, Spark, Elasticsearch etc.)Experience in using Python, .net, Java and/or other data engineering languages', '2+ years relevant experience in cloud based data engineering.', 'The employee frequently lifts and/or moves up to 10 pounds. Specific vision abilities required by this job include close vision and the ability to adjust focus.', 'Data Warehousing Experience with SQL Server, Oracle, Redshift, Teradata, etc.', 'Building and migrating the complex ETL pipelines from on premise system to cloud and Hadoop/Spark to make the system grow elastically', 'State:', 'Interfacing with business customers, gathering requirements and developing new datasets in data platformBuilding and migrating the complex ETL pipelines from on premise system to cloud and Hadoop/Spark to make the system grow elasticallyIdentifying the data quality issues to address them immediately to provide great user experienceExtracting and combining data from various heterogeneous data sourcesDesigning, implementing and supporting a platform that can provide ad-hoc access to large datasetsModelling data and metadata to support machine learning and AI', 'Community / Marketing Title:', 'Company Profile', 'Industry experience as a Data Engineer or related specialty (e.g., Software Engineer, Business Intelligence Engineer, Data Scientist) with a track record of manipulating, processing, and extracting value from large datasets.Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data setsExperience building data products incrementally and integrating and managing datasets from multiple sources', 'While performing the duties of this job, the employee is regularly required to talk or hear. The employee regularly is required to stand, walk, sit, use hand to finger, handle or feel objects, tools, or controls; and reach with hands and arms. ', 'Extracting and combining data from various heterogeneous data sources', 'Transformation', 'Experience leading large-scale data warehousing and analytics projects, including using Azure or AWS technologies – SQL Server, Redshift, S3, EC2, Data-pipeline, Data Lake, Data Factory and other big data technologiesExperience providing technical leadership and mentor other engineers for the best practices on the data engineering space']",Entry level,Full-time,Information Technology,Insurance,2021-02-05 10:31:41
Data Engineer,Scalable,"Dallas, TX",17 hours ago,Be among the first 25 applicants,"['', 'Proficiency in using BI dashboard tools.', 'Experience writing and executing complex SQL queries.', 'Regularly processes TBs of data quickly with cloud based distributed solutions.', ""Scalable\xa0is on an incredible growth trajectory and we're looking for passionate software engineers to help us build the future of ecommerce.\xa0Scalable builds world-class physical and software infrastructure to help entrepreneurs compete against the today’s giants in ecommerce"", 'Requirements:\xa0', '2+ years of data engineering experience.Regularly processes TBs of data quickly with cloud based distributed solutions.Robust experience with Spark, Redshift, Python and Jenkins.Experience writing and executing complex SQL queries.Experience building data pipelines and ETL design (implementation and maintenance).Experience with AWS or other cloud provider.Scrum/Agile software development process.', '2+ years of data engineering experience.', 'Experience with data technologies, Hadoop, Hive, Postgres, Kafka, Solr.', 'Experience with AWS or other cloud provider.', 'Scrum/Agile software development process.', 'Analytical experience debugging slow queries and scripts.', 'This position is responsible for understanding stakeholders requirements and building out ETL from variety of data sources\xa0(Mongo,\xa0MySQL, Kafka, ElasticSearch, Third Party REST APIs, etc).', '\xa0Global working environment', '\xa0Above market compensation package', 'We’re looking for an experienced Data Engineer to help deliver critical business intelligence through our data warehouse\xa0(Redshift)\xa0and data lake\xa0(AWS\xa0S3). Our Data Engineering team handles all aspects of managing our batched and real-time data pipelines from four e-commerce products.', '\xa0Above market compensation package\xa0Career growth opportunities\xa0Global working environment', 'Experience with data technologies, Hadoop, Hive, Postgres, Kafka, Solr.Deep understanding of AWS services: EMR, S3, Redshift and Terraform.Operational experience with Jenkins or Airflow.Proficiency in using BI dashboard tools.Analytical experience debugging slow queries and scripts.', 'Experience building data pipelines and ETL design (implementation and maintenance).', 'Deep understanding of AWS services: EMR, S3, Redshift and Terraform.', '\xa0Career growth opportunities', '\xa0Benefits & Perks:', 'Operational experience with Jenkins or Airflow.', 'Benefits & Perks:', 'Preferred:\xa0', ':\xa0', 'Robust experience with Spark, Redshift, Python and Jenkins.', 'Scalable']",Associate,Full-time,Information Technology,Information Technology and Services,2021-02-05 10:31:41
Data Engineer,Secure Source,United States,21 hours ago,Be among the first 25 applicants,"['', 'Secure Source is proud to once again be supporting one of the leaders within the Cyber Threat Intelligence market.\xa0This organisation has been going through exciting growth through 2020, and 2021 looks to be their biggest year yet. You will be joining an organisation which has one of the best working cultures with plenty of opportunity to work with cutting edge technology, and a career path with a clear road map to progress your career.', 'Contributing to the maintaining and improving of product quality.', 'Proficient in the use of databases: query and data definition.', 'Creating product and support documentation.', 'An ability to break-down complex requirements into workflows and identify KPI’s.', 'Knowledge of MVC frameworks', 'Recognizing trends and patterns in in the data that can be exploited into a repeatable analysis process.', 'Working level knowledge of algorithms.An ability to break-down complex requirements into workflows and identify KPI’s.Proficient in the use of databases: query and data definition.Proficiency in at least one of: Python, Golang, SQL, Bash, PerlProficiency in the use of industry standard tooling (Atlassian Stack)Competency with LinuxExperience with at least one alternative database technology: Elastic Search, Apache, Cassandra, Mongo DB, SparkExperience with Cloud Technologies (AWS, Google Cloud)Knowledge of MVC frameworksAn ability to create visualizations from resultant analytic results.', 'Identifying risks to projects, communicates and formulates mitigation plans.', 'An ability to create visualizations from resultant analytic results.', 'Contributing across the whole project lifecycle.', 'Responsibilities:', 'Experience with at least one alternative database technology: Elastic Search, Apache, Cassandra, Mongo DB, Spark', 'Design and develop innovative data analytics software and methods.', 'Performing triage of product support requests, determining problems and then assisting with the escalation of them where appropriate.', 'Incorporating effective test procedures, logging and monitoring in software.', 'Actively contributing to cross-functional team efforts.', 'Remote – US', 'The opportunity available with have you developing with two key teams in the business: The Threat Intelligence and research development teams. You will be utilising a wide range of technologies to be designing, developing and deploying innovative programming and technical solutions to data analytics and data processing. You will be working independently and producing work according to clear-cut and complete specifications. ', '\xa0', 'Competency with Linux', 'Skills required:', 'Experience with Cloud Technologies (AWS, Google Cloud)', 'Proficiency in at least one of: Python, Golang, SQL, Bash, Perl', 'Proficiency in the use of industry standard tooling (Atlassian Stack)', 'Design and develop innovative data analytics software and methods.Contributing across the whole project lifecycle.Recognizing trends and patterns in in the data that can be exploited into a repeatable analysis process.Performing triage of product support requests, determining problems and then assisting with the escalation of them where appropriate.Incorporating effective test procedures, logging and monitoring in software.Contributing to the maintaining and improving of product quality.Creating product and support documentation.Identifying risks to projects, communicates and formulates mitigation plans.Actively contributing to cross-functional team efforts.', 'Working level knowledge of algorithms.']",Mid-Senior level,Full-time,Information Technology,Staffing and Recruiting,2021-02-05 10:31:41
Data Engineer,Lightspeed Systems,"Portland, OR",18 hours ago,Be among the first 25 applicants,"['Wellness -- Lightspeed kicks cash into your HSA if you participate in our HDHP. Employees are provided an adjustable desk and onsite gyms at some offices. Healthy Holiday and PTO policy. ', 'Education is undergoing a technology revolution with new devices and tools being added to the classroom every day and IT departments are responsible for keeping all this technology managed, safe and working. That is where we come in! Lightspeed Systems, ed-tech provider and leader in K-12 device filtering for 20 years, partners with schools to make learning safe, managed and mobile. Learn more at www.lightspeedsystems.com.We love our employees, and we show it. A sneak peek into our BENEFITS & PERKS include: ', 'Provide technical leadership, oversight, and guidance for the data/schema. ', '', 'A track record of leading, designing and building cost-effective data solutions in AWS using products such as EC2, EMR, S3, Athena, DynamoDB, CloudFormation, CodeBuild, ECS ', 'Nice to Have ', 'Over 3 years of experience or sufficient experience with a range of database/data warehouse/data engineering technologies and data architecture with demonstrated results implementing for business intelligence and analytics use. ', 'Contribute and drive high-quality engineering practices towards building data infrastructure and pipelines at scale. ', 'Strategic thinker, self-motivated and team-oriented, should be able to work autonomously and effectively as part of geographically dispersed teams. ', ' Building AI/ML pipelines is an asset  Big Data visualization frameworks e.g  ', 'Architect and implement data governance and security for the data platforms. ', 'We are looking to add passionate, driven people to our team to help us fulfill our mission. ', 'ABOUT YOU: ', ""As our ideal person for this role, you will understand how data should be securely stored, consumed, and managed. You'll use your expertise to develop and innovate technical solutions to meet the needs of the business. You are passionate about data, problem-solving, and driven to achieve great results for customers. "", 'Retirement -- 401(k) matching up to 6%', 'Bachelor’s or Master’s degree in Computer Science, Computer Engineering or a related technical field. ', 'Nice to Have', 'Design data models for optimal storage and retrieval and optimize the data architecture to meet critical product and business requirements. ', 'Building AI/ML pipelines is an asset ', '3+ years hands-on experience writing complex, highly-optimized SQL queries across large data sets. ', 'Successful in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy ', 'Establish architectural approaches and patterns that incorporate modernizing data governance, metadata management and data quality. ', ' Over 3 years of experience or sufficient experience with a range of database/data warehouse/data engineering technologies and data architecture with demonstrated results implementing for business intelligence and analytics use.  Bachelor’s or Master’s degree in Computer Science, Computer Engineering or a related technical field.  Hands-on experience preferably with AWS (or another cloud platform).  Experience designing, building and operating distributed systems, real-time data pipelines  3+ years hands-on experience writing complex, highly-optimized SQL queries across large data sets.  Structuring data to be consumed easily, perform well, and be highly utilized in increasingly visual and analytic tools and use cases.  A track record of leading, designing and building cost-effective data solutions in AWS using products such as EC2, EMR, S3, Athena, DynamoDB, CloudFormation, CodeBuild, ECS  Strategic thinker, self-motivated and team-oriented, should be able to work autonomously and effectively as part of geographically dispersed teams.  Successful in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy  ', 'ABOUT THE ROLE: ', 'Hands-on experience preferably with AWS (or another cloud platform). ', 'Lightspeed Systems partners with schools to make learning safe, mobile and easily managed. We are growing and currently serve 15 million students and 28,000 schools in 35 countries. ', 'Structuring data to be consumed easily, perform well, and be highly utilized in increasingly visual and analytic tools and use cases. ', 'ABOUT YOU:', 'ABOUT US ', ' Contribute and drive high-quality engineering practices towards building data infrastructure and pipelines at scale.  Partner with cross-functional engineering teams to define highly scalable and reliable architectures for global data stores and data lakes.  Provide technical leadership, oversight, and guidance for the data/schema.  Design data models for optimal storage and retrieval and optimize the data architecture to meet critical product and business requirements.  Collaborate with engineering and data science teams to understand data challenges and provide scalable and flexible solutions.  Understand logging, event processing and how it impacts rest of our data flow, architect logging best practices where needed.  Establish architectural approaches and patterns that incorporate modernizing data governance, metadata management and data quality.  Architect and implement data governance and security for the data platforms.  ', 'Understand logging, event processing and how it impacts rest of our data flow, architect logging best practices where needed. ', ' ', 'NOTE: Sponsorship is not available for this position. ', 'ABOUT THE ROLE:', 'Data Engineer', 'ABOUT US', 'Collaborate with engineering and data science teams to understand data challenges and provide scalable and flexible solutions. ', 'Experience designing, building and operating distributed systems, real-time data pipelines ', 'Big Data visualization frameworks e.g ', ' Health -- Medical, dental and vision insurance with healthy company contribution toward premiums.  Wellness -- Lightspeed kicks cash into your HSA if you participate in our HDHP. Employees are provided an adjustable desk and onsite gyms at some offices. Healthy Holiday and PTO policy.  Retirement -- 401(k) matching up to 6%', 'Partner with cross-functional engineering teams to define highly scalable and reliable architectures for global data stores and data lakes. ', 'Health -- Medical, dental and vision insurance with healthy company contribution toward premiums. ', 'As a result, we are looking to add a Data Engineer to drive and continually evolve the data architecture. If you are passionate about data, understand how the attributes of data such as the velocity, volume and variety drive the underlying choices of data pipelines and datastores to be used, we want to talk to you. ']",Associate,Full-time,Engineering,Computer Software,2021-02-05 10:31:41
Data Engineer,DISYS,"Plano, TX",16 hours ago,Be among the first 25 applicants,"['SQL', 'Python for Data Transformation', 'Required Skills:', 'Tableau', 'AWS', 'ETL', 'SQLPython for Data TransformationAWSTableauETL']",Mid-Senior level,Contract,Information Technology,Banking,2021-02-05 10:31:41
Data Engineer,aim4hire,"Austin, Texas Metropolitan Area",1 day ago,Be among the first 25 applicants,"['', '• Experience with Snowflake or Redshift', 'Our Data Security Client here in Austin is seeking Senior-caliber Data Engineers w/ technical engineering chops around highly complex data infrastructure and operations with a familiarity working in cloud environments, building data pipelines, and ETL. The company has earned various awards and recognition for their culture and growth in the Cybersecurity space as they continue to scale!', '• 5+ years of experience in Python or Java', '• Experience with Kafka, Spark, Kinesis, and/or Hadoop', 'This is a permanent (salary) position and flexibility on full-time remote work!', '• Created big data architecture, built and operate data pipelines and maintained data storage, all within distributed systems', '• Familiarity with container technologies (Docker/Kubernetes)', '• basic automation and CI/CD and experience with data pipeline and workflow management tools: Jenkins/Git, Airflow, etc', 'Data Engineers', '• Apache Spark & experience with relational SQL databases (Snowflake & Postgres)', "" You'll collaborate with data architects on the vision and execution of our data platform."", '• Strong familiarity with cloud based services (AWS)', 'permanent (salary) position and flexibility on full-time remote work!', '• Built large-scale data pipelines and data-centric applications using any of the distributed storage platforms such as HDFS, S3, NoSQL databases (Hbase, Cassandra, etc.) ', '• stream-processing systems such as Spark-Streaming', 'This is a', 'Bonus Qualifications:', '• 5+ years of experience in a Data Engineer role - developing data pipelines or ETLs', 'Minimum Qualifications:']",Mid-Senior level,Full-time,Information Technology,Computer Software,2021-02-05 10:31:41
Data Engineer - Raleigh,TechSoup,"Raleigh, NC",8 hours ago,Be among the first 25 applicants,"['', 'Education', ""You'll translate business requirements into data models that are easy to understand and used by different disciplines across the companyYou'll design, implement and build pipelines that deliver data to stakeholders in the organizationYou’ll support our reporting team by performing quality checks on data and resolving data integrity issues as needed"", 'Full Time position in Raleigh, NC', ""You'll design, implement and build pipelines that deliver data to stakeholders in the organization"", 'Qualifications', 'You’ll support our reporting team by performing quality checks on data and resolving data integrity issues as needed', 'Duties And Responsibilities', 'You have experience with Java and Python', ""You'll translate business requirements into data models that are easy to understand and used by different disciplines across the company"", '5+ years of experience', 'You have 5+ years of data engineering experience in the industry building, optimizing, and maintaining ETL pipelines.', 'You have recent accomplishments working with relational as well as NoSQL data stores, methods and approaches (logging, columnar, star and snowflake, dimensional modeling)', 'Position', 'Currently all positions are remote', 'You have experience with cloud warehouse technologies, such as Snowflake and Redshift', 'You have a proven track record in scaling and optimizing schemas, performance tuning SQL and ETL pipelines in OLAP and Data Warehouse environments', ""Bachelor's degree in Computer Science, Engineering or a related field, or equivalent training, fellowship, or work experience"", 'Data Engineer ', 'You have 5+ years of data engineering experience in the industry building, optimizing, and maintaining ETL pipelines.You have recent accomplishments working with relational as well as NoSQL data stores, methods and approaches (logging, columnar, star and snowflake, dimensional modeling)You have a proven track record in scaling and optimizing schemas, performance tuning SQL and ETL pipelines in OLAP and Data Warehouse environmentsYou have experience with Java and PythonYou have experience with cloud warehouse technologies, such as Snowflake and Redshift']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-02-05 10:31:41
Data Visualization Engineer,Karmak,"Carlinville, IL",20 minutes ago,Be among the first 25 applicants,"['', 'Use time productively, maximize efficiency, and meet challenging work goals', 'Strong written and verbal communication skills', 'Adhere to applicable development standards and processes', 'Commitment to meet deadlines based off project and/or implementation timelines', 'Ability to learn new systems and reporting concepts quickly', 'Karmak is looking for an energetic, passionate, customer focused team player with an outstanding work ethic to join our Client Services team as a Data Visualization Engineer. \xa0The Data Visualization Engineers are responsible for creating customized Crystal and Logi reports which customers utilize to analyze and logically display data from our proprietary software. ', 'Possess the ability to work well as part of a team and independently', 'At least 2 years experience utilizing Crystal reports. ', 'Ability to multitask and still perform at a high level', 'Minimum 2-4 years of experience in report building and/or other comparable development team. ', 'Troubleshoot existing custom reports', 'Ability to partner with development and customer service teams as needed', 'Embrace accountability, task ownership, and innovation', 'Familiarity with business system concepts is preferred', 'At least 1 year experience utilizing SQL to create reports', 'Participate in the peer code review process', 'Ability to develop and maintain documentation as required ', 'Participate, when applicable, in resolution for issues in reporting tools (defects, performance, etc.)', 'This position will troubleshoot, maintain and provide bug fixes on existing custom reports as well as provide alternative custom reporting options to our standard report suite.\xa0This position is required to utilize SQL, Crystal reports, and Logi analytics and the goal is to become extremely proficient in these tools via on the job training and ongoing development experience. This role does assist to identify report projects scope, key requirements, and is expected to follow standard development lifecycle processes.', 'Gather requirements and functional specifications, which documents the needs and requirements of both internal and external customersCommunicate (oral and written) and understand the needs of the customerWork well with a team with a focus on helping customers formulate proper business concepts, design and content presentation in reporting formatDevelopment and testing of custom reportsTroubleshoot existing custom reportsParticipate, when applicable, in resolution for issues in reporting tools (defects, performance, etc.)Adhere to applicable development standards and processesParticipate in the peer code review processServe as a mentor to other team members to share knowledge and best practicesPlan, prioritize, and organize individual workload to provide best possible service to internal and external customersCommitment to meet deadlines based off project and/or implementation timelinesUse time productively, maximize efficiency, and meet challenging work goals', 'Serve as a mentor to other team members to share knowledge and best practices', 'Gather requirements and functional specifications, which documents the needs and requirements of both internal and external customers', 'Focus on customer experience', 'Detailed oriented and solution focused', 'Development and testing of custom reports', 'Prioritization skills and completing assigned tasks on time', 'Plan, prioritize, and organize individual workload to provide best possible service to internal and external customers', 'TSQL, Stored Procedures, Views', '\xa0', 'Essential Job Functions', 'Previous experience using Visual Studio, TFS and SSMS', 'Previous experience with Crystal Reports and/or Logi Info Studio', 'Essential Knowledge, Skills and Abilities', 'Communicate (oral and written) and understand the needs of the customer', 'Work well with a team with a focus on helping customers formulate proper business concepts, design and content presentation in reporting format', 'Embrace accountability, task ownership, and innovationFocus on customer experienceDetailed oriented and solution focusedAbility to multitask and still perform at a high levelPossess the ability to work well as part of a team and independentlyStrong written and verbal communication skillsAbility to develop and maintain documentation as required Ability to partner with development and customer service teams as neededAt least 1 year experience utilizing SQL to create reportsTSQL, Stored Procedures, ViewsAt least 2 years experience utilizing Crystal reports. Previous experience using Visual Studio, TFS and SSMSPrevious experience with Crystal Reports and/or Logi Info StudioPrioritization skills and completing assigned tasks on timeMinimum 2-4 years of experience in report building and/or other comparable development team. Familiarity with business system concepts is preferredAbility to learn new systems and reporting concepts quickly']",Mid-Senior level,Full-time,Administrative,Computer Software,2021-02-05 10:31:41
Data Engineer - Herndon,"SpringML, Inc.","Herndon, VA",10 hours ago,Be among the first 25 applicants,"['', 'B.S. or equivalent degree in computer science, mathematics or other relevant fields.5-10 years of experience in ETL, Datawarehouse, Visualization and building data pipelines.Strong Programming skills – experience and expertise in one of the following: Java, Python, Scala, C.Proficient in big data/distributed computing frameworks such as Apache Spark, Kafka, Experience with Agile implementation methodologies.', 'Strong Programming skills – experience and expertise in one of the following: Java, Python, Scala, C.', 'Proficient in big data/distributed computing frameworks such as Apache Spark, Kafka, ', 'About SpringML', '5-10 years of experience in ETL, Datawarehouse, Visualization and building data pipelines.', 'Build Data pipelines using standard frameworks in Hadoop, Apache Beam and other open source solutions.', 'Learn quickly – ability to understand and rapidly comprehend new areas – functional and technical – and apply detailed and critical thinking to customer solutions.', 'Experience with Agile implementation methodologies.', 'Ability to work as a member of a team assigned to design and implement data integration solutions.Build Data pipelines using standard frameworks in Hadoop, Apache Beam and other open source solutions.Learn quickly – ability to understand and rapidly comprehend new areas – functional and technical – and apply detailed and critical thinking to customer solutions.Propose design solutions and recommend best practices for large scale data analysis', 'Responsibilities', 'Propose design solutions and recommend best practices for large scale data analysis', 'B.S. or equivalent degree in computer science, mathematics or other relevant fields.', 'Desired Skills And Experience', 'Ability to work as a member of a team assigned to design and implement data integration solutions.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-02-05 10:31:41
Big Data Engineer,Mican Technologies Inc.,"Philadelphia, PA",8 hours ago,Be among the first 25 applicants,[],Entry level,Full-time,Engineering,Information Technology and Services,2021-02-05 10:31:41
"Data Engineer, Product Analytics",Facebook,"Washington, DC",9 hours ago,Be among the first 25 applicants,"['', 'BS/BA in Technical Field, Computer Science or Mathematics.', 'Preferred Qualification', 'Experience analyzing data to identify deliverables, gaps and inconsistencies.', '2+ years experience with schema design and dimensional data modeling.', 'Build data expertise and own data quality for allocated areas of ownership.', 'Responsibilities', 'Define and manage SLA for all data sets in allocated areas of ownership.', 'Work with data infrastructure to triage infra issues and drive to resolution.', 'Design, build and launch new data models in production.', 'Minimum Qualification', 'Design, build and launch new data extraction, transformation and loading processes in production.', 'Knowledge in Python or Java.', '2+ years experience in writing SQL statements.', 'Experience managing and communicating data warehouse plans to internal clients.', '2+ years experience in custom ETL design, implementation and maintenance.', 'BS/BA in Technical Field, Computer Science or Mathematics.Knowledge in Python or Java.', 'Support existing processes running in production.', 'Interface with engineers, product managers and product analysts to understand data needs.', 'Manage data warehouse plans for a product or a group of products.Interface with engineers, product managers and product analysts to understand data needs.Build data expertise and own data quality for allocated areas of ownership.Design, build and launch new data models in production.Design, build and launch new data extraction, transformation and loading processes in production.Support existing processes running in production.Define and manage SLA for all data sets in allocated areas of ownership.Work with data infrastructure to triage infra issues and drive to resolution.', '2+ years experience in the data warehouse space.2+ years experience in custom ETL design, implementation and maintenance.2+ years experience working with either a MapReduce or an MPP system.2+ years experience with object-oriented programming languages.2+ years experience with schema design and dimensional data modeling.2+ years experience in writing SQL statements.Experience analyzing data to identify deliverables, gaps and inconsistencies.Experience managing and communicating data warehouse plans to internal clients.', '2+ years experience in the data warehouse space.', '2+ years experience working with either a MapReduce or an MPP system.', '2+ years experience with object-oriented programming languages.', 'Manage data warehouse plans for a product or a group of products.']",Not Applicable,Full-time,Information Technology,Internet,2021-02-05 10:31:41
"Data Engineer, Product Analytics",Facebook,"Washington, DC",9 hours ago,Be among the first 25 applicants,[],Not Applicable,Full-time,Information Technology,Internet,2021-02-05 10:31:41
AWS Data Engineer/Developer,SES Corporation,"Chantilly, VA",13 hours ago,Be among the first 25 applicants,"['', 'Expert level knowledge in building data warehouses', 'Experience with Sagemaker, TEnsor Flow', 'STD', '  Organizational Skills Can plan and prioritize work, both their own and that of project team. Follows tasks to their logical conclusion and makes sure that everything has been done to the right standard. Good attention to detail.  Teamwork Comfortable working both individually and as part of a team. Prepared to challenge ideas within a group in a constructive way. Ability to influence others and move a proposal effort toward a common vision or goal.  Problem Solving Natural inclination for planning strategy and tactics. Ability to analyze problems and determine root cause, generating alternatives, evaluating and selecting alternatives and implementing solutions.  Results oriented Able to drive things forward regardless of personal interest in the task. ', 'Position Description ', 'Previous experience supporting dashboarding development desired', 'Knowledge of Python (minimum 3-4 years’ experience)', 'Experience in implementing ML Ops', 'Dental', 'Knowledge of both the LGY business (mortgage and loans), OIT operations, and understanding of system development life cycle preferred', 'Decompose data features into user stories', 'Vision', 'Experience with AWS and developing lambda functions to support data curation, notification & monitoring programs (minimum 3-4 years’ experience)', 'Knowledge of both the LGY business (mortgage and loans), OIT operations, and understanding of system development life cycle', ' Implement transformation to clean the source data, including the use of ETL tooling, scripting and data science techniques.  Decompose data features into user stories Design technical solution for features Refine user stories into tasks  Provide research and analysis on data sources in support of data discovery and requirements elicitation Monitor monthly data source replication and ETL jobs to ensure successful completion. Work with the System Team on any deployment or compilation issues encountered during automated deployments Conduct periodic ETL data refreshes in Analytics preprod environment in support of QA and UAT Provide UAT support, triage, troubleshooting, and explanations Provide GitLab peer review and code merges Manage code merges and deployments to preprod in support of QA activities ', 'Company paid Life Insurance', ' Teamwork Comfortable working both individually and as part of a team. Prepared to challenge ideas within a group in a constructive way. Ability to influence others and move a proposal effort toward a common vision or goal.', 'Refine user stories into tasks ', 'Years of Experience ', 'Preferred Skills And Qualifications', 'Required Skills', 'Provide UAT support, triage, troubleshooting, and explanations', 'Experience with Database technology, including index strategies, performance tuning and optimization and stored procedures. .', 'Experience presenting analytics results in clear and concise presentation, such as slides, memos, or reports.', ' Results oriented Able to drive things forward regardless of personal interest in the task.', 'A successful history of manipulating, processing and extracting value from large, disconnected datasets.', 'LTD', 'Strong project management and organizational skills.', 'Able to work and communicate with Solution Architects, System Developers, Data Modelers, DBAs', 'Build processes supporting data transformation, data structures, metadata, dependency and workload management.', 'Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.', 'Responsibilities ', ' Bachelor’s', 'Design technical solution for features', 'Implement transformation to clean the source data, including the use of ETL tooling, scripting and data science techniques. ', 'Mortgage analytics experience desired', 'Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.', 'Medical', 'Paid Time Off', '5 years', ' Communications Ability to communicate clearly and efficiently to team members and clients, verbally and in writing. Able to present ideas in a variety of ways depending upon audience and context. Excellent active listening skills. Strong interpersonal skills with the ability to interact effectively with all levels of personnel, elected officials, executive leadership, senior management, users, vendors and subcontractor personnel.', '2-3 years of ETL/Data Engineering experience', 'Experience supporting and working with cross-functional teams in a dynamic environment.', 'AD&D', 'Work with the System Team on any deployment or compilation issues encountered during automated deployments', 'Provide research and analysis on data sources in support of data discovery and requirements elicitation', 'Conduct periodic ETL data refreshes in Analytics preprod environment in support of QA and UAT', 'Able to create standard common business vocabulary, express strategic data requirements, outline high level integrated designs to meet system and application- s strategy and business architecture.', 'Strong analytic skills related to working with structured and unstructured datasets.', 'Manage code merges and deployments to preprod in support of QA activities', ' Mortgage analytics experience desired Previous experience supporting dashboarding development desired Knowledge of Python, R, Open Source tools desired Experience presenting analytics results in clear and concise presentation, such as slides, memos, or reports. Knowledge of both the LGY business (mortgage and loans), OIT operations, and understanding of system development life cycle Experience with Sagemaker, TEnsor Flow Experience in implementing ML Ops Experience with Veteran Affairs is an advantage (current PIV/GFE enablement a plus) ', ' Problem Solving Natural inclination for planning strategy and tactics. Ability to analyze problems and determine root cause, generating alternatives, evaluating and selecting alternatives and implementing solutions.', 'Education Requirements ', 'Soft Skills', 'Monitor monthly data source replication and ETL jobs to ensure successful completion.', 'Expert Level experience on RedShift (minimum 3-4 years’ experience)', 'Experience with Veteran Affairs is an advantage (current PIV/GFE enablement a plus)', '2+ years’ experience in data modeling and database design including both relational and data mart/data warehouse database design, breadth and depth in established and emerging data technologies.', 'Knowledge of Python, R, Open Source tools desired', 'Benefits', 'Requirements', '2+ years expertise in the following sectors is required housing and mortgage finance, Federal credit programs, and government benefit programs', ' Organizational Skills Can plan and prioritize work, both their own and that of project team. Follows tasks to their logical conclusion and makes sure that everything has been done to the right standard. Good attention to detail.', '401k with employer contribution', ' Medical Dental Vision AD&D STD LTD Company paid Life Insurance 401k with employer contribution Paid Time Off', ' Expert level knowledge in building data warehouses Experience with AWS and developing lambda functions to support data curation, notification & monitoring programs (minimum 3-4 years’ experience) Knowledge of Python (minimum 3-4 years’ experience) Expert Level experience on RedShift (minimum 3-4 years’ experience) Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases. Experience with Database technology, including index strategies, performance tuning and optimization and stored procedures. . Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. 2+ years’ experience in data modeling and database design including both relational and data mart/data warehouse database design, breadth and depth in established and emerging data technologies. Able to create standard common business vocabulary, express strategic data requirements, outline high level integrated designs to meet system and application- s strategy and business architecture. Able to work and communicate with Solution Architects, System Developers, Data Modelers, DBAs 2+ years expertise in the following sectors is required housing and mortgage finance, Federal credit programs, and government benefit programs Strong analytic skills related to working with structured and unstructured datasets. Build processes supporting data transformation, data structures, metadata, dependency and workload management. A successful history of manipulating, processing and extracting value from large, disconnected datasets. Strong project management and organizational skills. Experience supporting and working with cross-functional teams in a dynamic environment. Knowledge of both the LGY business (mortgage and loans), OIT operations, and understanding of system development life cycle preferred 2-3 years of ETL/Data Engineering experience ', 'Provide GitLab peer review and code merges']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-02-05 10:31:41
Data Python Engineer,"OpenArc, LLC.","Pittsburgh, PA",7 hours ago,Be among the first 25 applicants,"['', 'Ability to operate in Cloud Environments, with Containers, a strong plus', 'Work and/or academic experience building applications using any of the following:', 'Large scale distributed databases as well as more traditional options: key-value, graph, SQL, NoSQL, time series', 'Process unstructured data into a form suitable for large scale automation', 'Conduct research and implement automation solutions leveraging Python librariesGather and process raw data at scale by using Python packages (OpenCV at a minimum)Process unstructured data into a form suitable for large scale automationUse deployment tools to convert code into productionUnderstand business automation requirements and recommend/implement the best solutions within the Python languageCompare different Python libraries for diverse tasks and pursue the most meaningful alternative based on pros and consGather and process raw data at scale by Python data intake librariesProfile Python code to optimize performance and securityUtilize cutting edge methods in Python to ingest complex data (tables, images and complex files)Ability to operate in Cloud Environments, with Containers, a strong plusCritical thinking skills to assess how Python automation capabilities can best be applied to complex business situationsWork closely with other functional teams to integrate your ideas, innovations and algorithms into production systemsSupport business decisions with ad hoc coding tasks as neededHaving the ability to query databases with structured and unstructured data and interface them with PythonWork in a fast-paced multidisciplinary environment as in a competitive landscape new data keeps flowing in rapidly and the world is constantly changing', 'Responsibilities', 'Extensive experience in software or applications engineering and/or technical operations', 'Experience handling data with relational databases is preferred', 'API Development in Python: Django, Flask...', 'Gather and process raw data at scale by using Python packages (OpenCV at a minimum)', 'Compare different Python libraries for diverse tasks and pursue the most meaningful alternative based on pros and cons', 'Use deployment tools to convert code into production', 'Critical thinking skills to assess how Python automation capabilities can best be applied to complex business situations', 'Conduct research and implement automation solutions leveraging Python libraries', 'Understand business automation requirements and recommend/implement the best solutions within the Python language', 'Bachelor’s Degree in Computer Science related field', 'Having the ability to query databases with structured and unstructured data and interface them with Python', 'Gather and process raw data at scale by Python data intake libraries', 'Work closely with other functional teams to integrate your ideas, innovations and algorithms into production systems', 'Work in a fast-paced multidisciplinary environment as in a competitive landscape new data keeps flowing in rapidly and the world is constantly changing', 'Bachelor’s Degree in Computer Science related fieldA strong drive to learn and master new technologies and techniques in Python driven high throughput automationAbility to provide recommendations among many similar/overlapping Python librariesExtensive experience in software or applications engineering and/or technical operationsExperience developing over message queues systems: ServiceBus, Apache Kafk, RabbitMQAPI Development in Python: Django, Flask...Understanding of Docker, Kubernetes, and CI/CD processesWork and/or academic experience building applications using any of the following:Large scale distributed databases as well as more traditional options: key-value, graph, SQL, NoSQL, time seriesFamiliarity with data cleansing, manipulating datasets Exposure to cloud environments preferably Microsoft AzureExperience handling data with relational databases is preferred', 'Familiarity with data cleansing, manipulating datasets ', 'Requirements', 'Exposure to cloud environments preferably Microsoft Azure', 'Understanding of Docker, Kubernetes, and CI/CD processes', 'A strong drive to learn and master new technologies and techniques in Python driven high throughput automation', 'Experience developing over message queues systems: ServiceBus, Apache Kafk, RabbitMQ', 'Utilize cutting edge methods in Python to ingest complex data (tables, images and complex files)', 'Support business decisions with ad hoc coding tasks as needed', 'Profile Python code to optimize performance and security', 'Ability to provide recommendations among many similar/overlapping Python libraries']",Entry level,Full-time,Engineering,Information Technology and Services,2021-02-05 10:31:41
Data Engineer,Brooksource,"Charlotte, NC",19 hours ago,Be among the first 25 applicants,"['', 'Charlotte, NC', 'NICE TO HAVE:', 'Experience with Docker', 'COVID Schedule: Rotational, 2-week onsite/remote schedule', 'BENEFITS OF WORKING WITH BROOKSOURCE:', 'ENVIRONMENT', 'Work closely with Operations Production Support team in resolving escalated high priority incidents and the development\xa0coding issues.', 'Dedication to keep an open line of communication and provide full transparency.\xa0\xa0', 'MINIMUM REQUIREMENTS:', 'Experience working with Media/Advertising MSO data and applicationsExperience working with Snowflake and SnowSQLStrong desire to mentor others in the data integration space, providing technical and business guidance.Experience working with S3 bucketsExperience with Docker', 'Hands-on experience with GitHub for version control', 'Good team player, extremely adaptable and fast learner.', 'Design and develop ETL to load the data warehouse and data marts and test ETL data pipelines to maintain data infrastructures.', 'Familiarity with Agile development methodologies such as Scrum and SAFe', 'Experience with cloud infrastructure - AWS preferred', 'Strong desire to mentor others in the data integration space, providing technical and business guidance.', 'ENVIRONMENT:', 'Creating, scheduling and monitoring workflows - Airflow preferred', 'Direct communication with the hiring manager, which allows us to have a clear understanding of the timeline and move candidates through the interview process faster.Dedication to keep an open line of communication and provide full transparency.\xa0\xa0', 'The primary responsibility of this role is to design and develop reusable components, code, and document custom automation frameworks, maintain scripts, and update\xa0these items as needed to build continuous delivery pipelines, ensuring rapid availability of the product.', 'Support testing efforts as a part of SaFE Agile team.', 'Develop data mapping documentation to establish relationships between source and target tables including transformation processes.', 'Create, schedule and maintain data pipelines/workflows using Apache Airflow and Python.', 'Experience in ETL performance testing, including data validation', '12 month ongoing contract ', 'Strong data architecture knowledge around enterprise data warehousing concepts, SQL development and optimization, and data integration', 'Extract and transform data from multiple sources and load data into one or more destinations, and monitor integration and replication performance to ensure quality and stability.', 'Experience with atleast one MPP database (Teradata, Netezza, Snowflake, MongoDB, etc.)', 'Advanced SQL scripting and querying experience - Expert in creating SQL objects like Tables, Complex Stored Procedures, Triggers, Views, Indexes, and User Defined Functions to facilitate efficient data manipulation and consistent data storage.', 'Experienced in building Data Integration and Workflow Solutions and Extract, Transform, and Load (ETL) solutions for data warehousingStrong data architecture knowledge around enterprise data warehousing concepts, SQL development and optimization, and data integrationAdvanced SQL scripting and querying experience - Expert in creating SQL objects like Tables, Complex Stored Procedures, Triggers, Views, Indexes, and User Defined Functions to facilitate efficient data manipulation and consistent data storage.Writing Python scripts with knowledge of relevant frameworks and libraries, such as workflow management utilities and libraries for accessing and extracting data.Creating, scheduling and monitoring workflows - Airflow preferredHands-on experience with GitHub for version controlExperience in ETL performance testing, including data validationFamiliarity with Agile development methodologies such as Scrum and SAFeGood team player, extremely adaptable and fast learner.Experience with atleast one MPP database (Teradata, Netezza, Snowflake, MongoDB, etc.)Experience with cloud infrastructure - AWS preferred', 'Experience working with Snowflake and SnowSQL', '5+ years’ experience with the following:', 'Experience working with S3 buckets', 'Validate data quality and perform all aspects of verification, including functional, structural, regression and system testing.', 'Company size: Fortune 100', 'Dress code: Business casual', 'Experienced in building Data Integration and Workflow Solutions and Extract, Transform, and Load (ETL) solutions for data warehousing', '\xa0', 'This position requires strong collaboration with you Scrum Team (composed of Scrum Master, Product Owner, Lead Architect, Developers and QA) to understand business requirements and provide appropriate data integration solutions in alignment with solution implementation architecture.', 'Writing Python scripts with knowledge of relevant frameworks and libraries, such as workflow management utilities and libraries for accessing and extracting data.', 'RESPONSIBILITIES:', 'Design and develop reusable components, code, and document custom automation frameworks, maintain scripts, and update\xa0these items as needed to build continuous delivery pipelines, ensuring rapid availability of the product.', 'Brooksource is looking for an Data Integration Data Engineer for our Fortune 100 Telecommunications client in the Charlotte, NC area. The Data Integration Data Engineer will be an integral member of the Data Operations Architecture team. You will join the advertising/media organization and contribute to data solutions that power some of the largest and most successful marketing/advertising campaigns.', 'Design and develop reusable components, code, and document custom automation frameworks, maintain scripts, and update\xa0these items as needed to build continuous delivery pipelines, ensuring rapid availability of the product.Assist in requirements gathering and documents processes with internal stakeholders and collaborate with Product Owners, Scrum Master and other team members to determine data extraction and transformation requirements.Develop data mapping documentation to establish relationships between source and target tables including transformation processes.Write and analyze complex SQL for the purpose of data extraction and processing - Design, develop, and deploy data movement using SQL server Integrations Services (SSIS), TSQL and stored procedures.Extract and transform data from multiple sources and load data into one or more destinations, and monitor integration and replication performance to ensure quality and stability.Create, schedule and maintain data pipelines/workflows using Apache Airflow and Python.Design and develop ETL to load the data warehouse and data marts and test ETL data pipelines to maintain data infrastructures.Support testing efforts as a part of SaFE Agile team.Validate data quality and perform all aspects of verification, including functional, structural, regression and system testing.Work closely with Operations Production Support team in resolving escalated high priority incidents and the development\xa0coding issues.', 'Hours: Standard 40-hour weeks', 'Data Engineer – Data Integration', 'Assist in requirements gathering and documents processes with internal stakeholders and collaborate with Product Owners, Scrum Master and other team members to determine data extraction and transformation requirements.', 'Experience working with Media/Advertising MSO data and applications', 'Write and analyze complex SQL for the purpose of data extraction and processing - Design, develop, and deploy data movement using SQL server Integrations Services (SSIS), TSQL and stored procedures.', 'Direct communication with the hiring manager, which allows us to have a clear understanding of the timeline and move candidates through the interview process faster.', 'Company size: Fortune 100Hours: Standard 40-hour weeksDress code: Business casualCOVID Schedule: Rotational, 2-week onsite/remote schedule']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-02-05 10:31:41
Data Engineer,Madison Logic,New York City Metropolitan Area,17 hours ago,32 applicants,"['', 'Competitive Benefits including Medical, Dental, Vision, and FSA plans ', 'Bachelor’s degree in computer science, statistics or mathematics3+ year’s experience with Python or Node.js4+ year’s experience with SQL or mySQL3+ year’s experience with cloud computing services (AWS)Working understanding of big data analyticsExperience working with data cleaning and standardizing process\xa0', 'Employer-paid Life, AD&D and STD insurance', 'Opportunities for Advancement – As We Grow, You Grow!\xa0Competitive Benefits including Medical, Dental, Vision, and FSA plans Employer-paid Life, AD&D and STD insurance401k with Company Match Commuter Benefits (Transit & Parking)\xa0Generous Paid Time Off including: 9 paid Holidays, 17 Vacation Days (to start!), Sick Time, Summer Friday Program, and Parental Leave\xa02 Paid Volunteer DaysWork-Flex PolicyFlexible “You Do You” Monthly Cash Stipend ($50/month)Legal & Financial Services BenefitsCompany Outings, Social & Charity Events, Sponsored Healthy Hours & Happy Hours\xa0Wellness initiatives\xa0Global Mobility Program An innovative, energetic culture and a fantastic team!', 'Opportunities for Advancement – As We Grow, You Grow!\xa0', 'Use SQL/Python to create reports, dashboards, and visualizations.', 'Work-Flex Policy', 'An innovative, energetic culture and a fantastic team!', '2 Paid Volunteer Days', 'Global Mobility Program ', 'Use SQL/Python to create reports, dashboards, and visualizations.Aggregate/Model data and use that data to build reports in DomoAnalyze data to help improve business performance.Identify the best data sources for a given analysis.Develop processes for data mining, data modeling, and data production.Offer insights and recommendations to improve data reliability and quality.', 'Aggregate/Model data and use that data to build reports in Domo', 'Analyze data to help improve business performance.', 'Commuter Benefits (Transit & Parking)\xa0', '4+ year’s experience with SQL or mySQL', 'Flexible “You Do You” Monthly Cash Stipend ($50/month)', 'Responsibilities:', '3+ year’s experience with cloud computing services (AWS)', 'Offer insights and recommendations to improve data reliability and quality.', '3+ year’s experience with Python or Node.js', 'Generous Paid Time Off including: 9 paid Holidays, 17 Vacation Days (to start!), Sick Time, Summer Friday Program, and Parental Leave\xa0', 'Develop processes for data mining, data modeling, and data production.', 'Qualifications:', 'Company Outings, Social & Charity Events, Sponsored Healthy Hours & Happy Hours\xa0', 'Working understanding of big data analytics', 'Identify the best data sources for a given analysis.', '\xa0', 'Legal & Financial Services Benefits', 'Wellness initiatives\xa0', 'Benefits & Perks:', '401k with Company Match ', 'Experience working with data cleaning and standardizing process\xa0', 'Bachelor’s degree in computer science, statistics or mathematics', 'We are seeking a Data Engineer with a highly analytical mind who is skilled at understanding data and is able to translate it into actionable insights. In this role you will develop easy to understand reports, dashboards, and tools with the aim of optimizing and streamlining the way data is viewed. In this highly collaborative role, you will work closely with the architecture and data teams to reach business objectives.\xa0']",Mid-Senior level,Full-time,Information Technology,Marketing and Advertising,2021-02-05 10:31:41
Lead Data Engineer,General Motors,"Austin, TX",17 hours ago,Be among the first 25 applicants,"['', ""Development activities will include enhancing existing data systems and optimizing ETL systems.Additionally you will lead development activities to migrate out of legacy technologies to GM's Big Data Platform utilizing new technologies such as, Kafka, Hadoop, PySpark, Greenplum and GM's internally developed Big Data tools."", 'In this role you will also lead junior developers, mentor, coach and help them to develop their software development skills in Data Engineering technologies.Demonstrate mastery of many programming languages, tools and/or technologies with emphasis on ETL & Database developmentUnderstand and maintain compliance with GM standards and industry standard methodology', 'In this role you will also lead junior developers, mentor, coach and help them to develop their software development skills in Data Engineering technologies.', ""developing data architecture and ETL solutions using sound, repeatable, industry standard methodologies. You will have the opportunity to work hands-on defining ETL solutions based on business requirements and system specifications.Development activities will include enhancing existing data systems and optimizing ETL systems.Additionally you will lead development activities to migrate out of legacy technologies to GM's Big Data Platform utilizing new technologies such as, Kafka, Hadoop, PySpark, Greenplum and GM's internally developed Big Data tools."", 'Responsibilities', 'At least 3 years of hands on experience with Big Data Tools: Hadoop, Spark, Kafka, etc.', 'In recent years, GM Information Technology has successfully executed the largest IT transformation in the history of the automotive industry, fully insourcing what once was a nearly completely outsourced IT function. Today GM IT is a dynamic and fast paced organization that designs, develops and maintains all IT infrastructure, applications and solutions enabling GM’s global operations. From designing and building the next generation of electric and other vehicles to developing a world-class GM experience for our dealers and customers, GM IT is driving real change in the most iconic automaker on the planet. Our team delivers unique enterprise-wide IT solutions in cutting-edge technologies such as mobility, telematics, mission-critical business systems, supercomputing, cloud, vehicle engineering and real-time computing. We offer challenging positions for passionate professionals looking to advance their careers and be a part of an IT organization focused on innovation, speed and business value.', 'You will identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.', ""At least 3 years of hands on experience with Big Data Tools: Hadoop, Spark, Kafka, etc.Master databases - Advanced SQL and NoSQL databases, including Postgres and Cassandra, Oracle and GreenplumData Wrangling and Preparation using GM's approved tools: PySpark, Jupyter etc.Stream-processing systems: Storm, Spark-Streaming, etc.Ability to tackle software engineering and data problems quickly and completelyAbility to identify tasks which require automation and automate themAbility to multi-task and stay organized in a dynamic work environment and work collaboratively with other IT organizations (GDAAS, Platform Engineering etc)"", ' Healthcare (including a triple tax advantaged health savings account and wellness incentive), dental, vision and life insurance plans to cover you and your family;', 'As a Data Engineer, you will build industrialized data assets and optimize data pipelines in support of Vehicle Quality Business Intelligence and Advance Analytics objectives. You will work closely with our Quality Business teams, forward-thinking Data Scientists, BI Developers, System Architects and Data Architects to deliver value to our vision for the future.', 'You will assemble large, complex data sets that meet functional / non-functional business requirements', 'For This Role You Will Be Responsible For', 'Benefits Overview', ""Data Wrangling and Preparation using GM's approved tools: PySpark, Jupyter etc.Stream-processing systems: Storm, Spark-Streaming, etc."", 'Communicate and maintains Master Data, Metadata, Data Management Repositories, Logical Data Models, Data Standards', 'Work with business partners on data-related technical issues and develop requirements to support their data infrastructure needs', 'Ability to tackle software engineering and data problems quickly and completely', 'Master databases - Advanced SQL and NoSQL databases, including Postgres and Cassandra, Oracle and Greenplum', '7 or more years with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.', 'Create highly consistent and accurate analytic datasets suitable for business intelligence and data scientist team members', 'Demonstrate mastery of many programming languages, tools and/or technologies with emphasis on ETL & Database development', ' Paid time off including vacation days, holidays, and parental leave for mothers, fathers and adoptive parents;', 'Job Description', ' Discount on GM vehicles for you, your family and friends.', 'developing data architecture and ETL solutions using sound, repeatable, industry standard methodologies. You will have the opportunity to work hands-on defining ETL solutions based on business requirements and system specifications.', 'Ability to identify tasks which require automation and automate them', 'Communicate and maintains Master Data, Metadata, Data Management Repositories, Logical Data Models, Data StandardsCreate and maintain optimal data pipeline architectureYou will assemble large, complex data sets that meet functional / non-functional business requirementsYou will identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Build industrialized analytic datasets and delivery mechanisms that utilize the data pipeline to deliver actionable insights into vehicle quality, operational efficiency and other key business performance metricsWork with business partners on data-related technical issues and develop requirements to support their data infrastructure needsCreate highly consistent and accurate analytic datasets suitable for business intelligence and data scientist team members', 'Understand and maintain compliance with GM standards and industry standard methodology', 'Requirements', 'We are not able to accommodate international relocation.', 'Ability to multi-task and stay organized in a dynamic work environment and work collaboratively with other IT organizations (GDAAS, Platform Engineering etc)', 'Preferred', 'About GM', ' Global recognition program for peers and leaders to recognize and be recognized for results and behaviors that reflect our company values;', 'Build industrialized analytic datasets and delivery mechanisms that utilize the data pipeline to deliver actionable insights into vehicle quality, operational efficiency and other key business performance metrics', ' Company and matching contributions to 401K savings plan to help you save for retirement;', ' Tuition assistance and student loan refinancing;', 'Create and maintain optimal data pipeline architecture']",Not Applicable,Full-time,Information Technology,Automotive,2021-02-05 10:31:41
R&D Data Engineer,Callaway Golf,"Carlsbad, CA",19 hours ago,Be among the first 25 applicants,"['', 'Travel may be required during the course of employment.', 'Extensive use of office equipment to include computer, calculator, copier, fax, and other business related machines and software.', 'PDF copy of Resume Preferred.', 'We are looking for a skilled Data Engineer to join our Research and Development Team implementing data pipelines for use in our Data Science based product design.\xa0The hire will be responsible for the data and data pipeline architecture, as well as the CI/CD of the data to the R&D department and collaborating departments.\xa0The ideal candidate will be comfortable taking the lead in the data organization, pipelining, and wrangling of the data.\xa0The Data Engineer will support our data scientists, product designers, product analysts, and business analysts on data initiatives and will ensure optimal data delivery architecture is consistent throughout the ongoing projects. This ideal candidate will enjoy training business analysts in the company on the best practices in accessing the data. The right candidate will enjoy learning and keeping up to date on modern data architecture and technologies to optimize the delivery, efficiency, and accuracy of the data.\xa0The ideal candidate has a track record of successfully delivering self service data.', 'Research and test new tools and technologies', '\xa0\xa0', 'This job description indicates in general terms, the type and level of work performed as well as the typical responsibilities of employees in this classification and it may be changed by management at any time.\xa0Other duties may also apply.\xa0Nothing in this position description changes the at-will employment relationship existing between the Company and its employees.\xa0Distribution of this item outside of the Company without an authorized release is a violation of Company policy.', 'Experience with SQL and NoSQL Databases: MSSQL, Azure SQL, Azure Synapse and Azure Cosmos DB or equivalents ', 'Our company is a blend of experience and diverse backgrounds, and our leaders have a strong history of building and selling successful initiatives. We are working to build a truly groundbreaking company, and we want top-notch people to join us in that mission.', 'ROLES AND RESPONSIBILITIES', 'Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing existing processes for better data flow and accuracy.', 'Build DataOps for a modern approach to data\xa0', 'Callaway Golf Company is a leader in total performance, premium golf equipment and active lifestyle products while also being a great place to work! We are passionate and push the limits of innovation. We dare to be great while acting with integrity and respect. We stay hungry, yet humble. All while having fun and making golf enjoyable for everyone!', 'Create and maintain data pipeline architecture in a CI/CD framework built for self service data.', 'Track record of organizing dispersed datasets across different teams of people.', 'Document data schema and architecture design decisions for handoff to other teams and data scientist. ', 'Ability to work well with all levels of the organization in fast paced, dynamic environment.\xa0Cultivates positive relationships with other associates and collaborate with others in cross-functional teams.', 'Experience with data pipeline and workflow management tools: Azure Data Factory, Airflow, or equivalents', 'We are looking for a candidate with 3+ years experience in a Data Engineering role, who has attained a Graduate Degree in Computer Science, Information Systems, Engineering, or other quantitative field. Cloud based certifications a plus.Track record of organizing dispersed datasets across different teams of people.Focus on self-growth and improvement for the betterment of the business objectives.', 'JOB OVERVIEW', 'Keep data separated and secure across national boundaries through multiple regions and data centers.', 'Focus on self-growth and improvement for the betterment of the business objectives.', 'Monitor data pipelines to preempt data delivery issues.', 'Experience with Big Data Tools: Databricks or equivalent', 'Ability to interface with other groups and disciplines to successfully complete assignments and to make recommendations.\xa0Ability to work beyond functional boundaries to identify opportunities, develop and implement solutions to improve company performance.', 'EDUCATION AND EXPERIENCE', 'Build the infrastructure required for optimal data flow in Azure cloud technologies using a mix of open source and licensed technologies.', 'Assemble large, complex data sets that meet the data customers’ needs and enrich the data sources where possible. ', 'Experience with stream-processing systems: Azure Event Hub, Kafka, or equivalents', 'We are looking for a candidate with 3+ years experience in a Data Engineering role, who has attained a Graduate Degree in Computer Science, Information Systems, Engineering, or other quantitative field. Cloud based certifications a plus.', 'Build DataOps for a modern approach to data\xa0Create and maintain data pipeline architecture in a CI/CD framework built for self service data.Assemble large, complex data sets that meet the data customers’ needs and enrich the data sources where possible. Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing existing processes for better data flow and accuracy.Build the infrastructure required for optimal data flow in Azure cloud technologies using a mix of open source and licensed technologies.Document data schema and architecture design decisions for handoff to other teams and data scientist. Keep data separated and secure across national boundaries through multiple regions and data centers.Monitor data pipelines to preempt data delivery issues.Manage Data initiative projects from start to finish Research and test new tools and technologies', '\xa0', 'Work is performed in a designated professional office workstation and environment.Extensive use of office equipment to include computer, calculator, copier, fax, and other business related machines and software.Travel may be required during the course of employment.', 'DISCLAIMER', 'Experience with IaC and CI/CD integration.', 'Experience with Big Data Tools: Databricks or equivalentExperience with SQL and NoSQL Databases: MSSQL, Azure SQL, Azure Synapse and Azure Cosmos DB or equivalents Experience with data pipeline and workflow management tools: Azure Data Factory, Airflow, or equivalentsExperience with stream-processing systems: Azure Event Hub, Kafka, or equivalentsExperience with IaC and CI/CD integration.Ability to work well with all levels of the organization in fast paced, dynamic environment.\xa0Cultivates positive relationships with other associates and collaborate with others in cross-functional teams.Ability to interface with other groups and disciplines to successfully complete assignments and to make recommendations.\xa0Ability to work beyond functional boundaries to identify opportunities, develop and implement solutions to improve company performance.Able to consistently make data-driven decisions.', 'Work is performed in a designated professional office workstation and environment.', 'Able to consistently make data-driven decisions.', 'TECHNICAL COMPETENCIES (Knowledge, Skills & Abilities)', 'PHYSICAL REQUIREMENTS / WORK ENVIRONMENT (if applicable)', 'Manage Data initiative projects from start to finish ']",Mid-Senior level,Full-time,Engineering,Sporting Goods,2021-02-05 10:31:41
Data Engineer,Optello,"Charlottesville, VA",6 hours ago,Be among the first 25 applicants,"['', ' Top Benefits', ' Create and maintain data pipeline architecture', ' Process Improvements and automation', ' Small Team Environment', ' Python', ' Design physical database management systems using Postgress , SQL Server, MySQL etc...', ' Building and optimizing data pipelines', ' Analyzing complex data and root cause analysis', 'Please do NOT change the email subject line in any way. You must keep the JobID: linkedin : KT4-1613770 -- in the email subject line for your application to be considered.***', ' Java', ' Small Team Environment Top Benefits Job Security', ' Top Secret w/SCI eligibility SQL Python Java SCI Security Clearance Metadata Data Structures Large datasets', ' Data Structures', ' Large datasets', 'Optello is proud to be an Equal Opportunity Employer', ' Integrate OSINT', 'Email Your Resume In Word To', 'Your Right to Work', ' Apply modeling methods to classify and generalize data stored in database management systems', ' Job Security', ' Metadata', ' SQL', ' Process Improvements and automation Analyzing complex data and root cause analysis Building and optimizing data pipelines Integrate OSINT Create and maintain data pipeline architecture Design physical database management systems using Postgress , SQL Server, MySQL etc... Apply modeling methods to classify and generalize data stored in database management systems', ' SCI Security Clearance', 'This Will Include Aspects Like', ' Top Secret w/SCI eligibility']",Entry level,Full-time,Information Technology,Construction,2021-02-05 10:31:41
Data Engineer (R1691),Avalara,"Seattle, WA",2 hours ago,Be among the first 25 applicants,"['', 'Continuous Integration/Continuous Deployment setups (Jenkins or similar)', 'Web mapping tech, specifically OpenLayers, Mapbox GL, Turf.js', 'Description', '3+ Years of experience in Software and data Engineering', 'Experience with distributed source control tools and development workflow (git experience is ideal, but source control alternatives are OK too)', 'A good understanding of relational database principles, ability to read and write SQL', 'Qualifications', 'AWS or similar cloud infrastructure (GCP, Azure, etc.)', 'Understands what to look for and how to make a code review productive', 'Demonstrates experience and interest in creating great user experiences', 'Web mapping tech, specifically OpenLayers, Mapbox GL, Turf.jsPostgreSQL and PostGISAWS or similar cloud infrastructure (GCP, Azure, etc.)Understands what to look for and how to make a code review productiveIntegration TestingContinuous Integration/Continuous Deployment setups (Jenkins or similar)DockerPrevious experience working with transaction tax, customs brokerage, VAT or similar Exposure', 'Web application security fundamentals', 'Bonus Points If You Have Experience With', 'Integration Testing', '3+ Years of experience in Software and data Engineering3 years experience working with Big Data Platform in the commercial cloud such as AWSExperience in AWS Glue, S3, Lambda, Python, Java/SkalaA good understanding of relational database principles, ability to read and write SQLDemonstrates experience and interest in creating great user experiencesValues/enjoys prototyping, user-testing, training, documenting; generally engaging with peopleExperience with distributed source control tools and development workflow (git experience is ideal, but source control alternatives are OK too)Thorough understanding of testing strategies and toolsWeb application security fundamentals', 'Thorough understanding of testing strategies and tools', '3 years experience working with Big Data Platform in the commercial cloud such as AWS', 'PostgreSQL and PostGIS', 'Experience in AWS Glue, S3, Lambda, Python, Java/Skala', 'Docker', 'Previous experience working with transaction tax, customs brokerage, VAT or similar Exposure', 'Values/enjoys prototyping, user-testing, training, documenting; generally engaging with people']",Entry level,Full-time,Information Technology,Computer Software,2021-02-05 10:31:41
Data Engineer,FanDuel,New York City Metropolitan Area,3 hours ago,Over 200 applicants,"['', 'FanDuel Racing', 'What we’re looking for in our next teammate', 'FanDuel Group is based in New York, with offices in California, New Jersey, Florida, Oregon and Scotland. Our brands include:', 'Creating and maintain optimal data pipeline architectureDesigning and implementing data pipelines required in the data warehouse and data lake in batch or real-time using data transformation technologies', 'TVG', 'Our roster has an opening with your name on it', ""FanDuel Group is an equal opportunities employer. Diversity and inclusion in FanDuel means that we respect and value everyone as individuals. We don't tolerate bias, judgement or harassment.\xa0Our focus is on developing employees so that they reach their full potential."", 'Understanding of AWS and Google Cloud', 'Comfortable writing Python scripts', 'THE CONTRACT', 'Competitive compensation is just the beginning. As part of our team, you can expect:', 'Show proficiency understanding complex ETL processes', 'THE STATS', 'Everyone on our team has a part to play', '●\xa0\xa0\xa0\xa0\xa0Opportunities to build really cool products that fans love', 'Designing and deploying data models and views with large datasets that meet functional / non-functional business requirements', 'FanDuel Casino & Betfair Casino', 'Creating data tools for analytics and working with stakeholders across all departments to assist with data-related technical issues and supporting their data infrastructure needs', 'Advanced working SQL knowledge and experience working with relational databases', 'THE POSITION', 'THE GAME PLAN', 'FOXBet\xa0', 'Identifying, designing, and implementing internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability', 'FOXBet\xa0— A world-class betting platform and affiliate of FanDuel Group\xa0', 'FanDuel', '●\xa0\xa0\xa0\xa0\xa0Hall of Fame benefit programs and platforms', 'Delivering quality production-ready code in an agile environment', 'Advanced working SQL knowledge and experience working with relational databasesBuild processes supporting data transformation, data structures, metadata, dependency, and workload managementShow proficiency understanding complex ETL processesDemonstrate the ability to optimize processes (ram vs io)Knowledge of data integrity and relational rulesUnderstanding of AWS and Google CloudAbility to quickly learn new technologies is criticalProficiency with agile or lean development practicesComfortable writing Python scripts', 'FanDuel Sportsbook\xa0—\xa0America’s #1 sports betting app\xa0', 'PokerStars', '●\xa0\xa0\xa0\xa0\xa0An exciting and fun environment committed to driving real growth', 'Proficiency with agile or lean development practices', 'PokerStars\xa0US —\xa0The premier online poker product\xa0and affiliate of FanDuel Group', 'Delivering data integration solutions to downstream marketing and campaign software', 'Build processes supporting data transformation, data structures, metadata, dependency, and workload management', 'Creating and maintain optimal data pipeline architectureDesigning and implementing data pipelines required in the data warehouse and data lake in batch or real-time using data transformation technologiesIdentifying, designing, and implementing internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalabilityDesigning and deploying data models and views with large datasets that meet functional / non-functional business requirementsDelivering data integration solutions to downstream marketing and campaign softwareDelivering quality production-ready code in an agile environmentDelivering test plans, monitoring, debugging and technical documents as a part of development cycleCreating data tools for analytics and working with stakeholders across all departments to assist with data-related technical issues and supporting their data infrastructure needs', 'FanDuel Group is looking for an experienced Data Engineer with deep understanding of large-scale data handling and processing best practices in a cloud environment to help us build scalable systems. As our data is a key component of the business used by almost every facet of the company, including product development, marketing, operations, and finance. It is vital that we deliver robust solutions that ensure reliable access to data with a focus on quality and availability.', 'Demonstrate the ability to optimize processes (ram vs io)', 'Our competitive edge comes from making decisions based on accurate and timely data and your work will provide access to that data across the whole company. Looking ahead to the next phase of our data platform we are keen to do more with real time data processing and working with our data scientists to create machine learning pipelines.', 'We treat our team right', 'FanDuel Group is a world-class team of brands and products all built with one goal in mind —\xa0to give fans new and innovative ways to interact with their favorite games, sports, teams, and leagues. That’s no easy task, which is why we’re so dedicated to building a winning team. And make no mistake, we are here to win, but we believe in winning right. That means we’ll never compromise when it comes to looking out for our teammates. From our many opportunities for professional development to our generous insurance and paid leave policies, we’re committed to making sure our employees get as much out of FanDuel as we ask them to give.', 'Delivering test plans, monitoring, debugging and technical documents as a part of development cycle', 'FanDuel Sportsbook', '●\xa0\xa0\xa0\xa0\xa0Flexible vacation allowance to let you refuel', '●\xa0\xa0\xa0\xa0\xa0Mentorship and professional development resources to help you refine your game', 'FanDuel Racing\xa0—\xa0A horse racing app built for the average sports fan\xa0', 'FanDuel\xa0—\xa0A game-changing real-money fantasy sports app\xa0', 'Knowledge of data integrity and relational rules', 'Ability to quickly learn new technologies is critical', 'ABOUT FANDUEL GROUP', 'FanDuel Casino & Betfair Casino\xa0—\xa0Fan-favorite online casino apps\xa0', 'TVG\xa0—\xa0The best-in-class horse racing TV/media network and betting platform\xa0']",Mid-Senior level,Full-time,Information Technology,Sports,2021-02-05 10:31:41
Data Engineer - Visualization,Lucas James Talent Partners,United States,23 hours ago,38 applicants,"['', 'Create and maintain optimal data pipeline architecture,Assemble large, complex data sets that meet functional / non-functional business requirements.Build and maintain the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and ‘big data’ technologies.Work with stakeholders including the Partner, Client and Data teams to assist with data-related technical issues and support their data infrastructure needs.Work with Account Teams to develop effective visualizations.Work with Data Scientist and Scale Leadership to strive for greater functionality in our data systems.', 'RESPONSIBILITIES FOR DATA ENGINEER', 'Expert-level skills in writing and optimizing SQL.', 'Assemble large, complex data sets that meet functional / non-functional business requirements.', 'Create and maintain optimal data pipeline architecture,', ""This position requires a Bachelor's Degree in Computer Science or a related technical field2+ years of work experience with ETL, Data Modeling, and Data Architecture.1+ years Data VisualizationExpert-level skills in writing and optimizing SQL.Proficiency in one of the scripting languages - python, ruby, linux or similar.Experience with at least one cloud service: AWS and/or Microsoft AzureExperience building and optimizing ‘big data’ data pipelines, architectures and data sets.Experience operating very large data warehouses or data lakes.Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.Experience with cross team data interrogation process, define architecture, design reviews."", ""This position requires a Bachelor's Degree in Computer Science or a related technical field"", 'Experience with cross team data interrogation process, define architecture, design reviews.', 'Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.', '2+ years of work experience with ETL, Data Modeling, and Data Architecture.', 'Work with Account Teams to develop effective visualizations.', 'Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.', 'Experience with at least one cloud service: AWS and/or Microsoft Azure', 'Build and maintain the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and ‘big data’ technologies.', '1+ years Data Visualization', 'Work with stakeholders including the Partner, Client and Data teams to assist with data-related technical issues and support their data infrastructure needs.', 'Proficiency in one of the scripting languages - python, ruby, linux or similar.', 'We are looking for a savvy Data Engineer to join our growing team of analytics experts. The position will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow, collection and visualizations for cross-functional teams.\xa0The Data Engineer will support our Account teams and Data Scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects.\xa0You should have deep expertise in the design, creation, management, and business use of large datasets, across a variety of data platforms. You should have excellent business and interpersonal skills to be able to work with business owners to understand data requirements, and to build ETL to ingest the data into the data lake. You should be an authority at crafting, implementing, and operating stable, scalable, low cost solutions to flow data from production systems into the data lake. Above all you should be passionate about working with huge data sets and someone who loves to bring datasets together turning data to actionable knowledge.', 'BASIC QUALIFICATIONS', 'Work with Data Scientist and Scale Leadership to strive for greater functionality in our data systems.', 'Experience operating very large data warehouses or data lakes.']",Associate,Full-time,Information Technology,Marketing and Advertising,2021-02-05 10:31:41
Data Engineer,Envision,Greater St. Louis,19 hours ago,Be among the first 25 applicants,"['', 'Experience with cloud infrastructure such as Azure in container technologies like Kubernetes a mandatory ', 'Strong background with data modeling, data access, and data storage techniques', 'Exposure in storing and assembling large complex data sets with various formats such as CSV, JSON, Avro, Parquet', 'Familiarity with R, Python, Javascript is a big plus', 'Experience in building the data pipelines and workflows from ground up including defining the exception handling strategies', 'Hands-on coding and application development experience with programming languages such as Python, Shell, Java or similar scripting languages', 'Experience in Big Data Integration & Analytics is a plus', 'Strong experience with relational databases like SQL Server, MySQL etc ', 'Data Engineer ', '5+ years of experience in the field of business intelligence, application development, database development and ETL and/or data analysis domains with extensive SQL knowledge', '5+ years of experience in the field of business intelligence, application development, database development and ETL and/or data analysis domains with extensive SQL knowledgeExperience with cloud infrastructure such as Azure in container technologies like Kubernetes a mandatory Experience in building the data pipelines and workflows from ground up including defining the exception handling strategiesExposure in storing and assembling large complex data sets with various formats such as CSV, JSON, Avro, ParquetHands-on coding and application development experience with programming languages such as Python, Shell, Java or similar scripting languagesFamiliarity with R, Python, Javascript is a big plusStrong experience with relational databases like SQL Server, MySQL etc Strong background with data modeling, data access, and data storage techniquesExperience in Big Data Integration & Analytics is a plus']",Mid-Senior level,Full-time,Information Technology,Staffing and Recruiting,2021-02-05 10:31:41
