job_title,company,location,date_posted,applicants,job_text,seniority_level,employment_type,job_function,industries,date_scraped
Data Engineer,Apple,"Cupertino, CA",10 hours ago,29 applicants,"['', 'Key Qualifications', 'Description', 'Summary', 'Education & Experience']",Not Applicable,Full-time,Information Technology,Consumer Electronics,2020-12-30 19:31:23
Data Engineer,Airtable,"San Francisco, CA",19 hours ago,Be among the first 25 applicants,"['', 'Ensure that our business-critical data is accurate and correct.', 'You can write clear, correct code in at least one programming language, and are willing to become effective in others as needed to get your job done.', "" Help architect, build, and scale our initial data engineering platform, with an eye on security and privacy. Work closely with the rest of engineering, as well as other stakeholders from our growth, sales, marketing, and product teams, to understand the data needs of the business and produce systems that enable better product and growth decision-making. Work on the data collection pipeline across the entire stack, from client-side event logging to ETL. Systems you'll touch may include data warehousing using MPP databases (e.g. RedShift or Vertica), workflow systems (e.g. Airflow or Luigi), streaming data processors (Kinesis, Kafka, etc.), and distributed data processing systems (Spark, Hadoop, etc.). Ensure that our business-critical data is accurate and correct. "", 'Help architect, build, and scale our initial data engineering platform, with an eye on security and privacy.', ""Work on the data collection pipeline across the entire stack, from client-side event logging to ETL. Systems you'll touch may include data warehousing using MPP databases (e.g. RedShift or Vertica), workflow systems (e.g. Airflow or Luigi), streaming data processors (Kinesis, Kafka, etc.), and distributed data processing systems (Spark, Hadoop, etc.)."", 'Work closely with the rest of engineering, as well as other stakeholders from our growth, sales, marketing, and product teams, to understand the data needs of the business and produce systems that enable better product and growth decision-making.', 'Who You Are', ""You're passionate and thoughtful about building systems that enhance human understanding."", 'You communicate with clarity and precision in written form; experience communicating with graphs and plots, or at least understanding how to enable other people to do this, is a big bonus.', ""What You'll Do"", "" You're passionate and thoughtful about building systems that enhance human understanding. You have professional experience working with modern data storage and processing technologies, and you've wrangled enough data to understand how often the complex systems that produce data can go wrong. You can write clear, correct code in at least one programming language, and are willing to become effective in others as needed to get your job done. You communicate with clarity and precision in written form; experience communicating with graphs and plots, or at least understanding how to enable other people to do this, is a big bonus. You may have experience administering modern large-scale data management systems such as ELK."", ""You have professional experience working with modern data storage and processing technologies, and you've wrangled enough data to understand how often the complex systems that produce data can go wrong."", 'You may have experience administering modern large-scale data management systems such as ELK.']",Associate,Full-time,Engineering,Computer Software,2020-12-30 19:31:23
Associate Data Engineer,Uplight,"Boston, MA",24 hours ago,66 applicants,"['', 'Provide a 401k Match', 'Are committed to the environment, our employees, and our communities.', 'A value for testing and developing quality software', 'Are focused on career growth by following defined career ladders', 'Developing data pipelines to transform and process data between systems', 'Experience writing and maintaining data pipelines and ETLs leveraging SparkExperience working cross-functionally with design, product, customer success, sales, etc.', 'Experience working cross-functionally with design, product, customer success, sales, etc.', 'Skills programming in at least one languageInterest in developing Machine Learning Engineering skillsA value for testing and developing quality softwareStrong critical thinking skills and a desire to work with ambiguous challengesExperience working in an Agile environment and a strong understanding of the full SDLCStrong troubleshooting skills that span the full-stack (front-end clients, APIs, networking, DNS, Linux, containers, databases, distributed systems, etc.)Experience deploying production applications on at least one major cloud provider (AWS, GCP, Azure)', 'Experience writing and maintaining data pipelines and ETLs leveraging Spark', 'Keep you energized with plenty of food and drink', 'Description', 'Work as an Engineer on our analytics engineering team, primarily developing in Python and leveraging a wide range of technologies, notably: AWS and GCP, Docker, Apache Airflow, Apache Spark, and PostgreSQL', 'Are proud to be over 300+ rebels with an important cause by helping to create a more sustainable planet.Are committed to the environment, our employees, and our communities.Are focused on career growth by following defined career laddersTake our work and mission seriously and….we love to laugh!', 'What You Bring To Uplight', 'Take problems from inception all the way to completion - own the building, testing, deployment, and maintenance of the code that you work on', 'Uplight provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.', 'Skills programming in at least one language', 'Work effectively on an Agile team and collaborate well with your other team members.', 'The Position', 'Provide a 401k MatchHave an innovative flexible time-off policyKeep you energized with plenty of food and drink', 'Take our work and mission seriously and….we love to laugh!', 'Scaling our software to handle the ever-growing customer data', 'Interest in developing Machine Learning Engineering skills', 'Experience working in an Agile environment and a strong understanding of the full SDLC', 'Are proud to be over 300+ rebels with an important cause by helping to create a more sustainable planet.', 'Experience deploying production applications on at least one major cloud provider (AWS, GCP, Azure)', 'We Also', 'Bonus Points', 'Work as an Engineer on our analytics engineering team, primarily developing in Python and leveraging a wide range of technologies, notably: AWS and GCP, Docker, Apache Airflow, Apache Spark, and PostgreSQLTake problems from inception all the way to completion - own the building, testing, deployment, and maintenance of the code that you work onTackle complex problems that span a wide range of technical abilities, including:', 'Developing data pipelines to transform and process data between systemsProductionize machine learning pipelines leveraging billions of rows of dataScaling our software to handle the ever-growing customer data', 'Tackle complex problems that span a wide range of technical abilities, including:', 'What Makes Working At Uplight Amazing', 'Have an innovative flexible time-off policy', 'Strong troubleshooting skills that span the full-stack (front-end clients, APIs, networking, DNS, Linux, containers, databases, distributed systems, etc.)', 'What You Get To Do', 'Strong critical thinking skills and a desire to work with ambiguous challenges', 'What You Will Contribute', 'Skills and experience are necessary, but we hire on value alignment first, so if you feel you would be a good fit with us, still consider applying.', 'Productionize machine learning pipelines leveraging billions of rows of data']",Associate,Full-time,Information Technology,Information Technology and Services,2020-12-30 19:31:23
Data Engineer,Toyota North America,"Dallas, TX",7 hours ago,Be among the first 25 applicants,"['', 'Relocation assistance (if applicable)', 'Experience as a senior engineer of a development team that practices agile scrum', 'Comprehensive health care and wellness plans for your entire family', 'Develop and build code for data movement, storage, access, delivery, streaming data services, database schemas', 'Paid holidays and paid time off', 'Understanding of Kubernetes based platforms and container-based software solutions', 'Software delivery leveraging CI/CD (continuous integration and continuous deployment) pipeline and integrating software in DevOps tool chain', 'Experience building solutions in AWS architecture', 'Expertise coding and implementing modern data physical architecture, patterns and toolsets in Cloud (AWS/Azure) and Hadoop technologies', 'Work with solution architect and development team to build quick prototypes leveraging existing or new architecture', 'Flexible spending accounts', 'Toyota 401(k) Savings Plan featuring a company match, as well as an annual retirement contribution from Toyota regardless of whether you contribute', ' Master’s degree in Computer Science or related discipline Knowledge of IBM Sterling Integrator Understanding of Kubernetes based platforms and container-based software solutions Software delivery leveraging CI/CD (continuous integration and continuous deployment) pipeline and integrating software in DevOps tool chain Familiarity and understanding in data warehouse architecture Experience building solutions in AWS architecture ', 'Perform software analysis, code analysis, requirements analysis, release analysis and deployment', 'Strong hands-on programming experience in Java, Python, Spark, Scala, Spring Boot framework', 'Experience implementing software leveraging flow-based pipelines NiFi and Streaming services Kafka and NoSQL databases', 'Vehicle purchase & lease programs', 'Who We’re Looking For', 'To save time applying, Toyota does not offer sponsorship of job applicants for employment-based visas or any other work authorization for this position at this time', 'Flextime and virtual work options (if applicable)', 'What You Should Know', 'Provide end to end flow for data process, map technical solutions to the process', 'Perform hands on coding to demonstrate pattern implementation for the development team with guidance from the solution architect', 'Participate and lead in design sessions to understand customers functional needs', ' Added bonus if you have', 'Participate and lead in design sessions to understand customers functional needsWork with solution architect and development team to build quick prototypes leveraging existing or new architectureProvide end to end flow for data process, map technical solutions to the processEnsure proposed designs and implementation meet requirements from architecture, security and the businessPerform hands on coding to demonstrate pattern implementation for the development team with guidance from the solution architectDevelop & build code in continuous development pipelines leveraging off-the-shelf and open source. components and improve the full lifecycle solution development processes adhering to the solution architecturePerform software analysis, code analysis, requirements analysis, release analysis and deploymentDevelop and build code for data movement, storage, access, delivery, streaming data services, database schemas', 'Familiarity and understanding in data warehouse architecture', 'Ensure proposed designs and implementation meet requirements from architecture, security and the business', 'Experience developing software in Data Lake platform architecture and tools (Hadoop, Spark, Scala, HIVE and HBASE)', 'Experience building data solutions that have used different data formats (Parquet, Avro, JSON, XML)', 'Strong hands-on experience with ETL tools like Informatica PowerCenter, IICS', 'Bachelor’s Degree (or higher) in Computer Science or related discipline or equivalent work experience', ' Bachelor’s Degree (or higher) in Computer Science or related discipline or equivalent work experience Experience as a senior engineer of a development team that practices agile scrum Experience building software for data ingestion & process pipelines for application consumption in a complex environment Expertise coding and implementing modern data physical architecture, patterns and toolsets in Cloud (AWS/Azure) and Hadoop technologies Experience developing software in Data Lake platform architecture and tools (Hadoop, Spark, Scala, HIVE and HBASE) Strong hands-on experience with ETL tools like Informatica PowerCenter, IICS Strong hands-on programming experience in Java, Python, Spark, Scala, Spring Boot framework Experience building data solutions that have used different data formats (Parquet, Avro, JSON, XML) Experience implementing software leveraging flow-based pipelines NiFi and Streaming services Kafka and NoSQL databases ', 'Professional growth and development programs to help advance your career, as well as tuition reimbursement', 'Knowledge of IBM Sterling Integrator', 'Develop & build code in continuous development pipelines leveraging off-the-shelf and open source. components and improve the full lifecycle solution development processes adhering to the solution architecture', 'Who We Are', 'What You’ll Be Doing', 'A work environment built on teamwork, flexibility and respect', 'Master’s degree in Computer Science or related discipline', 'What We’ll Bring', 'Referral services related to prenatal services, adoption, child care, schools and more', 'What You Bring', ' A work environment built on teamwork, flexibility and respect Professional growth and development programs to help advance your career, as well as tuition reimbursement Vehicle purchase & lease programs Comprehensive health care and wellness plans for your entire family Flextime and virtual work options (if applicable) Toyota 401(k) Savings Plan featuring a company match, as well as an annual retirement contribution from Toyota regardless of whether you contribute Paid holidays and paid time off Referral services related to prenatal services, adoption, child care, schools and more Flexible spending accounts Relocation assistance (if applicable) ', 'Experience building software for data ingestion & process pipelines for application consumption in a complex environment']",Not Applicable,Full-time,Information Technology,Financial Services,2020-12-30 19:31:23
Data Engineer,PDI Software,"Dallas, TX",21 hours ago,Be among the first 25 applicants,"['', 'Experience in data cleansing, curation, parsing, integration, semantic mapping, or editing', ""Bachelor's degree in Math, Statistics, Computer Science or equivalent technical field. Working and practical knowledge of programming principles, techniques, standards and analytical abilityStrong proficiency in JavaProven experience with complex SQL query design and optimizationExperience with Bash scriptingExperience in data cleansing, curation, parsing, integration, semantic mapping, or editingExperience with analytics systems (data warehouses, dimensional models, etc.)Organizational skills and ability to balance multiple priorities in a dynamic and fast-paced environmentStrong team player with a passion for data and problem solvingExcellent oral and written communication skills"", ""Onboard new customers' data into the PDI data warehouse and platform, writing and installing the required ETL’s"", 'Proven experience with complex SQL query design and optimization', 'Participate as a key member of our agile development team', 'Experience with Google Cloud', ""Bachelor's degree in Math, Statistics, Computer Science or equivalent technical field."", 'Preferred Qualifications', 'Degree in Computer Science, Computer Engineering, Management Information Systems or related field1-3 years of applied data engineering-related experienceStrong competence in PythonExperience with Google CloudExperience with Linux/Unix', 'Strong team player with a passion for data and problem solving', 'Experience with analytics systems (data warehouses, dimensional models, etc.)', 'Strong competence in Python', 'Responsibilities & Tasks', 'Monitor, maintain, and, if needed rectify, various clients’ data integrity', 'Degree in Computer Science, Computer Engineering, Management Information Systems or related field', 'Gather technical requirements from multiple sources for new data-oriented features, integrating new solutions within a developed Java solution base', ""Onboard new customers' data into the PDI data warehouse and platform, writing and installing the required ETL’sOptimize and manage existing clients’ data pipelinesMonitor, maintain, and, if needed rectify, various clients’ data integrityDevelop and/or enhance automated processes to proactively identify any data related issues and/or simplify processGather technical requirements from multiple sources for new data-oriented features, integrating new solutions within a developed Java solution baseParticipate as a key member of our agile development teamCollaborate with team members to automate queries as needed"", '1-3 years of applied data engineering-related experience', 'Excellent oral and written communication skills', 'Organizational skills and ability to balance multiple priorities in a dynamic and fast-paced environment', 'Required Qualifications', 'Experience with Bash scripting', 'Experience with Linux/Unix', 'Strong proficiency in Java', ' Working and practical knowledge of programming principles, techniques, standards and analytical ability', 'Optimize and manage existing clients’ data pipelines', 'Collaborate with team members to automate queries as needed', 'Develop and/or enhance automated processes to proactively identify any data related issues and/or simplify process']",Entry level,Full-time,Information Technology,Information Technology and Services,2020-12-30 19:31:23
Data Engineer,Horizontal Talent,"Austin, TX",2 hours ago,Be among the first 25 applicants,"['Spark ', 'Design and maintain data pipelines and services using best practices for data management and governance ', 'Sqoop some CDR tables (like MDM) from Oracle ', 'Experience with data pipeline frameworks such as Airflow, Luigi or Oozie', 'Primary Responsibilities', 'Experience with Scala, in particular with Spark Scala API', 'Software Tools/skills', 'Experience running machine learning or NLP applications at scale', 'Experience with cloud-based computing (AWS or Azure)', 'Leading edge technology in an industry that’s improving the lives of millions. ', 'Team And Team Size', 'Here, innovation isn’t about another gadget, it’s about making health care data available wherever and whenever people need it, safely and reliably. ', 'Experience with HBase or other non-relational data bases', 'Description', 'Experience with ETL', 'You should understand the importance and value of writing maintainable, documented, and well-tested code throughout the entire product lifecycle. ', 'Demonstrated knowledge of data management best practices ', 'Work with EHR data across teams with ETL, NLP engineers and data scientists, researchers and clinicians to provide data services with high data quality control standard ', ' Programming experience, including solid Python experience, following software engineering best practices  Experience building and maintaining data pipelines and data assets  Experience with distributed data processing frameworks such as Spark or MapReduce  Experience as an individual contributor, hands-on developer, non-manager role executing on engineering projects as a primary job responsibility  Demonstrated knowledge of data management best practices  Prioritization skills; ability to manage ad-hoc requests in parallel with ongoing projects ', 'Top Responsibilities', 'Data pipeline experience', ' Combine two of the fastest-growing fields on the planet with a culture of performance, collaboration and opportunity and this is what you get.  Leading edge technology in an industry that’s improving the lives of millions.  Here, innovation isn’t about another gadget, it’s about making health care data available wherever and whenever people need it, safely and reliably.  There’s no room for error. Join us and start doing your life’s best work.(sm) ', 'You will need to develop a deep understanding of the data and drive efforts to maintain and improve data quality and usability. ', 'This data engineer will be maintaining, and if necessary re-architecting our data pipelines that ingest notes from a bunch a text files delivered to us on a share drive, move then over to HDFS, do some normalization (convert HTML to plain text, etc) and load them into Hive tables ', 'We are seeking a Data Engineer who is eager to tackle the challenges of processing vast amounts of EHR data originating from multiple sources. ', 'Experience with distributed data processing frameworks such as Spark or MapReduce ', 'Familiarity with containers might also be good to have.', '13 core team members (data scientists, project manager, medical informaticists) with support from ', 'You’ll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in', ' Small core NLP Team  13 core team members (data scientists, project manager, medical informaticists) with support from  12 clinical annotators integrated into the team via vendor contractor ', 'Projects The Candidate Will Be Working On', 'There’s no room for error. Join us and start doing your life’s best work.(sm)', 'Currently the main technologies we are using are Spark, Hadoop, Hive, Luigi, Python (and a little bit of Scala) and the platform we use is the on-prem Hadoop cluster. We need to make sure the candidate is solid with at least some of these technologies, and follows good engineering practices (such as testing, code reviews and putting in place monitoring systems like dashboards or alerts). ', ' Experience running machine learning or NLP applications at scale Experience with data pipeline frameworks such as Airflow, Luigi or Oozie Experience with search engines (Elasticsearch or Solr) Experience with cloud-based computing (AWS or Azure) Experience with Scala, in particular with Spark Scala API Familiarity with EHR data and standards (HL7 or FHIR) Experience with HBase or other non-relational data bases Experience with code and process documentation Experience with explaining, educating, presenting and/or training non-engineers on engineering concepts and processes Experience with continuous integration and delivery Experience with ETL', 'Experience with explaining, educating, presenting and/or training non-engineers on engineering concepts and processes', 'Schedule and run various NLP “apps” developed by data scientists ', ' Design and maintain data pipelines and services using best practices for data management and governance  Deploy machine learning and NLP applications in production  Work with EHR data across teams with ETL, NLP engineers and data scientists, researchers and clinicians to provide data services with high data quality control standard  You’ll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in ', 'Above all, you should be curious about what is possible in healthcare with the right tools and infrastructure.', '12 clinical annotators integrated into the team via vendor contractor', 'Skills/attributes', 'Preferred Qualifications:', 'Someone that will also be responsible for good data management practices (for example to make sure we can efficiently retire data from H-groups that need to be retired). ', ' We are seeking a Data Engineer who is eager to tackle the challenges of processing vast amounts of EHR data originating from multiple sources.  You will need to develop a deep understanding of the data and drive efforts to maintain and improve data quality and usability.  You should understand the importance and value of writing maintainable, documented, and well-tested code throughout the entire product lifecycle.  Above all, you should be curious about what is possible in healthcare with the right tools and infrastructure. ', 'Python ', 'Familiarity with EHR data and standards (HL7 or FHIR)', 'This is someone that will interface with the ProdOps team (for example, they are the ones delivering to us the notes as text files), with the CDR BE team (NLP2Panther) and others such as dCDR and Life Sciences engineering. ', 'Prioritization skills; ability to manage ad-hoc requests in parallel with ongoing projects', 'Experience with search engines (Elasticsearch or Solr)', 'Experience with continuous integration and delivery', 'Additional skills that would be good to have: cloud (since there is push for OA to move to AWS, nothing says we will stay forever on the on-prem cluster), and Elasticsearch (we need to build and keep up-to-date Elastic indices to allow users external to our group to search the notes). ', 'Experience with code and process documentation', ' Python  Spark  Data pipeline experience ', 'Experience as an individual contributor, hands-on developer, non-manager role executing on engineering projects as a primary job responsibility ', 'Combine two of the fastest-growing fields on the planet with a culture of performance, collaboration and opportunity and this is what you get. ', 'Deploy machine learning and NLP applications in production ', 'Programming experience, including solid Python experience, following software engineering best practices ', 'Experience building and maintaining data pipelines and data assets ', ' This data engineer will be maintaining, and if necessary re-architecting our data pipelines that ingest notes from a bunch a text files delivered to us on a share drive, move then over to HDFS, do some normalization (convert HTML to plain text, etc) and load them into Hive tables  Sqoop some CDR tables (like MDM) from Oracle  Schedule and run various NLP “apps” developed by data scientists  This is someone that will interface with the ProdOps team (for example, they are the ones delivering to us the notes as text files), with the CDR BE team (NLP2Panther) and others such as dCDR and Life Sciences engineering.  Someone that will also be responsible for good data management practices (for example to make sure we can efficiently retire data from H-groups that need to be retired).  Currently the main technologies we are using are Spark, Hadoop, Hive, Luigi, Python (and a little bit of Scala) and the platform we use is the on-prem Hadoop cluster. We need to make sure the candidate is solid with at least some of these technologies, and follows good engineering practices (such as testing, code reviews and putting in place monitoring systems like dashboards or alerts).  Additional skills that would be good to have: cloud (since there is push for OA to move to AWS, nothing says we will stay forever on the on-prem cluster), and Elasticsearch (we need to build and keep up-to-date Elastic indices to allow users external to our group to search the notes).  Familiarity with containers might also be good to have. ', 'Small core NLP Team ']",Entry level,Full-time,Information Technology,Information Technology and Services,2020-12-30 19:31:23
Data Engineer,"Starry, Inc.","Boston, MA",5 hours ago,Be among the first 25 applicants,"['', 'Work on an evolving data infrastructure platform at a fast growing company', 'Experience with relational databases like PostgreSQL', ' GIS data experience Spark experience Serverless experience AWS ecosystem experience Streaming data experience with Kafka or Kinesis Orchestration experience with frameworks like Airflow Front end experience Container experience with frameworks like Docker ', 'Participate in a positive work environment', 'Communicate with analytics and engineering stakeholders to help you get the job done', 'Learn new tools and concepts on the job', 'What We’re Looking For', 'Spark experience', 'We work hard, so we take care of each other and try to enjoy ourselves along the way. ', 'Partner with stakeholders to build data pipelines and tooling for their work', 'Participate in code reviews and design meetings', 'All Full Time Starry Employees Receive', 'Serverless experience', 'Container experience with frameworks like Docker', '3+ years data engineering experience', 'Bonus points if...', 'Catered meals on a weekly basis for employees working in the office', 'AWS ecosystem experience', ' Partner with stakeholders to build data pipelines and tooling for their work Become a domain expert for certain datasets within Starry Build tools to monitor the flow and quality of data moving through our data pipelines Lead by example with best practices in coding and automation Work on an evolving data infrastructure platform at a fast growing company Participate in code reviews and design meetings Learn new tools and concepts on the job Communicate with analytics and engineering stakeholders to help you get the job done Participate in a positive work environment ', 'Become a domain expert for certain datasets within Starry', ' 100% employer paid low deductible health plan, dental plan, vision plan, AD&D and life insurance 401(k) retirement plan and stock options  12 weeks of 100% paid parental leave for new mothers and fathers after one year of employment Professional development assistance after six months of employment Catered meals on a weekly basis for employees working in the office Casual dress, community clubs, annual fitness reimbursement, stocked kitchen and other perks and discounts ', 'About Starry', 'Front end experience', 'Streaming data experience with Kafka or Kinesis', 'GIS data experience', 'Lead by example with best practices in coding and automation', 'Professional development assistance after six months of employment', '100% employer paid low deductible health plan, dental plan, vision plan, AD&D and life insurance', '12 weeks of 100% paid parental leave for new mothers and fathers after one year of employment', 'Casual dress, community clubs, annual fitness reimbursement, stocked kitchen and other perks and discounts', 'Python and SQL fluency', 'Experience with batch ETL', ""What You'll Be Doing"", 'Build tools to monitor the flow and quality of data moving through our data pipelines', 'Experience with Git and CI/CD within the context of data engineering', ' 3+ years data engineering experience Python and SQL fluency Experience with relational databases like PostgreSQL Experience with batch ETL Experience with Git and CI/CD within the context of data engineering ', 'Orchestration experience with frameworks like Airflow', '401(k) retirement plan and stock options ', 'Qualifications']",Entry level,Full-time,Information Technology,Information Technology and Services,2020-12-30 19:31:23
Data Engineer,Workday,"Pleasanton, CA",18 hours ago,Be among the first 25 applicants,"['', 'BS/MS in computer science or equivalent is required', ' Strong experience in one or more programming languages for processing of large data sets, such as Python, Scala. ', ' \u200bPrior experience with CRM systems like SFDC is required.  ', 'Develop and automate high-performance data processing systems to drive Workday business growth and improve the product experience.', ' 6+ years of experience designing and building scalable and robust data pipelines to enable data-driven decisions for the business. ', ' Experience building analytical solutions to Sales and Marketing teams. ', 'Do what you love. Love what you do. ', 'Develop and automate high-performance data processing systems to drive Workday business growth and improve the product experience.Evangelize high quality software engineering practices towards building data infrastructure and pipelines at scale.Build reliable, efficient, testable, & maintainable data pipelines.Design and Develop data pipelines using Metadata driven ETL Tools and Open source data processing frameworks.Hands-on experience with source version control, continuous integration and experience with release/change management delivery tools.Provide production support and resolve high priority incidents and the development coding issues.Work with cross functional teams to enable data insights though Data lifecycle.', 'Evangelize high quality software engineering practices towards building data infrastructure and pipelines at scale.', 'Design and Develop data pipelines using Metadata driven ETL Tools and Open source data processing frameworks.', 'Build reliable, efficient, testable, & maintainable data pipelines.', 'Extensive experience in troubleshooting data issues, analyzing end to end data pipelines and in working with users in resolving issues', 'Ability to create data models, STAR schemas for data consuming.', 'Should be proficient in writing advanced SQLs, Expertise in performance tuning of SQLs', ' 6+ years of experience designing and building scalable and robust data pipelines to enable data-driven decisions for the business.  \u200bPrior experience with CRM systems like SFDC is required.   Experience building analytical solutions to Sales and Marketing teams. Experience with very large-scale data warehouse and data engineering projectsExperience developing low latency data processing solutions like AWS Kinesis, Kafka, Spark Stream processing.Should be proficient in writing advanced SQLs, Expertise in performance tuning of SQLs Experience working with AWS data technologies like S3, EMR, Lambda, DynamoDB, Redshift etc.  Strong experience in one or more programming languages for processing of large data sets, such as Python, Scala. Ability to create data models, STAR schemas for data consuming.Extensive experience in troubleshooting data issues, analyzing end to end data pipelines and in working with users in resolving issuesBS/MS in computer science or equivalent is required', 'Work with cross functional teams to enable data insights though Data lifecycle.', ' Experience working with AWS data technologies like S3, EMR, Lambda, DynamoDB, Redshift etc. ', 'Job Responsibilities:\xa0', 'Qualifications:\xa0', 'Experience with very large-scale data warehouse and data engineering projects', 'Provide production support and resolve high priority incidents and the development coding issues.', 'Experience developing low latency data processing solutions like AWS Kinesis, Kafka, Spark Stream processing.', 'Hands-on experience with source version control, continuous integration and experience with release/change management delivery tools.', 'Job Description']",Not Applicable,Full-time,Information Technology,Computer Software,2020-12-30 19:31:23
Data Engineer,Federal Reserve Bank of Boston,"Boston, MA",10 hours ago,Be among the first 25 applicants,"['', 'Job Summary', ' Works with stakeholders - both business and IT partners - to develop and analyze business intelligence needs', 'Analytics', 'Primary Location', ' Integrates data from one or more source systems into data repositories that are optimized for reporting and analytics', ' Performs other related duties as assigned.', ' Designs and develops reports, dashboards, and other data visualization solutions to meet business needs', 'Full-time / Part-time', 'Travel', 'Knowledge', ' Monitors data transmissions, provides production support 24 hrs x 7 days a week', ' Analyzes program needs and translates them into data warehousing and data mart requirements', 'Data Engineer', 'Decision Making', ' Recognizes and resolves conflicts between data models, ensuring consistency and compliance with enterprise standards', 'Shift', 'Job Qualifications', ' Translates business requirements and problems into conceptual, logical, and physical data models', ' Understands key business drivers and can translate into data requirements to provide insight and solutions', 'Monitors data transmissions, provides production support 24 hrs x 7 days a week', ' Understands key business drivers and can translate into data requirements to provide insight and solutions Works with stakeholders - both business and IT partners - to develop and analyze business intelligence needs Designs and develops reports, dashboards, and other data visualization solutions to meet business needs Analyzes program needs and translates them into data warehousing and data mart requirements Integrates data from one or more source systems into data repositories that are optimized for reporting and analytics Constructs, implements and operational data stores, data lakes, and data marts Recognizes and resolves conflicts between data models, ensuring consistency and compliance with enterprise standards Translates business requirements and problems into conceptual, logical, and physical data models Monitors data transmissions, provides production support 24 hrs x 7 days a week Performs other related duties as assigned.', 'Job Type', ' Constructs, implements and operational data stores, data lakes, and data marts', 'Overtime Status', 'Principal Accountabilities', 'Job Sensitivity', 'Job Description', 'Employee Status']",Entry level,Full-time,Information Technology,Information Technology and Services,2020-12-30 19:31:23
Data Engineer,CRS,"Baltimore, MD",16 hours ago,Be among the first 25 applicants,"['', 'Job Description']",Entry level,Full-time,Information Technology,Information Technology and Services,2020-12-30 19:31:23
Data Engineer,Sketchy,United States,7 hours ago,Be among the first 25 applicants,"['', 'Keep our data separated and secure across national boundaries through multiple data centers', 'SketchyGroup LLC is an Equal Opportunity Employer. SketchyGroup LLC encourages women and minorities to apply and does not and will not discriminate on the basis of race, religion, color, sex, age, sexual orientation, marital status, national origin, disability or any other basis prohibited by applicable law.', 'Experience with object-oriented/object function scripting languages', 'Must have at least an undergraduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field', 'Must have experience in building reports, dashboards, and/or data modeling layers in Looker, Tableau or Mode', 'Minimum of 5 years in Data EngineeringMust have at least an undergraduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative fieldAdvanced working SQL knowledge and experience working with relational databases, query authoring (SQL)Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data storesExperience building and optimizing ‘big data’ data pipelines, architectures and data sets.Strong analytic skills related to working with unstructured datasetsMust have experience in building reports, dashboards, and/or data modeling layers in Looker, Tableau or ModeExperience with relational SQL and NoSQL databasesExperience with data pipeline and workflow management toolsExperience with GCP servicesExperience with object-oriented/object function scripting languagesSelf-starter who is excited to be part of a growing startup companyAble to get into the weeds and propose and implement solutions without hand holding', 'Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery re-designing infrastructure for greater scalability and usability', 'Minimum of 5 years in Data Engineering', 'Great Benefits including 99 % Coverage of Medical, dental, vision. Up to 4% match on 401k and more!', 'Competitive compensation plan', 'What you bring to the role:', 'Create and maintain optimal data pipeline architecture', 'Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.', 'Experience with GCP services', 'Generous PTO package with floating holidays', '\ufeff', 'Experience with data pipeline and workflow management tools', 'Self-starter who is excited to be part of a growing startup company', 'Location:\xa0', 'Strong analytic skills related to working with unstructured datasets', 'Competitive compensation planInnovative, high growth and collaborative culture.Generous PTO package with floating holidaysFun team events (Monthly and virtual for now)Great Benefits including 99 % Coverage of Medical, dental, vision. Up to 4% match on 401k and more!', 'Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL)', 'Create and maintain optimal data pipeline architectureIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery re-designing infrastructure for greater scalability and usabilityBuild analytics tools that utilize the data pipeline to provide actionable insightsBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sourcesKeep our data separated and secure across national boundaries through multiple data centers', 'Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources', 'Able to get into the weeds and propose and implement solutions without hand holding', 'What We Offer', 'The Job:', 'What you will do:', 'Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores', 'Innovative, high growth and collaborative culture.', 'Location:\xa0Remote |California | Hawaii | Illinois | New York | Colorado | Massachusetts | Washington', 'Experience with relational SQL and NoSQL databases', 'Fun team events (Monthly and virtual for now)', 'We are looking for a Data Engineer who is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross-functional teams. You will play an integral role in helping us become more data-aware as a company and enabling data insights across our teams.', 'Build analytics tools that utilize the data pipeline to provide actionable insights']",Entry level,Full-time,Information Technology,Higher Education,2020-12-30 19:31:23
Data Engineer,KORE1,Greater Boston,10 hours ago,Be among the first 25 applicants,"['', 'Proven experience working with structured and semi-structured data', 'Works independently and collaboratively to resolve problems to ensure on time delivery', 'My client is a growing IT Services company with clients in the Boston area. They need a data engineer who can build an end to end pipeline from source to BI reports.\xa0The stack they are using is Fivetran and Snowflake, and we will be helping their client decide between Birst and PowerBI for dashboards/reports (they have both).', '5+ years of experience with Data warehousing methodologies and modelling techniques', 'Advanced SQL skills and comfort with query engines such as Athena, PrestoDB, Snowflake, BigQuery.', 'Experience working with diverse environments \xa0\xa0', 'Exceptional customer relations skills to engage and service customers effectively', 'Education and Experience: ', '2+ years of hands-on experience in Cloud technologies such as', 'AWS - S3, Glacier, EC2, Lambda, SQS, Redshift', 'Knowledge of presentation tools such as ThoughtSpot, Tableau, Looker, or Power BI', 'The key is that this candidate must have solid sql skills and have had experience with Fivetran and Snowflake.', 'Proficiency with scripting languages (JavaScript, Python, or Ruby)', 'Maintains a high level of confidentiality, professionalism and a courteous demeanor', 'Advanced SQL skills and comfort with query engines such as Athena, PrestoDB, Snowflake, BigQuery.Proficiency with scripting languages (JavaScript, Python, or Ruby)Comfort with Linux and Windows command line functionalityExposure to multiple cloud providers: AWS, Azure, Google CloudKnowledge of presentation tools such as ThoughtSpot, Tableau, Looker, or Power BIStrong analytic skills related to working with unstructured datasetsWorking knowledge of message queues, stream processing, and highly scalable ‘big data’ data storesSolid understanding of API and data securityExperience working with diverse environments \xa0\xa0Exceptional customer relations skills to engage and service customers effectivelyStrong organizational, multi-tasking, detail-oriented and time management skills\xa0\xa0Consistently maintains a high attention to detail\xa0\xa0Works independently and collaboratively to resolve problems to ensure on time deliveryStrong documentation skills \xa0\xa0Adaptability to changing priorities, as needed \xa0\xa0Maintains a high level of confidentiality, professionalism and a courteous demeanor', 'Strong analytic skills related to working with unstructured datasets', '5+ years of experience in database and data pipeline development5+ years of experience with Data warehousing methodologies and modelling techniques2+ years of experience working in Massively Parallel Processing (MPP) Analytical Datastores such as: Netezza, Teradata2+ years of hands-on experience in Cloud technologies such asAWS - S3, Glacier, EC2, Lambda, SQS, RedshiftAzure - ADLS, Virtual Machine, Functions, SQL DatawarehouseGCP - Cloud Storage, Compute Engine, Cloud Functions, Big QueryProven experience working with both cloud and on-premise DBMSs such as: Snowflake, RedShift, BigQuery, SQL Server, and OracleProven experience working with structured and semi-structured dataHands-on experience working with and creating RESTful web services and APIs', 'Skills:\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0', '\xa0', 'Proven experience working with both cloud and on-premise DBMSs such as: Snowflake, RedShift, BigQuery, SQL Server, and Oracle', 'Hands-on experience working with and creating RESTful web services and APIs', 'Exposure to multiple cloud providers: AWS, Azure, Google Cloud', 'Comfort with Linux and Windows command line functionality', 'Strong organizational, multi-tasking, detail-oriented and time management skills\xa0\xa0', 'Strong documentation skills \xa0\xa0', 'GCP - Cloud Storage, Compute Engine, Cloud Functions, Big Query', 'Azure - ADLS, Virtual Machine, Functions, SQL Datawarehouse', 'Solid understanding of API and data security', 'Consistently maintains a high attention to detail\xa0\xa0', 'Working knowledge of message queues, stream processing, and highly scalable ‘big data’ data stores', '5+ years of experience in database and data pipeline development', 'Adaptability to changing priorities, as needed \xa0\xa0', '2+ years of experience working in Massively Parallel Processing (MPP) Analytical Datastores such as: Netezza, Teradata']",Mid-Senior level,Contract,Information Technology,Staffing and Recruiting,2020-12-30 19:31:23
Data Engineer,"Genome Medical, Inc.","South San Francisco, CA",6 hours ago,Be among the first 25 applicants,"['', 'Position Summary:', '3+ years of integrating datasets between multiple microservices using one or more of SQL/noSQL databases', 'Uphold company values and contribute to making Genome Medical a great place to work.', 'Experience building backends of machine learning pipelines is a plus', 'Take full end-end ownership of new features and be a key point of contact from engineering for cross-functional stakeholders showing positivity and great communication skills', ' Design and implement scalable robust, and secure data lake and/or data warehouse solutions Take full end-end ownership of new features and be a key point of contact from engineering for cross-functional stakeholders showing positivity and great communication skills Help augment our genomics platform with new reporting functionalities to augment the business intelligence team Adhere to flexible, simple, well-documented, and secure design of internal and external-facing services and be an ardent preacher of it for the rest of the team Participate in design reviews and code reviews to further shape a healthy diligent technical culture Uphold company values and contribute to making Genome Medical a great place to work. ', '2+ years of experience designing/developing high performance ETL systems on top of queueing technology components like Kafka, RabbitMQ, Apache Storm is a plus', '3+ years of software development experience, from application design through implementation', 'Genome Medical is proud to be an ', 'Design and implement scalable robust, and secure data lake and/or data warehouse solutions', ""celebrate our employees' differences, regardless of race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, or Veteran status."", 'Desired Experience, Skills, And Competencies', 'equal opportunity employer. We ', '4+ years of experience writing production backend code in Python (preferred) or Java on Linux systems', 'Help augment our genomics platform with new reporting functionalities to augment the business intelligence team', 'Participate in design reviews and code reviews to further shape a healthy diligent technical culture', 'Knowledge of software engineering practices including coding standards, code reviews, SCM, CI/CD in a containerized environment (e.g. Docker)', ' 3+ years of software development experience, from application design through implementation 2+ years of experience building data warehousing technical components such as ETL, ELT, databases and reporting 4+ years of experience writing production backend code in Python (preferred) or Java on Linux systems 3+ years of integrating datasets between multiple microservices using one or more of SQL/noSQL databases 2+ years of experience designing/developing high performance ETL systems on top of queueing technology components like Kafka, RabbitMQ, Apache Storm is a plus Experience building backends of machine learning pipelines is a plus Knowledge of software engineering practices including coding standards, code reviews, SCM, CI/CD in a containerized environment (e.g. Docker) ', 'Responsibilities', 'Job Title: ', 'Adhere to flexible, simple, well-documented, and secure design of internal and external-facing services and be an ardent preacher of it for the rest of the team', 'About Genome Medical:', '2+ years of experience building data warehousing technical components such as ETL, ELT, databases and reporting']",Entry level,Full-time,Information Technology,Information Technology and Services,2020-12-30 19:31:23
Data Engineer,MSH Talent Solutions,"Miami, FL",5 hours ago,Be among the first 25 applicants,"['', '• Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.', '• Strong analytic skills related to working with structure and unstructured datasets.', '• Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using ingestion technics and AWS technologies.', '• Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.', 'With an ecosystem of 4,500 locations and a team of 15,000 people, REEF is the largest operator of mobility, logistics hubs, and neighborhood kitchens in North America.', '• Experience in Scala, Python, pySpark, Java, Rest API, Microservices etc.', '• Work with data and analytics experts to strive for greater functionality in our data systems.', '• Create and maintain optimal data pipeline architecture.', '• We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science or another quantitative field. They should also have experience using the following software/tools:', '• Experience with AWS cloud services: EC2, EMR, RDS, Redshift, ECS', 'What You’ll Do', 'Data Engineer', '• Experience with big data tools: Hadoop, Hdfs, Spark, Hive, Sqoop, Kafka, Yarn, Zookeeper etc.', '• Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.', '• A successful history of manipulating, processing and extracting value from large disconnected datasets.', '• Familiar with data platform like Cloudera, Hortonworks', 'What We Want From You:', 'Location: Miami', '• Work with stakeholders including internal and external to assist with data-related technical issues and support their data infrastructure needs.', '• Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.', 'Together we are leveraging the power of proximity to keep our communities moving forward in a sustainable and thoughtful way.', '• Experience supporting and working with cross-functional teams in a very fast dynamic environment.', '• Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.', 'We transform underutilized urban spaces into neighborhood hubs that connect people to locally curated goods, services, and experiences.', '• Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.', '• Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.', '• Build processes supporting data transformation, data structures, metadata, dependency and workload management.', '• Assemble large, complex data sets that meet functional / non-functional business requirements.', '• Strong project management and organizational skills.', '• In-depth technical knowledge and hands-on experience in the areas of Data Management, BI Architecture, Product Development, RDBMS and non-RDBMS platforms.', '• Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.', '• Experience with relational SQL and NoSQL databases, including redshift, Postgres and Cassandra.', '• Experience with stream-processing systems: Kafka, Storm, Spark-Streaming, etc.']",Mid-Senior level,Full-time,Information Technology,Management Consulting,2020-12-30 19:31:23
Data Engineer,Forrest Solutions,New York City Metropolitan Area,,N/A,"['', 'Develop tools to improve data flows between internal and/or external systems and the data warehouse', '4.\xa0\xa0\xa0\xa0\xa0\xa0PostgreSQL', ""You'll use analytics architecture to measure the improvement your work has on millions of users. We strongly believe in adding incremental value, so your work will reach those users in a matter of days, if not hours, using our Continuous Deployment strategies."", ""As an early member of our team, you'll provide significant strategic and technical guidance. You'll help us solve some of the many technical challenges that still lie ahead, have a direct impact on shaping our engineering culture, and will work alongside the rest of our team to lay out our company’s roadmap. The experience you will gain will be unique and unmatched."", 'Develop data pipelines to process structured and unstructured data in near real-time', '2.\xa0\xa0\xa0\xa0\xa0\xa0Apache beam', '\xa0Implement methods to improve data reliability and quality', '\xa0Analyzing raw data\xa0Developing and maintaining datasets\xa0Improving data quality and efficiency\xa0Implement methods to improve data reliability and qualityDevelop and test architectures that enable data extraction and transformation for predictive or prescriptive modeling', '\xa0Developing and maintaining datasets', '\xa0Improving data quality and efficiency', 'Data Engineer', 'Automate the data collection and analysis processes, data releasing and reporting tools', 'Develop and test architectures that enable data extraction and transformation for predictive or prescriptive modeling', 'We are looking for a Data Engineer for our client.', 'Required Qualifications', '3.\xa0\xa0\xa0\xa0\xa0\xa0DBA skills', 'NO SPONSORSHIP PROVIDED', '\xa0Strong analytical skills and the ability to combine data from different sources', '\xa0', '\xa0Analyzing raw data', 'Responsibilities', 'Tech MUST-HAVES for this role', '1.\xa0\xa0\xa0\xa0\xa0\xa0Apache\xa0Superset', 'The ideal candidate is an expert in building and maintaining data systems, has strong analytical skills, and the ability to combine data from different sources.']",Mid-Senior level,Full-time,Information Technology,Computer Software,2020-12-30 19:31:23
Senior Data Engineer,Great American Insurance Group,"Cincinnati, OH",6 hours ago,Be among the first 25 applicants,"['', 'Education:', 'Qualifications:', 'Complete descriptive analyses on various data sets', 'Excellent problem solving skills', '4+ years prior enterprise IT experience required', 'Responsibilities:', 'Understanding of ETL techniques and processes (Enterprise Informatica PowerCenter)', 'Previous experience in the P&C insurance industry Software Engineering practices developing enterprise applications – Java, Spring, XML, JDBC/JPA/Hibernate Familiar with approximate string matching techniques (fuzzy matching)Hadoop Development– interfacing with data stored in Hadoop environment (Familiar with technologies including: Hive, Pig, Spark, HDFS, Sqoop, Flume, HAWQ, Zeppelin)Informatica Data Quality Suite & Informatica Data Integration Suite (PowerCenter)R/R-Studio and/or PythonText Mining / Natural Language Processing Azure Databricks / Cognitive Services', 'R/R-Studio and/or Python', 'Advanced technical and business knowledge with ability to embrace new technology', 'Informatica Data Quality Suite & Informatica Data Integration Suite (PowerCenter)', 'Experience:', 'Strong SQL and database knowledge (Oracle preferred)Understanding of ETL techniques and processes (Enterprise Informatica PowerCenter)Strong Excel knowledge/experience including Macros and VB developmentSOAP and REST web service experience testing and developmentExperience with command line utilities (Linux, Unix, Windows, etc)Report development/design experience (Tableau / Cognos)', 'Contributes beyond job role and responsibilities', 'Strong SQL and database knowledge (Oracle preferred)', 'SOAP and REST web service experience testing and development', 'Education: Bachelor’s degree or higher in Information Technology, Informatics, Computer Science, Information Systems, or equivalent experience', 'Research business unit queries regarding model outputs; this includes score shifts, missing items, reason messages, etc.', 'Acquire and manipulate internal and external data to create clean, reproducible data sets to facilitate predictive modeling', 'Integrates multiple concepts across job functions with a goal of overall benefit to the organization', 'Design and implement database structures for modeling solutions (DDL / DML and ER-Diagrams)', 'Self-motivated team player who excels in a collaborative environment', 'Text Mining / Natural Language Processing ', 'Software Engineering practices developing enterprise applications – Java, Spring, XML, JDBC/JPA/Hibernate ', 'Familiar with approximate string matching techniques (fuzzy matching)', 'Experience: 6+ years of relevant experience', 'Work with Information Technology to develop production solutions to bring predictive analytics to the enterprise', '4+ years prior enterprise IT experience requiredExcellent problem solving skillsSuperior organizational leadership skillsIntegrates multiple concepts across job functions with a goal of overall benefit to the organizationAbility to communicate, develop and leverage strategic business relationships across the organization and externallyAdvanced technical and business knowledge with ability to embrace new technologyStrong knowledge of the Software Development Lifecycle and agile methodologiesSelf-motivated team player who excels in a collaborative environmentContributes beyond job role and responsibilities', 'Be Here. Be Great. Working for a leader in the insurance industry means opportunity for you. Great American Insurance Group’s member companies are subsidiaries of American Financial Group, a Fortune 500 company. We combine a ""small company"" culture where your ideas will be heard with ""big company"" expertise to help you succeed. With over 30 specialty property and casualty operations and a variety of financial services, there are always opportunities here to learn and grow.', 'Beneficial Technical Skills and Business Experience:', 'Hadoop Development– interfacing with data stored in Hadoop environment (Familiar with technologies including: Hive, Pig, Spark, HDFS, Sqoop, Flume, HAWQ, Zeppelin)', 'Research and evaluate new methods and tools to improve data gathering processes', 'Technical Skill Requirements:', 'Build comprehensive data sets from various source systems including relational data warehouses/marts, NoSQL data stores, REST API’s, structured and semi-structured flat files, etc.', 'Be Here. Be Great. ', 'Strong knowledge of the Software Development Lifecycle and agile methodologies', 'Strong Excel knowledge/experience including Macros and VB development', 'Ability to communicate, develop and leverage strategic business relationships across the organization and externally', 'Azure Databricks / Cognitive Services', 'Work with project team(s) and business stakeholders to determine data requirements for analysis', 'Experience with command line utilities (Linux, Unix, Windows, etc)', 'Previous experience in the P&C insurance industry ', ""Great American's Predictive Analytics division is looking for a Senior Data Engineer to join their growing and dynamic team."", 'Work with project team(s) and business stakeholders to determine data requirements for analysisAcquire and manipulate internal and external data to create clean, reproducible data sets to facilitate predictive modelingBuild comprehensive data sets from various source systems including relational data warehouses/marts, NoSQL data stores, REST API’s, structured and semi-structured flat files, etc.Work with Information Technology to develop production solutions to bring predictive analytics to the enterpriseResearch and evaluate new methods and tools to improve data gathering processesDesign and implement database structures for modeling solutions (DDL / DML and ER-Diagrams)Complete descriptive analyses on various data setsResearch business unit queries regarding model outputs; this includes score shifts, missing items, reason messages, etc.', 'Report development/design experience (Tableau / Cognos)', 'Superior organizational leadership skills']",Mid-Senior level,Full-time,Information Technology,Insurance,2020-12-30 19:31:23
Data Engineer,Lenmar Consulting Inc,"New Jersey, United States",9 hours ago,Be among the first 25 applicants,"[' o\xa0\xa0\xa0Overseeing the HCP/HCF data and ensuring they are correctly de-duplicated.', ' o Providing input on data mastering strategy', ' E: atatiparti@lenmarit.com', '', ' Deliverables:', ' o Building queries/coding sequences to automate data ingestion', ' O: 201.946.1777 ext. 4011', ' o Acting as a representative to cross-functional teams working to align overall data strategy', ' Education and Experience Requirements/Qualifications:\xa0', ' •\xa0Steward the underlining business data behind the operational system', 'Deliverables:', ' •\xa0Data extraction and uploads', ' ADVANCED LEVEL:', ' Working with the Feasibility Center of Excellence, the Data Sciences & Clinical Insights group and other client functions, to drive the definition and documentation of source data business rules for various data domains, and to manage the integration of that data across multiple systems and functions.\xa0', ' Thanks', ' o\xa0\xa0\xa0Assist with mapping reference values (LOVs) from various sources into the Janssen agreed values.', ' Contract 2yrs with extensions-  Open to C2C or W2', ' Sr. Technology Recruiter', ' Services Overview:', ' Amita Tatiparti', ' o Ability to convert data extracts into required formats. Knowledge of R and/or Python is preferred.', 'Remote\xa0Opportunity', 'Services Overview:', ' Lenmar Consulting, Inc. [a Kellton Tech Company]', 'My Client has a Remote "" Data Engineer"" opportunity.\xa0If interested, please contact Amita @ 3475170317 OR email your resume to atatiparti@lenmarit.com', '\xa0', ' •\xa0Fluent in English, strong written and verbal communication skills', ' •\xa0ADVANCED LEVEL:', 'Remote "" Data Engineer"" opportunity.\xa0', ' o\xa0\xa0\xa0Provide input to standardizing the Primary Indication of clinical trials coming from various sources.', ' ', ' Remote\xa0Opportunity\xa0', ' o Ability to build macros within Excel', 'Contract 2yrs with extensions-  Open to C2C or W2', ' D: 201.744.2278 M: 347.517.0317', ' o Ability to write sequel code and build data queries in SQL', ' •\xa0Experience in data analytics, metrics optimization, and/or data warehousing is required', ' •\xa0Experience in working in a matrix / cross-functional environment with a high degree of collaboration is preferred', '•\xa0ADVANCED LEVEL', ' Requires a deep understanding of data flow, warehousing, and governance to be able to support issue resolution and participate in related process and infrastructure improvement projects that relate to Feasibility and Data Science.', ' Harborside 5, Jersey City, NJ 07311', ' •\xa0Perform data wrangling/stewardship activities such as data pre-processing and formatting', ' •\xa0Partner with the Business users in formulating the correct metrics in line with the organization’s business rules', ' •\xa0A Bachelor of Science (BSc) or equivalent is required. Preference for degree in Computer Science, Information Systems, Mathematics, or Statistics.']",Mid-Senior level,Contract,Information Technology,Pharmaceuticals,2020-12-30 19:31:23
Data Engineer,Science 37,United States,5 hours ago,42 applicants,"['', 'Duties include but are not limited to:', 'Qualifications:', 'Some Experience with Cloud Computing management on the AWS platform', 'Working with cloud distributed file systems, data lakes, and data warehouses', 'Experience using SQL, NoSQL, and Graph Databases', 'Have an understanding of data architecture for microservices', 'Preferred Qualifications', 'As part of the Science 37 Tech team, the Data Engineer collaborates with motivated, energetic, and entrepreneurial individuals working together to achieve Science 37’s mission of changing the world of clinical research through patient-centered design. They have a hands-on role in building and developing the data pipeline/platform that enables Science 37’s groundbreaking clinical research model and collaborates with Product, Data, Clinical Operations, and other relevant stakeholders to define study-specific platform requirements.', 'Experience with JIRA, Confluence, SpiraTest is a plus', 'Creating a data pipelines to help with Internal and External analytics users', 'Utilize an understanding of Agile management to help the team with all release and configuration related tasks around software builds into preproduction and production environments.', 'Some Experience with highly available database technologies like clustering, replication, mirroring, etc.', 'Experience with data tools like Jupyter', 'Knowledge of administration, replication, backup, and restore of relational databases', 'Recommend operational efficiencies, eliminate duplicate work efforts and remove unnecessary complexities; create and implement new procedures and workflows', 'Working with cloud vendors like AWS or GCP', 'Experience with SAFe methodology', 'Experience with MuleSoft Anypoint Platform and Dataweave a plus.', ""Bachelor's degree in Computer Science or equivalentKnowledge architectural & database design skillsExperience using SQL, NoSQL, and Graph DatabasesMust have experience with AWS (other cloud providers are a plus)Scripting experience with Python or Bash required.Proficient with SQL and Programming Languages like Python, Java, or ScalaHave an understanding of data architecture for microservicesExperience across different database platforms and tools such as MySQL, PostgreSQL, SQL Server, DynamoDB, MongoDB, AWS Neptune, Cassandra, Neo4jExperience designing and building data lake and data warehouse solutionsLinux Server basic hands-on admin experience.Some Experience with Cloud Computing management on the AWS platformExperience with Monitoring/Alert planning for data services.Some Experience with highly available database technologies like clustering, replication, mirroring, etc.Knowledge of administration, replication, backup, and restore of relational databasesExperience with data tools like Jupyter"", 'Knowledge architectural & database design skills', 'Proficient with SQL and Programming Languages like Python, Java, or Scala', 'Understand how to install, configure, monitor, and maintain databases in the production, development, testing environments', 'Data Engineer ', 'Optimize database performance by identifying and resolving application bottlenecks, tuning of DB queries, implementation of stored procedures, conducting performance tests, troubleshooting and integrating new elements', 'Understand how to install, configure, monitor, and maintain databases in the production, development, testing environmentsWorking with cloud vendors like AWS or GCPWorking with cloud distributed file systems, data lakes, and data warehousesCreating a data pipelines to help with Internal and External analytics usersDefine and implement database schemas and configurations working with our development teamsOptimize database performance by identifying and resolving application bottlenecks, tuning of DB queries, implementation of stored procedures, conducting performance tests, troubleshooting and integrating new elementsWork with development team design and implement reporting capabilitiesImplement solutions for database performance monitoring and tuningRecommend operational efficiencies, eliminate duplicate work efforts and remove unnecessary complexities; create and implement new procedures and workflowsProcess database change requests, including the creation and modification of databases, tables, views, stored procedures, triggers, jobs, etc. in accordance with change control policiesUtilize an understanding of Agile management to help the team with all release and configuration related tasks around software builds into preproduction and production environments.', ""Science 37 is accelerating the research and development of breakthrough biomedical treatments by bringing clinical trials to patients' homes. By leveraging the latest innovations in mobile technology, cloud services, telemedicine, we are breaking down traditional geographic barriers to patient trial participation while shortening the time needed to bring new treatments to the market."", 'Linux Server basic hands-on admin experience.', ""Bachelor's degree in Computer Science or equivalent"", 'Experience with CSV (Computer Systems Validation)', 'Implement solutions for database performance monitoring and tuning', 'Experience across different database platforms and tools such as MySQL, PostgreSQL, SQL Server, DynamoDB, MongoDB, AWS Neptune, Cassandra, Neo4j', 'Experience in Clinical Trials and/or life science industry', 'Define and implement database schemas and configurations working with our development teams', 'Work with development team design and implement reporting capabilities', 'Experience designing and building data lake and data warehouse solutions', 'Experience with MuleSoft Anypoint Platform and Dataweave a plus.Experience in Clinical Trials and/or life science industryUnderstanding of regulatory framework for software deliveryExperience with operational efficiency improvement initiativesExperience with CSV (Computer Systems Validation)Experience with SAFe methodologyExperience with JIRA, Confluence, SpiraTest is a plus', 'Experience with Monitoring/Alert planning for data services.', 'Experience with operational efficiency improvement initiatives', 'The Data Engineer helps drive data democracy at Science. This position will report to the Data Architect and will work with our architects, software engineers, product managers, and DevOps to help design and build data solutions and architecture.\xa0They will learn how Science 37 data is used and help make and drive the accessibility of the data that is needed, keeping in mind regulations and data privacy policies.\xa0They will be able to use data processing libraries and tools to help the end-users of our data get the insights they need.', 'Process database change requests, including the creation and modification of databases, tables, views, stored procedures, triggers, jobs, etc. in accordance with change control policies', 'Must have experience with AWS (other cloud providers are a plus)', 'Scripting experience with Python or Bash required.', 'Understanding of regulatory framework for software delivery']",Mid-Senior level,Full-time,Information Technology,Research,2020-12-30 19:31:23
Data Engineer,Envision,Greater St. Louis,7 hours ago,Be among the first 25 applicants,"['', 'Desired Skills/Experience:', '• Experience working with multidisciplinary research and field teams', '• Ability to construct a storyline, perform analyses, and create presentations to effectively communicate complex concepts and recommendations', '• Understanding of Database systems and management of large data sets', ""• Bachelor's degree in Computer Science, Data Science, Ag/Life Sciences, or related field"", 'Must Have Spotfire experience', '• Strong data analysis skills utilizing tools such as Spotfire (is a must), Tableau, Pipeline Pilot, or SQL; API consumption/development experience.', '*No C2C or sponsorship, must be our W2 employee*', 'Required Skills/Experience:', 'Spotfire Data Engineer, St. Louis, MO', '• Ideal candidates will have a strong developmental desire to continually learn new technical skills to remain on the cutting edge of capabilities for data analysis and visualization.', '• Experience and skills with project management and communication of strategy with both technical and non-technical audiences.', 'Will consider OPT candidates\xa0if valid to 2022', '• Experience developing Business Intelligence Visualization, SQL, R\xa0http://www.envision.com/jobs/index.html#/jobs/71084', '• In addition to strong technical skills, the successful candidate must excel in time management; and have a proven track record in problem solution leveraging complex data.']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2020-12-30 19:31:23
Data Engineer,Walker Edison,"Salt Lake City, UT",2 hours ago,47 applicants,"['Qualifications:', 'Work closely with Analyst to provide the data and support they need.Modify, build, and maintain data warehouse tables and pipelines through various tools like SQL, python, and AWS stack.Wrangling data from disparate systems and sources such as RDBMS, flat files, and API connections.Optimization of existing SQL queries to improve reliability and performance.Lead the way in data warehouse documentation such as data pipelines, definitions, and warehouse change impacts.Can work remotely and independently under minimal supervision to achieve objectives.', 'The selected individual will be responsible for but not limited to the following:', 'Modify, build, and maintain data warehouse tables and pipelines through various tools like SQL, python, and AWS stack.', 'Basic understanding of Datawarehouse models like Kimbal and Inmon.', 'Familiar with Apache Airflow, DOMO, various AWS cloud platform tools.', 'Optimization of existing SQL queries to improve reliability and performance.', 'Walker Edison Furniture is a market-leading, e-commerce drop ship business in the home furnishings realm with an unwavering focus on customer satisfaction every step of the way! Our brand and designs represent our desire to break away from the standard mold and help our customers ""Live Outside The Box."" This is an exciting time to join our expanding operations. We are an established business with huge future potential -- don’t hesitate to apply and join our team!', 'Can work remotely and independently under minimal supervision to achieve objectives.', 'Benefits', 'Data Engineer', 'Bachelor’s degree in information systems, data analytics, or related field.', 'RDBMS: Redshift, Postgres.Experience with visualization tools such as Tableau, Looker, and DOMO.Coding experience with Python, R, and other languages.Experience with web scraping.Familiar with Apache Airflow, DOMO, various AWS cloud platform tools.', 'Should be familiar with terms like primary key, granularity, index, additive attributes, views.', 'In addition to their technical skills they are not afraid to work with people. They will work closely with Analyst to offer support and help with any fixes, questions, and request. \xa0', 'Preferred Qualifications:', 'RDBMS: Redshift, Postgres.', 'Experience with ETL processes and good data warehouse principles.', '2+ years SQL experience using CTE, window functions, and complex joins.', 'Work closely with Analyst to provide the data and support they need.', 'Bachelor’s degree in information systems, data analytics, or related field.2+ years SQL experience using CTE, window functions, and complex joins.Experience with ETL processes and good data warehouse principles.Basic understanding of Datawarehouse models like Kimbal and Inmon.Should be familiar with terms like primary key, granularity, index, additive attributes, views.', 'About the Company', 'Experience with web scraping.', 'Walker Edison is an Equal Opportunity Employer.', 'Benefits include company subsidized medical and dental insurance as well as vision insurance, life insurance, 401k matching, paid time off, and holidays. Additional perks include complementary vending machines, massage therapy, and regular company team building activities. Come join our team and be a part of a fun and growing company where we “Work Outside the Box”!', 'Wrangling data from disparate systems and sources such as RDBMS, flat files, and API connections.', 'Experience with visualization tools such as Tableau, Looker, and DOMO.', 'Lead the way in data warehouse documentation such as data pipelines, definitions, and warehouse change impacts.', 'Coding experience with Python, R, and other languages.', 'The Data Engineer is a key member of the Data Engineering team. This is a great position for anyone who gets excited about wrangling and shaping data that drives business decisions. \xa0The Data Engineer will work closely under the direction of senior engineers to modify, build, and maintain the data warehouse and its pipelines.']",Associate,Full-time,Engineering,Furniture,2020-12-30 19:31:23
BI Data Engineer,Cresco Labs,"Pittsburgh, PA",6 hours ago,Be among the first 25 applicants,"['', 'Job Summary', "" Bachelor's degree in computer science or a similar field, a plus 2+ years in a strong analytical background  Practical experience buildings out and implementing dashboards with Tableau/Power BI or other business intelligence and analytics software Strong knowledge in database technologies and web technologies Excellent written and verbal communication skills High level organization and structure Passion for data analytics and business intelligence  "", 'Core Job Duties', ' Must be 21 years of age or older to apply Must comply with all legal or company regulations for working in the industry  ', 'Strong knowledge in database technologies and web technologies', 'Collaborate with business stakeholders and other BI Engineers in identifying opportunities to build and analyze metrics', 'Passion for data analytics and business intelligence ', 'Required Experience, Education And Skills', 'Company Overview', 'High level organization and structure', 'Cresco Labs is an Equal Opportunity Employer and all applicants will be considered without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran, or disability status.', 'Implement data parsing and data cleansing to improve overall data quality and productivity', 'Manage progress, goals, and insights with business leadership', 'Must comply with all legal or company regulations for working in the industry ', 'Practical experience buildings out and implementing dashboards with Tableau/Power BI or other business intelligence and analytics software', ' Design and build dynamic dashboards to support our unique requirements for visualization, security, data access, etc. Collect, manage analyze and visualize large data sets while maintaining ETL pipelines Implement data parsing and data cleansing to improve overall data quality and productivity Collaborate with business stakeholders and other BI Engineers in identifying opportunities to build and analyze metrics Manage progress, goals, and insights with business leadership Stay up to data on data science trends and developments Support the VP of Software Engineering through data analysis and reporting Automate and document processes to ensure efficiency ', 'Stay up to data on data science trends and developments', 'Entrepreneur', 'Design and build dynamic dashboards to support our unique requirements for visualization, security, data access, etc.', 'Automate and document processes to ensure efficiency', 'Support the VP of Software Engineering through data analysis and reporting', '2+ years in a strong analytical background ', 'Collect, manage analyze and visualize large data sets while maintaining ETL pipelines', 'Additional Requirements', 'Excellent written and verbal communication skills', ""Bachelor's degree in computer science or a similar field, a plus"", 'MISSION STATEMENT', 'Must be 21 years of age or older to apply']",Entry level,Full-time,Business Development,Consumer Goods,2020-12-30 19:31:23
Associate Data Engineer,Built Technologies,"Nashville, TN",2 hours ago,Be among the first 25 applicants,"['', 'The Team:', 'Perks:', 'Our implementations team consists of Implementations Project Managers, Construction Lending Professionals, Data and Operations Analysts, and Software Engineers. Our team is motivated, professional, and full of top performers.', 'A dedication to the quality and ownership of your work product.', 'We strive to put people first and work with passion.', 'Delivering great experiences via both technology and our communications to clients and partners. Delivering the right solution at the right time with integrity.Transforming client data into BUILT data standardsAbility to multitask by balancing the needs of several clients Providing high quality, empathetic support to all clients and partners.Continually building and iteratively improving our integrations codebase to be increasingly reusable and enable our customers and partners to do more.Confronting and tackling odd, unique data problems on a weekly basis.Communicating and collaborating with non-technical team members to troubleshoot and resolve when bugs are found.', 'Deep empathy and support of your teammates.', 'Continually building and iteratively improving our integrations codebase to be increasingly reusable and enable our customers and partners to do more.', 'Communicating and collaborating with non-technical team members to troubleshoot and resolve when bugs are found.', 'Delivering great experiences via both technology and our communications to clients and partners. ', 'The codebase also makes heavy use of type hinting.', 'Test coverage is on average over 90% and Pylint is 10/10. We care a lot about quality and safety in our codebase.', 'Collaborative work and great sharing across the team both on the technology and product side. Your input is valued from the day you start.', 'Excellent communication skills.', 'Confronting and tackling odd, unique data problems on a weekly basis.', 'Associate Data Engineer', 'The system is built on Python 3.7, a bit of TypeScript in places, Luigi, Pandas, SQLAlchemy, Docker and AWS. (Don’t know all these things, no worries, there are plenty of opportunities to learn about them here.)The codebase also makes heavy use of type hinting.Test coverage is on average over 90% and Pylint is 10/10. We care a lot about quality and safety in our codebase.', 'Success in this role is measured by:', 'The system is built on Python 3.7, a bit of TypeScript in places, Luigi, Pandas, SQLAlchemy, Docker and AWS. (Don’t know all these things, no worries, there are plenty of opportunities to learn about them here.)', 'A strong focus on our customers both internal and external.', 'Delivering the right solution at the right time with integrity.', 'Ability to multitask by balancing the needs of several clients ', 'What we need you to bring:', 'Strong working knowledge of Python or a comparable programming language, SQL, and MS ExcelExcellent communication skills.A strong focus on our customers both internal and external.A dedication to the quality and ownership of your work product.Deep empathy and support of your teammates.', 'Providing high quality, empathetic support to all clients and partners.', 'You’ll report directly to the Director of Implementation', 'The Platform:', 'Our implementations team consists of Implementations Project Managers, Construction Lending Professionals, Data and Operations Analysts, and Software Engineers. Our team is motivated, professional, and full of top performers.You’ll report directly to the Director of ImplementationCollaborative work and great sharing across the team both on the technology and product side. Your input is valued from the day you start.We strive to put people first and work with passion.', 'Transforming client data into BUILT data standards', 'Strong working knowledge of Python or a comparable programming language, SQL, and MS Excel']",Associate,Full-time,Information Technology,Information Technology and Services,2020-12-30 19:31:23
Data Software Engineer,JPMorgan Chase & Co.,"Houston, TX",3 hours ago,Be among the first 25 applicants,"['', 'BS/BA degree or equivalent experience', 'Strong written and verbal communications skills.', 'Understanding of software skills such as business analysis, development, maintenance, and software improvement', 'Designed and developed scalable data solutions using cloud infra such as AWS, Azure or GCP.', 'Passionate about building an innovative culture', ""5+ years' experience with any of the following data orchestration stack, ingestion frameworks, cloud-native patterns: Apache Airflow, Kafka, Nomad/Terraform, Kubernetes/Docker, etc."", 'Working proficiency in developmental toolsets', 'Ability to manage multiple tasks and thrive in a fast-paced team environment.', 'Ability to work in large, collaborative teams to achieve organizational goals', 'Proficiency in one or more modern programming languages', 'Experience with infrastructure automation technologies like Docker and K8s is huge plus.', 'Understanding of architecture and design across all systems', '3+ years of experience with scripting languages like Python, Bash or PowerShell, etc.', 'Working knowledge of Agile and scrum practices.', 'This role requires a wide variety of strengths and capabilities, including:', 'Exposure with building APIs and services using REST.', 'BS/BA degree or equivalent experience Advanced knowledge of application, data, and infrastructure architecture disciplines Understanding of architecture and design across all systems Working proficiency in developmental toolsets Knowledge of industry-wide technology trends and best practices Ability to work in large, collaborative teams to achieve organizational goals Passionate about building an innovative culture Proficiency in one or more modern programming languages Understanding of software skills such as business analysis, development, maintenance, and software improvement 7+ years of software design and application development experience using any OOPs Languages (C#, C++ JAVA, etc....) 5+ years\' experience with any of the following data orchestration stack, ingestion frameworks, cloud-native patterns: Apache Airflow, Kafka, Nomad/Terraform, Kubernetes/Docker, etc. 3+ years of experience with scripting languages like Python, Bash or PowerShell, etc. Hands-on experience with any of the ""open-source"" distributed ingestion/processing stack like Hadoop, Spark and Kafka is a must. Solid exposure working in data engineering team, and can demonstrate best practices for building robust data controls and governance practices Experience with infrastructure automation technologies like Docker and K8s is huge plus. Exposure with building APIs and services using REST. Designed and developed scalable data solutions using cloud infra such as AWS, Azure or GCP. Ability to manage multiple tasks and thrive in a fast-paced team environment. Strong written and verbal communications skills. Working knowledge of Agile and scrum practices. ', 'Solid exposure working in data engineering team, and can demonstrate best practices for building robust data controls and governance practices', 'Hands-on experience with any of the ""open-source"" distributed ingestion/processing stack like Hadoop, Spark and Kafka is a must.', '7+ years of software design and application development experience using any OOPs Languages (C#, C++ JAVA, etc....)', 'Knowledge of industry-wide technology trends and best practices', 'Advanced knowledge of application, data, and infrastructure architecture disciplines']",Entry level,Full-time,Engineering,Banking,2020-12-30 19:31:23
Data Engineer,Govini,"Pittsburgh, PA",19 hours ago,Be among the first 25 applicants,"['', 'Experience utilizing open-source technologies such as Linux, PostgreSQL', 'Minimum of 3 years direct experience creating sustainable, automated processes for data discovery, curation and synthesis', 'Proficient usage of common data formats such as CSV, XML, and JSON', 'Requires strong analytical ability and attention to detailAbility to work independently with little supervision', ""Bachelor's degree in Computer Science, Mathematics or related technical field Minimum of 3 years direct experience creating sustainable, automated processes for data discovery, curation and synthesis3-5 years experience with programmatically manipulating data Experience with PostgreSQL or similar RDBMSExpert at advanced SQL programmingExperience utilizing open-source technologies such as Linux, PostgreSQLProficient usage of common data formats such as CSV, XML, and JSONRequires strong analytical ability and attention to detailAbility to work independently with little supervisionA burning desire to tackle hard problems and create sustainable solutions "", ' Required Skills ', 'Identify data sources, assess their value and quality and estimate the level of effort required to integrate into existing data model, infrastructure and products.', ""Define and lead Govini's data lifecycle strategy across data acquisition, data ingestion, data cleansing, normalization and linkage"", 'Experience using Amazon Web ServicesExperience in or exposure to the nuances of a startup or other entrepreneurial environmentStrong expertise with scripting languages such as Python, Ruby, Perl3-5 years Master Data Management experience including data consolidation, linkage, federation and dissemination', 'Ensure key entities within datasets are identified, resolved and linked to existing entities within the current master data repository.', 'Build best practices that help with chain of custody of data so it can be easily traced back to the source for accuracy and consistency.', 'Strong expertise with scripting languages such as Python, Ruby, Perl', 'Improve data sharing, increase data repurposing and improve cost efficiency associated with data management efforts. ', 'Apply various techniques to produce solutions to large-scale optimization problems, including data pre-processing, indexing, blocking, field and record comparison and classification.', 'Experience using Amazon Web Services', 'Company Description ', '3-5 years Master Data Management experience including data consolidation, linkage, federation and dissemination', 'Develop, refine and oversee master data management standards, including establishing and enforcing governance procedures and ensuring data integrity across multiple functions. Responsible for owning data quality metrics and meeting defined data accuracy goals according to industry best practices.', 'Expert at advanced SQL programming', 'A burning desire to tackle hard problems and create sustainable solutions ', 'Work directly with users as well as SMEs to establish, create and populate optimal data architectures and structures, as well as articulate techniques and results using non-technical language. ', 'Work across functional teams to understand advanced statistical, machine learning, and text processing models. Incorporate them into Govini’s existing data engineering infrastructure.', ""Define and lead Govini's data lifecycle strategy across data acquisition, data ingestion, data cleansing, normalization and linkageIdentify data sources, assess their value and quality and estimate the level of effort required to integrate into existing data model, infrastructure and products.Ensure key entities within datasets are identified, resolved and linked to existing entities within the current master data repository.Apply various techniques to produce solutions to large-scale optimization problems, including data pre-processing, indexing, blocking, field and record comparison and classification.Develop, refine and oversee master data management standards, including establishing and enforcing governance procedures and ensuring data integrity across multiple functions. Responsible for owning data quality metrics and meeting defined data accuracy goals according to industry best practices.Improve data sharing, increase data repurposing and improve cost efficiency associated with data management efforts. Build best practices that help with chain of custody of data so it can be easily traced back to the source for accuracy and consistency.Work across functional teams to understand advanced statistical, machine learning, and text processing models. Incorporate them into Govini’s existing data engineering infrastructure.Perform exploratory data analyses, generate and test working hypotheses, prepare and analyze historical data and identify patterns.Work directly with users as well as SMEs to establish, create and populate optimal data architectures and structures, as well as articulate techniques and results using non-technical language. "", 'Experience in or exposure to the nuances of a startup or other entrepreneurial environment', ' Desired Skills ', ""Bachelor's degree in Computer Science, Mathematics or related technical field "", 'Perform exploratory data analyses, generate and test working hypotheses, prepare and analyze historical data and identify patterns.', '3-5 years experience with programmatically manipulating data Experience with PostgreSQL or similar RDBMS', 'US Citizenship is Required', ' Job Description ']",Entry level,Full-time,Information Technology,Information Technology and Services,2020-12-30 19:31:23
Data Warehouse Engineer,Bamboo Consulting,"Medford, OR",16 hours ago,Be among the first 25 applicants,"['', ""The Data Warehouse Developer works on a cross-functional team in the support and growth of client's data warehouse.  They will design, build, and maintain ETL processes, data exchanges, and data integrations utilizing tools such as Microsoft SQL Server, Analysis Services, and Integration Services. "", '· \xa0 \xa0 \xa0 Member of rotational 24-hour on call support of ETL processing, databases and applications ', 'Responsibilities: ', 'The Data Warehouse Developer 1 Reports to the Manager of Data and Application Support. ', '· \xa0 \xa0 \xa0 Translates operational and business needs into technical solutions ', '· \xa0 \xa0 \xa0 Develops and maintains C# and XML web services within the Data Warehouse ', '· \xa0 \xa0 \xa0 Helps write and evolve existing documentation and development standards ', '· \xa0 \xa0 \xa0 Maintains and develops ETL processes using SSIS and SQL Server ', '· \xa0 \xa0 \xa0 Optimizes SQL Server, SSIS, SSAS, Queries, Views, Stored Procedures, Processes ', '· \xa0 \xa0 \xa0 Other duties and projects as assigned based on business needs', '\xa0 ', '· \xa0 \xa0 \xa0 Participates in Code Reviews and adheres to department Standards and Best Practices ']",Associate,Contract,Engineering,Information Technology and Services,2020-12-30 19:31:23
