job_title,company,location,date_posted,applicants,job_text,seniority_level,employment_type,job_function,industries,date_scraped
Data Engineer,Zoom,"Pittsburgh, PA",6 hours ago,Be among the first 25 applicants,"['', 'A knack for writing, clean, readable, maintainable code', 'Build data expertise and own data quality for the pipelines you create', 'Lead large-scale data integration and warehousing projects', 'Technical leadership in solving complex data-driven problems', 'Own and optimize Zoom’s data architecture to address the data needs of our rapidly-growing business', 'Expertise in building out data pipelines, efficient ETL design, implementation, and maintenance', 'Proven track-record of solving complex data processing and storage challenges through scalable, fault-tolerant architecture', 'Join a group of passionate people committed to delivering “happiness” to our users and to each other', 'Write complex and efficient queries to transform raw data sources into easily accessible models by coding across several languages such as Java, Python, and SQL', 'Comfort with open source technologies like Kafka, Hadoop, Hive, Presto, and Spark', 'Seven or more years of relevant software engineering experience (Python, Scala and Java) in a data-focused role', 'Own and optimize Zoom’s data architecture to address the data needs of our rapidly-growing businessJoin a group of passionate people committed to delivering “happiness” to our users and to each otherPartner with data scientists, sales, marketing, operation, and product teams to build and deploy machine learning models that unlock growthLead large-scale data integration and warehousing projectsBuild custom integrations between cloud-based systems using APIsWrite complex and efficient queries to transform raw data sources into easily accessible models by coding across several languages such as Java, Python, and SQLArchitect, build, and launch new data models that provide intuitive analytics to the teamBuild data expertise and own data quality for the pipelines you create', 'Passion for creating data infrastructure technologies from scratch using the right tools for the job', 'Experience with AWS tools', 'Architect, build, and launch new data models that provide intuitive analytics to the team', 'Responsibilities', 'Build custom integrations between cloud-based systems using APIs', 'Seven or more years of relevant software engineering experience (Python, Scala and Java) in a data-focused roleTechnical leadership in solving complex data-driven problemsPassion for creating data infrastructure technologies from scratch using the right tools for the jobA knack for writing, clean, readable, maintainable codeComfort with open source technologies like Kafka, Hadoop, Hive, Presto, and SparkExpertise in building out data pipelines, efficient ETL design, implementation, and maintenanceExperience with AWS toolsProven track-record of solving complex data processing and storage challenges through scalable, fault-tolerant architecture', 'Partner with data scientists, sales, marketing, operation, and product teams to build and deploy machine learning models that unlock growth', 'Requirements']",Not Applicable,Full-time,Information Technology,Information Technology and Services,2020-10-22 10:04:13
Data Engineer,Toyota North America,"Dallas, TX",9 hours ago,Be among the first 25 applicants,"['', 'Organization', '  Bachelor’s degree (or higher) in Computer Science, Information Systems or equivalent professional work experience  Experience as a Data Engineer  Experience developing time tracking solutions  Experience developing data extracts and reporting dashboards  Broad-based IT experience participating in projects and playing a key role toward successful implementation of the project  Experience in Data Integration  SPARK computing framework ', ' Paid holidays and paid time off', ' Referral services related to prenatal services, adoption, child care, schools and more', ' Experience developing data extracts and reporting dashboards', 'Who We Are', ' Flexible spending accounts', ' Relocation assistance (if applicable)', 'What You Bring', ' Experience developing time tracking solutions', ' Prior experience in ICP or equivalent cloud platform', ' Certification in AWS or any other Cloud platform', '  Expertise in DevOps toolchain leveraging Jenkins  Prior experience in ICP or equivalent cloud platform  Certification in AWS or any other Cloud platform  Certification in any Kubernetes based development ', ' Bachelor’s degree (or higher) in Computer Science, Information Systems or equivalent professional work experience', ' A work environment built on teamwork, flexibility and respect', ' Experience as a Data Engineer', 'What You’ll Be Doing', 'Who We’re Looking For', 'What We’ll Bring', ' Comprehensive health care and wellness plans for your entire family', 'Added bonus if you have', ' Develop and support services and integration patterns such as API, Events and Batch between enterprise systems comprised of, but not limited to, Mainframe, Salesforce, SAP, Informatica in a cloud infrastructure ', ' To save time applying, Toyota does not offer sponsorship of job applicants for employment-based visas or any other work authorization for this position at this time ', ' Expertise in DevOps toolchain leveraging Jenkins', ' Professional growth and development programs to help advance your career, as well as tuition reimbursement', ' Toyota 401(k) Savings Plan featuring a company match, as well as an annual retirement contribution from Toyota regardless of whether you contribute', ' SPARK computing framework ', ' Develop, enhance, operate and maintain an Enterprise Data Platform comprised of, but not limited to, features such as Data Ingestion, Metadata driven development, Data Preparation/Aggregation and polyglot data delivery in a containerized Cloud based Kubernetes cluster with data services capability that can be leveraged for enterprise data consumption  Develop and support services and integration patterns such as API, Events and Batch between enterprise systems comprised of, but not limited to, Mainframe, Salesforce, SAP, Informatica in a cloud infrastructure  Program in Java Spring boot framework and microservices based data processing  Develop data processing framework using NiFi, Kafka Streaming, deployment using Jenkins, any NoSQL Document database, AWS S3 object store and codebase management using Git in a containerized Kubernetes platform  Drive opportunities for increased efficiencies of the Enterprise Data Platform to be fully operational, understanding means of automation, tuning and uplift of the data applications ', ' Develop, enhance, operate and maintain an Enterprise Data Platform comprised of, but not limited to, features such as Data Ingestion, Metadata driven development, Data Preparation/Aggregation and polyglot data delivery in a containerized Cloud based Kubernetes cluster with data services capability that can be leveraged for enterprise data consumption ', 'Primary Location', ' Drive opportunities for increased efficiencies of the Enterprise Data Platform to be fully operational, understanding means of automation, tuning and uplift of the data applications ', ' Flextime and virtual work options (if applicable)', 'What You Should Know', 'Travel', '  A work environment built on teamwork, flexibility and respect  Professional growth and development programs to help advance your career, as well as tuition reimbursement  Vehicle purchase & lease programs  Comprehensive health care and wellness plans for your entire family  Flextime and virtual work options (if applicable)  Toyota 401(k) Savings Plan featuring a company match, as well as an annual retirement contribution from Toyota regardless of whether you contribute  Paid holidays and paid time off  Referral services related to prenatal services, adoption, child care, schools and more  Flexible spending accounts  Relocation assistance (if applicable) ', ' Certification in any Kubernetes based development ', 'Job', ' Experience in Data Integration ', 'Job Posting', ' Program in Java Spring boot framework and microservices based data processing ', ' Broad-based IT experience participating in projects and playing a key role toward successful implementation of the project', ' Vehicle purchase & lease programs', ' Develop data processing framework using NiFi, Kafka Streaming, deployment using Jenkins, any NoSQL Document database, AWS S3 object store and codebase management using Git in a containerized Kubernetes platform ']",Not Applicable,Full-time,Information Technology,Financial Services,2020-10-22 10:04:13
Data Engineer,Foursquare,"Seattle, WA",14 hours ago,Be among the first 25 applicants,"['', 'Responsibilities Of The Role', 'Excellent communication skills, including the ability to identify and communicate data-driven insights', '4-7 years of software development experience', 'Proficiency in Python, Java, C#, and/or Ruby', 'Focus on performance, throughput, and latency, and drive these throughout our architecture', 'Experience operating systems in AWS', ' Influence key decisions on architecture and implementation of scalable data processing and analytics structure Work with the Data Science team to bring machine learning models into production Build Hadoop MapReduce and Spark processing pipelines using Java, Python, and Ruby Build REST APIs for data access by systems across our infrastructure Focus on performance, throughput, and latency, and drive these throughout our architecture Write test automation, conduct code reviews, and take end-to-end ownership of deployments to production Mentor junior engineering staff ', '3+ years of experience with Hadoop MapReduce and/or Spark data processing pipelines, analytics systems (e.g. OLAP, BI tools), and machine learning technologies', 'Qualifications', 'Mentor junior engineering staff', 'Write test automation, conduct code reviews, and take end-to-end ownership of deployments to production', 'About The Team', 'Work with the Data Science team to bring machine learning models into production', 'Build Hadoop MapReduce and Spark processing pipelines using Java, Python, and Ruby', 'Build REST APIs for data access by systems across our infrastructure', ' BS/BA in a technical field such as computer science or equivalent experience 4-7 years of software development experience Proficiency in Python, Java, C#, and/or Ruby 3+ years of experience with Hadoop MapReduce and/or Spark data processing pipelines, analytics systems (e.g. OLAP, BI tools), and machine learning technologies Experience operating systems in AWS Excellent communication skills, including the ability to identify and communicate data-driven insights ', 'Influence key decisions on architecture and implementation of scalable data processing and analytics structure', 'BS/BA in a technical field such as computer science or equivalent experience']",Entry level,Full-time,Information Technology,Marketing and Advertising,2020-10-22 10:04:13
Data Engineer,Travelers,"Hartford, CT",15 hours ago,Be among the first 25 applicants,"['', 'Creates moderate (technology and features) data visualization techniques to help support data exploration.', 'Understanding of tools/technologies like Athena, Redshift and snowflake a plus', 'Ability to communicate thoughts/designs/ideas in an unambiguous manner and adjusts communication based on audience.', 'Able to clearly and concisely communicate with the business. Demonstrates willingness, cooperation, and concern for business issues. Able to understand assigned business unit and business priorities.', 'Minimum Qualifications', 'Advanced knowledge of data tools, techniques, and manipulation preferred.', 'Applies knowledge of current industry trends and techniques to formulate solutions within the context of assigned projects and/or enhancements.', 'Able to recognize, analyze, and diagnose business and data issues of advanced complexity.', 'Education, Work Experience, & Knowledge', 'Provides guidance and mentorship to lower level technical employees.', 'Performs analysis of complex (type, quality, volume) sources to determine value and use. Determines and recommends data to include in analytical projects.', 'Ensures customer satisfaction through professional communication, follow-up, and responsiveness to issues.', 'Hands-on experience in writing complex, optimized SQL queries across large datasets. ', 'Primary Job Duties & Responsibilities', 'Travelers reserves the right to fill this position at a level above or below the level included in this posting.', 'Hands on experience using Spark/Python. Utilizing DataBricks a plus!', 'Advanced knowledge of data tools, techniques, and manipulation preferred.Examples (but not limited to): Big data and Hadoop platforms and languages - SAS, SQL, Spark, Python, Pig, Hive, QlikView, Tableau.Ability to communicate thoughts/designs/ideas in an unambiguous manner and adjusts communication based on audience.Exhibits active and effective communication skills with team members - including active listening and effective written and verbal communication skills.Effectively contributes and communicates with the immediate team.Able to clearly and concisely communicate with the business. Demonstrates willingness, cooperation, and concern for business issues. Able to understand assigned business unit and business priorities.Able to coordinate with other technical areas to achieve project/department or division goals.Able to recognize, analyze, and diagnose business and data issues of advanced complexity.', 'Nice to have experience with Airflow DAG for orchestration.', 'Develops process to acquire and integrate data.', 'Having awareness of web development technologies and frameworks like Node Js, React Js, HTML, Java script, CSS is a plus', 'Reviews unfamiliar data sources. Connects to value for business. Able to link to systems.Develops process to acquire and integrate data.Independently reviews, prepares, designs and integrates complex (type, quality, volume) data correcting problems and recommends data cleansing/quality solutions to major issues.Works within Travelers standards, processes, and protocols.Develops moderate and applies complex data derivations, business transformation rules, and data requirements.Leads medium scale projects and coordinates aspects of larger projects with limited supervision.Performs analysis of complex (type, quality, volume) sources to determine value and use. Determines and recommends data to include in analytical projects.Creates moderate (technology and features) data visualization techniques to help support data exploration.Utilizes business knowledge to explain technical activities in business terms.Actively seeks opportunities to expand technical knowledge and capabilities.Develops and maintains relationships across the enterprise.Applies knowledge of current industry trends and techniques to formulate solutions within the context of assigned projects and/or enhancements.Ensures customer satisfaction through professional communication, follow-up, and responsiveness to issues.Consultation:Shares knowledge with users on data or analytic products.Builds effective relationships with stakeholders.Provides guidance and mentorship to lower level technical employees.', 'Utilizes business knowledge to explain technical activities in business terms.', 'Independently reviews, prepares, designs and integrates complex (type, quality, volume) data correcting problems and recommends data cleansing/quality solutions to major issues.', 'Taking care of our customers, our communities and each other. That’s the Travelers Promise. By honoring this commitment, we have maintained our reputation as one of the best property casualty insurers in the industry for over 160 years. Join us to discover a culture that is rooted in innovation and thrives on collaboration. Imagine loving what you do and where you do it.', 'Leads medium scale projects and coordinates aspects of larger projects with limited supervision.', 'Consultation:Shares knowledge with users on data or analytic products.', 'Reviews unfamiliar data sources. Connects to value for business. Able to link to systems.', 'If you have questions regarding the physical requirements of this role, please send us an email so we may assist you.', 'Exhibits active and effective communication skills with team members - including active listening and effective written and verbal communication skills.', 'Examples (but not limited to): Big data and Hadoop platforms and languages - SAS, SQL, Spark, Python, Pig, Hive, QlikView, Tableau.', 'Develops and maintains relationships across the enterprise.', 'Job Description Summary', 'Company Summary', 'Travelers is an equal opportunity employer. We value the unique abilities and talents each individual brings to our organization and recognize that we benefit in numerous ways from our differences. ', 'Effectively contributes and communicates with the immediate team.', 'Understanding of Visualization tools: Tableau, Qliksense or Microstrategy', 'Target Openings', 'Develops moderate and applies complex data derivations, business transformation rules, and data requirements.', 'Understanding of AWS Native tools like Lambda, Glue, Kinesis, Terraform scripting, EC2, IAM etc.', 'Able to coordinate with other technical areas to achieve project/department or division goals.', 'Actively seeks opportunities to expand technical knowledge and capabilities.', 'Works within Travelers standards, processes, and protocols.', '4 years of relevant experience with data tools, techniques, and manipulation required.', 'Employment Practices', 'Job Specific Technical Skills & Competencies', 'Builds effective relationships with stakeholders.', 'Hands on experience using Spark/Python. Utilizing DataBricks a plus!Understanding of AWS Native tools like Lambda, Glue, Kinesis, Terraform scripting, EC2, IAM etc.Hands-on experience in writing complex, optimized SQL queries across large datasets. Understanding of tools/technologies like Athena, Redshift and snowflake a plusUnderstanding of Visualization tools: Tableau, Qliksense or MicrostrategyNice to have experience with Airflow DAG for orchestration.Having awareness of web development technologies and frameworks like Node Js, React Js, HTML, Java script, CSS is a plus']",Not Applicable,Full-time,Information Technology,Insurance,2020-10-22 10:04:13
Data Engineer - Celonis,Mitchell Martin Inc.,"Tampa, FL",17 minutes ago,Be among the first 25 applicants,"['', 'Drive celonis initiatives and use celonis to dive into the business process to determine root causes, quantify potential and establish and drive improvement initiatives to make the business more efficient Set up and maintain data structures Setting data dictionary and maintaining data governance identify best strategy for data collection, ensure data quality and work with stake holders to ensure they can correctly measure and track necessary informaitonWork with IT SME to ensure IT systems are set up correctly to gather information and support the data structures ', 'Drive celonis initiatives and use celonis to dive into the business process to determine root causes, quantify potential and establish and drive improvement initiatives to make the business more efficient ', 'Setting data dictionary and maintaining data governance ', 'Experienced with SQL and Python', '5+ years of relevant work experience (Data science/data modeling)', 'Qualifications:', 'Qualifications', 'identify best strategy for data collection, ensure data quality and work with stake holders to ensure they can correctly measure and track necessary informaiton', 'Team player and has the ability to communicate data structural concepts and ideas to both technical and non-technical stakeholders', 'Knowledge of RPA technologies (Automation Anywhere, Blue Prism, UiPath, WorkFusion)', '5+ years of relevant work experience (Data science/data modeling)Expeirence working with data from software (SAP, Salesforce, ServiceNow, or other ERP systems)Experienced with SQL and PythonTeam player and has the ability to communicate data structural concepts and ideas to both technical and non-technical stakeholdersDegree in Computer ScienceKnowledge of RPA technologies (Automation Anywhere, Blue Prism, UiPath, WorkFusion)', 'Degree in Computer Science', 'Set up and maintain data structures ', 'This role will lay the data foundation fo broth data-driven decisions and initiatives to identify, eliminate, and standardize processes. This position will build data structures, and identify the best strategies for data collection. ', 'Responsibilities:', 'Expeirence working with data from software (SAP, Salesforce, ServiceNow, or other ERP systems)', 'Responsibilities', 'Our direct client is seeking a Data Engineer with Celonis experience to join the team for a contract to hire opportunity! ', 'Work with IT SME to ensure IT systems are set up correctly to gather information and support the data structures ']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2020-10-22 10:04:13
Data Engineer,WeWork,"New York, NY",21 hours ago,36 applicants,"['', 'Bonus Points For Experience In', 'Collaborate on improving efficiency and quality of internal data processes as well as stakeholder engagement, including implementation of system and model quality tracking ', 'Working in a cloud environment (AWS, GCP, Snowflake, etc)', ""We don't do everything the traditional way, and are always looking to innovate and push the envelope."", 'You are open to new and innovative solutions.', 'You love working with people!', 'Doer', 'Leverage data expertise to help build and evolve data models in various components of the data stack', 'Exceptional organizational and multitasking skills.', 'Strong communication skills, empathy and initiative', 'What You Will Do', 'Good understanding of SQL Engine and able to conduct advanced performance tuning', 'Strong background in data warehouse concept and design', 'DBT', 'Collaborate closely with data engineers, analysts,data scientists, ML Engineers, as well as cross-functional teams, to leverage huge amounts of WeWork data, for data-driven business and user behavior insights', 'Develop centralized source of truth data sets to encourage a democratized, data-driven culture', 'Salesforce and related tools', 'Credibility is earned at WeWork through execution and getting things done.', 'You must present well and communicate clearly and effectively to upper management and internal departments.', 'Develop tools supporting self-service data pipeline management (ETL)', 'Builds trust across the organization by being a good listener and inclusively soliciting input.', '3+ years of experience in data engineering', ""You have the flexibility to think outside the box.We don't do everything the traditional way, and are always looking to innovate and push the envelope.You have the ability to foresee and identify needs of the team.You take an innovator and creator’s approach to any issues that may arise.You take initiative and identify ways in which we can improve our data product and service."", 'There is no room for “I” at WeWork. Every role and individual is in the organization to serve We.Builds trust across the organization by being a good listener and inclusively soliciting input.You are open to new and innovative solutions.You must present well and communicate clearly and effectively to upper management and internal departments.You’re willing to adjust course when appropriate new ideas or objections are raised.You love working with people!', 'Leading / managing full project lifecycles', 'Proficient in at least one of the SQL dialects (Snowflake, MySQL, PostgreSQL, SqlServer, Oracle)', 'You’re excited to solve data challenges that combines digital and physical aspects, across multiple business verticals like Real Estate, Finance, Sales & Marketing, Social Media, and regions', ""You're motivated to enable and collaborate with engineering, analytics and product teams to tackle the most challenging business needs"", 'Strong communication skills, empathy and initiative3+ years of experience in data engineeringStrong background in data warehouse concept and designProficient in at least one of the SQL dialects (Snowflake, MySQL, PostgreSQL, SqlServer, Oracle)Good understanding of SQL Engine and able to conduct advanced performance tuningFamiliar with at least one scripting language (Python, Ruby, Perl, Bash)Experience with Git and the pull request workflowExperience working closely with Analytics/Data Science teamsExperience with modern BI tools (Looker, Tableau, etc)', 'You do what you love!', 'Distributed execution frameworks (Spark, Apache Beam, etc.)', 'You are able to get into the details and deliver results under highest expectations on time and quality.', 'Familiar with at least one scripting language (Python, Ruby, Perl, Bash)', 'You take an innovator and creator’s approach to any issues that may arise.', 'Solution-centric', 'You do what you love!Credibility is earned at WeWork through execution and getting things done.You are able to get into the details and deliver results under highest expectations on time and quality.Be ready to get hands-on with all aspects of the daily needs. The buck stops with you.Pragmatism and outcomes orientation are valued and lead to wins.Exceptional organizational and multitasking skills.You thrive in a fast-paced environment.You are resilient and can adapt to a changing environment.', 'Collaborator', 'Experience working closely with Analytics/Data Science teams', 'Kafka', 'Be ready to get hands-on with all aspects of the daily needs. The buck stops with you.', 'Experience with Git and the pull request workflow', 'You are resilient and can adapt to a changing environment.', 'You’re willing to adjust course when appropriate new ideas or objections are raised.', 'Leverage data expertise to help build and evolve data models in various components of the data stackWork on architecting, building, and launching highly scalable and reliable data pipelines to support WeWorkCollaborate closely with data engineers, analysts,data scientists, ML Engineers, as well as cross-functional teams, to leverage huge amounts of WeWork data, for data-driven business and user behavior insightsCollaborate on improving efficiency and quality of internal data processes as well as stakeholder engagement, including implementation of system and model quality tracking Develop centralized source of truth data sets to encourage a democratized, data-driven culture', 'You Will', 'Pragmatism and outcomes orientation are valued and lead to wins.', 'Work on architecting, building, and launching highly scalable and reliable data pipelines to support WeWork', 'You thrive in a fast-paced environment.', 'You have the flexibility to think outside the box.', ""You’re excited to solve data challenges that combines digital and physical aspects, across multiple business verticals like Real Estate, Finance, Sales & Marketing, Social Media, and regionsYou're motivated to enable and collaborate with engineering, analytics and product teams to tackle the most challenging business needs"", 'Leading / managing full project lifecyclesSalesforce and related toolsWorkflow management tools (Airflow, Luigi, etc..)Working in a cloud environment (AWS, GCP, Snowflake, etc)KafkaDBTDistributed execution frameworks (Spark, Apache Beam, etc.)', 'There is no room for “I” at WeWork. Every role and individual is in the organization to serve We.', 'Who You Are', 'Experience with modern BI tools (Looker, Tableau, etc)', 'You take initiative and identify ways in which we can improve our data product and service.', 'Workflow management tools (Airflow, Luigi, etc..)', 'You have the ability to foresee and identify needs of the team.', 'Requirements']",Mid-Senior level,Full-time,Information Technology,Computer Software,2020-10-22 10:04:13
Data Engineer,Insight Global,"Portland, Oregon Metropolitan Area",,N/A,"['- Hands-on experience using Apache Airflow for data warehousing - extract, transform and loading', 'An employer in the Portland, OR area is looking for a data engineer to join their team. This person will join a team currently working on', '- Previous experience working with Big Data tools such as Hadoop, Spark, Kafka, etc', '- 4-8+ years experience as a data engineer or similar', '- Strong background using Python to build out data pipelines', '\xa0', 'Day-to-Day\xa0', '- AWS cloud exposure for data storage', 'Desired Skills and Experience\xa0', 'a data science enablement project. They will be helping to build out new data models and supporting a data migration to the AWS cloud.', 'Plusses', '- Experience working with Python Pandas']",Mid-Senior level,Full-time,Information Technology,Staffing and Recruiting,2020-10-22 10:04:13
Data Engineer,Henderson Scott,"Austin, TX",,N/A,"['', 'Computer Science or STEM degree or equivalent experience\xa0Previous experience working as a Data Engineer developing data pipelines for analyticsSolid knowledge and understanding of Big Data & Data Warehousing concepts, data structures, algorithms and design patternsData Flow automation solutions development', 'Flat hierarchy structure where you have a voice', 'noSQL', 'RDBMS', 'Full development life cycle experience', 'What we need from you:', 'Our partner in Austin TX is looking for an experienced Data Engineer to help build out new Data Pipelines for their growing business. They provide improved pricing solutions for essential drug products and therefore create a better customer experience.', 'Interested? Send your resume for immediate consideration and a confidential chat!', 'Data Flow automation solutions development', 'Agile Extreme Programming experience', 'In return you will get:', 'Cloud Development experience with Google Cloud Platform background an added bonus', 'Server Side technologies including Python, Kotlin, NodeJS', 'Previous experience working as a Data Engineer developing data pipelines for analytics', 'Computer Science or STEM degree or equivalent experience\xa0', 'Are you a Data Engineer with a passion for building robust Data Solutions? Do you like working with like-minded people to develop products that can really impact the quality of life of millions of people?', 'Flexible / remote working\xa0', 'A competitive salary and career development options', 'Technical skills covering:', 'RDBMSnoSQLServer Side technologies including Python, Kotlin, NodeJSCloud Development experience with Google Cloud Platform background an added bonusFull development life cycle experienceAgile Extreme Programming experience', 'Solid knowledge and understanding of Big Data & Data Warehousing concepts, data structures, algorithms and design patterns', 'A competitive salary and career development optionsFlexible / remote working\xa0Flat hierarchy structure where you have a voice']",Mid-Senior level,Full-time,Information Technology,Computer Software,2020-10-22 10:04:13
Data Engineer,Playwire,"Boca Raton, FL",19 hours ago,Be among the first 25 applicants,"['Knowledge of common data design patterns. ', 'Identify bottlenecks and bugs, and devise solutions to these problems. ', ' ', 'To keep pace with our explosive growth, we are currently seeking a data engineer to extend our data collection, warehousing, and analysis capabilities. You’ll use your skills to facilitate objectives ranging from simple data pipelining and summarization for use in internal dashboards, to supporting our prediction initiatives by collecting and transforming data from a variety of sources and then crafting prediction algorithms to run at scale. As such, you’ll need to bring a variety of skills to bear, from pipelining to ETL to algorithmic manipulation of data.Playwire’s services are responsible for delivering millions of ads daily to sites and apps that depend on our technology and expertise for their ad revenue. Data is central to our latest advances in this regard; if you want to participate in the early expansion of our data-related capabilities, and thrive in a fast-paced, rapid-development environment, then talk to us. ', 'An understanding of source control, particularly git and Github. ', 'Knowledge of machine learning principles and technologies. ', 'Knowledge of business intelligence tools, and the relationship between data and BI-type instrumentation. ', 'Help maintain data-related code and service quality, organization, and reliability. ', 'Experience with digital ad servers, SSPs, or ad exchanges. ', 'This position requires full-time employment; availability at our main office in Boca Raton, FL is preferred, but remote work is a possibility. ', 'A basic knowledge of CDNs and the role they fill, including edge computing capabilities.', 'Experience with ETL. ', 'Architect and maintain data pipelines, data lakes, and databases, both independently and as part of a data engineering team. ', 'Experience with Amazon AWS’s data-related services. ', 'Playwire Media is a full-service digital innovation partner that leverages online advertising and proprietary technologies to build publishing brands in the gaming and entertainment verticals. ', 'Design and deploy algorithms best suited for Playwire’s optimization and prediction needs. ', 'Experience working in a team using Agile development and project management practices. ', 'Comfort with big data principles and management. ', 'Proficiency with one or more coding languages; Ruby, Go, and Javascript are all plusses. ', 'Essential functions: ', 'Do you want to get a foot in the door at an online Advertising and Media company that has seen tremendous growth? Do you want to work in the world of online video; one of the fastest growing advertising formats in the world? Then Playwire is the place for you… ', 'Integrate data from external sources, gathered both via API and from email-based reports. ', 'Experience with data pipelining. ', 'Nice-to-haves: ', 'Required qualifications: ', 'Experience with Amazon AWS, particularly RDS and S3. ', 'Experience with digital advertising units, terminology and standards. ', 'Work with the development team to support data-centric applications. ', 'Experience using APIs to access data. ']",Mid-Senior level,Full-time,Information Technology,Marketing and Advertising,2020-10-22 10:04:13
Data Engineer,"Lowe's Companies, Inc.","Charlotte, NC",14 hours ago,Be among the first 25 applicants,"['', 'Minimum Qualifications', 'Conducts the implementation and maintenance of complex business and enterprise data solutions to ensure successful deployment of released applications', '2 years of experience in Data, BI or Platform Engineering, Data Warehousing/ETL, or Software Engineering', 'Translates business requirements and specifications into logical program designs, modules, stable application systems, and data solutions with occasional guidance from senior colleagues; partners with Product Team to understand business needs and functional specifications', 'Supports the build, maintenance and enhancements of data lake development; supports simple to medium complexity API, unstructured data parsing and streaming data ingestionExcels in one more domain; understands pipelines and business metricsBuilds, tests and enhances data curation pipelines integration data from a wide variety of sources like DBMS, File systems and APIs for various KPIs and metrics development with high data quality and integritySupports the development of feature / inputs for data models in an Agile mannerWorks with Data Science team to understand mathematical models and algorithms; participates in continuous improvement activities including training opportunities; continuously strives to learn analytic best practices and apply them to daily activitiesHandles data manipulation (extract, load, transform), data visualization, and administration of data and systems securely and in accordance with enterprise data governance standardsMaintains the health and monitoring of assigned analytic capabilities for a specific data engineering solution; ensures high availability of the platform; monitors workload demands; works with Technology Job Description Page 2 of 3 Infrastructure Engineering teams to maintain the data platform; serves as an SME of one or more application', ""Bachelor\\'s Degree in Engineering, Computer Science, CIS, or related field (or equivalent work experience in a related field)"", 'Maintains the health and monitoring of assigned analytic capabilities for a specific data engineering solution; ensures high availability of the platform; monitors workload demands; works with Technology Job Description Page 2 of 3 Infrastructure Engineering teams to maintain the data platform; serves as an SME of one or more application', 'Excels in one more domain; understands pipelines and business metrics', ""Bachelor\\'s Degree in Engineering, Computer Science, CIS, or related field (or equivalent work experience in a related field)2 years of experience in Data, BI or Platform Engineering, Data Warehousing/ETL, or Software Engineering1 year of experience working on project(s) involving the implementation of solutions applying development life cycles (SDLC)"", 'Builds, tests and enhances data curation pipelines integration data from a wide variety of sources like DBMS, File systems and APIs for various KPIs and metrics development with high data quality and integrity', '4 years of experience working with defect or incident tracking software', 'Data Engineering Responsibilities', 'Experience with application and integration middleware', '2 years of experience working with an IT Infrastructure Library (ITIL) framework', 'About Lowe’s', '2 years of IT experience developing and implementing business systems within an organization', '2 years of experience leading teams, with or without direct reports', 'Supports systems integration testing (SIT) and user acceptance testing (UAT), provides insight into defining test plans, and ensures quality software deployment', '2 years of experience in Hadoop or any Cloud Bigdata components (specific to the Data Engineering role)Expertise in Java/Scala/Python, SQL, Scripting, Teradata, Hadoop (Sqoop, Hive, Pig, Map Reduce), Spark (Spark Streaming, MLib), Kafka or equivalent Cloud Bigdata components (specific to the Data Engineering role)', 'About Lowe’s In The Community', '2 years of experience in Hadoop or any Cloud Bigdata components (specific to the Data Engineering role)', 'Participates in the end-to-end product lifecycle by applying and sharing an in-depth understanding of company and industry methodologies, policies, standards, and controls', 'Works with Data Science team to understand mathematical models and algorithms; participates in continuous improvement activities including training opportunities; continuously strives to learn analytic best practices and apply them to daily activities', ""Master\\'s Degree in Computer Science, CIS, or related field"", 'Data Engineering Qualifications', 'Understands Computer Science and/or Computer Engineering fundamentals; knows software architecture and readily applies this to Data or Platform solutions', 'Develops, configures, or modifies integrated business and/or enterprise application solutions within various computing environments by designing and coding component-based applications using various programming languages', 'In most cases Lowe’s will not be able to provide sponsorship for roles located in the Tech Hub', 'Solves difficult technical problems; solutions are testable, maintainable, and efficient', 'Key Responsibilities', 'Experience with database technologies', 'Supports the build, maintenance and enhancements of data lake development; supports simple to medium complexity API, unstructured data parsing and streaming data ingestion', '4 years of experience with technical documentation in a software development environment', '1 year of experience working on project(s) involving the implementation of solutions applying development life cycles (SDLC)', 'Translates business requirements and specifications into logical program designs, modules, stable application systems, and data solutions with occasional guidance from senior colleagues; partners with Product Team to understand business needs and functional specificationsDevelops, configures, or modifies integrated business and/or enterprise application solutions within various computing environments by designing and coding component-based applications using various programming languagesConducts the implementation and maintenance of complex business and enterprise data solutions to ensure successful deployment of released applicationsSupports systems integration testing (SIT) and user acceptance testing (UAT), provides insight into defining test plans, and ensures quality software deploymentParticipates in the end-to-end product lifecycle by applying and sharing an in-depth understanding of company and industry methodologies, policies, standards, and controlsUnderstands Computer Science and/or Computer Engineering fundamentals; knows software architecture and readily applies this to Data or Platform solutionsAutomates and simplifies team development, test, and operations processes; develops conceptual, logical and physical architectures consisting of one or more viewpoints (business, application, data, and infrastructure) required for business solution deliverySolves difficult technical problems; solutions are testable, maintainable, and efficient', 'Automates and simplifies team development, test, and operations processes; develops conceptual, logical and physical architectures consisting of one or more viewpoints (business, application, data, and infrastructure) required for business solution delivery', 'Expertise in Java/Scala/Python, SQL, Scripting, Teradata, Hadoop (Sqoop, Hive, Pig, Map Reduce), Spark (Spark Streaming, MLib), Kafka or equivalent Cloud Bigdata components (specific to the Data Engineering role)', ""Master\\'s Degree in Computer Science, CIS, or related field2 years of IT experience developing and implementing business systems within an organization4 years of experience working with defect or incident tracking software4 years of experience with technical documentation in a software development environment2 years of experience working with an IT Infrastructure Library (ITIL) framework2 years of experience leading teams, with or without direct reportsExperience with application and integration middlewareExperience with database technologies"", 'Preferred Qualifications', 'Supports the development of feature / inputs for data models in an Agile manner', 'Job Summary', 'Handles data manipulation (extract, load, transform), data visualization, and administration of data and systems securely and in accordance with enterprise data governance standards']",Entry level,Full-time,Information Technology,Information Technology and Services,2020-10-22 10:04:13
Data Engineer,ICX Group,"Florida, United States",20 hours ago,43 applicants,"['', 'Strong knowledge & hands on experience in Python language, handling structured & semi-structured large data sets', 'Bachelor’s degree in MIS, Computer Science or Information Technology or related field OR 8 years relevant professional work experience.', 'Strong SQL and relational database design and development skills.Complex T-SQL stored procedure design, coding, troubleshooting and performance tuningRequire a very good working experience of T-SQL. Proficiency in complex stored procedures, writing complex queries with many joins and subqueries, user defined functions (UDF), using tools like SQL Profiler and Database Tuning Advisor (DTA)Strong knowledge & hands on experience in Python language, handling structured & semi-structured large data setsVery good knowledge of reporting tools such as Cognos, Power BI CloudStrong data analysis and complex report/data extract development skills.\xa0Experience in bulk importing CSV, XML and Flat files data using Bulk copy program (BCP)Strong knowledge of importing semi-structured data (large complex, nested JSON files)Strong knowledge of relational and star schema/dimensional database design, data warehouse designs/conceptsMust have the ability to participate on group discussion with the analytics and business teams to understand the requirement. Analyze the requirements details, prepare functional specification notes to use in design, development, and testingAbility to be flexible and work within a team environment', 'OR', 'Strong knowledge of importing semi-structured data (large complex, nested JSON files)', 'Python, R (preferred) language', 'Data warehouse development experience', '4 + years to include:SQL Server (2014-2019) relational database experienceSQL and Microsoft T-SQL Query and large data extract experienceSQL Server Integration (SSIS) experienceData warehouse development experience4 + year experience to include:Python, R (preferred) language.NET\xa0OR\xa0C# languageWeb scrapping & data extraction using PythonAd-hoc report development & delivery experience', 'REQUIRED EDUCATION AND EXPERIENCE:', 'SQL Server (2014-2019) relational database experience', 'SQL and Microsoft T-SQL Query and large data extract experience', 'REQUIRED KNOWLEDGE, SKILLS AND ABILITIES:', 'Complex T-SQL stored procedure design, coding, troubleshooting and performance tuning', 'Must have the ability to participate on group discussion with the analytics and business teams to understand the requirement. Analyze the requirements details, prepare functional specification notes to use in design, development, and testing', '4 + years to include:', 'SQL Server Integration (SSIS) experience', 'Ad-hoc report development & delivery experience', 'Strong data analysis and complex report/data extract development skills.\xa0Experience in bulk importing CSV, XML and Flat files data using Bulk copy program (BCP)', '.NET\xa0OR\xa0C# language', 'Require a very good working experience of T-SQL. Proficiency in complex stored procedures, writing complex queries with many joins and subqueries, user defined functions (UDF), using tools like SQL Profiler and Database Tuning Advisor (DTA)', '\xa0', 'AND', 'Strong knowledge of relational and star schema/dimensional database design, data warehouse designs/concepts', 'Very good knowledge of reporting tools such as Cognos, Power BI Cloud', 'Strong SQL and relational database design and development skills.', 'Web scrapping & data extraction using Python', '4 + year experience to include:', 'Ability to be flexible and work within a team environment']",Mid-Senior level,Contract,Information Technology,Staffing and Recruiting,2020-10-22 10:04:13
Data Center Operations Analytics Engineer,Facebook,"Ashburn, VA",19 hours ago,Be among the first 25 applicants,"['', 'Communication experience.', 'Act as key Subject Matter Expert on operation of servers in a hyper-scale data center environment.', 'Experience with Data Center Infrastructure and fleet management.', 'Ability to travel up to 30% required.', 'Align the global team behind these metrics, and be a key driver ensuring rapid, coordinated, and effective responses to issues as they arise.', ""Act as key Subject Matter Expert on operation of servers in a hyper-scale data center environment.Formulate the right metrics and definitions of success to drive quality, efficiency, cost, and timeliness.Align the global team behind these metrics, and be a key driver ensuring rapid, coordinated, and effective responses to issues as they arise.Work with data analytics partners to use data mining and statistical techniques to validate or identify operational inefficiencies, exceptions, and fault/event correlation.Provide data driven narratives, determining what is important and what next steps should be taken.Build data pipelines, tables and dashboards, and leverage these for analyzing incidents and trends.Create a deep understanding of a complex technical environment through data and reporting.Create key metrics in areas such as operational efficiency and failure rates, and evolve these over time to match changes to the infrastructure and business requirements.Build cross-functional relationships and have the ability to influence policies and procedures to improve global data center operations.Help create policies and machine learning algorithms to solve complex business problems.Become an expert in Facebook's site operations infrastructure, tools, systems, and data.Anticipate potential operational risks and develop strategies to mitigate/minimize.Partner with other organizations and develop dashboard/reporting for different audience levels to improve the state of the fleet.Ability to travel up to 30% required."", 'BS, BEng or BA in technical field or commensurate experience.', ""Become an expert in Facebook's site operations infrastructure, tools, systems, and data."", 'Build cross-functional relationships and have the ability to influence policies and procedures to improve global data center operations.', 'Experience working in a large-scale data center environment.', 'Experience working individually as well as in small and large groups on a regular basis.', 'BS, BEng or BA in technical field or commensurate experience.10+ years of technical IT experience.5+ years of experience working with data sets and knowledge of relational databases.Knowledge of data driven analysis.Experience communicating the results of analysis and insights to cross functional teams and influence the strategy of those teams.Experience with Data Center Infrastructure and fleet management.Experience analyzing and processing large or complex sets of data.Experience working individually as well as in small and large groups on a regular basis.Communication experience.', 'Build data pipelines, tables and dashboards, and leverage these for analyzing incidents and trends.', 'Experience analyzing and processing large or complex sets of data.', 'Help create policies and machine learning algorithms to solve complex business problems.', 'Partner with other organizations and develop dashboard/reporting for different audience levels to improve the state of the fleet.', 'Create a deep understanding of a complex technical environment through data and reporting.', 'Provide data driven narratives, determining what is important and what next steps should be taken.', '5+ years of experience working with data sets and knowledge of relational databases.', '10+ years of technical IT experience.', 'Anticipate potential operational risks and develop strategies to mitigate/minimize.', 'Formulate the right metrics and definitions of success to drive quality, efficiency, cost, and timeliness.', 'Create key metrics in areas such as operational efficiency and failure rates, and evolve these over time to match changes to the infrastructure and business requirements.', 'Knowledge of data driven analysis.', 'Work with data analytics partners to use data mining and statistical techniques to validate or identify operational inefficiencies, exceptions, and fault/event correlation.', 'Experience communicating the results of analysis and insights to cross functional teams and influence the strategy of those teams.']",Associate,Full-time,Information Technology,Internet,2020-10-22 10:04:13
Data Engineer,Sysco,"Houston, TX",15 hours ago,Be among the first 25 applicants,"['', ' Excellent communication skills to effectively manage key business stakeholder relationships ', ' Demonstrated “agile-development” mindset with strong customer-focus & results-orientation ', ' 5-10 years of experience in developing high performance and highly scalable applications in an agile environment, depending on education ', ' Fluency in DevOps, including continuous integration, continuous deployment / delivery, configuration and containerization, infrastructure as a code, and monitoring ', 'Mandatory Experience', ' Assists in the development of automated tests and environment management scripts ', ' Extensive hands-on experience and expertise in modern programming languages (Java, JavaScript, C#, Python, Ruby, Groovy) ', ' Evaluates emerging technologies continually to identify opportunities, trends and best practices to strengthen Sysco’s development practices ', 'High proficiency with SQL, ETL, and data visualization skills', ' May play a supervisory role toward junior technical resources as they commit to and deliver work ', "" Bachelor's degree in CS or 4-5 years of relevant developer experience "", ' Leverages strong understanding of business to develop high quality code to meet product/platform requirements ', ' Writes effective technical user stories and ensures that non-functional requirements are met to ensure performance, scaling, resilience and maintainability of software/solutions ', 'Excellent analytical capabilities, including experience collecting, analyzing and gathering insights from very large data sets', 'Zip Code', 'Company', 'D rives and leads adoption of architecture standards and development practices like Test-Driven development, code reviews, static code analysis etc. ', 'High proficiency with Microsoft Excel and other Microsoft Office Suite Applications', 'Travel Percentage:', 'Minimum Level Of Education', 'Minimum Years Of Experience', 'Competencies', ' Ability to excel in a fast paced agile environment ', ' Demonstrated “agile-development” mindset with strong customer-focus & results-orientation  Flexible mindset (not married to a single language) and demonstrated ability to grow skillset and work across the full technology stack  Ability to excel in a fast paced agile environment  Excellent communication skills to effectively manage key business stakeholder relationships  Highly proficient in teamwork and collaboration skills ', 'Preferred Experience', ' Strong understanding of Scrum, Lean, XP, Kanban and other agile development techniques ', ' Flexible mindset (not married to a single language) and demonstrated ability to grow skillset and work across the full technology stack ', "" Bachelor's degree in CS or 4-5 years of relevant developer experience  5-10 years of experience in developing high performance and highly scalable applications in an agile environment, depending on education  Extensive hands-on experience and expertise in modern programming languages (Java, JavaScript, C#, Python, Ruby, Groovy)  Strong understanding of Scrum, Lean, XP, Kanban and other agile development techniques  Strong experience building and deploying applications on a cloud platform such as AWS  Fluency in DevOps, including continuous integration, continuous deployment / delivery, configuration and containerization, infrastructure as a code, and monitoring "", ' Leverages strong understanding of business to develop high quality code to meet product/platform requirements D rives and leads adoption of architecture standards and development practices like Test-Driven development, code reviews, static code analysis etc.  Writes effective technical user stories and ensures that non-functional requirements are met to ensure performance, scaling, resilience and maintainability of software/solutions  Actively resolves defects and manages technical debt  Develops unit tests to ensure good coverage and regression testing ability  Assists in the development of automated tests and environment management scripts  Practices DevOps methods like CI/CD, SDLC automation and proactive monitoring/telemetry  Participates in sprint planning, daily stand-ups, sprint reviews and retrospectives to enable progress, and surface and resolve impediments  Evaluates emerging technologies continually to identify opportunities, trends and best practices to strengthen Sysco’s development practices  May play a supervisory role toward junior technical resources as they commit to and deliver work ', 'Responsibilities', ' Participates in sprint planning, daily stand-ups, sprint reviews and retrospectives to enable progress, and surface and resolve impediments ', 'Employment Type', ' Actively resolves defects and manages technical debt ', '5+ years of experience in data engineering', ' Practices DevOps methods like CI/CD, SDLC automation and proactive monitoring/telemetry ', ' Highly proficient in teamwork and collaboration skills ', ' Develops unit tests to ensure good coverage and regression testing ability ', ' Strong experience building and deploying applications on a cloud platform such as AWS ', 'Prior experience working with AWS technologies like EC2, Lambda, RedShift, S3, Kineses, Firehose, and Glue', 'Job Summary', 'Overview', '5+ years of experience in data engineeringExcellent analytical capabilities, including experience collecting, analyzing and gathering insights from very large data setsHigh proficiency with SQL, ETL, and data visualization skillsHigh proficiency with Microsoft Excel and other Microsoft Office Suite ApplicationsPrior experience working with AWS technologies like EC2, Lambda, RedShift, S3, Kineses, Firehose, and Glue']",Entry level,Full-time,Information Technology,Construction,2020-10-22 10:04:13
Data Engineer,TriCom Technical Services,"Hopkins, MN",13 hours ago,Be among the first 25 applicants,"['', "" High proficiency in AWS, Drone, and GitHub. Enjoys working in and designing CI/CD frameworks. Proficiency in working with NoSQL paradigms for data (specifically, HDFS, and graph frameworks). Experience working in Agile driven projects. Bachelor's Degree. "", 'Enjoys working in and designing CI/CD frameworks.', ' Create scalable data that provides sustainable data access within a large, enterprise company. ', 'High proficiency in AWS, Drone, and GitHub.', 'Summary', 'Create scalable data that provides sustainable data access within a large, enterprise company.', 'Experience working in Agile driven projects.', 'Proficiency in working with NoSQL paradigms for data (specifically, HDFS, and graph frameworks).', 'Data Engineer', ""Bachelor's Degree."", 'Responsibilities', 'Requirements']",Entry level,Full-time,Information Technology,Information Technology and Services,2020-10-22 10:04:13
Data Engineer,Upstart,"San Mateo, CA",24 hours ago,35 applicants,"['', '5+ year(s) of experience as a Data Engineer or Software Engineer', '401(k)', 'The Role', 'Code for reliability by leveraging unit tests, integration tests, and robust alerting', 'What You’ll Love', ""How You'll Make An Impact"", ' Python, Kafka, Avro, Spark, Kubernetes, Docker, Postgres, Redshift, Looker, Bash and Airflow ', 'Comprehensive medical, dental, and vision coverage', 'Partner with business stakeholders to meet their data requirements', 'Catered lunches + snacks & drinks', 'Clubs and Activities (Game Nights, Fitstarters, Uplift, Superwomen, Book Club, Investing Club, Money Discussions, Photography Club, Basketball teams and other Employee Resource Groups)', 'Generous vacation policy', 'Experience working with Python and SQL', ""What We're Looking For"", 'Our Stack', 'Life insurance and disability benefits', ' Implement streaming and batch ETL pipelines supporting both internal and external data sources Code for reliability by leveraging unit tests, integration tests, and robust alerting Partner with business stakeholders to meet their data requirements Maintain and expand our data warehouse (Redshift) and the business intelligence layer (Looker) sitting on top of it Participate in code reviews and provide production support for all DE processes Opportunity to lead and mentor junior engineers ', ' Competitive compensation (base + bonus & equity) Comprehensive medical, dental, and vision coverage Personal development and technology & ergonomic budgets Life insurance and disability benefits Clubs and Activities (Game Nights, Fitstarters, Uplift, Superwomen, Book Club, Investing Club, Money Discussions, Photography Club, Basketball teams and other Employee Resource Groups) Generous vacation policy 401(k) Catered lunches + snacks & drinks ', 'Python, Kafka, Avro, Spark, Kubernetes, Docker, Postgres, Redshift, Looker, Bash and Airflow', 'Experience with version control (git), issue tracking Jira), code reviews, and Agile development', 'Implement streaming and batch ETL pipelines supporting both internal and external data sources', 'Opportunity to lead and mentor junior engineers', 'Competitive compensation (base + bonus & equity)', 'Adaptability, curiosity and compulsive need to impose order upon chaos', 'Participate in code reviews and provide production support for all DE processes', 'Personal development and technology & ergonomic budgets', ' 5+ year(s) of experience as a Data Engineer or Software Engineer Experience working with Python and SQL Experience with version control (git), issue tracking Jira), code reviews, and Agile development Adaptability, curiosity and compulsive need to impose order upon chaos ', 'Maintain and expand our data warehouse (Redshift) and the business intelligence layer (Looker) sitting on top of it']",Entry level,Full-time,Information Technology,Computer Software,2020-10-22 10:04:13
Data Engineer/ Scientist,HTC Global Services,"McLean, VA",20 hours ago,Be among the first 25 applicants,"['', 'Requirements:', 'We are currently looking for Data Scientist/ Engineer. We will consider relocation for candidates with exceptional skills.', 'Benefits:', ' ', 'Experience with NLP and Data Science experience.', 'HTC’s competitive package includes besides compensation Health, Dental, Vision, Disability Cover, both Short and Long term, Life Insurance, Flexible Spending, 401k and Paid Vacation', 'Extract new data source from MongoDB using ADF and querying out required data at source', 'Creating Azure Data Factory (ADF) Pipelines which can read data from Azure Datalake(EDL) and MongoDB, copy filtered data to EDL, read functions from Postgres and finally get the cleaned data in EDL for tagging and training.', 'Be sure to reference the job number and title in the subject line. Relevant degree or its foreign equivalent is required.', 'Established in 1990, HTC Inc., a CMM Level 5 company with headquarters in Troy, Michigan, is a leading global Information Technology solution and BPO provider. HTC assists clients across multiple industry verticals, offering turnkey project lifecycle in, e-business, data warehousing, embedded systems, ECM, SCM, CRM, and ERP solutions. HTC Inc. offers ConnectIT, our Global Delivery Methodology that enables seamlessly delivery of outsourced IT services. HTC has global delivery centers across the globe. ', 'Create and manage inference pipelines in ADF which read from Databricks notebooks and import processed data to PostgresDB.', 'Candidates should e-mail their resumes to tennyson.regan@htcinc.com', 'Creating Azure Data Factory (ADF) Pipelines which can read data from Azure Datalake(EDL) and MongoDB, copy filtered data to EDL, read functions from Postgres and finally get the cleaned data in EDL for tagging and training.Create and manage inference pipelines in ADF which read from Databricks notebooks and import processed data to PostgresDB.Extract new data source from MongoDB using ADF and querying out required data at sourceCreating and managing postgres schemas, tables, functions in PGAdmin.Experience with NLP and Data Science experience.', ' EEO/M/F/V/H', 'Creating and managing postgres schemas, tables, functions in PGAdmin.', 'About the Job', 'HTC – A brief profile', 'Data Scientist/ Engineer.', '\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0', 'tennyson.regan@htcinc.com']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2020-10-22 10:04:13
Data Engineer,Peloton Interactive,"New York, NY",1 hour ago,Be among the first 25 applicants,"['', 'Familiar with REST for accessing cloud based services.', 'Nice to have', 'Develop the vision and map strategy to provide proactive solutions and enable stakeholders to extract insights and value from data.', 'Understand end to end data interactions and dependencies across complex data pipelines and data transformation and how they impact business decisions.', 'About Peloton', 'Experience running Agile methodology and applying Agile to data engineering.', 'Experience with Python or Java programming languages.', 'Has experiences with GIT, Github, JIRA and SCRUM.', ' Familiar with AWS ecosystem, including RDS, Glue, Athena, etc. Has experiences with Apache Hadoop, Hive, Spark and PySpark. ', 'Experience with big data architectures and data modeling to efficiently process large volumes of data.', '2+ years in building a data warehouse and data pipelines. Or, 3+ years in data intensive engineering roles.', 'Understand the data needs of different stakeholders across multiple business verticals including Finance, Marketing, Logistics, Product etc. ', ' Understand the data needs of different stakeholders across multiple business verticals including Finance, Marketing, Logistics, Product etc.  Develop the vision and map strategy to provide proactive solutions and enable stakeholders to extract insights and value from data. Understand end to end data interactions and dependencies across complex data pipelines and data transformation and how they impact business decisions. Design best practices for big data processing, data modeling and warehouse development throughout the company.  ', 'Comfortable with Linux operating system and command line tools such as Bash.', 'Excellent knowledge about databases, such as PostgreSQL and Redshift.', 'Has experiences with Apache Hadoop, Hive, Spark and PySpark.', 'Familiar with AWS ecosystem, including RDS, Glue, Athena, etc.', 'Background in ETL and data processing, know how to transform data to meet business goals.', 'Experience with Java, JDBC, AWS, SDK', ' Familiar with at least one of the programming languages: Python, Java. Comfortable with Linux operating system and command line tools such as Bash. Familiar with REST for accessing cloud based services. Excellent knowledge about databases, such as PostgreSQL and Redshift. Has experiences with GIT, Github, JIRA and SCRUM. 2+ years in building a data warehouse and data pipelines. Or, 3+ years in data intensive engineering roles. Experience with big data architectures and data modeling to efficiently process large volumes of data. Background in ETL and data processing, know how to transform data to meet business goals. Experience developing large data processing pipelines on Apache Spark. Experience with Python or Java programming languages. Strong understanding of SQL and working knowledge of using SQL(prefer PostgreSQL and Redshift) for various reporting and transformation needs. Excellent communication, adaptability and collaboration skills. Experience running Agile methodology and applying Agile to data engineering. Experience with Java, JDBC, AWS, SDK ', 'Responsibilities', 'Strong understanding of SQL and working knowledge of using SQL(prefer PostgreSQL and Redshift) for various reporting and transformation needs.', 'Design best practices for big data processing, data modeling and warehouse development throughout the company. ', 'Excellent communication, adaptability and collaboration skills.', 'Experience developing large data processing pipelines on Apache Spark.', 'Familiar with at least one of the programming languages: Python, Java.', 'Requirements']",Entry level,Full-time,Information Technology,"Health, Wellness and Fitness",2020-10-22 10:04:13
Data Engineer,7-Eleven,"Irving, TX",1 hour ago,Be among the first 25 applicants,"['', ' 7-10 years of work experience, Education- Masters preferred ', '▶ Who We Are', ' Code deployments, Dev Ops, Reviews ', ' Participating in designing new data applications ', ' Knowledgeable in cloud platforms (preferable AWS: both traditional EC2 and serverless Lambda), micro-services architecture, CI/CD solutions (including Docker), DevOps principles, message queue system ', ' Be Courageous with Your Point of View ', ' Be Accountable ', '▶ How We Lead', 'redefining convenience and the customer experience', ' 7-10 years of work experience, Education- Masters preferred  Expert knowledge of Big Data technologies including but not limited to Python and/or Databricks  Strong Analytical and problem solving skills  Knowledgeable in cloud platforms (preferable AWS: both traditional EC2 and serverless Lambda), micro-services architecture, CI/CD solutions (including Docker), DevOps principles, message queue system  Proficiency in API security frameworks, token management and user access control including OAuth, JWT, etc  Solid foundation and understanding of relational and NoSQL database princples', ' Be Customer Obsessed ', ' Do the Right Thing ', '▶ About This Opportunity', ' Automates Data pipelines and develops different libraries for Statistical Algorithms that helps interactions between 7-11 stores, customers and products. ', 'continuous improvement', ' Challenge the Status Quo ', ' Act Like an Entrepreneur ', ' Proficiency in API security frameworks, token management and user access control including OAuth, JWT, etc ', ' Designs and Develops Analytics and Data Integration solution for 7-11 Retail problems.  Automates Data pipelines and develops different libraries for Statistical Algorithms that helps interactions between 7-11 stores, customers and products.  Developing and implementing different aggregations, features and builds data pipelines to solve different problems related to supply chain management, dynamic pricing, customer preferance etc  Participating in designing new data applications  Code deployments, Dev Ops, Reviews ', 'better serve the needs of our customers', ' Expert knowledge of Big Data technologies including but not limited to Python and/or Databricks ', ' Developing and implementing different aggregations, features and builds data pipelines to solve different problems related to supply chain management, dynamic pricing, customer preferance etc ', 'entrepreneurial spirit', 'Leadership Principles', ' Designs and Develops Analytics and Data Integration solution for 7-11 Retail problems. ', ' Have an “It Can Be Done” Attitude ', ' Solid foundation and understanding of relational and NoSQL database princples', ' innovation', 'come make history with us', ' Strong Analytical and problem solving skills ']",Entry level,Full-time,Information Technology,Retail,2020-10-22 10:04:13
Data Engineer,Daugherty Business Solutions,"Columbus, Ohio Metropolitan Area",,N/A,"['', 'Daugherty Business Solutions, a leading advisory services and technology consulting firm will be expanding its operations to open a new, world-class Software Development Center in Columbus, Ohio to support its rapid growth and to engage the region’s diverse talent pool and thriving business community.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Collaborate and work closely with team to build data platforms.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Life, disability and long-term care insurance.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Recommend ways to improve data reliability, efficiency and quality.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Work with team members and functional leads to understand existing data requirements and validation rules to support moving existing data warehouse workloads into a distributed data platform.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Some experience with programming Languages, such as Scala, Java, R and Python.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Create custom software components (e.g. specialized UDFs) and analytics applications.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Knowledge of cloud and distributed systems principles, including load balancing, networks, scaling, and in-memory versus disk.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Maintain and manage Hadoop clusters in development and production environments.', 'Daugherty is hiring experienced Data Engineers to join our Columbus-based team. \xa0The ideal employee is a problem solver with the ability to utilize insights, creativity and perspective to drive business success for our clients.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Revenue sharing and a 401(k) retirement savings plan.', 'We offer members of Team Daugherty:', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Intermediate level of SQL programming and query performance tuning techniques for data integration and consumption using design for optimum performance against large data assets within an OLTP, OLAP and MPP architecture.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Implement & automate high-performance algorithms, prototypes and predictive models.', 'When you are a Daugherty employee, your job doesn’t end when a contract is up. You stay on as an indispensable member of the team with career growth opportunities tailored to your interests and talents. We want you to be eager to take on a new challenge. We are always 100% honest about what to expect, because we don’t want Daugherty to be just another job; we want Daugherty to be your dream job.\xa0', 'As a Data Engineer you will have the opportunity to:', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Little to no travel.', 'Due to COVID-19, most of our employees are working remotely. We’ve implemented a virtual hiring process and continue to interview candidates by phone or video and are onboarding new hires remotely. We value the safety of each member of our community because we know we’re all in this together.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Interest in Hadoop family languages including Pig and Hive.', 'If you require accommodations or assistance to complete the online application process, please inform any recruiter you are working with (or send an email to careers@daugherty.com) and identify the type of accommodation or assistance you are requesting. Do not include any medical or health information in this email. The recruiting team will respond to your email promptly.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Exposure to stream-processing and messaging, such as Storm, Spark-Streaming, Kafka and MQ.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Assemble large, complex data sets that meet functional/non-functional business requirements.', 'We are looking for motivated people with:', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Excellent health, dental and vision insurance.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Understanding of DevOps and CI/CD toolset, such as Jenkins, GitLab CI, Buildbot, Drone and Bamboo.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Proven ability to pick up new languages and technologies quickly.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Contribute to the creation and maintenance of optimal data pipeline architectures.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Interest in Python, R, sh/bash and JVM-based languages including Scala and Java.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience building data pipelines to connect analytics stacks, client data visualization tools and external data sources.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Interest working with AWS technologies such as Redshift, RDS, S3, EMR, ADP, Hive, Kinesis, ', 'Daugherty Business Solutions is an inclusive Equal Employment Opportunity employer that considers applicants without regard to gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Robust career development and training.', 'Interested? Apply today to take your career to the next level!', 'Data & Analytics', '\xa0', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Employ a variety of languages and tools to marry systems together.', 'From Ron Daugherty, founder and CEO of Daugherty Business Solutions:\xa0“I am excited to announce that Daugherty is expanding to Columbus, OH.\xa0This expansion is a direct result of the hard work of our consultants during this pandemic.\xa0Daugherty teammates continue to find ways to add even more value for our clients and to increase the demand for our consulting expertise.\xa0Columbus is a growing metro area with a strong base of talent, excellent universities, and interesting perspective clients.\xa0That’s why I’m proud to add Columbus to our existing development centers across the country.”', 'In addition to existing Software Development Centers across the organization; in Minneapolis, Atlanta, Dallas, Chicago, NY and the St. Louis Headquarters, the team in Columbus will strategically support Daugherty’s growth strategy and commitment to delivering high quality software fast and effectively for its Fortune 500 clients.\xa0', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Familiarity with high performance data libraries including Spark, NumPy and TensorFlow.', 'SNS/SQS and QuickSight.']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2020-10-22 10:04:13
Data Engineer,Spectraforce Technologies,"Durham, NC",16 hours ago,27 applicants,"['', 'Excellent quantitative, problem solving and analytic skills', 'Job Description', 'Own delivery of moderately sized data engineering projects', 'Duration: 4+ Months (strong possibility of extension)\xa0', 'Document, and test moderate data systems that bring together data from disparate sources, making it available to data scientists, and other users using scripting and/or programming languages', 'Analyze and evaluate databases in order to identify and recommend improvements and optimization', 'Define and implement integrated data models, allowing integration of data from multiple sources', 'Experience with Hadoop, Hive and/or other Big Data technologies', 'Reliable task estimation skills', 'Ensure performance and reliability of data processes', 'Ability to keep up with a rapidly evolving technology space', 'Job Title: Data Engineer', '3 years programming experience in Python, R or other programming language', 'Define and implement data stores based on system requirements and consumer requirements', 'Collaborate with cross functional team to resolve data quality and operational issues and ensure timely delivery of products', 'Bachelor’s degree in Computer Science or related field or equivalent experience3 years of SQL programming skills (Intermediate to Advance SQL programming skills)3 years programming experience in Python, R or other programming languageDemonstrated experience working with large and complex data setsExperience with business intelligence tools (Tableau)', 'Experience working in AWS and/or using Linux based systems', 'Ability to document data pipeline architecture and design', 'Design eye-catching visualizations to convey information to users', 'Participate in requirements gathering sessions with business and technical staff to distill technical requirement from business requests', 'The Data Engineer will work closely with senior engineers, data scientists and other stakeholders to design and maintain moderate to advanced data models. The Data Engineer is responsible for developing and supporting advanced reports that provide accurate and timely data for internal and external clients. The Data Engineer will design and grow a data infrastructure that powers our ability to make timely and data-driven decisions.', 'Ability to communicate risks, problems, and updates to leadership', 'Demonstrated experience working with large and complex data sets', 'Experience with Hadoop, Hive and/or other Big Data technologiesExperience with ETL or Data Pipeline toolsExperience with query and process optimizationExperience working in AWS and/or using Linux based systemsAbility to translate task/business requirements into written technical requirementsReliable task estimation skillsExcellent quantitative, problem solving and analytic skillsAbility to document data pipeline architecture and designAbility to collaborate effectively with business stakeholders, performance consultants, data scientists, and other data engineersProficient in use of MS Office applications including expert level Excel programmingAbility to quickly become an expert in operational processes and data of lines of businessAbility to troubleshoot and document findings and recommendationsAbility to communicate risks, problems, and updates to leadershipAbility to keep up with a rapidly evolving technology space', 'Hiring Requirements', 'Extract data from multiple sources, integrate disparate data into a common data model, and integrate data into a target database, application, or file using efficient programming processesDocument, and test moderate data systems that bring together data from disparate sources, making it available to data scientists, and other users using scripting and/or programming languagesWrite and refine code to ensure performance and reliability of data extraction and processingParticipate in requirements gathering sessions with business and technical staff to distill technical requirement from business requestsDevelop SQL queries to extract data for analysis and model constructionOwn delivery of moderately sized data engineering projectsDefine and implement integrated data models, allowing integration of data from multiple sourcesDesign and develop scalable, efficient data pipeline processes to handle data ingestion, cleansing, transformation, integration, and validation required to provide access to prepared data sets to analysts and data scientistsEnsure performance and reliability of data processesDefine and implement data stores based on system requirements and consumer requirementsDocument and test data processes including performance of through data validation and verificationCollaborate with cross functional team to resolve data quality and operational issues and ensure timely delivery of productsDevelop and implement scripts for database and data process maintenance, monitoring, and performance tuningAnalyze and evaluate databases in order to identify and recommend improvements and optimizationDesign eye-catching visualizations to convey information to users', 'Write and refine code to ensure performance and reliability of data extraction and processing', 'Proficient in use of MS Office applications including expert level Excel programming', 'Bachelor’s degree in Computer Science or related field or equivalent experience', 'Ability to troubleshoot and document findings and recommendations', 'Experience with business intelligence tools (Tableau)', 'Extract data from multiple sources, integrate disparate data into a common data model, and integrate data into a target database, application, or file using efficient programming processes', '3 years of SQL programming skills (Intermediate to Advance SQL programming skills)', 'Ability to translate task/business requirements into written technical requirements', 'Experience with ETL or Data Pipeline tools', 'Location: Durham, NC', 'Develop and implement scripts for database and data process maintenance, monitoring, and performance tuning', 'Ability to quickly become an expert in operational processes and data of lines of business', 'Experience with query and process optimization', 'Hiring Preferences', 'Job Profile Summary', 'Develop SQL queries to extract data for analysis and model construction', 'Document and test data processes including performance of through data validation and verification', 'Ability to collaborate effectively with business stakeholders, performance consultants, data scientists, and other data engineers', 'Design and develop scalable, efficient data pipeline processes to handle data ingestion, cleansing, transformation, integration, and validation required to provide access to prepared data sets to analysts and data scientists']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2020-10-22 10:04:13
Data Engineer,Blackcomb Consultants,"Tampa, FL",17 hours ago,Be among the first 25 applicants,"['', 'Experience in Cloud (Azure) Data engineering ', 'ABOUT THE POSITION', ' ', 'Strong SQL and relational database design and development skills. Complex T-SQL stored procedure design, coding, troubleshooting and performance tuning Require a very good working experience of T-SQL. Proficiency in complex stored procedures, writing complex queries with many joins and subqueries, user defined functions (UDF), using tools like SQL Profiler and Database Tuning Advisor (DTA) Strong knowledge & hands on experience in Python language, handling structured & semi-structured large data sets Very good knowledge of reporting tools such as Cognos, Power BI Cloud Strong data analysis and complex report/data extract development skills. Experience in bulk importing CSV, XML and Flat files data using Bulk copy program (BCP) Strong knowledge of importing semi-structured data (large complex, nested JSON files) Strong knowledge of relational and star schema/dimensional database design, data warehouse designs/concepts Must have the ability to participate on group discussion with the analytics and business teams to understand the requirement. Analyze the requirements details, prepare functional specification notes to use in design, development, and testing Ability to be flexible and work within a team environment ', 'Perform in depth & quick data analysis and data extraction activities for business stakeholders Analyze source systems data to understand the data structures, definitions, and anomalies Analyze data extract/report requirements to determine development methods, development templates, and to determine job scheduling. Learn existing critical business application business logic and stored procedures used by the Analytics team. Make code changes in the stored procedures to implement new changes and resolve defects Actively participate in detailed technical design, development and implementation of advanced analytics data project using existing and emerging technology platforms Work with both technical staff and business partners to translate requirements into technical implementations Extensive hands on experience in coding, designing, developing, and maintaining using SQL & Python to pull large data sets including structured & semi-structured. Support SSIS ETL process and jobs. Apply advanced level of SSIS development and support capabilities to meet organization’s business needs and requirement. Implement complex ETL/ELT solution using SSIS/python against Data Warehouse Estimate time to deliver and participating in an Agile team approach Attend regular standup meetings, take direction, follow recommendations Create & maintain technical design documents & mapping specifications ', 'Perform in depth & quick data analysis and data extraction activities for business stakeholders ', 'ABOUT BLACKCOMB CONSULTANTS\xa0Blackcomb Consultants is a trusted Guidewire partner that specializes in providing IT professional services to clients in the property and casualty insurance arena. We have a strong reputation for delivering exceptional Guidewire core system implementations, upgrades, cloud-hosted services, production support, change management consulting, and performance improvement offerings to our clients. ', 'ABOUT THE POSITION\xa0We are seeking a\u202fData Engineer to support our client projects on our delivery team. ', '\xa0PREFERRED QUALIFICATIONS ', 'Extensive hands on experience in coding, designing, developing, and maintaining using SQL & Python to pull large data sets including structured & semi-structured. ', 'Estimate time to deliver and participating in an Agile team approach ', 'OR', 'WHERE YOU WILL BE LOCATED', 'Strong data analysis and complex report/data extract development skills. Experience in bulk importing CSV, XML and Flat files data using Bulk copy program (BCP) ', 'Experience in Business intelligence reporting /OLAP tools, specifically Cognos and PowerBI ', 'Strong SQL and relational database design and development skills. ', 'Very good knowledge of reporting tools such as Cognos, Power BI Cloud ', 'Create & maintain technical design documents & mapping specifications ', 'Attend regular standup meetings, take direction, follow recommendations ', 'SQL Server Integration (SSIS) experience ', 'SQL Server (2014-2019) relational database experience SQL and Microsoft T-SQL Query and large data extract experience SQL Server Integration (SSIS) experience Data warehouse development experience ', 'WHAT WE OFFER', 'Experience in data extraction using REST APIs ', 'Implement complex ETL/ELT solution using SSIS/python against Data Warehouse ', 'Familiarity with Guidewire Insurance suit (Claims & Policy center preferred). Insurance related information delivery experience. Property & Casualty insurance and experience. Working knowledge of insurance industry data standards. DB2, Oracle or other comparable relational database experience. Experience in Business intelligence reporting /OLAP tools, specifically Cognos and PowerBI Experience in MS SQL BIDS – SSRS, SSAS Experience in Cloud (Azure) Data engineering Experience in data extraction using REST APIs ', 'Python, R (preferred) language .NET OR\xa0C# language Web scrapping & data extraction using Python ', 'Ad-hoc report development & delivery experience ', 'Must have the ability to participate on group discussion with the analytics and business teams to understand the requirement. Analyze the requirements details, prepare functional specification notes to use in design, development, and testing ', 'PREFERRED QUALIFICATIONS', '4 + years to include:SQL Server (2014-2019) relational database experience SQL and Microsoft T-SQL Query and large data extract experience SQL Server Integration (SSIS) experience Data warehouse development experience ', 'Work with both technical staff and business partners to translate requirements into technical implementations ', 'ABOUT BLACKCOMB CONSULTANTS', 'Require a very good working experience of T-SQL. Proficiency in complex stored procedures, writing complex queries with many joins and subqueries, user defined functions (UDF), using tools like SQL Profiler and Database Tuning Advisor (DTA) ', 'Strong knowledge & hands on experience in Python language, handling structured & semi-structured large data sets ', 'WHERE YOU WILL BE LOCATEDThe preferred work location is Tampa, FL, but it can be remote as well. WHAT WE OFFER\xa0A competitive starting salary, health benefits, 401k plan, opportunity for Guidewire certification training and relocation assistance. ', '4 + year experience to include:Python, R (preferred) language .NET OR\xa0C# language Web scrapping & data extraction using Python ', 'Ability to be flexible and work within a team environment ', 'REQUIRED EDUCATION AND EXPERIENCE:', 'Analyze source systems data to understand the data structures, definitions, and anomalies ', 'Web scrapping & data extraction using Python ', 'DB2, Oracle or other comparable relational database experience. ', 'Property & Casualty insurance and experience. ', 'We are seeking dynamic, creative, passionate professionals to join our quickly growing technology company and bring their talents and expertise to our clients. Why is this work important? Our clients are property and casualty insurance companies that are dedicated to delivering quality customer service and, most importantly, compassionate support during times of need for their customers. ', '\xa0REQUIRED KNOWLEDGE, SKILLS AND ABILITIES: ', '\xa0REQUIRED EDUCATION AND EXPERIENCE: ', 'REQUIRED KNOWLEDGE, SKILLS AND ABILITIES:', 'Strong knowledge of importing semi-structured data (large complex, nested JSON files) ', 'Python, R (preferred) language ', 'Working knowledge of insurance industry data standards. ', 'Learn existing critical business application business logic and stored procedures used by the Analytics team. Make code changes in the stored procedures to implement new changes and resolve defects ', 'SQL and Microsoft T-SQL Query and large data extract experience ', 'SQL Server (2014-2019) relational database experience ', 'Experience in MS SQL BIDS – SSRS, SSAS ', 'Insurance related information delivery experience. ', 'Bachelor’s degree in MIS, Computer Science or Information Technology or related field OR 8 years relevant professional work experience. AND ', 'Support SSIS ETL process and jobs. Apply advanced level of SSIS development and support capabilities to meet organization’s business needs and requirement. ', 'Actively participate in detailed technical design, development and implementation of advanced analytics data project using existing and emerging technology platforms ', 'Strong knowledge of relational and star schema/dimensional database design, data warehouse designs/concepts ', 'Data warehouse development experience ', 'Blackcomb Consultants is an Equal Employment Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability, protected veterans’ status, age, or any other characteristic protected by laws.', '.NET OR\xa0C# language ', 'Familiarity with Guidewire Insurance suit (Claims & Policy center preferred). ', 'Complex T-SQL stored procedure design, coding, troubleshooting and performance tuning ', 'Analyze data extract/report requirements to determine development methods, development templates, and to determine job scheduling. ']",Mid-Senior level,Full-time,Information Technology,Insurance,2020-10-22 10:04:13
Data Engineer,Capital Group,"Los Angeles, CA",8 hours ago,Be among the first 25 applicants,"['', 'Building relationships with your customers, partner teams and the engineers on your team.', 'You believe that a team is strongest when it is diverse and includes multiple perspectives.', '“I can be myself at work.”', 'Interest and curiosity in emerging technologies on the web like GraphQL, web assembly, Lambda functions, MLaaS etc', ' Build large-scale distributed data processing systems, data lakes, and optimize for both computational and storage efficiency on cloud platforms like AWS. Design, implement and automate data pipelines sourcing data from internal and external systems, transforming the data for the optimal needs of various systems. Design data schema and operate cloud-based data warehouses and SQL/NoSQL/temporal database systems. Write Extract-Transform-Load (ETL) jobs and Spark/Hadoop jobs. Own the design, development and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisions. Monitor and troubleshoot operational or data issues in the data pipelines. Drive architectural plans and implementation for future data storage, ETL, reporting, and analytic solutions. Influence your team’s technical and business strategy by making insightful contributions to team priorities and approach.  Provide insightful code reviews, receive code reviews constructively and take ownership of outcomes (“you ship it, you own it”), working very efficiently and routinely deliver the right things in the front-end UI area. Building relationships with your customers, partner teams and the engineers on your team. Influence your team’s technical decisions by making insightful contributions to team priorities and approach.  ', 'Write Extract-Transform-Load (ETL) jobs and Spark/Hadoop jobs.', 'Receive 2-for-1 matching gifts for your charitable contributions and the opportunity to secure annual grants for the organizations you love', 'Experience writing and optimizing advanced SQL queries in a business environment with large-scale, complex datasets.', '“I Can Learn More About Capital Group.”', 'Req ID', 'Build large-scale distributed data processing systems, data lakes, and optimize for both computational and storage efficiency on cloud platforms like AWS.', 'What You’ll Be Doing', 'Influence your team’s technical decisions by making insightful contributions to team priorities and approach. ', 'Qualifications', '“I can influence my income.”', 'You believe there are generally multiple ways to solve a technical problem, each with different trade-offs.', '3+ years coding proficiency in at least one modern programming language (Python, Ruby, Java, etc.).', ""You've made mistakes in the past and have learned a lot from them. You apply this learning regularly."", 'Design, implement and automate data pipelines sourcing data from internal and external systems, transforming the data for the optimal needs of various systems.', 'Drive architectural plans and implementation for future data storage, ETL, reporting, and analytic solutions.', ""You are able to put yourself into your customer's shoes. You frequently immerse yourself in the customer experience to understand how you can better serve them."", '“I can apply in less than 4 minutes.”', 'Enjoy generous time-away and health benefits from day one, with the opportunity for flexible work options', 'Knowledge of software engineering practices & best practices for the software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations.', 'COVID-19 HIRING Our recruiting and onboarding activities are virtual during the pandemic and we’ve transitioned to a work-from-home environment until further notice. We are offering generous work-from-home benefits to improve our associate’s ability to work remotely.', 'Design data schema and operate cloud-based data warehouses and SQL/NoSQL/temporal database systems.', 'You have a background in data and software engineering and a passion to learn. ', 'Other location(s)', 'Own the design, development and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisions.', 'Monitor and troubleshoot operational or data issues in the data pipelines.', 'Relocation benefits offered', 'Experience in data architecture, databases (e.g., MySQL, Oracle, PostgreSQL), SQL and DDD/ER/ORM design.', '“I can lead a full life.”', '3+ years experience implementing big data processing technology Hadoop, Apache Spark, etc.', 'Experience in cloud-first design, preferably AWS (VPC, Serverless databases and functions, dynamic autoscaling, container orchestration, etc.).', ' BS in Computer Science or related field, or an equivalent in relevant work experience. 3+ years experience implementing big data processing technology Hadoop, Apache Spark, etc. 3+ years coding proficiency in at least one modern programming language (Python, Ruby, Java, etc.). Experience writing and optimizing advanced SQL queries in a business environment with large-scale, complex datasets. Experience in cloud-first design, preferably AWS (VPC, Serverless databases and functions, dynamic autoscaling, container orchestration, etc.). Experience in data architecture, databases (e.g., MySQL, Oracle, PostgreSQL), SQL and DDD/ER/ORM design. Interest and curiosity in emerging technologies on the web like GraphQL, web assembly, Lambda functions, MLaaS etc Knowledge of software engineering practices & best practices for the software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations. ', 'Influence your team’s technical and business strategy by making insightful contributions to team priorities and approach. ', 'BS in Computer Science or related field, or an equivalent in relevant work experience.', ' Enjoy generous time-away and health benefits from day one, with the opportunity for flexible work options Receive 2-for-1 matching gifts for your charitable contributions and the opportunity to secure annual grants for the organizations you love Access on-demand professional development resources that allow you to hone existing skills and learn new ones ', 'Location', 'Travel required', "" You have a background in data and software engineering and a passion to learn.  You've made mistakes in the past and have learned a lot from them. You apply this learning regularly. You believe there are generally multiple ways to solve a technical problem, each with different trade-offs. You approach projects, tasks, and unknowns with curiosity, and enjoy sharing what you know and what you learn with the people around you. You believe that a team is strongest when it is diverse and includes multiple perspectives. You are able to put yourself into your customer's shoes. You frequently immerse yourself in the customer experience to understand how you can better serve them. "", 'Provide insightful code reviews, receive code reviews constructively and take ownership of outcomes (“you ship it, you own it”), working very efficiently and routinely deliver the right things in the front-end UI area.', 'Access on-demand professional development resources that allow you to hone existing skills and learn new ones', 'You approach projects, tasks, and unknowns with curiosity, and enjoy sharing what you know and what you learn with the people around you.', 'Your Background And Who You Are']",Not Applicable,Full-time,Information Technology,Financial Services,2020-10-22 10:04:13
Data Engineer,Stevens Capital Management LP,"Philadelphia, PA",3 hours ago,Be among the first 25 applicants,"['', 'Requirements of the Candidate include', 'Implement robust solutions for adoption, storage and management of large volumes of data', '2+ years professional experience in data management role', 'Bachelor’s degree in Computer Science or applicable degree and very strong exposure to programming and computer systemsExperience with Python and C++Experience with data storage and manipulation using approaches such as HDF5, Pandas, and RDBMS/SQL2+ years professional experience in data management roleThe desire to work in a fast-paced, hardworking and committed environment with a talented teamStrong problem solving skills, critical thinking and clear written and verbal communicationsFinancial industry, market data and cloud data environment experience are beneficial', 'Strong problem solving skills, critical thinking and clear written and verbal communications', 'Join a team of financial software specialists responsible for the acquisition and management of financial market and related data required for research implementation of systematic trading strategies.', 'Experience with Python and C++', 'Bachelor’s degree in Computer Science or applicable degree and very strong exposure to programming and computer systems', 'Assist quantitative analysts in crafting custom, bespoke data sets', 'Requirements of the Candidate include:', 'Primary Responsibilities', 'Meet tight deadlines in an efficient manner', 'Primary Responsibilities:', 'Develop quality assurance systems in support of high integrity data sets', 'Experience with data storage and manipulation using approaches such as HDF5, Pandas, and RDBMS/SQL', 'Financial industry, market data and cloud data environment experience are beneficial', 'The desire to work in a fast-paced, hardworking and committed environment with a talented team', 'Support timely and fault tolerant data systems in support of production trading algorithms', 'Implement robust solutions for adoption, storage and management of large volumes of dataDevelop quality assurance systems in support of high integrity data setsAssist quantitative analysts in crafting custom, bespoke data setsSupport timely and fault tolerant data systems in support of production trading algorithmsMeet tight deadlines in an efficient manner']",Associate,Full-time,Information Technology,Investment Management,2020-10-22 10:04:13
Data Engineer,EQRx,"Cambridge, MA",5 hours ago,Be among the first 25 applicants,"['', 'You have 5+ years of data engineering experience in industry or equivalent.', 'You are an excellent communicator, empathetic with end users and internal customers. ', 'You thrive on mapping and designing ingestion and transformation of data from multiple sources, creating a cohesive data asset.', 'You have experience working in health care, life sciences, and/or with scientific data.', 'You have a curious mind, constantly looking into novel approaches for the organization of complex life scientific data, e.g. FHIR for clinical data.', 'Job Code: ', 'You are fluent in SQL and at least one scripting language. Preferably, you have experience in Spark/Scala, too.', 'You are an expert in cloud data warehousing tools (eg BigQuery, Snowflake) and ELT tools (eg Stitch, Fivetran, DBT). You thrive on building modern, cloud-native data pipelines and operations, with an ELT philosophy.', 'You intuitively think of how to organize, normalize, and store complex data, enabling both ETL and end users.', ""The Impact You'll Have"", 'ad hoc', 'EQRx is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability status, protected veteran status, or any other characteristic protected by law.', 'You have 5+ years of data engineering experience in industry or equivalent.You have experience working in health care, life sciences, and/or with scientific data.You are fluent in SQL and at least one scripting language. Preferably, you have experience in Spark/Scala, too.You intuitively think of how to organize, normalize, and store complex data, enabling both ETL and end users.You have a curious mind, constantly looking into novel approaches for the organization of complex life scientific data, e.g. FHIR for clinical data.You thrive on mapping and designing ingestion and transformation of data from multiple sources, creating a cohesive data asset.You are an expert in cloud data warehousing tools (eg BigQuery, Snowflake) and ELT tools (eg Stitch, Fivetran, DBT). You thrive on building modern, cloud-native data pipelines and operations, with an ELT philosophy.You are an excellent communicator, empathetic with end users and internal customers. Job Code: ', 'Your Superpowers', 'This role will help build a novel model of life sciences company, where data is stored and integrated across functions, informing every process from the beginning. You will speed the development and trialing of novel medicines by bringing a digital and data native technology stack to drug development. You will connect and integrate data sources that map the complex web of interrelationships that bring drugs to patients and empower experts to explore them. You will enable experts across EQRx to see further, connecting data from biology and chemistry, to the clinic and patients, to drug pricing and delivery, powering a modern, and radically new paradigm in drug development, bringing cheaper and more accessible medicines to the market.']",Mid-Senior level,Full-time,Information Technology,Pharmaceuticals,2020-10-22 10:04:13
Data Engineer,Visual Concepts,"Agoura Hills, CA",16 hours ago,Be among the first 25 applicants,"['', 'Expert communication, facilitation, and collaboration skills to effectively present, explain, influence, and advise within cross-functional teams.', ' Create statistical models that can predict and analyze player behavior, optimize engagement, and inform post-launch feature creation. Using created statistical models and analyses, interpret the underlying story to answer business and game design questions. Develop code to facilitate statistical queries and implement hooks to improve data gathering techniques. Work closely with development teams to provide insights into game quality, difficulty, and fun. Establish and distribute regular performance reports based on telemetry and advanced models. Collaborate with other departments to determine project needs and analytics requirements. Maintain data integrity and resolve issues found with incorrect or incomplete datasets. ', ' Video games industry experience and/or gaming familiarity. WWE knowledge and familiarity a plus. Experience with JIRA, DeltaDNA, Perforce, and Confluence. Experience with predictive modeling techniques and accomplished in use of R or Python machine learning and related modules.', 'Video games industry experience and/or gaming familiarity.', 'Experience with varied data modeling approaches.', 'Maintain data integrity and resolve issues found with incorrect or incomplete datasets.', 'What You Will Do', 'Establish and distribute regular performance reports based on telemetry and advanced models.', 'Expert in relational databases and SQL data carpentry.', 'Using created statistical models and analyses, interpret the underlying story to answer business and game design questions.', 'Minimum of 2 years of experience in data mining & analytics, building models with very large, complex and multi-dimensional data sets.', 'Work closely with development teams to provide insights into game quality, difficulty, and fun.', ' Minimum of 2 years of experience in data mining & analytics, building models with very large, complex and multi-dimensional data sets. Minimum of 1 year of basic programming experience with C++, and able to demonstrate C++ coding proficiency. Expert in relational databases and SQL data carpentry. Experience with varied data modeling approaches. Expert communication, facilitation, and collaboration skills to effectively present, explain, influence, and advise within cross-functional teams. ', 'Create statistical models that can predict and analyze player behavior, optimize engagement, and inform post-launch feature creation.', 'Minimum of 1 year of basic programming experience with C++, and able to demonstrate C++ coding proficiency.', 'Experience with predictive modeling techniques and accomplished in use of R or Python machine learning and related modules.', 'Experience with JIRA, DeltaDNA, Perforce, and Confluence.', 'WWE knowledge and familiarity a plus.', 'Collaborate with other departments to determine project needs and analytics requirements.', 'Preferred Qualifications', 'Develop code to facilitate statistical queries and implement hooks to improve data gathering techniques.', 'Requirements']",Entry level,Full-time,Information Technology,Computer Software,2020-10-22 10:04:13
