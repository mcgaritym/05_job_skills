job_title,company,location,date_posted,applicants,job_text,seniority_level,employment_type,job_function,industries,date_scraped
Data Engineer,Apple,"Austin, TX",7 hours ago,Be among the first 25 applicants,"['', 'Education & Experience', 'Key Qualifications', 'Summary', 'Description']",Not Applicable,Full-time,Information Technology,Consumer Electronics,2021-03-02 16:49:33
Data Engineer,Amazon,"Seattle, WA",14 hours ago,Be among the first 25 applicants,"['', 'Core Responsibilities', ' Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers', ' Hands-on experience and advanced knowledge of SQL', ' 5+ years of industry experience in Software Development, Data Engineering, Business Intelligence, Data Science, or related field with a track record of manipulating, processing, and extracting value from large datasets', ' Knowledge of Data Management fundamentals and Data Storage principles', ' Contribute to the architecture, design and implementation of next generation BI solutions – including streaming data applications.', ' Proficiency in at least one modern programming language such as Python, Perl, Ruby, or Java.', ' Contribute to the architecture, design and implementation of next generation BI solutions – including streaming data applications. Manage AWS resources including EC2, RDS, Redshift, Kinesis, EMR, Lambda etc. Collaborate with data scientists, BIEs and BAs to deliver high quality data architecture and pipelines. Interface with other technology teams to extract, transform, and load data from a wide variety of data sources Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers', 'Preferred Qualifications', ' Experience using business intelligence reporting tools such as Tableau or Quick Sight', ' Masters in computer science, mathematics, statistics, economics, or other quantitative field', ' Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy Experience providing technical leadership and mentoring other engineers for best practices on data engineering Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations Masters in computer science, mathematics, statistics, economics, or other quantitative field', "" Bachelor's degree in Computer Science, Engineering, Mathematics, or a related technical discipline"", ' Experience in Data Modeling, ETL Development, and Data Warehousing', ' Experience providing technical leadership and mentoring other engineers for best practices on data engineering', 'Description', "" Bachelor's degree in Computer Science, Engineering, Mathematics, or a related technical discipline 5+ years of industry experience in Software Development, Data Engineering, Business Intelligence, Data Science, or related field with a track record of manipulating, processing, and extracting value from large datasets Hands-on experience and advanced knowledge of SQL Experience in Data Modeling, ETL Development, and Data Warehousing Experience using business intelligence reporting tools such as Tableau or Quick Sight Experience using big data technologies (Hadoop, Hive, Hbase, Spark, EMR, etc.) Knowledge of Data Management fundamentals and Data Storage principles Proficiency in at least one modern programming language such as Python, Perl, Ruby, or Java."", ' Interface with other technology teams to extract, transform, and load data from a wide variety of data sources', ' Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations', 'Basic Qualifications', ' Experience using big data technologies (Hadoop, Hive, Hbase, Spark, EMR, etc.)', ' Manage AWS resources including EC2, RDS, Redshift, Kinesis, EMR, Lambda etc.', ' Collaborate with data scientists, BIEs and BAs to deliver high quality data architecture and pipelines.', 'Company', ' Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy']",Not Applicable,Full-time,Information Technology,Computer Software,2021-03-02 16:49:33
Data Engineer,SambaSafety,"Denver, CO",17 hours ago,33 applicants,"['', 'Understanding of modern Devops concepts. Docker, Kubernetes, Serverless, Terraform.', 'Implement data classification of incoming data and manage access control.', 'Lots of Samba swag', 'Build an CICD automation pipeline facilitating automated deployment and automated testing.', 'Extensive experience deploying software to a cloud platform environment. AWS, GCP, Azure.', 'Code for and architect streaming pipelines to include data acquisition, staging, as well as integration of new data sources.', 'Understanding of micro-services architectures.', 'A chance to work with some of the brightest minds in technology', 'Provide data usage pattern for analytics, API and other consumption patterns from target data store.', '3+ years experience with ETL tools and or streaming concepts', 'Most importantly you will be part of a team that is raising the bar when it comes to data, data movement and data stewardship. Working in a unique environment of “build it, test it, support it, own it” that makes your daily contributions something you can be proud of.', '3+ years Java development experience, or an equivalent language with a desire to learn new things.', 'Some exposure to Hadoop, Hive, Spark, PrestoDB.', 'Data Movement at a large scale.Code for and architect streaming pipelines to include data acquisition, staging, as well as integration of new data sources.Implement data classification of incoming data and manage access control.Develop transformation processes for handling batch and streaming data.Participate in discussions around dimensional analysis and entity resolution for a complex disparate data sourced system.Provide data usage pattern for analytics, API and other consumption patterns from target data store.Build an CICD automation pipeline facilitating automated deployment and automated testing.Deliver end to end comprehensive documentation along with code samples for other teams to leverage.Most importantly you will be part of a team that is raising the bar when it comes to data, data movement and data stewardship. Working in a unique environment of “build it, test it, support it, own it” that makes your daily contributions something you can be proud of.', 'Strong knowledge of modern software engineering principles, patterns and best-practices.', 'Deliver end to end comprehensive documentation along with code samples for other teams to leverage.', 'What You’ll Do', 'Data Movement at a large scale.', 'Experience with distributed messaging and streaming technologies, RabbitMQ, Kinesis, Kafka, Spring cloud data flow, NiFi.', 'Unlimited Paid Time Off and Paid Volunteer Days401k match and generous Healthcare Benefits including a fully employer paid family medical planWellness &Tuition ReimbursementZoom Happy HoursFlexible Work From Home schedule & a Monthly Internet stipendLots of Samba swagSamba Virtual Events including our famous Samba SprintA chance to work with some of the brightest minds in technology', 'Participate in discussions around dimensional analysis and entity resolution for a complex disparate data sourced system.', 'Samba Virtual Events including our famous Samba Sprint', 'Strong communication skills. The ability to effectively explain technical concepts to team members, architects and team leads.', 'Experience with NoSQL, as well as relational data stores. PostgreSQL, Mysql, RedshiftDB, Redis, Cassandra, Snowflake, etc.', 'Flexible Work From Home schedule & a Monthly Internet stipend', 'Develop transformation processes for handling batch and streaming data.', 'Degree in Computer Science, Software Engineering, or a related discipline.3+ years experience with ETL tools and or streaming concepts3+ years Java development experience, or an equivalent language with a desire to learn new things.Strong knowledge of modern software engineering principles, patterns and best-practices.Understanding of micro-services architectures.Experience designing and supporting high traffic, highly available systems.Strong communication skills. The ability to effectively explain technical concepts to team members, architects and team leads.Extensive experience deploying software to a cloud platform environment. AWS, GCP, Azure.Understanding of modern Devops concepts. Docker, Kubernetes, Serverless, Terraform.Experience with NoSQL, as well as relational data stores. PostgreSQL, Mysql, RedshiftDB, Redis, Cassandra, Snowflake, etc.Experience with distributed messaging and streaming technologies, RabbitMQ, Kinesis, Kafka, Spring cloud data flow, NiFi.Some exposure to Hadoop, Hive, Spark, PrestoDB.Capable of delivering on multiple competing priorities with little supervision.', 'Wellness &Tuition Reimbursement', 'Zoom Happy Hours', 'Capable of delivering on multiple competing priorities with little supervision.', 'Who We Are', 'Data Engineer', 'What You’ll Need', 'Benefits And Perks', 'Experience designing and supporting high traffic, highly available systems.', 'Degree in Computer Science, Software Engineering, or a related discipline.', 'Unlimited Paid Time Off and Paid Volunteer Days', '401k match and generous Healthcare Benefits including a fully employer paid family medical plan']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-02 16:49:33
Data Engineer,CommunityCare,"Tulsa, OK",51 minutes ago,Be among the first 25 applicants,"['', 'Create and maintain optimal data pipeline architecture to support our next generation of products and data initiatives.', 'Create and maintain optimal data pipeline architecture to support our next generation of products and data initiatives.Assemble large, complex data sets that meet functional business requirements.Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability.Experience in the development of SSIS, ETL and other standardized data management tools.Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.Performs other duties as required.', 'QUALIFICATIONS:', 'Experience in the development of SSIS, ETL and other standardized data management tools.', 'Build processes supporting data transformation, data structures, metadata, dependency and workload management.', 'Willingness to work in a high-tech, continually evolving, innovative environment.', 'Build processes supporting data transformation, data structures, metadata, dependency and workload management.Strong project management and organizational skills.Ability to work independently, handle multiple tasks and projects simultaneously.', 'College degree or equivalent experience required.', 'Ability to work independently, handle multiple tasks and projects simultaneously.', 'EDUCATION/EXPERIENCE:', 'Project management skills preferred.', 'JOB SUMMARY:', 'KEY RESPONSIBILITIES:', 'Strong project management and organizational skills.', 'Assemble large, complex data sets that meet functional business requirements.', 'Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.', 'The Data Engineer will be responsible for expanding, optimizing and monitoring our data and data pipeline architecture, as well as optimizing data flow and collection across organizational teams. The Data Engineer will support our software engineers, database architects and data analysts on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. ', 'Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability.', 'Performs other duties as required.', '\xa0', 'College degree or equivalent experience required.Project management skills preferred.Willingness to work in a high-tech, continually evolving, innovative environment.']",Not Applicable,Full-time,Health Care Provider,Insurance,2021-03-02 16:49:33
Data Engineer ,Austin Fraser,United States,,N/A,"['', 'Postgres', 'ETL', 'Austin Fraser has partnered with an exciting established company based in Boulder with a strong remote presence. They run a global platform and are looking for a solid Data Engineer to join their team. ', 'NoSQL', ""What you'll do:"", 'Data visualization tools', 'ETLNoSQLPostgresData visualization tools', 'Work with 3rd party API data integration', 'Ingest, clean & digest data ', 'Build data solutions', 'Ingest, clean & digest data Work with 3rd party API data integrationBuild data solutionsWork in an agile environment', 'Work in an agile environment', ""What you'll work with:""]",Mid-Senior level,Full-time,Information Technology,Staffing and Recruiting,2021-03-02 16:49:33
Data Engineer,Calm,San Francisco Bay Area,7 hours ago,Be among the first 25 applicants,"['', 'Build data integrations within our data platform and between partners\xa0Write well-tested, production ready code in Python, Go, and/or SQLImprove the efficiency, reliability, and latency of our data systemCreate automated, highly reliable data pipelinesHelp define and craft our data modelOnboard onto Airflow and Redshift and assist with improvements and maintenanceDefine, design, and build data testing and quality frameworksTest all code written and ensure production readiness before shippingWork cross-functionally with our product, marketing and growth teams on complex and exciting projects that propel Calm’s businessStays up-to-date with high-potential new technologies, and can evaluate and present to the team for Calm’s use case', 'Test all code written and ensure production readiness before shipping', 'Commuter benefits', 'Excellent sense of how to drive business impact', 'At least 2 years of data engineering experience', 'Unlimited PTO', 'Write well-tested, production ready code in Python, Go, and/or SQL', 'Improve the efficiency, reliability, and latency of our data system', '401K', 'Fluent in SQL', 'We pay your medical, dental, & vision insurance premiums', 'The heart of Calm is digital but the brand is expanding offline into a variety of products and services that bring more peace, clarity and perspective into people’s busy lives. We are building Calm into the Nike of the Mind. We believe Calm can become one of the most valuable and meaningful brands in the world.', 'Calm is deeply committed to diversity, equity and inclusion, both in our hiring practices and in our experiences as a Calm employee. We strive to create a mindful and respectful environment where everyone can bring their authentic self to work, and experience a culture that is free of harassment, racism, and discrimination.\xa0', 'Experience building and maintaining critical, reliable ETL pipelines', 'Outcomes', 'Mission', 'And much more!', 'Calm is proud to be an Equal Employment Opportunity employer. We do not discriminate based upon race, religion, color, national origin, sex, sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, genetic information, political views or activity, or other applicable legally protected characteristics.\xa0', 'Work cross-functionally with our product, marketing and growth teams on complex and exciting projects that propel Calm’s business', 'Life insurance and disability benefits', 'Experience writing production-level code in Python or Go\xa0', 'Experience working with at least one data processing tool such as Spark, Redshift/Snowflake, Airflow/Luigi, etcBasic working knowledge of other big data technologies\xa0Experience building and maintaining critical, reliable ETL pipelinesExperience writing production-level code in Python or Go\xa0Fluent in SQLProactive communicator who can translate between technical and non-technical stakeholdersExcellent sense of how to drive business impactTeam player who gives and takes feedback in a thoughtful way, and loves to help others.Work cross-functionally with our product, marketing and growth teams on complex and exciting projects that propel Calm’s businessBonus points if you have some devops related-experience working with, e.g., kubernetes, terraform, etc. Or if you have previous experience as a Data Scientist or analystAt least 2 years of data engineering experience', 'Define, design, and build data testing and quality frameworks', 'Stays up-to-date with high-potential new technologies, and can evaluate and present to the team for Calm’s use case', 'We have a simple mission at Calm: To make the world a happier and healthier place.', 'Onboard onto Airflow and Redshift and assist with improvements and maintenance', 'Bonus points if you have some devops related-experience working with, e.g., kubernetes, terraform, etc. Or if you have previous experience as a Data Scientist or analyst', 'Experience working with at least one data processing tool such as Spark, Redshift/Snowflake, Airflow/Luigi, etc', 'Competitive salary and equityUnlimited PTOWe pay your medical, dental, & vision insurance premiums401KCommuter benefitsLife insurance and disability benefitsApple equipmentOpportunity to work with a product focused on making the world happier and healthierAnd much more!', 'Basic working knowledge of other big data technologies\xa0', 'Competencies', 'Proactive communicator who can translate between technical and non-technical stakeholders', '\xa0', 'Apple equipment', 'Calm is also committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures. Please inform Calm’s Recruiting team if you need any assistance completing any forms or to otherwise participate in the application process.', 'Create automated, highly reliable data pipelines', 'Over 75 Million people have downloaded the app and we are growing by 100,000 new downloads a day. The company is profitable and headquartered in San Francisco, CA.', 'Calm was co-founded by Alex Tew (Million Dollar Homepage) and Michael Acton Smith (Mind Candy, Moshi Monsters, Firebox).', 'Benefits', 'Help define and craft our data model', 'Team player who gives and takes feedback in a thoughtful way, and loves to help others.', 'Competitive salary and equity', 'Opportunity to work with a product focused on making the world happier and healthier', 'Build data integrations within our data platform and between partners\xa0', 'About Calm', ""The Data Engineering team’s mission is to make Calm’s data reliable, trustworthy and easy to use.\xa0To do that, we’re building a data ecosystem that enables the entire organization to use data to make our product better, facilitate decision making and help drive business value.Data Engineers at Calm work closely with internal stakeholders, defining requirements and designing and building solutions that meet those requirements.\xa0We are a group of strong communicators, who care about our stakeholders and each other. We love to work together as a team to find simple solutions to complex problems.\xa0Our current stack includes Airflow, DBT, Tableau, Redshift, SQS, SNS, and Sagemaker all deployed on Docker and Kubernetes on AWS - but we’re always open to the right technology for the job.You can read more\xa0about our interview process,\xa0our\xa0engineering organization, and\xa0what we've worked on recently.""]",Mid-Senior level,Full-time,Engineering,"Health, Wellness and Fitness",2021-03-02 16:49:33
Data Engineer,DevzAI,United States,2 hours ago,Be among the first 25 applicants,"['', 'Develop data products using microservices architecture', 'Outstanding verbal, written, and visual communication skills.', 'Build and maintain CI/CD pipelines using Gitlab CI, Circle CI , Jenkins or other CI/CD tools', 'Exposure to serverless technologies (AWS lambda/Step functions, Google Cloud Functions , Azure Functions)', 'Comfortable using massive datasets and the big data eco-system.', 'Preferred Experience & Technical Skills\xa0', 'Taking ownership of your work', 'Implementing data quality checks', 'Job description', 'Knowledge on traditional data warehousing (Dimensional Modeling, ETL)', 'Must be collaborative and\xa0fun to work with, good communication skills', 'Writing production grade code to process, transport, analyze, and store data within distributed systems.Develop data products using microservices architectureDevelop high performance API to expose data productsImplementing data quality checksBuild and maintain CI/CD pipelines using Gitlab CI, Circle CI , Jenkins or other CI/CD tools', 'Experience with building and maintaining in-house code libraries for distribution', 'Demonstrated 3+ years’ experience in driving business decisions with data-driven analyses.', 'Develop high performance API to expose data products', 'Redshift, Oracle and OBIEE experience', 'Strong Python programing skillsExposure to serverless technologies (AWS lambda/Step functions, Google Cloud Functions , Azure Functions)Demonstrated 3+ years’ experience in driving business decisions with data-driven analyses.Comfortable using massive datasets and the big data eco-system.Proficient with SQL.Must be collaborative and\xa0fun to work with, good communication skillsOutstanding verbal, written, and visual communication skills.', 'Knowlege of Scala and/or Java', 'Experience with building and maintaining in-house code libraries for distributionRedshift, Oracle and OBIEE experienceKnowlege of Scala and/or JavaKnowledge on traditional data warehousing (Dimensional Modeling, ETL)Deploying and Managing and SQL and NoSQL databasesTaking ownership of your work', 'Writing production grade code to process, transport, analyze, and store data within distributed systems.', 'Strong Python programing skills', 'Proficient with SQL.', 'Deploying and Managing and SQL and NoSQL databases', 'Required Experience & Skills\xa0', 'You Will\xa0']",Entry level,Contract,Information Technology,Information Technology and Services,2021-03-02 16:49:33
Data Engineer,Advisor360°,"Weston, MA",6 hours ago,Be among the first 25 applicants,"['', 'Create processes to aggregate and auto categorize data categories.', 'Bachelor’s degree in computer science or equivalent work experience of 3+ years3+ years of hands-on SQL and T-SQL programming skills2+ years of experience in a highly regulated data environment such as financial services or healthcare.2+ years of experience with enterprise data management tool sets such as Collibra, Informatica, Purview or Alation.2+ years’ experience with data sharing through APIs in any scripting language - .Net and Python are a plus.Strong exploratory, problem-solving, and analytical skills as related to data quality, categorization, and transformation tasksExperience working against datasets of varying size, type, and volumeDemonstrated ability to create and deliver high quality code using software engineering best practices including coding standard, code review, source control management, build process, testing, and operations.Must be self-motivated, detail-oriented, and have excellent communication and interpersonal skillsMust demonstrate flexibility, self-direction, and a growth mindset that is open to change.', '3+ years of hands-on SQL and T-SQL programming skills', 'Exposure to machine learning concepts for automated data categorization', '2+ years of experience with enterprise data management tool sets such as Collibra, Informatica, Purview or Alation.', 'Responsible for the design, development, and support of data management, quality and privacy processes, frameworks, and tools. Responsible for data and process improvement, identifying methods to enhance existing methods.Responsible for the design, development and implementation of data quality assessment, verification, and improvement practices.Adhere to best practices in engineering and data managementCreate processes to aggregate and auto categorize data categories.', 'Responsible for data and process improvement, identifying methods to enhance existing methods.', 'Requirements', 'Additional skills and knowledge', 'Experience with .NET, Python, or PowerShell', '2+ years of experience in a highly regulated data environment such as financial services or healthcare.', '2+ years’ experience with data sharing through APIs in any scripting language - .Net and Python are a plus.', '\u200b', 'Responsible for the design, development, and support of data management, quality and privacy processes, frameworks, and tools. ', 'Adhere to best practices in engineering and data management', 'Knowledge of laws and regulations that can impact data environments and use, such as GDPR, CCPA, HIPAA, etc.', 'Data modeling experienceExperience with .NET, Python, or PowerShellExposure to machine learning concepts for automated data categorizationExposure to cloud ETL products such as Azure Data Factory or DatabricksKnowledge of laws and regulations that can impact data environments and use, such as GDPR, CCPA, HIPAA, etc.', 'The Data Engineer role will be part of Advisor360°’s Engineering\xa0organization, as a member of our Data Management team. The primary responsibilities of this role will be creating and technically supporting data management, quality and privacy frameworks and processes. Working across the organization, the data engineer will assist in the selection, implementation and adoption of enterprise data management tooling and integration.\xa0They will also be responsible for creating processes improve overall data quality, ensure that data management and quality requirements are represented in the overall data pipeline, and implement our data privacy framework.', 'Exposure to cloud ETL products such as Azure Data Factory or Databricks', 'Key responsibilities', 'Responsible for the design, development and implementation of data quality assessment, verification, and improvement practices.', 'Strong exploratory, problem-solving, and analytical skills as related to data quality, categorization, and transformation tasks', 'Bachelor’s degree in computer science or equivalent work experience of 3+ years', 'Experience working against datasets of varying size, type, and volume', 'Data modeling experience', 'Demonstrated ability to create and deliver high quality code using software engineering best practices including coding standard, code review, source control management, build process, testing, and operations.', 'Must be self-motivated, detail-oriented, and have excellent communication and interpersonal skills', 'Must demonstrate flexibility, self-direction, and a growth mindset that is open to change.']",Mid-Senior level,Full-time,Information Technology,Financial Services,2021-03-02 16:49:33
Data Engineer,The Motley Fool,"Alexandria, VA",38 minutes ago,Be among the first 25 applicants,"['', 'Experience with object-oriented/object function scripting languages like Python.', 'Experience building and optimizing ‘Big Data’ data pipelines, architectures, and data sets.', 'Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.', 'Build processes supporting data transformation, data structures, metadata, dependency and workload management.', 'Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.', 'A successful history of manipulating, processing and extracting value from large disconnected datasets.', 'Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.', 'Experience working with or understanding formal ETL tools like SSIS a plus. ', 'Primary Responsibilities', 'Experience with relational SQL databases.Experience with some cloud services like Azure and AWS.Experience with object-oriented/object function scripting languages like Python.Experience with streaming tools like Kafka/Kinesis and Spark Structured Streaming a plus.Experience with serverless technologies like AWS Lambda a plus.Experience working with or understanding formal ETL tools like SSIS a plus. ', 'Work with data and analytics experts to strive for greater functionality in our data systems.', 'Experience with some cloud services like Azure and AWS.', 'Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.', 'Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.Experience building and optimizing ‘Big Data’ data pipelines, architectures, and data sets.Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.Strong analytical skills and detailed oriented.Build processes supporting data transformation, data structures, metadata, dependency and workload management.A successful history of manipulating, processing and extracting value from large disconnected datasets.Strong project management and organizational skills.Experience supporting and working with cross-functional teams in a dynamic environment.Ability to work with stakeholders to translate business requirements into technical requirements. ', 'Preferred Qualifications', 'Experience with relational SQL databases.', 'Experience with streaming tools like Kafka/Kinesis and Spark Structured Streaming a plus.', 'Ability to work with stakeholders to translate business requirements into technical requirements. ', 'Strong project management and organizational skills.', 'Assemble large, complex data sets that meet functional / non-functional business requirements.', 'Experience supporting and working with cross-functional teams in a dynamic environment.', 'Description', 'Create and maintain optimal data pipeline architecture.', 'Strong analytical skills and detailed oriented.', 'They Should Also Have Experience Using The Following Software/tools', 'Experience with serverless technologies like AWS Lambda a plus.', 'Create and maintain optimal data pipeline architecture.Assemble large, complex data sets that meet functional / non-functional business requirements.Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, and re-designing infrastructure for greater scalability.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL.Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.Work with data and analytics experts to strive for greater functionality in our data systems.', 'Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, and re-designing infrastructure for greater scalability.', 'Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.', 'Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL.']",Entry level,Full-time,Information Technology,Online Media,2021-03-02 16:49:33
Data Engineer,Collabera Inc.,"New York, NY",21 hours ago,Be among the first 25 applicants,"['', 'Familiarity with TOGAF 9.2, Cloud or any enterprise/data architecture frameworks ', 'Data Studio, SQL, Looker, Tableau, G Sheets, and G Slides ', 'Experience modeling data and related processes at the conceptual, logical, and physical levels ', 'Experience distilling complex information into easy-to-digest graphics, slides, notes, and tables. ', 'Mandatory ', 'Analytical and problem-solving skills required to identify impacts to data, processes, programs, policy, and create or enhancement processes as needed. ', 'Currently, supply planning, site selection and PfM processes and analytics are managed through a pathwork of legacy tools and trixes (worksheets). This leads to significant data reconciliation and manual work by team members, unduly lengthening the process and introducing potential errors. In order to correct this and enhance supply planning and site selection efforts, the D&A team needs a Data Governance Support professional to support the operationalization of the ELS Data Governance processes, tools, and data management functions such as data quality, reference data management, master/metadata management, business glossary, data dictionary, as well as support the data governance council  initiatives. ', 'Desired ', 'Job Title      :               ', 'Ability to effectively facilitate meetings using collaboration, dashboard, and other DG tools. ', 'The ideal candidates can build consensus and organize their work. A solid track record of rolling out projects building scalable, repeatable, and simple processes is a requirement for these roles. ', 'Demonstrated project management and organizational skills. ', 'This PgM TVC work will include ', 'Experience working with non-technical users, developers, data architects, and product development teams to ensure delivery of a product. ', 'The Data and Analytics group, which includes Analytics, Data Governance, Data Engineering, and BI, aims to minimize  risk by leveraging data, governance, standards, and insights, to assist with strategy, planning, and execution. We provide flexible, high-quality, and actionable supply planning and energy-related predictive analytics, data governance, tools, and business intelligence that enable efficient, rapid delivery of global capacity. ', 'Develop & maintain inventory of the enterprise information artifacts, including authoritative systems, owners. ', 'Position Description ', 'Knowledge of data center infrastructure requirements. ', 'Job Title      :               Data Governance PM', '. ', 'Responsibilities ', '5+ years of experience implementing data governance/data management capabilities that enable interoperability, alignment, decision making, and execution. ', 'Support and implement data governance processes or frameworks as defined in the ELS data governance and data management vision/ strategy, with a focus on improvement of data quality through policies and standards, principles, governance metrics, processes, related tools and data architecture. Participate and support developing the data driven culture, adoption of common data standards,  and taxonomy. Support data quality monitoring and remediation using dashboards, reports, and other DG tools. Support capture of data/ business requirements and translating into functional requirements that are  clearly defined, communicated, and well understood. Extend the findings from any data analysis and requirements for decisioning, alignment, and implementation, working closely with the Data Governance Lead, Taxonomy lead, Data Owners, Data Stewards, Architects, Managers, Data Engineering, and other key stakeholders. Develop & maintain inventory of the enterprise information artifacts, including authoritative systems, owners. Provide progress/performance updates on data governance processes to the Data Governance Lead and other key stakeholders. ', 'Knowledge of data center infrastructure requirements. Experience modeling data and related processes at the conceptual, logical, and physical levels Familiarity with TOGAF 9.2, Cloud or any enterprise/data architecture frameworks ', 'Who We Are?  ', 'Participate and support developing the data driven culture, adoption of common data standards,  and taxonomy. ', 'Provide progress/performance updates on data governance processes to the Data Governance Lead and other key stakeholders. ', 'Duration      :               6 + Months (High possibility of extension)', 'Support data quality monitoring and remediation using dashboards, reports, and other DG tools. ', 'Data Governance PM', 'Our Mission ', 'About the Job ', ""Bachelor's degree in Technology, Engineering, Science, Economics, Business or related field or equivalent practical experience. 5+ years of experience implementing data governance/data management capabilities that enable interoperability, alignment, decision making, and execution. Building dashboards and proficient in spreadsheet software and SQL. Ability to effectively facilitate meetings using collaboration, dashboard, and other DG tools. Effective stakeholder management and verbal/written communication skills. "", 'Additional desired qualifications ', 'Support and implement data governance processes or frameworks as defined in the ELS data governance and data management vision/ strategy, with a focus on improvement of data quality through policies and standards, principles, governance metrics, processes, related tools and data architecture. ', 'Data governance is critical to the supply planning and PfM/NSS delivery workstreams. Currently, we have poor data quality that results in critical errors and undue delays in analyses - this PgM TVC will help implement improved DQ processes and tracking -via new dashboards and solutions being deployed - working alongside the QB delivery team and Data Governance office within ELS. ', 'Familiarity with using: ', 'Having many of these specific qualifications is a plus, but transferable skills/experiences are equally valuable. Experience distilling complex information into easy-to-digest graphics, slides, notes, and tables. Analytical and problem-solving skills required to identify impacts to data, processes, programs, policy, and create or enhancement processes as needed. Demonstrated project management and organizational skills. Experience working with non-technical users, developers, data architects, and product development teams to ensure delivery of a product. Familiarity with data management concepts such as Data Sourcing, Data Quality Management, Reference data, Metadata, Process, and Procedures. Ability to write solid and appealing technical documentation and user manuals. Familiarity with Data Governance Tools such as Collibra, GCP, Informatica, and others. ', ""Bachelor's degree in Technology, Engineering, Science, Economics, Business or related field or equivalent practical experience. "", 'Familiarity with Data Governance Tools such as Collibra, GCP, Informatica, and others. ', 'Having many of these specific qualifications is a plus, but transferable skills/experiences are equally valuable. ', 'Duration      :               6 + Months (High possibility of extension) ', ' ', 'Skill/Experience/Education ', 'Position: ', 'Ability to write solid and appealing technical documentation and user manuals. ', 'Effective stakeholder management and verbal/written communication skills. ', 'Extend the findings from any data analysis and requirements for decisioning, alignment, and implementation, working closely with the Data Governance Lead, Taxonomy lead, Data Owners, Data Stewards, Architects, Managers, Data Engineering, and other key stakeholders. ', 'Position: Data Governance Support PgM- Analyst/Program Manager. ', 'Location      :               Remote (NY) ', 'Client           :              Technology ', 'The Energy and Location Strategy (ELS) team is 135+ strong and growing, and dispersed globally across EMEA, NASA, APAC, into several functions such as Supply Planning (SP), New Site Selection (NSS), Energy, and Analytics to mention a few. Our mission requires collaboration on planning and strategies to ensure the delivery of infrastructure (capacity MWs) and services for  future -- everything from third-party collocation space to greenfield sites.  ', 'Building dashboards and proficient in spreadsheet software and SQL. ', 'Support capture of data/ business requirements and translating into functional requirements that are  clearly defined, communicated, and well understood. ', 'Familiarity with data management concepts such as Data Sourcing, Data Quality Management, Reference data, Metadata, Process, and Procedures. ']",Mid-Senior level,Full-time,Information Technology,Information Services,2021-03-02 16:49:33
Data engineer,Maxonic,"Las Vegas, NV",3 hours ago,Be among the first 25 applicants,"['', '•\xa0\xa0\xa0Knowledge of NoSQL databases ', '•\xa0\xa0\xa0Experience with CI/CD pipelines and unit test ', '•\xa0\xa0\xa0Java/Scala/Python Language ', '•\xa0\xa0\xa0Understanding of Data modelling and schema design ', '•\xa0\xa0\xa0Strong SQL skills ', '•\xa0\xa0\xa0Experience developing Spark jobs/processes ', 'Must have Spark Experience, Scala Experience, prior ML experience is a BIG BONUS, or prior Data Science work is a bonus Job Description:\xa0\xa0\xa0', 'This is Piyush from Maxonic. We are a staffing company based in the SF Bay Area. We currently have an immediate requirement with one of our direct clients and your resume looks like a good fit for the position.', 'Position: Data engineer', 'Location: Remote/ Las Vegas, NV', 'Duration: 6+ months', 'Please find the job description below and if you are interested in the opportunity, I could give you a call back to discuss the client and hourly pay. ', '•\xa0\xa0\xa0BS Degree in STEM area\xa0', '•\xa0\xa0\xa0Experience with AWS or Azure (preferred) development ', '•\xa0\xa0\xa0Minimum of 3 years experience, 5+ preferred ', '•\xa0\xa0\xa0Agile (Scrum) development – ability to accurately size user stories ', '•\xa0\xa0\xa0Create new customer data products ', '•\xa0\xa0\xa0Knowledge of AWS Lambda or Azure Functions (serverless compute) ', 'Job Description', '•\xa0\xa0\xa0Github ', '•\xa0\xa0\xa0ETL work to automate creation of data marts ']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-03-02 16:49:33
Data Engineer,Zycron,"Farmers Branch, TX",23 hours ago,Be among the first 25 applicants,"['', 'Essential Duties & Responsibilities', 'Work closely with business analysts and product owners to understand requirements.', 'Zycron is currently seeking a\xa0Data Engineer\xa0for our client in the Dallas, TX area.\xa0This is a direct hire/full time position with generous salary, bonus and benefits.\xa0The client is not able to sponsor visas at this time.\xa0No CTC.', 'Zycron, a Brand of BG Staffing, Inc. (NYSE American: BGSF), is one of the largest IT solutions firms headquartered in Tennessee. We provide client-specific solutions from staffing to outsourcing across all industries, with extensive experience in health care, energy and utilities, and state and local government. To learn more about our services visit\xa0www.zycron.com', 'Build, manage, and optimize data pipelines for data structures encompassing data transformation, data models, schemas, metadata, data quality, and workload management.', '2 years experience using\xa0SQL to\xa0query.', 'Prepare design documents, create unit tests, apply version control, and perform related operational duties.', '3 years combined experience with SQL and/or No SQL databases.', 'Troubleshoot and own defects identified by the QA team and customers.', '3+ years of total data engineering experience.', 'Job ID Number:', 'Develop, test, and deploy solutions for data warehousing.', 'Create and support data processing solutions using a wide array of technologies across Google, Amazon, and Azure cloud environments.', 'Experience with at least one of Google Cloud, AWS Cloud, or Azure Cloud preferred.', 'Experience with Google Big Query or SQL Server is a preferred.', 'Participate in the data management life cycle process from requirements through production support.', 'A strong desire for continuous growth and learning.', '3+ years of total data engineering experience.3 years combined experience with SQL and/or No SQL databases.2 years combined experience with object-oriented scripting languages such as C#, Python, Go, or Java.2 years experience using\xa0SQL to\xa0query.1 year experience working with a version control system..Experience exposing data as a service using RESTful APIs is preferred.Experience with Google Big Query or SQL Server is a preferred.Experience with big data tools: Hadoop, Spark, Kafka, etc. is a plus.Experience with at least one of Google Cloud, AWS Cloud, or Azure Cloud preferred.A strong desire for continuous growth and learning.', 'Data Engineer\xa0', ""Only candidates with backgrounds who match our client's requested experience will be contacted. Do not take this as a poor reflection on your experience, just a decision for the specific needs of our client's project/job. We look forward to working with you."", 'Experience with big data tools: Hadoop, Spark, Kafka, etc. is a plus.', 'Job ID Number:\xa0117375 (Please reference in call or email)', '2 years combined experience with object-oriented scripting languages such as C#, Python, Go, or Java.', 'Create technical documentation.', 'Create and support data processing solutions using a wide array of technologies across Google, Amazon, and Azure cloud environments.Develop, test, and deploy solutions for data warehousing.Participate in the data management life cycle process from requirements through production support.Create technical documentation.Participate in Agile ceremonies like standup, grooming and retrospectives.Prepare design documents, create unit tests, apply version control, and perform related operational duties.Build, manage, and optimize data pipelines for data structures encompassing data transformation, data models, schemas, metadata, data quality, and workload management.Troubleshoot and own defects identified by the QA team and customers.Work closely with business analysts and product owners to understand requirements.', 'Participate in Agile ceremonies like standup, grooming and retrospectives.', '1 year experience working with a version control system..', '\xa0', 'Minimum Qualifications (Knowledge, Skills, and Abilities)', 'The\xa0Data Engineer\xa0will\xa0deliver quality data intelligence solutions to the organization, assist our analytics team with drawing insights, and participate in creating our data platform. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up.', 'Experience exposing data as a service using RESTful APIs is preferred.', 'Data Engineer']",Mid-Senior level,Full-time,Information Technology,Apparel & Fashion,2021-03-02 16:49:33
Data Engineer,Digital Asset Research,"Pittsburgh, PA",3 minutes ago,Be among the first 25 applicants,"['', 'Interest in or experience with cryptocurrency or financial markets\xa0', 'Expert in development using Python\xa0', 'Maintain/update data ingestion systems according to SLA requirementsDevelop and maintain strong knowledge of detailed application behaviors in our data ingestion and certification systems\xa0Manage and optimize data infrastructure\xa0Data preparation/enrichment using Python\xa0Support, test and debug a production quality systemAutomation of data-intensive analytical processesImplement unit and other automated testsAutomate deployment utilizing CI/CD and containerization practices', 'Proficient using Python Data Analysis Tools (Pandas)', 'Storing and querying data in SQL and AWS-based databases\xa0', 'Digital Asset Research (DAR) is looking for a Data Engineer/Developer to join our team. Preference will be given to applicants in Pittsburgh, PA. We are seeking an individual for full time employment.', 'Expert in development using Python\xa0Proficient using Python Data Analysis Tools (Pandas)Working in AWS environments (EKS,ECS,S3,Ec2, RDS, route 53, ELB)Storing and querying data in SQL and AWS-based databases\xa0Working in Linux Environments (Ubuntu)Core competency in: Automated testing, SDLC best practices, with deployment automation, Containerization (Docker/Kubernetes), Mentorship / Peer review, Data migrations, Relational model designs', 'Automate deployment utilizing CI/CD and containerization practices', 'Experience with BASH, Django, JavascriptExperience connecting to FIX APIsDegree or equivalent experience in applied math, statistics or data analysisInterest in or experience with cryptocurrency or financial markets\xa0PubSub, Kafka/data streaming technologies, Redshift', 'PubSub, Kafka/data streaming technologies, Redshift', 'Working in AWS environments (EKS,ECS,S3,Ec2, RDS, route 53, ELB)', 'Experience connecting to FIX APIs', 'Manage and optimize data infrastructure\xa0', 'Core competency in: Automated testing, SDLC best practices, with deployment automation, Containerization (Docker/Kubernetes), Mentorship / Peer review, Data migrations, Relational model designs', 'Applicants with experience in the the following will receive increased preference:', 'Develop and maintain strong knowledge of detailed application behaviors in our data ingestion and certification systems\xa0', 'Applicants must have experience with the following skills:', 'Experience with BASH, Django, Javascript', 'Automation of data-intensive analytical processes', 'Implement unit and other automated tests', 'Degree or equivalent experience in applied math, statistics or data analysis', 'Data preparation/enrichment using Python\xa0', 'Key Functions:\xa0', 'Support, test and debug a production quality system', 'DAR provides objective and transparent cryptocurrency pricing and market data to institutional clients entering the digital asset marketplace. Digitalassetresearch.com\xa0', 'Working in Linux Environments (Ubuntu)', 'Maintain/update data ingestion systems according to SLA requirements']",Entry level,Full-time,Information Technology,Financial Services,2021-03-02 16:49:33
Data Scientist ,LinkedIn,"Sunnyvale, CA",18 hours ago,173 applicants,[''],Not Applicable,Full-time,Analyst,Internet,2021-03-02 16:49:33
Data Engineer,Electronic Arts (EA),"Seattle, WA",15 hours ago,Be among the first 25 applicants,"['', ' EA Security  ', ' Experience with AWS cloud services: EC2, EMR, RDS, Redshift. Or similar experience with GCP or Azure. ', ' Experience with data pipeline and workflow management tools: Airflow, Azkaban, Luigi, etc. ', ' Strong interpersonal and communication skills ', ' Onboard new data to support business needs ', ' Experience with big data tools: Hadoop, Spark, Kafka, AWS etc. ', ' Design, develop, and maintain data pipelines and backend services for real-time decisioning, reporting, optimization, data collection, and related functions ', 'Data Engineer ', ' Create and maintain SLAs on data reporting through reports and dashboards ', 'Requirements', ' Able to multi-task and thrive in a dynamic, fast-paced environment  Strong problem solving skills and ability to collaborate within a team environment  Results-driven and highly quantitative  True passion for understanding customer behavior on-platform and in-game  Strong interpersonal and communication skills ', ' Design, develop, and maintain data pipelines and backend services for real-time decisioning, reporting, optimization, data collection, and related functions  Design efficient data structures and database schemas  Incorporate data processing and workflow management tools into pipeline design  Onboard new data to support business needs  Use tools, debugging logs, performance metrics, and other data sources to make code- and application-level improvements  Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.  Create and maintain SLAs on data reporting through reports and dashboards  Work collaboratively and communicate effectively with stakeholders  Experiment with and recommend new technologies that simplify or improve the tech stack ', ' Design efficient data structures and database schemas ', ' Experiment with and recommend new technologies that simplify or improve the tech stack ', 'Skills And Experience', ' Use tools, debugging logs, performance metrics, and other data sources to make code- and application-level improvements ', ' Work collaboratively and communicate effectively with stakeholders ', ' Strong problem solving skills and ability to collaborate within a team environment ', ' Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases. ', ' Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.  Experience with big data tools: Hadoop, Spark, Kafka, AWS etc.  Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.  Experience with data pipeline and workflow management tools: Airflow, Azkaban, Luigi, etc.  Experience with AWS cloud services: EC2, EMR, RDS, Redshift. Or similar experience with GCP or Azure.  Experience with stream-processing systems: Storm, Spark-Streaming, etc.  Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.  Master degree required in a engineering discipline (i.e. Computer Science, Industrial Engineering or equivalent)  3+ years experience in data technologies as data engineer or related roles', ' Incorporate data processing and workflow management tools into pipeline design ', ' Able to multi-task and thrive in a dynamic, fast-paced environment ', ' Keep our data separated and secure across national boundaries through multiple data centers and AWS regions. ', 'Responsibilities', ' Experience with stream-processing systems: Storm, Spark-Streaming, etc. ', ' Master degree required in a engineering discipline (i.e. Computer Science, Industrial Engineering or equivalent) ', ' The Challenge Ahead ', ' 3+ years experience in data technologies as data engineer or related roles', ' Results-driven and highly quantitative ', ' Experience with relational SQL and NoSQL databases, including Postgres and Cassandra. ', 'Data Engineer', ' Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc. ', ' True passion for understanding customer behavior on-platform and in-game ', 'EA Security ']",Not Applicable,Full-time,Strategy/Planning,Computer Games,2021-03-02 16:49:33
Data Engineer,US Tech Solutions,"Boston, MA",3 hours ago,Be among the first 25 applicants,"['', 'Qualifications', 'Has a business value-driven perspective with regards to understanding of work context and impact - interacts with customers to gather and define requirements.', 'Demonstrated success in building infrastructure in the public cloud and deploying through CI/CD pipelines, utilizing technologies such as AWS CloudFormation Templates, YAML, Python, Ansible, Docker, Kubernetes; cloud platforms such as AWS, Clienture or Google Cloud.', 'Soft skills- ', 'Works within the team on iterative development that delivers a high-quality product.', 'Problem solver: Complex problems often require innovative, creative approaches—and you’ll work to come up with outside-the-box solutions to solve them.', 'A background in business operations and strategies, with a focus on business IT', 'The product that is being created, is a technology health portal. This eventual product will be collecting data from databases and then using this data to tell a story. ', 'The Global Digital Services group is looking for a Senior Data Engineer to join the Technology Health team. The Technology Health team is collecting, analyzing and presenting data for our customers to help them understand their technology’s performance and platform health. This role requires expertise in a broad range of predictive analytics techniques and their application to business opportunities. A successful candidate will be organized, creative, and interested in solving business problems using data and analytics. They will have the right mix of analytical, technical, and communication skills to thrive in a fast-paced environment.', 'Expert knowledge of predictive toolset; reflects as expert resource for tool development.', 'Collaboration, adaptability, flexibility and the ability to manage time and prioritize work with a globally distributed development team', 'Must be able to communicate with business users. ', 'A minimum of five years of software engineering experience', 'Agile engineering capabilities and a design-thinking mindset', 'An ability to code in multiple languages, including an object-oriented language.', 'Identifies and recommends appropriate continuous improvement opportunities.', 'A Bachelor’s or Master’s degree in a technical or business discipline, or equivalent experience', 'Customer-centric engineer: You understand who we’re here to serve and the products you engineer will keep the end-user front and center.', 'Candidate must have strong Python skills. An ideal candidate would be Adobe Air Flow – is a new language that is Python based. Could have the potential to go FTE down the road. Preference for AWS/Kubernetes cloud experience (Clienture could come up to speed with AWS). Preference for coding skills is Python, ability to understand Java. A lot is written in RESTful services.', '3 to 5+ years of data engineering experience.', 'Collaborative partner: Working side-by-side with business colleagues and interacting with customers, you’ll address their technical challenges and ensure quality through collaborative design and development.', 'Eventually this will get to the point where it can be used for predictive for health monitoring.', 'Experience working in an agile environment utilizing Scrum, Kanban or XP', 'Strong oral and written communication skills—and a knack for explaining your decision-making process to non-engineers.', 'Familiarity with CI/CD and DevOps tools.', 'Must be able to communicate with business users. This person will be a member of full stack engineer team. The product that is being created, is a technology health portal. This eventual product will be collecting data from databases and then using this data to tell a story. Eventually this will get to the point where it can be used for predictive for health monitoring.', '3 to 5+ years of data engineering experience.Experience working in an agile environment utilizing Scrum, Kanban or XPDemonstrated success in building infrastructure in the public cloud and deploying through CI/CD pipelines, utilizing technologies such as AWS CloudFormation Templates, YAML, Python, Ansible, Docker, Kubernetes; cloud platforms such as AWS, Clienture or Google Cloud.Experience building non-cloud native vendor products in AWS, Clienture or GCPStrong oral and written communication skills—and a knack for explaining your decision-making process to non-engineers.A collaborative, adaptable working style, demonstrated initiative and the ability to prioritize your time and efforts.A thorough grasp of technology concepts, business operations, design and development tools, system architecture and technical standards.An ability to code in multiple languages, including an object-oriented language.Familiarity with CI/CD and DevOps tools.Understanding of data analytics, processing and BI tools (PowerBI, Microstrategy, Tableau, others).A Bachelor’s or Master’s degree in a technical or business discipline, or equivalent experience.A minimum of five years of software engineering experienceA background in business operations and strategies, with a focus on business ITA history of translating client requirements into technical designsAgile engineering capabilities and a design-thinking mindsetCollaboration, adaptability, flexibility and the ability to manage time and prioritize work with a globally distributed development teamStrong oral and written communication skills — and a knack for explaining your decision-making process to non-engineersA thorough grasp of IT concepts, business operations, design and development tools, system architecture and technical standards, shared software concepts and layered solutions and designsProficiency in software engineering languages and tools, including Java and RESTful services, spanning horizontal and vertical packagesAn understanding of how modifications affect different parts of a system and an ability to explain your decision-making process to non-engineersA Bachelor’s or Master’s degree in a technical or business discipline, or equivalent experience', 'Proficiency in software engineering languages and tools, including Java and RESTful services, spanning horizontal and vertical packages', 'Understanding of data analytics, processing and BI tools (PowerBI, Microstrategy, Tableau, others).', 'A thorough grasp of technology concepts, business operations, design and development tools, system architecture and technical standards.', 'Experience building non-cloud native vendor products in AWS, Clienture or GCP', 'Preference for coding skills is Python, ability to understand Java. A lot is written in RESTful services.', 'A Bachelor’s or Master’s degree in a technical or business discipline, or equivalent experience.', 'Broad knowledge of predictive analytic techniques and statistical diagnostics of models.', 'An understanding of how modifications affect different parts of a system and an ability to explain your decision-making process to non-engineers', 'Forward thinker: Simply fixing the problem isn’t enough; using your proactive mindset and initiative, you’ll continually look for ways to improve performance, quality and efficiency.', 'Could have the potential to go FTE down the road. Preference for AWS/Kubernetes cloud experience (Clienture could come up to speed with AWS). ', 'A thorough grasp of IT concepts, business operations, design and development tools, system architecture and technical standards, shared software concepts and layered solutions and designs', 'Collaborative partner: Working side-by-side with business colleagues and interacting with customers, you’ll address their technical challenges and ensure quality through collaborative design and development.Problem solver: Complex problems often require innovative, creative approaches—and you’ll work to come up with outside-the-box solutions to solve them.Customer-centric engineer: You understand who we’re here to serve and the products you engineer will keep the end-user front and center.Forward thinker: Simply fixing the problem isn’t enough; using your proactive mindset and initiative, you’ll continually look for ways to improve performance, quality and efficiency.Broad knowledge of predictive analytic techniques and statistical diagnostics of models.Expert knowledge of predictive toolset; reflects as expert resource for tool development.Has a business value-driven perspective with regards to understanding of work context and impact - interacts with customers to gather and define requirements.Works within the team on iterative development that delivers a high-quality product.Identifies and recommends appropriate continuous improvement opportunities.', '\xa0', 'This person will be a member of full stack engineer team. ', 'A history of translating client requirements into technical designs', 'Strong oral and written communication skills — and a knack for explaining your decision-making process to non-engineers', 'Description:', 'Candidate must have strong Python skills. An ideal candidate would be Adobe Air Flow – is a new language that is Python based. ', 'A collaborative, adaptable working style, demonstrated initiative and the ability to prioritize your time and efforts.', 'Responsibilities:']",Mid-Senior level,Contract,Information Technology,Staffing and Recruiting,2021-03-02 16:49:33
Data Engineer,StaffChase,"Santa Clara, CA",1 hour ago,Be among the first 25 applicants,"['Interested in early pipeline research and development/prototype efforts', 'Proficient with relational SQL ( Microsoft SQL , MySQL, Postgres, Mongo etc.)', '3 years of hands-on experience with Python and C# programming (required)', 'M.S. in Computer Science, Software/Computer Engineering, or Applied Math with 4-5 years of hands-on experience or B.S. degree with minimum 7-9 years hands-on experienceDemonstrated excellent communication skills both written and verbal Ability to independently work with services team to gather product requirements and manage development life cycleDemonstrated ability to work on large data setsInterested in early pipeline research and development/prototype effortsProficient with relational SQL ( Microsoft SQL , MySQL, Postgres, Mongo etc.)Proficient in at least two of Python, C#, Java, JavaScriptSolid knowledge of statisticsAny experience in the following would be idealDatabase design, management and operationCI/CD pipeline and Build tools such as Jenkins, CircleCI, GitLab etc.', '2 years of hands-on experience with Application Development Skills - web based or service based (preferred)', 'Requirements: ', 'Solid knowledge of statistics', 'Following skills sets is must:', '3 years of hands-on experience with ETL tools and automation (required)', '1 year of experience AWS technology stack understanding (desirable)', 'Establish product requirements with proper documentation for quality control, and support testing effort.', 'Database design, management and operation', '4 years of hands-on experience working on data using SQL on multiple platforms (SQL server, my SQL, Postgres, Mongo) including database operations3 years of hands-on experience with Python and C# programming (required)2 years of hands-on experience with Data analytics (dashboards) and derive insights2 years of hands-on experience with Application Development Skills - web based or service based (preferred)3 years of hands-on experience with ETL tools and automation (required)1 year of experience AWS technology stack understanding (desirable)', 'Establishes and builds processes and structures based on business and technical requirements to channel data from multiple inputs, route appropriately and store using any combination of distributed (cloud) structures, local databases, and other applicable storage forms as required. ', 'Demonstrated excellent communication skills both written and verbal ', 'Ability to independently work with services team to gather product requirements and manage development life cycle', 'Develops technical tools and programming that leverage artificial intelligence, machine learning and big-data techniques to cleanse, organize and transform data and to maintain, defend and update data structures and integrity on an automated basis. ', 'Designs, builds and oversees the deployment and operation of technology architecture, solutions and software to capture, manage, store and utilize structured and unstructured data from internal and external sources. Establishes and builds processes and structures based on business and technical requirements to channel data from multiple inputs, route appropriately and store using any combination of distributed (cloud) structures, local databases, and other applicable storage forms as required. Develops technical tools and programming that leverage artificial intelligence, machine learning and big-data techniques to cleanse, organize and transform data and to maintain, defend and update data structures and integrity on an automated basis. Creates and establishes design standards and assurance processes for software, systems and applications development to ensure compatibility and operability of data connections, flows and storage requirements. Establish product requirements with proper documentation for quality control, and support testing effort.', 'CI/CD pipeline and Build tools such as Jenkins, CircleCI, GitLab etc.', 'M.S. in Computer Science, Software/Computer Engineering, or Applied Math with 4-5 years of hands-on experience or B.S. degree with minimum 7-9 years hands-on experience', 'Designs, builds and oversees the deployment and operation of technology architecture, solutions and software to capture, manage, store and utilize structured and unstructured data from internal and external sources. ', 'Any experience in the following would be ideal', 'Demonstrated ability to work on large data sets', '\xa0', '4 years of hands-on experience working on data using SQL on multiple platforms (SQL server, my SQL, Postgres, Mongo) including database operations', '2 years of hands-on experience with Data analytics (dashboards) and derive insights', 'Creates and establishes design standards and assurance processes for software, systems and applications development to ensure compatibility and operability of data connections, flows and storage requirements. ', 'Responsibilities:', 'Proficient in at least two of Python, C#, Java, JavaScript']",Associate,Contract,Information Technology,Electrical/Electronic Manufacturing,2021-03-02 16:49:33
Data Engineer,Eliassen Group,"Charlotte, NC",,N/A,"['', 'Please send over an updated resume to Jdonahue@eliassen.com', 'Related Terms: Python, Kafka, JavaScript, AWS Glue, Redshift', 'Expert level Kafka streaming and Kafka connectors', 'Requirements', 'Bachelors Degree in Computer Science, Engineering or related field preferredExpert level Kafka streaming and Kafka connectorsGood experience with AWS servicesGood experience with Python, JavaSpring and AWS Serverless Lambda functionsExperience with AWS GlueExperience with Redshift Excellent written and verbal communication skills', 'Excellent written and verbal communication skills', 'Good experience with AWS services', 'Bachelors Degree in Computer Science, Engineering or related field preferred', 'A Fortune 500 client in the Charlotte market is seeking a Data Engineer. This Data Engineer will design, write, develop and implement innovative integration solutions for internal and external stakeholders.', 'Experience with Redshift ', 'This position can offer a very strong compensation with benefits and PTO if needed.\xa0We cannot work through a 3rd party employer for this role.\xa0', 'Good experience with Python, JavaSpring and AWS Serverless Lambda functions', 'The ideal Data Engineer will have experience with Python, AWS Glue, and Redshift.\xa0This Data Engineer will also have strong Kafka experience as well as back-end development experience using Java Spring, Python, and AWS Serverless Lambda functions .', 'Experience with AWS Glue']",Mid-Senior level,Full-time,Information Technology,Staffing and Recruiting,2021-03-02 16:49:33
Data Engineer,Amazon Web Services (AWS),"Seattle, WA",17 hours ago,Be among the first 25 applicants,"['', ' Knowledge of python or any general purpose scripting language.', ' Graduate/Master degree in Computer Science, Engineering or related technical field.', ' Hands-on experience with cloud computing and UNIX/Linux based systems.', ' 3+ years of experience as a Data Engineer or in a similar role', ' Exceptional troubleshooting and problem-solving abilities.', ' Experience with AWS Tools and Technologies.', ' Experience in SQL', 'Overview', ' Experience with data modeling, data warehousing, and building ETL pipelines', 'Preferred Qualifications', ' Excellent written and verbal communications skills.', 'Description', ' Industry experience as a Data Engineer or related specialty (e.g., Software Engineer, Business Intelligence Engineer, Data Scientist) with a track record of manipulating, processing, and extracting value from large datasets.', 'Basic Qualifications', ' Demonstrated ability to work effectively across various internal organizations.', ' Experience with Amazon Redshift or other distributed computing technology.', ' 3+ years of experience as a Data Engineer or in a similar role Experience with data modeling, data warehousing, and building ETL pipelines Experience in SQL Knowledge of python or any general purpose scripting language. Experience with Big Data technologies such as Hive/Spark.', ' Graduate/Master degree in Computer Science, Engineering or related technical field. Exceptional troubleshooting and problem-solving abilities. Experience with Amazon Redshift or other distributed computing technology. Industry experience as a Data Engineer or related specialty (e.g., Software Engineer, Business Intelligence Engineer, Data Scientist) with a track record of manipulating, processing, and extracting value from large datasets. Experience with AWS Tools and Technologies. Hands-on experience with cloud computing and UNIX/Linux based systems. Demonstrated ability to work effectively across various internal organizations. Excellent written and verbal communications skills.', 'Company', ' Experience with Big Data technologies such as Hive/Spark.']",Not Applicable,Full-time,Strategy/Planning,Computer Software,2021-03-02 16:49:33
Data Engineer,"AlphaForce Solutions,Inc. ","Reno, NV",2 hours ago,Be among the first 25 applicants,"['', 'Good human relations skills to select, develop, mentor, discipline and reward employees', 'Assists in managing the research of problems/issues and develop effective solutions, and provide guidance and training to users as necessary.', 'Vertica DBA: Eight+ years of experience with Vertica installation, administration including table distribution, performance enhancements and user restrictions so the database is never over-taxed. Also is experienced in AWS migrations including RedShift and migrating Vertica from one environment into RedShift in AWS.', 'Assists in selecting and hiring employees. Trains and evaluates employees in the area of work product. Provides information related to performance issues and suggests recommendations for personnel actions.', 'Ability to work independently and as part of a team', 'Essential Job Functions', '\ufeffJob description:', 'Good organization skills to balance and prioritize work', ""Bachelor's degree in computer science, management information systems, or related field preferred"", 'Other Qualifications', 'Experience working with data entry devices and databases', 'Position: Data Engineer', 'Develops and implements standards and procedures to improve team efficiency and accuracy.', 'Develops, implements, and coordinates database policies, making sure staff follows and implements policies and procedures during everyday business communication with internal and external clients. Interfaces with clients to ensure that database issues are resolved.', 'Strong communication skills', 'Monitors work flow, maintaining levels of productivity, quality and timeliness. Prepares and recommends operating and personnel budgets for approval. Monitors spending for adherence to budget, recommends variances as necessary.', 'Ensures that production schedules are met. Monitors and maintains quality control standards to ensure high quality customer service and data/database integrity.', 'Provides inputs on high-level review and feedback on portions of the data/database management processes including department workflow, edit review specifications, and quality control plans.', 'Strong interpersonal and presentation skills for interacting with team members and clients', 'Good analytical and problem solving skills', 'Strong personal computer and business solutions software skills', 'Ability to create and maintain formal and informal networks', 'Location: Reno,NV', 'Basic Qualifications', 'Leadership skills', '\xa0', ""Bachelor's degree or equivalent combination of education and experience"", 'Evaluates processes on new and existing accounts, including suggestions to senior management on desirable additions or replacements.', 'Nine or more years of data management experience', 'Prepares and recommends operating and personnel budgets for approval. Monitors spending for adherence to budget, recommends variance as necessary.']",Entry level,Contract,Information Technology,Staffing and Recruiting,2021-03-02 16:49:33
Data Engineer,Amitech Solutions,United States,5 hours ago,Be among the first 25 applicants,"['', '• Two plus years of experience with GIS data and relevant tools (ArcGIS Pro, ArcGIS', '• Work on the development, deployment, and support of systems computing solutions;', 'Required Qualifications:', '• Experience with distributed systems;', '• Collaborate and influence with cross-functional stakeholders to develop our strategic target state data infrastructure and organization model;', 'o Experience with big data tools (Spark, Kafka, Flink, Hadoop, etc.);', 'Health, Dental, and Vision insuranceLong and Short-Term Disability401 (k)Life insurancePet insuranceReferral program2018 - 2020 “Top Work Places” Winner', '• Optimize algorithms and data workers to scale horizontally and contribute to the development of new algorithms and capabilities that will enable connected pipeline analytics for all pipelines;', 'Referral program', '• Network and Database administration;', 'Life insurance', 'Sr. Data Engineer', '• Knowledge of algorithms and data structures;', '• Experience with python, Java, R, or Scala.', '• Work on all aspects of the design, development, validation, scaling and delivery of analytical solutions;', '• Collaborate with interdisciplinary scientists to gather requirement for data pipelines;', '• Familiarity with ML workflows including validation and hyper parameter tuning approaches, and ML frameworks such as scikit-learn and tensor flow.', '• Bachelor’s degree in Computer Science, Electrical Engineering or a closely-related field with at least 6-8 years of industry experience OR Master’s Degree in Computer Science, Electrical Engineering or a closely-related field with at least eight years of industry experience OR Doctorate in Computer Science, Electrical Engineering, or a closely-related field with at least four years of industry experience;', '\ufeffAbout Us', '• Technical knowledge with at least of seven years of experience in at least four of the following:', '• Design and maintain data storage systems and access patterns;', '• Integrate proactive strategies and best practices to ensure security of stored data;', '• Experience in running production cloud systems and diagnosing and fixing problems;', '401 (k)', '• Drive practices which help raise the success of the overall team, such as code reviews, integrated testing, and other practices;', 'o SQL and NoSQL databases (data warehousing, data modeling, etc.);', '• Experience in Developing and supporting large scale geospatial and Imaging Processing data pipelines', '• Actively drive skill development of others in the space through proactive coaching, feedback, and training;', '• Work on the deployment, delivery and expansion of data pipelines;', 'Health, Dental, and Vision insurance', 'Preferred Qualifications:', '2018 - 2020 “Top Work Places” Winner', 'Pet insurance', '• Design and maintain ETL workflows;', '• Proven systems administration and operations experience;', 'Long and Short-Term Disability', '• Design, build, and maintain integrated data solutions such as “data lakes” and “data warehouses”;', 'Online, ArcMap)', '• Primary focus on developing data pipelines with geospatial and imaging processing data pipelines to enable batch and real-time analytics.', '• Partner cross-functionally in the development of shared infrastructure where aligned with Breeding business needs;', '• Experience with tools for authoring workflows and pipelines (Airflow, AWS Step Functions, KubeFlow, etc.);', ' We believe healthcare should and can be better.\xa0With a single-minded focus on value, we combine people, process, culture and technology to drive real and lasting change. We partner with our clients to deliver data analytics and digital transformation strategies and solutions to make healthcare more proactive, higher quality and less expensive for everyone.', '• Collaborate with analytics and discovery teams to design and plan data engineering solutions;', '• Proven ability to plan, schedule and deliver quality software DevOps methodology;', '• Provide consulting and feedback to partner teams on data architecture and strategy;', ' Amitech is a rapidly growing organization focused on our employees. Our diverse and innovative approach to everything we do means we’re looking for the groundbreakers and the pioneers—people who think differently and create the future.', '• Implement, configure, and maintain critical third-party solutions related to engineering work, including compute environments, BI platforms, and cloud systems;', 'Why Amitech:', '\xa0', ""• Actively identify new technologies and practice within the domain of engineering and drive review for potential introduction to the team's infrastructure."", 'Location: Remote', 'Description:', '• Experience with AWS cloud services (EMR, S3, RedShift, EC2, etc.);']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-03-02 16:49:33
Data Engineer (Remote),Jun Group,"New York, NY",3 hours ago,Be among the first 25 applicants,"['', 'Qualifications', ' You enjoy a fun, creative, and engaging working atmosphere free of brilliant jerks You want to be part of a small team inside a large company with massive opportunity for growth You enjoy collaboration with other teams including product, biz dev, and our in-house QA team You eagerly dig into complex engineering problems ', 'Collaborate with our engineering team to improve our existing machine learning models and tooling', ""What You'll Do"", 'Designated time to work on company-related projects you feel strongly about', 'Paid vacation, work from home, and sick days', 'Contribute to exciting greenfield projects', 'What We Offer', 'You eagerly dig into complex engineering problems', 'Practical knowledge of how to build efficient end-to-end ML workflows', 'You want to be part of a small team inside a large company with massive opportunity for growth', "" Experience working with streaming and batch data processing tools like Apache Beam, Spark, Flink, etc. You've built and maintained an ETL pipeline using a data warehouse like BigQuery or Redshift You are comfortable writing SQL and manipulating large structured or unstructured datasets for analysis Practical knowledge of how to build efficient end-to-end ML workflows Familiarity with AWS or Google Cloud big data products "", 'Experiment with new tech to find the right tool for the job', 'Jun Group will only consider candidates for this position who are currently legally authorized to work in the United States.', 'Who You Are', 'Experience working with streaming and batch data processing tools like Apache Beam, Spark, Flink, etc.', 'Macbook Pros and any other equipment you need to work effectively from home', 'Maintain high code quality through code reviews and automated tests', 'Use Kanban to manage multiple releases per week', ""You've built and maintained an ETL pipeline using a data warehouse like BigQuery or Redshift"", ' A highly competitive compensation package 401k with company match Paid vacation, work from home, and sick days Annual personal development budget to attend a conference of your choice Designated time to work on company-related projects you feel strongly about Macbook Pros and any other equipment you need to work effectively from home Monthly company events  ', ' Contribute to exciting greenfield projects Own all things data - including our ETL processes, reporting APIs, and internal dashboards Collaborate with our engineering team to improve our existing machine learning models and tooling Experiment with new tech to find the right tool for the job Use Kanban to manage multiple releases per week Maintain high code quality through code reviews and automated tests ', 'You enjoy collaboration with other teams including product, biz dev, and our in-house QA team', 'Annual personal development budget to attend a conference of your choice', 'Familiarity with AWS or Google Cloud big data products', '401k with company match', 'You are comfortable writing SQL and manipulating large structured or unstructured datasets for analysis', 'You enjoy a fun, creative, and engaging working atmosphere free of brilliant jerks', 'Monthly company events ', 'Own all things data - including our ETL processes, reporting APIs, and internal dashboards', 'A highly competitive compensation package']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-02 16:49:33
Data Engineer,Progressive Insurance,"Mayfield, OH",7 hours ago,Be among the first 25 applicants,"['', ""Bachelor's Degree or higher in an Information Technology discipline or related field of study and minimum of one year work experience designing, programming, and supporting software programs or applications"", 'Cloud experience - we use AWS (EC2, S3, Athena, Lambda, Aurora, etc.)', 'Work From Home', 'Experience with Linux, Python, Git, Terraform, SQL, building APIs (Flask), data orchestration ', 'Employee Status', 'Ability to develop and support web applications using a popular web framework', '401(k) with dollar-for-dollar company match up to 6%', 'Experience deploying or working with machine learning models. ', 'Schedule', 'Primary Location', 'Sponsorship for work authorization for foreign national candidates is not available for this position.', 'Data Engineer Senior', 'Wellness programs to help you maintain a better quality of life', "" Bachelor's Degree or higher in an Information Technology discipline or related field of study and minimum of one year work experience designing, programming, and supporting software programs or applications instead of a degree, minimum of two years related work described in above bullet "", 'Diverse, inclusive and welcoming culture with Employee Resource Groups', 'Must-have Qualifications', 'Career development and tuition assistance', 'Onsite gym and healthcare at large locations', 'Medical, dental and vision, including free preventive care', 'Job', 'Data Engineer', 'Self-learner with ability to scope and recommend new tools (patterns, cloud services, etc.) as required by projects', ' Gainshare bonus up to 24% of your eligible earnings; Progressive rewards each of us with an annual bonus based on company performance 401(k) with dollar-for-dollar company match up to 6% Diverse, inclusive and welcoming culture with Employee Resource Groups Career development and tuition assistance Onsite gym and healthcare at large locations Wellness programs to help you maintain a better quality of life Medical, dental and vision, including free preventive care ', 'instead of a degree, minimum of two years related work described in above bullet', ' Cloud experience - we use AWS (EC2, S3, Athena, Lambda, Aurora, etc.) Experience with Linux, Python, Git, Terraform, SQL, building APIs (Flask), data orchestration  Ability to develop and support web applications using a popular web framework Experience deploying or working with machine learning models.  Self-learner with ability to scope and recommend new tools (patterns, cloud services, etc.) as required by projects ', 'Benefits', 'Preferred Skills', 'Gainshare bonus up to 24% of your eligible earnings; Progressive rewards each of us with an annual bonus based on company performance']",Not Applicable,Full-time,Information Technology,Information Technology and Services,2021-03-02 16:49:33
Lead Data Engineer,General Motors,"Austin, TX",3 hours ago,Be among the first 25 applicants,"['', 'About GM', ' Company and matching contributions to 401K savings plan to help you save for retirement;', 'Communicate and maintains Master Data, Metadata, Data Management Repositories, Logical Data Models, Data StandardsCreate and maintain optimal data pipeline architectureYou will assemble large, complex data sets that meet functional / non-functional business requirementsYou will identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Build industrialized analytic datasets and delivery mechanisms that utilize the data pipeline to deliver actionable insights into vehicle quality, operational efficiency and other key business performance metricsWork with business partners on data-related technical issues and develop requirements to support their data infrastructure needsCreate highly consistent and accurate analytic datasets suitable for business intelligence and data scientist team members', ""Data Wrangling and Preparation using GM's approved tools: PySpark, Jupyter etc.Stream-processing systems: Storm, Spark-Streaming, etc."", 'You will identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.', ' Global recognition program for peers and leaders to recognize and be recognized for results and behaviors that reflect our company values;', 'Master databases - Advanced SQL and NoSQL databases, including Postgres and Cassandra, Oracle and Greenplum', 'As a Data Engineer, you will build industrialized data assets and optimize data pipelines in support of Vehicle Quality Business Intelligence and Advance Analytics objectives. You will work closely with our Quality Business teams, forward-thinking Data Scientists, BI Developers, System Architects and Data Architects to deliver value to our vision for the future.', ' Discount on GM vehicles for you, your family and friends.', 'You will assemble large, complex data sets that meet functional / non-functional business requirements', ' Tuition assistance and student loan refinancing;', 'Communicate and maintains Master Data, Metadata, Data Management Repositories, Logical Data Models, Data Standards', 'Build industrialized analytic datasets and delivery mechanisms that utilize the data pipeline to deliver actionable insights into vehicle quality, operational efficiency and other key business performance metrics', '7 or more years with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.', 'At least 3 years of hands on experience with Big Data Tools: Hadoop, Spark, Kafka, etc.', 'Ability to identify tasks which require automation and automate them', 'Requirements', 'In recent years, GM Information Technology has successfully executed the largest IT transformation in the history of the automotive industry, fully insourcing what once was a nearly completely outsourced IT function. Today GM IT is a dynamic and fast paced organization that designs, develops and maintains all IT infrastructure, applications and solutions enabling GM’s global operations. From designing and building the next generation of electric and other vehicles to developing a world-class GM experience for our dealers and customers, GM IT is driving real change in the most iconic automaker on the planet. Our team delivers unique enterprise-wide IT solutions in cutting-edge technologies such as mobility, telematics, mission-critical business systems, supercomputing, cloud, vehicle engineering and real-time computing. We offer challenging positions for passionate professionals looking to advance their careers and be a part of an IT organization focused on innovation, speed and business value.', 'Benefits Overview', ""Development activities will include enhancing existing data systems and optimizing ETL systems.Additionally you will lead development activities to migrate out of legacy technologies to GM's Big Data Platform utilizing new technologies such as, Kafka, Hadoop, PySpark, Greenplum and GM's internally developed Big Data tools."", 'Preferred', 'Understand and maintain compliance with GM standards and industry standard methodology', 'For This Role You Will Be Responsible For', 'Work with business partners on data-related technical issues and develop requirements to support their data infrastructure needs', 'Ability to tackle software engineering and data problems quickly and completely', 'In this role you will also lead junior developers, mentor, coach and help them to develop their software development skills in Data Engineering technologies.Demonstrate mastery of many programming languages, tools and/or technologies with emphasis on ETL & Database developmentUnderstand and maintain compliance with GM standards and industry standard methodology', 'developing data architecture and ETL solutions using sound, repeatable, industry standard methodologies. You will have the opportunity to work hands-on defining ETL solutions based on business requirements and system specifications.', 'Ability to multi-task and stay organized in a dynamic work environment and work collaboratively with other IT organizations (GDAAS, Platform Engineering etc)', ""developing data architecture and ETL solutions using sound, repeatable, industry standard methodologies. You will have the opportunity to work hands-on defining ETL solutions based on business requirements and system specifications.Development activities will include enhancing existing data systems and optimizing ETL systems.Additionally you will lead development activities to migrate out of legacy technologies to GM's Big Data Platform utilizing new technologies such as, Kafka, Hadoop, PySpark, Greenplum and GM's internally developed Big Data tools."", ' Paid time off including vacation days, holidays, and parental leave for mothers, fathers and adoptive parents;', 'In this role you will also lead junior developers, mentor, coach and help them to develop their software development skills in Data Engineering technologies.', ' Healthcare (including a triple tax advantaged health savings account and wellness incentive), dental, vision and life insurance plans to cover you and your family;', 'Responsibilities', 'We are not able to accommodate international relocation.', 'Job Description', ""At least 3 years of hands on experience with Big Data Tools: Hadoop, Spark, Kafka, etc.Master databases - Advanced SQL and NoSQL databases, including Postgres and Cassandra, Oracle and GreenplumData Wrangling and Preparation using GM's approved tools: PySpark, Jupyter etc.Stream-processing systems: Storm, Spark-Streaming, etc.Ability to tackle software engineering and data problems quickly and completelyAbility to identify tasks which require automation and automate themAbility to multi-task and stay organized in a dynamic work environment and work collaboratively with other IT organizations (GDAAS, Platform Engineering etc)"", 'Create highly consistent and accurate analytic datasets suitable for business intelligence and data scientist team members', 'Create and maintain optimal data pipeline architecture', 'Demonstrate mastery of many programming languages, tools and/or technologies with emphasis on ETL & Database development']",Not Applicable,Full-time,Information Technology,Automotive,2021-03-02 16:49:33
Data Engineer,Sketchy,United States,5 hours ago,Be among the first 25 applicants,"['', 'Experience building and optimizing ‘big data’ data pipelines, architectures and data sets', 'Experience with relational SQL and NoSQL databases', 'Able to get into the weeds and propose and implement solutions without hand holding', 'Since its inception in 2013, Sketchy has become the premiere learning destination for Medical School students around the world, currently serving over 30,000 active users (or a third of the total 89,000 medical students in the United States) and an alumni base of 100,000+ students. Sketchy is creating the most engaging and effective educational service for students of higher education everywhere by combining visual storytelling with interactive learning tools that together dramatically enhance recall and knowledge acquisition.', 'Fun team events (Monthly and virtual for now)', 'Experience with data pipeline and workflow management tools', 'Competitive compensation planInnovative, high growth and collaborative cultureGenerous PTO package with floating holidayFun team events (Monthly and virtual for now)Great Benefits including 99 % Coverage of Medical, dental, vision. Up to 4% match on 401k and more', 'SketchyGroup LLC is an Equal Opportunity Employer. All applicants will receive consideration without discrimination on the basis of race, religion, color, sex, age, sexual orientation, marital status, national origin, disability or any other basis prohibited by applicable law.', 'Position overview:', 'Authorization to work in the U.S. ', 'Keep our data separated and secure across national boundaries through multiple data centers', 'Innovative, high growth and collaborative culture', 'Self-starter who is excited to be part of a growing startup company', 'Experience with GCP services', 'Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery re-designing infrastructure for greater scalability and usability', 'Competitive compensation plan', 'What We Offer:', 'Experience with object-oriented/object function scripting languages', 'Strong analytic skills related to working with unstructured datasets', 'Generous PTO package with floating holiday', 'We are looking for a Data Engineer who is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The individual will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross-functional teams. You will play an integral role in helping us become more data-aware as a company and enabling data insights across our teams.', ""Bachelor's degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field (Master's degree in a similar field preferred but not required)"", 'Requirements:', 'Must have experience in building reports, dashboards, and/or data modeling layers in Looker, Tableau or Mode', 'Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources', 'Create and maintain optimal data pipeline architectureIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery re-designing infrastructure for greater scalability and usabilityBuild analytics tools that utilize the data pipeline to provide actionable insightsBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sourcesKeep our data separated and secure across national boundaries through multiple data centers', 'Great Benefits including 99 % Coverage of Medical, dental, vision. Up to 4% match on 401k and more', 'Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL)', 'Build analytics tools that utilize the data pipeline to provide actionable insights', '3+ years experience in Data EngineeringAdvanced working SQL knowledge and experience working with relational databases, query authoring (SQL)Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data storesExperience building and optimizing ‘big data’ data pipelines, architectures and data setsStrong analytic skills related to working with unstructured datasetsMust have experience in building reports, dashboards, and/or data modeling layers in Looker, Tableau or ModeExperience with relational SQL and NoSQL databasesExperience with data pipeline and workflow management toolsExperience with GCP servicesExperience with object-oriented/object function scripting languagesSelf-starter who is excited to be part of a growing startup companyAble to get into the weeds and propose and implement solutions without hand holdingAuthorization to work in the U.S. ', 'Education:', 'Sketchy is an online visual learning platform that helps students effortlessly learn and recall information through a blend of art, story, spaced repetition and memory palace techniques. Sketchy was born when four medical students began creating sketched stories to distinguish and memorize similarly named viruses, as they realized that the same learning methodologies can be used across a variety of subjects.', 'Create and maintain optimal data pipeline architecture', 'Location: This role is open to remote employees in select US states: California, New York, Hawaii, Illinois, Colorado, Massachusetts, Washington and Washington, D.C.\xa0', 'Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores', 'Responsibilities:', '3+ years experience in Data Engineering', 'Sketchy is a TCG (The Chernin Group) portfolio company (joining other companies such as Headspace, Surfline Food52 and Crunchyroll) and a Reach Capital portfolio company (joining other start-up companies that bring a playfulness to learning.)']",Mid-Senior level,Full-time,Information Technology,Higher Education,2021-03-02 16:49:33
