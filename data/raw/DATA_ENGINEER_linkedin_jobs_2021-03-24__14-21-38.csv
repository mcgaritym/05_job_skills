job_title,company,location,date_posted,applicants,job_text,seniority_level,employment_type,job_function,industries,date_scraped
Data Engineer,Apple,"Austin, TX",5 days ago,105 applicants,"['', 'Description', 'Education & Experience', 'Summary', 'Key Qualifications']",Not Applicable,Full-time,Information Technology,Consumer Electronics,2021-03-24 13:05:10
Data Engineer,Mozilla,"Pennsylvania, United States",16 hours ago,Be among the first 25 applicants,"['', 'You will work with other data engineers to design and maintain scalable data models and ETL pipelines.', 'We Have Multiple Openings For The Following', 'Proficiency with one or more of the programming languages used by our teams (SQL, Python, Java, Rust).', 'Ability to work collaboratively with a distributed team.', 'Commitment to diversity, equity, inclusion, and belonging', ' Strong CS fundamentals: data structures, algorithms, etc. Proficiency with one or more of the programming languages used by our teams (SQL, Python, Java, Rust). Ability to work collaboratively with a distributed team. Ability to write and speak English well. ', 'You have experience with data systems: Databases, message queues, batch and stream processing', 'You will help design, build, and improve the infrastructure for ingesting, storing, and transforming data at a scale of tens of terabytes per day.', 'Specific Skills/Experience', 'You have experience with highly scalable distributed systems hosted on cloud providers (e.g. Google Cloud Platform)', 'You have a working knowledge of web development technologies: HTML, Javascript, CSS, HTTP.', 'General Professional Requirements', 'You will help design and build systems to monitor and analyze data from Mozilla’s products. See https://telemetry.mozilla.org', 'Ability to write and speak English well.', ' You have experience with data systems: Databases, message queues, batch and stream processing You have experience with highly scalable distributed systems hosted on cloud providers (e.g. Google Cloud Platform) You have a working knowledge of web development technologies: HTML, Javascript, CSS, HTTP. ', 'About Mozilla', 'You will work with data scientists to answer questions and guide product decisions.', 'Strong CS fundamentals: data structures, algorithms, etc.', 'The Role', ' You will help design, build, and improve the infrastructure for ingesting, storing, and transforming data at a scale of tens of terabytes per day. You will help design and build systems to monitor and analyze data from Mozilla’s products. See https://telemetry.mozilla.org You will work with other data engineers to design and maintain scalable data models and ETL pipelines. You will work with data scientists to answer questions and guide product decisions. ']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,MMIT (Managed Markets Insight & Technology),"Yardley, PA",16 hours ago,28 applicants,"['o\xa0\xa0 Working with complex business logic including applying logic to transforms that fall outside of basic schema formatting conversion', '·\xa0\xa0\xa0\xa0\xa0\xa0 Build reports, dashboards and/or other analyses as needed and/or as specific initiatives arise', 'o\xa0\xa0 Importing/exporting data from a database; SSIS, SQL Import/Export Wizard, or equivalent', 'Provide critical data to support our quality management system (60%)', '·\xa0\xa0\xa0\xa0\xa0\xa0 5+ years of practical experience with SQL, T-SQL, including:', 'Position Description', 'o\xa0\xa0 Creating functions, views, triggers, and stored procedures', '·\xa0\xa0\xa0\xa0\xa0\xa0 Familiarity with reporting software; PowerBI, Tableau a plus', '·\xa0\xa0\xa0\xa0\xa0\xa0 Experience in the commercial health insurance industry', '·\xa0\xa0\xa0\xa0\xa0\xa0 Excellent written and spoken communication skills, including ability to present technical concepts to lay audiences and pitch ideas effectively & persuasively to internal stakeholders', '·\xa0\xa0\xa0\xa0\xa0\xa0 Inform, create, and/or maintain master data hierarchy, attributes, and structure', '·\xa0\xa0\xa0\xa0\xa0\xa0 Translate operational strategy into actionable data science deliverables, guiding development of data models, business rules, & other advanced analytical capabilities', '·\xa0\xa0\xa0\xa0\xa0\xa0 Effectively partner with internal stakeholders to develop meaningful and projectable analytics about trends in market access dynamics across payers, geographies, indications, drugs, etc.', '·\xa0\xa0\xa0\xa0\xa0\xa0 Excel at communicating across a broad set of cross-functional stakeholders', '·\xa0\xa0\xa0\xa0\xa0\xa0 Develop a system of measurements that provide confidence scores & control limits in various aspects of the data based on statistical sampling and failure rates', '·\xa0\xa0\xa0\xa0\xa0\xa0 Proven ability to work both collaboratively to solve complex problems and individually to research/advance areas of interest & exploration', '·\xa0\xa0\xa0\xa0\xa0\xa0 Experience in managed markets and pharma market access ecosystem', '·\xa0\xa0\xa0\xa0\xa0\xa0 Develop leading & lagging measure quality checks that fulfill our QMS design', '·\xa0\xa0\xa0\xa0\xa0\xa0 Bachelor’s degree or higher in technology field or related field preferred', 'Additional Information', 'o\xa0\xa0 Working with complex queries including use of CTEs, table variables, merge and dynamic SQL', '\xa0', '·\xa0\xa0\xa0\xa0\xa0\xa0 Strong understanding of the data science package landscape', '·\xa0\xa0\xa0\xa0\xa0\xa0 Strong related programming skills including experience with good coding practices', 'MMIT is seeking an experienced Data Engineer to make meaningful contributions to the Data Operations quality initiatives. The Data Engineer will partner with our Quality Manager to define & produce critical insight about our operational processes & outputs that allow for preventive & predictive measures to be established. As part of the quality program, the analysis produced in this role will drive improvements in timeliness, accuracy, and completeness of our data product. Errors in our data can results in inaccurate representation of access to therapies that are critical for one’s health.', '·\xa0\xa0\xa0\xa0\xa0\xa0 Knowledge of payer marketing, drug development life cycle and drug launch process', 'Qualifications', 'To succeed in this role, you will have the following responsibilities.', 'Establish Data Engineering/Science capabilities in Data Operations (40%)', '·\xa0\xa0\xa0\xa0\xa0\xa0 Proven ability to develop meaningful/valuable analytic insights from multiple data sources and promote high quality outcomes', 'Position based in Yardley, PA or home based with light travel to local offices', '·\xa0\xa0\xa0\xa0\xa0\xa0 Familiarity with agile software development practices', 'Preferred candidates may also have:']",Mid-Senior level,Full-time,Engineering,Information Technology and Services,2021-03-24 13:05:10
Data Engineer - Segments Data Science,LinkedIn,"Sunnyvale, CA",18 hours ago,132 applicants,[''],Not Applicable,Full-time,Engineering,Internet,2021-03-24 13:05:10
Data Engineer,Amazon,"Arlington, VA",16 hours ago,Be among the first 25 applicants,"['', ' Willingness to dive deep, experiment rapidly and get things done', 'Company', ' Experience working with AWS big data technologies (EMR, Redshift, S3, AWS Glue, Kinesis and Lambda for Serverless ETL)', ' Experience with Object Oriented programming (e.g. Java, Scala, Python)', ' Experience with agile methodologies, coding standards, code reviews, source control management, build processes, testing, and operations', ' Experience using big data technologies (Hadoop, Hive, Hbase, Spark, EMR, etc.)', 'Preferred Qualifications', ' Experience of distributed systems as it pertains to data storage and computing', ' Experience with Machine Learning/deep learning development projects', ' High attention to detail and proven ability to manage multiple, competing priorities simultaneously', ' Love to get your hands dirty and solve challenging technical issues?', ' BS/MS in Computer Science or equivalent industry experience Domain knowledge of Distributed SOA Architecture, Relational DB knowledge, ElasticSearch, DynamoDB, and various AWS technologies 5+ years of experience as a Data Engineer, BI Engineer, Systems Analyst in a company with large, complex data sources 3+ years’ full-time experience working on a software design and development team Experience with Object Oriented programming (e.g. Java, Scala, Python) Experience using big data technologies (Hadoop, Hive, Hbase, Spark, EMR, etc.) Experience building complex software systems that have been successfully delivered to customers Experience with building high-performance, highly-available and scalable distributed systems Experience successfully mentoring junior DEs Experience with Machine Learning/deep learning development projects High attention to detail and proven ability to manage multiple, competing priorities simultaneously Passion for building great solutions which directly impact customers Experience working in a fast-paced environment where continuous innovation is desired', ' Are you passionate about building tools for complex use cases?', ' 3+ years’ full-time experience working on a software design and development team', ' Experience building complex software systems that have been successfully delivered to customers', ' 5+ years of experience as a Data Engineer, BI Engineer, Systems Analyst in a company with large, complex data sources', ' 3+ years of experience as a Data Engineer or in a similar role', 'Description', ' Are you excited about working directly to empower users? Love to get your hands dirty and solve challenging technical issues? Are you passionate about building tools for complex use cases?', ' Experience with data modeling, data warehousing, and building ETL pipelines', ' Experience with building high-performance, highly-available and scalable distributed systems', ' Ability to communicate clearly and concisely with technical and non-technical customers in order to understand ambiguous problems and articulate technical designs and solutions to complex problem', ' Passion for building great solutions which directly impact customers', ' Experience working in a fast-paced environment where continuous innovation is desired', ' Experience in SQL', ' Are you excited about working directly to empower users?', ' Experience of having built and delivered multiple large scale, cross-functional projects', 'Basic Qualifications', ' Domain knowledge of Distributed SOA Architecture, Relational DB knowledge, ElasticSearch, DynamoDB, and various AWS technologies', ' Experience successfully mentoring junior DEs', ' BS/MS in Computer Science or equivalent industry experience', ' 3+ years of experience as a Data Engineer or in a similar role Experience with data modeling, data warehousing, and building ETL pipelines Experience in SQL Experience working with AWS big data technologies (EMR, Redshift, S3, AWS Glue, Kinesis and Lambda for Serverless ETL) Experience of distributed systems as it pertains to data storage and computing Experience of having built and delivered multiple large scale, cross-functional projects Willingness to dive deep, experiment rapidly and get things done Ability to communicate clearly and concisely with technical and non-technical customers in order to understand ambiguous problems and articulate technical designs and solutions to complex problem Experience with agile methodologies, coding standards, code reviews, source control management, build processes, testing, and operations']",Not Applicable,Full-time,Strategy/Planning,Computer Software,2021-03-24 13:05:10
Data Engineer,Capital One,"Wilmington, DE",2 days ago,31 applicants,"['', '2+ years of experience with NoSQL implementation (Mongo, Cassandra) ', 'Bachelor’s Degree At least 2 years of experience in application developmentAt least 1 years of experience in big data technologies (Cassandra, Accumulo, HBase, Spark, Hadoop, HDFS, AVRO, MongoDB, or Zookeeper)', ' slides 76-91', '1+ year experience working on streaming data applications (Spark Streaming, Kafka, Kinesis, and Flink', 'At least 2 years of experience in application development', 'Preferred Qualifications', 'Bachelor’s Degree ', 'Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment', 'Capital One Data Engineer', '2+ years of experience developing Java based software solutions ', '2+ years of experience developing software solutions to solve complex business problems', ""Master's Degree 3+ years of experience in application development1+ year experience working on streaming data applications (Spark Streaming, Kafka, Kinesis, and Flink1+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)1+ years of experience with Ansible / Terraform2+ years of experience with Agile engineering practices 2+ years in-depth experience with the Hadoop stack (MapReduce, Pig, Hive, Hbase) 2+ years of experience with NoSQL implementation (Mongo, Cassandra) 2+ years of experience developing Java based software solutions 2+ years of experience in at least one scripting language (Python, Perl, JavaScript, Shell) 2+ years of experience developing software solutions to solve complex business problems2+ years of experience with UNIX/Linux including basic commands and shell scripting"", 'Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community', '2+ years of experience with Agile engineering practices ', 'At least 1 years of experience in big data technologies (Cassandra, Accumulo, HBase, Spark, Hadoop, HDFS, AVRO, MongoDB, or Zookeeper)', 'diversity & inclusion', 'inclusive,', '#lifeatcapitalone', 'Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Snowflake', 'Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies', 'What You’ll Do', '2+ years of experience in at least one scripting language (Python, Perl, JavaScript, Shell) ', ""Master's Degree "", 'Perform unit tests and conducting reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance', '2+ years in-depth experience with the Hadoop stack (MapReduce, Pig, Hive, Hbase) ', '2+ years of experience with UNIX/Linux including basic commands and shell scripting', 'Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems', 'Basic Qualifications', 'Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologiesWork with a team of developers with deep experience in machine learning, distributed microservices, and full stack systemsUtilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as SnowflakeShare your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering communityCollaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowermentPerform unit tests and conducting reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance', '3+ years of experience in application development', '1+ years of experience with Ansible / Terraform', '1+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)']",Associate,Full-time,Information Technology,Banking,2021-03-24 13:05:10
Data Engineer,Palladium Consulting,"Houston, TX",17 hours ago,42 applicants,"['', 'Knowledge of message queuing, stream processing, and scalable data stores', 'Some SQL experience', 'Occasional travel to meet new clients and solve problems with them (once that’s a thing again)', 'Requirements', 'Data transforming, massaging, cleaning, merging, parsing', 'Benefits', 'Some knowledge of AWS and GCP technologies', 'R experience a plus', 'Working with large data sets', ' Working with large data sets Data transforming, massaging, cleaning, merging, parsing Python expert Experience with pandas Some SQL experience Some knowledge of AWS and GCP technologies R experience a plus Knowledge of message queuing, stream processing, and scalable data stores Occasional travel to meet new clients and solve problems with them (once that’s a thing again) ', 'Experience with pandas', 'Python expert']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Lockheed Martin,"Fort Worth, TX",1 week ago,Be among the first 25 applicants,"['', 'Developing CI/CD pipelines to manage effective changes to multiple infrastructure foot prints', ' Develop and implement automated regression test plans', 'Managing and creating ETL processes to prepare data for consumption', 'Creating monitoring capabilities to ensure data streams are effective and active', 'Ingesting Data from Batch and Streaming processes to load it into our Data Lake']",Entry level,Full-time,Information Technology,Construction,2021-03-24 13:05:10
Data Engineer,Cedar,"New York, NY",22 hours ago,60 applicants,"['', 'The ability to impact the growth of our company, we value all comments and suggestions', ' Build and optimize the process of analytics aggregations and experimentation data pipelines Collaborate with data scientists, analysts, product managers and business development teams to understand data needs and transform these into usable data products. Partner with Analytics to systematize and scale high-integrity value-oriented analysis. Partner with other engineering teams to guide refactors of existing data infrastructure to improve data quality and features. Identify and resolve issues that may negatively impact data product timelines or quality. Troubleshoot transformations for data consistency and integrity. ', 'Strong computer science fundamentals - a degree in computer science, engineering, or a related field or similar experience', '3+ years experience in designing, building and maintaining data pipelines.', 'Experience working with relational databases, such as PostgreSQL, or MySQL', 'Collaborate with data scientists, analysts, product managers and business development teams to understand data needs and transform these into usable data products.', 'Transparency across teams and interaction with multiple departments', ' 3+ years experience in designing, building and maintaining data pipelines. Experience working with relational databases, such as PostgreSQL, or MySQL Experience with Python and ORM tools such as SQL Alchemy Solid understanding of data warehouse Demonstrates a passion for breaking down and understanding complex systems and data structures A great teammate with excellent communication and listening skills. Ability to thrive in an entrepreneurial environment, comfortable with ambiguity Excited about Cedar’s mission of improving the healthcare financial experience Encompass Cedar’s core values: mission driven, no mediocrity, use good judgement, positivity Strong computer science fundamentals - a degree in computer science, engineering, or a related field or similar experience Eligible to work in the United States ', 'Competitive pay, employer-paid healthcare, stock options', 'What do we offer to the ideal candidate?', ' Experience with AWS data services/tools such as Redshift, S3, Athena, Glue is preferred. Experience with data warehouse or data lake solutions such as Snowflake, BigQuery, Redshift Experience with MapReduce, Hadoop, Spark is preferred ', 'Build and optimize the process of analytics aggregations and experimentation data pipelines', 'Experience with MapReduce, Hadoop, Spark is preferred', ' An opportunity to work on a platform that is scaling very rapidly, engaging with millions of patients per year and growing at a rate of 360% YoY as of January 2021 A chance to join a high-growth company at an early stage The ability to impact the growth of our company, we value all comments and suggestions Transparency across teams and interaction with multiple departments Competitive pay, employer-paid healthcare, stock options ', 'A chance to join a high-growth company at an early stage', 'A great teammate with excellent communication and listening skills.', 'Prefered Skills And Experiences', 'Ability to thrive in an entrepreneurial environment, comfortable with ambiguity', 'Responsibilities', 'Solid understanding of data warehouse', 'An opportunity to work on a platform that is scaling very rapidly, engaging with millions of patients per year and growing at a rate of 360% YoY as of January 2021', 'Experience with Python and ORM tools such as SQL Alchemy', 'Experience with AWS data services/tools such as Redshift, S3, Athena, Glue is preferred.', 'Skills And Experience', ' Cedar is committed to a flexible work environment, so this as well as many of our roles are remote friendly.Responsibilities Build and optimize the process of analytics aggregations and experimentation data pipelines Collaborate with data scientists, analysts, product managers and business development teams to understand data needs and transform these into usable data products. Partner with Analytics to systematize and scale high-integrity value-oriented analysis. Partner with other engineering teams to guide refactors of existing data infrastructure to improve data quality and features. Identify and resolve issues that may negatively impact data product timelines or quality. Troubleshoot transformations for data consistency and integrity. Skills And Experience 3+ years experience in designing, building and maintaining data pipelines. Experience working with relational databases, such as PostgreSQL, or MySQL Experience with Python and ORM tools such as SQL Alchemy Solid understanding of data warehouse Demonstrates a passion for breaking down and understanding complex systems and data structures A great teammate with excellent communication and listening skills. Ability to thrive in an entrepreneurial environment, comfortable with ambiguity Excited about Cedar’s mission of improving the healthcare financial experience Encompass Cedar’s core values: mission driven, no mediocrity, use good judgement, positivity Strong computer science fundamentals - a degree in computer science, engineering, or a related field or similar experience Eligible to work in the United States Prefered Skills And Experiences Experience with AWS data services/tools such as Redshift, S3, Athena, Glue is preferred. Experience with data warehouse or data lake solutions such as Snowflake, BigQuery, Redshift Experience with MapReduce, Hadoop, Spark is preferred What do we offer to the ideal candidate? An opportunity to work on a platform that is scaling very rapidly, engaging with millions of patients per year and growing at a rate of 360% YoY as of January 2021 A chance to join a high-growth company at an early stage The ability to impact the growth of our company, we value all comments and suggestions Transparency across teams and interaction with multiple departments Competitive pay, employer-paid healthcare, stock options Applicants must be currently authorized to work in the United States on a full-time basis. Cedar will not hire any applicants for Data Engineer who are present in the United States on an F-1 visa', 'Applicants must be currently authorized to work in the United States on a full-time basis. Cedar will not hire any applicants for Data Engineer who are present in the United States on an F-1 visa', 'Data Engineer', 'Excited about Cedar’s mission of improving the healthcare financial experience', 'Demonstrates a passion for breaking down and understanding complex systems and data structures', 'Encompass Cedar’s core values: mission driven, no mediocrity, use good judgement, positivity', 'Eligible to work in the United States', 'Partner with Analytics to systematize and scale high-integrity value-oriented analysis.', 'Experience with data warehouse or data lake solutions such as Snowflake, BigQuery, Redshift', 'Partner with other engineering teams to guide refactors of existing data infrastructure to improve data quality and features.', 'Identify and resolve issues that may negatively impact data product timelines or quality. Troubleshoot transformations for data consistency and integrity.']",Entry level,Full-time,Information Technology,Hospital & Health Care,2021-03-24 13:05:10
Data Engineer,Quadrant Resource ,"Redmond, WA",17 hours ago,51 applicants,"['strong skills (Azure, BI, DW, AS, Big Data), please make sure to check analytics skillset (TABULAR MODEL/SSAS/CUBES) as well along side of DE.', 'TABULAR MODEL/SSAS/CUBES', 'needs someone strong in Big Data who can work as an individual contributor- This resource will work from end-to end process, getting data from sources to build the cubes. In short, needs a strong data engineer with strong Big data technologies expertise who should have experience in building cubes as well.', ' ', 'Azure, BI, DW, AS, Big Data', ' needs someone strong in Big Data who can work as an individual contributor- This resource will work from end-to end process, getting data from sources to build the cubes. In short, needs a strong data engineer with strong Big data technologies expertise who should have experience in building cubes as well.', 'Best Regards']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Bank of America,"New York, NY",6 days ago,Be among the first 25 applicants,"['', 'Hours Per Week', 'Corporate Actions', 'Required', 'Vendor Data Products - Bloomberg Back Office - Reuters Datascope - Markit', 'Shift', 'Job Description', 'Security IdentifiersCorporate ActionsVendor Data Products - Bloomberg Back Office - Reuters Datascope - Markit', 'Security Identifiers', 'Preferred']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Evalueserve,United States,1 day ago,121 applicants,"['', '•\tDevelop data query and analysis services to be leveraged by clients, internally and externally', 'Evalueserve is a global professional services provider offering research, analytics, and data management services.\xa0', 'Requirements:', ""The team's focus is building scalable frameworks and services used to analyze large datasets in the context of financial analysis. They leverage well-established engineering processes and techniques pioneered by giants in the tech industry. Projects span general pipelining to building internal web tools to helping develop low-latency query and analysis engines."", 'Job Posting | Data Engineer\xa0', 'Job Description:\xa0', '•\tBachelor’s in Computer Science or related quantitative field', 'We’re powered by mind+machine – a unique combination of human expertise and best-in-class technologies that use smart algorithms to simplify key tasks. This approach enables us to design and manage processes that can generate and harness insights on a large scale, significantly cutting costs and timescales and helping businesses that partner with us to overtake the competition. We work with clients across a wide range of industries and business functions, helping them to make better decisions faster; reach new levels of efficiency and effectiveness; and see a tangible impact on their top and bottom line.\xa0', 'Position at a glance:\xa0\xa0', '•\tAdapt the latest data processing and infrastructure techniques to our growing stack', 'Preferred Requirements:', '•\tSecond-nature knowledge of algorithms and data structures', '•\tMaintain a high standard of code quality within the broader engineering team', 'Evalueserve is committed to providing equal opportunities (EEO) globally, eliminating discrimination and promoting good relations among employees, regardless of age, disability, race, ethnicity or origin, religion or belief, sex, gender assignment, gender identity, sexual orientation, marital or civil partnership status. Evalueserve is an inclusive employer and is proud of its diverse workforce.\xa0\xa0\xa0', 'Responsibilities include and are not limited to:', '•\tLeading edge global work environment', '•\tComplete Benefits Package', '•\tExperience with Hadoop or Spark', 'Location: New York, NY\xa0\xa0', '•\tCertification Reimbursement', '•\tEstablished track record in designing, building, deploying, and maintaining scalable systems', '•\tKnowledge developing and debugging Python', 'Department: Financial Services\xa0', 'Compensation & Benefits:', 'Duration: Contract 6 months', 'The Data Engineer will be supporting our client, global investment bank, located in New York, NY. The Data Engineer will be responsible for designing the data warehouse and all related extraction, transformation and load of data functions. The Data Engineer will also test your designs to ensure the system runs smoothly.', '•\tDevelop highly scalable data processing pipelines', '•\tExperience with distributed systems', '•\tCompetitive compensation']",Associate,Contract,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Staff Data Engineer,Walmart,"Dallas, TX",2 days ago,Be among the first 25 applicants,"['', ' Experience with BI Tool Tableau or Looker is a plus', 'Minimum Qualifications...', ' Build robust and scalable applications using SQL, Scala/Python and Spark.', ' Create real time data streaming and processing using Kafka and/or Spark streaming.', ' 10+ years of experience with 5+ years of Big data development experience', ' Demonstrates expertise in writing complex, highly optimized queries across large data sets', ' Design, configure and implement systems that can scale to process terabytes of data between heterogeneous systems on premise and cloud.', ' Design, develop and build database to power Big Data analytical systems.', ' Design, develop and build database to power Big Data analytical systems. Design data integration pipeline architecture and ensure successful creation of the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using Spark, SQL, HQL and other technologies. Build robust and scalable applications using SQL, Scala/Python and Spark. Create real time data streaming and processing using Kafka and/or Spark streaming. Work on creating data ingestion processes to maintain Global Data lake on Google cloud or Azure Engage with architects and senior technical leads to create and enhance complex software components. Design, configure and implement systems that can scale to process terabytes of data between heterogeneous systems on premise and cloud. Work with business customers, product managers and engineers to design feature-based solutions and implement them in an agile fashion. Develop proof-of-concept prototype with fast iteration and experimentation. Develop and maintain design documentation, test cases, performance and monitoring and performance evaluation using Git, Crontab, Putty, Jenkins, Maven, Confluence, ETL, Automic, Zookeeper, Cluster Manager Perform continuous integration and deployment using Jenkins and Git', ' Engage with architects and senior technical leads to create and enhance complex software components.', ' 10+ years of experience with 5+ years of Big data development experience Experience in HDFS, Hive, Hive UDF’s, MapReduce, Druid, Spark, Python, Hue, Shell Scripting, Unix. Demonstrates expertise in writing complex, highly optimized queries across large data sets Retail experience and knowledge of commercial data is a huge plus Experience with BI Tool Tableau or Looker is a plus', ' Work on creating data ingestion processes to maintain Global Data lake on Google cloud or Azure', 'You’ll Make An Impact By', ' Develop proof-of-concept prototype with fast iteration and experimentation.', ' Experience in HDFS, Hive, Hive UDF’s, MapReduce, Druid, Spark, Python, Hue, Shell Scripting, Unix.', 'About Global Tech', 'Outlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications. ', 'Outlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications. ', ""Position Summary... What You'll Do..."", ' Design data integration pipeline architecture and ensure successful creation of the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using Spark, SQL, HQL and other technologies.', ' Develop and maintain design documentation, test cases, performance and monitoring and performance evaluation using Git, Crontab, Putty, Jenkins, Maven, Confluence, ETL, Automic, Zookeeper, Cluster Manager', ' Perform continuous integration and deployment using Jenkins and Git', ' Retail experience and knowledge of commercial data is a huge plus', ' Work with business customers, product managers and engineers to design feature-based solutions and implement them in an agile fashion.']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,GoExpedi,Greater Houston,22 hours ago,73 applicants,"['', 'Medical, Dental, and Vision Plan401K PlanPaid Leave (vacation, sick time, and company holidays)Casual dress environmentProfessional growth & career longevity', 'We are transforming an industry and actively seeking talented professionals with diverse backgrounds like you. At GoExpedi, we recognize and value each other’s differences as well as our combined strengths. We cultivate a work environment that supports creative thinking, inspires the formation of new ideas, encourages communication with all levels of the organization, and promotes contribution and ownership.', 'Strong knowledge of DevOps, in particular, using the Repo and pushing and pulling source code with version control', 'Accountable for the delivery of the data strategy surrounding business intelligence and predictive analytics projects that integrate data into a centralized data environment', 'RESPONSIBILITIES:', 'Strong knowledge of data architectures and systems – SQL and others for ETL purposes', 'Do you want to help us disrupt the industry with innovation? Are you inventive, bright, and determined? If you answered yes to all of these, GoExpedi may be the perfect place for you to grow your career!', 'Minimum 3-4 years of relevant experience in data analytics as a data engineer, data analyst, data scientist (or applied scientist role)', 'Utilize your knowledge of modern data driven methods and domain understanding to support the creation of new products, services and insights using predictive models', 'Think independently and work as part of a cross-functional team to achieve common goals', 'Experience in using Machine Learning, AI and other data science technologies', 'Use DevOps Repository to build, manage and optimize data pipelines and deploy these pipelines into production', 'BENEFITS:', 'GoExpedi is hiring!\xa0', 'Work independently on data modeling/engineering tasks and are eager to learn', 'Paid Leave (vacation, sick time, and company holidays)', 'Knowledge of object-oriented programming and building Rest APIs for data analysis', '\xa0', 'BS degree required; MS preferred in Data Engineering, Data Science Computer Science, Engineering, or another relevant field', 'Develop and design presentations, reports, and other deliverables and summarize findings to key stakeholders and executive leadership, communicating with both technical and non-technical team members.', 'Use your experience and are hands-on in Python, Database Systems (such as SQL) and Azure cloud tools to contribute towards solving data analytics problems', 'Advanced experience with any of the following: Looker, Tableau, Domo, Sisense, or Power BI', 'Solid and demonstrable experience with programming languages such as Python (used numpy, pandas, etc.) within platforms such as Spydr, Anaconda, and Jupyter Notebook', 'Perform data wrangling and processing from multiple databases (structured or un-structured data) while having a keen eye for data quality', 'Medical, Dental, and Vision Plan', 'Professional growth & career longevity', '401K Plan', 'Write independent source code, validate and test for model quality improvement', 'GoExpedi is hiring!\xa0We are looking to hire a Data Engineer who will be responsible for analyzing complex and unstructured data using state-of-the-art data science and engineering methods to help with data driven decision making. This role requires knowledge of data science methods and applies analytics to solve real world problems. The Data Engineer will work with customers and/or internal stake holders to collect and understand data, perform wrangling, develop and integrate applications and collaborate. The ideal candidate will be able to anticipate what data insights GoExpedi will need.', 'Utilize your knowledge of modern data driven methods and domain understanding to support the creation of new products, services and insights using predictive modelsWrite independent source code, validate and test for model quality improvementUse your experience and are hands-on in Python, Database Systems (such as SQL) and Azure cloud tools to contribute towards solving data analytics problemsPerform data wrangling and processing from multiple databases (structured or un-structured data) while having a keen eye for data qualityUse DevOps Repository to build, manage and optimize data pipelines and deploy these pipelines into productionThink independently and work as part of a cross-functional team to achieve common goalsWork independently on data modeling/engineering tasks and are eager to learnStay current in the field of advanced analytics and take initiative to apply new technologiesDevelop and design presentations, reports, and other deliverables and summarize findings to key stakeholders and executive leadership, communicating with both technical and non-technical team members.Accountable for the delivery of the data strategy surrounding business intelligence and predictive analytics projects that integrate data into a centralized data environment', 'BS degree required; MS preferred in Data Engineering, Data Science Computer Science, Engineering, or another relevant fieldMinimum 3-4 years of relevant experience in data analytics as a data engineer, data analyst, data scientist (or applied scientist role)Experience in using Machine Learning, AI and other data science technologiesSolid and demonstrable experience with programming languages such as Python (used numpy, pandas, etc.) within platforms such as Spydr, Anaconda, and Jupyter NotebookStrong knowledge of data architectures and systems – SQL and others for ETL purposesStrong knowledge of DevOps, in particular, using the Repo and pushing and pulling source code with version controlAbility to work with MS Azure cloud tools experience with using Web APIs and JSONKnowledge of object-oriented programming and building Rest APIs for data analysisAdvanced experience with any of the following: Looker, Tableau, Domo, Sisense, or Power BI', 'Casual dress environment', 'REQUIREMENTS:', 'Ability to work with MS Azure cloud tools experience with using Web APIs and JSON', 'Stay current in the field of advanced analytics and take initiative to apply new technologies']",Associate,Full-time,Information Technology,Oil & Energy,2021-03-24 13:05:10
Data Engineer,"SmartAC.com, Inc.","Houston, TX",19 hours ago,30 applicants,"['', 'Experience gathering insights or maintaining big data infrastructure, Redshift, Hadoop, Spark', 'Identify gaps and improvement opportunities in existing data and processing architecture', 'Bachelor’s Degree in Computer Science, Master’s preferred', '5+ years experience as a professional data engineer or software developer', 'Bachelor’s Degree in Computer Science, Master’s preferred5+ years experience as a professional data engineer or software developer3+ years of recent experience ingesting and processing IoT data at speed and scale5+ years, mastery of Java programming language5+ years operating services in linux environments3+ years of data administration in both NoSQL and TSQL architectures3+ years building and deploying serverless Lambda applications on AWS, experience with API Gateway, DynamoDB, CloudFormationability working with data scientists to define algorithms and pipelines to feed machine learning models', 'AWS experience with tools such as Kinesis, Timestream, IoT Core, and GreengrassExperience gathering insights or maintaining big data infrastructure, Redshift, Hadoop, SparkHistory of involvement with machine learning modelsFluency in Python or Node.js', 'History of involvement with machine learning models', 'Work regularly with our subject matter experts to develop and enhance logical algorithms to detect conditions based on IoT telemetryIdentify gaps and improvement opportunities in existing data and processing architectureManage AWS cloud based application infrastructure and IoT pipelinesDesign and develop big data pipelines to support Data Science and Machine Learning objectivesBe part of the software team contributing to technology supporting business objectivesDevelop edge-computing processes', 'Preferred Qualifications', 'Design and develop big data pipelines to support Data Science and Machine Learning objectives', 'Be part of the software team contributing to technology supporting business objectives', '3+ years of data administration in both NoSQL and TSQL architectures', 'SmartAC.com is looking for a passionate, hard-working, experienced, and technically skilled individual to join our technology team as an IoT Data Engineer.\xa0In this role, you will drive the development of data engineering solutions from initial experimentation to production deployment as part of our innovative IoT and cloud-based HVAC monitoring services. You will:', 'Manage AWS cloud based application infrastructure and IoT pipelines', 'AWS experience with tools such as Kinesis, Timestream, IoT Core, and Greengrass', 'A successful candidate will bring strong technical skills and an analytical mindset to deliver business value in a fast-paced environment in a growing organization.\xa0You are self-driven, communicate and collaborate effectively with different technical and non-technical stake-holders.', '3+ years of recent experience ingesting and processing IoT data at speed and scale', 'Requirements', '3+ years building and deploying serverless Lambda applications on AWS, experience with API Gateway, DynamoDB, CloudFormation', 'Fluency in Python or Node.js', '5+ years operating services in linux environments', '5+ years, mastery of Java programming language', 'Develop edge-computing processes', 'ability working with data scientists to define algorithms and pipelines to feed machine learning models', 'Work regularly with our subject matter experts to develop and enhance logical algorithms to detect conditions based on IoT telemetry']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Affinity Search Partners,"Nashville, TN",21 hours ago,33 applicants,"['', 'Have an understanding of AI and ML principals and is able to work with Data Scientists and Software Engineers to build effective data structures that allow for the building of efficient data models', 'Responsibilities:', 'Build, design and implement high-volume, high-scale data solutions that make an impact on the lives of others.Build and scale data platform infrastructure that powers analytics both batch and real-time.Responsible for overall system architecture, scalability, reliability, and performance of data pipelines and data platform components.Have an understanding of AI and ML principals and is able to work with Data Scientists and Software Engineers to build effective data structures that allow for the building of efficient data models', 'Deep understanding in ETL optimization, designing, coding, and tuning big data processes using Apache Spark or Apache Airflo', 'Our client is a rapidly growing healthcare company that is working diligently to eradicate the Fatty Liver Disease. In this role you will create the Data Architecture and the Data Pipelines that extract data from multiple EMR’s and funnels the data into an internal application. Once the data is in the application the data will be used to run internal operations, but they would also like to create AI and ML models from that data to begin to predict the likelihood of fatty liver disease occurring in the patient population. You will be responsible for creating the data pipelines and funnels using Python and AWS. This is a results focused environment where you will experience both freedom and autonomy while working with an exceptional team.\xa0', 'Sound knowledge of distributed systems and data architecture with the ability to design and implement batch and stream data processing pipelines.', 'Build and scale data platform infrastructure that powers analytics both batch and real-time.', 'Bachelor or Master’s degree in Computer Science, Engineering or Data Science highly preferred', 'Build, design and implement high-volume, high-scale data solutions that make an impact on the lives of others.', '5+ years of experience in building large scale data pipelines for distributed systems using PythonDeep understanding in ETL optimization, designing, coding, and tuning big data processes using Apache Spark or Apache AirfloSound knowledge of distributed systems and data architecture with the ability to design and implement batch and stream data processing pipelines.Strong knowledge of AWS and how to optimize the cloud for efficient Data extracting and processing.Bachelor or Master’s degree in Computer Science, Engineering or Data Science highly preferred', 'Strong knowledge of AWS and how to optimize the cloud for efficient Data extracting and processing.', 'Skills and Qualifications:', '5+ years of experience in building large scale data pipelines for distributed systems using Python', 'Responsible for overall system architecture, scalability, reliability, and performance of data pipelines and data platform components.']",Associate,Full-time,Information Technology,Hospital & Health Care,2021-03-24 13:05:10
Data Engineer,American Express,"Phoenix, AZ",1 week ago,Over 200 applicants,"['', 'Evangelize industry best practices.', 'Minimum Qualifications', 'Hands-on experience on implementing RESTful APIs using Java and Python frameworks is a plus.', 'Self-Starter, Problem solver, Highly collaborative and adaptive.', ' BS degree or higher in computer science or related discipline. 4+ years of demonstrated experience in Agile development, application design, software development, and testing. Good knowledge of Algorithms and Data structures, Software Design patterns and enterprise integration patterns. Strong Hands-on Big data technologies like Apache Spark Data frame and Datasets, Hive Query Language, HDFS. Experience in Java, Python, Scala languages. Good understanding of CI/CD processes leveraging Jenkins, SBT, XLR and Maven. Knowledge/ experience in Elastic Cache, RDBMS and NoSQL databases. Hands-on experience on implementing RESTful APIs using Java and Python frameworks is a plus. Experience in building ETL pipelines using tools like NIFI, Informatica, Ab Initio. Ability to implement scalable, high performing, secure, highly available solutions. Self-Starter, Problem solver, Highly collaborative and adaptive. Excellent communication skills, enthusiasm and ability ask questions, understand business value. Any knowledge of Salesforce ecosystem is a plus. ', 'Knowledge/ experience in Elastic Cache, RDBMS and NoSQL databases.', 'Any knowledge of Salesforce ecosystem is a plus.', 'Experience in Java, Python, Scala languages.', 'Good understanding of CI/CD processes leveraging Jenkins, SBT, XLR and Maven.', 'Prepare data pipelines to create features for prescriptive and predictive modeling.', 'Build ETL pipelines to analyze, combine and organize raw data from different sources.', 'Promote inner sourcing.', 'Collaborate with data scientists and architects to build end to end AI powered products.', 'Perform hands on coding and configuration to deliver solutions that impact multiple platforms and products.', 'Develop and improve our CI/CD workflow tools and processes', 'Manage risks and issues as well as cross dependencies with other teams. Communicate effectively with internal teams and client to address technical design and functional gaps.', 'Experience in building ETL pipelines using tools like NIFI, Informatica, Ab Initio.', 'Sometimes be involved in conducting complex data analysis and transform the result into visual report using data visualization tools.', '4+ years of demonstrated experience in Agile development, application design, software development, and testing.', 'Ability to implement scalable, high performing, secure, highly available solutions.', 'Partner with stakeholders to continuously upgrade Enterprise platforms as per industry standards and Technology stack latest offerings.', 'Learn new technologies and drive opportunities for adoption.', 'Excellent communication skills, enthusiasm and ability ask questions, understand business value.', "" Build ETL pipelines to analyze, combine and organize raw data from different sources. Sometimes be involved in conducting complex data analysis and transform the result into visual report using data visualization tools. Prepare data pipelines to create features for prescriptive and predictive modeling. Collaborate with data scientists and architects to build end to end AI powered products. Perform hands on coding and configuration to deliver solutions that impact multiple platforms and products. Debug complex issues spanning multiple systems. Harness the opportunities to build Intellectual Property within the team and for the enterprise. Develop and improve our CI/CD workflow tools and processes Promote inner sourcing. Evangelize industry best practices. Seek to understand your customer's needs and problems Learn new technologies and drive opportunities for adoption. Manage risks and issues as well as cross dependencies with other teams. Communicate effectively with internal teams and client to address technical design and functional gaps. Partner with stakeholders to continuously upgrade Enterprise platforms as per industry standards and Technology stack latest offerings. "", 'Debug complex issues spanning multiple systems.', 'Good knowledge of Algorithms and Data structures, Software Design patterns and enterprise integration patterns.', 'Strong Hands-on Big data technologies like Apache Spark Data frame and Datasets, Hive Query Language, HDFS.', 'Harness the opportunities to build Intellectual Property within the team and for the enterprise.', ""Seek to understand your customer's needs and problems"", 'BS degree or higher in computer science or related discipline.']",Associate,Full-time,Information Technology,Financial Services,2021-03-24 13:05:10
Data Engineer,Dashlane,"New York, NY",1 day ago,45 applicants,"['', ' You have 3+ years of experience as a data engineer, business intelligence analyst, or in a highly analytical role ', 'Develop data models and schemas in our data warehouse that enable performant, intuitive analysis', 'You have experience designing SQL tables, choosing indexes, tuning queries, and optimizations ac ross different functional en vironments. ', 'You have experience administrating, ingesting, and monitoring data in data warehouses such as Amazon Redshift or Microsoft SQL Server', 'Build data pipelines and python-based ETL tools for acquiring, processing, and delivering data', 'Develop the server applications and APIs that are used by our Data Team', ' Build data pipelines and python-based ETL tools for acquiring, processing, and delivering data Develop data models and schemas in our data warehouse that enable performant, intuitive analysis Handle the challenges that come with managing terabytes of data Collaborate with business leaders and analysts to define key metrics and build reporting to monitor and understand company performance Develop the server applications and APIs that are used by our Data Team ', 'Your Interview Experience', 'You are a self-starter that can work in a fast-paced, distributed environment, as you will be collaborating with our Paris and Lisbon teams', '  You have 3+ years of experience as a data engineer, business intelligence analyst, or in a highly analytical role  You have 3+ years of experience with a scripting language (preferably Python) for data processing and analysis You have experience designing SQL tables, choosing indexes, tuning queries, and optimizations ac ross different functional en vironments.   You have experience writing complex SQL queries and using a BI tool  ', 'About The Role', 'Diversity, Equity, Inclusion And Belonging At Dashlane', 'You have 3+ years of experience with a scripting language (preferably Python) for data processing and analysis', ' You have experience writing complex SQL queries and using a BI tool ', ""We're Also Looking For"", 'You have a passion for sharing the value of data and communicating insights to a broad audience with varying levels of technical expertise', 'Requirements', 'Collaborate with business leaders and analysts to define key metrics and build reporting to monitor and understand company performance', 'Handle the challenges that come with managing terabytes of data', ' You have a passion for sharing the value of data and communicating insights to a broad audience with varying levels of technical expertise You have experience with data lakes and designing and maintaining data solutions using Spark and AWS serverless services such as Kinesis, Lambda, or SQS You have experience administrating, ingesting, and monitoring data in data warehouses such as Amazon Redshift or Microsoft SQL Server You are a self-starter that can work in a fast-paced, distributed environment, as you will be collaborating with our Paris and Lisbon teams ', 'At Dashlane You Will', 'About Dashlane', 'You have experience with data lakes and designing and maintaining data solutions using Spark and AWS serverless services such as Kinesis, Lambda, or SQS']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Microsoft,"Fargo, ND",18 hours ago,Be among the first 25 applicants,"['', ' Experience delivering data solutions via Agile methodologies.', ' Excellent problem solving, critical thinking, and communication skills.', 'Preferred Qualifications:\u202f ', ' Work closely with other CE&S BI PMs, as well as key stakeholders, to facilitate and coordinate the data platform backlog grooming process, triaging new feature requests in preparation for the next sprint.', ' Experience on Dimensional Data Modeling and Taxonomy Classification.', ' Ensure data extraction, transformation and loading data meet data security & compliance requirements', ' Experience working with large-scale data processing and storage using Azure Data Lake and Data Services, Databricks, Stream Analytics, SQL, Power BI and Spark as well as coding with Python scripting', ' Experience building and optimizing ‘Big Data’ data pipelines, architectures and data sets.', ' Understanding of Privacy and Compliance aspects of data storage & processing as it relates to GDPR and PII.', ' Engage with data source platform leads to gain tactical and strategic understanding of data sources required by CE&S.', ' Create and maintain optimal data pipeline architecture Assemble large, complex data sets that meet functional / non-functional business requirements. Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and Big Data technologies. Deliver automation & lean processes to ensure high quality throughput & performance of the entire data & analytics platform.\u202f Engage with data source platform leads to gain tactical and strategic understanding of data sources required by CE&S. Work closely with other CE&S BI PMs, as well as key stakeholders, to facilitate and coordinate the data platform backlog grooming process, triaging new feature requests in preparation for the next sprint. Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics. Ensure data extraction, transformation and loading data meet data security & compliance requirements Create data tools for analytics and data scientist team members that assist them in building and optimizing our data product into an innovative industry leader. Work with data and analytics experts to strive for greater functionality in our data systems. \u202fEnsure data quality assurance and establish SLAs for data publishes.', ' An attention to detail with self-discipline and a drive for results.', ' Deliver automation & lean processes to ensure high quality throughput & performance of the entire data & analytics platform.\u202f', ' Create data tools for analytics and data scientist team members that assist them in building and optimizing our data product into an innovative industry leader.', ' Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.', ' Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.', ' BA/BS in Computer Science, Engineering or equivalent software/services experience', 'Required Qualifications: ', ' Assemble large, complex data sets that meet functional / non-functional business requirements.', ' BA/BS in Computer Science, Engineering or equivalent software/services experience Expertise in T-SQL and SQL development using stored procedures & frameworks. Experience working with large-scale data processing and storage using Azure Data Lake and Data Services, Databricks, Stream Analytics, SQL, Power BI and Spark as well as coding with Python scripting Understanding of Privacy and Compliance aspects of data storage & processing as it relates to GDPR and PII. Experience on Dimensional Data Modeling and Taxonomy Classification. Experience delivering data solutions via Agile methodologies. Experience building and optimizing ‘Big Data’ data pipelines, architectures and data sets. Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. A successful history of manipulating, processing and extracting value from large disconnected datasets. Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores. Excellent problem solving, critical thinking, and communication skills. An attention to detail with self-discipline and a drive for results. A passion for finding solutions to hard problems at scale and operationalizing them.', ' 7+ years of experience developing data platform with large and complex datasets 4+ years of experience working with SQL, Azure Databricks, Spark and/or other Big Data technologies.\u202f', ' A successful history of manipulating, processing and extracting value from large disconnected datasets.', 'Key Accountabilities:', ' 7+ years of experience developing data platform with large and complex datasets', 'Responsibilities', ' Expertise in T-SQL and SQL development using stored procedures & frameworks.', ' A passion for finding solutions to hard problems at scale and operationalizing them.', 'Qualifications', ' \u202fEnsure data quality assurance and establish SLAs for data publishes.', ' Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and Big Data technologies.', ' Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.', 'Data Engineer', ' Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.', ' 4+ years of experience working with SQL, Azure Databricks, Spark and/or other Big Data technologies.\u202f', ' Create and maintain optimal data pipeline architecture', ' Work with data and analytics experts to strive for greater functionality in our data systems.']",Not Applicable,Full-time,Information Technology,Computer Hardware,2021-03-24 13:05:10
Data Engineer,Tesla,"Fremont, CA",6 days ago,Over 200 applicants,"['', 'Your Role', 'Technology We Use', 'Plan effective data storage, security, sharing and publishing within the organization', '5 – 8 years of development experience at an Enterprise level in the following tools and languages: Informatica , Python', 'Experience in Spark Framework on both batch and real-time data processing is a plus', 'Vertica', 'Derive an overall strategy of data management, within an established information architecture that supports the development and secure operation of existing and new information and digital services', 'Kafka', 'SQL Server and MySQL', 'Experience in Big Data Integration & Analytics is a plus', 'Python', 'Experience in Supply Chain and Logistics data is a plus', 'Collaborate with internal and external data providers on data validation providing feedback and making customized changes to data feeds and data mappings', 'Bachelor’s degree in Computer Science or related field or equivalent combination of industry related professional experience and education', 'PythonInformaticaSQL Server and MySQLVerticaKafka', 'Experience', 'Strong background with data modeling, data access, and data storage techniques', 'Ensures data quality and implements tools and frameworks for automating the identification of data quality issues', 'Gathers and processes raw, structured, semi-structured, and unstructured data using batch and real-time data processing frameworks', 'Experience with design, development, and implementation of highly scalable, high-volume software systems and components, source of truth systems for different business areas, developing and maintaining web services in an agile environment', 'Design, code, test, correct and document programs and scripts using agreed standards and tools to achieve a well-engineered resultDerive an overall strategy of data management, within an established information architecture that supports the development and secure operation of existing and new information and digital servicesPlan effective data storage, security, sharing and publishing within the organizationGathers and processes raw, structured, semi-structured, and unstructured data using batch and real-time data processing frameworksEnsures data quality and implements tools and frameworks for automating the identification of data quality issuesCollaborate with internal and external data providers on data validation providing feedback and making customized changes to data feeds and data mappingsMentor and lead data engineers providing technical guidance and oversightProvides ongoing support, monitoring, and maintenance of deployed products', 'Informatica', '5 – 8 years of development experience at an Enterprise level in the following tools and languages: Informatica , PythonStrong experience with relational databases like SQL Server, MySQL and Vertica. NoSQL databases experience is a plusStrong background with data modeling, data access, and data storage techniquesExperience with design, development, and implementation of highly scalable, high-volume software systems and components, source of truth systems for different business areas, developing and maintaining web services in an agile environmentWorking experience with Kafka Streaming layerExperience in Spark Framework on both batch and real-time data processing is a plusExperience in Big Data Integration & Analytics is a plusExperience in Supply Chain and Logistics data is a plusBachelor’s degree in Computer Science or related field or equivalent combination of industry related professional experience and education', 'Qualifications', 'Provides ongoing support, monitoring, and maintenance of deployed products', 'Design, code, test, correct and document programs and scripts using agreed standards and tools to achieve a well-engineered result', 'Mentor and lead data engineers providing technical guidance and oversight', 'Strong experience with relational databases like SQL Server, MySQL and Vertica. NoSQL databases experience is a plus', 'Working experience with Kafka Streaming layer']",Entry level,Full-time,Information Technology,Automotive,2021-03-24 13:05:10
Data Engineer (48221),Zipcar,"Boston, MA",7 days ago,57 applicants,"['', 'What You Will Do', 'What you’ll love about being a Zipster in Engineering:', 'You’ll get to work in a flexible work environment that is innovative and resourceful', 'Free Zipcar Membership and other employee discounts, including discounts on renting and buying Avis/Budget cars', 'Bachelor’s degree in computer science\u202for related field\u202f\u202f', 'We encourage Zipsters to bring their whole selves to work - unique perspectives, personal experiences, backgrounds, and however they identify. We are proud to be an equal opportunity employer – M/F/D/V.', 'Develop solutions in bleeding-edge data technologies on AWS eco-system', ' Competitive Medical, Dental, Vision, Life and Disability Insurance and other voluntary benefits through our parent company, Avis Budget Group Generous paid time off, including holidays, vacation, personal, sick, volunteer and Parental Leave options Tax-free benefit for public transportation or parking expenses Bicycle Reimbursement program 401(k) Retirement Plan with company matched contributions Free Zipcar Membership and other employee discounts, including discounts on renting and buying Avis/Budget cars Community involvement opportunities ', 'Community involvement opportunities', "" You’ll get to come to work every day knowing that you’re contributing to the success of the world’s leading car sharing network You'll be joining a collaborative, diverse, passionate team of Zipsters You’ll get to work in a flexible work environment that is innovative and resourceful At Zipcar, we encourage new tools and ideas- your opinion always matters! "", '3+\u202fyears’ experience\u202fwith ETL technologies (e.g. Talend, Informatica,\u202fMatillion)\u202f', '3+\u202fyears’ experience\u202fwith Java, Python,\u202fScala\u202fand other programming languages\u202f', 'You’ll get to come to work every day knowing that you’re contributing to the success of the world’s leading car sharing network', '3+ years’ experience with SQL, optimizing queries and tuning database performance\u202f', ' Build mission critical data pipelines that power Zipcar’s business Develop solutions in bleeding-edge data technologies on AWS eco-system Partner with architects and data analysts to make the best technology decisions Create and maintain data models, data catalogs, and data security Mentor team members and assist them in developing, testing and deploying software ', 'Generous paid time off, including holidays, vacation, personal, sick, volunteer and Parental Leave options', 'Who are we? ', '1+\u202fyears’ experience\u202fwith key AWS technologies (Redshift, S3, IAM;\u202falso\u202fKinesis, Lambda, EMR, Spark, Hive)\u202f', 'What Tops Off The Tank', '3+\u202fyears’ experience\u202fwith data engineering or data warehousing, or an equivalent data-oriented software engineering background\u202f', 'Bicycle Reimbursement program', 'Create and maintain data models, data catalogs, and data security', 'Experience with\u202fdbt\u202fand Airflow a strong plus\u202f', 'At Zipcar, we encourage new tools and ideas- your opinion always matters!', 'Competitive Medical, Dental, Vision, Life and Disability Insurance and other voluntary benefits through our parent company, Avis Budget Group', 'Familiarity with data analysis and data science tools (Looker,\u202fJupyter, R) is a plus\u202f', ""You'll be joining a collaborative, diverse, passionate team of Zipsters"", 'Qualifications', 'Partner with architects and data analysts to make the best technology decisions', 'The Extra Mile', 'Mentor team members and assist them in developing, testing and deploying software', 'Build mission critical data pipelines that power Zipcar’s business', 'Tax-free benefit for public transportation or parking expenses', ' Bachelor’s degree in computer science\u202for related field\u202f\u202f 3+\u202fyears’ experience\u202fwith data engineering or data warehousing, or an equivalent data-oriented software engineering background\u202f 3+ years’ experience with SQL, optimizing queries and tuning database performance\u202f 1+\u202fyears’ experience\u202fwith key AWS technologies (Redshift, S3, IAM;\u202falso\u202fKinesis, Lambda, EMR, Spark, Hive)\u202f 3+\u202fyears’ experience\u202fwith ETL technologies (e.g. Talend, Informatica,\u202fMatillion)\u202f 3+\u202fyears’ experience\u202fwith Java, Python,\u202fScala\u202fand other programming languages\u202f Experience with\u202fdbt\u202fand Airflow a strong plus\u202f Familiarity with data analysis and data science tools (Looker,\u202fJupyter, R) is a plus\u202f ', '401(k) Retirement Plan with company matched contributions']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Riskalyze,"Philadelphia, PA",3 weeks ago,91 applicants,"['', 'Consult with R&D Product Managers to ensure that new products and features are developed to feed data seamlessly into the data pipeline.', '3 weeks Vacation & 1 week of sick time per year + 11 paid holidays.', '401(k) Retirement savings with employer matching dollar-for-dollar, up to 4%.', 'Work with stakeholders to assist with data-related technical issues and support data infrastructure needs.', ' Medical, dental and vision with access to HSA or FSA depending on chosen medical plan. Available pet insurance. 401(k) Retirement savings with employer matching dollar-for-dollar, up to 4%. Annual bonus subject to company/individual performance. On-site financial planning with a registered financial advisor. 3 weeks Vacation & 1 week of sick time per year + 11 paid holidays. All hands team meetings every 6 weeks with catering. Fully stocked drink fridges. In office snacks 3x per week. ', 'Requirements', 'Strong communication, collaboration, and presentation skills.', 'Mentor other RevOps team members on data pipelines and architecture to create resilience in our resourcing and systems.', '5+ years of experience working with at least half of the following technologies SQL, cURL, JSON, Javascript, Kotlin, Apex and/or Python. Clear ability to rapidly extend your natural skills to encompass the other half.', 'Experience working independently on data pipeline and integration projects; owning and leading a function without a large team to fall back on; willingness to be entrepreneurial and scrappy in creating infrastructure from scratch. Strong systems thinker.', 'All hands team meetings every 6 weeks with catering.', 'Medical, dental and vision with access to HSA or FSA depending on chosen medical plan.', 'Strong experience with ETL tools, databases, data warehousing solutions', 'Responsibilities', 'Build monitoring and alerting tools to notify multiple people on the Revenue Operations team when our core systems or products have issues producing data, our data pipelines have issues extracting and transforming data, or our customer-facing systems have issues receiving data.', ' Experience working independently on data pipeline and integration projects; owning and leading a function without a large team to fall back on; willingness to be entrepreneurial and scrappy in creating infrastructure from scratch. Strong systems thinker. Strong experience with ETL tools, databases, data warehousing solutions 5+ years of experience working with at least half of the following technologies SQL, cURL, JSON, Javascript, Kotlin, Apex and/or Python. Clear ability to rapidly extend your natural skills to encompass the other half. Elite at constructing data schemas and structuring datasets for ease of use by our customer-facing teams. Ability to distill down complex concepts and communicate them to non-technical teams. Strong communication, collaboration, and presentation skills. ', 'Design and build automated and scalable data pipelines to extract that data from our core databases and products, transform that data into usable datasets for our customer-facing teams, and load that data into Salesforce, Domo and other tools.', ' Work with leaders across the company to develop a strong understanding of their needs for data sets in Salesforce, Domo and other tools for our customer-facing teams. Work with engineering teams in the R&D organization to understand the data schemas and structures. Design and build automated and scalable data pipelines to extract that data from our core databases and products, transform that data into usable datasets for our customer-facing teams, and load that data into Salesforce, Domo and other tools. Coordinate the validation of data quality by requiring both the R&D stakeholders who have knowledge of data structure, and the customer-facing teams who have knowledge of customer activities and business operations, to review the outputs of the data pipeline and ensure it is accurate and complete. Build monitoring and alerting tools to notify multiple people on the Revenue Operations team when our core systems or products have issues producing data, our data pipelines have issues extracting and transforming data, or our customer-facing systems have issues receiving data. Work with stakeholders to assist with data-related technical issues and support data infrastructure needs. Consult with R&D Product Managers to ensure that new products and features are developed to feed data seamlessly into the data pipeline. Mentor other RevOps team members on data pipelines and architecture to create resilience in our resourcing and systems. ', 'Elite at constructing data schemas and structuring datasets for ease of use by our customer-facing teams. Ability to distill down complex concepts and communicate them to non-technical teams.', 'Available pet insurance.', 'On-site financial planning with a registered financial advisor.', 'Benefits', 'Coordinate the validation of data quality by requiring both the R&D stakeholders who have knowledge of data structure, and the customer-facing teams who have knowledge of customer activities and business operations, to review the outputs of the data pipeline and ensure it is accurate and complete.', 'In office snacks 3x per week.', 'Work with leaders across the company to develop a strong understanding of their needs for data sets in Salesforce, Domo and other tools for our customer-facing teams. Work with engineering teams in the R&D organization to understand the data schemas and structures.', 'Fully stocked drink fridges.', 'Annual bonus subject to company/individual performance.']",Mid-Senior level,Full-time,Engineering,Computer Software,2021-03-24 13:05:10
Data Engineer,Conexess Group,"Gladstone, OR",16 hours ago,34 applicants,"['', 'Desired Skills', 'Application Development/Programming', 'Experience in Big Data technologies like Hadoop, Spark, and Kafka', 'Python (Required)', 'Strong Data Wrangling acumen (Pandas, SQL, Spark)', 'Experience with AWS', ' Design, develop, and support Machine Learning pipelines, components, code, algorithms, and processes  Supports other product delivery partners in successful build, test, and release of solutions  Works with distributed requirements and technical stakeholders to complete shared design and development  Supports the full software lifecycle of design, development, testing, and support for technical delivery  Occasional ownership of small projects, and managing some ongoing project communication (status reporting, requirements alignment, etc.) Has delivered solutions through to production using Python / R / Scala Experience with AWS Experience in Big Data technologies like Hadoop, Spark, and Kafka Experience with Docker and/or containerized deployments preferred  Experience with ML aligned artifacts like Pandas, Numpy, etc. preferred Has worked with and is comfortable querying / joining complex data sets ', 'Advanced degree preferred', 'Design, develop, and support Machine Learning pipelines, components, code, algorithms, and processes ', 'Occasional ownership of small projects, and managing some ongoing project communication (status reporting, requirements alignment, etc.)', ' Strong Data Wrangling acumen (Pandas, SQL, Spark) Application Development/Programming Python (Required) Experience with AI/ML technologies (PyTorch / TensorFlow, Jupyter) Experience in the Full Software Development Life Cycle Excellent Communication skills Advanced degree preferred ', 'Supports the full software lifecycle of design, development, testing, and support for technical delivery ', 'Experience in the Full Software Development Life Cycle', 'Experience with Docker and/or containerized deployments preferred ', 'Works with distributed requirements and technical stakeholders to complete shared design and development ', 'Experience with ML aligned artifacts like Pandas, Numpy, etc. preferred', 'Has worked with and is comfortable querying / joining complex data sets', 'Supports other product delivery partners in successful build, test, and release of solutions ', 'Our History:', 'Excellent Communication skills', 'Has delivered solutions through to production using Python / R / Scala', 'Experience with AI/ML technologies (PyTorch / TensorFlow, Jupyter)']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
"Lead Data Engineer, Enterprise Data and Technology",The Hershey Company,"Hershey, PA",19 hours ago,Be among the first 25 applicants,"[' Microsoft Azure (ADF, ADLS Gen 2, DevOps) Apache Hadoop, Spark, or Hive Relational databases (Teradata, Snowflake, or SQL Server) Data ETL/ELT (Informatica, Talend or Matillion) ', 'Experience with large data sets (Terabytes in size) and the techniques required to handle, store, and process. ', 'Ensures relevant data and analytics are available to meet business needs by continuously wrangling, consolidating, and modernizing existing data solutions and platforms.', 'Strategic thinker with holistic vision. Identify existing manual processes and proposing automation solutions to drive key business performance.', '3+ years of experience in data management, ingesting new data and transforming/integrating data.', '3+ years of experience with public and private cloud solutions, including building and maintaining a data ecosystem; Azure preferred', 'Apache Hadoop, Spark, or Hive', 'Relational databases (Teradata, Snowflake, or SQL Server)', 'Designs and implements data pipelines to transform source data into target data models. Ensures the pipeline is validated, consistent, and performant with internal standards. ', '3+ years of experience in mapping data sources, documenting interfaces and data movement providing lineage.', 'Engages with project teams to: understand a project’s needs/requirements, provide technical expertise with data and analytical tools (including legacy and cloud), and to deliver on project goals and timelines.', 'Duties', 'Drives end-to-end data and analytic solutions through accurate requirements development and focused execution.', '3+ years of professional experience related to data engineering and analytics, including: Microsoft Azure (ADF, ADLS Gen 2, DevOps) Apache Hadoop, Spark, or Hive Relational databases (Teradata, Snowflake, or SQL Server) Data ETL/ELT (Informatica, Talend or Matillion)  ', ""This position is opened to fully remoteSummaryThe Enterprise Data organization drives value for Hershey by providing high-quality, well governed data to the Enterprise for analytics and decision-making. Working with technical product owners and business stakeholders in the commercial domain, the incumbent will ensure their data solutions meet expectations and requirements. The Data Engineering team will execute the data strategy to deliver cloud-based intelligent systems to collect, distribute, model, and analyze disparate and diverse data assets to automate insights and drive business performance.Duties Designs and implements data pipelines to transform source data into target data models. Ensures the pipeline is validated, consistent, and performant with internal standards.  Drives end-to-end data and analytic solutions through accurate requirements development and focused execution. Works with cross-functional teams to ensure automated data pipelines are secured and optimized for scaling transformations. Ensures relevant data and analytics are available to meet business needs by continuously wrangling, consolidating, and modernizing existing data solutions and platforms. Evolves the systems and capabilities that drive transformational analytics and enterprise-wide solutions and transforms data assets into business-driven solutions. Engages with project teams to: understand a project’s needs/requirements, provide technical expertise with data and analytical tools (including legacy and cloud), and to deliver on project goals and timelines. Strategic thinker with holistic vision. Identify existing manual processes and proposing automation solutions to drive key business performance. Leads the operational activities of data transformation, data structures, metadata, dependency and workload management post go-live.  QualificationsBachelor’s degree with quantitative, STEM focus (e.g. Business, Computer Science, Engineering, Economics, Mathematics, Statistics, Psychology - other degrees will be considered). 3+ years of experience in data management, ingesting new data and transforming/integrating data. 3+ years of experience with scripting and programming languages; SQL, Python/PySpark/Scala preferred 3+ years of professional experience related to data engineering and analytics, including: Microsoft Azure (ADF, ADLS Gen 2, DevOps) Apache Hadoop, Spark, or Hive Relational databases (Teradata, Snowflake, or SQL Server) Data ETL/ELT (Informatica, Talend or Matillion)   3+ years of experience with public and private cloud solutions, including building and maintaining a data ecosystem; Azure preferred 3+ years of experience in mapping data sources, documenting interfaces and data movement providing lineage. Experience with large data sets (Terabytes in size) and the techniques required to handle, store, and process.  Experience working with business owners to define and deliver key performance indicators within data and analytical solutions. The Hershey Company is an Equal Opportunity Employer. The policy of The Hershey Company is to extend opportunities to qualified applicants and employees on an equal basis regardless of an individual's race, color, gender, age, national origin, religion, citizenship status, marital status, sexual orientation, gender identity, transgender status, physical or mental disability, protected veteran status, genetic information, pregnancy, or any other categories protected by applicable federal, state or local laws.The Hershey Company is an Equal Opportunity Employer - Minority/Female/Disabled/Protected Veterans"", ' 3+ years of experience in data management, ingesting new data and transforming/integrating data. 3+ years of experience with scripting and programming languages; SQL, Python/PySpark/Scala preferred 3+ years of professional experience related to data engineering and analytics, including: Microsoft Azure (ADF, ADLS Gen 2, DevOps) Apache Hadoop, Spark, or Hive Relational databases (Teradata, Snowflake, or SQL Server) Data ETL/ELT (Informatica, Talend or Matillion)   3+ years of experience with public and private cloud solutions, including building and maintaining a data ecosystem; Azure preferred 3+ years of experience in mapping data sources, documenting interfaces and data movement providing lineage. Experience with large data sets (Terabytes in size) and the techniques required to handle, store, and process.  Experience working with business owners to define and deliver key performance indicators within data and analytical solutions. ', '3+ years of experience with scripting and programming languages; SQL, Python/PySpark/Scala preferred', ' Designs and implements data pipelines to transform source data into target data models. Ensures the pipeline is validated, consistent, and performant with internal standards.  Drives end-to-end data and analytic solutions through accurate requirements development and focused execution. Works with cross-functional teams to ensure automated data pipelines are secured and optimized for scaling transformations. Ensures relevant data and analytics are available to meet business needs by continuously wrangling, consolidating, and modernizing existing data solutions and platforms. Evolves the systems and capabilities that drive transformational analytics and enterprise-wide solutions and transforms data assets into business-driven solutions. Engages with project teams to: understand a project’s needs/requirements, provide technical expertise with data and analytical tools (including legacy and cloud), and to deliver on project goals and timelines. Strategic thinker with holistic vision. Identify existing manual processes and proposing automation solutions to drive key business performance. Leads the operational activities of data transformation, data structures, metadata, dependency and workload management post go-live.  ', 'Job Location: Hershey, PA', 'Summary', 'Evolves the systems and capabilities that drive transformational analytics and enterprise-wide solutions and transforms data assets into business-driven solutions.', 'Experience working with business owners to define and deliver key performance indicators within data and analytical solutions.', 'Qualifications', 'Microsoft Azure (ADF, ADLS Gen 2, DevOps)', 'Works with cross-functional teams to ensure automated data pipelines are secured and optimized for scaling transformations.', 'Job Title: Lead Data Engineer, Enterprise Data and Technology ', 'Leads the operational activities of data transformation, data structures, metadata, dependency and workload management post go-live. ', 'Data ETL/ELT (Informatica, Talend or Matillion)']",Associate,Full-time,Information Technology,Food & Beverages,2021-03-24 13:05:10
Data Engineer,Rent The Runway,"Brooklyn, NY",2 days ago,54 applicants,"['', ' Paid Sabbatical after 5 years of continuous service - Unplug, recharge, and have some fun! ', ' Extensive practice using SQL for analytics research and data cleansing. ', ' High proficiency in relational databases (e.g. MySQL) and NoSQL stores (e.g. MongoDB); experience in merging disparate data forms. ', ' Universal Paid Parental Leave for both parents + flexible return to work program - because we know your newest family member(s) deserve your undivided attention. ', ' Paid Time Off including vacation, paid bereavement, and family sick leave - every employee needs time to take care of themselves and their family. ', 'About You', ' Comprehensive health, vision, dental, FSA and dependent care from day 1 of employment - Your health comes first and we’ve got you covered. ', ' Participate in the full lifecycle of ownership for the data warehousing and reporting infrastructure including design, build, and maintenance. ', '  Paid Time Off including vacation, paid bereavement, and family sick leave - every employee needs time to take care of themselves and their family.   Universal Paid Parental Leave for both parents + flexible return to work program - because we know your newest family member(s) deserve your undivided attention.   Paid Sabbatical after 5 years of continuous service - Unplug, recharge, and have some fun!   Exclusive employee subscription and rental discounts - to ensure you experience the magic of renting the runway (and give us valued feedback!).   Comprehensive health, vision, dental, FSA and dependent care from day 1 of employment - Your health comes first and we’ve got you covered.   401k match - an investment in your future.   Company wide events and outings - our team spirit is no joke - we know how to have fun!   Flexibility Policy - when our corporate employees return to the office post COVID they will have the option to work remotely 2-3 days a week.  ', ' Flexibility Policy - when our corporate employees return to the office post COVID they will have the option to work remotely 2-3 days a week. ', ' Experience in custom ETL development and tools including scripting languages (Python preferable). ', ""What You'll Do"", 'About The Job', '  Participate in the full lifecycle of ownership for the data warehousing and reporting infrastructure including design, build, and maintenance.   Support new reporting requirements while maintaining system performance and handling increasing complexity by setting standards across the stack.   Increase the reliability and efficiency of backend systems to empower analysts, and provide an exceptional level of confidence around data quality.   Familiarity with tools and best practices that support data warehousing and reporting at scale, and continuously seek/implement new tools to improve the infrastructure.  ', ' Support new reporting requirements while maintaining system performance and handling increasing complexity by setting standards across the stack. ', ' 401k match - an investment in your future. ', ' Increase the reliability and efficiency of backend systems to empower analysts, and provide an exceptional level of confidence around data quality. ', 'About Us', ' 3+ years of relevant professional work experience. ', ' Company wide events and outings - our team spirit is no joke - we know how to have fun! ', 'About The Team', 'Benefits', ' Worked actively in distributed data warehousing platforms like Vertica, Aster, Redshift, Snowflake, etc. – experience in sharding, handling data-shipping. ', '  Academic background in Computer Science, MIS, or other quantitative/engineering field   3+ years of relevant professional work experience.   Worked actively in distributed data warehousing platforms like Vertica, Aster, Redshift, Snowflake, etc. – experience in sharding, handling data-shipping.   High proficiency in relational databases (e.g. MySQL) and NoSQL stores (e.g. MongoDB); experience in merging disparate data forms.   Experience in custom ETL development and tools including scripting languages (Python preferable).   Extensive practice using SQL for analytics research and data cleansing.  ', ' Exclusive employee subscription and rental discounts - to ensure you experience the magic of renting the runway (and give us valued feedback!). ', ' Academic background in Computer Science, MIS, or other quantitative/engineering field ', ' Familiarity with tools and best practices that support data warehousing and reporting at scale, and continuously seek/implement new tools to improve the infrastructure. ']",Mid-Senior level,Full-time,Supply Chain,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer,Hollstadt Consulting,"Minneapolis, MN",1 day ago,90 applicants,"['', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Maintain existing Microsoft Power BI data infrastructure (Azure SQL, Pipeline, and tables)', 'Since 1990, Hollstadt has been a trusted partner to more than 150 domestic and global companies and has successfully completed over 1,000 projects. Our continued growth has created challenging and rewarding opportunities for accomplished project managers and business analysts. Hollstadt Consulting is an Equal Opportunity/Affirmative Action employer.\xa0', ""•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa05+ years' hands-on experience with Information Technology teams and technology"", '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Create and maintain documentation of data dictionary, data models, and available reports', 'EDUCATION AND EXPERIENCE', ""•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Bachelor's degree in MIS/CIS, business management, or commensurate experience"", 'In this role you will:', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Contribute to overall design and architecture of solutions including internal and external data sources, and the respective flows from Azure to Pipeline to PBI Apps', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Able to work collaboratively with minimal guidance', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa03+ years’ experience with Microsoft Power BI (i.e., Power Pivot/ OLAP, Power Query, Power BI Desktop, Power BI Services) in a private, public, government, or military environment', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Manage all data and technologies across the data source and warehouse layers for Power BI', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Collaborate with business resources, including Analytics lead, to define opportunities and capabilities for AI/ML utilization within the company', '\xa0', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Support data acquisition needs for existing and new data sources across Sales, Operations, and other functional areas', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Collaborate with BI and IT teams to establish data elements, basic MDM practices, and relationships in multiple data models', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Participate in information gathering sessions to validate business cases and define requirements', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Self-starter, results-oriented, able to execute multiple projects simultaneously', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Strong interpersonal and problem solving skills', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Excellent verbal and written communication skills', 'REQUIRED JOB COMPETENCIES', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Share knowledge and collaborate with PBI Analyst/Desktop users across the business', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Functional abilities using SQL queries, SQL Server Reporting Services (SSRS) and SQL Server Integration Services (SSIS), SQL Server Analysis Services (SSAS)', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Familiarity with project management methodologies such as waterfall, agile, etc.', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience using Jira or similar project/agile project tracking software', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Intermediate understanding of business and system analysis methods and process such as use case development, brainstorming, etc. is helpful', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Conduct unit testing and troubleshooting', ""Hollstadt Consulting is a Twin Cities-based management and technology consulting firm dedicated to placing professionals at engagements where they will excel.\xa0When you work with us, you'll work with a refreshingly real company led and staffed by seasoned experts who are also down-to-earth, good people. We're committed to treating you with respect and helping you achieve your career aspirations."", 'Hollstadt Overview', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0In-depth understanding of database management systems, online analytical processing (OLAP) and ETL (Extract, transform, load) framework', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Create and maintain tools to store and analyze data']",Mid-Senior level,Contract,Information Technology,Management Consulting,2021-03-24 13:05:10
Data Engineer,Snap Inc.,"Los Angeles, CA",2 days ago,48 applicants,"['', ' Experience working with a MapReduce or an MPP system ', ' Work closely with IT, HRIS, & People Analytics to influence the build the HR data infrastructure ', ' Hands on experience with Google BigQuery ', 'What you’ll do: Work closely with IT, HRIS, & People Analytics to influence the build the HR data infrastructure  Collaboration cross-functionally with other engineers and product managers to develop the best data practices in line with things we value.  Grow the technical expertise of the People Tech Team  Help cultivate agile methodologies and foster a culture of balanced tech health  Ensure accuracy and timeliness of data is maintained in the data layer  Actively participate in Data Governance efforts. ', 'Knowledge, Skills & Abilities: Experience cultivating a strong engineering culture in an agile environment  Proven engineering background with previous experience developing large scale systems for data engineering, including processing, storage, quality and management  In-depth knowledge of agile software processes, data-driven development, reliability, and responsible experimentation  Proven track record of collaborating with other teams on relevant topics such as data science, and software systems design ', ' Proven engineering background with previous experience developing large scale systems for data engineering, including processing, storage, quality and management ', ' Hands on experience with Google BigQuery  Experience in version control systems such as Git  Data architecture experience  Experience in ETL / Data application development  Experience working with a MapReduce or an MPP system ', ' Experience in version control systems such as Git ', ' Data architecture experience ', 'What you’ll do:', ' Ensure accuracy and timeliness of data is maintained in the data layer ', 'Minimum Qualifications: ', 'What you’ll do: Work closely with IT, HRIS, & People Analytics to influence the build the HR data infrastructure  Collaboration cross-functionally with other engineers and product managers to develop the best data practices in line with things we value.  Grow the technical expertise of the People Tech Team  Help cultivate agile methodologies and foster a culture of balanced tech health  Ensure accuracy and timeliness of data is maintained in the data layer  Actively participate in Data Governance efforts. Knowledge, Skills & Abilities: Experience cultivating a strong engineering culture in an agile environment  Proven engineering background with previous experience developing large scale systems for data engineering, including processing, storage, quality and management  In-depth knowledge of agile software processes, data-driven development, reliability, and responsible experimentation  Proven track record of collaborating with other teams on relevant topics such as data science, and software systems design ', ' Help cultivate agile methodologies and foster a culture of balanced tech health ', ' 5+ years development experience in at least one object-oriented language (Python, Perl, Java, etc.) ', ' Experience cultivating a strong engineering culture in an agile environment  Proven engineering background with previous experience developing large scale systems for data engineering, including processing, storage, quality and management  In-depth knowledge of agile software processes, data-driven development, reliability, and responsible experimentation  Proven track record of collaborating with other teams on relevant topics such as data science, and software systems design ', ' In-depth knowledge of agile software processes, data-driven development, reliability, and responsible experimentation ', 'Preferred Qualifications: Hands on experience with Google BigQuery  Experience in version control systems such as Git  Data architecture experience  Experience in ETL / Data application development  Experience working with a MapReduce or an MPP system ', 'accommodations-ext@snap.com', ' 5+ years experience in SQL or similar languages. ', ' 2+ years of technical people management experience ', ' Proven track record of collaborating with other teams on relevant topics such as data science, and software systems design ', 'Minimum Qualifications:  BS/BA degree in Computer Science, Math, Physics, or related field, or equivalent experience in a relevant field  5+ years working as a Software Engineering or in relevant field  5+ years experience in SQL or similar languages.  5+ years development experience in at least one object-oriented language (Python, Perl, Java, etc.)  2+ years of technical people management experience ', ' BS/BA degree in Computer Science, Math, Physics, or related field, or equivalent experience in a relevant field ', ' Grow the technical expertise of the People Tech Team ', ' 5+ years working as a Software Engineering or in relevant field ', ' Experience cultivating a strong engineering culture in an agile environment ', ' Collaboration cross-functionally with other engineers and product managers to develop the best data practices in line with things we value. ', ' Actively participate in Data Governance efforts. ', 'Knowledge, Skills & Abilities:', ' Experience in ETL / Data application development ', ' BS/BA degree in Computer Science, Math, Physics, or related field, or equivalent experience in a relevant field  5+ years working as a Software Engineering or in relevant field  5+ years experience in SQL or similar languages.  5+ years development experience in at least one object-oriented language (Python, Perl, Java, etc.)  2+ years of technical people management experience ', 'Preferred Qualifications:', ' Work closely with IT, HRIS, & People Analytics to influence the build the HR data infrastructure  Collaboration cross-functionally with other engineers and product managers to develop the best data practices in line with things we value.  Grow the technical expertise of the People Tech Team  Help cultivate agile methodologies and foster a culture of balanced tech health  Ensure accuracy and timeliness of data is maintained in the data layer  Actively participate in Data Governance efforts. ']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer,CBTS,United States,1 day ago,69 applicants,"['', 'Performance tuning and documentation of new data assets - 15%', 'Build and maintain data pipelines & ETL for data modeling, mining and analytical processes - 15%', 'Fully Remote', 'Develop and maintain front end user interfaces for data input/output, optimized for user experience - 15%', 'Work Environment', 'Familiarity with Agile methodologies', 'Demonstrated proficiency working with APIs, SDKs and Databases', '2-4 years working knowledge Strong understanding of Relational Data Structures', ""Employ a variety of programming languages and tools (i.e. Python, C++, IDE's, etc.) to integrate systems/data packages and infrastructure - 20%"", ""Four years of College resulting in a Bachelor's Degree or equivalentAWS Data Analytics or Azure Data Engineer -- Preferred"", 'Strong understanding of APIs and programmatic data extraction', 'Employ data extraction and modeling methodologies to generate consumable information for reporting. - 20%', 'Demonstrated proficiency working with APIs, SDKs and DatabasesMicrosoft SQL Server 2012, 2016, 2019PowerBI & Visual Studio Experience RequiredMySQL working knowledge2-4 years working knowledge of Python, C#, Node.js, JavascriptExperience with ETL Tools required (e.g. Talend, Pentaho, SSIS) Required2-4 years working knowledge Strong understanding of Relational Data StructuresExperience with non-relational data structures PreferredFamiliarity with Agile methodologiesStrong understanding of APIs and programmatic data extractionFamiliarity with scientific & statistical methodologies -- preferred', 'AWS Data Analytics or Azure Data Engineer -- Preferred', 'Collaborate with Analytics & Insights team members on agile approaches to achieving project goals - 15%', '2-4 years working knowledge of Python, C#, Node.js, Javascript', ""Four years of College resulting in a Bachelor's Degree or equivalent"", 'Experience with non-relational data structures Preferred', 'Experience:', 'Education and Certifications:', 'No Supervisory Responsibility', '3 to 5 years of work experience', 'Work Environment:', 'Experience', 'Experience with ETL Tools required (e.g. Talend, Pentaho, SSIS) Required', 'MySQL working knowledge', ""Employ a variety of programming languages and tools (i.e. Python, C++, IDE's, etc.) to integrate systems/data packages and infrastructure - 20%Employ data extraction and modeling methodologies to generate consumable information for reporting. - 20%Build and maintain data pipelines & ETL for data modeling, mining and analytical processes - 15%Develop and maintain front end user interfaces for data input/output, optimized for user experience - 15%Performance tuning and documentation of new data assets - 15%Collaborate with Analytics & Insights team members on agile approaches to achieving project goals - 15%"", '3+ plus years in corporate Data Engineering/Development', 'Microsoft SQL Server 2012, 2016, 2019', '3 to 5 years of work experience3+ plus years in corporate Data Engineering/Development', 'PowerBI & Visual Studio Experience Required', 'ESSENTIAL FUNCTIONS', 'Education and Certifications', 'Special Knowledge, Skills and Abilities', 'Special Knowledge, Skills and Abilities:', 'The primary role of the Data Engineer II is to support the extraction & transformation of data objects between systems across the enterprise. The role will focus on creating, delivering and maintaining ETL & Data packages across MS systems that support needs for the entire organization.', 'JOB PURPOSE:', 'JOB PURPOSE', 'Supervisory Responsibility', 'Familiarity with scientific & statistical methodologies -- preferred', 'Supervisory Responsibility:', 'ESSENTIAL FUNCTIONS:']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,"Docker, Inc","New York, NY",4 weeks ago,Be among the first 25 applicants,"['', '2+ years of relevant industry experience', 'Creating production-ready ETL scripts with Python and SQL', 'Preferred Qualifications', 'Strong verbal and written communication skills', 'Data Engineer (Remote)', 'Maintain the integrity of data within our data pipeline and warehouse', 'Experience using data collection platforms such as Segment, RudderStack, Fivetran etc. ', 'Collaborate with product managers and analysts throughout the company to deliver reliable data that powers actionable insights', ' 2+ years of relevant industry experience Familiarity with data warehousing concepts including data model design and query optimization strategies Experience using and maintaining BI visualization tools such as (but not limited to) Looker, Tableau, or Power BI.  Creating production-ready ETL scripts with Python and SQL Experience with version control systems such as Github, Gitlab, Bitbucket etc.  Experience automating business and reporting processes Experience using data analysis and/or statistics to inform decisions  Strong verbal and written communication skills ', 'Experience with version control systems such as Github, Gitlab, Bitbucket etc. ', 'Ensure quality of data and completeness of event logging across Docker codebase', 'Experience using and maintaining BI visualization tools such as (but not limited to) Looker, Tableau, or Power BI. ', 'Integrate emerging methodology, technology, and version control practices that best fit the team. ', 'Champion a data-informed mindset within our culture', 'Responsibilities', 'Transform raw event logs into higher-order tables to make existing analysis easier and new analysis possible', 'Experience using data analysis and/or statistics to inform decisions ', 'Familiarity with at least one cloud ecosystem: AWS/Azure Infrastructure/Google Cloud', 'Creating automated reporting of weekly and monthly metrics and ROI for the executive management team and board', 'Integrate data from 3rd party services via ETL tools and custom pipelines', 'Proficiency in building data pipelines leveraging Docker and orchestration frameworks such as Jenkins/Airflow/Luigi', 'Qualifications', 'Experience automating business and reporting processes', 'Proficiency working with a Data Warehouse platform (e.g. Snowflake, Redshift, BigQuery, AWS Athena, or Spectrum)', 'At least 6 months of experience with Looker and LookML ', 'Experience designing and deploying high-performance systems with reliable monitoring and logging practices', 'Develop ETL jobs and tests to process, validate, transport, collate, aggregate, and distribute data', 'BS/MS in Computer Science, Math, Physics, or other technical fields', 'Design, build and automate business metrics into self-serve dashboards via Looker ', 'Experience of working in an agile environment and using tools such as JIRA/Asana/Trello ', 'Familiarity with data warehousing concepts including data model design and query optimization strategies', ' BS/MS in Computer Science, Math, Physics, or other technical fields At least 6 months of experience with Looker and LookML  Proficiency in building data pipelines leveraging Docker and orchestration frameworks such as Jenkins/Airflow/Luigi Experience designing and deploying high-performance systems with reliable monitoring and logging practices Proficiency working with a Data Warehouse platform (e.g. Snowflake, Redshift, BigQuery, AWS Athena, or Spectrum) Experience using data collection platforms such as Segment, RudderStack, Fivetran etc.  Familiarity with at least one cloud ecosystem: AWS/Azure Infrastructure/Google Cloud Experience of working in an agile environment and using tools such as JIRA/Asana/Trello  ', ' Implement, document, oversee and evolve the Snowflake and ETL infrastructure Maintain the integrity of data within our data pipeline and warehouse Ensure quality of data and completeness of event logging across Docker codebase Integrate data from 3rd party services via ETL tools and custom pipelines Develop ETL jobs and tests to process, validate, transport, collate, aggregate, and distribute data Transform raw event logs into higher-order tables to make existing analysis easier and new analysis possible Creating automated reporting of weekly and monthly metrics and ROI for the executive management team and board Integrate emerging methodology, technology, and version control practices that best fit the team.  Design, build and automate business metrics into self-serve dashboards via Looker  Collaborate with product managers and analysts throughout the company to deliver reliable data that powers actionable insights Champion a data-informed mindset within our culture ', 'Implement, document, oversee and evolve the Snowflake and ETL infrastructure']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Levi Strauss & Co.,"San Francisco, CA",2 days ago,29 applicants,"['', 'FULL TIME/PART TIME', 'Working knowledge of Git hub /Git Toolkit.', 'Strong knowledge of data pipeline and workflow management tools (Airflow).', 'Strong experience with at least one object-oriented/functional languages: Python, Java, Scala, etc.', 'What We Are Looking For', 'Experience with data modeling, data warehousing, KPI generation etc.', 'Hands-on experience in Data engineering Spectrum, for e.g. developing metadata based framework based solutions for Ingestion, Processing etc., building Data Lake/Lake House solutions.', 'Experience in at-scale infrastructure design, build and deployment with a focus on distributed systems.Build and maintain architecture patterns for data processing, workflow definitions, and system to system integrations using BigData and Cloud technologies.Evaluate and translate technical design to workable technical solutions/code and technical specifications at par with industry standards. Drive creation of re-usable artifacts.Actively scan and evaluate relevant new technologies which drive standardization and reduction of complexity within the enterprise.Contribute to and promote good software engineering practices across the team.Works on the collaborative Enterprise team and deliveries work products that support a digital transformation by leading assigned product and solution teams.Embody the values and passions that characterize Levi Strauss & Co., with empathy to engage with colleagues from a wide range of backgrounds.Performs technical design reviews and code reviews, to include cloud-based architecture and integrations.Communicate clearly and effectively to technical and non-technical audiences.', 'LOCATION', 'Works on the collaborative Enterprise team and deliveries work products that support a digital transformation by leading assigned product and solution teams.', 'Embody the values and passions that characterize Levi Strauss & Co., with empathy to engage with colleagues from a wide range of backgrounds.', 'Experience with at least one cloud provider solution (AWS, GCP, Azure)', 'Working in a collaborative environment and interacting effectively with technical and non-technical team members equally well.', 'Strong experience with MPP Database systems like Teradata, Oracle or SAP Hana.', 'Proficient in documenting use cases, requirements and functional specifications.', 'Performs technical design reviews and code reviews, to include cloud-based architecture and integrations.', 'Strong experience working with Big Data technologies ex: Spark, Spark SQL, Pyspark Structured Streaming, Kafka.', 'Current LS&Co Employees, apply via your Workday account.', 'Experience designing various consumption patterns on top of data lake/lake house to cater to different personas within organization.', 'Strong experience working with a variety of relational SQL and NoSQL databases and ability to choose a database based on the need.', 'Responsibilities', 'Experience in at-scale infrastructure design, build and deployment with a focus on distributed systems.', 'Minimum of 7 years in a hands-on technical role as an engineer and/or Data engineering role.', ' EOE M/F/Disability/Vets ', 'Contribute to and promote good software engineering practices across the team.', 'Build and maintain architecture patterns for data processing, workflow definitions, and system to system integrations using BigData and Cloud technologies.', 'Evaluate and translate technical design to workable technical solutions/code and technical specifications at par with industry standards. Drive creation of re-usable artifacts.', ' Current LS&Co Employees, apply via your Workday account.', 'Communicate clearly and effectively to technical and non-technical audiences.', ""Bachelor's Degree or higher in Computer Science or related discipline"", 'Actively scan and evaluate relevant new technologies which drive standardization and reduction of complexity within the enterprise.', 'Job Description', 'Ability to effectively prioritize and execute tasks in a high-pressure environment is crucial.', ""Bachelor's Degree or higher in Computer Science or related disciplineMinimum of 7 years in a hands-on technical role as an engineer and/or Data engineering role.Experience with at least one cloud provider solution (AWS, GCP, Azure)Strong experience working with Big Data technologies ex: Spark, Spark SQL, Pyspark Structured Streaming, Kafka.Experience designing various consumption patterns on top of data lake/lake house to cater to different personas within organization.Strong experience with at least one object-oriented/functional languages: Python, Java, Scala, etc.Strong knowledge of data pipeline and workflow management tools (Airflow).Hands-on experience in Data engineering Spectrum, for e.g. developing metadata based framework based solutions for Ingestion, Processing etc., building Data Lake/Lake House solutions.Strong experience working with a variety of relational SQL and NoSQL databases and ability to choose a database based on the need.Working knowledge of Git hub /Git Toolkit.Strong experience with MPP Database systems like Teradata, Oracle or SAP Hana.Experience with data modeling, data warehousing, KPI generation etc.Proficient in documenting use cases, requirements and functional specifications.Ability to effectively prioritize and execute tasks in a high-pressure environment is crucial.Working in a collaborative environment and interacting effectively with technical and non-technical team members equally well.""]",Not Applicable,Full-time,Information Technology,Apparel & Fashion,2021-03-24 13:05:10
Data Engineer,GSK,"Collegeville, PA",5 days ago,Be among the first 25 applicants,"['', 'Continuously looking for opportunities to learn, build skills and share learning.', 'Site Name:', 'Provide Tier 3 support/administration of DNA Nexus bioinformatics system', 'Why you?', 'Understanding of diverse ‘omic data types including RNA-Seq, DNA-Seq, Chip-Seq, WES, WGS, ATAC-seq, microbiome, proteomic, metabolomic data etc. from different sources.', 'Preferred Qualifications', 'Support DCS and broader R&D in self-service/exploratory efforts', 'Assist the design, build, test and maintenance of data acquisition and processing pipelines including but not limited to the creation/maintenance of appropriate artifacts', 'This position requires a Computer Science, Bioinformatics, or related degree; 5+ years’ experience in data movement, data wrangling and delivery of data or analytics pipelines', 'Support the use and growth of the Data Engineering DataOps environment, influence strategy and roadmap for the curation toolset, work with R&D and Tech to prioritize enhancements', 'Why GSK?', 'Experience with data movement and management in the Pharmaceutical industry or related scientific fields.', 'Background and experience in LIMS systems, Next Generation Sequencing (NGS) workflows, Cloud computing and HPC systems.', 'Committed to delivering high quality results, overcoming challenges, focusing on what matters, execution.', 'Sustaining energy and well-being.', 'Partner with data teams to implement pipeline designs to support R&D strategy and conceptual data flowsPartner with the metadata leads to translate conceptual data models into physical database/tables optimized for data analytics in RDIP using established environments and toolsAssist the design, build, test and maintenance of data acquisition and processing pipelines including but not limited to the creation/maintenance of appropriate artifactsEnsure the preservation of data integrity from source to target state including but not limited to the acquisition of appropriate metadata and the incorporation of appropriate QC checks into the pipelinesSupport the use and growth of the Data Engineering DataOps environment, influence strategy and roadmap for the curation toolset, work with R&D and Tech to prioritize enhancementsProvide Tier 3 support for production pipelinesSupport DCS and broader R&D in self-service/exploratory effortsInfluence vendor roadmaps, work with R&D and Tech to prioritize DataOps enhancements, and onboard these tools or enhancementsEnsure the quality consistency and availability of guidance documentation of end users of the tools to support high quality outputsExtend current pipelines to support clinical biomarkersAssess GxP readiness as it related to the upstream data pipelines and develop a plan for addressing any gapsProvide Tier 3 support/administration of DNA Nexus bioinformatics system', 'Posted Date:', 'Experience with open source software, bioinformatics tools and languages such as SQL, R, Perl, Python, Java, and ETL tools.', 'Building strong relationships and collaboration, honest and open conversations.', 'Ensure the quality consistency and availability of guidance documentation of end users of the tools to support high quality outputs', 'Operating at pace and agile decision-making – using evidence and applying judgement to balance pace, rigour and risk.', 'Provide Tier 3 support for production pipelines', 'Responsibilities', 'Familiarity with data mining, machine learning and artificial intelligence techniques', 'Budgeting and cost-consciousness', 'Assess GxP readiness as it related to the upstream data pipelines and develop a plan for addressing any gaps', 'Strong interpersonal skills and effective communication of complex concepts to stake holders with wide range of expertise.', 'Partner with data teams to implement pipeline designs to support R&D strategy and conceptual data flows', 'Experience with Big Data technologies, Cloud-based offerings (Microsoft Azure, GCP, AWS, etc), and corresponding tools.', 'This position requires a Computer Science, Bioinformatics, or related degree; 5+ years’ experience in data movement, data wrangling and delivery of data or analytics pipelinesExperience implementing and maintaining, data or analytic pipelines.Experience with Big Data technologies, Cloud-based offerings (Microsoft Azure, GCP, AWS, etc), and corresponding tools.Experience with open source software, bioinformatics tools and languages such as SQL, R, Perl, Python, Java, and ETL tools.', 'Experience with the core components of the Hadoop stack including HDFS and Apache Spark, ideally a Cloudera based stack', 'Extend current pipelines to support clinical biomarkers', 'Influence vendor roadmaps, work with R&D and Tech to prioritize DataOps enhancements, and onboard these tools or enhancements', 'Proven ability to contribute to development projects.', 'Experience implementing and maintaining, data or analytic pipelines.', 'Partner with the metadata leads to translate conceptual data models into physical database/tables optimized for data analytics in RDIP using established environments and tools', 'Experience with data movement and management in the Pharmaceutical industry or related scientific fields.Experience with the core components of the Hadoop stack including HDFS and Apache Spark, ideally a Cloudera based stackBackground and experience in LIMS systems, Next Generation Sequencing (NGS) workflows, Cloud computing and HPC systems.Understanding of diverse ‘omic data types including RNA-Seq, DNA-Seq, Chip-Seq, WES, WGS, ATAC-seq, microbiome, proteomic, metabolomic data etc. from different sources.Familiarity with data mining, machine learning and artificial intelligence techniquesProven ability to contribute to development projects.Strong interpersonal skills and effective communication of complex concepts to stake holders with wide range of expertise.', 'Basic Qualifications', 'Operating at pace and agile decision-making – using evidence and applying judgement to balance pace, rigour and risk.Committed to delivering high quality results, overcoming challenges, focusing on what matters, execution.Continuously looking for opportunities to learn, build skills and share learning.Sustaining energy and well-being.Building strong relationships and collaboration, honest and open conversations.Budgeting and cost-consciousness', 'Ensure the preservation of data integrity from source to target state including but not limited to the acquisition of appropriate metadata and the incorporation of appropriate QC checks into the pipelines', 'As GSK Focuses On Our Values And Expectations And a Culture Of Innovation, Performance, And Trust, The Successful Candidate Will Demonstrate The Following Capabilities']",Not Applicable,Full-time,Strategy/Planning,Pharmaceuticals,2021-03-24 13:05:10
Data Engineer,Park West Asset Management LLC,"Larkspur, CA",2 days ago,93 applicants,"['', 'Interest in financial markets', '3-5\xa0years of applicable work experience', 'Build customized data visualizations that incorporate internal and external data', 'Skills & Qualifications', 'Organize the ingestion of third party data sources', 'Self-motivated, detail-oriented, and a creative thinker', 'Experience with data visualization tools', 'Process, clean, and verify the integrity of data used for analysis', 'Perform ad-hoc analysis and present results in a clear manner', 'Bachelor’s degree3-5\xa0years of applicable work experienceExperience working with and analyzing large data sets Technical proficiency extracting information from relational databasesKnowledge of AWSExperience with a programming language, e.g. PythonExperience with Microsoft ExcelExperience with data visualization toolsExcellent analytical and problem solving skillsEffective written and verbal communicatorInterest in financial marketsSelf-motivated, detail-oriented, and a creative thinker', 'Experience with a programming language, e.g. Python', 'Bachelor’s degree', 'Knowledge of AWS', '\xa0', 'Experience with Microsoft Excel', 'This Data Engineer will join one other Data Analyst on a 13 person investment team and work to expand our firm’s data sourcing, ingestion, analysis, and reporting capabilities.\xa0This role provides a front seat to the intersection of alternative data, investment research, and financial markets.', 'Organize the ingestion of third party data sourcesProcess, clean, and verify the integrity of data used for analysisPerform ad-hoc analysis and present results in a clear mannerBuild customized data visualizations that incorporate internal and external data', 'Responsibilities', 'Serious Candidates: Please attach a supplemental file describing or showing an example from a recent data project you worked on.\xa0Please highlight where the data was located, the type of data used, tools needed to extract the data, manipulations made to the data, and the takeaways from your analysis.', 'Excellent analytical and problem solving skills', 'Technical proficiency extracting information from relational databases', 'Experience working with and analyzing large data sets ', 'Effective written and verbal communicator']",Entry level,Full-time,Information Technology,N/A,2021-03-24 13:05:10
Data Engineer,Regions Bank,"Birmingham, AL",3 days ago,Be among the first 25 applicants,"['', 'Experience developing solutions for the financial services industry', 'Location Details', 'Experience building data solutions at scaleExperience designing and building relational data structures in multiple environmentsExperience with Airflow, Argo, Luigi, or similar orchestration toolExperience with DevOps principals and CI/CD.Experience with Docker and KubernetesExperience with No-SQL databases such as HBase, Cassandra, or MongoDBExperience with streaming technologies such as Kafka, Flink, or Spark StreamingExperience working with Hadoop ecosystem building Data Assets at an enterprise scaleProven record of accomplishment of delivering operational Data solutions including Report and Model Ready Data AssetsSignificant experience working with senior executives in the use of data, reporting and visualizations to support strategic and operational decision makingStrong ability to transform and integrate complex data from multiple sources into accessible, understandable, and usable data assets and frameworksStrong background in synthesizing data and analytics in a large (fortune 500), complex, and highly regulated environmentStrong technical background including database and business intelligence skills', 'Builds data pipelines to collect and arrange data and manage data storage in Regions’ big data environment', 'Position Type', 'Experience with Docker and Kubernetes', 'Primary Responsibilities', 'Supports any team members in the development of such information delivery and aid in the automation of data products', 'Provides consultation to all areas of the organization that plan to use data to make decisions', 'Experience working with Hadoop ecosystem building Data Assets at an enterprise scale', 'Prior banking or financial Services experience', 'Experience with streaming technologies such as Kafka, Flink, or Spark Streaming', 'Two (2) years of data-oriented experience including experience managing data and analytics resources', 'Partners with Regions Technology partners to Design, Build, and Maintain the data-based structures and systems in support of Data and Analytics and Data Product use cases', 'Proven record of accomplishment of delivering operational Data solutions including Report and Model Ready Data Assets', 'Skills And Competencies', 'Requirements', 'Prior banking or financial Services experienceExperience developing solutions for the financial services industryBackground in Big Data Engineering and Advanced Data Analytics ', 'Background in Big Data Engineering and Advanced Data Analytics ', 'Strong ability to transform and integrate complex data from multiple sources into accessible, understandable, and usable data assets and frameworks', 'Strong background in synthesizing data and analytics in a large (fortune 500), complex, and highly regulated environment', 'Location:', 'Strong technical background including database and business intelligence skills', 'Experience designing and building relational data structures in multiple environments', 'Builds robust, testable programs for moving, transforming, and loading data using big data tools such as Spark.', 'Experience with Airflow, Argo, Luigi, or similar orchestration tool', 'Experience with DevOps principals and CI/CD.', 'Experience with No-SQL databases such as HBase, Cassandra, or MongoDB', 'Experience building data solutions at scale', 'Significant experience working with senior executives in the use of data, reporting and visualizations to support strategic and operational decision making', ""Bachelor's degree in Business or a technical related fieldTwo (2) years of data-oriented experience including experience managing data and analytics resources"", 'Preferences', ""Bachelor's degree in Business or a technical related field"", 'Partners with Regions Technology partners to Design, Build, and Maintain the data-based structures and systems in support of Data and Analytics and Data Product use casesBuilds data pipelines to collect and arrange data and manage data storage in Regions’ big data environmentBuilds robust, testable programs for moving, transforming, and loading data using big data tools such as Spark.Ensures data is prepared, arranged and ready for each defined business use caseProvides consultation to all areas of the organization that plan to use data to make decisionsSupports any team members in the development of such information delivery and aid in the automation of data products', 'Job Description', 'Ensures data is prepared, arranged and ready for each defined business use case']",Not Applicable,Full-time,Information Technology,Banking,2021-03-24 13:05:10
Data Engineer,the LEGO Group,"Enfield, CT",4 weeks ago,163 applicants,"['', ' Facilitate operational efficiency, simplicity, and openness through automation and implementation of the data tools and systems to advise real-time decisions. Own end-to-end data acquisition and organization across multiple sources with the objective of structuring and maintaining data integrity. Use common global tools and data available to advise business insights and recommendations', ' Play a key role in driving eCommerce results by leading test & learn initiatives that lead to more effective shopper targeting and sales conversion. Execute relevant data analysis for business-as-usual and specific LEGO campaigns (e.g. holidays, product releases, etc.) across a range of eCommerce partner sites or other areas and Initiatives', ' Provide the platform of data and tools for partners, including analysts across the business', ' Responsible for the software creation process (from development through maintenance). Proactively investigate new opportunities to add business value and support the evolution of our digital business through a dynamic technical infrastructure in close collaboration with multi-functional partners', ' Proven expertise in data manipulation, test design, predictive analytics, segmentation, and related areas. Ability to synthesize results, develop stories, and make recommendations to business and technical leader. Good interpersonal communication, organization, and time-management skills', ' Experience building complex data analyses by bring to bear languages including, R, Python, Java, Alteryx, etc. Experience with data visualization tools and crafting data visualization concepts. (Tableau, DOMO, Qlik, etc)', ' Ability to travel domestically and internationally ~5%', ' 3+ years of data science or data architecture experience with in-depth knowledge in sophisticated analytics. Experience working with enterprise reporting systems (SAP), large data sets and writing sophisticated SQL queries', ' Experience in AGILE environment, which focuses on iterative development and testing', ' Improved end to end performance understanding across the organization through dashboards, scorecards and clear insights', ' Measure sales & marketing stimuli (A/B and multi-variant) test results in collaboration with other teams to build more robust campaigns. Define and uphold standards and processes for analysis & reporting, including evolution of important metric dashboards', ' Familiarity with marketing and business metrics; experienced in the design, development and preparation of business presentations and reports', ' Confirmed LEGO® Leadership Playground behaviors – Bravery, Curiosity, and Focus', ' Drive better decisions through easier access to data']",Entry level,Full-time,Other,Consumer Goods,2021-03-24 13:05:10
Data Engineer,TuneCore,"Dongan Hills, NY",6 days ago,Be among the first 25 applicants,"['', 'Be highly proficient in SQL & Python with an ability to work with large and highly complex datasetsHave a thorough understanding of building data pipelines for RDBMS systems and various cloud-based data sources.Be comfortable working in a complex data environment involving in-depth data discovery, exploration and troubleshooting.Be able to apply their skills in ingenious ways to solve new and existing problems.Be open to exploring and learning new technologies on the go.Have the desire to lead and own the data engineering component of our new enterprise cloud-based data architecture.Be comfortable working in a cross-functional responsibility.Ensure adequate and thorough documentation of all existing and new processes.', 'Ensure adequate and thorough documentation of all existing and new processes.', 'Be highly proficient in SQL & Python with an ability to work with large and highly complex datasets', 'Be comfortable working in a complex data environment involving in-depth data discovery, exploration and troubleshooting.', 'Have a thorough understanding of building data pipelines for RDBMS systems and various cloud-based data sources.', 'Candidates Should', 'Be able to apply their skills in ingenious ways to solve new and existing problems.', 'Be open to exploring and learning new technologies on the go.', 'Job Description', 'Be comfortable working in a cross-functional responsibility.', 'Have the desire to lead and own the data engineering component of our new enterprise cloud-based data architecture.']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer,Amazon Web Services (AWS),"Herndon, VA",2 weeks ago,42 applicants,"['', ' 1+ years of experience with Data Architecture and Design', ' Meets/exceeds Amazon’s leadership principles requirements for this role', 'Company', ' Collaborate with Business Intelligence Engineering team members, engineering stakeholders, partner technical teams, and business stakeholders, to gather business and functional requirements, and translate these requirements into a robust, scalable, and operable data infrastructure that works well within the overall AWS data architecture, and leads to improved engineering decisions.', ' Develop a deep understanding of our vast data sources, and provide continuous recommendations for use to solve specific business problems.', ' 1+ years of experience in preparing data for direct use in visualization tools, such as Salesforce, Tableau, or Amazon QuickSight', 'Preferred Qualifications', ' 2+ years of experience with data warehouse technical architecture, infrastructure components, and extract, transform, load (ETL) procedure', ' Be self-driven, detail-oriented, and show ability to deliver on ambiguous projects with incomplete or dirty data', ' Experience working with data scientists on research and machine learning problems', ' Take ownership of data reliability by, among other things, performing deep-dives to find root causes of potential data anomalies, and taking subsequent action to address these anomalies.', ' 2+ years of experience with Python or other relevant scripting language', ' Bachelor’s Degree in Computer Science, Information Systems, Data Analytics, or related technical/engineering field', 'Description', ' Develop a deep understanding and awareness of operational data from the AWS fleet, and build mechanisms for retrieving and aggregating such data for use by downstream business intelligence solutions.', ' Bachelor’s Degree in Computer Science, Information Systems, Data Analytics, or related technical/engineering field 3+ years Structured Query Language (SQL) experience 2+ years of experience with Python or other relevant scripting language 2+ years of experience with data warehouse technical architecture, infrastructure components, and extract, transform, load (ETL) procedure 1+ years of experience with the AWS tech stack – Glue, Redshift, EMR, S3, EC2, and Lambda will be used regularly in this role', ' 3+ years Structured Query Language (SQL) experience', ' Expert-level knowledge of SQL', 'Responsibilities', ' Experience in documenting technical/data systems for technical and business leaders', ' Collaborate with Business Intelligence Engineering team members, engineering stakeholders, partner technical teams, and business stakeholders, to gather business and functional requirements, and translate these requirements into a robust, scalable, and operable data infrastructure that works well within the overall AWS data architecture, and leads to improved engineering decisions. Develop a deep understanding and awareness of operational data from the AWS fleet, and build mechanisms for retrieving and aggregating such data for use by downstream business intelligence solutions. Develop a deep understanding of our vast data sources, and provide continuous recommendations for use to solve specific business problems. Take ownership of data reliability by, among other things, performing deep-dives to find root causes of potential data anomalies, and taking subsequent action to address these anomalies. Continuously optimize the performance of data queries, and address extract, transform, load (ETL) procedures. Insist on the highest standards by recognizing and adopting best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation.', 'DESCRIPTION', ' Insist on the highest standards by recognizing and adopting best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation.', ' Proficient in Scala/Spark/Hadoop', ' 1+ years of experience with Data Architecture and Design 1+ years of experience in preparing data for direct use in visualization tools, such as Salesforce, Tableau, or Amazon QuickSight Expert-level knowledge of SQL Proficient in Scala/Spark/Hadoop Experience in documenting technical/data systems for technical and business leaders Experience working with data scientists on research and machine learning problems Be self-driven, detail-oriented, and show ability to deliver on ambiguous projects with incomplete or dirty data Meets/exceeds Amazon’s leadership principles requirements for this role Meets/exceeds Amazon’s functional/technical depth and complexity for this role', ' Meets/exceeds Amazon’s functional/technical depth and complexity for this role', 'Basic Qualifications', ' 1+ years of experience with the AWS tech stack – Glue, Redshift, EMR, S3, EC2, and Lambda will be used regularly in this role', ' Continuously optimize the performance of data queries, and address extract, transform, load (ETL) procedures.']",Not Applicable,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Data Engineer,Porsche Cars North America,"Atlanta, GA",1 week ago,81 applicants,"['', 'Skills', 'Tasks', ' Knowledge of best practices and IT operations in an always-up, always-available services ', 'Education:', ' Analyzes and profiles large and complex data sets to discover and/or validate data quality issues and business rules ', '  5+ years professional data engineering experience focused on batch and real time data pipelines using Spark, Python, SQL, Java   8+ Years of hands-on design and development experience in data space : data processing / data transformation using ETL tools, data warehouse (data modeling, programming), RDBMS   Experience with creating API’s.   Fundamental experience with leveraging AWS for Analytics   Exposure in Microsoft SSIS and SQL Server   Working knowledge of MPP systems or Snowflake a plus   Experience using JIRA and Agile Project Management software   Experience with code repository solutions   Able to work in a global, multicultural environment   Knowledge of best practices and IT operations in an always-up, always-available services   Critical thinking/problem solving   Experience in the Automotive and/or Financial Industries is a plus   German language capability a plus  ', ' 8+ Years of hands-on design and development experience in data space : data processing / data transformation using ETL tools, data warehouse (data modeling, programming), RDBMS ', ' Experience with creating API’s. ', ' Able to work in a global, multicultural environment ', "" Actively researches philosophical and technological changes in data management and pursue an understanding of the organization's business plans and strategies. Evaluates the value of these trends to the organization and presents the information to management and other team members. "", ' Experience using JIRA and Agile Project Management software ', ' Resolve technical and user issues ', ""  Design, develop and maintain reliable automated data solutions based on the identification, collection and evaluation of business requirements. Including but not limited to data models, database objects, stored procedures and views   Developing new and enhancing existing data processing (Data Ingest, Data Transformation, Data Store, Data Management, Data Quality ) components   Conduct system monitoring across cloud environments   Analyzes and profiles large and complex data sets to discover and/or validate data quality issues and business rules   Resolve technical and user issues   Automate installation, configuration, backup, monitoring and alerting processes in Snowflake   Drive collaboration across a global, multicultural, multi-company team   Implementation of RESTful API’s supporting system integrations   Actively researches philosophical and technological changes in data management and pursue an understanding of the organization's business plans and strategies. Evaluates the value of these trends to the organization and presents the information to management and other team members.   Facilitates the development of data-related policies, processes, procedures and standards  "", ' Experience with code repository solutions ', 'Exclusive look behind the scenes', ' Automate installation, configuration, backup, monitoring and alerting processes in Snowflake ', ' Experience in the Automotive and/or Financial Industries is a plus ', ' Exposure in Microsoft SSIS and SQL Server ', ' Facilitates the development of data-related policies, processes, procedures and standards ', ' Working knowledge of MPP systems or Snowflake a plus ', 'Responsibilities', ' Fundamental experience with leveraging AWS for Analytics ', ' Conduct system monitoring across cloud environments ', 'Qualifications', ' Implementation of RESTful API’s supporting system integrations ', ' Drive collaboration across a global, multicultural, multi-company team ', ' German language capability a plus ', ' Design, develop and maintain reliable automated data solutions based on the identification, collection and evaluation of business requirements. Including but not limited to data models, database objects, stored procedures and views ', ' Critical thinking/problem solving ', ' Developing new and enhancing existing data processing (Data Ingest, Data Transformation, Data Store, Data Management, Data Quality ) components ', ' 5+ years professional data engineering experience focused on batch and real time data pipelines using Spark, Python, SQL, Java ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,The Walt Disney Company,"Santa Monica, CA",2 weeks ago,168 applicants,"['', 'Good understanding of SQL Engines and able to conduct advanced performance tuning', 'Develop and maintain Dashboards/reports using Tableau and Looker', '1+ years of experience with workflow management tools (Airflow, Oozie, Azkaban, UC4)', 'Partner with technical and non-technical colleagues to understand data and reporting requirements.Work with Engineering teams to collect required data from internal and external systems.Design table structures and define ETL strategy to build performant Data solutions that are reliable and scalable in a fast growing data ecosystem.Develop Data Quality checks for source and target data sets. Develop UAT plans and conduct QA.Develop and maintain ETL routines using ETL and orchestration tools such as Airflow, Luigi and Jenkins.Document and publish Metadata and table designs to facilitate data adoption.Perform ad hoc analysis as necessary.Perform SQL and ETL tuning as necessary.Develop and maintain Dashboards/reports using Tableau and LookerCoordinate and resolve escalated production support incidents in Tier 2 support rotationCreate runbooks and actionable alerts as part of the development process', 'Coordinate and resolve escalated production support incidents in Tier 2 support rotation', 'Excellent conceptual and analytical reasoning competencies.', 'What To Bring', 'Ability to think strategically, analyze and interpret market and consumer information.', '2+ years of relevant Professional experience.1+ years work experience implementing and reporting on business key performance indicators in data warehousing environments. Strong understanding of data modeling principles including Dimensional modeling, data normalization principles etc.1 + years experience using analytic SQL, working with traditional relational databases and/or distributed systems such as Hadoop / Hive, BigQuery, Redshift.Experience programming languages (e.g. Python, R, bash) preferred.1+ years of experience with workflow management tools (Airflow, Oozie, Azkaban, UC4)Good understanding of SQL Engines and able to conduct advanced performance tuningExperience with Hadoop (or similar) Ecosystem (MapReduce, Yarn, HDFS, Hive, Spark, Presto, Pig, HBase)Familiarity with data exploration / data visualization tools like Tableau, Looker, Chartio, etc.Ability to think strategically, analyze and interpret market and consumer information.Strong communication skills – written and verbal presentations.Excellent conceptual and analytical reasoning competencies.Degree in an analytical field such as economics, mathematics, or computer science is desired.Comfortable working in a fast-paced and highly collaborative environment.Process-oriented with phenomenal documentation skills', 'Develop Data Quality checks for source and target data sets. Develop UAT plans and conduct QA.', 'Document and publish Metadata and table designs to facilitate data adoption.', 'Create runbooks and actionable alerts as part of the development process', ""What You'll Do"", 'Work with Engineering teams to collect required data from internal and external systems.', 'Process-oriented with phenomenal documentation skills', 'Degree in an analytical field such as economics, mathematics, or computer science is desired.', '1+ years work experience implementing and reporting on business key performance indicators in data warehousing environments. Strong understanding of data modeling principles including Dimensional modeling, data normalization principles etc.', 'Design table structures and define ETL strategy to build performant Data solutions that are reliable and scalable in a fast growing data ecosystem.', 'Perform ad hoc analysis as necessary.', 'Partner with technical and non-technical colleagues to understand data and reporting requirements.', 'Experience with Hadoop (or similar) Ecosystem (MapReduce, Yarn, HDFS, Hive, Spark, Presto, Pig, HBase)', 'Summary', 'Perform SQL and ETL tuning as necessary.', '1 + years experience using analytic SQL, working with traditional relational databases and/or distributed systems such as Hadoop / Hive, BigQuery, Redshift.', 'Develop and maintain ETL routines using ETL and orchestration tools such as Airflow, Luigi and Jenkins.', 'Strong communication skills – written and verbal presentations.', 'Familiarity with data exploration / data visualization tools like Tableau, Looker, Chartio, etc.', 'Experience programming languages (e.g. Python, R, bash) preferred.', '2+ years of relevant Professional experience.', 'Comfortable working in a fast-paced and highly collaborative environment.']",Mid-Senior level,Full-time,Business Development,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer,Welcome,"Remote, OR",24 hours ago,26 applicants,"['', 'high-end virtual events', 'Be part of an inclusive, collaborative team that embraces healthy discussions and supports each other in their goals', ' Responsibilities ', 'Competitive salary and equity compensationGreat medical, dental, and vision insurance optionsFitness and internet reimbursementsTop-of-the-line work computer$2,000 budget to build your dream home officeOpportunities to attend the most high-end virtual events in the world', 'Help define, build, and own key datasets and the quality and evolution of these datasets as use cases grow', 'Fitness and internet', 'SQL databases', 'Opportunities to attend the most high-end virtual events in the world', 'Solid understanding of SQL databases and relational database schema design', '$2,000 budget to build your dream home office', 'professional software development experience ', 'medical, dental, and vision', 'salary and equity', 'Experience with data science platforms (we use Mode)', 'Be part of an inclusive, collaborative team that embraces healthy discussions and supports each other in their goalsHelp define, build, and own key datasets and the quality and evolution of these datasets as use cases growWork closely with the rest of the engineering team, as well as other stakeholders, to understand the data needs of our customers and implement solutionsWork on the data collection pipeline across the entire stack, from the FE/BE event logging to the ETLEnsure that our business critical customer data is accurate', 'inclusive, collaborative team', 'work computer', 'Top-of-the-line work computer', 'Competitive salary and equity compensation', '3+ years of professional software development experience building high quality data solutionsExpertise building data pipelines using open source frameworks (Hadoop, Spark, etc)Experience with data science platforms (we use Mode)Expertise in one or more programming languages (ideally Ruby or Javascript)Solid understanding of SQL databases and relational database schema designExcellent written and verbal communication skills', '3+ years of professional software development experience building high quality data solutions', 'Work closely with the rest of the engineering team, as well as other stakeholders, to understand the data needs of our customers and implement solutions', 'Fitness and internet reimbursements', 'Ensure that our business critical customer data is accurate', 'Great medical, dental, and vision insurance options', 'Work on the data collection pipeline across the entire stack, from the FE/BE event logging to the ETL', 'dream home office', 'communication', 'Expertise in one or more programming languages (ideally Ruby or Javascript)', 'Expertise building data pipelines using open source frameworks (Hadoop, Spark, etc)', 'Excellent written and verbal communication skills']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer,Dyson,"Chicago, IL",4 weeks ago,Be among the first 25 applicants,"['', 'Desirable', 'Strong programming skills in languages such as Python/Java/Scala including building, testing and releasing code into productionStrong SQL skills and experience working with relational/columnar databases (e.g. SQLServer, Postgres, Oracle, Presto, Hive, BigQuery etc…)Knowledge of data modelling techniques and integration patternsPractical experience writing data analytic pipelinesExperience integrating/interfacing with REST APIs / Web ServicesExperience handling data securelyExperience with DevOps software delivery and CI/CD processesA willingness to learn and find solutions to complex problemsResilient and comfortable with high pace change.', 'Experience of designing and building real/near real time solutions using streaming technologies (e.g. Dataflow/Apache Beam, Fink, Spark Streaming etc) ', 'Hands-on experience with cloud environments (GCP & AWS preferred)', 'Experience with non-relational database solutions (e.g. Big Query, Big Table, MongoDB, Dynamo, HBase, Elasticsearch)', 'Lifestyle Benefits', 'Experience migrating from on-premise data stores to cloud solutions', 'Wellness Program', ""Building API's and apps using Python/JavaScript or an alternative language"", '…deliver conceptualized & future solutions to introduce net-new capability ', 'Responsibilities Include', 'Company paid Life Insurance and AD&D', 'Competitive Paid Time Off Benefits including Separate Holiday, Sick, and Vacation Time', 'Aligning work to both core development standards and architectural principles', 'Vision & Dental Coverage', 'Practical experience writing data analytic pipelines', '401K with up to a 4% match', 'Experience with AWS data pipeline, Azure data factory or Google Cloud Dataflow', 'Designing and building end to end Data Engineering solutions on the Google Cloud Platform', 'Pre-tax Commuter Benefits (applicable areas only)', 'Financial Benefits', 'Experience with DevOps software delivery and CI/CD processes', 'Designing and building end to end Data Engineering solutions on the Google Cloud PlatformBeing a proactive member of DevOps / Agile scrum driven team; always looking for ways to tune and optimize all aspects of work delivered on the platformAligning work to both core development standards and architectural principles', 'A willingness to learn and find solutions to complex problems', 'Multi-Level Healthcare Coverage OptionsVision & Dental CoverageCompany paid Short-Term and Long-Term Disability', 'Resilient and comfortable with high pace change.', 'Generous Dyson Product Discounts', 'Multi-Level Healthcare Coverage Options', 'About the role', 'Flexible Savings Account (FSA) and Health Savings Account (HSA)', 'About Us', 'Being a proactive member of DevOps / Agile scrum driven team; always looking for ways to tune and optimize all aspects of work delivered on the platform', 'Practical experience with traditional Big Data stacks (e.g Spark, Flink, Hbase, Flume, Impala, Hive etc)', 'Working with containerization technologies (Docker, Kubernetes etc…)', 'Experience handling data securely', 'Strong SQL skills and experience working with relational/columnar databases (e.g. SQLServer, Postgres, Oracle, Presto, Hive, BigQuery etc…)', '…evolve existing solutions to stay ahead …embed emerging solutions to capitalize on potential benefits …deliver conceptualized & future solutions to introduce net-new capability ', 'Knowledge of data modelling techniques and integration patterns', 'Company paid Short-Term and Long-Term Disability', 'Generous Child Care Leave Program', 'About you', 'Competitive Paid Time Off Benefits including Separate Holiday, Sick, and Vacation TimePre-tax Commuter Benefits (applicable areas only)Generous Child Care Leave ProgramWellness ProgramEmployee Assistance ProgramGenerous Dyson Product Discounts', '401K with up to a 4% matchCompany paid Life Insurance and AD&DFlexible Savings Account (FSA) and Health Savings Account (HSA)', 'Health Benefits', '…embed emerging solutions to capitalize on potential benefits ', 'Employee Assistance Program', 'Experience integrating/interfacing with REST APIs / Web Services', 'Benefits', 'Experience working with data warehouse solutions including extracting and processing data using a variety of programming languages, tools and techniques (e.g. SSIS, Azure Data Factory, T-SQL, PL-SQL, Talend, Matillion, Nifi, AWS Data Pipelines)', '…evolve existing solutions to stay ahead ', ""Experience migrating from on-premise data stores to cloud solutionsExperience of designing and building real/near real time solutions using streaming technologies (e.g. Dataflow/Apache Beam, Fink, Spark Streaming etc) Hands-on experience with cloud environments (GCP & AWS preferred)Building API's and apps using Python/JavaScript or an alternative languagePractical experience with traditional Big Data stacks (e.g Spark, Flink, Hbase, Flume, Impala, Hive etc)Experience with non-relational database solutions (e.g. Big Query, Big Table, MongoDB, Dynamo, HBase, Elasticsearch)Experience with AWS data pipeline, Azure data factory or Google Cloud DataflowWorking with containerization technologies (Docker, Kubernetes etc…)Experience working with data warehouse solutions including extracting and processing data using a variety of programming languages, tools and techniques (e.g. SSIS, Azure Data Factory, T-SQL, PL-SQL, Talend, Matillion, Nifi, AWS Data Pipelines)"", 'Strong programming skills in languages such as Python/Java/Scala including building, testing and releasing code into production']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,PayPal,"Scottsdale, AZ",18 hours ago,Be among the first 25 applicants,"['', 'Ability to translate complex problems into simpler termsAbility to work independently', 'Perform development of new stored procedures, integrations, views etc as part of bi-weekly agile sprints', 'Work with the DB architect to create and maintain optimal data pipeline architectureAssemble large, complex data sets that meet functional / non-functional business requirementsBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and Google ‘big data’ technologiesBuild APIs that expose the data so customers can consume and integrate with our dataEvaluate large and complex queries / stored procedures and recommend changes to optimize performanceAssist with the expansion into public cloud cost transparencyLead automation efforts leveraging innersource interaction models with other teamsAssist data analysts on our team with query development, statistical analysis, and data visualization as neededMaintain technical documentation on stored procedures and ETL logic used to managed the database', 'Build APIs that expose the data so customers can consume and integrate with our data', 'Ability to write clear and concise communications', 'Very strong foundational knowledge in Object-Oriented Design Principles, Data Structures, Algorithms, and Software Engineering', 'BS in computer science, Information systems or equivalent field, MS preferred', 'Attended technical meetings with customers of the IT efficiency product so they can integrate with us to consume data', 'Create, update, maintain documentation on stored procedures, ETL logic, db schema etc.', 'Must Have', 'Thorough understanding of RDBMS concepts and ability to write complex SQL queries, Stored procedures, Functions & triggers', 'Work with lead to define technical requirements and design for new tables, ETLs, integrations,', 'Experience with SignalFX analytics platformExperience with gimmel notebooksExperience with some visualization tool (e.g. tableau or power BI)Experience managing database upgrades / data migrationsExperience optimizing queriesBasic fundamentals of JAVAUnderstanding of Angular and/or ReactJS', 'Nice to Have', 'Experience & Education', 'Work with cross-functional teams to build and enhance automation workflows', 'Strong written and verbal communication skills', 'Understand or be keen to learn about REST API development and design', 'Experience managing database upgrades / data migrations', 'Experience building and optimizing ETLs, data pipelines, architectures and data sets', 'Proficiency with python scripting language', 'Work with lead to define technical requirements and design for new tables, ETLs, integrations,Perform development of new stored procedures, integrations, views etc as part of bi-weekly agile sprintsMaintain scheduled jobs and debug when necessaryAttended technical meetings with customers of the IT efficiency product so they can integrate with us to consume dataAttend daily status calls to track sprint progressCreate, update, maintain documentation on stored procedures, ETL logic, db schema etc.Work with cross-functional teams to build and enhance automation workflowsBuild / optimize queries needed by team as neededProvide demos and code reviews when applicableTrack data quality / integrity issues with the db and report them', 'A constant desire to grow, learn, and explore new things', 'Build / optimize queries needed by team as needed', '4-5 years working experience as a Data Engineer3+ years software development in front-end and/or back-end technologiesBS in computer science, Information systems or equivalent field, MS preferred', 'Job Description:', 'Proficient in Microsoft SQL Server 2016 & 2019 Database Development (T-SQL), Data Analysis and Support.', 'Experience writing and maintaining technical documentation', 'Prerequisite Knowledge & Proficiencies', 'Assist with the expansion into public cloud cost transparency', 'Experience with code repositories like Git/GitHub', 'Experience building and optimizing ETLs, data pipelines, architectures and data setsProficient in Microsoft SQL Server 2016 & 2019 Database Development (T-SQL), Data Analysis and Support.Thorough understanding of RDBMS concepts and ability to write complex SQL queries, Stored procedures, Functions & triggersProficiency with python scripting languageExperience with Splunk and splunk query languageExperience Google Big Query and AWS redshiftExperience with No SQL / HadoopDemonstrated experience working with large data sets and a love for working with dataUnderstand or be keen to learn about REST API development and designVery strong foundational knowledge in Object-Oriented Design Principles, Data Structures, Algorithms, and Software EngineeringWorking knowledge of web technologies (such as HTTP, HTML/DOM, JavaScript, CSS, AJAX)Experience with code repositories like Git/GitHubExperience writing and maintaining technical documentationBasic familiarity with MS Word, Excel & Visio knowledge neededA constant desire to grow, learn, and explore new things', 'Core Behavioral Competencies', '3+ years software development in front-end and/or back-end technologies', 'Maintain scheduled jobs and debug when necessary', 'Role Specific Behavioral Competencies', 'Basic fundamentals of JAVA', 'Experience with No SQL / Hadoop', 'Maintain technical documentation on stored procedures and ETL logic used to managed the database', 'Assemble large, complex data sets that meet functional / non-functional business requirements', 'Work with the DB architect to create and maintain optimal data pipeline architecture', 'Ability to work independently', 'Experience with some visualization tool (e.g. tableau or power BI)', 'Understanding of Angular and/or ReactJS', 'Core Duties', 'Demonstrated experience working with large data sets and a love for working with data', 'Experience with SignalFX analytics platform', '4-5 years working experience as a Data Engineer', 'Evaluate large and complex queries / stored procedures and recommend changes to optimize performance', 'Ability to translate complex problems into simpler terms', 'Experience Google Big Query and AWS redshift', 'Experience with Splunk and splunk query language', 'Working knowledge of web technologies (such as HTTP, HTML/DOM, JavaScript, CSS, AJAX)', 'Basic familiarity with MS Word, Excel & Visio knowledge needed', 'Track data quality / integrity issues with the db and report them', 'Examples Of Specific Job Tasks/Accountabilities', 'Strong written and verbal communication skillsAbility to write clear and concise communicationsAbility to work effectively in a global team environment', 'Ability to work effectively in a global team environment', 'Experience optimizing queries', 'Experience with gimmel notebooks', 'Position Overview', 'Attend daily status calls to track sprint progress', 'Core Competencies', 'Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and Google ‘big data’ technologies', 'Provide demos and code reviews when applicable', 'Job Description Summary:', 'Lead automation efforts leveraging innersource interaction models with other teams', 'Assist data analysts on our team with query development, statistical analysis, and data visualization as needed']",Not Applicable,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Data Engineer,PulsePoint,"New York, NY",2 days ago,78 applicants,"['', 'Graphite/Beacon - for monitoring data flows', 'Willingness to participate in 24x7 on-call rotation', 'Proficiency in Linux', 'Volunteer Time Off and Donation Matching, ongoing group volunteer opportunities', ' Design, build and maintain reliable and scalable enterprise level distributed transactional data processing systems for scaling the existing business and supporting new business initiatives Optimize jobs to utilize Kafka, Hadoop, Presto, Spark Streaming and Kubernetes resources in the most efficient way Monitor and provide transparency into data quality across systems (accuracy, consistency, completeness, etc) Increase accessibility and effectiveness of data (work with analysts, data scientists, and developers to build/deploy tools and datasets that fit their use cases) Collaborate within a small team with diverse technology backgrounds Provide mentorship and guidance to junior team members ', 'Technologies We Use', 'Optimize jobs to utilize Kafka, Hadoop, Presto, Spark Streaming and Kubernetes resources in the most efficient way', 'Strong understanding of RDBMS, SQL;', 'SQL Server - Reliable OLTP RDBMS ', 'Tool evaluation/selection/implementation', 'Gym reimbursement, local gym membership discounts', ' Installation, upkeep, maintenance and monitoring of Kafka, Hadoop, Presto, RDBMS Ingest, validate and process internal & third party data Create, maintain and monitor data flows in Hive, SQL and Presto for consistency, accuracy and lag time Maintain and enhance framework for jobs(primarily aggregate jobs in Hive)  Create different consumers for data in Kafka using Spark Streaming for near time aggregation Train Developers/Analysts on tools to pull data Tool evaluation/selection/implementation Backups/Retention/High Availability/Capacity Planning Review/Approval - DDL for database, Hive Framework jobs and Spark Streaming to make sure they meet our standards 24*7 On call rotation for Production support ', '$2,000 annual training and development budget', 'Up to $100 emergency childcare credit per year', 'BA/BS degree in Computer science or related field', 'Review/Approval - DDL for database, Hive Framework jobs and Spark Streaming to make sure they meet our standards', 'Knowledge and exposure to distributed production systems i.e Hadoop is a huge plus', 'Presto - fast parallel data warehouse and data federation layer', 'Annual company retreat', 'Kafka- distributed commit log storage ', 'Spark Streaming - Near time aggregation', '$100 work-from-home productivity stipend', ""What You'll Be Doing"", 'Increase accessibility and effectiveness of data (work with analysts, data scientists, and developers to build/deploy tools and datasets that fit their use cases)', 'Provide mentorship and guidance to junior team members', 'Comprehensive healthcare with 100%-paid medical, vision, life & disability insurance', ' BA/BS degree in Computer science or related field 5+ years of software engineering experience Fluency in Python, Experience in Scala/Java is a huge plus (Polyglot programmer preferred!) Proficiency in Linux Strong understanding of RDBMS, SQL; Passion for engineering and computer science around data Knowledge and exposure to distributed production systems i.e Hadoop is a huge plus Knowledge and exposure to Cloud migration is a plus Willingness to participate in 24x7 on-call rotation ', 'Requirements', 'Collaborate within a small team with diverse technology backgrounds', 'Docker - Packaged container image with all dependencies', ' Comprehensive healthcare with 100%-paid medical, vision, life & disability insurance 401(k) Match and free access to a financial advisor Generous paid vacation and company holidays Vacation reimbursement (we give you $500 each year to take vacation), sabbatical, pawternity leave, marriage leave, honeymoon bonus $2,000 annual training and development budget Complimentary annual memberships to One Medical (for you and your family), NY Citi Bike and SF Ford GoBike Paid parental leave and a lot of new parent perks Gym reimbursement, local gym membership discounts $100 work-from-home productivity stipend Annual company retreat Up to $100 emergency childcare credit per year Volunteer Time Off and Donation Matching, ongoing group volunteer opportunities Game Nights, meditation/mindfulness sessions, fitness and stretch classes, book club, health and wellness seminars and workshops, weekly virtual happy hours with DJ ', 'Paid parental leave and a lot of new parent perks', 'Description', 'Maintain and enhance framework for jobs(primarily aggregate jobs in Hive) ', 'Vacation reimbursement (we give you $500 each year to take vacation), sabbatical, pawternity leave, marriage leave, honeymoon bonus', 'Team Responsibilities', 'Fluency in Python, Experience in Scala/Java is a huge plus (Polyglot programmer preferred!)', 'Passion for engineering and computer science around data', 'Installation, upkeep, maintenance and monitoring of Kafka, Hadoop, Presto, RDBMS', '5+ years of software engineering experience', '401(k) Match and free access to a financial advisor', 'Ingest, validate and process internal & third party data', 'Backups/Retention/High Availability/Capacity Planning', '24*7 On call rotation for Production support', 'Design, build and maintain reliable and scalable enterprise level distributed transactional data processing systems for scaling the existing business and supporting new business initiatives', 'Complimentary annual memberships to One Medical (for you and your family), NY Citi Bike and SF Ford GoBike', 'Kubernetes - Distributed cluster resource manager', 'Impala- faster SQL layer on top of Hive', 'Knowledge and exposure to Cloud migration is a plus', 'Benefits', 'Monitor and provide transparency into data quality across systems (accuracy, consistency, completeness, etc)', 'Generous paid vacation and company holidays', 'Game Nights, meditation/mindfulness sessions, fitness and stretch classes, book club, health and wellness seminars and workshops, weekly virtual happy hours with DJ', 'Airflow - for job scheduling', 'Train Developers/Analysts on tools to pull data', 'Sqoop - Import/Export data to RDBMS', 'Create different consumers for data in Kafka using Spark Streaming for near time aggregation', 'Create, maintain and monitor data flows in Hive, SQL and Presto for consistency, accuracy and lag time', ' Airflow - for job scheduling Docker - Packaged container image with all dependencies Graphite/Beacon - for monitoring data flows Hive - SQL data warehouse layer for data in HDFS Impala- faster SQL layer on top of Hive Kafka- distributed commit log storage  Kubernetes - Distributed cluster resource manager Presto - fast parallel data warehouse and data federation layer Spark Streaming - Near time aggregation SQL Server - Reliable OLTP RDBMS  Sqoop - Import/Export data to RDBMS ', 'Hive - SQL data warehouse layer for data in HDFS']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer,STAND 8 Technology Services,"Atlanta, GA",20 hours ago,25 applicants,"['', 'Design, develop and maintain reliable automated data solutions based on the identification, collection and evaluation of business requirements. Including but not limited to data models, database objects, stored procedures and views', 'Skills', 'Developing new and enhancing existing data processing (Data Ingest, Data Transformation, Data Store, Data Management, Data Quality ) components', '5+ years professional data engineering experience focused on batch and real time data pipelines using Spark, Python, SQL, Java', 'Experience using JIRA and Agile Project Management software', 'Analyzes and profiles large and complex data sets to discover and/or validate data quality issues and business rules', 'Exposure in Microsoft SSIS and SQL Server', 'Drive collaboration across a global, multicultural, multi-company team', 'EDUCATION ', 'Conduct system monitoring across cloud environments', 'Resolve technical and user issues', 'Automate installation, configuration, backup, monitoring and alerting processes in Snowflake', 'Implementation of RESTful API’s supporting system integrations', 'Facilitates the development of data-related policies, processes, procedures and standards', ' 5+ years professional data engineering experience focused on batch and real time data pipelines using Spark, Python, SQL, Java 8+ Years of hands-on design and development experience in data space : data processing / data transformation using ETL tools, data warehouse (data modeling, programming), RDBMS Experience with creating API’s. Fundamental experience with leveraging AWS for Analytics Exposure in Microsoft SSIS and SQL Server Working knowledge of MPP systems or Snowflake a plus Experience using JIRA and Agile Project Management software Experience with code repository solutions Able to work in a global, multicultural environment Knowledge of best practices and IT operations in an always-up, always-available services Critical thinking/problem solving Experience in the Automotive and/or Financial Industries is a plus German language capability a plus', 'German language capability a plus', ""Actively researches philosophical and technological changes in data management and pursue an understanding of the organization's business plans and strategies. Evaluates the value of these trends to the organization and presents the information to management and other team members."", 'Critical thinking/problem solving', 'Experience with code repository solutions', 'Fundamental experience with leveraging AWS for Analytics', 'Responsibilities', 'EDUCATION', "" Design, develop and maintain reliable automated data solutions based on the identification, collection and evaluation of business requirements. Including but not limited to data models, database objects, stored procedures and views Developing new and enhancing existing data processing (Data Ingest, Data Transformation, Data Store, Data Management, Data Quality ) components Conduct system monitoring across cloud environments Analyzes and profiles large and complex data sets to discover and/or validate data quality issues and business rules Resolve technical and user issues Automate installation, configuration, backup, monitoring and alerting processes in Snowflake Drive collaboration across a global, multicultural, multi-company team Implementation of RESTful API’s supporting system integrations Actively researches philosophical and technological changes in data management and pursue an understanding of the organization's business plans and strategies. Evaluates the value of these trends to the organization and presents the information to management and other team members. Facilitates the development of data-related policies, processes, procedures and standards "", 'Qualifications', '8+ Years of hands-on design and development experience in data space : data processing / data transformation using ETL tools, data warehouse (data modeling, programming), RDBMS', 'Knowledge of best practices and IT operations in an always-up, always-available services', 'Working knowledge of MPP systems or Snowflake a plus', 'Experience in the Automotive and/or Financial Industries is a plus', 'Data Engineer', 'Able to work in a global, multicultural environment', 'Experience with creating API’s.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Virgin Pulse,"Remote, OR",1 day ago,Be among the first 25 applicants,"['', 'Security Competencies', ' A passion for learning technologies and applying them to the right projects will make you successful in this position. ', ' Columnar databases (Redshift, Snowflake, Firebolt, etc) ', ' Responsibilities ', ' Python programming language ', ' Enjoy actively experimenting with new technologies ', ' Python programming language  One or more data visualization / business intelligence tools (Tableau, MicroStrategy, etc.)  You are enthusiastic about working in a team and have great people skills  A passion for learning technologies and applying them to the right projects will make you successful in this position.  You understand the principles of agile software development  You can present technical concepts in a way that is easy for non-technical people to understand  You are tenacious in your investigation of issues and problems and are able to find the root cause of problems and propose solutions correct the root causes. ', ' Teach and train other team members ', ' Design and implement data models for applications, operations, or analytics ', ' In this role you will wear many hats but your skills will be especially essential in the following: ', ' Use Python to enhance and automate our existing capabilities ', ' You understand the principles of agile software development ', ' You are enthusiastic about working in a team and have great people skills ', ' Excellent verbal and written communication skills ', ' B.S./M.S. in Computer Science or equivalent technology experience ', 'Overview', ' Hands-on experience with some of the tech we use: Git, PostgreSQL, Python, Tableau ', ' Knowledge of OO programming and applications built on distributed service architecture ', ' Skills in scripting, data engineering or modeling, and business intelligence ', 'Our technology', ' Design and implement analytics reports ', ' Investigate and troubleshoot data reporting issues ', ' You can present technical concepts in a way that is easy for non-technical people to understand ', ' Have fun doing all of the above ', ' Perform QA tasks to verify the accuracy of reporting ', ' You are self-motivated, creative, and detail oriented ', 'Who are our employees? ', ' Why work here? ', ' B.S./M.S. in Computer Science or equivalent technology experience  5 or more years’ experience working directly with data and data warehouses  Hands-on experience with some of the tech we use: Git, PostgreSQL, Python, Tableau  You are self-motivated, creative, and detail oriented  Skills in scripting, data engineering or modeling, and business intelligence  Knowledge of OO programming and applications built on distributed service architecture  Enjoy actively experimenting with new technologies  Excellent verbal and written communication skills ', ' One or more data visualization / business intelligence tools (Tableau, MicroStrategy, etc.) ', ' Build files and reports that impact hundreds-of-thousands of people around the world ', 'Qualifications', 'We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to any protected class status.', ' 5 or more years’ experience working directly with data and data warehouses ', 'In a Typical Week You May', ' Use a wide variety of modern technologies, including ', ' Who is Virgin Pulse? ', ' What you bring to the team ', ' Build files and reports that impact hundreds-of-thousands of people around the world  Work on a product that changes people’s lives  Write and maintain advanced SQL queries for reporting extracts and assist more junior team members  Perform QA tasks to verify the accuracy of reporting  Investigate and troubleshoot data reporting issues  Design and implement data models for applications, operations, or analytics  Design and implement analytics reports  Use a wide variety of modern technologies, including ', ' You are tenacious in your investigation of issues and problems and are able to find the root cause of problems and propose solutions correct the root causes. ', ' Write and maintain advanced SQL queries for reporting extracts and assist more junior team members ', ' Commonly used AWS services (S3, Lambda, Redshift, EC2, etc) ', ' BI tools (Tableau, Domo, MicroStrategy) ', ' Work on a product that changes people’s lives ', 'Who You Are', ' Data streaming (Kafka, SQS/SNS queuing, etc)  Columnar databases (Redshift, Snowflake, Firebolt, etc)  Commonly used AWS services (S3, Lambda, Redshift, EC2, etc)  BI tools (Tableau, Domo, MicroStrategy) ', ' Data streaming (Kafka, SQS/SNS queuing, etc) ']",Entry level,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Data Engineer,McDonald's,"Chicago, IL",3 weeks ago,143 applicants,"['', ' Bachelor’s Degree in Computer Science, Computer Engineering, Information Systems, or similar. Or equivalent work experience. Master’s degree or equivalent work experience preferred. Experience managing applications in cloud based technologies and pipelining and familiarity with core services (AWS or GCP preferred) Experience in ETL and data warehouse technologies old and new (Oracle, SQL Server, Redshift, S3, snowflake) Familiarity with modern Machine Learning techniques Experience and desire to work in a Global delivery environment is a plus Comfortable with ambiguity and willing to proactively problem solve Strong knowledge of relational and multi-dimensional database architecture Expert at scripting ETL and application related processes in Python Experience building capabilities to support data science and analytics functions Knowledge of front-end technologies as they relate to data delivery a plus Strong verbal and written communication skills, and ability to synthesize technical information for a business audience', ' Bachelor’s Degree in Computer Science, Computer Engineering, Information Systems, or similar. Or equivalent work experience.', ' Comfortable with ambiguity and willing to proactively problem solve', ' Knowledge of front-end technologies as they relate to data delivery a plus', 'Job Description</strongOur core data science organization is looking to hire a Data Engineer. The role will work closely with data scientists and ML engineers and includes designing, architecting, and building data pipelines to support business use cases. Responsibilities also include collaborating with business leaders to translate business requirements into technical, scale-able solution.Qualifications</strong Bachelor’s Degree in Computer Science, Computer Engineering, Information Systems, or similar. Or equivalent work experience. Master’s degree or equivalent work experience preferred. Experience managing applications in cloud based technologies and pipelining and familiarity with core services (AWS or GCP preferred) Experience in ETL and data warehouse technologies old and new (Oracle, SQL Server, Redshift, S3, snowflake) Familiarity with modern Machine Learning techniques Experience and desire to work in a Global delivery environment is a plus Comfortable with ambiguity and willing to proactively problem solve Strong knowledge of relational and multi-dimensional database architecture Expert at scripting ETL and application related processes in Python Experience building capabilities to support data science and analytics functions Knowledge of front-end technologies as they relate to data delivery a plus Strong verbal and written communication skills, and ability to synthesize technical information for a business audienceAdditional Information</strongMcDonald’s is committed to providing qualified individuals with disabilities reasonable accommodations to perform the essential functions of their jobs. Additionally, if you (or another applicant of whom you are aware) require assistance accessing or reading this job posting or otherwise seek assistance in the application process, please contact recruiting.supportteam@us.mcd.comMcDonald’s provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to sex, sex stereotyping, pregnancy (including pregnancy, childbirth, and medical conditions related to pregnancy, childbirth, or breastfeeding), race, color, religion, ancestry or national origin, age, disability status, medical condition, marital status, sexual orientation, gender, gender identity, gender expression, transgender status, protected military or veteran status, citizenship status, genetic information, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.Nothing in this job posting or description should be construed as an offer or guarantee of employment.', 'Company Description', ' Experience and desire to work in a Global delivery environment is a plus', ' Experience building capabilities to support data science and analytics functions', ' Strong knowledge of relational and multi-dimensional database architecture', 'Additional Information</strongMcDonald’s is committed to providing qualified individuals with disabilities reasonable accommodations to perform the essential functions of their jobs. Additionally, if you (or another applicant of whom you are aware) require assistance accessing or reading this job posting or otherwise seek assistance in the application process, please contact recruiting.supportteam@us.mcd.comMcDonald’s provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to sex, sex stereotyping, pregnancy (including pregnancy, childbirth, and medical conditions related to pregnancy, childbirth, or breastfeeding), race, color, religion, ancestry or national origin, age, disability status, medical condition, marital status, sexual orientation, gender, gender identity, gender expression, transgender status, protected military or veteran status, citizenship status, genetic information, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.Nothing in this job posting or description should be construed as an offer or guarantee of employment.', ' Experience managing applications in cloud based technologies and pipelining and familiarity with core services (AWS or GCP preferred)', ' Master’s degree or equivalent work experience preferred.', ' Experience in ETL and data warehouse technologies old and new (Oracle, SQL Server, Redshift, S3, snowflake)', ' Strong verbal and written communication skills, and ability to synthesize technical information for a business audience', ' Expert at scripting ETL and application related processes in Python', 'Qualifications</strong Bachelor’s Degree in Computer Science, Computer Engineering, Information Systems, or similar. Or equivalent work experience. Master’s degree or equivalent work experience preferred. Experience managing applications in cloud based technologies and pipelining and familiarity with core services (AWS or GCP preferred) Experience in ETL and data warehouse technologies old and new (Oracle, SQL Server, Redshift, S3, snowflake) Familiarity with modern Machine Learning techniques Experience and desire to work in a Global delivery environment is a plus Comfortable with ambiguity and willing to proactively problem solve Strong knowledge of relational and multi-dimensional database architecture Expert at scripting ETL and application related processes in Python Experience building capabilities to support data science and analytics functions Knowledge of front-end technologies as they relate to data delivery a plus Strong verbal and written communication skills, and ability to synthesize technical information for a business audienceAdditional Information</strongMcDonald’s is committed to providing qualified individuals with disabilities reasonable accommodations to perform the essential functions of their jobs. Additionally, if you (or another applicant of whom you are aware) require assistance accessing or reading this job posting or otherwise seek assistance in the application process, please contact recruiting.supportteam@us.mcd.comMcDonald’s provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to sex, sex stereotyping, pregnancy (including pregnancy, childbirth, and medical conditions related to pregnancy, childbirth, or breastfeeding), race, color, religion, ancestry or national origin, age, disability status, medical condition, marital status, sexual orientation, gender, gender identity, gender expression, transgender status, protected military or veteran status, citizenship status, genetic information, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.Nothing in this job posting or description should be construed as an offer or guarantee of employment.', ' Familiarity with modern Machine Learning techniques']",Not Applicable,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Scientist/Engineer,Oracle,"Austin, Texas Metropolitan Area",3 weeks ago,Over 200 applicants,"['', ' Identify recurring problems and bottlenecks that might be improved through upgrades to our software product, new technologies in the analytics team infrastructure, or further research into statistical methodology.', ' Model evaluation and validation expertise, should be demonstrated as a consequence of having to do this in current work developing ML models. ', ' Efficiently execute well-defined, discrete analytic tasks based on the needs of client, as communicated by the client management team;', ' Oracle Utilities Analytics Insights (OUAI) Data Science & Analytics | Data Analyst ', ' Prior experience working in complex data environments within the energy and utility industry preferred. A successful candidate will demonstrate the following qualities:', 'Location', ' Breadth and depth across machine learning techniques: specifically Deep Learning (using TF/Keras or other frameworks), as well as other ML techniques to re-create ML equivalents of legacy rule-based classification algorithms. ', ' Be a team player and be willing to delve into the details of the product to make sure clients are seeing value. ', 'Preferred Qualifications', 'Organization', 'Other Locations', ' MS in Computer Science, Mathematics, Statistics, Physics, Economics or other STEM field; 1-3 years of professional work experience in a data analytics role involving customer-facing deliverables and internal team coordination;', 'Oracle is an Affirmative Action-Equal Employment Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability, protected veterans status, age, or any other characteristic protected by law.', ' Work with application management team, and where necessary, other members of the analytics team to efficiently execute larger-scale analytic deliverables and operationalize the results;', ' Efficiently execute well-defined, discrete analytic tasks based on the needs of client, as communicated by the client management team; Work with client management team and our customers to define and take ownership of scope, intermediate deliverables, and timelines around larger-scale analytic deliverables; Work with application management team, and where necessary, other members of the analytics team to efficiently execute larger-scale analytic deliverables and operationalize the results; Identify recurring problems and bottlenecks that might be improved through upgrades to our software product, new technologies in the analytics team infrastructure, or further research into statistical methodology.', ' Ability and willingness to be able to use OUAI’s proprietary tool to recreate or build rule-based solutions to analytics problems, mostly classification ', ' Ability to write well structured, testable Python or PySpark code to implement use cases using Machine Learning techniques in a production grade environment. ', ' A passion for and curiosity about new big data analytics technologies and methods;', ' Work with client management team and our customers to define and take ownership of scope, intermediate deliverables, and timelines around larger-scale analytic deliverables;', 'Responsibilities', 'Job Type', ' Ability to write SQL queries and capability to do exploratory analysis on structured data. ', ' Ability to work closely with other stakeholders in the client delivery organization for OUAI (Customer success and Implementation Engineering) ', 'This is a remote/office based position which may be performed anywhere in the United States except for within the state of Colorado.', ' 1-3 years of professional work experience in a data analytics role involving customer-facing deliverables and internal team coordination;', ' Prior experience framing and conducting analyses in a relational database environment (e.g. Oracle DB, MySQL, PostgreSQL, MSSQL) and through spreadsheet-based tools; Prior experience with programming languages (e.g. Python) and statistical programming languages (e.g. R, SAS, SPS, etc.) strongly preferred; A passion for and curiosity about new big data analytics technologies and methods; Strong written & oral communication skills; Prior experience working in complex data environments within the energy and utility industry preferred. A successful candidate will demonstrate the following qualities: Ability to write SQL queries and capability to do exploratory analysis on structured data.   Ability to write well structured, testable Python or PySpark code to implement use cases using Machine Learning techniques in a production grade environment.   Prior experience working as a data scientist in role that at a minimum requires creation of ""proof of concept"" data science models.   Ability and willingness to be able to use OUAI’s proprietary tool to recreate or build rule-based solutions to analytics problems, mostly classification   Breadth and depth across machine learning techniques: specifically Deep Learning (using TF/Keras or other frameworks), as well as other ML techniques to re-create ML equivalents of legacy rule-based classification algorithms.   Model evaluation and validation expertise, should be demonstrated as a consequence of having to do this in current work developing ML models.   Ability to work closely with other stakeholders in the client delivery organization for OUAI (Customer success and Implementation Engineering)   Be a team player and be willing to delve into the details of the product to make sure clients are seeing value. ', ' Prior experience with programming languages (e.g. Python) and statistical programming languages (e.g. R, SAS, SPS, etc.) strongly preferred;', ' Prior experience framing and conducting analyses in a relational database environment (e.g. Oracle DB, MySQL, PostgreSQL, MSSQL) and through spreadsheet-based tools;', ' Strong written & oral communication skills;', 'Job', ' Prior experience working as a data scientist in role that at a minimum requires creation of ""proof of concept"" data science models. ', ' MS in Computer Science, Mathematics, Statistics, Physics, Economics or other STEM field;']",Entry level,Full-time,Engineering,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Stitch Fix,"San Francisco, CA",2 days ago,39 applicants,"['', 'We are challenged, developed and have meaningful impact', 'We Get Excited About Candidates Who Have…', 'You will be proud to say that you work for Stitch Fix and will know that the work you do brings joy to our clients every day', 'About Stitch Fix', 'We are a successful, fast-growing company at the forefront of tech and fashion, redefining retail for the next generation', ' Senior IC position on the data engineering team, within our Algorithms organization, focusing on our client and marketing data infrastructure, optimization and scalability You will build and own large additions to our data engineering framework, charged with finding ways to create and improve scalable and reliable tables and central data pipelines Work in a collaborative, production-facing codebase that has close coupling with engineering systems You will build and own scalable, efficient, and well-tested data engineering solutions using Spark, Amazon S3, and a mature collection of in-house technologies. You will be involved in the day-to-day operations of the team, including maintaining and improving our current tools & scripts and supporting full-stack data scientists You will have autonomy to help shape the future of data engineering at Stitch Fix by bringing your ideas on improving and automating what we do and how we do it ', 'Work with teams of world-class data scientists and engineers on how to solve data and business problems in a scalable way', 'Be part of a team which has high visibility across the organization', 'Strong cross functional communication skills that help simplify and move complex problems forward', 'Strong prioritization skills with business impact in mind', 'You’re Excited About This Opportunity Because You Will...', 'You will be involved in the day-to-day operations of the team, including maintaining and improving our current tools & scripts and supporting full-stack data scientists', 'You will build and own scalable, efficient, and well-tested data engineering solutions using Spark, Amazon S3, and a mature collection of in-house technologies.', 'We offer competitive compensation packages and comprehensive health benefits', 'About The Role', 'We take what we do seriously. We don’t take ourselves seriously', ""Please Review Stitch Fix's Recruiting Privacy Policy Here"", ' 5+ years of fully independent project experience with significant contributions. Experience in building out scalable data engineering capabilities Exceptional coding and design skills in Python and SQL Experience in Spark optimization and an understanding of data storage with Amazon S3 Experience in working autonomously and taking ownership of projects. Ability to think globally, devising and building solutions to meet many needs rather than completing individual projects or tasks Strong prioritization skills with business impact in mind Strong cross functional communication skills that help simplify and move complex problems forward ', 'Senior IC position on the data engineering team, within our Algorithms organization, focusing on our client and marketing data infrastructure, optimization and scalability', 'Contribute to a culture of technical collaboration and scalable development ', 'We love solving problems, thinking creatively and trying new things', 'We are committed to our clients and connected through our vision of “Transforming the way people find what they love”', 'Contribute ideas and direct the team’s investment to impactful directions', 'We are a technologically and data-driven business', 'We believe in autonomy & taking initiative', 'Experience in building out scalable data engineering capabilities', 'Experience in working autonomously and taking ownership of projects.', '5+ years of fully independent project experience with significant contributions.', 'Work in a collaborative, production-facing codebase that has close coupling with engineering systems', 'Ability to think globally, devising and building solutions to meet many needs rather than completing individual projects or tasks', 'About The Team', 'YOU’LL LOVE WORKING AT STITCH FIX BECAUSE…', 'We have a smart, experienced leadership team that wants to do it right & is open to new ideas', ' Work with teams of world-class data scientists and engineers on how to solve data and business problems in a scalable way Be part of a team which has high visibility across the organization Contribute ideas and direct the team’s investment to impactful directions Contribute to a culture of technical collaboration and scalable development  ', 'You will build and own large additions to our data engineering framework, charged with finding ways to create and improve scalable and reliable tables and central data pipelines', 'Exceptional coding and design skills in Python and SQL', 'We are a group of bright, kind and goal oriented people. You can be your authentic self here, and are empowered to encourage others to do the same!', 'Experience in Spark optimization and an understanding of data storage with Amazon S3', ' We are a group of bright, kind and goal oriented people. You can be your authentic self here, and are empowered to encourage others to do the same! We are a successful, fast-growing company at the forefront of tech and fashion, redefining retail for the next generation We are a technologically and data-driven business We are committed to our clients and connected through our vision of “Transforming the way people find what they love” We love solving problems, thinking creatively and trying new things We believe in autonomy & taking initiative We are challenged, developed and have meaningful impact We take what we do seriously. We don’t take ourselves seriously We have a smart, experienced leadership team that wants to do it right & is open to new ideas We offer competitive compensation packages and comprehensive health benefits You will be proud to say that you work for Stitch Fix and will know that the work you do brings joy to our clients every day ', 'You will have autonomy to help shape the future of data engineering at Stitch Fix by bringing your ideas on improving and automating what we do and how we do it']",Mid-Senior level,Full-time,Research,Apparel & Fashion,2021-03-24 13:05:10
Senior Data Engineer,Vanguard,"Malvern, PA",2 days ago,Be among the first 25 applicants,"['', 'Inclusion Statement', 'Requirements', 'About Vanguard']",Mid-Senior level,Full-time,Research,Financial Services,2021-03-24 13:05:10
Data Engineer,Veritas Partners,"Cockeysville, MD",,N/A,"['', '\xa0Desired Skills and Experience:\xa0', 'Veritas Partners has an immediate need for a full-time Data Engineer to join our team in the greater Baltimore, MD area!', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Ability to tackle complex problems with creative solutions when the path may not be clear.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Familiar with the modern cloud data platforms (Azure preferred but AWS is ok). Experience in some of the following is preferred:', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Algorithmic concepts from at least one information-centric discipline (e.g. statistics, machine learning, information processing, natural language processing, etc.)', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Gathering data requirements from stakeholders', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience with Object-Oriented Programming Languages such as Python, JavaScript, Java, C#, etc.\xa0', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Mindful of practices, procedures and legal guidelines that govern PII and other sensitive data', 'Also Desired: ', 'This role is a hybrid of software development and analytics, so either of the following background will be relevant: a Software Developer with experience in data interpretation and desire to focus on the analytics space, or a Business Analyst with experience in software development, especially focused on process automation.', '\xa0', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Data Governance concepts', 'o\xa0\xa0Other preferred technologies include: Advanced T-SQL (SQL Server), REST API concepts, PowerShell, SSIS, PowerBI', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Fundamental understanding of Data Lakes, Data Catalogs, Hardware and Network Topology', 'This is an excellent opportunity for you to join a rapidly growing organization who utilizes the latest Microsoft/Big Data Technology!', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Evaluating data collection for accuracy', 'o\xa0\xa0Azure Data Lake (Analytics and Storage), Data Warehouse / Synapse Analytics / Amazon Redshift, Data Factory, Logic Apps, Data Bricks', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience in database design, architecture and warehousing', 'Data Engineer', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience creating ETL/ELT processes to move data between internal and external sources using APIs, ETL.\xa0', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience using data reporting and visualization tools (e.g. Cognos, SSRS, Qlik, Data Studio, Power BI, Tableau, etc.)']",Mid-Senior level,Full-time,Engineering,Broadcast Media,2021-03-24 13:05:10
Data Engineer,IBM,"New York, NY",2 weeks ago,Be among the first 25 applicants,"['', 'software engineering', 'Key Responsibilities', '3+ years design & implementation experience with distributed applications', 'DevOps', 'Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores. ', 'data process ', 'About Business Unit', 'Develop code using Python, Scala, R languages', 'Experience in software engineering with object-oriented design, coding and testing patterns on large-scale data infrastructures', 'Preferred Technical And Professional Expertise', 'Experience with stream-processing systems: Storm, Spark-Streaming, etc.Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.', 'Demonstrated knowledge of software development tools and methodologies', 'Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical requirements and support their data infrastructure needs.', 'Experience with big data tools: Hadoop, Spark, Kafka, etc.', 'stream', 'Use DevOps best practices such as continuous integration, continuous delivery in the production implementation. ', 'Familiar with big data solutions with experience on Hadoop based technologies such as MapReduce, Hive MongoDB or Cassandra.', 'Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical requirements and support their data infrastructure needs.Provide the ability to work within agile development methodology and collaborate effectively with multi-disciplinary teamsBuild modern enterprise solutions which support scaling, development, test and data quality evaluation big data solutions based on the requirements. Understand data architecture, build large-scale data processing systems supporting data transformation, data structures, metadata, dependency and workload management. and optimizes data flow. Have experience on big data process including collecting, parsing, manipulating, managing, analyzing and visualizing large sets of data to turn information into insights using multiple platforms. Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores. Have expertise in data persistence solutions, experience with the latest (NoSQL) database technologies, and experience with building complex SQL queries using various (NoSQL or RDBMS) databases such as MongoDB or DB2Experience in software engineering with object-oriented design, coding and testing patterns on large-scale data infrastructuresUse DevOps best practices such as continuous integration, continuous delivery in the production implementation. ', 'Experience with relational SQL and NoSQL databases, including Postgres and Cassandra', 'Experience with big data tools: Hadoop, Spark, Kafka, etc.Familiar with big data solutions with experience on Hadoop based technologies such as MapReduce, Hive MongoDB or Cassandra.Experience with stream-processing systems: Storm, Spark-Streaming, etc.Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.Knowledge of cloud technologies such as Kubernetes, Cloud Foundry, PaaS, and IaaS (SoftLayer)', 'Develop code using Python, Scala, R languagesExperience with relational SQL and NoSQL databases, including Postgres and Cassandra3+ years design & implementation experience with distributed applications3+ years of working experience in database architectures and data pipeline developmentDemonstrated knowledge of software development tools and methodologiesComputer Science with software engineering and Math background desired', 'Build modern enterprise solutions which support scaling, development, test and data quality evaluation big data solutions based on the requirements. ', 'About IBM', 'Provide the ability to work within agile development methodology and collaborate effectively with multi-disciplinary teams', 'Knowledge of cloud technologies such as Kubernetes, Cloud Foundry, PaaS, and IaaS (SoftLayer)', 'Have expertise in data persistence solutions, experience with the latest (NoSQL) database technologies, and experience with building complex SQL queries using various (NoSQL or RDBMS) databases such as MongoDB or DB2', 'persistence', 'Have experience on big data process including collecting, parsing, manipulating, managing, analyzing and visualizing large sets of data to turn information into insights using multiple platforms. ', 'Understand data architecture, build large-scale data processing systems supporting data transformation, data structures, metadata, dependency and workload management. and optimizes data flow. ', '3+ years of working experience in database architectures and data pipeline development', 'Computer Science with software engineering and Math background desired']",Not Applicable,Full-time,Sales,Computer Hardware,2021-03-24 13:05:10
Data Engineer,StackPath,"United, LA",6 days ago,27 applicants,"['', 'Build algorithms and prototypes.', 'Prepare data for prescriptive and predictive modeling.', 'Explore ways to enhance data quality and reliability.', 'Improving data quality and efficiency.', 'Essential Duties And Responsibilities', 'Conduct complex data analysis and report on results.', 'Hands-on experience with SQL and NOSQL database design', 'About The Role', 'Knowledge of programming languages (e.g. Go, Java, Python)', 'Model events, notification and alerts.', 'Degree in Computer Science, IT, or similar field; a master’s is a plus', 'StackPath is an Equal Opportunity Employer. EOE/AA M/F/D/V', 'Desired Skills And Experience', 'Analyze and interpret trends and patterns related to the data.', 'Strong numerical and analytical skills', 'About StackPath', 'Transform raw data to actional data from various sources.', 'Develop analytical tools and programs.', 'Data engineering certification is a plus', 'Identify opportunities for data acquisition.', 'Analyzing raw data.', 'Collaborate with data scientists and architects on several projects.', 'Developing and maintaining datasets.', 'Model queries and build templates for querying large data sets.', 'This Job Description Is Not Intended To Be All-inclusive.', 'Technical expertise with data models, data mining, and segmentation techniques', 'Previous experience as a data engineer or in a similar role', 'Build optimal data systems and pipelines.', 'Analyzing raw data.Developing and maintaining datasets.Improving data quality and efficiency.Model queries and build templates for querying large data sets.Model events, notification and alerts.Build optimal data systems and pipelines.Transform raw data to actional data from various sources.Analyze and interpret trends and patterns related to the data.Conduct complex data analysis and report on results.Prepare data for prescriptive and predictive modeling.Build algorithms and prototypes.Explore ways to enhance data quality and reliability.Identify opportunities for data acquisition.Develop analytical tools and programs.Collaborate with data scientists and architects on several projects.', 'Previous experience as a data engineer or in a similar roleTechnical expertise with data models, data mining, and segmentation techniquesKnowledge of programming languages (e.g. Go, Java, Python)Hands-on experience with SQL and NOSQL database designStrong numerical and analytical skillsDegree in Computer Science, IT, or similar field; a master’s is a plusData engineering certification is a plus']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer (AWS),KeHE Distributors,"Naperville, IL",,N/A,"['', 'Minimum Requirements, Qualifications, Additional Skills, Aptitude:', 'Drive innovation and efficiency through new approaches', 'Develop, construct, test and maintain optimal data pipeline/ETL architecturesWork closely within the team to prepare data for predictive and prescriptive modelingOptimize AWS data delivery infrastructure for greater scalabilityUtilize SQL as well as big data tools and frameworks to optimize data acquisition and preparation from enterprise data lake and data warehouseWork with Enterprise Cloud Architecture teams to strive for greater functionality in our data systemsDevelop architecture required to return data to data warehouse for front-end product utilizationCurate data models in the data warehouse to be used by front-end advanced analytics designersProvide production level code reviews for the teamHelp design, maintain and implement quality assurance and testing approachesDeploy scripts and architectures to production via Jenkins', 'Desire to stay up to date with current technologies and best practices for data management and data science', 'The Data Engineer will assist in the design and implementation of a modern cloud data architecture that will enable KeHE to continue pushing the limits in the advanced analytics space. This role will work in close conjunction with the Data Science and Enterprise Data teams while also closely collaborating with other departments in the organization to construct scalable solutions that leverage both internal and external data sources. This person will possess a wide range of skills such as; creating reliable pipelines, sources and integrates the data, design and build optimized data delivery solutions. We are looking for an experienced data professional who will integrate traditional and emerging technologies to unlock greater efficiency and scalability of the data.', 'Optimize AWS data delivery infrastructure for greater scalability', 'This position will eventually be on-site in our Naperville, Illinois office.  No visa sponsorship available for this opportunity.', 'Essential Functions:', 'Work closely within the team to prepare data for predictive and prescriptive modeling', 'Primary Responsibilities:', 'Ability to work in a team environment that promotes collaboration', 'Curate data models in the data warehouse to be used by front-end advanced analytics designers', 'Bachelor’s Degree in Computer Science, Mathematics, Engineering, Management Information Systems or related field', 'Provide production level code reviews for the team', 'Deploy scripts and architectures to production via Jenkins', 'Help design, maintain and implement quality assurance and testing approaches', 'Develop architecture required to return data to data warehouse for front-end product utilization', '1-3 years of experience building data pipelines within the AWS ecosystem', 'Develop, construct, test and maintain optimal data pipeline/ETL architectures', 'Proficient programing experience using Python, R or similar language with experience building production level code', 'Utilize SQL as well as big data tools and frameworks to optimize data acquisition and preparation from enterprise data lake and data warehouse', 'Advanced SQL and data design concepts', 'Good people, working with good people, for our common good.', 'KeHE-a natural, organic, specialty and fresh food distributor-is all about ""good"" and is growing, so there\'s never been a more exciting time to join our team. If you\'re enthusiastic about working in an environment with a people-first culture and an organization committed to good living, good food and good service, we\'d love to talk to you!', 'Sound good?', 'Proficient working with Jenkins and deploying to production via Jenkin’s jobs', 'Bachelor’s Degree in Computer Science, Mathematics, Engineering, Management Information Systems or related field1-3 years of experience building data pipelines within the AWS ecosystem1-3 years of experience designing and implementing data warehouse solutionsAdvanced SQL and data design conceptsProficient programing experience using Python, R or similar language with experience building production level codeProficient working with Jenkins and deploying to production via Jenkin’s jobsDesire to stay up to date with current technologies and best practices for data management and data scienceDrive innovation and efficiency through new approachesAbility to work in a team environment that promotes collaboration', 'Work with Enterprise Cloud Architecture teams to strive for greater functionality in our data systems', '1-3 years of experience designing and implementing data warehouse solutions']",Associate,Full-time,Information Technology,Food & Beverages,2021-03-24 13:05:10
Data Engineer,US Tech Solutions,"Philadelphia, PA",2 days ago,105 applicants,"['', 'Min BS degree (economics, statistics, mathematics) with knowledge of computer programming.Knowledge of cloud platforms and common architectures: AWS, Google¬Strong Familiarity with Big Data technologies and architectures: Hadoop, Spark, Kafka, etc.Good Programming Skills: Python, Java, Scala, R, SQLExperience with containers and scalable computing platforms: Docker (ECS), Mesos, KubernetesDemonstrable ability to write, optimize and troubleshoot complex SQL queries to retrieve and analyze data from databases such as Oracle, MS SQL Server, MySQL and/or PostgreSQL.Hands-on working knowledge of SQL, relational databases, and data warehouse architectures.1+ year of experience in any of the following next gen tools and technologies: Hadoop, HIVE, Pentaho (preferred), Mapreduce, Python, Scala, R, Spark, Tableau.\xa0', 'Location: Philadelphia, PA 19103', 'Candidate should have the following:', 'Knowledge of cloud platforms and common architectures: AWS, Google¬', 'Strong Familiarity with Big Data technologies and architectures: Hadoop, Spark, Kafka, etc.', 'Hands-on working knowledge of SQL, relational databases, and data warehouse architectures.', 'Data Engineer', 'Duration: 3 Months', 'Experience with containers and scalable computing platforms: Docker (ECS), Mesos, Kubernetes', 'Demonstrable ability to write, optimize and troubleshoot complex SQL queries to retrieve and analyze data from databases such as Oracle, MS SQL Server, MySQL and/or PostgreSQL.', 'Min BS degree (economics, statistics, mathematics) with knowledge of computer programming.', 'Good Programming Skills: Python, Java, Scala, R, SQL', '1+ year of experience in any of the following next gen tools and technologies: Hadoop, HIVE, Pentaho (preferred), Mapreduce, Python, Scala, R, Spark, Tableau.\xa0']",Mid-Senior level,Contract,Information Technology,Capital Markets,2021-03-24 13:05:10
Data Engineer,Cognizant,"Plano, TX",20 hours ago,Be among the first 25 applicants,"[""Please note, this role is not able to offer visa transfer or sponsorship now or in the future*Practice - AIA - Artificial Intelligence and AnalyticsAbout AI & Analytics: Artificial intelligence (AI) and the data it collects and analyzes will soon sit at the core of all intelligent, human-centric businesses. By decoding customer needs, preferences, and behaviors, our clients can understand exactly what services, products, and experiences their consumers need. Within AI & Analytics, we work to design the future—a future in which trial-and-error business decisions have been replaced by informed choices and data-supported strategies.By applying AI and data science, we help leading companies to prototype, refine, validate, and scale their AI and analytics products and delivery models. Cognizant’s AIA practice takes insights that are buried in data, and provides businesses a clear way to transform how they source, interpret and consume their information. Our clients need flexible data structures and a streamlined data architecture that quickly turns data resources into informative, meaningful intelligence.Roles And Responsibilities Data Engineer is responsible for creating and supporting data processing solutions in cloud for our clients. In this role you will use Open Source technologies like Spark , Scala/Python/Java in AWS cloud environment to design and develop data processing applications to provide our clients with faster insight into their business. You will develop, test and deploy the solutions for warehousing systems. In addition, you will be responsible for resolving any issues with the systems as well as creating technical documentation outlining the design, troubleshooting and repair steps. After completing a comprehensive training program, you will join our AI&A practice as an Data Engineer supporting an enterprise client. Participate in Agile ceremonies like standup , grooming and retrospectives Design and develop solutions , which includes Preparation of design documents , unit testing , code version control and other relevant operational duties. Prepare codes for all modules according to require specification. Monitor all production issues and inquiries and provide efficient resolution Evaluate all functional requirements and map documents and perform troubleshoot on all development processes. Collaborate with application groups to prepare effective solutions for all programs. Documents all technical specifications and associate project deliverables. Employee Status : Full Time EmployeeShift : Day JobTravel : NoJob Posting : Mar 23 2021About CognizantCognizant (Nasdaq-100: CTSH) is one of the world's leading professional services companies, transforming clients' business, operating and technology models for the digital era. Our unique industry-based, consultative approach helps clients envision, build and run more innovative and efficient businesses. Headquartered in the U.S., Cognizant is ranked 194 on the Fortune 500 and is consistently listed among the most admired companies in the world. Learn how Cognizant helps clients lead with digital at www.cognizant.com or follow us @USJobsCognizant.Cognizant is recognized as a Military Friendly Employer and is a coalition member of the Veteran Jobs Mission. Our Cognizant Veterans Network assists Veterans in building and growing a career at Cognizant that allows them to leverage the leadership, loyalty, integrity, and commitment to excellence instilled in them through participation in military service.Cognizant is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected Veteran status, age, or any other characteristic protected by law.If you have a disability that requires a reasonable accommodation to search for a job opening or submit an application, please email CareersNA2@cognizant.com with your request and contact information."", 'After completing a comprehensive training program, you will join our AI&A practice as an Data Engineer supporting an enterprise client.', 'About AI & Analytics: Artificial intelligence (AI) and the data it collects and analyzes will soon sit at the core of all intelligent, human-centric businesses. By decoding customer needs, preferences, and behaviors, our clients can understand exactly what services, products, and experiences their consumers need. Within AI & Analytics, we work to design the future—a future in which trial-and-error business decisions have been replaced by informed choices and data-supported strategies.', ""Cognizant (NASDAQ: CTSH) is a leading provider of information technology, consulting, and business process outsourcing services, dedicated to helping the world's leading companies build stronger businesses. Headquartered in Teaneck, New Jersey (U.S.). Cognizant is a member of the NASDAQ-100, the S&P 500, the Forbes Global 1000, and the Fortune 500 and we are among the top performing and fastest growing companies in the world."", ' Data Engineer is responsible for creating and supporting data processing solutions in cloud for our clients. In this role you will use Open Source technologies like Spark , Scala/Python/Java in AWS cloud environment to design and develop data processing applications to provide our clients with faster insight into their business. You will develop, test and deploy the solutions for warehousing systems. In addition, you will be responsible for resolving any issues with the systems as well as creating technical documentation outlining the design, troubleshooting and repair steps. After completing a comprehensive training program, you will join our AI&A practice as an Data Engineer supporting an enterprise client. Participate in Agile ceremonies like standup , grooming and retrospectives Design and develop solutions , which includes Preparation of design documents , unit testing , code version control and other relevant operational duties. Prepare codes for all modules according to require specification. Monitor all production issues and inquiries and provide efficient resolution Evaluate all functional requirements and map documents and perform troubleshoot on all development processes. Collaborate with application groups to prepare effective solutions for all programs. Documents all technical specifications and associate project deliverables. ', 'Collaborate with application groups to prepare effective solutions for all programs.', 'About Cognizant', 'Participate in Agile ceremonies like standup , grooming and retrospectives', 'Design and develop solutions , which includes Preparation of design documents , unit testing , code version control and other relevant operational duties.', 'Monitor all production issues and inquiries and provide efficient resolution', 'Practice - AIA - Artificial Intelligence and Analytics', 'By applying AI and data science, we help leading companies to prototype, refine, validate, and scale their AI and analytics products and delivery models. Cognizant’s AIA practice takes insights that are buried in data, and provides businesses a clear way to transform how they source, interpret and consume their information. Our clients need flexible data structures and a streamlined data architecture that quickly turns data resources into informative, meaningful intelligence.', 'Documents all technical specifications and associate project deliverables.', 'Prepare codes for all modules according to require specification.', 'Evaluate all functional requirements and map documents and perform troubleshoot on all development processes.', 'Data Engineer is responsible for creating and supporting data processing solutions in cloud for our clients. In this role you will use Open Source technologies like Spark , Scala/Python/Java in AWS cloud environment to design and develop data processing applications to provide our clients with faster insight into their business. You will develop, test and deploy the solutions for warehousing systems. In addition, you will be responsible for resolving any issues with the systems as well as creating technical documentation outlining the design, troubleshooting and repair steps.', 'Roles And Responsibilities']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
"Data Engineer, Users and Products",Google,"Boulder, CO",1 day ago,Be among the first 25 applicants,"['', "" Bachelor's degree in Computer Science, related technical field or equivalent practical experience. Experience with one general purpose programming language (e.g., Java, C/C++, Python).  Experience in data processing using traditional and distributed systems (e.g., Hadoop, Spark, Dataflow, Airflow). Experience designing data models and data warehouses and using SQL and NoSQL database management systems. "", 'Excellent communication, organizational, and analytical skills.', 'Advanced degree in engineering or technical/scientific field of study. ', 'Experience in large scale distributed data processing.', 'Note: Disclosure as required by sb19-085 (8-5-20) of the minimum salary compensation for this role when being hired into our offices in Colorado.', 'Experience with Unix or GNU/Linux systems.', ""Bachelor's degree in Computer Science, related technical field or equivalent practical experience."", 'Create extract, transform, and load (ETLs) and reporting systems for new data using a variety of traditional as well as large-scale distributed data systems.', 'Experience designing data models and data warehouses and using SQL and NoSQL database management systems.', 'Design, develop and support data pipelines, warehouses and reporting systems to solve business operations, users and product problems.', 'About The Job', 'Additional Information', 'Collaborate and influence Users and Products stakeholders and support engineers to ensure our data infrastructure meets constantly evolving requirements.', 'Mountain View, CA, USA; Boulder, CO, USA', 'Experience with one general purpose programming language (e.g., Java, C/C++, Python). ', 'Responsibilities', 'Write and review technical documents, including design, development, and revision documents.', 'Experience in data processing using traditional and distributed systems (e.g., Hadoop, Spark, Dataflow, Airflow).', ' Design, develop and support data pipelines, warehouses and reporting systems to solve business operations, users and product problems. Create extract, transform, and load (ETLs) and reporting systems for new data using a variety of traditional as well as large-scale distributed data systems. Collaborate and influence Users and Products stakeholders and support engineers to ensure our data infrastructure meets constantly evolving requirements. Work closely with analysts to productionize various statistical and machine learning models using data processing pipelines. Write and review technical documents, including design, development, and revision documents. ', 'Experience designing data models and data warehouses and with non-relational data storage systems (NoSQL and distributed database management systems).', 'Work closely with analysts to productionize various statistical and machine learning models using data processing pipelines.', ' Advanced degree in engineering or technical/scientific field of study.  Experience designing data models and data warehouses and with non-relational data storage systems (NoSQL and distributed database management systems). Experience writing and maintaining ETLs which operate on a variety of structured and unstructured sources. Experience in large scale distributed data processing. Experience with Unix or GNU/Linux systems. Excellent communication, organizational, and analytical skills. ', 'Experience writing and maintaining ETLs which operate on a variety of structured and unstructured sources.']",Not Applicable,Full-time,Information Technology,Information Services,2021-03-24 13:05:10
Data Engineer,SECU Credit Union,"Linthicum Heights, MD",2 days ago,Be among the first 25 applicants,"['', 'Demonstrated experience with self-directed work on large and complex problems.', 'Bachelor’s degree or greater in any of the following or similar areas: Data Science, Computer Science, Management Information Science, etc. Equivalent professional experience considered in lieu of a degree.', 'Implement proper data governance procedures with a strong focus on data cleanliness to support key decision making throughout the organization. ', 'Strong interpersonal and communication skills.', 'SECU is an Equal Opportunity Employer', 'Basic understanding of statistical techniques and concepts.', 'Experience communicating among multiple stakeholders and balancing multiple projects.\xa0', 'Design, develop and maintain data structures and pipelines to facilitate the development of reports, visualizations, data sets and analytical models used by analysts, data scientists and other business stakeholders across the organization.Play an important role in the continued evolution of SECU’s modern data architecture. By collaborating with other data engineers, data scientists, analysts, technical consultants and other business users, you will\xa0ensure that data sets are of high integrity while in proper alignment with business needs.Implement proper data governance procedures with a strong focus on data cleanliness to support key decision making throughout the organization. Integrate automated data quality assurance mechanisms into orchestrated data pipelines using modern frameworks and performs root-cause analysis as needed to uncover data irregularities.Provide SQL, Python and other programming-specific technical guidance to other engineers and analysts to support continued learning and development throughout the organization. Support corporate research by designing and executing complex ad hoc queries as needed.Utilize version control systems such as Git as part of a CI/CD code pipeline, which includes data ingestion, transformation, orchestration, anomaly detection and metadata management.Develop a detailed understanding of the table structure of all critical databases and pipelines, serving as a liaison during key planning phases, which include technical and non-technical stakeholders. Be largely self-directed and will provide extensive communication and documentation in support of their work.Demonstrated experience with self-directed work on large and complex problems.', '**Occasional in-person meetings and commitments throughout the year are required**', 'What We’re Looking For.', '4+ years of data-centric programming experience as a software/data engineer, including at least one year of experience implementing data pipelines using modern tools.', 'Experience with modern data warehouse frameworks, tools and architectures.', 'Innovative, creative and forward thinking; solutions-driven.', 'We can’t wait to get to know you!', '\xa0', 'Design, develop and maintain data structures and pipelines to facilitate the development of reports, visualizations, data sets and analytical models used by analysts, data scientists and other business stakeholders across the organization.', 'Utilize version control systems such as Git as part of a CI/CD code pipeline, which includes data ingestion, transformation, orchestration, anomaly detection and metadata management.', 'Bachelor’s degree or greater in any of the following or similar areas: Data Science, Computer Science, Management Information Science, etc. Equivalent professional experience considered in lieu of a degree.4+ years of data-centric programming experience as a software/data engineer, including at least one year of experience implementing data pipelines using modern tools.Software/data engineering experience with Python.Advanced SQL skills.Experience with modern data warehouse frameworks, tools and architectures.Basic understanding of statistical techniques and concepts.Innovative, creative and forward thinking; solutions-driven.Strong interpersonal and communication skills.Ability to work alone or in teams as appropriate.Experience working in a highly collaborative, team environment.Experience communicating among multiple stakeholders and balancing multiple projects.\xa0', 'The SECU pledge: Be relevant and significant, day in and day out, in the lives of our members, employees and the communities we serve in a highly ethical and fiscally responsible manner.', 'What You’ll Get.', 'Advanced SQL skills.', 'Develop a detailed understanding of the table structure of all critical databases and pipelines, serving as a liaison during key planning phases, which include technical and non-technical stakeholders. ', 'If you’re interested in a challenging and rewarding career, then SECU is for you!', 'Play an important role in the continued evolution of SECU’s modern data architecture. By collaborating with other data engineers, data scientists, analysts, technical consultants and other business users, you will\xa0ensure that data sets are of high integrity while in proper alignment with business needs.', 'As a SECU Data Engineer, you will also receive the following support and total rewards package: \xa02021 Benefits Guide', 'What We’re Looking For', '**This position is remote for candidates living in Maryland or bordering/nearby states**\xa0\xa0', 'Software/data engineering experience with Python.', 'Integrate automated data quality assurance mechanisms into orchestrated data pipelines using modern frameworks and performs root-cause analysis as needed to uncover data irregularities.', 'Ability to work alone or in teams as appropriate.', 'Data Engineer', 'We are looking for innovative and dynamic professionals with a passion for exceptional service to join our SECU team as a', 'Experience working in a highly collaborative, team environment.', 'What You’ll Do.', 'Provide SQL, Python and other programming-specific technical guidance to other engineers and analysts to support continued learning and development throughout the organization. Support corporate research by designing and executing complex ad hoc queries as needed.', 'Be largely self-directed and will provide extensive communication and documentation in support of their work.']",Associate,Full-time,Information Technology,Financial Services,2021-03-24 13:05:10
Data Engineer,Ford Motor Company,"Dearborn, MI",7 days ago,37 applicants,"['', 'Knowledge and experience with Agile methodology', 'Familiar with big data and machine learning tools and platforms', 'Audit implemented processes and procedures to ensure compliance and achievement of OKRs pertaining to data access', 'Ability to work as a global team member, as well as independently, in a changing environment and managing multiple priorities.', 'Reviews and approves Test Scenarios and Test cases. Plans and conducts UAT for the Data products', 'Work with OGC and GDI&A data governance to ensure customer privacy is respected and contractual purposes of use are honored', 'Our Preferred Requirements', 'Experience building Data processing solutions ', 'Develops use cases to drive development and prioritization of operational needs', 'Familiar with BI tools, such as Tableau, Data Stage, or QlikView etc.', 'Monitor Data Pipelines and Data flows and address the issues within established First Responder processes', 'Ability to establish and maintain cooperative and effective working relationships with application implementation teams, IT project teams, business customers, and end users.', 'Work with 3rd party data teams to ensure that intended use cases are accounted for with external data contracts', 'Bachelor’s degree ', 'Experience building Data processing solutions Experience as a Product Owner is a plus. Ability to take business requirements and translate them into technical requirements Knowledge and experience with Agile methodologyPossess excellent oral and written communication skills, as well as facilitation and presentation skills, and engaging presentation style. Ability to establish and maintain cooperative and effective working relationships with application implementation teams, IT project teams, business customers, and end users.Strong analytical and problem-solving skillsAbility to work as a global team member, as well as independently, in a changing environment and managing multiple priorities.Ability to deliver work within deadlines.Familiar with big data and machine learning tools and platformsFamiliar with BI tools, such as Tableau, Data Stage, or QlikView etc.Proficiency in Word, Excel and PowerPointProficiency in Visio, Microsoft Project or Rally tools a plus', 'Work with data scientists and software engineers to understand data activities, data solution ideation, and implementation Work with OGC and GDI&A data governance to ensure customer privacy is respected and contractual purposes of use are honoredWork with 3rd party data teams to ensure that intended use cases are accounted for with external data contractsAudit implemented processes and procedures to ensure compliance and achievement of OKRs pertaining to data accessMonitor Data Pipelines and Data flows and address the issues within established First Responder processesDevelops use cases to drive development and prioritization of operational needsManages Product backlog, prioritize features and drive delivery of the productReviews and approves Test Scenarios and Test cases. Plans and conducts UAT for the Data products', 'Possess excellent oral and written communication skills, as well as facilitation and presentation skills, and engaging presentation style. ', 'The Minimum Requirements We Seek', 'What You’ll Be Able To Do', 'Manages Product backlog, prioritize features and drive delivery of the product', 'Proficiency in Word, Excel and PowerPoint', 'Bachelor’s degree Four or more years of experience in Technology projects (i.e. IT projects)At least two years of experience within a governance or audit organization', 'What You’ll Receive In Return', 'Work with data scientists and software engineers to understand data activities, data solution ideation, and implementation ', 'Proficiency in Visio, Microsoft Project or Rally tools a plus', 'Ability to deliver work within deadlines.', 'Experience as a Product Owner is a plus. Ability to take business requirements and translate them into technical requirements ', 'Strong analytical and problem-solving skills', 'At least two years of experience within a governance or audit organization', 'Four or more years of experience in Technology projects (i.e. IT projects)']",Entry level,Contract,Information Technology,Automotive,2021-03-24 13:05:10
Data Engineer,Mindshare,"New York, NY",2 days ago,52 applicants,"['', 'Your Qualifications', 'Bachelors or advanced degree in Statistics, Economics, Business, Math, Computer Science, or Sciences is preferredMinimum 3 years’ experience in data governance processes in a data warehousing, ETL, and business intelligence environmentBackground in media; digital investment, analytics, or ad operations is preferred1-2 years’ experience in business analysisExperience and understanding of processes across multiple media channels (digital, search, social, offline, etc.); and experience in handling diverse datasets (paid media, owned, etc.)Exceptional organizational skills, multi-tasking capabilities, and detail-orientedAbility to foster collaborative relationships with other cross-functional teamsAbility to manage and prioritize competing projects and deliverablesComfortable working with budgets and numbers', 'Manage/Build Data pipelines and automation.', 'Supervise Senior Associates and Associates to perform data quality assurance and data processing tasks', 'Analyze data and consult with subject matter experts to design and develop business requirements for various tools and processes; Create, gather and write Business and Technical Requirements DocumentsAct as the subject matter expert for questions/projects relating to data quality, BI tools, business rules; Ensure that best practices are being followed amongst all projectsMust familiarize themselves with the full campaign implementation workflow across all functional teams, including campaign set up, launch, testing, reporting and reconciliation', 'Builds positive relationships with internal teams; effectively communicate roles, responsibilities and expectationsBuilds positive relationships with vendors and partners', 'Experience and understanding of processes across multiple media channels (digital, search, social, offline, etc.); and experience in handling diverse datasets (paid media, owned, etc.)', 'Background in media; digital investment, analytics, or ad operations is preferred', 'Manage/Build Data pipelines and automation.Manage the investigation and resolution of potential data issues existing within business intelligence toolsTrack the progress and resolution of data issues, elevating any critical issues to upper managementSupervise Senior Associates and Associates to perform data quality assurance and data processing tasks', 'Track the progress and resolution of data issues, elevating any critical issues to upper management', 'Ability to manage and prioritize competing projects and deliverables', 'Comfortable working with budgets and numbers', 'Analyze data and consult with subject matter experts to design and develop business requirements for various tools and processes; Create, gather and write Business and Technical Requirements Documents', '1-2 years’ experience in business analysis', 'Strong familiarity with relational database management systems; Experience with SQL, Microsoft Azure a plus', 'Business Analysis/Project ManagementAnalyze data and consult with subject matter experts to design and develop business requirements for various tools and processes; Create, gather and write Business and Technical Requirements DocumentsAct as the subject matter expert for questions/projects relating to data quality, BI tools, business rules; Ensure that best practices are being followed amongst all projectsMust familiarize themselves with the full campaign implementation workflow across all functional teams, including campaign set up, launch, testing, reporting and reconciliation', 'Business Analysis/Project ManagementAnalyze data and consult with subject matter experts to design and develop business requirements for various tools and processes; Create, gather and write Business and Technical Requirements DocumentsAct as the subject matter expert for questions/projects relating to data quality, BI tools, business rules; Ensure that best practices are being followed amongst all projectsMust familiarize themselves with the full campaign implementation workflow across all functional teams, including campaign set up, launch, testing, reporting and reconciliationETL Processes/Business Intelligence DashboardsManage/Build Data pipelines and automation.Manage the investigation and resolution of potential data issues existing within business intelligence toolsTrack the progress and resolution of data issues, elevating any critical issues to upper managementSupervise Senior Associates and Associates to perform data quality assurance and data processing tasksOverallBuilds positive relationships with internal teams; effectively communicate roles, responsibilities and expectationsBuilds positive relationships with vendors and partners', 'Familiarity with business intelligence tools such as Tableau, PowerBI, Datorama, DOMO, Looker', 'Builds positive relationships with internal teams; effectively communicate roles, responsibilities and expectations', 'Experience with advertising campaign management/buying platforms such as Sizmek, DCM, Moat, DV, IAS, Mediaocean, Media Tools, Nielsen, ComScore, Facebook, Twitter, Pinterest, Snapchat, 4C, Brand Networks, YouTube, Google Ads, Bing, Kenshoo, SA360, DV360, Google Analytics, Adobe Analytics', 'Manage the investigation and resolution of potential data issues existing within business intelligence tools', 'Familiarity with programming languages/development tools a plus: Python, R, JavaScript, GitHub, etc.', 'About Mindshare', 'Bachelors or advanced degree in Statistics, Economics, Business, Math, Computer Science, or Sciences is preferred', 'Strong familiarity with relational database management systems; Experience with SQL, Microsoft Azure a plusFamiliarity with business intelligence tools such as Tableau, PowerBI, Datorama, DOMO, LookerExperience with advertising campaign management/buying platforms such as Sizmek, DCM, Moat, DV, IAS, Mediaocean, Media Tools, Nielsen, ComScore, Facebook, Twitter, Pinterest, Snapchat, 4C, Brand Networks, YouTube, Google Ads, Bing, Kenshoo, SA360, DV360, Google Analytics, Adobe AnalyticsAdvanced excel data manipulation skills required; VBA/macros is a plusFamiliarity with programming languages/development tools a plus: Python, R, JavaScript, GitHub, etc.', 'Your Impact', 'Must familiarize themselves with the full campaign implementation workflow across all functional teams, including campaign set up, launch, testing, reporting and reconciliation', 'Advanced excel data manipulation skills required; VBA/macros is a plus', 'Act as the subject matter expert for questions/projects relating to data quality, BI tools, business rules; Ensure that best practices are being followed amongst all projects', 'Systems & Tooles', 'Exceptional organizational skills, multi-tasking capabilities, and detail-oriented', 'OverallBuilds positive relationships with internal teams; effectively communicate roles, responsibilities and expectationsBuilds positive relationships with vendors and partners', 'The Role', 'ETL Processes/Business Intelligence DashboardsManage/Build Data pipelines and automation.Manage the investigation and resolution of potential data issues existing within business intelligence toolsTrack the progress and resolution of data issues, elevating any critical issues to upper managementSupervise Senior Associates and Associates to perform data quality assurance and data processing tasks', 'Builds positive relationships with vendors and partners', 'Ability to foster collaborative relationships with other cross-functional teams', 'Minimum 3 years’ experience in data governance processes in a data warehousing, ETL, and business intelligence environment']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer,Blue River Technology,"Sunnyvale, CA",1 day ago,26 applicants,"['', 'Expertise in data modeling for time series, spatial, and image data for analytic and operational use cases', 'Experience crafting data systems to support machine learning and robotics applications', 'Position Description', 'Experienced in data mining and visualization of large data sets', 'Help enable our users to find their data! Develop best practices for data access and queries.', 'Strong Python programmer', 'Experience working with high dimensional data: images, videos, point clouds, etc.', 'Work with product, ML scientists, roboticists, and software engineers to build a data platform that supports development of Intelligent Machines', 'Data lifecycle management experience', 'John Deere & Company, with over 180 years of experience in designing, manufacturing, and distributing innovative products to farmers, acquired Blue River Technology in the fall of 2017 as an independently run subsidiary. In partnership with John Deere, Blue River has expanded rapidly and together both companies see many opportunities to apply advanced computer vision, machine learning, and robotics technologies to other areas in agriculture beyond spraying.', 'Blue River offers competitive compensation and benefits, including a great 401(K) match. We believe in a work life balance and offer generous Paid Time Off and Sick Leave as well as Paid Parental Leave and an adoption benefit. Subsidized lunches (when we return to the office), flexible work hours, CalTrain passes (with mobile Wi-Fi!) and a collaborative and supportive environment also contribute to making Blue River a great place to work.', '\xa0', 'Self-motivated, ability to work both independently and in team environments', 'Experience developing on Kubernetes based systems', 'Design and build updates to our data solutions supporting robotics and machine learning development cycleDevelop and architect enhanced systems to enable rapid retrieval of imagery and time series dataPromote standard methodologies in data modeling, storage, and processingAssess, benchmark and select new technologies to be added to the digital product portfolio.Help enable our users to find their data! Develop best practices for data access and queries.Work with product, ML scientists, roboticists, and software engineers to build a data platform that supports development of Intelligent Machines', 'Bachelor’s Degree in Computer Science or related technical subject area', 'Design and build updates to our data solutions supporting robotics and machine learning development cycle', 'Experience with infrastructure as code, such as Terraform', 'Required Professional Skills & Experience:', 'Assess, benchmark and select new technologies to be added to the digital product portfolio.', 'We are committed to building a diverse team and encourage applications from people of all backgrounds.', 'Experience with a diversity of datastores such as Redshift, Dynamo, Athena, Mongo, Postgres', 'Develop and architect enhanced systems to enable rapid retrieval of imagery and time series data', 'Blue River is based in Sunnyvale, CA and has 100+ team members with diverse experience including computer vision, machine learning, systems software, autonomous vehicles and precision agriculture. Our working environment is fast paced and highly collaborative, and employees are excited to use their talents to improve food production and protect the environment.', 'Expertise in data modeling for time series, spatial, and image data for analytic and operational use casesStrong Python programmerExperience with a diversity of datastores such as Redshift, Dynamo, Athena, Mongo, PostgresExperience developing ETL in a microservice architectureData lifecycle management experienceExperience with infrastructure as code, such as TerraformSelf-motivated, ability to work both independently and in team environmentsExcellent communicatorBachelor’s Degree in Computer Science or related technical subject area', 'We’re seeking a talented Data Engineer specializing in building and maintaining production machine learning software platforms to join our team. Our machine learning platform helps manage the various components of the ML application development life cycle, starting from data ingestion, annotation, exploration to model training, deployment and monitoring. All of these components are interdisciplinary, so you will be working closely with roboticists and ML researchers in both defining interfaces and optimizing implementations to meet the final product specifications.', 'Experience working with high dimensional data: images, videos, point clouds, etc.Experience crafting data systems to support machine learning and robotics applicationsExperienced in data mining and visualization of large data setsExperience developing on Kubernetes based systems', 'Experience developing ETL in a microservice architecture', 'Preferred Skills & Experience:', 'Excellent communicator', 'Blue River Technology serves the agricultural industry by designing and building advanced farm machines that utilize technology like computer vision and machine learning to enable farmers to understand and manage every plant. These machines help farmers to improve profitability, protects the environment by reducing pesticide use, and captures valuable plant-by-plant data. Blue River is a pioneer in the agricultural robotics space and has developed the See & Spray precision sprayer, which applies pesticide only where needed, and can reduce pesticide use 90%.', 'Promote standard methodologies in data modeling, storage, and processing', 'Role Responsibilities: ']",Entry level,Full-time,Engineering,Industrial Automation,2021-03-24 13:05:10
Data Engineer,InterQuest Group,"Jacksonville, FL",,N/A,"['Nice To Have Skills ', ""•\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience with data modeling, data warehousing, ETL/ELT development, building and optimizing 'big data' pipelines "", '·\xa0\xa0\xa0\xa0\xa0\xa0Demonstrated ability to create compelling data visualizations (charts, graphs, maps) that provide insight into business problems. ', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience with SQL including joins, inserts, updates, deletes, conditional logic, subqueries, temp tables, and data transformation functions. ', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience with Advanced SQL techniques (SSIS, SSAS) ', '·\xa0\xa0\xa0\xa0\xa0\xa0Experience with visualization platforms (Power BI preferred, alternatives i.e. Tableau accepted). ', 'SENIOR DATA ENGINEER ', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0Demonstrated ability to work collaboratively as part of an agile development team, balancing between contributing to the team and working independently to complete tasks. ', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience with Synapse Analysis Services', ""This position will lean heavily on utilizing Azure Data Lake, by first ingesting large amounts of clinical data into the lake, building relevant SQL models and exporting this information into Power BI to capitalize on the organization's ability to deliver high-quality clinical outcomes in the most efficient manner possible. The ideal candidate has SQL skills (SSIS, SSAS), experience with data visualization platforms such as Power BI, and experience with Azure Analytics Tools including Data Lake Store, Data Factory, SQL DB, and Synapse Analysis Services or equivalent products from other cloud service providers."", '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience with Machine Learning, AI, or RPA. ', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience with scripting languages such as Python and R nice to have not required. ', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience handling Structured and unstructured datasets ', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0Healthcare experience nice to have not required.', ""•\xa0\xa0\xa0\xa0\xa0\xa0\xa0At least a Bachelor's Degree in a quantitative or technical field (engineering, mathematics, physics, machine learning, statistics or computer science)"", 'Must Have Skills ', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience with Microsoft Azure Analytics tools including Data Lake Store, Data Factory, SQL DB, Azure Synapse, Databricks and Analysis Services or equivalent products from other cloud service providers. ', '\xa0']",Mid-Senior level,Contract,Information Technology,Staffing and Recruiting,2021-03-24 13:05:10
Senior Data Engineer,Casper,New York City Metropolitan Area,19 hours ago,38 applicants,"['', 'You will develop data workflows to ingest, validate, and model data to support analytics and business operations needsBe accountable for technical delivery and work with your team and stakeholders on strategy and executionCollaborate with other engineers and product managers in solving interesting and challenging problems across our systemsAdvance modern, agile software development practices and help build a culture of great engineering and organizational practicesLearn. Learn from the team and on your own what you might not know about data infrastructure, business operations, and open source tooling to make you a stronger asset to the team', 'Experience writing Spark ETL jobs a plus', 'Airflow', 'Wellness programs like cash incentives for tracking sleep and fitness, credits for your favorite studios and in-office activities', 'Learn. Learn from the team and on your own what you might not know about data infrastructure, business operations, and open source tooling to make you a stronger asset to the team', 'Python and SQL expertise', 'MuleSoft or AWS Lambda experience a plus', 'Medical, vision, and dental insurance to help you with those coughs or cavities (too many waffles...)..', 'Unlimited vacation policy.\xa0If you need time off just take it; we trust you!', 'You have experience delivering timely, high quality data products and models', 'You have experience working with a diverse set of stakeholders and helping them understand the trade-offs to different product and technical decisions', 'You will develop data workflows to ingest, validate, and model data to support analytics and business operations needs', 'You have the following technical capabilities, developed through at least 4 years of relevant work experience:', 'Looker', 'Participation in our HQ bonus program for some splurging and equity so that you’re part of the Casper family.Medical, vision, and dental insurance to help you with those coughs or cavities (too many waffles...)..Wellness programs like cash incentives for tracking sleep and fitness, credits for your favorite studios and in-office activitiesUnlimited vacation policy.\xa0If you need time off just take it; we trust you!Catered lunches twice a week to give you time to catch up with your teammateFree snacks and coffee, including a huge breakfast selection (10 types of cereal anyone?)A full gifted bed set when you join and product discounts for friends and family!', 'You have a proven engineering background with experience developing data workflows and a good operational understanding of data operations, including processing, storage, quality and management (ML and streaming infra a plus)', 'Experience working with data modeling (star schema) and validation', 'dbt', 'Our vision to be the world’s most-loved and largest sleep company is supported by leveraging data and insights to drive the business forward. Data Engineering provides the platform that enables Casper to efficiently produce, manage and consume high quality data. The data engineering team at Casper is small but sophisticated. ', 'Collaborate with other engineers and product managers in solving interesting and challenging problems across our systems', 'We are looking for a passionate, experienced Senior Data Engineer (will be our lead) with strong Data Engineering experience in AWS, Python, SQL, Snowflake and Airflow to join our team as a tech lead as we continue building our data warehouse and platform capabilities.', 'dbt and Looker experience a plus', 'Looking for a job to get you out of bed?', 'Our Stack includes...', 'We are deeply committed to building a diverse and inclusive workforce so that we represent all those who dream big equally.', 'AWS RedshiftLookerSnowflakedbtAirflowKubernetesPrestoPostgres and Oracle databases', 'Postgres and Oracle databases', 'Free snacks and coffee, including a huge breakfast selection (10 types of cereal anyone?)', 'As an engineer, you will support the evolution of our data platform and analytics capabilities by driving how we process and model new data sets, develop/iterate on internal tooling and infrastructure, and support secure, scalable, and responsible data practices.', 'A full gifted bed set when you join and product discounts for friends and family!', '\xa0', 'AWS Redshift', 'If you dream about this stuff this job is probably right for you.', 'Catered lunches twice a week to give you time to catch up with your teammate', 'You care deeply about agile software processes, data-driven development, reliability, and efficiency', 'When you’re not catching zzz’s, this is what you’ll do...', 'You have the following technical capabilities, developed through at least 4 years of relevant work experience:Python and SQL expertiseExperience working with data modeling (star schema) and validationdbt and Looker experience a plusMuleSoft or AWS Lambda experience a plusExperience writing Spark ETL jobs a plusCollaborative, independent-thinking, and detail-oriented in how you approach projectsYou appreciate the business context that drives the need for data solutions and love working on projects to support those effortsYou have experience delivering timely, high quality data products and modelsYou have a proven engineering background with experience developing data workflows and a good operational understanding of data operations, including processing, storage, quality and management (ML and streaming infra a plus)You have experience working with a diverse set of stakeholders and helping them understand the trade-offs to different product and technical decisionsYou care deeply about agile software processes, data-driven development, reliability, and efficiency', 'Kubernetes', 'You appreciate the business context that drives the need for data solutions and love working on projects to support those efforts', 'Snowflake', 'Presto', ""Casper (casper.com) was created to re-imagine sleep from the ground up, beginning with its obsessively engineered, outrageously comfortable mattress. All of Casper’s sleep products — including its pillow, bedding, and furniture — are developed in-house by the company’s award-winning R&D team in San Francisco. Casper was named one of Fast Company’s Most Innovative Companies in the World and its eponymous mattress was crowned one of TIME Magazine's Best Inventions."", 'The syrup on your waffles...', 'Our dream candidate...', 'Advance modern, agile software development practices and help build a culture of great engineering and organizational practices', 'We look forward to learning more about you!', 'Collaborative, independent-thinking, and detail-oriented in how you approach projects', 'Participation in our HQ bonus program for some splurging and equity so that you’re part of the Casper family.', 'Be accountable for technical delivery and work with your team and stakeholders on strategy and execution']",Mid-Senior level,Full-time,Information Technology,Internet,2021-03-24 13:05:10
Data Engineer,Sevan Multi-Site Solutions,United States,3 weeks ago,128 applicants,"['', 'Embrace key Sevan-wide initiatives, like Safety, Sustainability, and core Values.', 'MS SQL Server, Azure Cosmos DB, Azure Logic Apps/FlowAzure Data Factory, SSIS, Hadoop, Azure DatabricksAzure SQL and Azure Cosmos DB Power BI and Paginated Reports (SSRS)', 'Work with technical team to prioritize technical needs of solutions and take appropriate action to fulfill', 'Summary:', 'Essential Duties and Responsibilities ', 'Qualifications: ', 'Participates in personal career development through on-the-job training, attends training programs and assists in the development of interns / co-ops. ', 'Serves as a role model and promotes professional behavior.', 'MS SQL Server, Azure Cosmos DB, Azure Logic Apps/Flow', 'Exemplifies and promotes our values of integrity, respect, teamwork, excellence, and charity. ', 'Culture, Leadership and Employee Development', 'Excellent communication skills ', 'Sevan Multi-Site Solutions, Inc. is proud to be an equal opportunity employer committed to a diverse and inclusive work environment. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity or expression, national origin, age, marital status, genetics, disability, pregnancy, veteran status or any other basis protected by law.', 'Hi-level of competency and experience with SQL and platform specific versions (e.g.: T-SQL)Competency with web technologies including REST/SOAP APIs, Web Services, JSON, and ODataExperience with OLTP/OLAP database technologies (MS SQL Server, MySQL, or similar)Experience with data visualization tools (Power BI, Tableau)Competency with non-relational database technologies (MongoDB, CosmosDB)Excellent communication skills Demonstrate professionalism, adaptability, and self-motivationProactive approach and capacity to work independentlyAbility taking technical information and adapting it for various audiences', 'Power BI and Paginated Reports (SSRS)', 'Qualifications: To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The requirements listed below are representative of the knowledge, skill, and/or ability required.', 'Analyze and understand business requirements and translate into logical data models', 'Work directly with end users, analysts, and project managers to understand business requirements and develop technical software requirements alongside other engineers', 'Azure Data Factory, SSIS, Hadoop, Azure Databricks', 'Experience with OLTP/OLAP database technologies (MS SQL Server, MySQL, or similar)', 'Ability taking technical information and adapting it for various audiences', 'Demonstrate professionalism, adaptability, and self-motivation', 'Author queries and pipelines for data extraction, movement, integration, and storage', 'Communicates our vision and purpose through Service, Talent, and Choices. ', 'Competency with non-relational database technologies (MongoDB, CosmosDB)', 'Troubleshoot, diagnose, upgrade, and improve the performance of reporting and data solutions', 'Proactive approach and capacity to work independently', 'Azure SQL and Azure Cosmos DB ', 'Embrace key Sevan-wide initiatives, like Safety and Sustainability. ', 'Summary: The Data Engineer is an organized, self-motivated, and detail-oriented individual with the ability to prioritize and meet deadlines to multiple stakeholders and sponsors. DEs are responsible for developing data-driven reporting and solutions that meet requirements in an agile and efficient manner.\xa0Additionally, DEs support the deployment, training, and maintenance of solutions, reports, and integrations when needed.', 'Hi-level of competency and experience with SQL and platform specific versions (e.g.: T-SQL)', 'Additional duties and projects as assigned', 'Design and build reporting solutions and dashboards from myriad disparate data sources', 'Follow DevOps best practices throughout the solution development lifecycle, user acceptance testing, and production release', 'Preferred Skills:', 'Utilize appropriate data analysis and modeling tools and languages based on the given requirements; learn new data analysis languages and tools when necessary.', 'Competency with web technologies including REST/SOAP APIs, Web Services, JSON, and OData', ""Minimum bachelor's degree in Business Analytics, Data Science, Computer Science, Information Systems, or related areas/experience with 3-6 years of relevant experience in data modelling, ETL processing, data analysis and integration, and data visualization."", 'Experience/Education:', 'Design, create and maintain solutions, extensions, and integrations for applications', 'Essential Duties and Responsibilities include but are not limited to the following statements.', 'Analyze and understand business requirements and translate into logical data modelsAuthor queries and pipelines for data extraction, movement, integration, and storageDesign, create and maintain solutions, extensions, and integrations for applicationsUnderstand, explain, develop, and modify relational and non-relational data models; including designing, creating, and customizing along with optimizing based on system performanceDesign and build reporting solutions and dashboards from myriad disparate data sourcesTroubleshoot, diagnose, upgrade, and improve the performance of reporting and data solutionsWork directly with end users, analysts, and project managers to understand business requirements and develop technical software requirements alongside other engineersFollow DevOps best practices throughout the solution development lifecycle, user acceptance testing, and production releaseUtilize appropriate data analysis and modeling tools and languages based on the given requirements; learn new data analysis languages and tools when necessary.Work with technical team to prioritize technical needs of solutions and take appropriate action to fulfillAdditional duties and projects as assigned', 'Exemplifies and promotes our values of integrity, respect, teamwork, excellence, and charity. Embrace key Sevan-wide initiatives, like Safety and Sustainability. Communicates our vision and purpose through Service, Talent, and Choices. Serves as a role model and promotes professional behavior.Participates in personal career development through on-the-job training, attends training programs and assists in the development of interns / co-ops. ', 'Understand, explain, develop, and modify relational and non-relational data models; including designing, creating, and customizing along with optimizing based on system performance', 'Experience with data visualization tools (Power BI, Tableau)']",Associate,Full-time,Information Technology,Construction,2021-03-24 13:05:10
Data Engineer,Transcend Digital,"Los Angeles, CA",23 hours ago,28 applicants,"['', 'Architect, build, and maintain flexible & scalable data infrastructure', 'Location', 'Strong familiarity with SQL databases', 'Experience building and maintaining a data warehouse in production', 'Strong machine learning skills: first-principles knowledge of canonical algorithms and experience using popular libraries such as Tensorflow and Pytorch', ""At least two years of professional software engineering experience, with proficiency in at least one major language (Python, Golang, Typescript preferred)Bachelor's Degree in Computer Science or a related fieldStrong familiarity with SQL databasesExperience building and maintaining a data warehouse in productionExperience designing, implementing, and maintaining extensible and scalable ETL pipelinesStrong machine learning skills: first-principles knowledge of canonical algorithms and experience using popular libraries such as Tensorflow and PytorchApplied statistics skills such as experimental design and hypothesis testingExperience with data transformation tools like Spark or HadoopExperience deploying data services in the cloud (AWS preferred)"", 'Requirements', 'Applied statistics skills such as experimental design and hypothesis testing', 'Experience deploying data services in the cloud (AWS preferred)', ""Bachelor's Degree in Computer Science or a related field"", 'Experience with data transformation tools like Spark or Hadoop', 'Responsibilities', 'Preferred Skills / Experience', 'Summary', ""Function as a leader, contributing towards establishing a culture of excellence as Automotus' engineering team grows"", 'At least two years of professional software engineering experience, with proficiency in at least one major language (Python, Golang, Typescript preferred)', 'Experience working with time-series and geospatial datasets', 'Experience designing, implementing, and maintaining extensible and scalable ETL pipelines', 'Design and implement ML models to deliver predictive insights to our customers', ""Architect, build, and maintain flexible & scalable data infrastructureDesign and implement ML models to deliver predictive insights to our customersFunction as a leader, contributing towards establishing a culture of excellence as Automotus' engineering team grows"", 'Experience working on IoT products', 'Experience working with time-series and geospatial datasetsExperience working on IoT products', 'This role is based in Los Angeles, CA, but we operate as a fully distributed team. If the time zone where you are is within 4 hours of GMT-8, feel free to apply!']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer Senior (Remote),USAA,"San Antonio, TX",23 hours ago,Be among the first 25 applicants,"['', 'Geographical Differential: Geographic pay differential is additional pay provided to eligible employees working in locations where market pay levels are above the national average.Shift premium will be addressed on an individual-basis for applicable roles that are consistently scheduled for non-core hours.BenefitsAt USAA our employees enjoy best-in-class benefits to support their physical, financial, and emotional wellness. These benefits include comprehensive medical, dental and vision plans, 401(k), pension, life insurance, parental benefits, adoption assistance, paid time off program with paid holidays plus 16 paid volunteer hours, and various wellness programs. Additionally, our career path planning and continuing education assists employees with their professional goals.Please click on the link below for more details.USAA Total RewardsRelocation assistance is notavailable for this position.For Internal CandidatesMust complete 12 months in current position (from date of hire or date of placement), or must have manager’s approval prior to posting.Last day for internal candidates to apply to the opening is 3/28/21 by 11:59 pm CST time.', 'And 6 years of data management experience implementing data solutions demonstrating depth of technical understanding within a specific discipline(s)/technology(s)', 'Data Modeling experience', 'Implement efficient defect management, root cause analysis, and resolution processes.', 'to the opening is 3/28/21 by 11:59 pm CST time.', ""Bachelor's degree in related field of study OR 4 additional years of related experience beyond the minimum required."", 'Strong experience with SQL, Hadoop and ETL', 'Create proof of concepts and prototypes.', 'Ensures risks associated with business activities are effectively identified, measured, monitored, and controlled. ', 'Deep knowledge of a technology or product line.', 'Exposure to near real-time Analytic applications', 'For Internal Candidates', 'Breakdown business features into technical stories and approaches.', 'Mentor and coach junior engineers.', 'Shift premium', 'About USAA', 'Recent experience with Python and Spark', 'Preferred Requirements', 'Design and implement complex technical solutions.', 'Identifies and manages existing and emerging risks that stem from business activities and the job role.', 'Minimum Requirements', 'not', 'Last day for internal candidates to apply ', 'Compensation', 'Recent experience with Python and SparkStrong experience with SQL, Hadoop and ETLExposure to near real-time Analytic applicationsData Modeling experienceWorking knowledge of AWS and Snowflake', 'Design, build, manage and optimize data pipelines for data structures encompassing data transformation, data models, schemas, metadata, data quality, and workload management.', 'Benefits', 'Assist in setting technical direction for the team.', 'Identifies and manages existing and emerging risks that stem from business activities and the job role.Ensures risks associated with business activities are effectively identified, measured, monitored, and controlled. Follows written risk and compliance policies and procedures for business activities.Design and implement complex technical solutions.Design, build, manage and optimize data pipelines for data structures encompassing data transformation, data models, schemas, metadata, data quality, and workload management.Participate in daily standups and lead design reviews.Breakdown business features into technical stories and approaches.Analyze data and enable machine learning.Create proof of concepts and prototypes.Implement efficient defect management, root cause analysis, and resolution processes.Assist in setting technical direction for the team.Mentor and coach junior engineers.', 'USAA Total Rewards', 'Relocation', 'available', 'Follows written risk and compliance policies and procedures for business activities.', 'Working knowledge of AWS and Snowflake', 'Participate in daily standups and lead design reviews.', ""Bachelor's degree in related field of study OR 4 additional years of related experience beyond the minimum required.And 6 years of data management experience implementing data solutions demonstrating depth of technical understanding within a specific discipline(s)/technology(s)Deep knowledge of a technology or product line."", 'Analyze data and enable machine learning.']",Not Applicable,Full-time,Information Technology,Financial Services,2021-03-24 13:05:10
Data Engineer,Berkley Technology Services (a Berkley Company),"Wilmington, DE",2 weeks ago,Be among the first 25 applicants,"['', ' 3+ years of experience in a Data Engineer role ', ' Knowledge of cloud based data warehousing products such as Snowflake', ' Assemble large, complex data sets that meet functional/non-functional business requirements. ', ' A budget for continual improvement ', 'Skills You’ll Need', ' Bachelor’s Degree ', ' Bachelor’s Degree  3+ years of experience in a Data Engineer role  3+ years of experience with relational SQL databases.  1+ year of experience with object-oriented/object function scripting languages like Python.  1+ year of experience working with or understanding formal ETL tools  Knowledge of cloud based data warehousing products such as Snowflake', ' An engaged and supportive leadership team that will invest in you ', 'What We’ll Bring', ' Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery/ dataOps. ', ' A broad group of industry experts who work closely with us on everything we do ', ' Talented engineering teams to build products with ', ' 1+ year of experience working with or understanding formal ETL tools ', ' An engaged and supportive leadership team that will invest in you  Talented engineering teams to build products with  A broad group of industry experts who work closely with us on everything we do  A budget for continual improvement  Generous retirement plan  Excellent medical and dental insurance (and other health benefits) ', ' Create and maintain optimal data pipeline architecture. ', ' Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics. ', 'Responsibilities', ' 3+ years of experience with relational SQL databases. ', 'Company Details', 'Qualifications', ' Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL. ', ' Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. ', 'What We’re Looking For', ' 1+ year of experience with object-oriented/object function scripting languages like Python. ', ' Generous retirement plan ', ' Create and maintain optimal data pipeline architecture.  Assemble large, complex data sets that meet functional/non-functional business requirements.  Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery/ dataOps.  Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL.  Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.  Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. ', ' Excellent medical and dental insurance (and other health benefits) ']",Mid-Senior level,Full-time,Information Technology,Financial Services,2021-03-24 13:05:10
Data Engineer,Bloomberg LP,"New York, NY",3 weeks ago,110 applicants,"['Champion improvements: identify strategic technical gaps in our ecosystem and advocate for solutions over workarounds', 'Apply your coding skills: automate the influx of data and build flexible solutions for data acquisition, ETL and machine learning pipelines, and human-in-the-loop data processing to drive successful product adoptionInspire and impact our business: act as an internal consultant to influence and implement more efficient products through analysis, dashboards, web apps, and user documentationDevelop your career: sharpen your technical skills and strengthen relationships through project management, partnering with stakeholders across the firm, and establishing scalable architectureChampion improvements: identify strategic technical gaps in our ecosystem and advocate for solutions over workaroundsGrow our business: Approach each day knowing this role is mission-critical to the success of the firm. We will rely on your expertise, and our flat structure allows for you to make real impact, real quick', 'Develop your career: sharpen your technical skills and strengthen relationships through project management, partnering with stakeholders across the firm, and establishing scalable architecture', 'We""ll trust you to:', '2+ years of experience working with restful APIs and data modeling within SQL and NoSQL databases  ', 'Grow our business: Approach each day knowing this role is mission-critical to the success of the firm. We will rely on your expertise, and our flat structure allows for you to make real impact, real quick', '2+ years of Python programming and scripting in a production environment', 'Legal authorization to work full-time in the United States without requiring visa sponsorship', 'A BA/BS degree or higher in Computer Science, Mathematics, or relevant data technology field, or equivalent professional work experience in software development, data engineering, data science or information technology', 'You""ll need to have:', 'Does this sound like you?', 'Inspire and impact our business: act as an internal consultant to influence and implement more efficient products through analysis, dashboards, web apps, and user documentation', 'Apply your coding skills: automate the influx of data and build flexible solutions for data acquisition, ETL and machine learning pipelines, and human-in-the-loop data processing to drive successful product adoption', 'Deep understanding of large-scale, distributed systems', 'A BA/BS degree or higher in Computer Science, Mathematics, or relevant data technology field, or equivalent professional work experience in software development, data engineering, data science or information technology2+ years of Python programming and scripting in a production environment2+ years of experience working with restful APIs and data modeling within SQL and NoSQL databases  Deep understanding of large-scale, distributed systemsLegal authorization to work full-time in the United States without requiring visa sponsorship']",Not Applicable,Full-time,Information Technology,Financial Services,2021-03-24 13:05:10
Data Engineer,Fearless,"Baltimore, MD",3 weeks ago,195 applicants,"['', 'Perform data analysis required to troubleshoot data related issues and assist in the resolution of data issues.', 'Effective organizational and time management skills with the ability to work independently, as well as with remote teams, under strict project deadlines', ""What you'll be doing:"", 'About Fearless:', 'your ', 'Cultural Interview', 'Family-friendly workplace', ' Take initiative for their own growth through personal leadership Produce stories in Tableau (a story is a feature in Tableau that helps tell a literal story, and setting up data in tableau) Analyze stories and data and determines which are important to tell to provide value to our customers and will be able to share instances in which they have done this and the results Design and implement internal processes for automating data delivery and optimization Build and maintain secure tooling and infrastructure for scalable automated data pipelines and machine learning models that align with the business objectives Collaborate with data science engineers and business analysts to improve data models that solve challenges and support data-driven decision making Support regular ad-hoc data and analysis needs to better understand customer behaviors.  Implement processes and systems to monitor data quality and volume, ensuring data is accurate and available Perform data analysis required to troubleshoot data related issues and assist in the resolution of data issues. Evaluate data and machine learning tools for efficacy ', 'Evaluate data and machine learning tools for efficacy', 'This position will be 100% remote during COVID.', '3 weeks accrued PTO + 1 week sick leave + 10 federal holidays + your birthday off', 'Experience with Machine Learning and Deep Learning Tools and Software', ' Strong communication skills-both writing and orally what the data is communicating to many levels of stakeholders  Strong analytical and problem-solving skills with attention to detail Experience with Machine Learning and Deep Learning Tools and Software Experience in statistics, data processing, or data annotation Understanding of quality assurance with respect to data and models Proficiency with programming languages including Python, R, SAS, and/or Java Experience working with at least one data analysis tool like Hadoop, Apache Spark, or cloud-provider equivalents Effective organizational and time management skills with the ability to work independently, as well as with remote teams, under strict project deadlines Ability and willingness to work virtually and with online collaboration tools Ability to operate and manage work, strategically reason, and build relationships and influence others Empathy-recognizing that data has humans behind it ', 'Strong communication skills-both writing and orally what the data is communicating to many levels of stakeholders ', 'Experience in statistics, data processing, or data annotation', ' This position is located in Baltimore. This position has the flexibility to support some remote work / telecommuting. This position will be 100% remote during COVID. This position will require a Public Trust Clearance that Fearless will sponsor. ', 'This position will require a Public Trust Clearance that Fearless will sponsor.', 'Empathy-recognizing that data has humans behind it', 'Implement processes and systems to monitor data quality and volume, ensuring data is accurate and available', 'Ability and willingness to work virtually and with online collaboration tools', "" Cultural Interview - We're a people-first company, so we always start off by getting to know more about you, how you work, what your career goals are, and what you're passionate about. This is your opportunity to ask questions and get a feel for Fearless, so don't be shy! Technical Interview - This is where we get into the nitty gritty of the project. During the Technical Interview, you'll be interviewed by our Passion Coaches and/or the team's Project Lead to make sure your skills align with the project requirements. Business Interview - At this point, you've made it to the final frontier! The Business Interview is when you'll meet with Fearless leadership to dot the i's, cross the t's, and determine whether or not we'll be moving forward with the hiring process. "", 'Business Interview', 'Support regular ad-hoc data and analysis needs to better understand customer behaviors. ', '100% coverage of the employee-only premium for HSA, HMO, or PPO plan and Employee Wellness Plan', 'Compensation:', 'Build and maintain secure tooling and infrastructure for scalable automated data pipelines and machine learning models that align with the business objectives', 'Proficiency with programming languages including Python, R, SAS, and/or Java', ""Technical Interview - This is where we get into the nitty gritty of the project. During the Technical Interview, you'll be interviewed by our Passion Coaches and/or the team's Project Lead to make sure your skills align with the project requirements."", 'Experience working with at least one data analysis tool like Hadoop, Apache Spark, or cloud-provider equivalents', 'Ability to operate and manage work, strategically reason, and build relationships and influence others', ""Why we're excited about you:"", ""Business Interview - At this point, you've made it to the final frontier! The Business Interview is when you'll meet with Fearless leadership to dot the i's, cross the t's, and determine whether or not we'll be moving forward with the hiring process."", ' Flexible schedule Family-friendly workplace 3 weeks accrued PTO + 1 week sick leave + 10 federal holidays + your birthday off 100% coverage of the employee-only premium for HSA, HMO, or PPO plan and Employee Wellness Plan Tech, education / training, and snack allowances Free parking in downtown Baltimore / public transit coverage Safe Harbor 401(k) plan with employer contributions ', 'Understanding of quality assurance with respect to data and models', 'Tech, education / training, and snack allowances', 'Flexible schedule', 'What you should know:', 'Strong analytical and problem-solving skills with attention to detail', 'Analyze stories and data and determines which are important to tell to provide value to our customers and will be able to share instances in which they have done this and the results', 'Technical Interview', 'Why Fearless?', 'Free parking in downtown Baltimore / public transit coverage', 'This position is located in Baltimore.', 'Collaborate with data science engineers and business analysts to improve data models that solve challenges and support data-driven decision making', 'Design and implement internal processes for automating data delivery and optimization', 'This position has the flexibility to support some remote work / telecommuting.', 'Produce stories in Tableau (a story is a feature in Tableau that helps tell a literal story, and setting up data in tableau)', ""Cultural Interview - We're a people-first company, so we always start off by getting to know more about you, how you work, what your career goals are, and what you're passionate about. This is your opportunity to ask questions and get a feel for Fearless, so don't be shy!"", ""So, what's next?"", 'Take initiative for their own growth through personal leadership', 'Safe Harbor 401(k) plan with employer contributions']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Adobe,"Lehi, UT",2 weeks ago,65 applicants,"['', 'Extensive SQL experience and strong background in optimizing SQL queries for transforming data. At least 5 years experience preferred.', 'Previous experience in software application development in other languages', 'Technologies like Presto, Spark, Apache Iceberg, and DatabricksAWS technologies like EC2, S3, Glue, Etc.Experience building or managing a data lake in AzurePrevious experience in software application development in other languagesWriting python connectors to 3rd party data APIsExperience automating machine learning data pipelines and models', 'Our Company', 'The Opportunity', ""What You'll Do"", 'Bonus Skills', 'Distributed systems (pipelines and databases)', 'Experience building or managing a data lake in Azure', 'Significant experience in Linux environments, batch automation, and shell scripts', 'Writing python connectors to 3rd party data APIs', 'Specialties in Python development for data pipelines and automation, including debugging, testing, and handling development and production code bases. At least 5 years experience preferred.Building and maintaining an enterprise data lakeExperience partitioning data for optimal performanceDistributed systems (pipelines and databases)Columnar technologies like ParquetExtensive SQL experience and strong background in optimizing SQL queries for transforming data. At least 5 years experience preferred.Significant experience in Linux environments, batch automation, and shell scripts', 'Technologies like Presto, Spark, Apache Iceberg, and Databricks', 'What You Need To Succeed', 'Experience partitioning data for optimal performance', 'Specialties in Python development for data pipelines and automation, including debugging, testing, and handling development and production code bases. At least 5 years experience preferred.', 'Building and maintaining an enterprise data lake', 'Experience automating machine learning data pipelines and models', 'AWS technologies like EC2, S3, Glue, Etc.', 'Columnar technologies like Parquet']",Mid-Senior level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer,Avanade,"Philadelphia, PA",8 hours ago,27 applicants,"['', 'About Avanade', ' Implemented analytics systems for more than 550 clients ', ' 14-time winner of Microsoft Partner of the Year ', ' 14-time winner of Microsoft Partner of the Year  24,000+ certifications in Microsoft technology  90+ Microsoft partner awards  17 Gold Competencies  3,500 analytics professionals worldwide  1,000 data engineers  Implemented analytics systems for more than 550 clients  400 AI practitioners  300 cognitive service experts ', ' Assess client needs to build bespoke data design services ', 'About You', ' Do you enjoy making sure that information is accessible and easy to use? So do we. ', 'Your Skills And Business Experience Include', 'About The Role', ' Transforming business needs into technical solutions  Mapping data and analytics  Data profiling, cataloguing and mapping to enable the design and build of technical data flows  Use proven methods to solve business problems using Azure Data and Analytics services in combination with building data pipelines, data streams and system integration  Knowledge of multiple Azure data applications including Azure Databricks  Experience in preparing data for and building pipelines and architecture ', ' 90+ Microsoft partner awards ', ' 3,500 analytics professionals worldwide ', ' Build the building blocks for transforming enterprise data solutions ', ' Mapping data and analytics ', ' Use proven methods to solve business problems using Azure Data and Analytics services in combination with building data pipelines, data streams and system integration ', ' 400 AI practitioners ', ' Use your sound eye for business to translate business requirements into technical solutions ', ' 300 cognitive service experts ', ' Transforming business needs into technical solutions ', ' Give colleagues and clients the tools to find and use data for routine and non-routine analysis  Use your sound eye for business to translate business requirements into technical solutions  Analyze current business practices, processes and procedures to spot future opportunities  Assess client needs to build bespoke data design services  Build the building blocks for transforming enterprise data solutions  Design and build modern data pipelines, data streams, and data service Application Programming Interfaces (APIs)  Craft the architectures, data warehouses and databases that support access and Advanced Analytics, and bring them to life through modern visualization tools  Implement effective metrics and monitoring  Be comfortable to make your own decisions and guide your colleagues  Travel as needed to various client locations ', ' Design and build modern data pipelines, data streams, and data service Application Programming Interfaces (APIs) ', 'Why Avanade', ' 17 Gold Competencies ', 'Day-to-day, You Will', ' 1,000 data engineers ', ' Knowledge of multiple Azure data applications including Azure Databricks ', ' Be comfortable to make your own decisions and guide your colleagues ', ' 24,000+ certifications in Microsoft technology ', ' Travel as needed to various client locations ', ' Experience in preparing data for and building pipelines and architecture ', ' Implement effective metrics and monitoring ', ' Craft the architectures, data warehouses and databases that support access and Advanced Analytics, and bring them to life through modern visualization tools ', 'How We Support You', ' Analyze current business practices, processes and procedures to spot future opportunities ', ' Give colleagues and clients the tools to find and use data for routine and non-routine analysis ', ' Data profiling, cataloguing and mapping to enable the design and build of technical data flows ', 'Job Description']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Bradford & Galt,Greater St. Louis,2 days ago,57 applicants,"['', 'Nice to haves', 'Big Data  ', 'Data Services Engineer', 'PL/SQL', 'Experience partnering with business stakeholders to ensure that technical requirements and solutions meet business needs.', 'ETL experience, Talend a major plus but any ETL tool will suffice', 'Perl\xa0or Python', 'Familiarity with consuming and working with API’s', '\xa0', 'We are looking for a contractor to help support our data services group. The ideal candidate will be a self-starter that is comfortable working in a fast-paced environment and supporting multiple projects simultaneously.', 'Experience with oracle or big query (or both)', '5+ years of professional development/engineering experience ', 'Bradford & Galt is an equal opportunity employer. We will not discriminate, and will take affirmative action measures to ensure against discrimination in employment, recruitment, advertisements for employment, compensation, termination, and other conditions of employment, against any employee or job applicant on the basis of race, color, gender, national origin, age, religion, creed, disability, veteran’s status, or sexual orientation.', 'Bachelor Degree a nice to haveExperience with airflowPL/SQLMQ/PubSubPerl\xa0or PythonBig Data  ', 'High level understanding of cloud environments (preferably GCP)', '5+ years of professional development/engineering experience Strong ANSI SQL skillsExperience with oracle or big query (or both)Familiarity with consuming and working with API’sETL experience, Talend a major plus but any ETL tool will sufficeComfortable working in JiraHigh level understanding of cloud environments (preferably GCP)Experience partnering with business stakeholders to ensure that technical requirements and solutions meet business needs.', '*Must be local to St. Louis, MO*', 'Experience with airflow', 'MQ/PubSub', 'Strong ANSI SQL skills', 'Bachelor Degree a nice to have', 'Comfortable working in Jira', 'Required Skills']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Creative Circle,"Austin, Texas Metropolitan Area",,N/A,"['', 'Starts', 'Location', 'Status: Freelance (40 hours a week)', 'Rate: Up to $37.80/hr. (DOE)', ""-Bachelor's degree in related field or equivalent experience. "", ' ', '-Responsible for the coordination/set-up and proofing of select categories on the internet site.', 'Responsibilities:', 'Position', 'Job Description:', 'Skills/Qualifications:', 'Location: WFH during Covid-19', '-2-5 years of experience', '-Experience with internet site production with keen understanding of what is involved with changing content on the web.', 'Position: eCommerce Data Engineer', '-Proof and edit product and collection pages, verify copy accuracy, crop images and ensures that all content meets our formatting rules, copy guidelines and makes for an optimal shopping experience.', 'Status', 'Estimated Duration:', '-Serves as liaison between the merchants and IT. ', 'Starts: Within 2+ weeks', '-Utilize the mainframe to identify needed assets for each launch and communicate to outside vendors as needed.', 'Estimated Duration: 6+ Months', ' -Visit competitor websites to generate ideas and improvements.', 'Rate', '-Fix errors on the live site and make mid-window updates based on business needs.']",Mid-Senior level,Full-time,Marketing,Information Technology and Services,2021-03-24 13:05:10
Junior Data Engineer,Invesco US,"Atlanta, GA",2 weeks ago,59 applicants,"['', 'Financials services/asset management industry experience is a plus', 'Self-motivated. Capable of working with little or no supervision', 'Ability to react positively under pressure to meet tight deadlines', 'Your Role', 'Employee stock purchase plan', 'The Department', 'Experience in data warehousing concepts', 'Health & wellbeing benefits', 'Flexible time off and opportunities for a flexible work schedule', 'Maintains a broad understanding of implementation, integration, and inter-connectivity issues with emerging technologies to define data strategies.', 'Experience using GitHub, Bit Bucket, or other code repository solution', 'Familiarity with shell/python scripting languages', 'What’s in it for you?', 'Prepare for and support user acceptance testing.', '1- 2 years of experience with Oracle and/or SQL server database and deep knowledge in SQL', 'Strong data analytical skills', ' Flexible time off and opportunities for a flexible work schedule 401(K) matching of 100% up to the first 6% with additional supplemental contribution Health & wellbeing benefits Parental Leave benefits Employee stock purchase plan ', ' 1- 2 years of experience working in an Oracle, Informatica, UNIX production data warehousing environment as a Data Warehouse developer 1- 2 years of experience in Autosys job scheduler 1 - 2 years of experience designing and implementing ETL processes using Informatica PowerCenter 1- 2 years of experience with Oracle and/or SQL server database and deep knowledge in SQL Understanding of relational database design and development life cycle principles and standard methodologies Experience using GitHub, Bit Bucket, or other code repository solution Experience with Agile methodology and working on scrum teams. Familiarity with shell/python scripting languages Experience with troubleshooting issues in the software and bug-fixes Experience in data warehousing concepts Financials services/asset management industry experience is a plus ', ' No more than 8 bullet points of 1-2 sentences max ', 'No more than 8 bullet points of 1-2 sentences max', '1- 2 years of experience working in an Oracle, Informatica, UNIX production data warehousing environment as a Data Warehouse developer', 'The experience you bring:', 'Enjoy challenging and thought provoking work and have a strong desire to learn and progress', '401(K) matching of 100% up to the first 6% with additional supplemental contribution', 'Provide post implementation support.', 'Experience with Agile methodology and working on scrum teams.', 'Skills / Other Personal Attributes Required:', 'Understanding of relational database design and development life cycle principles and standard methodologies', 'Parental Leave benefits', 'Complete all tasks related to technical analysis, building and unit testing, quality assurance, system test and implementation in accordance with the Technology development life cycle.', 'Assist with data modeling skills and architecture', 'Open minded, flexible and willing to listen for other people’s opinions.', 'Analyze requirements, design, build and test system components.', ' Work with development teams and other project leaders/stakeholders to provide technical solutions that enable business capabilities. Complete all tasks related to technical analysis, building and unit testing, quality assurance, system test and implementation in accordance with the Technology development life cycle. Design and/or understand complex data models with strong SQL skills Assist with data modeling skills and architecture Analyze requirements, design, build and test system components. Maintains a broad understanding of implementation, integration, and inter-connectivity issues with emerging technologies to define data strategies. Execute strategies that inform data design and architecture partnering with enterprise standard. Understand client business and make recommendations and technology suggestions to solve problems and improve efficiency. Understand architecture of applications to effectively troubleshoot problems and develop more efficient production processes. Be prepared to carry out business analysis tasks to ensure that the development/change meets user requirements and expectations. Prepare for and support user acceptance testing. Provide post implementation support. ', '1- 2 years of experience in Autosys job scheduler', 'Execute strategies that inform data design and architecture partnering with enterprise standard.', '1 - 2 years of experience designing and implementing ETL processes using Informatica PowerCenter', 'Understand client business and make recommendations and technology suggestions to solve problems and improve efficiency.', 'Design and/or understand complex data models with strong SQL skills', 'Strong written, verbal communication and presentation skills', 'Be prepared to carry out business analysis tasks to ensure that the development/change meets user requirements and expectations.', 'Able to work independently or as a team player and ramp up on new technologies', 'Comfortable working with ambiguity (e.g. imperfect data, loosely defined concepts, ideas, or goals) and translating these into more tangible outputs.', 'Able to work in a global, multicultural environment', 'Work with development teams and other project leaders/stakeholders to provide technical solutions that enable business capabilities.', 'Understand architecture of applications to effectively troubleshoot problems and develop more efficient production processes.', 'Experience with troubleshooting issues in the software and bug-fixes', ' Strong written, verbal communication and presentation skills Able to work in a global, multicultural environment Self-motivated. Capable of working with little or no supervision Ability to react positively under pressure to meet tight deadlines Able to work independently or as a team player and ramp up on new technologies Open minded, flexible and willing to listen for other people’s opinions. Strong data analytical skills Enjoy challenging and thought provoking work and have a strong desire to learn and progress Comfortable working with ambiguity (e.g. imperfect data, loosely defined concepts, ideas, or goals) and translating these into more tangible outputs. ']",Not Applicable,Full-time,Information Technology,Investment Management,2021-03-24 13:05:10
Junior Data Engineer,Cypress HCM,Washington DC-Baltimore Area,2 weeks ago,Over 200 applicants,"['', 'Designing intricate data workflow solutions and data models', 'Researching and recommending alternative procedures to resolve issues', 'Knowledge of scripting languages (e.g., Java or Python)', 'Designing intricate data workflow solutions and data modelsDeveloping SQL and unit testingDesigning and coding review functions for the development projectsResearching and recommending alternative procedures to resolve issuesAnalyzing trends in performance to prevent complications proactively', 'Developing SQL and unit testing', 'Responsibilities', 'Designing and coding review functions for the development projects', 'Capability to work independently with a self-starting mindset and the capacity to research and find solutions', 'Bachelor’s degree', 'Analyzing trends in performance to prevent complications proactively', 'Requirements', '1-2 years of developing intricate SQL queries and analyzing data, and systems integrationKnowledge of scripting languages (e.g., Java or Python)Bachelor’s degreeCapability to work independently with a self-starting mindset and the capacity to research and find solutions', '1-2 years of developing intricate SQL queries and analyzing data, and systems integration']",Associate,Full-time,Engineering,Education Management,2021-03-24 13:05:10
Junior Data Engineer,Blackstone and Cullen,"Duluth, GA",4 days ago,Be among the first 25 applicants,"['', 'Qualifications For Data Engineer Middot']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Russell Tobin,"New York, NY",,N/A,"['', 'Customer overview: ', 'Job Location:', 'Job Duration:', 'Perform and report on ad-hoc data analyses', 'Startup experience is a major plus.', 'Take ownership of what we’re building and participate in the technology stack decisions', 'Customer overview:', 'Customer is into Biotechnology industry, who provide science-driven solutions and health intelligence company.', 'RESPONSIBILITIES:', 'Work within a software engineering team to deliver data products to market', 'Responsibilities will include:', 'Create quality metrics and monitoring tools to ensure high fidelity data', 'Strong, clear communication ability, work ethic', 'Participate in Schema design with our bioinformatics team', 'Job Description:-\xa0', 'Job Title: Data Engineer', '\xa0', 'Write high quality, well tested, production grade code.', 'Proven track record of data engineering, backing real-world shipping software products.', 'Job Title: ', 'Strong knowledge of databases (relational and non-relational)', 'No Sponsorship / C2C ', 'As a Data Engineer, you will help design, build, and maintain the data pipelines that power Customer’s information and diagnostic business lines. ', 'Strong communication both written and verbal\xa0', 'Proficiency in SQL and Git', 'Job Location: New York, NY 10017, United States', 'Optimize data structures, schemas, indices, and storage engines. Perform and report out ad-hoc data analyses, establish and maintain data security protocols. Optimize data storage cost.', 'Job Duration: Fulltime / PERM / DirectHire\xa0(Remote to start – will be onsite after Pandemic with flexibility)', 'QUALIFICATIONS:', 'You will work alongside software development and devops teams to instrument and monitor data stores with performance and latency. ', 'Assist in database administration. Backups, performance tuning, load balancing', 'Develop and maintain ETL processes']",Mid-Senior level,Full-time,Information Technology,Biotechnology,2021-03-24 13:05:10
Data Engineer,Illumina,"San Diego, CA",2 days ago,Be among the first 25 applicants,"['', 'Create, construct, test and maintain data architectures integrating the various data sources servicing GQO. This will include enterprise level systems (SAP, Salesforce) and manufacturing transactional systems such as LIMS and MES. The role will require working directly with the business owner(s) to aid in requirement setting of client’s issues. It will involve integrating different operational enterprise data sources into a wholistic architecture to enable insights into cost, yield, root cause analysis and other operational metrics.The role is focused on providing the foundational data and data services for a variety of teams within GQO. You will need to be able to maintain queries and views for the above, with the ability to design and build efficient and effective solutions to very complex query and analytical scenarios. You will work closely with stakeholders, SMEs and analysts to create and refine the requirements and ensure the deliverables match their expectations.The role will involve integrating different operational enterprise solutions into a wholistic architecture to unlock and enable data analysts and data scientists to drive to opportunistic insights into cost and impact of poor quality, supply chain, manufacturing issues, root cause analysis of customer complaints and other key metrics for management reporting.You will have opportunities to recommend and assist in data acquisition, data quality and reliability improvements and other areas to improve not only the data but the business processes / systems generating the data.Extremely detail oriented person with strong organization skills to manage multiple projects and ability to effectively to work across the organization and interpret needs into a solution. Collaboration across the organization will be essential and ability to clearly communicate up to executive team is required. Excellent analytical, problem solving skills, combined with strong business judgment, and an ability to present your solution(s) in a clear and compelling manner.', 'Minimum of a BS degree in Computer Science, Information Architecture, System Integration or equivalent degree or equivalent job experience, with a MS Preferred.', 'Expert level proficiency in data modeling, SQL query solution design and coding, query optimization and performance tuning, and Denodo view development.', 'Must be able to work independently or as part of a larger group.', 'Create, construct, test and maintain data architectures integrating the various data sources servicing GQO. This will include enterprise level systems (SAP, Salesforce) and manufacturing transactional systems such as LIMS and MES. ', 'High degree of initiative and ownership, as well as a proven history of delivering results while working with several different departments in a fast-paced environment.', 'The role will involve integrating different operational enterprise solutions into a wholistic architecture to unlock and enable data analysts and data scientists to drive to opportunistic insights into cost and impact of poor quality, supply chain, manufacturing issues, root cause analysis of customer complaints and other key metrics for management reporting.', 'Extremely detail oriented person with strong organization skills to manage multiple projects and ability to effectively to work across the organization and interpret needs into a solution. ', 'Enterprise data experience using relational databases such as SQL Server or Postgres. HANA or Denodo a plusExpert level proficiency in data modeling, SQL query solution design and coding, query optimization and performance tuning, and Denodo view development.Strong ETL Tools expertise (Informatica, SQL Server Integration Services).Experience with large, complex relational data models and very large data sets (billions of rows).Expert level proficiency in database architecture and design.Demonstrated ability to work with ambiguous requirements, adapt, and learnMust be able to work independently or as part of a larger group.High degree of initiative and ownership, as well as a proven history of delivering results while working with several different departments in a fast-paced environment.', 'Education', 'Requirements', 'Experience with large, complex relational data models and very large data sets (billions of rows).', 'Demonstrated ability to work with ambiguous requirements, adapt, and learn', 'Responsibilities', 'Collaboration across the organization will be essential and ability to clearly communicate up to executive team is required. Excellent analytical, problem solving skills, combined with strong business judgment, and an ability to present your solution(s) in a clear and compelling manner.', 'Strong ETL Tools expertise (Informatica, SQL Server Integration Services).', 'Expert level proficiency in database architecture and design.', 'The role is focused on providing the foundational data and data services for a variety of teams within GQO. You will need to be able to maintain queries and views for the above, with the ability to design and build efficient and effective solutions to very complex query and analytical scenarios. You will work closely with stakeholders, SMEs and analysts to create and refine the requirements and ensure the deliverables match their expectations.', 'You will have opportunities to recommend and assist in data acquisition, data quality and reliability improvements and other areas to improve not only the data but the business processes / systems generating the data.', 'The role will require working directly with the business owner(s) to aid in requirement setting of client’s issues. It will involve integrating different operational enterprise data sources into a wholistic architecture to enable insights into cost, yield, root cause analysis and other operational metrics.', 'Enterprise data experience using relational databases such as SQL Server or Postgres. HANA or Denodo a plus']",Not Applicable,Full-time,Information Technology,Biotechnology,2021-03-24 13:05:10
Data Engineer,Shippo,"San Francisco, CA",19 hours ago,36 applicants,"['', 'Develop clean, well-designed, reusable, scalable code following TDD practices Champion engineering organization’s adoption and ongoing use of the data infrastructure ', 'Embody Shippo’s cultural values in your everyday work and interactions', 'Experience designing, building, and maintaining data pipeline systemsCoding experience in server-side programming languages (e.g. Python, Scala, Go, Java) as well as database languages (SQL)', 'BS or MS degree in Computer Science or equivalent experience', 'Experience with data technologies and concepts such as Airflow, Kafka, Hadoop, Hive, Spark, MapReduce, RDBMS, NoSQL, and Columnar databases', 'Design, build, scale, and evolve our large scale data infrastructure and processing workflows to support running our business intelligence, data analytics and data science processesBuild robust, efficient and reliable data pipelines and data integration consisting of diverse data sources and transformation techniques, and ensure consistency and availability of data insights Collaborates with product, engineering and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-informed decision making across the organization Articulate and present findings and recommendations at different levels, with a clear bias towards impactful learning and results Develop clean, well-designed, reusable, scalable code following TDD practices Champion engineering organization’s adoption and ongoing use of the data infrastructure Embody Shippo’s cultural values in your everyday work and interactions', ' 3+ years of experience in software developmentExperience designing, building, and maintaining data pipeline systemsCoding experience in server-side programming languages (e.g. Python, Scala, Go, Java) as well as database languages (SQL)Experience with data technologies and concepts such as Airflow, Kafka, Hadoop, Hive, Spark, MapReduce, RDBMS, NoSQL, and Columnar databasesExceptional verbal, written, and interpersonal communication skillsDeep understanding of customer needs and passion for customer successExhibit core behaviors focused on craftsmanship, continuous improvement, and team successBS or MS degree in Computer Science or equivalent experience', 'Medical, dental, and vision healthcare coverage for you and your dependents. Pets coverage is also available!', 'Flexible policy for PTO and work arrangement', '3 VTO days for ShippoCares volunteering events$2,500 annual learning stipend for your personal and professional growth', 'Experience building stream-processing systems, using solutions such as Kinesis Stream, Kafka or Spark-Streaming', 'Deep understanding of customer needs and passion for customer success', 'Requirements', 'Charity donation match up to $100Free daily catered lunch, drinks, and snacks', 'Articulate and present findings and recommendations at different levels, with a clear bias towards impactful learning and results ', 'Exhibit core behaviors focused on craftsmanship, continuous improvement, and team success', 'Experience with implementing ETL process', 'Medical, dental, and vision healthcare coverage for you and your dependents. Pets coverage is also available!Flexible policy for PTO and work arrangement3 VTO days for ShippoCares volunteering events$2,500 annual learning stipend for your personal and professional growthCharity donation match up to $100Free daily catered lunch, drinks, and snacksFun team events outside of work hours - happy hours, “escape room” adventures, hikes, and more!', 'Design, build, scale, and evolve our large scale data infrastructure and processing workflows to support running our business intelligence, data analytics and data science processes', 'Exceptional verbal, written, and interpersonal communication skills', ' 3+ years of experience in software development', 'Experience with implementing ETL processExperience with Big Data frameworks such as Hadoop, MapReduce and associated toolsExperience building stream-processing systems, using solutions such as Kinesis Stream, Kafka or Spark-StreamingExperience integrating with APIs that use REST, gRPC, SOAP and other technologiesExperience with cloud environments and DevOps tools; working experience with AWS and its associated products a plus Experience with machine learning a plus ', 'Build robust, efficient and reliable data pipelines and data integration consisting of diverse data sources and transformation techniques, and ensure consistency and availability of data insights ', 'Experience integrating with APIs that use REST, gRPC, SOAP and other technologies', 'Collaborates with product, engineering and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-informed decision making across the organization ', 'Experience with cloud environments and DevOps tools; working experience with AWS and its associated products a plus Experience with machine learning a plus ', 'Fun team events outside of work hours - happy hours, “escape room” adventures, hikes, and more!', 'Experience with Big Data frameworks such as Hadoop, MapReduce and associated tools']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer - Analytics,EVERSANA,"New Jersey, United States",1 day ago,Over 200 applicants,"['', 'Strong IT background is critical (SQL / ETL / SDLC / Data Engineering / Cloud Solutions / APIs / Data Governance / Data Visualization / etc.)Business Domain knowledge is an asset (Digital Analytics / Precision Marketing / CRM / CDP / 1st Party Data / eCommerce / GMP / DMP / etc.)', 'Business Domain knowledge is an asset (Digital Analytics / Precision Marketing / CRM / CDP / 1st Party Data / eCommerce / GMP / DMP / etc.)', 'You will be joining our team of 60+ consultants with this client', 'This is a long term consulting role with our fortune 100 client in central NJ', 'Strong IT background is critical (SQL / ETL / SDLC / Data Engineering / Cloud Solutions / APIs / Data Governance / Data Visualization / etc.)', 'Business/Data Analyst -', 'No relocation, no 3rd party candidates']",Mid-Senior level,Contract,Information Technology,Pharmaceuticals,2021-03-24 13:05:10
Data Engineer,Equifax,"St Louis, MO",7 days ago,Be among the first 25 applicants,"['', 'Develop strategies, standards and best practices in the areas of data wrangling, data visualization and data integration and lead adoption throughout analytical platformsLead analysis of internal and external data assets in consultation with internal stakeholdersDevelop new processes to create metrics to track the ongoing quality across Equifax data assetsDesign new methodologies and execute investigative data analyses of moderate complexity in response to business request for internal and external customersDevelop and Collaborate extensively with Data, Analytics, and Technology leads to ensure the seamless consumption of insights generated from Equifax’s new big data analytical platform.Stay current with rapidly developing Big Data technologies landscape and share knowledge internally and with customers.\xa0 Prototype and integrate tools into Cambrian environment and lead adoption of tools through Data and Analytics.', 'You will be a part of a Data Engineering team that is at the center of driving enterprise level best practices on big data solutions. Through interactions with Data Stewards, Data Scientists, IT, and Analysts, the Data Engineer facilitates D&A’s cloud transformation utilizing GCP tools and data storage, and ultimately helps D&A’s transition from on-site analytics to the GCP.\xa0\xa0', 'Ownership', 'Decide-Execute-Ship', 'Accountability', 'If this sounds like somewhere you want to work, don’t delay, apply today - we’re looking for you!', 'Data and Analytics is a high-energy, fast-growing organization within Equifax charged with building and supporting best-in-industry data assets and analytical solutions. We deliver solutions and insights to enable success for our customers, drive growth for our business, and create value for our shareholders. At Workforce Solutions, divisional and enterprise data assets are combined, transformed, and delivered to our customers through many delivery channels. As a Data Engineer, you will be instrumental in designing and developing frameworks for supporting data solutions at scale with minimal friction moving from prototype to production.', 'Bravery', 'We offer excellent compensation packages with high-reaching market salaries, 401k matching, along with the works: comprehensive healthcare packages, schedule flexibility, collaborative work spaces, work from home opportunities, paid time off, and organizational growth potential', '5+ years of scripting/coding experience required. Proficiency in one or more of the following languages: Python, R, SQL, SAS', 'Stay current with rapidly developing Big Data technologies landscape and share knowledge internally and with customers.\xa0 Prototype and integrate tools into Cambrian environment and lead adoption of tools through Data and Analytics.', '5+\xa0 years of work experience in building, loading, transforming and analyzing data within and across database platforms and other data stores is required.5+ years of scripting/coding experience required. Proficiency in one or more of the following languages: Python, R, SQL, SASProficiency in SQL and data visualization techniques and tools (Tableau, Spotfire, Excel, etc)Bachelor’s degree in Computer Science, Statistics, Economics, or equivalent quantitative field with heavy emphasis on programming required', '5+\xa0 years of work experience in building, loading, transforming and analyzing data within and across database platforms and other data stores is required.', 'Bachelor’s degree in Computer Science, Statistics, Economics, or equivalent quantitative field with heavy emphasis on programming required', 'We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.', 'Proficiency in SQL and data visualization techniques and tools (Tableau, Spotfire, Excel, etc)', 'Primary Location:', 'We offer excellent compensation packages with high-reaching market salaries, 401k matching, along with the works: comprehensive healthcare packages, schedule flexibility, collaborative work spaces, work from home opportunities, paid time off, and organizational growth potentialGrow at your own pace through online courses at Learning @ Equifax\xa0', 'Think and act differently', 'Schedule:', 'Function:', 'Develop and Collaborate extensively with Data, Analytics, and Technology leads to ensure the seamless consumption of insights generated from Equifax’s new big data analytical platform.', 'Curiosity', 'Collaboration', 'Grow at your own pace through online courses at Learning @ Equifax\xa0', 'Develop new processes to create metrics to track the ongoing quality across Equifax data assets', 'You will become an expert in our technical environment as it relates to data delivery, design data structures and processes balancing immediate deliverables with future reusability, and evangelize best practices for data solution architecture. Being able to blend disparate technologies/platforms (BigQuery, DataFlow, GCP, Python, SAS, SQL, Hadoop, Spark, Kafka) into cohesive solutions is critical for success.', 'AccountabilityBraveryCuriosityCollaborationThink and act differentlyTrustOwnershipDecide-Execute-Ship', 'Design new methodologies and execute investigative data analyses of moderate complexity in response to business request for internal and external customers', 'We work to help create seamless and positive experiences during life’s pivotal moments: applying for jobs or a mortgage, financing an education or buying a car. Our impact is real and to accomplish our goals we focus on nurturing our people for career advancement and their learning and development, supporting our next generation of leaders, maintaining an inclusive and diverse work environment, and regularly engaging and recognizing our employees. Regardless of location or role, the individual and collective work of our employees makes a difference and we are looking for talented team players to join us as we help people live their financial best.', 'Lead analysis of internal and external data assets in consultation with internal stakeholders', 'Develop strategies, standards and best practices in the areas of data wrangling, data visualization and data integration and lead adoption throughout analytical platforms', 'At Equifax, we believe knowledge drives progress. As a global data, analytics and technology company, we play an essential role in the global economy by helping employers, employees, financial institutions and government agencies make critical decisions with greater confidence.\xa0', 'The Data Engineer must be organized, detail-oriented, and a fast learner in order to work with all Equifax’s major data assets and a rapidly changing technology environment. The ability to work in a matrix environment to deliver complete solutions is required.', 'Trust']",Mid-Senior level,Full-time,Analyst,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,JUST Capital,"New York, NY",2 weeks ago,59 applicants,"['', ""Bachelor's degree in Math, Statistics, Computer Science, or other quantitative discipline. "", 'Development of the data model(s) for the DW or a data source', 'Requirements (must Be Able To Demonstrate With Past Experience)', 'Responsible for database administration and tuning.', '2+ year experience in SQL with focus on Business Intelligence and Data warehousing', 'Experience in ODS design, data warehouse and mart design methodologies such as Star-schema, Snowflake, designing slowly changing dimensions and fact tables', 'Subsidized gym memberships, Citi Bike membership, and pre-tax commuter benefits.', 'Best-in-class health, vision, and dental care benefits for you and your family including membership in One Medical Group for on-demand primary care and access to Health Advocate, the nation’s leading healthcare advocacy & assistance company.', 'Provide technical guidance, direction and leadership on unusual or complex problems.', 'Design, implement, and support Data Models, ETLs that provide structured and timely access to large datasets.', 'Refine the logical design so that it can be translated into a specific data model.', 'Thorough understanding of relational and dimensional database modeling', 'Respect for work-life balance, including a flexible work from home policy.', ""Bachelor's degree in Math, Statistics, Computer Science, or other quantitative discipline. 3+ years of experience in enterprise and open source ETL tools such as Talend or Pentaho PDI platform2+ year experience in SQL with focus on Business Intelligence and Data warehousing3+ year experience in Python development; focus on data engineering or data science packages like numpy, pandas, scipy, and etc.Working experience in designing and orchestrating Pentaho kettle transformations and jobs flowsProven experience of Data Architecture and Data Modeling, developing and implementing metadata and master data strategies in data warehouse and mart environmentsThorough understanding of relational and dimensional database modelingExperience in ODS design, data warehouse and mart design methodologies such as Star-schema, Snowflake, designing slowly changing dimensions and fact tablesExperience in extracting data from disparate and heterogeneous data sources like mysql, Flat files, ftp, web services, XML and weblogs into target ODS and Data warehouse"", 'Working knowledge of Amazon workflow services', 'Working knowledge of Amazon workflow servicesExperience in managing big data for BI and insight analyticsExtremely strong analytical and problem-solving skills', '3+ year experience in Python development; focus on data engineering or data science packages like numpy, pandas, scipy, and etc.', '20 personal time off days, 5 sick days, as well as 15 holidays a year – and we encourage you to take them!', 'Proven track record of solving challenging problems in both academia and industry ', 'Provide technical guidance, direction and leadership on unusual or complex problems.Design, implement, and support Data Models, ETLs that provide structured and timely access to large datasets.Build fault tolerant, self-healing, adaptive and highly accurate ETL platforms.Developing and maintaining programs on source systems, ETL applications, data cleansing functions, system management functions including load automation, data acquisition functions and others.Development of the data model(s) for the DW or a data sourceResponsible for database administration and tuning.Refine the logical design so that it can be translated into a specific data model.Develop, institutionalize and drive best practice and architectural awareness.', 'Developing and maintaining programs on source systems, ETL applications, data cleansing functions, system management functions including load automation, data acquisition functions and others.', '401(k) retirement plan, with employer matching of 100% up to 4% of salary.', 'Extremely strong analytical and problem-solving skills', 'Best-in-class health, vision, and dental care benefits for you and your family including membership in One Medical Group for on-demand primary care and access to Health Advocate, the nation’s leading healthcare advocacy & assistance company.Respect for work-life balance, including a flexible work from home policy.20 personal time off days, 5 sick days, as well as 15 holidays a year – and we encourage you to take them!Other paid time off when the office generally closes the last week of the year.Fully paid parental leave of up to 12 weeks.401(k) retirement plan, with employer matching of 100% up to 4% of salary.Subsidized gym memberships, Citi Bike membership, and pre-tax commuter benefits.', 'Build fault tolerant, self-healing, adaptive and highly accurate ETL platforms.', 'Develop, institutionalize and drive best practice and architectural awareness.', 'Experience in extracting data from disparate and heterogeneous data sources like mysql, Flat files, ftp, web services, XML and weblogs into target ODS and Data warehouse', '3+ years of experience in enterprise and open source ETL tools such as Talend or Pentaho PDI platform', 'Experience in managing big data for BI and insight analytics', 'Working experience in designing and orchestrating Pentaho kettle transformations and jobs flows', 'Proven experience of Data Architecture and Data Modeling, developing and implementing metadata and master data strategies in data warehouse and mart environments', 'Other paid time off when the office generally closes the last week of the year.', 'Fully paid parental leave of up to 12 weeks.']",Mid-Senior level,Full-time,Information Technology,Nonprofit Organization Management,2021-03-24 13:05:10
Data Engineer,Themesoft Inc.,"New York, NY",3 days ago,Be among the first 25 applicants,[''],Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Labcorp,"Bethesda, MD",2 weeks ago,36 applicants,"['', ' T-SQL, dynamic SQL, scripting, reporting services and Integration Services, flat file automation (pull, post).', ' Exceptional skills in relational database systems - MS SQL Server and Azure SQL Server.', '  5+ years working in information systems development roles or proven technical skills coupled with a degree in Computer Science, Information Systems or similar field.   3+ years working in a data engineering capacity.  Exceptional skills in relational database systems - MS SQL Server and Azure SQL Server.  T-SQL, dynamic SQL, scripting, reporting services and Integration Services, flat file automation (pull, post).  Highly proficient with SQL Server Integration Services, SQL Server Agent automation.  Ability to identify and diagnose ETL and database related issues, perform root cause analysis, and recommend corrective actions.  Python experience in the context of data orchestration, automation and analysis.  Data visualization design and development capability in Tableau or other tools/languages.  Ability to work with complex business requirements in developing SQL stored procedures.  Solid understanding of flat file formats and file format conversion.  Understanding of EDI file formats.  Solid communication skills for documenting complex processes and oral communication skills in communicating with technical and non-technical employees.  Data workflow mapping in Visio or other similar tool. ', ' 3+ years working in a data engineering capacity.', ' Ability to work with complex business requirements in developing SQL stored procedures.', ' 5+ years working in information systems development roles or proven technical skills coupled with a degree in Computer Science, Information Systems or similar field. ', ' Solid communication skills for documenting complex processes and oral communication skills in communicating with technical and non-technical employees.', ' Experience with Azure Cloud service offerings such as: Azure Data Factory, Azure Databricks, Azure Functions, Azure Synapse.', 'Requirements', ' Experience with Agile/Scrum methodologies and working as part of a cross-functional delivery team.', ' Python experience in the context of data orchestration, automation and analysis.', ' Highly proficient with SQL Server Integration Services, SQL Server Agent automation.', ' Understanding of EDI file formats.', ' Ability to identify and diagnose ETL and database related issues, perform root cause analysis, and recommend corrective actions.', 'Strongly Preferred', '  Experience with Agile/Scrum methodologies and working as part of a cross-functional delivery team.  Experience with Azure Cloud service offerings such as: Azure Data Factory, Azure Databricks, Azure Functions, Azure Synapse. ', ' Data visualization design and development capability in Tableau or other tools/languages.', 'Your Background and Experience ', ' Solid understanding of flat file formats and file format conversion.', ' Data workflow mapping in Visio or other similar tool.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer/REMOTE,The Bachrach Group,New York City Metropolitan Area,2 weeks ago,108 applicants,"['', 'PLUS SKILLS:', 'Golang', 'Prior experience in an AWS environment', 'Goal of this role is to review through trillions of datapoints, and petabytes of data to help make it faster, more cost effective, and stable.', 'Php', 'Competitive compensation', 'Ideal person will love working autonomously, spending most of their time with all their data!!', 'MUST HAVE TECH SKILLS:', 'Do you love data??? Are you looking for an opportunity to join an innovative, well established small business that is all about data? Then this is the opportunity for you to jump right in and help oversee one of the largest data marketplaces!', 'Prior experience must be in working with node.js', 'node.js', 'SQL']",Mid-Senior level,Full-time,Engineering,Computer Software,2021-03-24 13:05:10
Data Engineer - Remote,SecureTalent Partners,"Denver, CO",5 days ago,65 applicants,"['', 'Experience with ETL to data warehouse systems.', 'Has 2+ years of experience in SaaS development environments.', '3- 5 years of experience.', 'Experience with large-scale data and query optimization techniques.', 'Experience with AWS cloud services: EC2, RDS, Redshift, Aurora Postgres.', 'Troubleshoot and improve the infrastructure required for optimal extraction, transformation, and loading of data from a variety of data sources using SQL and AWS ‘big data’ technologies.', 'Has a Computer Science degree.Has 2+ years of experience in SaaS development environments.Is a ""student of the game"" and thrives on new challenges.Enjoys learning from teammates, and isn\'t afraid to teach others at the same time.Sees the glass half-full.This is a new industry space...your vision could make all the difference!Wants to make a lasting impact and lifelong connections, this is not just another paycheck', 'The ideal fit...', 'Knowledge in multiple scripting languages (e.g. Python).', 'Knowledge of cloud, distributed systems, and stream-processing systems.', 'Collect, parse, analyze, and visualize large sets of data and turn data into insights.', 'This is a new industry space...your vision could make all the difference!', 'Is a ""student of the game"" and thrives on new challenges.', 'Implement and support systems that reliably and efficiently provide interactive query performance on large amounts of multi-modal data.', 'Responsibilities', ""Enjoys learning from teammates, and isn't afraid to teach others at the same time."", 'Understanding of NoSQL and RDBMS.', 'Qualifications', 'Proficient in SQL', 'Wants to make a lasting impact and lifelong connections, this is not just another paycheck', 'Passionate about learning new technologies and solving hard problems in a fast-paced environment.', 'Implement and support systems that reliably and efficiently provide interactive query performance on large amounts of multi-modal data.Improve systems that handle scale.Troubleshoot and improve the infrastructure required for optimal extraction, transformation, and loading of data from a variety of data sources using SQL and AWS ‘big data’ technologies.Collect, parse, analyze, and visualize large sets of data and turn data into insights.Create data tools for analytics and data scientist team members that assist them in building and optimizing our product', 'Sees the glass half-full.', 'Create data tools for analytics and data scientist team members that assist them in building and optimizing our product', 'Has a Computer Science degree.', 'Improve systems that handle scale.', '3- 5 years of experience.Experience with large-scale data and query optimization techniques.Experience with ETL to data warehouse systems.Experience with AWS cloud services: EC2, RDS, Redshift, Aurora Postgres.Proficient in SQLUnderstanding of NoSQL and RDBMS.Knowledge in multiple scripting languages (e.g. Python).Knowledge of cloud, distributed systems, and stream-processing systems.Passionate about learning new technologies and solving hard problems in a fast-paced environment.']",Mid-Senior level,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Data Engineer,"Starry, Inc.","Boston, MA",3 days ago,75 applicants,"['', 'Contributor to open source software', 'Experience with batch ETL or backend bulk data operations', 'API and backend application development', 'Data engineering focused DevOps, observability, CI/CD, etc', 'Data warehouse and data lake architecture and design', 'Unit and integration testing data pipelines', 'Shell scripting, Git, jq, awk, and a variety of other Unix utilities', ' 100% employer paid low deductible health plan, dental plan, vision plan, AD&D and life insurance 401(k) retirement plan and stock options  12 weeks of 100% paid parental leave for new mothers and fathers after six months of continuous employment Professional development assistance after six months of employment Catered meals on a weekly basis for employees working in the office Casual dress, community clubs, annual fitness reimbursement, stocked kitchen and other perks and discounts ', 'Virtualization with Docker or an equivalent', '401(k) retirement plan and stock options ', ' 1+ years of data engineering experience or 3+ years of engineering experience Familiar with SQL, profile query performance, and design database schema Experience with batch ETL or backend bulk data operations Familiar with CI/CD, Docker or similar, and testing data intensive processes ', 'Workflow management tools like Airflow or Beam to manage ETL', 'Bonus points if...', 'Spark, DBT, and other data tools', '100% employer paid low deductible health plan, dental plan, vision plan, AD&D and life insurance', 'All Full Time Starry Employees Receive', 'Professional development assistance after six months of employment', 'Geographic information systems (GIS) experience', 'Orchestration experience with frameworks like Airflow', '1+ years of data engineering experience or 3+ years of engineering experience', ' An object-oriented programming language like Python or Java Shell scripting, Git, jq, awk, and a variety of other Unix utilities Various flavors of SQL, particularly those with GIS functionality Data warehouse and data lake architecture and design Workflow management tools like Airflow or Beam to manage ETL Stream processing platforms like Kafka or services like Kinesis Virtualization with Docker or an equivalent Spark, DBT, and other data tools Data engineering focused DevOps, observability, CI/CD, etc API and backend application development Unit and integration testing data pipelines ', 'We work hard, so we take care of each other and try to enjoy ourselves along the way. ', 'Catered meals on a weekly basis for employees working in the office', 'What You Will Be Working On', 'Streaming data experience with Kafka, Kinesis, RabbitMQ, etc', 'Qualifications', '12 weeks of 100% paid parental leave for new mothers and fathers after six months of continuous employment', 'Various flavors of SQL, particularly those with GIS functionality', 'About The Data Team', ' Streaming data experience with Kafka, Kinesis, RabbitMQ, etc Orchestration experience with frameworks like Airflow Container experience with frameworks like Docker Geographic information systems (GIS) experience Contributor to open source software ', 'Casual dress, community clubs, annual fitness reimbursement, stocked kitchen and other perks and discounts', 'Container experience with frameworks like Docker', 'About Starry', 'Stream processing platforms like Kafka or services like Kinesis', 'Familiar with CI/CD, Docker or similar, and testing data intensive processes', 'An object-oriented programming language like Python or Java', 'Familiar with SQL, profile query performance, and design database schema']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,GSTV,"Detroit, MI",1 day ago,Be among the first 25 applicants,"['', 'You must have a strong understanding of the following:', 'Hands on experience leveraging cloud based tools (i.e. AWS, Snowflake, Matallion, DOMO, Tableau, Power-Bi, Mulesoft, Boomi or equivalents) to deliver quality, scalable and performant enterprise data and integration infrastructure', 'Creating and maintaining an optimal pipeline and integration architectureAssemble large, transform and load complex sets of data from multiple sources (both internal and external) that meet business needs and support business outcomesIdentify, design and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processesParticipate in peer reviews and quality assurance activitiesAttend meetings with core project team members and key stakeholdersTurn complex concepts and content into consumable functionalityOther responsibilities as directed by the Senior Leadership Team', 'Assemble large, transform and load complex sets of data from multiple sources (both internal and external) that meet business needs and support business outcomes', 'Identify, design and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes', 'Able and patience to work through and resolve tough problems', 'Experience developing and optimizing data flows for large complex data sets.', 'Turn complex concepts and content into consumable functionality', 'Education/Experience', 'Working Conditions', 'Understanding of advertising (campaign delivery) reporting and analytics data a strong plus (but not required)', 'Job Summary', 'Physical Requirements', 'Willingness to admit mistakes and communicate transparently.', 'This position does not have any specific physical requirements.', 'Engage with data providers (SaaS solutions, 3rd party data providers, internal engineering team) and the Insights & Analytics team to create unique custom data and models', 'Creating and maintaining an optimal pipeline and integration architecture', ""Experience working with large complex data sets.\xa0Experience developing and optimizing data flows for large complex data sets.Bachelor's Degree in Computer or Data Science, related field or professional/technology certifications."", 'Remote until Covid resolution', '\xa0', 'Direct Reports', '\xa0None', 'Responsibilities', '\xa0This position may include occasional night or weekend work to support launches.', 'Proven ability to learn new technologies and skills - constantly curious', 'Other responsibilities as directed by the Senior Leadership Team', 'Qualifications', 'Experience working with large complex data sets.\xa0', 'Participate in peer reviews and quality assurance activities', 'The role will require hands on capabilities to execute the strategy with skills leveraging cloud based solutions such as Snowflake (our data lake), integration platforms (Dell-Boomi or Mulesoft) and BI Tools (DOMO preferred but Tableau will apply).\xa0', ""Bachelor's Degree in Computer or Data Science, related field or professional/technology certifications."", 'Proven ability to translate complex data-related customer (and user)\xa0needs into value-delivering products.', 'The Data Solutions Department is a new function at GSTV with the mandate to define and build an enterprise-wide data capability and culture enabling new data capabilities, services, products, and data rich insights and decision capabilities.', 'In this new role you will be supporting GSTV media operations, retailer operations, sales, finance, media and advertising insights and analytics.', 'Celebrate victories and encourage iterative improvement within the team.', 'Proven interpersonal skills and ability to work cross-functionally.', 'Strong proficiency with the data tools and/or data languages (SQL, R or Python) to quickly and efficiently extract, transform, publish and visualize/present to deliver solutions', 'Hands on experience leveraging cloud based tools (i.e. AWS, Snowflake, Matallion, DOMO, Tableau, Power-Bi, Mulesoft, Boomi or equivalents) to deliver quality, scalable and performant enterprise data and integration infrastructureStrong proficiency with the data tools and/or data languages (SQL, R or Python) to quickly and efficiently extract, transform, publish and visualize/present to deliver solutionsProven ability to translate complex data-related customer (and user)\xa0needs into value-delivering products.Proven interpersonal skills and ability to work cross-functionally.Willingness to admit mistakes and communicate transparently.Proven ability to learn new technologies and skills - constantly curiousAble and patience to work through and resolve tough problemsCelebrate victories and encourage iterative improvement within the team.Delivery in an Agile environment applying lean principles and rapid prototypingEngage with data providers (SaaS solutions, 3rd party data providers, internal engineering team) and the Insights & Analytics team to create unique custom data and modelsUnderstanding of advertising (campaign delivery) reporting and analytics data a strong plus (but not required)', 'Attend meetings with core project team members and key stakeholders', 'Delivery in an Agile environment applying lean principles and rapid prototyping']",Mid-Senior level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Senior Data Engineer - Finance Data Engineering,Netflix,"Los Gatos, CA",6 days ago,76 applicants,"['', 'SQL ', 'business complexity', 'working independently', 'accuracy is critical', 'earnings release', 'Netflix Culture', 'You LOVE data of all sorts, big and small! You enjoy helping teams push the boundaries of extracting business insights from our data.', '200m paid memberships', 'better metrics', 'Location of work', 'fully-remote candidates ', 'build reusable components', 'Financial Accounting teams', 'one major language ', ' elegant insights', 'With 8.5m paid net additions in Q4, we crossed the ', 'data pipelining', 'You have preferably worked with Financial Accounting teams and understand their terminology and processes. You can help Netflix meet the compliance requirements of these teams (e.g., SOX)', ' mark. ', ""You are proficient in SQL (any variant) and at least one major language (e.g., Java, Scala, Python). You strive to write beautiful code, and you're comfortable with picking up new technologies."", 'You have a strong background in data pipelining, distributed data processing, software engineering components, and data modeling concepts. ', 'compliance requirements', 'reliable distributed data pipelines', 'candid feedback', 'You have an eye for detail and realize where accuracy is critical. You like to spark joy in internal partners with high-quality data products that are well documented, modeled, and easy to understand', 'forecasts ', 'Learn more', 'grew operating profit 76%', 'interview process', 'You relate to and embody many of the aspects of the Netflix Culture. You love working independently while also collaborating and giving/receiving candid feedback.', 'You have strong communication skills to partner with data scientists and engineering stakeholders effectively. You love being the bridge between Finance and Engineering teams.', 'proficient ', '$25 billion in annual revenue', 'business and finance teams', 'big and small', 'remote-work expert ', 'bridge between Finance and Engineering teams', 'You are always looking for opportunities to simplify, automate tasks, and build reusable components reusable across multiple use cases and teams.', 'Who Are You', ""You are proficient in SQL (any variant) and at least one major language (e.g., Java, Scala, Python). You strive to write beautiful code, and you're comfortable with picking up new technologies.You have strong communication skills to partner with data scientists and engineering stakeholders effectively. You love being the bridge between Finance and Engineering teams.You LOVE data of all sorts, big and small! You enjoy helping teams push the boundaries of extracting business insights from our data.You have a strong background in data pipelining, distributed data processing, software engineering components, and data modeling concepts. You are always looking for opportunities to simplify, automate tasks, and build reusable components reusable across multiple use cases and teams.You have an eye for detail and realize where accuracy is critical. You like to spark joy in internal partners with high-quality data products that are well documented, modeled, and easy to understandYou relate to and embody many of the aspects of the Netflix Culture. You love working independently while also collaborating and giving/receiving candid feedback.You have preferably worked with Financial Accounting teams and understand their terminology and processes. You can help Netflix meet the compliance requirements of these teams (e.g., SOX)"", 'thought leadership', 'high-quality data products', 'For the full year, we added a record 37m paid memberships, achieved $25 billion in annual revenue (+24% year over year) and grew operating profit 76% to $4.6 billion', 'beautiful code', 'LOVE data ', 'mission-critical']",Not Applicable,Full-time,Information Technology,Entertainment,2021-03-24 13:05:10
Data Engineer / Analyst,Vision Technology Services,"Baltimore, MD",6 days ago,Be among the first 25 applicants,"['OR', 'Expert in designing complex and semantically rich data structures.', 'Worked with OLTP/data warehouse modeling, and play key role in multiple projects and act as knowledge expert (Technical, Functional).', 'Expertise on NoSQL data modeling using Hackolade tool for databases like MongoDB, HBASE, Hive and JSON Schema.', 'As part of enterprise data architect team, enterprise data modeler is responsible for providing the data architect solutions and develop data models to support OLTP, BI and advanced analytics across the enterprise in cloud database platforms.', 'Extensively used ETL methodologies for supporting data extraction, transformations and loading processing, in a corporate-wide-ETL Solution.', 'Data Engineer / Analyst', ' ', 'Preferred Knowledge/Skills:\xa0', 'As part of enterprise data architect team, enterprise data modeler is responsible for providing the data architect solutions and develop data models to support OLTP, BI and advanced analytics across the enterprise in cloud database platforms.Candidate will work with project leads, business analysts, enterprise data architects, development teams and DBAs to design conceptual data models for relational and non-relational databases.', 'Ability to create dashboards in Tableau or Webfocus using various sources like database, CSV files, XL spreadsheets etc.', 'Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.', 'Responsibilities:', ""Bachelor's degree and a minimum of 5 years of experience requiredCollaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirementsAble to work independently with BA and PM to gather requirements, perform data discovery, come up with data model, work with data engineers to build data pipeline, and work with developers to answer their questions.Extensively used ETL methodologies for supporting data extraction, transformations and loading processing, in a corporate-wide-ETL Solution.Expertise in Conceptual Data Modeling, Logical Data Modeling, Physical Data Modeling, Enterprise Data Warehouse Design, DataMart Design, Metadata, Data Quality, Master Data Management and Master Data/Data Governance using ErwinTechnical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.Ability to understand complex business processes to derive conceptual and logical data models.Lead complex discussions and engagements that may involve multiple project teams from client.Experience designing relational data models, analytical data models and non-relational data models for OLTP and advanced analytic applications.Expert in designing complex and semantically rich data structures.Worked with OLTP/data warehouse modeling, and play key role in multiple projects and act as knowledge expert (Technical, Functional).Ability to optimize and performance tune SQL queriesGood data analysis, problem solving and SQL skills."", 'Ability to understand complex business processes to derive conceptual and logical data models.', 'If interested in learning more about Vision Technology Services and the opportunity, please submit your resume for consideration to careers@vistechs.com.', 'Experience in PostGreSQL / Greenplum database is good to have', 'Excellent communication skills and ability to convey complex topics through effective documentation as well as presentation.', 'Good data analysis, problem solving and SQL skills.', 'Expertise in Conceptual Data Modeling, Logical Data Modeling, Physical Data Modeling, Enterprise Data Warehouse Design, DataMart Design, Metadata, Data Quality, Master Data Management and Master Data/Data Governance using Erwin', 'Education/Experience Requirements:\xa0', 'Job Order Number: 117802 ', 'Experience working in large-scale cloud database environments is a plus.', ""Only candidates with backgrounds who match our client's requested experience will be contacted. Do not take this as a poor reflection on your experience, just a decision for the specific needs of our client's project/job. We look forward to working with you."", 'Preferred Knowledge/Skills:', 'Candidate will work with project leads, business analysts, enterprise data architects, development teams and DBAs to design conceptual data models for relational and non-relational databases.', 'Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements', 'Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards', ""Bachelor's degree and a minimum of 5 years of experience required"", 'Lead complex discussions and engagements that may involve multiple project teams from client.', 'Ability to optimize and performance tune SQL queries', 'Select Apply and attach your resume in Word Format', 'Job Order Number: 117802 (Please reference in call or email) ', 'Education/Experience Requirements:', 'Able to work independently with BA and PM to gather requirements, perform data discovery, come up with data model, work with data engineers to build data pipeline, and work with developers to answer their questions.', 'Currently, Vision Technology Services is seeking a Data Engineer / Analyst to join our team and work directly on client projects. ', 'Experience designing relational data models, analytical data models and non-relational data models for OLTP and advanced analytic applications.', 'Familiar with machine learning and advanced analytic application development.', 'Expertise on NoSQL data modeling using Hackolade tool for databases like MongoDB, HBASE, Hive and JSON Schema.Familiar with machine learning and advanced analytic application development.Experience working in large-scale cloud database environments is a plus.Ability to create dashboards in Tableau or Webfocus using various sources like database, CSV files, XL spreadsheets etc.Experience in PostGreSQL / Greenplum database is good to haveProvide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standardsExcellent communication skills and ability to convey complex topics through effective documentation as well as presentation.']",Mid-Senior level,Full-time,Design,Information Technology and Services,2021-03-24 13:05:10
Data Center Engineer,Cloudflare,"Washington, DC",18 hours ago,Be among the first 25 applicants,"['', ' Collaborating with internal teams (infrastructure, network engineering and SRE). Create documentation and manage remote contractors to complete datacenter installations and upgrades, including hardware manufacturers, datacenter and network providers, logistics partners and other service providers in support of our 120 and growing datacenter location  Build rack elevations, and work with remote contractors to rack and cable infrastructure globally Coordinate installation of cross-connects globally in support of physical network expansion Assist with the definition, documentation and implementation of consistent processes across all region Aggressively seek opportunities to introduce cutting-edge technology and automation solutions that are effective, efficient and scalable in order to improve our ability to deploy and maintain our global infrastructure Limited travel ', 'Must be proactive with proven ability to learn fast and execute on multiple tasks simultaneously', 'Ability to write scripts for internal tool', ' Minimum of 2 years of related data center or Linux systems administration experience Linux/Unix systems administration Basic configuration management tool experience like Saltstack, Chef, Puppet or Ansible Network hardware administration Familiarity with day-to-day tasks and projects common in Data Center Operations Ability to write scripts for internal tool Experience running and improving operational processes in a rapidly changing environment ', 'Basic configuration management tool experience like Saltstack, Chef, Puppet or Ansible', '1.1.1.1', 'Scripting or software development experience in Bash, Python or Go-lang', 'Strong verbal and written communication skills, problem-solving skills, attention to detail, and interpersonal skills', 'Bachelor’s degree; technical background in engineering, computer science, or MIS a plus', 'Familiarity with day-to-day tasks and projects common in Data Center Operations', 'Knowledge of the OSI-model and experience isolating network, hardware and software issues', ' Build rack elevations, and work with remote contractors to rack and cable infrastructure globally', 'Aggressively seek opportunities to introduce cutting-edge technology and automation solutions that are effective, efficient and scalable in order to improve our ability to deploy and maintain our global infrastructure', 'What Makes Cloudflare Special?', 'Ability to manage MS excel and Google spreadsheets', 'Linux/Unix systems administration', 'Previous experience installing / maintaining datacenter (and other IT) infrastructure and DCIM tools', 'Assist with the definition, documentation and implementation of consistent processes across all region', ' Multi-lingual; experience working with infrastructure in multiple countries Comfortable with remote “lights-out” and out-of-band access to data center resources Linux certifications Knowledge of the OSI-model and experience isolating network, hardware and software issues Configuration management systems such as Saltstack, Chef, Puppet or Ansible Scripting or software development experience in Bash, Python or Go-lang Familiarity with load balancing and reverse proxies such as Nginx, Varnish, HAProxy, Apache ', 'Must be a team player', ' Athenian Project ', 'Collaborating with internal teams (infrastructure, network engineering and SRE). Create documentation and manage remote contractors to complete datacenter installations and upgrades, including hardware manufacturers, datacenter and network providers, logistics partners and other service providers in support of our 120 and growing datacenter location', 'Coordinate installation of cross-connects globally in support of physical network expansion', 'About Us', 'Limited travel', 'Familiarity with load balancing and reverse proxies such as Nginx, Varnish, HAProxy, Apache', 'About The Department', 'Project Galileo', 'Network hardware administration', 'Multi-lingual; experience working with infrastructure in multiple countries', 'Minimum of 2 years of related data center or Linux systems administration experience', 'Comfortable with remote “lights-out” and out-of-band access to data center resources', 'Direct experience executing on datacenter / infrastructure projects with many moving parts', ' Bachelor’s degree; technical background in engineering, computer science, or MIS a plus Direct experience executing on datacenter / infrastructure projects with many moving parts Previous experience installing / maintaining datacenter (and other IT) infrastructure and DCIM tools Experience running and improving operational processes in a rapidly changing environment Strong verbal and written communication skills, problem-solving skills, attention to detail, and interpersonal skills Must be proactive with proven ability to learn fast and execute on multiple tasks simultaneously Ability to manage MS excel and Google spreadsheets Comfortable handling basic program management responsibilities (prioritization, planning, scheduling, status reporting) such as JIRA Experience managing remote contractors Must be a team player ', 'Experience managing remote contractors', 'Configuration management systems such as Saltstack, Chef, Puppet or Ansible', 'Bonus Points', 'Other Responsibilities May Include', 'Required Experience', 'Experience running and improving operational processes in a rapidly changing environment', 'Comfortable handling basic program management responsibilities (prioritization, planning, scheduling, status reporting) such as JIRA', 'Examples Of Desirable Skills, Knowledge And Experience', 'Path Forward Partnership', 'Linux certifications']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Huxley,Greater Chicago Area,2 days ago,62 applicants,"['', 'Benefits from day one including comprehensive health, dental, vision', 'Experience with OOP Python programming and automation', 'Extensive career development opportunities and programs', 'a Built in Chicago’s “Top 100 Best Places to Work”', 'Ability to work for any US employer is required. This position will NOT offer Visa Sponsorship or accept C2C.', '6% guaranteed 401k employer contribution (even if you don’t contribute)', 'Design and build out ETL data pipelines including all steps from initial information gathering to execution', 'Sthree US is acting as an Employment Agency in relation to this vacancy.', 'Requirements', 'Minimum 2+ years professional experience building data pipelines using PythonStrong experience with ETL and data warehousingExperience with OOP Python programming and automation', 'A Chicago based eCommerce organization is looking for a Data Engineer to join their growing, high-performing engineering team in the Chicago office. As an organization, they have been named a Built in Chicago’s “Top 100 Best Places to Work”, a Chicago Tribune’s “Top Workplace”, and was recently awarded a “Top Workplaces USA Award” for 2021. As a member of the engineering team, you will collaborate with multiple stakeholders and focus on the design, development, implementation, and operations of a new analytics platform.', 'Design and build out ETL data pipelines including all steps from initial information gathering to executionCollaborate with Data QA Engineers to improve Data Quality Program including monitoring, alerting, triaging, and remediating as necessaryCreate clear and accurate documentation outlining development processes', 'If this position interests you, please apply as soon as possible!', 'Responsibilities', 'Collaborate with Data QA Engineers to improve Data Quality Program including monitoring, alerting, triaging, and remediating as necessary', 'Minimum 2+ years professional experience building data pipelines using Python', 'Paid parental leave and adoption assistance', '“Top Workplaces USA Award”', 'Chicago Tribune’s “Top Workplace”,', 'Strong experience with ETL and data warehousing', 'Data Engineer', 'Create clear and accurate documentation outlining development processes', 'Perks', '6% guaranteed 401k employer contribution (even if you don’t contribute)Benefits from day one including comprehensive health, dental, visionPaid parental leave and adoption assistanceExtensive career development opportunities and programs']",Mid-Senior level,Full-time,Information Technology,Business Supplies and Equipment,2021-03-24 13:05:10
Data Engineer,PlayStation,"San Diego, CA",3 weeks ago,158 applicants,"['', ' Sensitive/Protected Data. ', 'Categories of personal information we collect from you', ' ', 'Strong verbal and written communication skills, and be able to work with others at all levels, effective at working with geographically remote and culturally diverse teams.', 'You will be expected to build a deep understanding of PlayStation data and help data consumers drive business value from the data', 'Indirect identifiers such as a government ID, your Social Security, work permit or passport #.', 'You will participate in product road-map discussions and identify key areas for improvement.', 'Contact information such as your email address, mailing address, telephone number.', ' Professional or job position-related information , including your past professional experience, references; background verification; talent management and assessment; information regarding any conflicts of interests; and the terms and conditions of your job offer.', 'Possess the drive and passion for quality with the ability to inspire, excite and motivate other team members.', ' Sensitive/Protected Data. During the recruitment process, you may (voluntarily) provide us with your ethnicity, gender, military service information, or physical or mental health information, as well as your national origin and citizenship.', 'Experience with cloud-based services at enterprise scale such as AWS', 'Preferred:', 'Qualifications:', ' Experience and working knowledge of SQL. Ability to write complex, highly performing SQL queries and optimize performance of existing queries preferred Experience with cloud-based services at enterprise scale such as AWS Experience with cloud-based data warehousing solutions such as Snowflake or Redshift Experience with AWS Big Data technologies such as Kinesis, Lambda, Firehose, Dynamo DB, Athena/Presto. ', ' You will be part of a team building data and software solutions that enable frictionless data delivery, enabling business value to our internal business units You will be expected to build a deep understanding of PlayStation data and help data consumers drive business value from the data You will play a role in design, development, automation, quality, and operations for the PlayStation data platform Mentors and inspires to effectively deliver awesome software/data solutions! You will participate in product road-map discussions and identify key areas for improvement. ', ' Non-public education information ', 'Generally, We Obtain This Information Through Our Recruiting Team', ' Identification and contact information ', ' Non-public education information , including information about your education records, such as grades and transcripts.', 'You will be part of a team building data and software solutions that enable frictionless data delivery, enabling business value to our internal business units', 'Strong foundation in data engineering and software design.', '  Identification and contact information  Direct identifiers such as your first and last name. Indirect identifiers such as a government ID, your Social Security, work permit or passport #. Contact information such as your email address, mailing address, telephone number.   Other information about you or that can be associated with you such as:  Sensitive/Protected Data. During the recruitment process, you may (voluntarily) provide us with your ethnicity, gender, military service information, or physical or mental health information, as well as your national origin and citizenship.  Professional or job position-related information , including your past professional experience, references; background verification; talent management and assessment; information regarding any conflicts of interests; and the terms and conditions of your job offer.  Non-public education information , including information about your education records, such as grades and transcripts.', 'Direct identifiers such as your first and last name.', 'BS Degree in Engineering, Computer Science or equivalent experience', 'Experience and working knowledge of SQL. Ability to write complex, highly performing SQL queries and optimize performance of existing queries preferred', 'PRIVACY NOTICE TO SIE LLC’S JOB APPLICANTS', 'Responsibilities', 'Mentors and inspires to effectively deliver awesome software/data solutions!', ' Professional or job position-related information ', 'Experience with cloud-based data warehousing solutions such as Snowflake or Redshift', '  Other information about you or that can be associated with you such as:  Sensitive/Protected Data. During the recruitment process, you may (voluntarily) provide us with your ethnicity, gender, military service information, or physical or mental health information, as well as your national origin and citizenship.  Professional or job position-related information , including your past professional experience, references; background verification; talent management and assessment; information regarding any conflicts of interests; and the terms and conditions of your job offer.  Non-public education information , including information about your education records, such as grades and transcripts.', 'Strong experience in advanced software development (i.e. Python, Scala, Java programming), design, and analysis.', ' BS Degree in Engineering, Computer Science or equivalent experience Strong experience in advanced software development (i.e. Python, Scala, Java programming), design, and analysis. Strong foundation in data engineering and software design. Possess the drive and passion for quality with the ability to inspire, excite and motivate other team members. Strong verbal and written communication skills, and be able to work with others at all levels, effective at working with geographically remote and culturally diverse teams. ', 'Experience with AWS Big Data technologies such as Kinesis, Lambda, Firehose, Dynamo DB, Athena/Presto.', ' Other information about you or that can be associated with you such as: ', 'Must Have:', 'You will play a role in design, development, automation, quality, and operations for the PlayStation data platform']",Not Applicable,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Data Engineer - REMOTE,"Medable, Inc","Palo Alto, CA",2 days ago,131 applicants,"['', 'Problem-solving aptitude', 'Advanced software development ', 'Enhance data collection procedures to include information that is relevant for building analytic systems', 'Develop C++ libraries/frameworks to support data science infrastructure and applications', 'Develop C++ libraries/frameworks to support data science infrastructure and applicationsDevelop distributed software systems for training large-scale machine learning modelsDevelop custom machine learning algorithms from lower-level building blocksCreate abstractions and easy-to-use interfaces to lower-level machine learning tools and software componentsSelect features, build and optimize classifiers using machine learning techniquesData mining using state-of-the-art methods such as natural language processingEnhance data collection procedures to include information that is relevant for building analytic systemsProcess, clean, and verify the integrity of data used for analysis', 'Select features, build and optimize classifiers using machine learning techniques', 'Process, clean, and verify the integrity of data used for analysis', '5+ of working experience in Computer Science or a combination of education and experienceBachelor’s degree in Computer Science, Artificial Intelligence, Engineering, or a related fieldAdvanced skills in R programming language, C++, Python, Java, Node.JSAdvanced software development Expertise in machine learning methods and data analysisCritical thinking and problem-solving skillsPassion for machine-learning and researchExperience in data miningAnalytical mind and business acumenProblem-solving aptitudeExcellent communication skills', 'Expertise in machine learning methods and data analysis', 'Analytical mind and business acumen', 'Experience in data mining', 'Bachelor’s degree in Computer Science, Artificial Intelligence, Engineering, or a related field', 'Data mining using state-of-the-art methods such as natural language processing', 'Develop custom machine learning algorithms from lower-level building blocks', '5+ of working experience in Computer Science or a combination of education and experience', 'Create abstractions and easy-to-use interfaces to lower-level machine learning tools and software components', 'Qualifications', 'Excellent communication skills', 'Passion for machine-learning and research', 'Company Description', 'Critical thinking and problem-solving skills', 'Advanced skills in R programming language, C++, Python, Java, Node.JS', 'Job Description', 'Develop distributed software systems for training large-scale machine learning models']",Entry level,Full-time,Information Technology,Internet,2021-03-24 13:05:10
Data Analyst Engineer,"FAR Consulting, Inc.",Atlanta Metropolitan Area,2 days ago,171 applicants,"['', 'Ability to gather and document business requirements and translate them to technical requirements', 'We are seeking a Data Analyst Engineer to build analysis from the ground up. The ideal candidate will embed analytics into applications and research new solutions to store and present information.', 'Analytical scripting language skills, such as Python, R, SAS', 'Develop data strategy and data analysisDevelop analytics using power BI, Looker or TableauEmbed analytics into web applications hosted on the cloud (AWS, GCP, Azure)Participate in the design, development and implementation of end-to-end Business Intelligence Solutions', 'Professional certifications in data technologies (BI tools, cloud technologies)', 'Preferred Qualifications', 'Bachelor\'s Degree in Information Systems, Computer Science or Computer Engineering or other related fieldsAt least 2-5 years of hands-on data experienceExperience developing embedded analytics in software applicationsSolid interpersonal and communication skills (written and verbal) to technical and non-technical audience of wide variety of levels including client-facing senior managementAnalytical scripting language skills, such as Python, R, SASData Visualization tools: Tableau, Power BI, LookerFamiliarity with the scripting data and machine learning ecosystems - (Jupyter Notebooks, scikit-learn, SciPy, NumPy, pandas, Matplotlib, TensorFlow, etc.)Experience with wide array of analytical approaches (correlation analysis, predictive and explanatory modeling, data mining, unsupervised clustering, analysis of unstructured data)Experience with SQL via exposure to RDBMS’s such as Amazon Redshift, Google Cloud BigQuery, SQL Server, Oracle, Teradata, postgres, etc.Familiarity with cloud technologies Data warehouse (BigQuery, RedShift) / cloud storage (GS, S3) / app engine (GCP App Engine, AWS Elastic Beanstalk)Experience with ""Big Data"" environmentsAbility to gather and document business requirements and translate them to technical requirements', ""Master's degree in Management Information Systems, Analytics or Data Engineering"", 'Data Visualization tools: Tableau, Power BI, Looker', 'Experience developing embedded analytics in software applications', '(No Corp-to-Corp)', 'Participate in the design, development and implementation of end-to-end Business Intelligence Solutions', ""Master's degree in Management Information Systems, Analytics or Data EngineeringProfessional certifications in data technologies (BI tools, cloud technologies)"", 'Note: This role is available as a 1099 contractor position or W2 position with benefits', 'As a Data Scientist, you will be part of the technology team responsible for data modeling, application development, technical product assistance and tuning to meet performance and functional requirements. The Data Analyst Engineer will play a key role in building state-of-the-art business intelligence solutions that deliver business insights and support data-driven decision making.', ""Bachelor's Degree in Information Systems, Computer Science or Computer Engineering or other related fields"", 'Solid interpersonal and communication skills (written and verbal) to technical and non-technical audience of wide variety of levels including client-facing senior management', 'Experience with wide array of analytical approaches (correlation analysis, predictive and explanatory modeling, data mining, unsupervised clustering, analysis of unstructured data)', 'Embed analytics into web applications hosted on the cloud (AWS, GCP, Azure)', 'Experience with SQL via exposure to RDBMS’s such as Amazon Redshift, Google Cloud BigQuery, SQL Server, Oracle, Teradata, postgres, etc.', 'Develop data strategy and data analysis', 'Familiarity with cloud technologies Data warehouse (BigQuery, RedShift) / cloud storage (GS, S3) / app engine (GCP App Engine, AWS Elastic Beanstalk)', 'Essential Functions', 'Basic Qualifications', 'Familiarity with the scripting data and machine learning ecosystems - (Jupyter Notebooks, scikit-learn, SciPy, NumPy, pandas, Matplotlib, TensorFlow, etc.)', 'Develop analytics using power BI, Looker or Tableau', 'Experience with ""Big Data"" environments', 'At least 2-5 years of hands-on data experience']",Entry level,Full-time,Information Technology,Management Consulting,2021-03-24 13:05:10
Data Engineer,Motion Recruitment,"Rockville, MD",,N/A,"['', 'Person interest in video gaming is a plus', 'The Offer', 'Scala', 'Bachelor degree with 3+ years of experience or equivalentPythonJavaSQLC/C++SparkScalaAWS and/or Microsoft Azure for data managementWorking experience with Scrum', 'Applicants must be currently authorized to work in the United States on a full-time basis now and in the future.', 'Experience working with technical and business teams for data sourcing', 'AWS and/or Microsoft Azure for data management', 'Must be a critical thinker and self-starter', 'Python', 'Strong analytical, verbal and written communications skillsExperience working with technical and business teams for data sourcingMust be a critical thinker and self-starterAbility to work in fast-paced nature of a high-growth organizationUnderstanding of Free to Play/Micro TransactionPerson interest in video gaming is a plus', 'Java', 'Strong analytical, verbal and written communications skills', 'Spark', 'SQL', 'Ability to work in fast-paced nature of a high-growth organization', 'Desired Skills & Experience', 'C/C++', 'Required Skills & Experience', 'Bachelor degree with 3+ years of experience or equivalent', 'Competitive Salary: Up to $120K/year + Benefits', 'Understanding of Free to Play/Micro Transaction', 'Working experience with Scrum', ""This is an innovative and exciting video game publishing company in the heart of Rockville, Maryland. The position we are looking to fill is for a Data Engineer who will be working with an Enterprise BI team supporting the data pipeline process for ingesting data at a large scale. While being a part of an award-winning development team, you will be working directly with Data Modelers, Enterprise Architect, and Analysts to ensure business requirements are being met. It is essential that you can straddle differing subject areas such as in-game vs. business data sources. This is an established and reputable growing company that values independent, dynamic and strategic thinkers who are ready to analyze, troubleshoot and resolve complex business and technical problems. You must be able to facilitate the creation of data pipeline processes to move data from enterprise data sources such as relational databases and log files. The company's mission is to bring together a force of creative world-class game developers,  programmers, artists and designers, and talent from traditional media.""]",Associate,Full-time,Engineering,Computer Games,2021-03-24 13:05:10
Data Engineer,Zenith,Atlanta Metropolitan Area,17 hours ago,Be among the first 25 applicants,"['', '•\xa0\xa0\xa0Write ad-hoc queries based on schema knowledge for various application requirements', ""At Zenith we are dedicated to creating a sustainable advantage for our clients, providing them with a quantifiable return from their communications investment that outstrips their competitors.\xa0Our focus on ROI ensures that clients' budgets are invested, not simply spent. We believe the most important consideration for any campaign is a demonstrably effective outcome.\xa0We are committed to developing close partnerships with all our clients and to delivering a service that always exceeds expectations.\xa0"", '•\xa0\xa0\xa0Design, development, and enhancement of data models and workflows.', '•\xa0\xa0\xa0Experience executing solutions with Salesforce Marketing Cloud and leveraging APIs and ETL tools.', '•\xa0\xa0\xa0Excellent communication and intra-personal skills', '•\xa0\xa0\xa0Self-motivated and a self-starter with strong ability to multitask projects/tasks effectively', ""Above all, we know that ideas and imaginative solutions are vital to make our clients' brands stand out from the crowd; connecting with consumers in a striking and persuasive manner."", '•\xa0\xa0\xa0Responsible for system performance of the data tier and the reliabitlity of data feeds across the enterprise system.', '•\xa0\xa0\xa0Modify and improve data engineering processes to handle ever larger, more complex, and more types of data sources and pipelines\xa0', '•\xa0\xa0\xa0Expertise in SQL, especially within cloud-based data warehouses MS Azure and Amazon Redshift', 'All your information will be kept confidential according to EEO guidelines.', ""•\xa0\xa0\xa0Bachelor's degree in information technology, computer science, or related field"", '\ufeffAdditional Information', '•\xa0\xa0\xa0Contribute to on call support and to operational support documentation', '•\xa0\xa0\xa01+ years of Python coding experience', 'Qualifications', '•\xa0\xa0\xa0Write application and workflow based on business requirements or user stories, architectural requirements, and existing code.\xa0', '•\xa0\xa0\xa0Estimate and plan development work, track and report on task progress, and deliver work on schedule', 'About Zenith:', '•\xa0\xa0\xa01+ years of experience with data technologies including Hadoop(Cloudera) , HBase, Spark, Hive, RedShift etc.', 'Job Description']",Associate,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer,Honeywell,"Atlanta, GA",4 weeks ago,50 applicants,"['', 'Hands-on experience with SQL database design', 'Identify opportunities for data acquisition', 'You Must Have ', 'We Value', 'Intermediate Business modeling skills using Excel or BI Tool', 'Financial modeling ability', 'Category: Information Technology', 'Build algorithms and prototypes', 'Responsibilities Include', 'JOB ID: req263748Category: Information TechnologyLocation: 715 Peachtree Street, N.E.,Atlanta,Georgia,30308,United StatesExempt', 'Ability to operate independently and proactively and drive the business forwards', 'Basic familiarity with Machine Learning concepts', '5 years of experience with at least one statistical programming language: Python, R, SAS, Julia (Python Preferred)', 'Dive deep into data to uncover trends, data quality, and data completeness.', 'Prepare data for prescriptive and predictive modeling', 'Good Understanding of data structures, data modeling, data mining, and segmentation techniques', ' Bachelor’s degree 5 years of experience with at least one statistical programming language: Python, R, SAS, Julia (Python Preferred) 5 years of experience using SQL with any database 2 years of technical expertise with data models, data mining, and segmentation techniques ', 'Location: ', 'Location: 715 Peachtree Street, N.E.,Atlanta,Georgia,30308,United States', 'Bachelor’s degree', 'Conduct complex data analysis and Prepare trend analysis and report results to Team', 'Time management, organization, and prioritization skills ', 'Great numerical and analytical skills', 'Explore ways to enhance data quality and reliability', 'Collaborate with data scientists and architects on several projects', 'Data Engineer ', 'Build data systems and pipelines and Evaluate business needs and objectives', 'JOB ID: req263748', 'Supporting initiatives for data integrity and normalization', 'Category: ', 'Exempt', 'Excellent oral, written, and presentation skills with the ability to deal tactfully, confidently, and ethically with both internal and external customers', 'Combine raw information from different sources', 'Utilize statistical modelling techniques where appropriate to drive business decisions', '5 years of experience using SQL with any database', '2 years of technical expertise with data models, data mining, and segmentation techniques ', 'Advanced statistical knowledge', 'Analyze Business KPI across different functions Engineering, ISC, Marketing, Finance, and more as needed', ' Hands-on experience with SQL database design Great numerical and analytical skills Previous experience to work as Data Engineer Ability to operate independently and proactively and drive the business forwards Intermediate Business modeling skills using Excel or BI Tool Excellent oral, written, and presentation skills with the ability to deal tactfully, confidently, and ethically with both internal and external customers Time management, organization, and prioritization skills  Financial modeling ability Basic familiarity with Machine Learning concepts Advanced statistical knowledge ', 'Previous experience to work as Data Engineer', 'Analyze Business KPI across different functions Engineering, ISC, Marketing, Finance, and more as neededBuild data systems and pipelines and Evaluate business needs and objectivesDive deep into data to uncover trends, data quality, and data completeness.Conduct complex data analysis and Prepare trend analysis and report results to TeamUtilize statistical modelling techniques where appropriate to drive business decisionsPrepare data for prescriptive and predictive modelingBuild algorithms and prototypesCombine raw information from different sourcesExplore ways to enhance data quality and reliabilityIdentify opportunities for data acquisitionCollaborate with data scientists and architects on several projectsGood Understanding of data structures, data modeling, data mining, and segmentation techniquesSupporting initiatives for data integrity and normalization', 'JOB ID: ']",Entry level,Full-time,Information Technology,Medical Devices,2021-03-24 13:05:10
Data Engineer - McKinsey Digital,McKinsey & Company,"New York, NY",2 days ago,166 applicants,"['', 'Knowledge of agile software development process and familiarity with performance metric tools', 'Willingness to travel', 'Solid grasp of ETL across various platforms', 'Strong command of English language (both verbal and written)', 'Knowledge of web application development technologies such as Ruby on Rails, Java, UNIX, HTML, CSS, Perl, or PHP is a plus', ""Who You'll Work With"", 'Strong analytical and problem-solving skills paired with the ability to develop creative and efficient solutions; tolerance in dealing with bad quality data', 'Ability to work efficiently with a solid sense for setting priorities', ""What You'll Do"", 'Distinct customer focus and quality mindset', 'Excellent interpersonal, leadership and communication skills', 'Experienced with data modeling, design patterns, building highly scalable and secured solutions, distributed systems', 'Ability to see from and sell to multiple viewpoints', ""Bachelor's degree in computer science or engineering; master's degree preferredExperienced on Big Data platforms and tools like Hadoop, hbase, CouchDB, hive, Pig, Spark, etc.Experienced with data modeling, design patterns, building highly scalable and secured solutions, distributed systemsSolid grasp of ETL across various platformsKnowledge of agile software development process and familiarity with performance metric toolsKnowledge of web application development technologies such as Ruby on Rails, Java, UNIX, HTML, CSS, Perl, or PHP is a plusStrong analytical and problem-solving skills paired with the ability to develop creative and efficient solutions; tolerance in dealing with bad quality dataProficiency in visualization tool (Tableau, D3, etc.)Distinct customer focus and quality mindsetAbility to see from and sell to multiple viewpointsAbility to work at an abstract level and build consensus; ability to work both independently and with a teamAbility to work efficiently with a solid sense for setting prioritiesAbility to guide own learning and contribute to domain knowledge buildingExcellent interpersonal, leadership and communication skillsStrong command of English language (both verbal and written)Willingness to travel"", 'Qualifications', 'Experienced on Big Data platforms and tools like Hadoop, hbase, CouchDB, hive, Pig, Spark, etc.', 'Proficiency in visualization tool (Tableau, D3, etc.)', ""Bachelor's degree in computer science or engineering; master's degree preferred"", 'Ability to work at an abstract level and build consensus; ability to work both independently and with a team', 'Ability to guide own learning and contribute to domain knowledge building']",Associate,Full-time,Consulting,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Brooksource,"Milwaukee, WI",,N/A,"['• Plans data integration process by developing common definitions of sourced data; designing common keys in physical data structure; establishing data integration specifications; examining data applications; examining data models and data warehouse schema; determining best-fit data interchange methods; assessing middleware tools for data integration, transformation, and routing; developing project scope and specifications; identifying factors that negatively impact integration; forecasting resource requirements; establishing delivery timetables.', '', 'Brooksource + Client Benefits:', '• Experience with object-oriented/object function scripting languages: Python, Java, C++, etc.', 'Responsible for: ', 'Medical/Dental/Vision plans available', '• Experience with ETL/ELT tools. (SSIS, Informatica, Pentaho).', '• Improves data integration by designing and evaluating new data interchange formats; improving physical design; rewriting data policy, standards, and procedures. ', 'Competitive compensation, weekly paycheck', 'Requires:', '• Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.', '• Experience with relational SQL databases.', 'Contract to Hire', '\xa0', 'Competitive compensation, weekly paycheckMedical/Dental/Vision plans availablePTO for the 6 national holidays401K eligibility after 1 yearWork with a company utilizing best of breed technology; come grow your skillset with us!', '• 3+ years of experience in a data analyst/data integration role.', '• A successful history of manipulating, processing and extracting value from large disconnected data sets.', '• Experience supporting and working with cross-functional teams in a dynamic environment.', '• Delivers data solutions by implementing shared databases; integrating data shared across legacy, new development, and purchased package environments; developing system modification specifications; mapping data; establishing interfaces; developing and modifying functions, programs, routines, and stored procedures to export, transform, and load data; meeting performance parameters; resolving and escalating integration issues; coordinating actions among users, operations staff, and outside vendors; recommending adjustments as objectives change; documenting operational procedures and data connections.', 'PTO for the 6 national holidays', 'Milwaukee, WI - Remote', '401K eligibility after 1 year', 'Work with a company utilizing best of breed technology; come grow your skillset with us!', 'Data Integration Engineer', '• Validates data solutions by developing and executing test plans and scenarios including data design, tool design, data extract/transform, networks, and hardware.']",Mid-Senior level,Full-time,Information Technology,Banking,2021-03-24 13:05:10
Data Engineer,JPMorgan Chase & Co.,"Brooklyn, NY",4 weeks ago,Be among the first 25 applicants,"['', 'Experience with stream processing platforms such as Kafka or Dataflow', 'Proficiency in one or more modern programming languages', 'Develop strategy to provide proactive solutions and enable stakeholders to extract insights and value from data', 'Familiarity with data transformation and collection tools such as Pentaho, Informatica', 'Expert level skills in Python its standard library and its package', 'Design best practices for data processing, data modeling and warehouse development throughout our team and group', 'Familiarity with AWS ecosystem including S3, Glue, Redshift, Kinesis, EMR, EC2, SQS', 'Organization', 'Hands-on experience with data warehouse / data lake architectures based on Hadoop, Redshift or Snowflake', 'Build large-scale batch, ETL and real-time data pipelines using cloud and on-premises data technologies, such as Redshift, Python, Spark, PySpark, and Apache Kafka', 'Knowledge of industry-wide technology trends and best practices', 'Understand end to end data interactions and dependencies across complex data pipelines and data transformation and how they impact business decisions.', 'About Us', 'Responsibilities', 'Experience with container technologies such as Docker and Kubernetes', 'Familiarity of microservices stack based on AWS Lambdas. Elastic Search, Spring Boot, NodeJS', 'Advanced knowledge of application, data, and infrastructure architecture disciplines', 'Understanding of software skills such as business analysis, development, maintenance, and software improvement', 'Advanced level skills in SQL, data integration, data modeling and data architecture', 'Knowledge of data columnar and serialization formats such as JSON, XML, Parquet, Avro', 'Working proficiency in developmental toolsets', 'Hands-on experience building a data warehouse and data pipelines using Java, Python or Scala in a data intensive engineering role', 'BS/BA degree or equivalent experience', ' Build large-scale batch, ETL and real-time data pipelines using cloud and on-premises data technologies, such as Redshift, Python, Spark, PySpark, and Apache Kafka Design best practices for data processing, data modeling and warehouse development throughout our team and group Develop strategy to provide proactive solutions and enable stakeholders to extract insights and value from data Understand end to end data interactions and dependencies across complex data pipelines and data transformation and how they impact business decisions. ', ' BS/BA degree or equivalent experience Advanced knowledge of application, data, and infrastructure architecture disciplines Understanding of architecture and design across all systems Working proficiency in developmental toolsets Knowledge of industry-wide technology trends and best practices Ability to work in large, collaborative teams to achieve organizational goals Passionate about building an innovative culture Proficiency in one or more modern programming languages Understanding of software skills such as business analysis, development, maintenance, and software improvement Advanced level skills in SQL, data integration, data modeling and data architecture Expert level skills in Python its standard library and its package Hands-on experience building a data warehouse and data pipelines using Java, Python or Scala in a data intensive engineering role Hands-on experience with data warehouse / data lake architectures based on Hadoop, Redshift or Snowflake Experience with workflow orchestration tools such as Apache Airflow, Autosys Familiarity with data transformation and collection tools such as Pentaho, Informatica Experience with stream processing platforms such as Kafka or Dataflow Knowledge of data columnar and serialization formats such as JSON, XML, Parquet, Avro Experience with container technologies such as Docker and Kubernetes Experience with CI/CD systems e.g. Jenkins and automation / DevOps best practices Familiarity with AWS ecosystem including S3, Glue, Redshift, Kinesis, EMR, EC2, SQS Familiarity of microservices stack based on AWS Lambdas. Elastic Search, Spring Boot, NodeJS ', 'Passionate about building an innovative culture', 'Understanding of architecture and design across all systems', 'Experience with CI/CD systems e.g. Jenkins and automation / DevOps best practices', 'Experience with workflow orchestration tools such as Apache Airflow, Autosys', 'Ability to work in large, collaborative teams to achieve organizational goals', 'Job Description']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Cisco,"Raleigh, NC",5 days ago,77 applicants,"['', 'Use a variety of tools to extract and analyze customer and market data', 'Have a toolkit of technical skills enabling data exploration, dynamic analysis, and sophisticated analysis.', 'You are a game-changer with innovation on your mind', '3-5 years in a data analyst/engineer role or similar role with technical critical thinking capabilities', 'What you will do', 'Minimum Qualifications', 'Cloud DBs (Snowflake, AWS, etc.)', ""Bachelor's Degree in CS, Engineering, MIS, a related field or equivalent work experience"", 'Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy', 'Communicate insights and provide solutions that have proven results.', "" Bachelor's Degree in CS, Engineering, MIS, a related field or equivalent work experience Demonstrated ability in cybersecurity audit and/or compliance Strong technical, analytical, interpersonal, communication and writing skills. Data Visualization Tools Tableau (ideal), PowerBI, DOMO, Chartion Cloud DBs (Snowflake, AWS, etc.) Excel and SQL (Outstanding proficiency) Salesforce API extractions and ETL Scripts Deep Statistics (SPSS, SAS) & basic understanding of ML models Ability to work both independently and within a global team environment Self-starter, quick-learner, and pro-active problem-solving skills. "", 'Excel and SQL (Outstanding proficiency)', ' Use a variety of tools to extract and analyze customer and market data Report your findings with data visualizations that are easy to understand. Communicate insights and provide solutions that have proven results. Work with development teams to implement your solutions. Grasp data APIs, integration and automation. Have a toolkit of technical skills enabling data exploration, dynamic analysis, and sophisticated analysis. ', 'You are excited by a “start-up” environment and enjoy tackling new challenges', ' You are a game-changer with innovation on your mind You enjoy a fast-paced environment that requires high levels of collaboration. You are highly proficient with data including web analytics, analyzing sets of data, and visualization of insights You are excited by a “start-up” environment and enjoy tackling new challenges Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy 3-5 years in a data analyst/engineer role or similar role with technical critical thinking capabilities ', 'Who you are', 'Grasp data APIs, integration and automation.', 'Data Visualization Tools Tableau (ideal), PowerBI, DOMO, Chartion', 'Strong technical, analytical, interpersonal, communication and writing skills.', 'Salesforce', 'Self-starter, quick-learner, and pro-active problem-solving skills.', 'You enjoy a fast-paced environment that requires high levels of collaboration. You are highly proficient with data including web analytics, analyzing sets of data, and visualization of insights', 'Ability to work both independently and within a global team environment', 'Deep Statistics (SPSS, SAS) & basic understanding of ML models', 'API extractions and ETL Scripts', 'Work with development teams to implement your solutions.', 'Demonstrated ability in cybersecurity audit and/or compliance', 'Approval Job ID STO204 ', 'Why Cisco', 'Report your findings with data visualizations that are easy to understand.']",Not Applicable,Full-time,Information Technology,Computer Hardware,2021-03-24 13:05:10
Data Engineer,Hulu,"Santa Monica, CA",2 weeks ago,160 applicants,"['', 'Good understanding of SQL Engines and able to conduct advanced performance tuning', 'Develop and maintain Dashboards/reports using Tableau and Looker', '1+ years of experience with workflow management tools (Airflow, Oozie, Azkaban, UC4)', 'Partner with technical and non-technical colleagues to understand data and reporting requirements.Work with Engineering teams to collect required data from internal and external systems.Design table structures and define ETL strategy to build performant Data solutions that are reliable and scalable in a fast growing data ecosystem.Develop Data Quality checks for source and target data sets. Develop UAT plans and conduct QA.Develop and maintain ETL routines using ETL and orchestration tools such as Airflow, Luigi and Jenkins.Document and publish Metadata and table designs to facilitate data adoption.Perform ad hoc analysis as necessary.Perform SQL and ETL tuning as necessary.Develop and maintain Dashboards/reports using Tableau and LookerCoordinate and resolve escalated production support incidents in Tier 2 support rotationCreate runbooks and actionable alerts as part of the development process', 'Coordinate and resolve escalated production support incidents in Tier 2 support rotation', 'Excellent conceptual and analytical reasoning competencies.', 'What To Bring', 'Ability to think strategically, analyze and interpret market and consumer information.', '2+ years of relevant Professional experience.1+ years work experience implementing and reporting on business key performance indicators in data warehousing environments. Strong understanding of data modeling principles including Dimensional modeling, data normalization principles etc.1 + years experience using analytic SQL, working with traditional relational databases and/or distributed systems such as Hadoop / Hive, BigQuery, Redshift.Experience programming languages (e.g. Python, R, bash) preferred.1+ years of experience with workflow management tools (Airflow, Oozie, Azkaban, UC4)Good understanding of SQL Engines and able to conduct advanced performance tuningExperience with Hadoop (or similar) Ecosystem (MapReduce, Yarn, HDFS, Hive, Spark, Presto, Pig, HBase)Familiarity with data exploration / data visualization tools like Tableau, Looker, Chartio, etc.Ability to think strategically, analyze and interpret market and consumer information.Strong communication skills – written and verbal presentations.Excellent conceptual and analytical reasoning competencies.Degree in an analytical field such as economics, mathematics, or computer science is desired.Comfortable working in a fast-paced and highly collaborative environment.Process-oriented with phenomenal documentation skills', 'Develop Data Quality checks for source and target data sets. Develop UAT plans and conduct QA.', 'Document and publish Metadata and table designs to facilitate data adoption.', 'Create runbooks and actionable alerts as part of the development process', ""What You'll Do"", 'Work with Engineering teams to collect required data from internal and external systems.', 'Process-oriented with phenomenal documentation skills', 'Degree in an analytical field such as economics, mathematics, or computer science is desired.', '1+ years work experience implementing and reporting on business key performance indicators in data warehousing environments. Strong understanding of data modeling principles including Dimensional modeling, data normalization principles etc.', 'Design table structures and define ETL strategy to build performant Data solutions that are reliable and scalable in a fast growing data ecosystem.', 'Perform ad hoc analysis as necessary.', 'Partner with technical and non-technical colleagues to understand data and reporting requirements.', 'Experience with Hadoop (or similar) Ecosystem (MapReduce, Yarn, HDFS, Hive, Spark, Presto, Pig, HBase)', 'Summary', 'Perform SQL and ETL tuning as necessary.', '1 + years experience using analytic SQL, working with traditional relational databases and/or distributed systems such as Hadoop / Hive, BigQuery, Redshift.', 'Develop and maintain ETL routines using ETL and orchestration tools such as Airflow, Luigi and Jenkins.', 'Strong communication skills – written and verbal presentations.', 'Familiarity with data exploration / data visualization tools like Tableau, Looker, Chartio, etc.', 'Experience programming languages (e.g. Python, R, bash) preferred.', '2+ years of relevant Professional experience.', 'Comfortable working in a fast-paced and highly collaborative environment.']",Not Applicable,Full-time,Information Technology,Internet,2021-03-24 13:05:10
Data Engineer,L&T Technology Services Limited,"Ridgefield, NJ",4 weeks ago,Over 200 applicants,"['-> PYTHON, SHELL, SQL', '\xa0Ability to work as part of a team, as well as work independently or with minimal direction.', '\xa0Strong PC skills including knowledge of Microsoft SharePoint', '\xa0Ensure systems meet business requirements and industry practices.', '\xa0Design, construct, install, test and maintain highly scalable data management systems.', '\xa0Build high-performance algorithms, prototypes, predictive models and proof of concepts.', '\xa0Collaborate with data architects, modelers and IT team members on project goals.', 'Job Responsibilities:', '\xa0Excellent written, presentation, and verbal communication skills.', '\xa0Design, implement, automate and maintain large scale enterprise data ETL processes.', 'The main function of the Data Engineer is to develop, evaluate, test and maintain architectures and data solutions within our organization. The typical Data Engineer executes plans, policies, and practices that control, protect, deliver, and enhance the value of the organization s data assets.', 'Skills:']",Mid-Senior level,Full-time,Engineering,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,"Cadence Bank, N.A.","Houston, TX",3 weeks ago,192 applicants,"['REQUIRED EDUCATION', 'Perform root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.', 'Position Summary:', 'Create data tools for analytics and business intelligence team members that assist them in building and optimizing our products into an innovative industry leader.', 'Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.', '0% - 25%', 'Advanced, working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.Strong analytic skills related to working with unstructured datasets.A successful history of manipulating, processing and extracting value from large disconnected datasets.Strong teamwork and interpersonal skillsExperience in leading process improvement initiativesAbility to motivate high performance, multi-discipline teamsDemonstrated competency in project executionDemonstrated abilities in relationship management', 'Maintains direction and focus through proactive planning and organized approaches to work resulting in target deadlines that are consistently met', 'Strong teamwork and interpersonal skills', 'Essential Responsibilities:', 'Ability to motivate high performance, multi-discipline teams', 'Strong analytic skills related to working with unstructured datasets.', ""Bachelor's Degree - Computer Science, Statistics, Informatics, Information Systems or another quantitative field"", '5 - 7 years of experience as a Data Engineer3 - 5 years of experience manipulating, processing and extracting value from large disconnected datasets.1 - 3 years of experience in developing, communicating and presenting concepts to varying audiences1 - 3 years of experience participating in developing strategic plans to realize business objectivesIndustry organization participation / leadership (ISACA, EDM Council, IAPP, etc.)', '3 - 5 years of experience manipulating, processing and extracting value from large disconnected datasets.', 'TRAVEL REQUIREMENT', 'Proven ability to quickly learn new applications, processes, and procedures', 'Professional image with ability to form good partner relationships across functions', 'Microsoft Office including advanced Microsoft Excel and PowerBI. Experience with Snowflake, Qlik, or Datamart or similar systems. Powershell or Python scripting experience.', 'Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.', '3 - 5 years of experience as a Data Engineer or Sr. Data Analyst', '\xa0', ""The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and line of business partners on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company's data architecture to support our next generation of products and data initiatives."", 'Demonstrates a meticulous attention to detail', '3 - 5 years of experience as a Data Engineer or Sr. Data Analyst1 - 3 years of experience manipulating, processing and extracting value from large disconnected datasets.1 - 3 years of experience in developing, communicating and presenting concepts to varying audiences1 - 3 years of experience participating in developing strategic plans to realize business objectives', 'Conduct regular meetings with internal and external stakeholders to ensure clarification and meet specific requirements in a timely manner.Create and maintain optimal data pipeline architectureAssemble and optimize large, complex data sets that meet functional / non-functional business requirements.Perform root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.Keep our data separated and secure across national boundaries through multiple data centers and regions.Create data tools for analytics and business intelligence team members that assist them in building and optimizing our products into an innovative industry leader.Work with data and analytics experts to strive for greater functionality in our data systems.Gains understanding of customer needs and adapt product strategies to meet their expectationsOther duties as assigned or requested', '1 - 3 years of experience manipulating, processing and extracting value from large disconnected datasets.', 'Keep our data separated and secure across national boundaries through multiple data centers and regions.', 'Advanced, working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.', 'The above statements are intended to describe the general nature and level of the work being performed by people assigned to this work. This is not an exhaustive list of all duties and responsibilities. Cadence Management reserves the right to amend and change responsibilities to meet business and organizational needs as necessary.', 'Gains understanding of customer needs and adapt product strategies to meet their expectations', '5 - 7 years of experience as a Data Engineer', 'Create and maintain optimal data pipeline architecture', 'PREFERRED EXPERIENCE', 'Conduct regular meetings with internal and external stakeholders to ensure clarification and meet specific requirements in a timely manner.', 'EQUIPMENT/SOFTWARE:', 'Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.', 'A successful history of manipulating, processing and extracting value from large disconnected datasets.', 'Work with data and analytics experts to strive for greater functionality in our data systems.', 'KNOWLEDGE, SKILLS & ABILITIES', '1 - 3 years of experience participating in developing strategic plans to realize business objectives', '1 - 3 years of experience in developing, communicating and presenting concepts to varying audiences', 'Experience in leading process improvement initiatives', 'Demonstrates initiative, resourcefulness, and independence', 'Proven ability to quickly learn new applications, processes, and proceduresDemonstrates a meticulous attention to detailDemonstrates the capacity to manage changing priorities and ambiguity while remaining calm and controlledMaintains direction and focus through proactive planning and organized approaches to work resulting in target deadlines that are consistently metCommunicates a ""can do"" attitude and positive outlook, minimizing negative behaviorsProfessional image with ability to form good partner relationships across functionsDemonstrates initiative, resourcefulness, and independence', 'Communicates a ""can do"" attitude and positive outlook, minimizing negative behaviors', 'Demonstrates the capacity to manage changing priorities and ambiguity while remaining calm and controlled', 'Assemble and optimize large, complex data sets that meet functional / non-functional business requirements.', 'Industry organization participation / leadership (ISACA, EDM Council, IAPP, etc.)', 'Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.', 'Other duties as assigned or requested', 'MINIMUM EXPERIENCE', 'Behavioral Traits', 'Cadence Bank is an affirmative action/equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, religion, color, national origin, sex, age, status as a protected veteran, among other things, or status as a qualified individual with disability.', 'Demonstrated abilities in relationship management', 'Demonstrated competency in project execution']",Mid-Senior level,Full-time,Other,Banking,2021-03-24 13:05:10
Data Engineer,Tradeswell,"Baltimore, MD",3 weeks ago,82 applicants,"['', 'An passion for data, data engineering, and data science', 'Strong engineering background and experience with strong understanding of Python, Spark, and Airflow, Data Lakes & Warehousing, AWS Step Functions', 'At the core of Tradeswell is a data platform optimized for the intricacies of real-time commerce. As a data engineer you will maintain and evolve data flows that enable critical aspects of the application and data science stack. You will work closely with other engineers and product management as part of a cross-functional team solving problems for internal and external customers.', 'An passion for data, data engineering, and data scienceStrong engineering background and experience with strong understanding of Python, Spark, and Airflow, Data Lakes & Warehousing, AWS Step FunctionsExperience managing managing infrastructure for data pipelines, ETL process, and data warehousesA desire to continually grow, learn, and iterate on the product and yourself', 'Any unsolicited resumes/candidate profiles submitted through our website or to personal email accounts of employees of Tradeswell are considered property of Tradeswell and are not subject to payment of agency fees.', 'The opportunity:', 'Tradeswell is the operating system for real-time commerce. We’re on a mission to democratize ecommerce intelligence and empower growth for brands by making ecommerce actions more informed, more coordinated, and more profitable. We use AI to create insights and make real-time optimization decisions across the entire ecommerce value chain. We put the power of data science at the fingertips of our customers, empowering their decision-making and giving them unprecedented control.', 'About Tradeswell:', 'What you will do:', 'About you:', 'Integrate with 3rd party systems to ingest and normalize data for customers', 'Design, build, and maintain business critical data infrastructure', 'Experience managing managing infrastructure for data pipelines, ETL process, and data warehouses', 'A desire to continually grow, learn, and iterate on the product and yourself', 'Integrate with 3rd party systems to ingest and normalize data for customersDesign, build, and maintain business critical data infrastructureCollaborate with other engineers, product managers, and data scientists to solve problems for internal and external customers', 'Collaborate with other engineers, product managers, and data scientists to solve problems for internal and external customers', '\xa0']",Mid-Senior level,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Data Engineer,Collabera Inc.,"McLean, VA",1 day ago,Be among the first 25 applicants,"['', 'Description', 'mayur.panchal@collabera.com', 'Experience with Python/Scala will be a plus.', 'Data Engineer', 'Experience working with Cassandra.', 'Experience with Spark Framework.', 'Skills : ', 'AWS ', 'Spark', 'Experience with AWS infrastructure and its services.', 'Feel free to reach out to me for additional queries::', 'Cassandra.', 'Experience with AWS infrastructure and its services.Experience with Python/Scala will be a plus.Experience with Spark Framework.Experience working with Cassandra.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Lyve Tech LLC,"Portland, OR",23 hours ago,Be among the first 25 applicants,[''],Entry level,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Data Engineer,Softchoice,"Malvern, PA",2 weeks ago,Be among the first 25 applicants,"['', 'We have raised over $3 Million through our team member run charity Softchoice Cares.', ' Solution design and data migration Security and Compliance Database tuning and optimization Big Data technologies; designing and implementing Data Lakes Integration Services: Azure Data Factory Reporting Tools such as; Power BI, SSRS, etc. Foundational cloud IaaS and PaaS technologies Oracle Python scripting ', 'Foundational cloud IaaS and PaaS technologies', 'Delivering excellence: Developing, testing and managing Data cloud native and 3rd party tooling and automation technologies, delivered through code, templates and scripts to ensure top-quality client experiences, delivered with consistency and efficiency. ', 'GIT', 'Experience in the following areas are an asset: Azure Cosmos DB Azure Synapse Azure Databricks Apache Spark Data Virtualization GIT  ', 'Delivering excellence', ' Great communication and consultative skills, with the ability to occasionally work onsite with customers and be able to present technically advanced topics to large groups of people with confidence and impact. End to end ability to chart a strategy and identify the solution, while also being able to deliver it and support the client through adoption and maturity Experience working in professional services and/or software engineering for clients in a provider to client relationship. Bonus if experience in a consulting firm. ', 'You will have the opportunity to take an ownership position here at Softchoice.', 'What you’ll do:', 'Why you’ll love Softchoice:', 'Advanced development skills and strong scripting experience, for example: Python or PowerShell.', 'Azure Cosmos DB', 'Experience working in professional services and/or software engineering for clients in a provider to client relationship. Bonus if experience in a consulting firm.', ' Softchoice has been recognized as a Best Workplace in Canada by the Great Place to Work Institute for 15 consecutive years. Softchoice has been certified as a Great Place to Work in the United States for several years. We offer meaningful work that drives professional development. Our team members have 2 paid volunteer days per year to give back to a cause of their choice. We offer an opportunity to build a career in the technology industry. We have raised over $3 Million through our team member run charity Softchoice Cares. You will have the opportunity to take an ownership position here at Softchoice. ', 'Evangelizing the New: You aren’t just deeply familiar with cutting-edge concepts and technologies. You know how to tailor your insights to customers, occasionally working onsite with customers as a consultant to explore their needs to provide a compelling vision forward.', 'Bonus points for having multiple related certifications for Azure (DP-200 & DP-201), AWS or GCP.', 'Evangelizing the New', 'Minimum 2 years of experience of IT Cloud Implementations', 'Python scripting', 'Apache Spark', 'Azure Synapse', 'Security and Compliance', 'End to end ability to chart a strategy and identify the solution, while also being able to deliver it and support the client through adoption and maturity', 'The impact you will have:', 'Expert-level understanding about Data Services', 'You will also have a wealth of professional services and consulting experience, such as:', 'We offer an opportunity to build a career in the technology industry.', 'Bachelor’s Degree or Diploma in a relevant field or equivalent industry experience.', 'Require an accommodation? We are ready to help:', 'Softchoice has been recognized as a Best Workplace in Canada by the Great Place to Work Institute for 15 consecutive years.', 'Our commitment to your experience:', ' Delivering excellence: Developing, testing and managing Data cloud native and 3rd party tooling and automation technologies, delivered through code, templates and scripts to ensure top-quality client experiences, delivered with consistency and efficiency.  Leading by collaboration: Working with software developers, application architects, QA engineers, and operations teams to deliver on – and exceed – client expectations. You will also team up with SME’s, practice leads and vendors to fine tune our strategy and overcome obstacles to adoption at every step. Evangelizing the New: You aren’t just deeply familiar with cutting-edge concepts and technologies. You know how to tailor your insights to customers, occasionally working onsite with customers as a consultant to explore their needs to provide a compelling vision forward. ', 'Big Data technologies; designing and implementing Data Lakes', 'Prior to commencing employment:', 'Great communication and consultative skills, with the ability to occasionally work onsite with customers and be able to present technically advanced topics to large groups of people with confidence and impact.', 'Data Virtualization', 'Reporting Tools such as; Power BI, SSRS, etc.', 'Inclusion & Equal opportunity employment:', 'Leading by collaboration: Working with software developers, application architects, QA engineers, and operations teams to deliver on – and exceed – client expectations. You will also team up with SME’s, practice leads and vendors to fine tune our strategy and overcome obstacles to adoption at every step.', 'Leading by collaboration', 'Solution design and data migration', 'Softchoice has been certified as a Great Place to Work in the United States for several years.', 'Azure Databricks', 'Proven experience driving results with relevant technologies and methods, such as architecting and implementing public cloud data solutions on Azure.', 'Strong knowledge of Oracle, SQL Server, Azure SQL Database and Amazon RDS', 'Integration Services: Azure Data Factory', ' Expert-level understanding about Data Services Good understanding about DevOps culture, methodologies, coding and automation. Proven experience driving results with relevant technologies and methods, such as architecting and implementing public cloud data solutions on Azure. Bachelor’s Degree or Diploma in a relevant field or equivalent industry experience. Minimum 2 years of experience of IT Cloud Implementations Strong knowledge of Oracle, SQL Server, Azure SQL Database and Amazon RDS Knowledge of SQL with High Availability such as Always On Availability Groups Experience designing and implementing Digital Transformational solutions with expertise in the following areas: Solution design and data migration Security and Compliance Database tuning and optimization Big Data technologies; designing and implementing Data Lakes Integration Services: Azure Data Factory Reporting Tools such as; Power BI, SSRS, etc. Foundational cloud IaaS and PaaS technologies Oracle Python scripting   Experience in the following areas are an asset: Azure Cosmos DB Azure Synapse Azure Databricks Apache Spark Data Virtualization GIT   Advanced development skills and strong scripting experience, for example: Python or PowerShell. Bonus points for having multiple related certifications for Azure (DP-200 & DP-201), AWS or GCP. ', ' Azure Cosmos DB Azure Synapse Azure Databricks Apache Spark Data Virtualization GIT ', 'Knowledge of SQL with High Availability such as Always On Availability Groups', 'Data Engineer', 'Database tuning and optimization', 'Oracle', 'What you’ll bring to the table:', 'Experience designing and implementing Digital Transformational solutions with expertise in the following areas: Solution design and data migration Security and Compliance Database tuning and optimization Big Data technologies; designing and implementing Data Lakes Integration Services: Azure Data Factory Reporting Tools such as; Power BI, SSRS, etc. Foundational cloud IaaS and PaaS technologies Oracle Python scripting  ', 'Good understanding about DevOps culture, methodologies, coding and automation.', 'Our team members have 2 paid volunteer days per year to give back to a cause of their choice.', 'Some reasons why our employees love working here: ', 'We offer meaningful work that drives professional development.']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Workday,"Pleasanton, CA",22 hours ago,Be among the first 25 applicants,"['', 'Hands-on experience with source version control, continuous integration and experience with release/change management delivery tools.', ' 6+ years of experience designing and building scalable and robust data pipelines to enable data-driven decisions for the business.  \u200bPrior experience with CRM systems like SFDC is required.   Experience building analytical solutions to Sales and Marketing teams. Experience with very large-scale data warehouse and data engineering projectsExperience developing low latency data processing solutions like AWS Kinesis, Kafka, Spark Stream processing.Should be proficient in writing advanced SQLs, Expertise in performance tuning of SQLs Experience working with AWS data technologies like S3, EMR, Lambda, DynamoDB, Redshift etc.  Strong experience in one or more programming languages for processing of large data sets, such as Python, Scala. Ability to create data models, STAR schemas for data consuming.Extensive experience in troubleshooting data issues, analyzing end to end data pipelines and in working with users in resolving issuesBS/MS in computer science or equivalent is required', 'Qualifications:\xa0', 'Build reliable, efficient, testable, & maintainable data pipelines.', 'Design and Develop data pipelines using Metadata driven ETL Tools and Open source data processing frameworks.', ' 6+ years of experience designing and building scalable and robust data pipelines to enable data-driven decisions for the business. ', 'Ability to create data models, STAR schemas for data consuming.', 'Work with cross functional teams to enable data insights though Data lifecycle.', 'Extensive experience in troubleshooting data issues, analyzing end to end data pipelines and in working with users in resolving issues', 'Job Responsibilities:\xa0', 'Should be proficient in writing advanced SQLs, Expertise in performance tuning of SQLs', ' Strong experience in one or more programming languages for processing of large data sets, such as Python, Scala. ', 'Develop and automate high-performance data processing systems to drive Workday business growth and improve the product experience.Evangelize high quality software engineering practices towards building data infrastructure and pipelines at scale.Build reliable, efficient, testable, & maintainable data pipelines.Design and Develop data pipelines using Metadata driven ETL Tools and Open source data processing frameworks.Hands-on experience with source version control, continuous integration and experience with release/change management delivery tools.Provide production support and resolve high priority incidents and the development coding issues.Work with cross functional teams to enable data insights though Data lifecycle.', ' Experience building analytical solutions to Sales and Marketing teams. ', 'Develop and automate high-performance data processing systems to drive Workday business growth and improve the product experience.', ' Experience working with AWS data technologies like S3, EMR, Lambda, DynamoDB, Redshift etc. ', 'Experience developing low latency data processing solutions like AWS Kinesis, Kafka, Spark Stream processing.', 'BS/MS in computer science or equivalent is required', 'Experience with very large-scale data warehouse and data engineering projects', 'Evangelize high quality software engineering practices towards building data infrastructure and pipelines at scale.', 'Do what you love. Love what you do. ', 'Provide production support and resolve high priority incidents and the development coding issues.', ' \u200bPrior experience with CRM systems like SFDC is required.  ', 'Job Description']",Not Applicable,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Data Engineer,NYC Health + Hospitals,New York City Metropolitan Area,,N/A,"['', 'NYC Health + Hospitals is the largest public health care system in the United States. We provide essential outpatient, inpatient and home-based services to more than one million New Yorkers every year across the city’s five boroughs. Our large health system consists of ambulatory centers, acute care centers, post-acute care/long-term care, rehabilitation programs, Home Care, and Correctional Health Services. Our diverse workforce is uniquely focused on empowering New Yorkers, without exception, to live the healthiest life possible.', 'Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise', '3+ years of\xa0experience in a data training, analytics, management, or QC role', 'Checking and preparing data to provide to the Data Scientist and other analysts, as needed', 'Minimum Qualifications:', 'Cleaning large datasets, with responsibility for accuracy and complex analyses through programming and performing statistical analyses on large data sets to ensure integrity of data for analyses and use across the Data, Analytics and Product Development Team to support planning, reporting and development initiatives under the Test and Trace CorpsCompleting complex network, statistical and programming analyses to clean and prepare data, ensuring accuracyServing as the internal engineering consultant to solve structural data issues within the Quality Control (QC) workflow\xa0\xa0\xa0\xa0Regulating the cleanliness of data produced from Data Engineers, Data Scientists, and Data Managers across the Data, Analytics and Product Development TeamCollaborating with data staff and other departments to identify and mitigate potential data issuesAuditing and reporting on the QC of data migration and integrations throughout the Test and Trace Corps organizationChecking and preparing data to provide to the Data Scientist and other analysts, as needed', 'Summary of Duties and Responsibilities:', '1. A Baccalaureate Degree from an accredited college or university with a major in Computer Science, Systems Engineering, applied Mathematics, Business Administration, Economics/Statistics, Telecommunications, Data Communications, or a related field of study; and', '3. Broad knowledge and expertise in the characteristics of computers, peripheral devices, communications systems and hardware capabilities, programming languages, E.D.P. applications, systems analysis methodology, data management and retrieval techniques; or', 'Serving as the internal engineering consultant to solve structural data issues within the Quality Control (QC) workflow\xa0\xa0\xa0\xa0', 'Regulating the cleanliness of data produced from Data Engineers, Data Scientists, and Data Managers across the Data, Analytics and Product Development Team', 'Ability to work autonomously, think analytically, and anticipate data issues to solve before they ariseExcellent written and verbal communication skills, with the ability to explain data systems to non-technical teamsStrong quality control abilities and exceptional attention to detailGeneral knowledge of SQL, R, Python, Excel and related data analytics toolsets3+ years of\xa0experience in a data training, analytics, management, or QC roleNYC residency', 'Collaborating with data staff and other departments to identify and mitigate potential data issues', 'Network Services requires a telecommunications background and experience.', '2. Five (5) years of progressive, responsible experience in the field of data processing, computer systems and applications.', 'Operations Specialty requires supervisory experience (5 years).', 'Empower Every New Yorker — Without Exception — to Live the Healthiest Life Possible', 'Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams', '\ufeff', 'Department Preferences:', 'The Test and Trace Corps is looking for a Data Engineer', 'Reporting to the Senior Data Engineer within the Data and Analytics Unit, the Data Engineer designs, evaluates and tests data structures and will be responsible for:', 'Completing complex network, statistical and programming analyses to clean and prepare data, ensuring accuracy', 'The Test and Trace Corps is looking for a Data Engineer to join the Data, Analytics and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.', 'NYC residency', '4. A satisfactory equivalent combination of training, education and experience', 'Auditing and reporting on the QC of data migration and integrations throughout the Test and Trace Corps organization', 'Strong quality control abilities and exceptional attention to detail', 'Cleaning large datasets, with responsibility for accuracy and complex analyses through programming and performing statistical analyses on large data sets to ensure integrity of data for analyses and use across the Data, Analytics and Product Development Team to support planning, reporting and development initiatives under the Test and Trace Corps', 'General knowledge of SQL, R, Python, Excel and related data analytics toolsets']",Director,Full-time,Other,Hospital & Health Care,2021-03-24 13:05:10
Data Engineer,Accenture,"Philadelphia, PA",7 days ago,Be among the first 25 applicants,"['', 'Understand and used Object Orientated design techniques', 'Data & Analytics Transformation (current state assessment, strategy development, value case, roadmap, and blueprint)', 'Experience using TDD and unit testing as part of normal software development using packages such as Postman, Jest, JUnit, PyUnit, Swagger, etc.', 'Create a value chain to help address the challenges of acquiring data, evaluating its value, distilling & analyzing ', 'Data architecture (Understanding logical ways of organizing and analyzing data and how this affects building databases, APIs, and UIs)', 'Experience in data migration from on-prem to cloud. Multi-Cloud experience - AWS/Azure/Google a plus', 'Process Automation, Machine Learning, and Artificial Intelligence practices (knowledge of how advancing digital tools and techniques are applied in enterprise data and analytics strategies and roadmaps)', 'Basic Qualifications:', 'Business Translator (identifying business problem, initiative, analytics intervention, data science management, data science interpretation, storytelling)', 'Work in an agile CI/CD environment, (Jenkins/Ansible experience a plus)', 'Minimum of 3 years of combined data, analytics and strategic consulting experience', 'Minimum of two years of experience in one or more of the following areas: ', 'Database design and performance optimization with multiple databases (relational data stores - RDS, Aurora; NoSQL data stores - DynamoDB; data warehouses -Teradata, Redshift, snowflake; graph databases)', 'Experience with containerization platforms a plus, such as Docker, Kubernetes ', 'For now, all Accenture business travel, international and domestic, is currently restricted to client-essential sales/delivery activity only.Please note: The safety and well-being of our people continues to be the top priority, and our decisions around travel are informed by government COVID-19 response directives, recommendations from leading health authorities and guidance from a number of infectious disease experts.', 'Infrastructure as Code (Terraform or similar technology)', 'Minimum of 3 years of hands-on experience on one or more of these technologies -Python, Scala, Spark, PySpark', 'Lead data modeling activities to capture and model data requirements, business rules, and logical and physical models', '2 years of Experience writing REST or GraphQL APIs', 'A Bachelor’s Degree or equivalent work experience (12 years) or an Associate’s Degree with 6 years of work experience', 'Experience working with large data sets in distributed data environments ', 'Preferred Qualifications:']",Mid-Senior level,Full-time,Strategy/Planning,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,GrainBridge,United States,23 hours ago,Be among the first 25 applicants,"['', 'Experience with stream processing, message queuing, and highly scalable big datastore', '5+ years experience as a data engineer in an enterprise settingAWS DynamoDBAmazon Aurora PostgreSQLExperience working with AWS big data technologies (Redshift, S3, Kinesis, Athena, Glue)Python scripting experienceExperience developing and maintaining ETL applications using Apache Spark and PythonExperience deploying data-related artifacts utilizing automated AWS pipelinesGit and GitHub or equivalent', 'Amazon Aurora PostgreSQL', 'Design, implement and maintain ETL and ELT pipelines that ingest internal and external data sources into our data refinery', 'Experience working with AWS big data technologies (Redshift, S3, Kinesis, Athena, Glue)', 'Qualifications and Skills', 'Bash scripting experience', 'Data migrations', 'Collaborate with internal and partner architects to design, deploy, maintain, and evolve our data refinery', 'Experience deploying data-related artifacts utilizing automated AWS pipelines', '5+ years experience as a data engineer in an enterprise setting', 'Work with Data Scientists, Data Architects, and Engineering team to create, deploy, manage and tune AWS cloud data stores', 'Obtain a deep understanding of our internal data sources', 'Deployment of databases', 'AWS DynamoDB', 'Duties and Responsibilities', 'Experience developing and maintaining ETL applications using Apache Spark and Python', 'Python scripting experience', 'Performance tuning and optimization', 'Bash scripting experienceKnowledge of one or more of the following:\xa0AWS Lake Formation, AWS RDS, AWS Quicksight, AWS IAM, AWS EC2, AWS DMS, AWS Athena, AWS Glue, AWS EMR, Redis, Elasticsearch', 'Develop domain expertise in grain marketingObtain a deep understanding of our internal data sourcesCollaborate with internal and partner architects to design, deploy, maintain, and evolve our data refineryDesign, implement and maintain ETL and ELT pipelines that ingest internal and external data sources into our data refineryWork with Data Scientists, Data Architects, and Engineering team to create, deploy, manage and tune AWS cloud data storesPerformance tuning and optimizationExperience with stream processing, message queuing, and highly scalable big datastoreData migrationsManaging data backup and data disaster recovery plan and policiesDeployment of databasesAssist in the generation of test data', 'Assist in the generation of test data', ""GrainBridge is seeking a versatile and talented Data Engineer who is passionate about using cutting-edge cloud technology to help us build the next generation of Ag software for North America's producers. Successful candidates will have the opportunity to join our highly collaborative agile development team utilizing the latest cloud and data technology stacks to build customer-facing applications that will revolutionize the Ag Industry."", 'Bonus Qualifications & Skills', 'About GrainBridge LLC', 'Develop domain expertise in grain marketing', 'GrainBridge, LLC is a technology company based in Omaha, NE that is developing tools to provide grain marketing decision support, e-commerce, and account management software for North American farmers. This includes the development of digital tools designed to help farmers across the U.S. and Canada consolidate information on production economics and grain marketing activities into a single digital platform.', 'Knowledge of one or more of the following:\xa0AWS Lake Formation, AWS RDS, AWS Quicksight, AWS IAM, AWS EC2, AWS DMS, AWS Athena, AWS Glue, AWS EMR, Redis, Elasticsearch', 'Git and GitHub or equivalent', 'Managing data backup and data disaster recovery plan and policies']",Mid-Senior level,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Data Engineer,Beyond Meat,"El Segundo, CA",2 days ago,30 applicants,"['', 'Support the implementation of new data systems and software applications into existing systems and infrastructure', 'Experience with version control and source control methodologies (Git, Mercurial)', 'Design, implement, and support our data infrastructure and systems development', 'Keep up to date with the latest developments in data management and AWS offerings to support the Innovation team', 'Partner closely with domain experts (food science, analytical chemistry, protein bioinformatics, etc.) and machine learning engineers to adopt best practices in data management and identify and develop areas where data science can bring disruptive value', 'Using application APIs to source and model data', 'The Innovation team is seeking an innovative and hands-on Data Engineer who can partner with scientists and engineers on the team to develop infrastructure and data pipelines for our machine learning solutions. The Data Engineer will act as the subject matter expert for data management, systems, and feature engineering. You will be responsible for developing accurate, consistent, and architecturally sound data sets across research and cross-functional units, as well as managing the execution and delivery of databases across projects to meet the Innovation team’s goals.', 'Support the development of data systems from complex and/or multiple inputs, including to provide processing solutions for raw data where needed', 'Preserve all forms of intellectual and material property, support IP development, and ensure confidentiality and security of information', 'QUALIFICATIONS', '3+ years experience as a Data Engineer or similar role', 'Extensive experience with data warehousing, data modeling, and building/managing ETL pipelines', 'Experience with big data technologies such as Spark, Hive, Redshift or BigQuery', 'Experience generating value and insights from datasets\xa0', 'Office environments with some travel (5%)', 'OVERVIEW', 'Interface with various functional teams to perform ETL using a wide variety of data sources and leveraging SQL and AWS big data technologies', 'Experience in SQL, Python and/or R, Bash, or a similar scripting language', 'Design, implement, and support our data infrastructure and systems developmentUtilize and manage AWS resources such as S3, EC2, EMR, Redshift, Glue, etc.\xa0Interface with various functional teams to perform ETL using a wide variety of data sources and leveraging SQL and AWS big data technologiesPartner closely with domain experts (food science, analytical chemistry, protein bioinformatics, etc.) and machine learning engineers to adopt best practices in data management and identify and develop areas where data science can bring disruptive valueKeep up to date with the latest developments in data management and AWS offerings to support the Innovation teamSupport the implementation of new data systems and software applications into existing systems and infrastructureSupport the development of data systems from complex and/or multiple inputs, including to provide processing solutions for raw data where neededDevelop and optimize codeMaintain and upkeep databases, including performance validation and optimizationMaintain security on data, data infrastructure and systemsPreserve all forms of intellectual and material property, support IP development, and ensure confidentiality and security of information', 'Must have experience with cloud computing platforms (AWS or GCP)', 'Masters in Computer Science, Math, Statistics, or other quantitative fields', 'Ability to work collaboratively with various teams across the organization', 'Ability to thrive in independent work with minimal oversight', 'Masters in Computer Science, Math, Statistics, or other quantitative fields3+ years experience as a Data Engineer or similar roleExtensive experience collecting requirements, describing data modeling decisions, and developing data engineering strategiesExtensive experience with data warehousing, data modeling, and building/managing ETL pipelinesStrong knowledge of the theoretical principles underlying data management and data storageMust have experience with cloud computing platforms (AWS or GCP)Experience with big data technologies such as Spark, Hive, Redshift or BigQueryDesigning/implementing cloud architectures for machine learning is a plusExperience in SQL, Python and/or R, Bash, or a similar scripting languageUsing application APIs to source and model dataExperience generating value and insights from datasets\xa0Familiarity with statistical models and commonly used data mining approachesWorking knowledge of containerization/virtualization of applications (Docker, Kubernetes)Experience with version control and source control methodologies (Git, Mercurial)Ability to work collaboratively with various teams across the organizationCan manage multiple projects and priorities to support shifting team needsAbility to thrive in independent work with minimal oversightExcellent oral and written communication skills; fluent in both written and spoken English', 'Develop and optimize code', 'Strong knowledge of the theoretical principles underlying data management and data storage', 'Working knowledge of containerization/virtualization of applications (Docker, Kubernetes)', 'RESPONSIBILITIES', 'Extensive experience collecting requirements, describing data modeling decisions, and developing data engineering strategies', 'Maintain security on data, data infrastructure and systems', 'Utilize and manage AWS resources such as S3, EC2, EMR, Redshift, Glue, etc.\xa0', 'Can manage multiple projects and priorities to support shifting team needs', 'Maintain and upkeep databases, including performance validation and optimization', 'WORK ENVIRONMENT', 'Excellent oral and written communication skills; fluent in both written and spoken English', 'Designing/implementing cloud architectures for machine learning is a plus', 'Familiarity with statistical models and commonly used data mining approaches']",Mid-Senior level,Full-time,Research,Food & Beverages,2021-03-24 13:05:10
Data Engineer,Babylon Health,"Austin, TX",4 weeks ago,124 applicants,"['', 'Build, test and refine data pipelines for data analytics and business intelligence (BI)', 'Experience with using a cloud platform provider (such as AWS/GCP) to develop tools and infrastructure', 'Experience with design and development of relational databases and data warehouses', 'Proven ability of looking at solutions unconventionally and explore opportunities and devise innovative solutions', 'Work closely with the business intelligence teams to design, build and test end-to-end solutions', 'Bachelor’s degree in computer science or related fieldProven ability of looking at solutions unconventionally and explore opportunities and devise innovative solutionsExcellent communication skills (verbal and written) and interpersonal skills and an ability to effectively communicate with both business and technical teamsExperience gathering complex business requirements and identifying data needsExperience with design and development of relational databases and data warehousesAdvanced level of proficiency in SQL developmentKnowledge and expertise with Python, Shell, Java ScriptingETL development experience with large-scale databases or big data systems such as Hive, BigQuery, AWS Redshift, Snowflake, etc.Experience using data transformation tools such as dbtExperience using data orchestration tools such as Apache Airflow or Apache BeamExperience with using a cloud platform provider (such as AWS/GCP) to develop tools and infrastructureExposure to a BI reporting tool (such as Tableau, Looker or PowerBI) with an understanding of why they are an important part of the analytics stackExperience analyzing data to identify deliverables, gaps and inconsistenciesNice to have: experience working in a start-up', 'Ensure the data quality and consistency with monitoring support, and play an active role in establishing data governance around company KPIs', 'Experience gathering complex business requirements and identifying data needs', 'Work closely with the data science team to support processing data into a form suitable for machine learning models', 'Nice to have: experience working in a start-up', 'Experience using data transformation tools such as dbt', 'Advanced level of proficiency in SQL development', 'Excellent communication skills (verbal and written) and interpersonal skills and an ability to effectively communicate with both business and technical teams', 'Experience analyzing data to identify deliverables, gaps and inconsistencies', 'Requirements', 'ETL development experience with large-scale databases or big data systems such as Hive, BigQuery, AWS Redshift, Snowflake, etc.', 'Bachelor’s degree in computer science or related field', 'Responsibilities', 'Our technology stack includes Python, dbt, Airflow and a host of Google Cloud products that run on a range of technologies (GCP/AWS, Docker, GitHub, CircleCI & Jenkins) ', 'Data modeling, process design and overall data pipeline architecture', 'Knowledge and expertise with Python, Shell, Java Scripting', 'Champion high engineering standards through comprehensive testing, code reviews, continuous integration and continuous deployment across the team', 'Exposure to a BI reporting tool (such as Tableau, Looker or PowerBI) with an understanding of why they are an important part of the analytics stack', 'Build, test and refine data pipelines for data analytics and business intelligence (BI)Data modeling, process design and overall data pipeline architectureEnsure the data quality and consistency with monitoring support, and play an active role in establishing data governance around company KPIsWork closely with the business intelligence teams to design, build and test end-to-end solutionsWork closely with the data science team to support processing data into a form suitable for machine learning modelsChampion SSDLC (secure software development lifecycle) within analytics and data science and lead by example in building self-service, well tested solutionsChampion high engineering standards through comprehensive testing, code reviews, continuous integration and continuous deployment across the teamOur technology stack includes Python, dbt, Airflow and a host of Google Cloud products that run on a range of technologies (GCP/AWS, Docker, GitHub, CircleCI & Jenkins) ', 'Champion SSDLC (secure software development lifecycle) within analytics and data science and lead by example in building self-service, well tested solutions', 'Experience using data orchestration tools such as Apache Airflow or Apache Beam', 'Nice to have:']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Harmony Analytics,"Atlanta, GA",2 days ago,93 applicants,"['', 'Build, monitor, and maintain a network of\xa0ETL pipelines\xa0\xa0Improve\xa0our\xa0data extraction methodology\xa0Standardize code- and script-level documentation\xa0Manage\xa0resources on\xa0Redshift\xa0and S3\xa0Organize data storage and increase\xa0data\xa0accessibility\xa0', 'Qualifications\xa0', 'Responsibilities\xa0', 'Self-starter with excellent organizational skills\xa0', 'Understanding of software engineering fundamentals\xa0', 'Expertise in\xa0database technologies such as Redshift, Teradata or equivalent\xa0', 'Harmony Analytics is\xa0looking for an experienced data engineer to join our team.\xa0\xa0You will be responsible for expanding our data infrastructure\xa0and\xa0maintaining our product dashboard.\xa0This is a technical role that requires strong engineering and organizational skills.\xa0Our\xa0ideal candidate\xa0would have\xa0a\xa0deep\xa0understanding of pipeline architecture, data storage, and strategies for extracting, ingesting, and processing data, as well as exposure to the data needs of a small organization.\xa0\xa0\xa0', 'Build, monitor, and maintain a network of\xa0ETL pipelines\xa0\xa0', 'Proficient\xa0in\xa0Python and SQL; bonus if familiar with Node.js, Java, Hadoop\xa0', '3+\xa0years\xa0of industry experience\xa0', '3+\xa0years\xa0of industry experience\xa0Proficient\xa0in\xa0Python and SQL; bonus if familiar with Node.js, Java, Hadoop\xa0Understanding of software engineering fundamentals\xa0Self-starter with excellent organizational skills\xa0Expertise in\xa0database technologies such as Redshift, Teradata or equivalent\xa0', 'Improve\xa0our\xa0data extraction methodology\xa0', 'Manage\xa0resources on\xa0Redshift\xa0and S3\xa0', 'Organize data storage and increase\xa0data\xa0accessibility\xa0', 'Standardize code- and script-level documentation\xa0', '\xa0']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Verizon,"Alpharetta, GA",4 weeks ago,66 applicants,"['', 'Experience knitting disperate data sources together', 'Four or more years of experience as a data engineer', 'Ability to travel occasionally', 'Master’s degree in Computer Science, Engineering, Statistics, IT, or related field.', 'Experience in data engineering, databases, and data warehouses.', 'Diversity and Inclusion at Verizon', 'Four or more years of experience building data pipelines', 'Experience as an open source Contributor.', 'What You’ll Be Doing...', ""You'll Need To Have"", 'Master’s degree in Computer Science, Engineering, Statistics, IT, or related field.Experience with Scala, Julia, R, Python or other machine learning programming languageExperience on Big Data platforms (i.e., Hadoop, Map/Reduce, Spark, HBase, CouchDB, Hive, etc.)Strong analytical and problem-solving skills.Experience working in a network operations center environment.Experience as an open source Contributor.', 'Explore suitable options and designs for specific analytical solutions.', 'diversity and inclusion', 'Work closely with Data Analysts to ensure data quality and availability for analytical modelling.', 'Bachelor’s degree or four or more years of work experience.Four or more years of experience as a data engineerFour or more years of experience finding, cleaning, and preparing data for use by Data ScientistsExperience knitting disperate data sources togetherFour or more years of experience building data pipelinesExperience using SQL (i.e., PL/SQL or T-SQL with RDBMSs like Teradata, MS SQL Server, Oracle, etc.)Experience in data engineering, databases, and data warehouses.Strong experience with data engineering in Python.Ability to travel occasionally', 'Strong analytical and problem-solving skills.', 'Four or more years of experience finding, cleaning, and preparing data for use by Data Scientists', 'What we’re looking for...', 'Support maintenance, bug fixes and, performance analysis along data pipeline.', 'Equal Employment Opportunity', 'When you join Verizon', 'Gather requirements, assess gaps, and build roadmaps and architectures to help the analytics driven organization achieve its goals.Work closely with Data Analysts to ensure data quality and availability for analytical modelling.Explore suitable options and designs for specific analytical solutions.Define extract, load, and transform (ELT) based on jointly defined requirements.Prepare, clean, and massage data for use in modeling and prototypesIdentify gaps and implement solutions for data security, quality, and automation of processes.Support maintenance, bug fixes and, performance analysis along data pipeline.', 'Experience using SQL (i.e., PL/SQL or T-SQL with RDBMSs like Teradata, MS SQL Server, Oracle, etc.)', 'Bachelor’s degree or four or more years of work experience.', 'Define extract, load, and transform (ELT) based on jointly defined requirements.', 'Experience with Scala, Julia, R, Python or other machine learning programming language', 'Prepare, clean, and massage data for use in modeling and prototypes', 'Identify gaps and implement solutions for data security, quality, and automation of processes.', 'Strong experience with data engineering in Python.', 'Experience on Big Data platforms (i.e., Hadoop, Map/Reduce, Spark, HBase, CouchDB, Hive, etc.)', 'Even Better If You Have', 'Gather requirements, assess gaps, and build roadmaps and architectures to help the analytics driven organization achieve its goals.', 'Experience working in a network operations center environment.']",Mid-Senior level,Full-time,Information Technology,Telecommunications,2021-03-24 13:05:10
Data Engineer,CapTech Consulting,"Philadelphia, PA",2 weeks ago,Be among the first 25 applicants,"['', 'Competitive salary with performance-based bonus opportunities', 'Single and Family Health Insurance plans, including Dental coverage', 'Communicate with all levels of stakeholders as appropriate, including executives, data modelers, application developers, business users, and customers', 'CapTech is an equal opportunity employer committed to fostering a culture of equality, inclusion and fairness — each foundational to our core values. We strive to create a diverse environment where each employee is encouraged to bring their unique ideas, backgrounds and experiences to the workplace.', 'Development experience building ETL graphs using the Ab Initio GDE, EME and Co-Operating system', 'Experience tuning SQL queries to ensure performance and reliability', 'Strong SQL development skills', 'Minimum of 4-5 years experience designing, developing, and testing software aligned with defined requirements', ""Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experienceDevelopment experience building ETL graphs using the Ab Initio GDE, EME and Co-Operating systemStrong SQL development skillsDevelopment experience with at least two different programming languages (Python, Java, Scala, etc.)Development experience with Unix tools and shell scriptsDevelopment experience with at least two different database platforms (Teradata, Oracle, MySQL, MS SQL, etc.)Minimum of 4-5 years experience designing, developing, and testing software aligned with defined requirementsExperience tuning SQL queries to ensure performance and reliabilitySoftware engineering best-practices, including version control (Git, TFS, JIRA, etc.) and test-driven development"", 'Some of our technologies might include: Python, Cassandra, Spark, Java, Scala, Informatica, SQL Server, SSIS, Oracle, Kafka.', 'Development experience with Unix tools and shell scripts', 'Present programming documentation and design to team members and convey complex information in a clear and concise manner.', 'Training and Certification opportunities eligible for expense reimbursement', 'Development experience with at least two different programming languages (Python, Java, Scala, etc.)', 'Matching 401(k)', 'Specific Responsibilities For The Data Engineer, Analytics Position Include', 'Team building and social activities', 'Development experience with at least two different database platforms (Teradata, Oracle, MySQL, MS SQL, etc.)', 'Software engineering best-practices, including version control (Git, TFS, JIRA, etc.) and test-driven development', 'Extract data from multiple sources, integrate disparate data into a common data model, and integrate data into a target database, application, or file using efficient programming processes.', 'Short-Term and Long-Term disability', 'Collaborate with Quality Assurance resources to debug code and ensure the timely delivery of products.', 'Write and refine code to ensure performance and reliability of data extraction and processing.', 'Design, develop, document, and test advanced data systems that bring together data from disparate sources, making it available to data scientists, analysts, and other users using scripting and/or programming languages (Python, Java, Scala, etc)', 'Qualifications', 'Participate in requirements gathering sessions with business and technical staff to distill technical requirements from business requests.', 'Design, develop, document, and test advanced data systems that bring together data from disparate sources, making it available to data scientists, analysts, and other users using scripting and/or programming languages (Python, Java, Scala, etc)Evaluate structured and unstructured datasets utilizing statistics, data mining, and predictive analytics to gain additional business insightsDesign, develop, and implement data processing pipelines at scalePresent programming documentation and design to team members and convey complex information in a clear and concise manner.Extract data from multiple sources, integrate disparate data into a common data model, and integrate data into a target database, application, or file using efficient programming processes.Write and refine code to ensure performance and reliability of data extraction and processing.Communicate with all levels of stakeholders as appropriate, including executives, data modelers, application developers, business users, and customersParticipate in requirements gathering sessions with business and technical staff to distill technical requirements from business requests.Partner with clients to fully understand business philosophy and IT Strategy; recommend process improvements to increase efficiency and reliability in ETL development.Collaborate with Quality Assurance resources to debug code and ensure the timely delivery of products.Some of our technologies might include: Python, Cassandra, Spark, Java, Scala, Informatica, SQL Server, SSIS, Oracle, Kafka.', 'Design, develop, and implement data processing pipelines at scale', 'Partner with clients to fully understand business philosophy and IT Strategy; recommend process improvements to increase efficiency and reliability in ETL development.', 'Company Description', 'Evaluate structured and unstructured datasets utilizing statistics, data mining, and predictive analytics to gain additional business insights', 'Job Description', 'Mentor program to help you develop your career', ""Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience"", 'Competitive salary with performance-based bonus opportunitiesSingle and Family Health Insurance plans, including Dental coverageShort-Term and Long-Term disabilityMatching 401(k)Competitive Paid Time OffTraining and Certification opportunities eligible for expense reimbursementTeam building and social activitiesMentor program to help you develop your career', 'Competitive Paid Time Off']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,CVS Health,"Hartford, CT",2 days ago,27 applicants,"['', 'Required Qualifications', 'Business Overview', 'Education', 'Preferred Qualifications', 'Job Description']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Canary,"Seattle, WA",19 hours ago,Be among the first 25 applicants,"['', ' Knowledge of data management fundamentals and data storage principles', ' Support the teams through our 3 year planning journey, helping design our future data architecture', ' Hands-on experience and advanced knowledge of SQL', ' Knowledge of distributed systems as it pertains to data storage and computing', ' Masters in computer science, mathematics, statistics, economics, or other quantitative fields.', 'Preferred Qualification', ' Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations', ' Masters in computer science, mathematics, statistics, economics, or other quantitative fields. 5+ years of experience as a Data Engineer, BI Engineer or related field in a company with large, complex data sources. Experience working with AWS big data technologies (Redshift, S3, EMR) Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy. Familiarity with data quality automation.', ' Data modeling to support realtime & batch monitoring & alerts, Enable more efficient adhoc queries & analysis Build data pipelines to feed machine learning models and decision engines for real-time and large-scale offline use cases Create a new cluster for Capacity for the US and CA Work closely with analytics and supply chain leaders to scale data Ensure consistency between various platform, operational, and analytic data sources to enable faster and more efficient detection and resolution of issues Explore and learn the latest AWS technologies to provide new capabilities and increase efficiencies Raise the bar on the importance of data within the operations and analytics teams Support the teams through our 3 year planning journey, helping design our future data architecture', ' 5+ years of experience as a Data Engineer, BI Engineer or related field in a company with large, complex data sources.', ' Create a new cluster for Capacity for the US and CA', ' Raise the bar on the importance of data within the operations and analytics teams', ' Experience working with AWS big data technologies (Redshift, S3, EMR)', ' Familiarity with data quality automation.', ' 4+ years of industry experience in software development, data engineering, business intelligence, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets', "" Bachelor's degree in Computer Science, Engineering, Mathematics, or a related technical discipline 4+ years of industry experience in software development, data engineering, business intelligence, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets Hands-on experience and advanced knowledge of SQL Experience in Data Modeling, ETL Development, and Data Warehousing Solid understanding of data design approaches (and how to best use them) Knowledge of data management fundamentals and data storage principles Knowledge of distributed systems as it pertains to data storage and computing"", ' Work closely with analytics and supply chain leaders to scale data', "" Bachelor's degree in Computer Science, Engineering, Mathematics, or a related technical discipline"", ' Enable more efficient adhoc queries & analysis', ' Build data pipelines to feed machine learning models and decision engines for real-time and large-scale offline use cases', ' Explore and learn the latest AWS technologies to provide new capabilities and increase efficiencies', ' Experience in Data Modeling, ETL Development, and Data Warehousing', ' Ensure consistency between various platform, operational, and analytic data sources to enable faster and more efficient detection and resolution of issues', 'This Individual Will Be Responsible For Driving', ' Data modeling to support realtime & batch monitoring & alerts,', 'Basic Qualifications', ' Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy.', ' Solid understanding of data design approaches (and how to best use them)']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Omnicom Media Group,United States,2 days ago,Over 200 applicants,"['', 'Experience building scalable data pipelines (Airflow experience a plus)', 'Intellectual curiosity and drive; self-starters will thrive in this position', 'Key Responsibilities', 'Designing, building, testing and deploying scalable, reusable and maintainable applications that handle large amounts of data', 'Additional Skills', 'As part of Omnicom, we have the backing and resources of a global billion-dollar company, but also have the flexibility and pace of a “startup” - we move fast, break things, and innovate.', 'Curiosity in learning the business requirements that are driving the engineering requirementsInterest in new technologies and eager to bring those technologies and out of the box ideas to the team4+ years of development experience on web applications using Python, Ruby, Java, or C#8+ years of SQL experience.Intellectual curiosity and drive; self-starters will thrive in this positionPassion for Technology: Excitement for new technology, bleeding edge applications, and a positive attitude towards solving real world challenges', 'Proven ability to independently execute projects from concept to implementation to launch and to maintain a live product', ""Annalect is currently seeking a Data Engineer to join our technology team. In this role, you will build Annalect products which sit atop our Big Data infrastructure and utilize our componentized design system. We're looking for people who have a shared passion for technology, design & development, data, and fusing these disciplines together to build cool things. In this role, you will work on one or more software and data products in the Annalect Engineering Team. You will participate in technical architecture, design and development of software products as well as research and evaluation of new technical solutions."", 'Culture. We have an incredibly fun, collaborative and friendly environment, and often host social and learning activities such as game night, speaker series, and so much more! ', 'Passion for Technology: Excitement for new technology, bleeding edge applications, and a positive attitude towards solving real world challenges', 'We are an equal opportunity employer', 'BS, MS or PhD in Computer Science, Engineering, or equivalent real-world experience ', '4+ years of development experience on web applications using Python, Ruby, Java, or C#', 'Experience working with classical relational databases. You will be writing/maintaining aggregates and optimizing queries for quick reads from a Django web app.', 'Ability to learn and teach new technologies', 'Experience with Hadoop, Hive, Spark, or other data processing tools (Lots of time will be spent building and optimizing transformations)', 'This position is with Annalect, the Data, Technology, and Analytics division of Omnicom Media Group.', 'Designing, building, testing and deploying scalable, reusable and maintainable applications that handle large amounts of dataWrite at-scale ETL processes in Python, Spark, and other technologiesPerform code reviews and provide leadership and guidance to junior developersAbility to learn and teach new technologies', 'Curiosity in learning the business requirements that are driving the engineering requirements', 'Significant experience with Python, C++, or other popular language', 'Annalect, a division of Omnicom Media Group, reaffirms its commitment to the policy of Equal Employment Opportunity and to carrying out this policy at all of its offices. It shall be the policy of Omnicom Media Group to (1) recruit, select, hire, train, promote, pay, discipline and terminate employees in all job classifications without regard to age, race, color, creed, national origin, citizenship status, alienage, religion, sex, sexual orientation, marital status, veteran status, disability or any other basis upon which discrimination against or harassment of employees or applicants for employment is prohibited under any applicable federal, state or local equal opportunity employment laws and (2) ensure that all personnel actions are administered without discrimination in violation of applicable law.', '8+ years of SQL experience.', 'Perks of working at Annalect', 'Culture. We have an incredibly fun, collaborative and friendly environment, and often host social and learning activities such as game night, speaker series, and so much more! Generous vacation policy. Paid time off (PTO) includes vacation days, personal days, and a Summer Friday program.Extended time off around the holiday season. Our office is closed between Xmas and New Year to encourage our hardworking employees time to rest, recharge and celebrate the season with family and friends.As part of Omnicom, we have the backing and resources of a global billion-dollar company, but also have the flexibility and pace of a “startup” - we move fast, break things, and innovate.', 'BS, MS or PhD in Computer Science, Engineering, or equivalent real-world experience Significant experience with Python, C++, or other popular languageExperience with big data and/or infrastructure. Bonus for having experience in setting up Petabytes of data so they can be easily accessed. Understanding of data organization, ie partitioning, clustering, file sizes, file formats. Data cataloging with Hive/Hive metastore or Glue or something similar.Experience working with classical relational databases. You will be writing/maintaining aggregates and optimizing queries for quick reads from a Django web app.Experience with Hadoop, Hive, Spark, or other data processing tools (Lots of time will be spent building and optimizing transformations)Experience building scalable data pipelines (Airflow experience a plus)Significant experience working with AWS and/or GCPProven ability to independently execute projects from concept to implementation to launch and to maintain a live productInterest or experience in ML technologies (TensorFlow, PyTorch or SageMaker, BigQueryML)', 'Extended time off around the holiday season. Our office is closed between Xmas and New Year to encourage our hardworking employees time to rest, recharge and celebrate the season with family and friends.', 'Generous vacation policy. Paid time off (PTO) includes vacation days, personal days, and a Summer Friday program.', 'Position Overview', 'Interest in new technologies and eager to bring those technologies and out of the box ideas to the team', 'Experience with big data and/or infrastructure. Bonus for having experience in setting up Petabytes of data so they can be easily accessed. Understanding of data organization, ie partitioning, clustering, file sizes, file formats. Data cataloging with Hive/Hive metastore or Glue or something similar.', 'Significant experience working with AWS and/or GCP', 'Perform code reviews and provide leadership and guidance to junior developers', 'Interest or experience in ML technologies (TensorFlow, PyTorch or SageMaker, BigQueryML)', 'Write at-scale ETL processes in Python, Spark, and other technologies', 'Required Skills']",Mid-Senior level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer,LumenData,Washington DC-Baltimore Area,1 week ago,79 applicants,"['', 'Experience as a data engineer, software developer, database developer or ETL developer', 'Experience with hybrid data architectures consisting of on prep and cloud based COT applications', 'Experience with working in Agile environments a plusExperience with data management conceptsExperience with hybrid data architectures consisting of on prep and cloud based COT applicationsExperience working with data science and BI teamsExperience developing solutions involving unstructured dataExperience with the Medical Product / Healthcare domainsExperience with informatica IDQ, EDC, MDM, or Axon a plusExperienced with Mulesoft or other API platform a plus', 'Experienced with Mulesoft or other API platform a plus', 'Experience with working in Agile environments a plus', 'Demonstrate exceptional written communication skills with strong documentation skills focusing on accuracy, consistency, and standardized terminology.', 'Experience as a data engineer, software developer, database developer or ETL developerExperience with data engineering tools and techniques in support of developing data pipelines, including testing and automationExperience developing software code with object-oriented programing languages, including Java, C++, or C# Java and scripting languages, including Python, R, Bash, Batch, and PowerShellAbility to learn new technologies quicklyExperience with\xa0big data technologies (e.g. Sqoop, Flume, NiFi, Kinesis, Kafka, Elasticsearch, DeltaLake, Hive, Pig, ERM, etc.)Familiarity with enterprise cloud architectures like AWS, Azure, Open Stack, etc.Experience carrying out or supporting data migrations and integration (i.e. ETL and API development)Experience working with data management concepts and activities including an understanding of industry best practices for Data Sourcing, Key Data Elements, Data Quality Management, Metadata, and Enterprise Data Management Policy, Process, and ProceduresDemonstrate exceptional written communication skills with strong documentation skills focusing on accuracy, consistency, and standardized terminology.Ability to work within standardized and non-standardized processes to accomplish assigned tasks.Ability to effectively communicate and collaborate with internal teams', 'Leverage expertise in structured and unstructured data to perform data engineering activities to enable data intensive solutions. Architect data systems stand up data platforms, build-out ETL pipelines, write custom codes, interface with data stores, perform data ingestion, and enable automation. Assess, design, build, and maintain scalable data platform components. Perform analytical exploration and examination of data from multiple sources. Work with multi-disciplinary teams of analysts, data engineers, scientist, developers, and data consumers to deliver data solutions at scale.', 'Ability to learn new technologies quickly', 'Experience with data management concepts', 'Experience working with data management concepts and activities including an understanding of industry best practices for Data Sourcing, Key Data Elements, Data Quality Management, Metadata, and Enterprise Data Management Policy, Process, and Procedures', 'Experience carrying out or supporting data migrations and integration (i.e. ETL and API development)', 'Experience with the Medical Product / Healthcare domains', 'Basic Qualifications:', 'Additional Qualifications:', 'Experience developing solutions involving unstructured data', 'Familiarity with enterprise cloud architectures like AWS, Azure, Open Stack, etc.', '** Please send your resume to Neha.singh@LumenData.com and\xa0subhrojit.paul@LumenData.com. Citizens/Permanent Residents only please. Location: Washington DC/Virginia/Maryland **', 'Experience working with data science and BI teams', 'Experience developing software code with object-oriented programing languages, including Java, C++, or C# Java and scripting languages, including Python, R, Bash, Batch, and PowerShell', 'Ability to work within standardized and non-standardized processes to accomplish assigned tasks.', 'Experience with\xa0big data technologies (e.g. Sqoop, Flume, NiFi, Kinesis, Kafka, Elasticsearch, DeltaLake, Hive, Pig, ERM, etc.)', 'Experience with data engineering tools and techniques in support of developing data pipelines, including testing and automation', 'Experience with informatica IDQ, EDC, MDM, or Axon a plus', 'Ability to effectively communicate and collaborate with internal teams']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Stanley Martin Homes,"Reston, VA",2 days ago,Be among the first 25 applicants,"['', 'Comfortable building data lake storage and data processing strategies for analytic workloads', 'Familiarity with Microsoft Dynamics Finance and Operations 365 highly desired', '2+ years of experience using Azure Databricks to build scalable ETL pipelines', 'The Data Engineer supports the maintenance, synchronization, wrangling, and migration of data across a hybrid environment', 'Good communication skills. Able to present the technical solutions and understand business requirements', 'Familiarity with scripting and command line operations including execution using MS PowerShell', 'Someone comfortable wearing multiple data hats on a small team, and continuously seeking new techniques to improve data processing and analytics', '3+ years building scripts using SQL and at least one of the following: R, Python, Scala or Java2+ years developing data flow and data orchestration workflows (e.g. Azure Data Factory)2+ years of experience using Azure Databricks to build scalable ETL pipelinesComfortable building data lake storage and data processing strategies for analytic workloadsExperienced with developing automation workflows using RESTful APIsExperienced with change management workflows (Azure DevOps, Visual Studio, GitHub, TFS)Familiarity with scripting and command line operations including execution using MS PowerShellKnowledgeable of techniques to automate business-oriented data capture (e.g. Power Platform)Interest in developing streaming data pipelines using resources such as Azure Event Hubs, Spark-Streaming, Kafka, or Flink StreamsFamiliarity with Microsoft Dynamics Finance and Operations 365 highly desiredComfortable working in an Agile Kanban environment and using tools such as Jira or TrelloDetailed oriented/self-motivated with the ability to learn and deploy new technology quicklyGood communication skills. Able to present the technical solutions and understand business requirementsSomeone comfortable wearing multiple data hats on a small team, and continuously seeking new techniques to improve data processing and analyticsAdhere to company safety standards and help promote a safe working environmentAdhere to and promote the Mission, Vision, and Values of Stanley Martin Homes', 'The Data Engineer supports the maintenance, synchronization, wrangling, and migration of data across a hybrid environmentCollaborates to understand, analyze, document, and efficiently implement data pipelines for event-based data replication, including micro-batch and batch-based data pipelinesEstablishes methods to enrich the platform with new sources of information that support analytics and web application data delivery needsActively participates with broader team to enhance the capabilities of the data platform, including strengthening analytical workflows executing on our Business Intelligence tool (Looker)', 'Actively participates with broader team to enhance the capabilities of the data platform, including strengthening analytical workflows executing on our Business Intelligence tool (Looker)', 'Experienced with developing automation workflows using RESTful APIs', 'Establishes methods to enrich the platform with new sources of information that support analytics and web application data delivery needs', '3+ years building scripts using SQL and at least one of the following: R, Python, Scala or Java', '2+ years developing data flow and data orchestration workflows (e.g. Azure Data Factory)', 'Experienced with change management workflows (Azure DevOps, Visual Studio, GitHub, TFS)', 'Adhere to and promote the Mission, Vision, and Values of Stanley Martin Homes', 'Adhere to company safety standards and help promote a safe working environment', 'Interest in developing streaming data pipelines using resources such as Azure Event Hubs, Spark-Streaming, Kafka, or Flink Streams', 'Collaborates to understand, analyze, document, and efficiently implement data pipelines for event-based data replication, including micro-batch and batch-based data pipelines', 'Knowledgeable of techniques to automate business-oriented data capture (e.g. Power Platform)', 'Detailed oriented/self-motivated with the ability to learn and deploy new technology quickly', 'Comfortable working in an Agile Kanban environment and using tools such as Jira or Trello']",Entry level,Full-time,Information Technology,Construction,2021-03-24 13:05:10
Data Engineer,The Athletic,"United, LA",2 weeks ago,Be among the first 25 applicants,"['', 'Build tools and data marts to enable analytics.', ""Bachelor's degree in CS or relevant discipline."", 'Investigate, debug, and fix user-reported production issues.', ' About The Role ', ' Responsibilities ', 'Able to communicate results, outcomes and issues to different audiences.', 'Identify and fix issues and reduce tech debt.', 'Experience in orchestrating/scheduling data pipelines.', 'The current team is largely based in San Francisco, CA, but this role can be based in a 100% remote capacity within the United States or Canada.', 'Work with Data Science, and Product teams to build and deploy features backed by machine learning and modern data toolsets.', 'Architect and build data pipelines to optimize for performance, data quality, scalability, ease of future development, and cost.', 'Work with Data Science, and Product teams to build and deploy features backed by machine learning and modern data toolsets.Architect and build data pipelines to optimize for performance, data quality, scalability, ease of future development, and cost.Build tools and data marts to enable analytics.Identify and fix issues and reduce tech debt.Investigate, debug, and fix user-reported production issues.', 'Experience with Redshift, Kubernetes, and Spark desired.', 'Minimum 2 years data engineering experience.', 'Experience with Python, SQL, Docker, and cloud environment - AWS, GCP or Azure.', ""Minimum 2 years data engineering experience.Bachelor's degree in CS or relevant discipline.Experience with Python, SQL, Docker, and cloud environment - AWS, GCP or Azure.Experience with Redshift, Kubernetes, and Spark desired.Experience in orchestrating/scheduling data pipelines.Proficient in source code control systems.Able to communicate results, outcomes and issues to different audiences.The current team is largely based in San Francisco, CA, but this role can be based in a 100% remote capacity within the United States or Canada."", 'About Us ', 'Proficient in source code control systems.']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer - Backend,Roche,"South San Francisco, CA",2 days ago,31 applicants,"['', ' Synthesize requirements from non-technical stakeholders ', ' Git/Github (branches, code reviews, pull requests) ', ' RDBMS/NoSQL (MySQL preferred) ', ' Plotly/Dash experience preferred ', ' Work on agile teams (Scrum/Kanban) ', ' AWS experience preferred (lambda/glue/s3) ', ' Deploy data science models as microservices ', ' Develop and maintain RESTful JSON apis (FastAPI) ', ' Consume RESTful apis ', ' Work closely with data governance and technology practice areas ', ' Understand and maintain data schemas and architectures ', ' Develop and maintain scalable data pipelines  Develop and maintain RESTful JSON apis (FastAPI)  Consume RESTful apis  Understand and maintain data schemas and architectures  Understand and architect ETL processes  Deploy data science models as microservices  Synthesize requirements from non-technical stakeholders  Work closely with data governance and technology practice areas  Work on agile teams (Scrum/Kanban) ', 'Qualifications, Skills & Knowledge', ' Understand and architect ETL processes ', 'Data Engineer - Backend ', ' Develop and maintain scalable data pipelines ', ' Docker ', ' Python (FastAPI, Pandas preferred) ', ' Python (FastAPI, Pandas preferred)  Git/Github (branches, code reviews, pull requests)  RDBMS/NoSQL (MySQL preferred)  Docker  Plotly/Dash experience preferred  AWS experience preferred (lambda/glue/s3) ']",Entry level,Full-time,Information Technology,Hospital & Health Care,2021-03-24 13:05:10
Data Engineer,Horizontal Talent,"Minneapolis, MN",22 hours ago,28 applicants,"['Great understanding of Lambda architecture patterns.', 'Design, build, optimize, and manage modern large scale data pipelines ETL/ELT processing to support data integration for analytics, machine learning features and predictive modelling.', 'Participate in architectural evolution of data engineering patterns, frameworks, systems, and platforms including defining best practices and standards for managing data collections and integration.', 'Experience in Kafka', ' Design, build, optimize, and manage modern large scale data pipelines ETL/ELT processing to support data integration for analytics, machine learning features and predictive modelling. Consume data from a variety of sources (RDBMS, APIs, FTPs and other cloud storage) & formats (Excel, CSV, XML, JSON, Parquet, Unstructured) Write advanced / complex SQL with performance tuning and optimization. Identify ways to improve data reliability, data integrity, system efficiency and quality. Participate in architectural evolution of data engineering patterns, frameworks, systems, and platforms including defining best practices and standards for managing data collections and integration. Work with data scientists to deploy machine learning models to real-time analytics systems. Design and build data service APIs. Mentor other data engineers and provide significant technical direction by teaching other data engineers how to leverage cloud data platforms. ', "" 1) Design and develop ETL/ELT solutions on Azure Databricks, Delta Lake and Spark to support OptumRx Digital MBO's.  2) Develop, implement, and deploy large scale data pipelines powering machine learning algorithms, insights generation, business intelligence dashboards, reporting and new data products.  3) Partner with Optum Technology to create and maintain the technical architecture of the Enterprise Delta Lake to consolidate data from many systems into a single source for machine learning and reporting analytics. "", 'Experience in CI/CD technology', 'This is a unique, high visibility opportunity for someone who wants to have business impact, dive deep into large scale data pipeline and work closely with cross functional team..', 'Preferred Qualifications', 'Experience with at least one of the following cloud platforms: Azure, AWS or GCP', ' As a Data Engineer, you will work cross-functionally with data scientists, data analysts, product managers, and stakeholders to understand business needs and develop, maintain and optimize the data sets, data models and large-scale data pipelines primarily in the Azure Databricks  Spark cloud stack used for data science models and visualizations.  You will partner with Optum Technology team to drive best practices and set standards for data engineering patterns and optimization. You are a key influencer in data engineering strategy. This is a unique, high visibility opportunity for someone who wants to have business impact, dive deep into large scale data pipeline and work closely with cross functional team.. ', 'Mentor other data engineers and provide significant technical direction by teaching other data engineers how to leverage cloud data platforms.', 'Consume data from a variety of sources (RDBMS, APIs, FTPs and other cloud storage) & formats (Excel, CSV, XML, JSON, Parquet, Unstructured)', 'Experience in Git', ' An undergraduate degree in Computer Science, Engineering, Mathematics, Statistics, Economics or related discipline. 2+ years of experience in data engineering, data integration, data modeling, data architecture, and ETL/ELT processes to provide quality data and analytics solutions. 2+ years of experience in SQL with designing complex data schemas and query performance optimization. 2+ years of experience in Apache Spark (PySpark / Spark SQL) 2+ years of experience in Python Experience in integrating data from semi-structured. Experience with at least one of the following cloud platforms: Azure, AWS or GCP Excellent collaborator that are able to collaborate effectively cross-functional teams such as leadership, product management and engineering. and willingness to inspire other data engineers, data scientists and analysts. Excellent communication skills - ability to communicate technical concepts to both technical and non-technical audience. ', 'Required Qualifications', 'Experience in Databricks', 'Design and build data service APIs.', 'Experience in Big Data processing', '2+ years of experience in Apache Spark (PySpark / Spark SQL)', 'Identify ways to improve data reliability, data integrity, system efficiency and quality.', '2+ years of experience in data engineering, data integration, data modeling, data architecture, and ETL/ELT processes to provide quality data and analytics solutions.', 'As a Data Engineer, you will work cross-functionally with data scientists, data analysts, product managers, and stakeholders to understand business needs and develop, maintain and optimize the data sets, data models and large-scale data pipelines primarily in the Azure Databricks ', 'Ability to independently troubleshoot and performance tune large scale enterprise systems.', '2+ years of experience in SQL with designing complex data schemas and query performance optimization.', 'Excellent collaborator that are able to collaborate effectively cross-functional teams such as leadership, product management and engineering. and willingness to inspire other data engineers, data scientists and analysts.', 'Write advanced / complex SQL with performance tuning and optimization.', 'Extensive knowledge of data architecture principles (e.g., Data Lake, Databricks Delta Lake, Data Warehousing, etc.).', 'Work with data scientists to deploy machine learning models to real-time analytics systems.', '2+ years of experience in Python', 'Experience in working with large size data sets using Big Data Frameworks (Hadoop/EMR/Databricks/Spark/Hive etc.)', 'Experience in integrating data from semi-structured.', 'Description', 'You will partner with Optum Technology team to drive best practices and set standards for data engineering patterns and optimization. You are a key influencer in data engineering strategy.', 'An undergraduate degree in Computer Science, Engineering, Mathematics, Statistics, Economics or related discipline.', 'Spark cloud stack used for data science models and visualizations. ', '3) Partner with Optum Technology to create and maintain the technical architecture of the Enterprise Delta Lake to consolidate data from many systems into a single source for machine learning and reporting analytics.', ' Experience in working with large size data sets using Big Data Frameworks (Hadoop/EMR/Databricks/Spark/Hive etc.) Experience in Big Data processing Experience in Databricks Experience in Regular Expression Experience in Rest API Experience in NoSQL Experience in Kafka Experience in CI/CD technology Experience in Git Extensive knowledge of data architecture principles (e.g., Data Lake, Databricks Delta Lake, Data Warehousing, etc.). Extensive knowledge of data modelling techniques including slowly changing dimensions, aggregation, partitioning and indexing strategies. Ability to independently troubleshoot and performance tune large scale enterprise systems. Great understanding of Lambda architecture patterns.', 'Experience in Rest API', 'Extensive knowledge of data modelling techniques including slowly changing dimensions, aggregation, partitioning and indexing strategies.', '2) Develop, implement, and deploy large scale data pipelines powering machine learning algorithms, insights generation, business intelligence dashboards, reporting and new data products. ', ""1) Design and develop ETL/ELT solutions on Azure Databricks, Delta Lake and Spark to support OptumRx Digital MBO's. "", 'Major Responsibilities', 'Excellent communication skills - ability to communicate technical concepts to both technical and non-technical audience.', 'Purpose Of Position', 'Experience in Regular Expression', 'Experience in NoSQL']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Vaco,Nashville Metropolitan Area,,N/A,"['', 'Position Summary:', 'Minimum Qualifications:', 'Minimum Qualifications', 'Collaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.', 'Excellent problem solving and troubleshooting skills', 'The Analytics Engineer will be responsible for expanding and optimizing our data architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data wrangler who enjoys optimizing data systems and building them from the ground up. The Analytics Engineer will support a broad range of business stakeholders on all things analytics and data initiatives, while ensuring a consistent and optimal data delivery architecture. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products, tools and data initiatives.', 'Design, deploy and manage data integrations and platforms from a wide variety of data sources using Snowflake, SQL, and AlteryxCollaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.Performs data analysis required to troubleshoot data related issues and assists in the resolution of data issues.Ensures existing data sources are always accurate and available for key stakeholders and business processes that depend on it.Seeks out and designs new data sources and integrations to aid in new insights and metrics to achieve business objectives, drive business growth and revenue.Maintain a good knowledge of relevant analytical techniques.', 'Performs data analysis required to troubleshoot data related issues and assists in the resolution of data issues.', '**This role will offer a relocation package for the right candidate**', '3+ years experience with Alteryx and other data blending tools.', 'Maintain a good knowledge of relevant analytical techniques.', 'Experience working with and manipulating large and disparate data sources with tools like SQL, Alteryx, etc.', '\xa0', 'Design, deploy and manage data integrations and platforms from a wide variety of data sources using Snowflake, SQL, and Alteryx', 'Bachelor’s degree in an analytical area such as MIS, Finance, or Business Analytics and 2-5 years of experience in reporting, analytics or business insights', 'Ability to distill, cater, consult, and communicate complex analysis and insights to senior leaders, both verbally and in writing', 'Strong understanding of data extraction and manipulation from ERP systems like SAP', 'Strong project and time management skills with experience partnering across multiple stakeholders', 'Seeks out and designs new data sources and integrations to aid in new insights and metrics to achieve business objectives, drive business growth and revenue.', 'Key Responsibilities:', 'Bachelor’s degree in an analytical area such as MIS, Finance, or Business Analytics and 2-5 years of experience in reporting, analytics or business insights2+ years experience in relational cloud based database technologies like Snowflake, Amazon Redshift, etc.3+ years experience with Alteryx and other data blending tools.Strong understanding of data extraction and manipulation from ERP systems like SAPExperience in data visualization, including the ability to design meaningful data analyses in Tableau, PowerBI or similar data visualization software.Experience working with and manipulating large and disparate data sources with tools like SQL, Alteryx, etc.Ability to distill, cater, consult, and communicate complex analysis and insights to senior leaders, both verbally and in writingExcellent problem solving and troubleshooting skillsStrong project and time management skills with experience partnering across multiple stakeholders', 'Experience in data visualization, including the ability to design meaningful data analyses in Tableau, PowerBI or similar data visualization software.', '**Team is currently remote, but plan to come back into the office when safe**', '2+ years experience in relational cloud based database technologies like Snowflake, Amazon Redshift, etc.', 'Ensures existing data sources are always accurate and available for key stakeholders and business processes that depend on it.']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer (x2 Openings),H2O / Overgroup,"Atlanta, GA",1 week ago,54 applicants,"['', 'Specific Duties Will Include', 'Ability to work full-time onsite daily and work with the team', '1-2 years minimum of professional experience working with SQL (required)', 'Company Overview', 'Position Description', 'Bachelor’s Degree in Database Management, Information Technology, Computer Science, Computer Information Science, or other database and software development-focused majors (preferred)', 'Atlassian products (JIRA, Confluence, etc.)', 'Full Microsoft Stack (Visual Studio 2013, SQL Server 2014/2019, HTML5/JS)', 'Responsible for working with teammates and clients to determine innovative solutions to problems experienced when installing new customers.', 'Strong debugging skills', 'Gain exposure to large customer data sets involved in product migrations, and responsible for helping data flow into Rev.io from legacy customer systems.', 'Implement client-driven reporting and data migration needs within the Rev.io platform with exposure to user interface and database changes using SQL and ETL tools.Responsible for working with teammates and clients to determine innovative solutions to problems experienced when installing new customers.Involved in the database testing and will actively participate in workflows involving business intelligence and reporting.Gain exposure to large customer data sets involved in product migrations, and responsible for helping data flow into Rev.io from legacy customer systems.Hands-on experience working with live client systems and configuring real production environments.', 'Involved in the database testing and will actively participate in workflows involving business intelligence and reporting.', 'Hands-on experience working with live client systems and configuring real production environments.', 'Have worked with BI/Visualization tools (preferred)', 'Have worked with ETL tools for data migration (preferred)', 'Strong communicator, self-driven, and ability to meet deadlines', '1-2 years minimum of professional experience working with SQL (required)1-2 years of experience with T-SQL (preferred)Bachelor’s Degree in Database Management, Information Technology, Computer Science, Computer Information Science, or other database and software development-focused majors (preferred)Have worked with ETL tools for data migration (preferred)Have worked with BI/Visualization tools (preferred)Strong communicator, self-driven, and ability to meet deadlinesAbility to work full-time onsite daily and work with the teamStrong debugging skills', 'Implement client-driven reporting and data migration needs within the Rev.io platform with exposure to user interface and database changes using SQL and ETL tools.', '1-2 years of experience with T-SQL (preferred)', 'Python/Spark', 'Atlassian products (JIRA, Confluence, etc.)Full Microsoft Stack (Visual Studio 2013, SQL Server 2014/2019, HTML5/JS)Python/Spark', 'Uses Cutting-edge Technology In The Workplace Including']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,ServiceNow,"Santa Clara, CA",19 hours ago,47 applicants,"['', 'Experience in machine learning especially in data quality applications a plus ', 'Strong statistical background ', 'Able to identify stakeholders, build relationships, and influence others to get work done. ', 'Proactively seek out data quality opportunities to evangelize a data quality mindset across ServiceNow', 'BA/BS in Mathematics, Statistics, Computer Science, Economics, Business or analytical field ', '\u202f\u202f', 'Develop interactive data visualizations that enable business insights; work with business stakeholders to tease out the best stories to tell from a given dataset ', '2 + years of SQL experience and delivering actionable insights on data quality for high scale data processes ', 'Highly detailed oriented ', 'To be successful in this role you have:', 'Additional Information', 'Develop interactive data visualizations that enable business insights; work with business stakeholders to tease out the best stories to tell from a given dataset Build scalable analytical framework to monitor data quality supporting analytics organization (Sales Analytics, Finance Analytics, Marketing Analytics, Product, and Talent Analytics) Work with various data teams to deploy data quality across critical pipelines and to set up processes to triage data issues Create and drive data quality standards and frameworks to ensure inclusion into telemetry Create metrics and reporting functions to measure and monitor data quality for our most critical data assets Effectively communicate data quality insights and drive projects to improve data quality Proactively seek out data quality opportunities to evangelize a data quality mindset across ServiceNow', 'Deep experience in data visualization and reporting leveraging open source libraries/packages and third-party tools (Tableau or similar) ', 'BA/BS in Mathematics, Statistics, Computer Science, Economics, Business or analytical field 2 + years of SQL experience and delivering actionable insights on data quality for high scale data processes Deep experience in data visualization and reporting leveraging open source libraries/packages and third-party tools (Tableau or similar) Strong statistical background Experience in machine learning especially in data quality applications a plus Highly detailed oriented Excellent judgment, critical-thinking, and decision-making skills; can balance attention to detail with swift execution Able to identify stakeholders, build relationships, and influence others to get work done. Exceptional verbal and written communication skills and ability to engage effectively at all levels of the organization.', 'Create and drive data quality standards and frameworks to ensure inclusion into telemetry ', 'Exceptional verbal and written communication skills and ability to engage effectively at all levels of the organization.', 'Build scalable analytical framework to monitor data quality supporting analytics organization (Sales Analytics, Finance Analytics, Marketing Analytics, Product, and Talent Analytics) ', 'Work with various data teams to deploy data quality across critical pipelines and to set up processes to triage data issues ', 'Effectively communicate data quality insights and drive projects to improve data quality ', 'Qualifications', 'Excellent judgment, critical-thinking, and decision-making skills; can balance attention to detail with swift execution ', 'Company Description', 'What you get to do in this role:', 'Create metrics and reporting functions to measure and monitor data quality for our most critical data assets ', 'Job Description']",Not Applicable,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Data Engineer,Charles Schwab,"Westlake, TX",2 weeks ago,41 applicants,"['', ' 2+ years of experience working on agile teams delivering data solutions ', ' Analyzing the current technology environment to detect critical deficiencies, and recommend solutions for improvement ', ' Basic understanding of at least one IT Management frameworks such as ITIL or COBiT ', 'Your Opportunity ', ' Exceptional interpersonal skills, including teamwork, communication, and negotiation', ' Developing continuous integration and continuous deployment pipelines for data solutions that include automated unit & integration testing ', ' Designing, implementing, and maintaining data warehouses and near real-time data pipelines via the practical application of existing and new data engineering techniques ', ' Ability to quickly learn & become proficient with new technologies ', ' Ensuring consistency with published development, coding and testing standards ', 'What You Have', ' Experience writing automated unit, integration, and acceptance tests for data interfaces & data pipelines ', ' 1+ years of experience delivering solutions on public cloud platforms (Google Cloud preferred) ', ' 3+ years of experience designing, building, and supporting near real-time data pipelines and analytical solutions using Hadoop, Teradata, MS SQL Server, Talend, Informatica, and/or SSIS  2+ years of experience working on agile teams delivering data solutions  2+ years of experience building data pipelines and interfaces with object oriented languages (.Net, Java, Python)  1+ years of experience modeling star schema data warehouses using the Kimball dimensional modeling techniques  1+ years of experience delivering solutions on public cloud platforms (Google Cloud preferred)  Basic understanding of at least one IT Management frameworks such as ITIL or COBiT  Experience writing automated unit, integration, and acceptance tests for data interfaces & data pipelines  Ability to quickly learn & become proficient with new technologies  Exceptional interpersonal skills, including teamwork, communication, and negotiation', ' Collaborating directly with business and technology stakeholders to define future-state business capabilities & requirements, and translating those into transitional and target state data architectures  Analyzing the current technology environment to detect critical deficiencies, and recommend solutions for improvement  Designing, implementing, and maintaining data warehouses and near real-time data pipelines via the practical application of existing and new data engineering techniques  Developing continuous integration and continuous deployment pipelines for data solutions that include automated unit & integration testing  Mentoring, motivating, and supporting the team to achieve organizational objectives and goals  Advocating for agile practices to increase delivery throughput  Ensuring consistency with published development, coding and testing standards ', ' Mentoring, motivating, and supporting the team to achieve organizational objectives and goals ', 'What You Are Good At', ' 3+ years of experience designing, building, and supporting near real-time data pipelines and analytical solutions using Hadoop, Teradata, MS SQL Server, Talend, Informatica, and/or SSIS ', ' 1+ years of experience modeling star schema data warehouses using the Kimball dimensional modeling techniques ', ' 2+ years of experience building data pipelines and interfaces with object oriented languages (.Net, Java, Python) ', ' Collaborating directly with business and technology stakeholders to define future-state business capabilities & requirements, and translating those into transitional and target state data architectures ', ' Advocating for agile practices to increase delivery throughput ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Red Oak Technologies,"Austin, TX",2 days ago,87 applicants,"['', 'NoSQL\t2 + years', 'Austin, TX********relocation is required by client***************', '- Relational, NoSQL and Graph databases, HDFS, Kafka, multiple data centers replication strategies and disaster recovery\t', 'Nginx\tAt least 1 year', ' ', '- Java 11+, reactive Java, Scala, Netty and Nginx\t', 'Data Engineering\t8 + years', 'it on code heavily relying on distributed messaging, storage and computing\xa0', 'Lead Experience \t2 + years', 'Distributed Systems\t5 + years of experience\xa0', 'Java 11\t2 + years', 'Kafka\t2 + years', 'and batching for billions of events and multi-petabytes of data\t', '- Extensive experience designing distributed applications architecture and implementing\xa0', 'Scala\t2 + years', '- 2+ years as a tech lead\t', '- Hands-on experience in high volume events processing using both reactive streaming\xa0', 'Lead Data Engineer (Java11, Scala, Kafka, NoSQL, Streaming)\xa0', '- Education - B.S./M.S. in Computer Science or Computer Engineering or 3+ years of equivalent experience', 'Stream processing\t5 + years of experience\xa0', '- Experience with CI/CD pipelines, unit and integration testing, containerization, monitoring and alerting, production logs debugging\t', '- Strong collaboration skills, system thinking and ability to clearly explain complex concepts\t', '- 8+ years of experience building scalable back end services\t']",Mid-Senior level,Contract,Information Technology,Staffing and Recruiting,2021-03-24 13:05:10
Data Engineer,"Eclypsium, Inc.","Portland, Oregon Metropolitan Area",7 days ago,94 applicants,"['', 'POSITION DESCRIPTION', 'ABOUT ECLYPSIUM', 'Strong troubleshooting skills', 'Comprehensive medical, dental, vision coverage', 'Home office support for remote employees', 'Proactively identify and rectify any issues found in the scraping and data extraction pipelines.', 'Experience with the Scrapy framework + Splash\xa0', 'Hands-on experience with GPG, PGP, Yubikeys and Authenticode', 'Execute release and deployment procedures', 'Competitive compensation & startup equity', 'Eclypsium is seeking an engineer to support the data discovery, collection, update and the final packaging, assurance, and integration of our products. This role will support overall product quality but specifically ensure customer success by providing quick turn-around for data quality and product improvements in mission-critical customer environments.\xa0', 'Regular events and celebrations', 'Eclypsium delivers a cloud-based enterprise device security platform for modern distributed organizations. From corporate laptops and desktops, to servers in data centers, to network infrastructure devices, Eclypsium protects the devices that organizations rely on, all the way down to firmware. Eclypsium provides comprehensive device and firmware inventory, automatically identifies and patches firmware risks, scans devices for supply chain breaches, and continuously monitors devices for persistent and stealthy firmware attacks. Eclypsium’s cloud-based solution is deployed in minutes. Protecting Fortune 100 enterprises and federal agencies, Eclypsium was named a Gartner Cool Vendor in Security Operations and Threat Intelligence, a TAG Cyber Distinguished Vendor, one of the World’s 10 Most Innovative Security Companies by Fast Company, a CNBC Upstart 100, a CB Insights Cyber Defender, and an RSAC Innovation Sandbox finalist. For more information, visit\xa0eclypsium.com.', 'LOCATION REQUIREMENT:\xa0US West coast, Oregon, Washington, or SF Bay area. ', 'Eclypsium is headquartered in Portland, OR with distributed remote employees and global teams in Argentina and the Bay Area. We offer competitive compensation and benefits packages and are committed to the wellbeing of our employees and their families.\xa0', 'BENEFITS', 'EQUAL OPPORTUNITY', 'Strong understanding of web scraping techniques, data extraction and how to analyse crawling processes, create\xa0data pipelinesSolid understanding of web technologies (HTML, JavaScript, CSS, XPath, JSON, etc)Strong troubleshooting skillsSolid Linux and Git foundationsHands-on experience with GPG, PGP, Yubikeys and AuthenticodeExperience with a document oriented databaseFamiliarity with data processing tools (pandas, regex, SQL)', 'Competitive compensation & startup equityComprehensive medical, dental, vision coverageLife insurance, short term and long term disability coverageFlexible time off\xa0Employee assistance programPaid parental leaveHome office support for remote employeesRegular events and celebrations', 'Create and maintain crawlers and scraping processes', 'Experience with a document oriented database', 'Flexible time off\xa0', '3+ years of experience as a Developer with Python', 'Solid understanding of web technologies (HTML, JavaScript, CSS, XPath, JSON, etc)', 'Solid Linux and Git foundations', 'Knowledge of PKI and code signing concepts', 'Eclypsium is an equal opportunity employer. We believe in the importance of diverse teams and value candidates of all backgrounds.\xa0We do not discriminate on the basis of age, ancestry, citizenship, color, ethnicity, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or invisible disability status, political affiliation, veteran status, race, religion, or sexual orientation.\xa0', 'Strong understanding of web scraping techniques, data extraction and how to analyse crawling processes, create\xa0data pipelines', '\ufeff', 'Perform high level investigations for issues in client data', 'Benefits & Perks include:', 'Familiarity with data processing tools (pandas, regex, SQL)', 'RESPONSIBILITIES', '3+ years of experience as a Developer with PythonExperience with the Scrapy framework + Splash\xa0Knowledge of PKI and code signing conceptsComputer Science or equivalent education', 'Life insurance, short term and long term disability coverage', 'Provide quick resolution to data issues and new scraping requests generated by internal or external clients.', 'The scope of your responsibilities will include web-scraping, data extraction and data analytics (50%), product deployment management and signing (25%) and production support and feature implementation (25%).', 'MINIMUM QUALIFICATIONS', 'ABOVE AND BEYOND', 'Computer Science or equivalent education', 'Paid parental leave', 'The scope of your responsibilities will include web-scraping, data extraction and data analytics (50%), product deployment management and signing (25%) and production support and feature implementation (25%).Create and maintain crawlers and scraping processesProactively identify and rectify any issues found in the scraping and data extraction pipelines.Provide quick resolution to data issues and new scraping requests generated by internal or external clients.Execute release and deployment proceduresPerform high level investigations for issues in client data', 'Employee assistance program']",Entry level,Full-time,Information Technology,Computer & Network Security,2021-03-24 13:05:10
Data Engineer,Atyeti Inc,"New York, United States",5 hours ago,172 applicants,"['', 'Demonstrable experience designing and developing big data applications using Apache Spark and Airflow', 'in NYC & Raleigh, NC', '\ufeffJob Description', 'Atyeti Ranks No. 270 on the 2012 Inc. 500 List', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Must have expertise on RDBMS solutions such as Oracle, SQL Server, Azure SQL DB etc..', 'Data Engineer for its team', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Excellent Python programming skills and experience delivering projects using PySpark is a must. Scala programming experience is nice to have.', '2012,2016 and 2017 NJ 50 Fastest Growing Companies', 'Global Investment Bank is looking for a Data Engineer for its team in NYC & Raleigh, NC. Long term contract.', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience leveraging Databricks to develop Spark based applications along with very good understanding about data engineering capabilities provided by the service', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience designing and developing Airflow DAGs, Operators etc…', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience using Cloud data warehouse technologies such as Snowflake is highly desired', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience designing and developing real time data processing applications using Spark streaming, Kafka, Event Hub etc.. ', 'Atyeti Recognition:', 'Inc. 500 & 5000 Honoree Company for 2012,2013,2014,2015, 2016, and 2017Atyeti Ranks No. 270 on the 2012 Inc. 500 List2012,2016 and 2017 NJ 50 Fastest Growing Companies', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience leveraging Azure Storage and Data Lake Store Gen 2 in Azure based big data projects along with\xa0integration to other Azure Data services', 'Those authorized to work in the United States without sponsorship are encouraged to apply due to the nature of the project.', 'Inc. 500 & 5000 Honoree Company for 2012,2013,2014,2015, 2016, and 2017']",Associate,Contract,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Overhaul,"Austin, TX",6 days ago,Be among the first 25 applicants,"['', 'Collaborative, communicative, and consultative work style', ""Bachelor's degree or higher in a quantitative/technical field (e.g. Computer Science, Statistics, Engineering)"", '3+ years of experience working with BI or data warehouse technologies in support of insights and reporting', 'Understanding of ETL, ELT, star schema, and other data model and data warehouse concepts, techniques, and best practices', 'Ability to succeed in a fast-paced, innovative, and rapidly evolving industry and business organization', 'Progressive advancement opportunity and career mobility', 'Write and maintain containerized (e.g. Docker) ETL code using Python, R, or other programming language', 'Engage and coordinate with data science, engineering, analytics, and others at Overhaul in support of data initiatives', 'Caregiver/adoption/family leave', 'Good communication and presentation skills with the ability to explain concepts and conclusions around data and insights in a clear, concise, and compelling way', 'Detail-oriented with the ability to effectively manage multiple competing priorities', 'Experience with data pipeline tools such as Airflow or Pachyderm', 'Who We Are', 'Experience with Data Lake architectures, and with combining structured and unstructured data into unified representations', 'About You', 'Experience with Docker and Kubernetes', 'Experience working with Snowflake or other applicable SQL data warehouse technologies', 'Develop effective schema and queries on structured data in support of insights for both internal-facing and customer-facing use cases, and effective repositories of unstructured data in support of same', 'Strong interpersonal skills and experience interfacing with others internally and externally from the company', ' Proven work experience as a Data Engineer or similar role Collaborative, communicative, and consultative work style Detail-oriented with the ability to effectively manage multiple competing priorities Ability to succeed in a fast-paced, innovative, and rapidly evolving industry and business organization Excellent time-management skills ', 'Flexible Working', 'Experience with AWS services including S3, EKS, ECR, EMR, and Kinesis', 'Manage and drive improvements for the metrics collection pipeline, data processing, and self-service data & insight tools', '3+ years of experience in scripting languages like Python etc', "" 3+ years of experience working with BI or data warehouse technologies in support of insights and reporting 3+ years of SQL experience with ability to write and tune SQL jobs for a variety of usage patterns 3+ years of relevant experience in one of the following areas: Data engineering, database engineering, business intelligence or business analytics 3+ years of experience in scripting languages like Python etc Bachelor's degree or higher in a quantitative/technical field (e.g. Computer Science, Statistics, Engineering) Experience with AWS services including S3, EKS, ECR, EMR, and Kinesis Strong interpersonal skills and experience interfacing with others internally and externally from the company Understanding of ETL, ELT, star schema, and other data model and data warehouse concepts, techniques, and best practices Good communication and presentation skills with the ability to explain concepts and conclusions around data and insights in a clear, concise, and compelling way Experience working with Snowflake or other applicable SQL data warehouse technologies Experience with Data Lake architectures, and with combining structured and unstructured data into unified representations Experience with Docker and Kubernetes Experience with data pipeline tools such as Airflow or Pachyderm Experience working with Tableau, Looker, or other modern data visualization tools "", 'Be stewards and evangelists for data driven culture and data best practices within the company', 'Be customer zero, leveraging our product and providing feedback as one of the key target personas that the product intends to provide value for', 'Collaborate with business groups at Overhaul to gather requirements around key insights and reporting needs', 'Unlimited vacation policy', ' Collaborate with business groups at Overhaul to gather requirements around key insights and reporting needs Develop effective schema and queries on structured data in support of insights for both internal-facing and customer-facing use cases, and effective repositories of unstructured data in support of same Develop new data and analytics capabilities for our customer-facing across a variety of initiatives Administer and optimize our SaaS-based data warehouse and BI infrastructure in support of self-service analytics dashboards, ad hoc analytics, and reporting Create a star schema data model (or better!) and architecture to ensure performance and usefulness across the organization Write and maintain containerized (e.g. Docker) ETL code using Python, R, or other programming language Write and maintain SQL jobs in support of ETL/ELT and BI analysis, reporting, and visualization, ability to troubleshoot SQL jobs as required Engage and coordinate with data science, engineering, analytics, and others at Overhaul in support of data initiatives Maintain diagrams and documentation of data models and data flows as needed to support understanding and troubleshooting of data infrastructure Build and maintain internal data catalog including data dictionaries, glossary, and curated datasets in support of easy consumption by the rest of the company Manage and drive improvements for the metrics collection pipeline, data processing, and self-service data & insight tools Be stewards and evangelists for data driven culture and data best practices within the company Be customer zero, leveraging our product and providing feedback as one of the key target personas that the product intends to provide value for ', 'What We Commit To You', 'Write and maintain SQL jobs in support of ETL/ELT and BI analysis, reporting, and visualization, ability to troubleshoot SQL jobs as required', 'Develop new data and analytics capabilities for our customer-facing across a variety of initiatives', 'Create a star schema data model (or better!) and architecture to ensure performance and usefulness across the organization', 'Proven work experience as a Data Engineer or similar role', 'Build and maintain internal data catalog including data dictionaries, glossary, and curated datasets in support of easy consumption by the rest of the company', 'Excellent time-management skills', 'Administer and optimize our SaaS-based data warehouse and BI infrastructure in support of self-service analytics dashboards, ad hoc analytics, and reporting', 'Key Responsibilities:', '3+ years of SQL experience with ability to write and tune SQL jobs for a variety of usage patterns', 'Experience working with Tableau, Looker, or other modern data visualization tools', ' Competitive starting base salary with performance-based increases Progressive advancement opportunity and career mobility Top employee health and well- being benefits Unlimited vacation policy Rotating company perks at work program Caregiver/adoption/family leave Free parking Flexible Working ', 'Maintain diagrams and documentation of data models and data flows as needed to support understanding and troubleshooting of data infrastructure', '3+ years of relevant experience in one of the following areas: Data engineering, database engineering, business intelligence or business analytics', 'Competitive starting base salary with performance-based increases', 'Top employee health and well- being benefits', 'Minimum requirements:', 'Our Culture', 'Rotating company perks at work program', 'The Role', 'Free parking']",Not Applicable,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Junior Data Engineer,AimHire,"Denver, CO",,N/A,"['', 'Remote work flexibility', 'Be a key contributor to the cloud-based applications and services using SQL, Azure Data Factory, Rest APIs, SQL Server/Synapse and Power Bi', '401k match', 'We are recruiting on behalf of our client, Insurium, located in Downtown Denver. They are a recognized leader in the SaaS insurance technology market. The software delivers automation and digital transformation support for the entire commercial insurance lifecycle. CHSI’S dynamic, high energy environment is focused on collaboration and a Work Hard, Play Hard mentality. If you have an entrepreneurial spirit and the passion to work directly with customers, this is a great opportunity to put those to good use!', 'Unlimited PTO', 'Experience with databases (Postgres, SQL Server, etc)', 'Insurium is seeking a driven and motivated Junior Data Engineer to join their growing company. In this role, you will work as part of a truly innovative and growing team responsible for developing and advancing how we deal with all things data related. You will be a key member of the data team responsible for automating, standardizing, and optimizing every piece of our architecture that involves ETL, warehousing, reporting, and data visualization.', 'Insurium', 'Energetic and collaborate office culture', 'Preferred expert level SQL knowledge with extensive ETL experience and the ability to diagnose and solve DB performance issues', 'Develop solutions that support their customers self-service reporting needs and streamline their implementation team’s workflows', '9 paid holidays', 'Embed reporting and data visualizations within the application', 'This is an exciting opportunity to quickly grow your career and offers a generous benefits package and compensation in the $90-$100K range, depending on experience.\xa0', '1-2 years of experience in ReactJS', 'Comprehensive medical plan, including monthly allowance added to HSA', '\xa0', 'Unlimited PTO9 paid holidays401k matchComprehensive medical plan, including monthly allowance added to HSAPaid parental leaveRemote work flexibilityEnergetic and collaborate office cultureFree office snacks and beverages', 'Familiarity with agile software development methodologies ', 'Focus on continuous improvement', '2-3 years of experience building solutions in Power BI and Azure Synapse or a comparable business intelligence solution preferredPreferred expert level SQL knowledge with extensive ETL experience and the ability to diagnose and solve DB performance issuesBS/MS in Computer Science (Or related field), code bootcamp graduate, or equivalent experience requiredFamiliarity with agile software development methodologies 1-3 years of professional software development experience1-2 years of experience in ReactJSExperience with databases (Postgres, SQL Server, etc)Willingness to jump in and get the job done no matter how big or smallFocus on continuous improvementExcellent communication skills', 'Be a key contributor to the cloud-based applications and services using SQL, Azure Data Factory, Rest APIs, SQL Server/Synapse and Power BiEmbed reporting and data visualizations within the applicationDevelop solutions that support their customers self-service reporting needs and streamline their implementation team’s workflowsWrite maintainable code and apply automated testing where applicableWork with your team to make architectural decisionsDeliver value to the business in a fast-paced agile environment', 'Free office snacks and beverages', 'Write maintainable code and apply automated testing where applicable', 'BS/MS in Computer Science (Or related field), code bootcamp graduate, or equivalent experience required', 'What You Will Get', 'What You Will be Doing', 'Excellent communication skills', 'This is NOT a fully remote position. This position is based in their office in Denver, CO, but will be temporarily remote due to COVID-19.', 'Work with your team to make architectural decisions', 'Deliver value to the business in a fast-paced agile environment', 'What You Need', '2-3 years of experience building solutions in Power BI and Azure Synapse or a comparable business intelligence solution preferred', 'Paid parental leave', '1-3 years of professional software development experience', 'AimHire is an Equal Opportunity/Affirmative Action Employer.', 'Willingness to jump in and get the job done no matter how big or small']",Associate,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Data Engineer,Crystal Equation Corporation,"New York, NY",1 week ago,184 applicants,"['BS/BTech in Computer Science, Math or related field', 'Experience with programming languages, Python', '4+ of SQL (Oracle, Vertica, Hive, etc.) or relational database', 'Define and manage SLA for all data sets in allocated areas of ownership', 'Build visualization to drive business decisions', 'Responsibilities:', 'Work with data infrastructure to triage infra issues and drive to resolution', 'Work cross-functionally to define problem statements, collect data, and make recommendations', 'Support on-call shift as needed to support the team', ""4+ years' experience in the data warehouse space"", 'Build and launch efficient & reliable data pipelines to move and transform data (both large and small amounts)', 'Build and ensure data tables can serve as the source of truth for various high-level priorities', 'Experience with data architecture, data modeling, schema design and software development', 'Experience initiating and driving projects, and communicating data warehouse plans to internal clients/stakeholders', 'Build data expertise and own data quality for allocated areas of ownership', '\xa0', '4+ experience (Oracle, MySQL) writing queries', 'Experience with large data sets, Hadoop, and data visualization tools', 'Skills: Minimum Qualifications', 'Familiar with version control systems (git, mercurial, etc.)', ""BS/BTech in Computer Science, Math or related field4+ years' experience in the data warehouse space4+ custom ETL/data pipeline design, implementation and maintenance\xa04+ of SQL (Oracle, Vertica, Hive, etc.) or relational database4+ experience (Oracle, MySQL) writing queriesExperience with programming languages, PythonExperience with data architecture, data modeling, schema design and software developmentExperience with large data sets, Hadoop, and data visualization toolsExperience initiating and driving projects, and communicating data warehouse plans to internal clients/stakeholdersFamiliar with version control systems (git, mercurial, etc.)"", 'Build and launch efficient & reliable data pipelines to move and transform data (both large and small amounts)Optimize and maintain existing pipelines, ensuring that data arrives accurately and on-timeBuild and ensure data tables can serve as the source of truth for various high-level prioritiesCreate scripts to automate operational processesWork cross-functionally to define problem statements, collect data, and make recommendationsBuild data expertise and own data quality for allocated areas of ownershipDefine and manage SLA for all data sets in allocated areas of ownershipWork with data infrastructure to triage infra issues and drive to resolutionSupport on-call shift as needed to support the teamBuild visualization to drive business decisions', '4+ custom ETL/data pipeline design, implementation and maintenance\xa0', 'Optimize and maintain existing pipelines, ensuring that data arrives accurately and on-time', 'Create scripts to automate operational processes']",Entry level,Contract,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Eliassen Group,"Columbia, MD",4 weeks ago,146 applicants,"['', 'Hands-on experience with SQL database design', 'Facilitate data-driven decision making across the organization', 'At least 3 years of experience and a background in software architecture', 'Strong communication skills', ""Bachelor's degree in Computer Science or equivalentAt least 3 years of experience and a background in software architectureTechnical expertise with data models, data mining, and segmentationKnowledge of programming languages like Java and PythonExperience with Salesforce and SAP data structuresHands-on experience with SQL database designKnowledge of reporting tools like Power BI or TableauExcellent numerical and analytical skillsStrong communication skills"", 'Knowledge of reporting tools like Power BI or Tableau', 'Keywords: Data Engineer, Salesforce, SAP, SQL, Power BI, Tableau', 'Implement processes and system to monitor data quality', 'Eliassen Group is working with a growing organization and is looking to for a Data Engineer to add to the dynamic team. You will work in a collaborative team to build and maintain scalable data platforms that support data-driven decisions enterprise-wide.\xa0\xa0', 'Requirements of the Data Engineer:', 'Develop and maintain scalable data pipelines', 'Experience with Salesforce and SAP data structures', ""Bachelor's degree in Computer Science or equivalent"", '\xa0', 'This is a contract to hire opportunity. Applicants must be willing and able to work on a w2 basis and convert to FTE following contract duration. For our w2 consultants, we offer a great benefits package that includes Medical, Dental, and Vision benefits, 401k with company matching, and life insurance.', 'Develop and maintain scalable data pipelinesBuild out new API integrations to support increasing data volume and complexityWork with analytics and business teams to improve data models that feed BI toolsFacilitate data-driven decision making across the organizationImplement processes and system to monitor data qualityWrite unit and integration tests and document workHelp to drive a strategy of continuous improvement', 'Job ID: 349364', 'Help to drive a strategy of continuous improvement', 'Knowledge of programming languages like Java and Python', 'Excellent numerical and analytical skills', 'Build out new API integrations to support increasing data volume and complexity', 'Work with analytics and business teams to improve data models that feed BI tools', 'Write unit and integration tests and document work', 'Technical expertise with data models, data mining, and segmentation', 'Responsibilities of the Data Engineer:']",Associate,Contract,Information Technology,Staffing and Recruiting,2021-03-24 13:05:10
Data Engineer (Any Location) ,Aimpoint Digital,United States,2 days ago,92 applicants,"['', 'Knowledge of software engineering concepts and best practices (DevOps experience is\xa0bonus!)\xa0', 'Experience with data modeling (e.g.\xa0star schema, entity-relationship)\xa0', 'Experience with managing stakeholders and collaborating with\xa0customers\xa0', 'Consulting experience\xa0preferred\xa0', 'Degree educated in Computer Science, Engineering, Mathematics, or equivalent\xa0experience\xa0', 'Strategic assessment of existing infrastructure and business processes\xa0Designing and developing the analytical layer (data warehouse, data lake, ETL, ELT, etc.)\xa0Enabling business analysts by understanding their processes and attentively listening to their needs to deliver impactful\xa0solutions\xa0Supporting data scientists and deploying machine learning models into production\xa0Note:\xa0You will not be\xa0developing\xa0machine learning models\xa0or algorithms\xa0Optimizing non-performant databases, queries, pipelines, and ML models\xa0', 'Note:\xa0You will not be\xa0developing\xa0machine learning models\xa0or algorithms\xa0', 'Designing and developing the analytical layer (data warehouse, data lake, ETL, ELT, etc.)\xa0', 'Ability to write clean, maintainable, and robust code in Python, Scala, Java, or similar coding\xa0languages\xa0', 'Willingness to\xa0travel\u202f\xa0', 'Optimizing non-performant databases, queries, pipelines, and ML models\xa0', 'Degree educated in Computer Science, Engineering, Mathematics, or equivalent\xa0experience\xa0Experience with managing stakeholders and collaborating with\xa0customers\xa0Strong written and verbal communication skills\xa0required\xa0Experience building data pipelines in production and ability to work across structured, semi-structured and unstructured\xa0data\xa0Experience with data modeling (e.g.\xa0star schema, entity-relationship)\xa0Ability to write clean, maintainable, and robust code in Python, Scala, Java, or similar coding\xa0languages\xa0Knowledge of software engineering concepts and best practices (DevOps experience is\xa0bonus!)\xa0Familiarity with the cloud, container, query languages, and database technologies (SQL & NoSQL)\xa0Experience preparing data for analytics and following a data science\xa0workflow\xa0Consulting experience\xa0preferred\xa0Willingness to\xa0travel\u202f\xa0', 'Strong written and verbal communication skills\xa0required\xa0', '\xa0', 'You will become a trusted advisor working\xa0hand-in-hand\xa0with our clients, from data owners and analytic users to C-level executives. You will engage in multi-disciplinary teams to deliver complex data / analytic cases across a variety of industries. These cases can range from:\xa0', 'Enabling business analysts by understanding their processes and attentively listening to their needs to deliver impactful\xa0solutions\xa0', 'What you will do\u202f\xa0', 'Note:\xa0You will not be\xa0developing\xa0machine learning models\xa0or algorithms', 'Experience building data pipelines in production and ability to work across structured, semi-structured and unstructured\xa0data\xa0', 'Strategic assessment of existing infrastructure and business processes\xa0', 'We are looking for people who deeply understand business problems and enjoy solving them. You are a driven self-starter who loves working with data and transforming it to pull out valuable business insights. You love building analytical tools that business users can leverage daily to do their jobs better. You are passionate about contributing to a growing team and establishing best practices.\xa0', 'Experience preparing data for analytics and following a data science\xa0workflow\xa0', 'Supporting data scientists and deploying machine learning models into production\xa0', 'Who we are looking\u202ffor\u202f\xa0', 'Familiarity with the cloud, container, query languages, and database technologies (SQL & NoSQL)\xa0']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,"The Resource Collaborative, Inc.",New York City Metropolitan Area,1 week ago,69 applicants,"['', 'Work directly with end users and understand requirements', 'Requirements:', 'Responsibilities:', 'This is a Full time Role ', 'Understand the requirements and understand the larger the initiatives that each project would be part of and build solutions with end state in mindCreate end to end solutions for data engineering, such as:Analyzing source dataIdentifying optimal connectorDevelop frameworks or reuse/leverage on existing frameworksParameterize and AutomateWork directly with end users and understand requirements', 'Our client', 'Our client is an investment firm with headquarters in NYC. They are undergoing tremendous growth and are adding to their technology team...plenty of room for learning and advancement!', 'Develop frameworks or reuse/leverage on existing frameworks', 'About the Data Engineer:', 'Understand the requirements and understand the larger the initiatives that each project would be part of and build solutions with end state in mind', 'Identifying optimal connector', 'The Data Engineer will work within BI/DW and will be responsible for Informatica Cloud Development. The person should have experience in developing End to End Data Pipelines using Informatica Cloud (Including Parameters) and have worked on using multiple types of connectors including SQL Server, Snowflake, Dynamics 365 and Azure Blob', 'Analyzing source data', '3+ years in Snowflake and have experience as SQL Developer and DB Administration', 'Create end to end solutions for data engineering, such as:', '\ufeff', 'Parameterize and Automate', ""8+ years' experience in Informatica PowerCenter including Informatica Intelligent Cloud Services.8+ years' experience working with Relational Databases and Data Warehouses with ability to write, analyze and optimize complex SQL queries3+ years in Snowflake and have experience as SQL Developer and DB Administration"", ""8+ years' experience working with Relational Databases and Data Warehouses with ability to write, analyze and optimize complex SQL queries"", ""8+ years' experience in Informatica PowerCenter including Informatica Intelligent Cloud Services.""]",Mid-Senior level,Full-time,Information Technology,Financial Services,2021-03-24 13:05:10
Data Analytics Engineer,Navisite,"Boston, MA",23 hours ago,59 applicants,"['', 'Design, implement, deploy and maintain data integration solutions using data lake architectures, ETL/ELT and data flow processes, including data quality, governance, performance best practices and unit testing.', 'Troubleshoot existing Data Analytics and BI solutions including processing, performance and failures related to ETL/ELT, data warehouse and reporting\xa0\xa0', 'Engage in detailed and advanced requirements gathering for Data Analytics and BI projectstatements of work\xa0\xa0Create and maintain technical writing and guidelines for Data Analytics and BI Solution developmentDesign relational and analytical database schemas and architectures based on best practices and customer requirements/needsAdminister production databases and processes including maintenance related tasks andactivitiesTroubleshoot existing Data Analytics and BI solutions including processing, performance and failures related to ETL/ELT, data warehouse and reporting\xa0\xa0Provide recommendations on existing server and database level BI architecture usingbest practices and knowledge of business requirementsDesign, implement, deploy and maintain data integration solutions using data lake architectures, ETL/ELT and data flow processes, including data quality, governance, performance best practices and unit testing.Architect, design, deploy and maintain enterprise level data warehouses and data marts including various data modeling techniques\xa0\xa0Write complex T-SQL code and stored procedures to be utilized by applications, ETL/ELT and reportsReview, recommend and implement changes to existing T-SQL code and stored procedures for performance improvementsDesign complex reports and dashboards to satisfy business needs using native tools as well as some third-party applications', '.', 'Education & Experience: \xa0\xa0\xa0\xa0\xa0\xa0\xa0', 'Write complex T-SQL code and stored procedures to be utilized by applications, ETL/ELT and reports', 'Minimum 5 years of experience in Microsoft SQL Server or equivalent work experience', 'Experience with project management methodologies including agile and scrum\xa0\xa0\xa0\xa0', 'Architect, design, deploy and maintain enterprise level data warehouses and data marts including various data modeling techniques\xa0\xa0', 'Microsoft SQL Server 2000 to 2019 and Azure IaaS/PaaS/SaaS technologies', 'Navisite', 'Bachelor’s degree in a technical or science related field of study.\xa0\xa0Minimum 5 years of experience in Microsoft SQL Server or equivalent work experience', 'best practices and knowledge of business requirements', 'Create and maintain technical writing and guidelines for Data Analytics and BI Solution development', 'Experience with third party BI Tools and such as QlikView, Tableau, Cognos, etc. is a plus\xa0\xa0', '\xa0', 'Bachelor’s degree in a technical or science related field of study.\xa0\xa0', 'Development of Multi-dimensional and/or tabular data warehouses', 'Hands on exposure in Microsoft Azure Data Lakes, Data Factory, Synapse, Analysis Services and Power BI\xa0', 'Familiarity with metadata management concepts, change management/version control concepts', 'Responsibilities:\xa0', 'Navisite, is a leading worldwide provider of enterprise-class, cloud services, managed applications and managed hosting solutions. Navisite provides a full suite of reliable and scalable IT-as-a-service solutions for organizations looking to outsource IT infrastructure and lower their capital and operational costs. Enterprises depend on Navisite to bring a customized, fully managed set of IT solutions that address both their end-user and core IT infrastructure needs. Navisite delivers these solutions using a hybrid model across public and private cloud. For more information, please visit\xa0www.navisite.com.', '\xa0\xa0Technical Skills: ', 'Some experience in database administration', 'Design relational and analytical database schemas and architectures based on best practices and customer requirements/needs', 'Exceptional analytical, conceptual and problem-solving abilities\xa0', 'activities', 'Provide recommendations on existing server and database level BI architecture using', 'Administer production databases and processes including maintenance related tasks and', 'Technical writing and documentation experience\xa0\xa0', 'Engage in detailed and advanced requirements gathering for Data Analytics and BI project', 'Versatile and knowledgeable in T-SQL programming and SSIS design', 'Experience with other programming and machine learning languages is a plus', 'statements of work\xa0\xa0', 'Experience in AWS Data Analytics Technologies is a plus\xa0\xa0\xa0', 'Design complex reports and dashboards to satisfy business needs using native tools as well as some third-party applications', 'Microsoft SQL Server 2000 to 2019 and Azure IaaS/PaaS/SaaS technologiesExperience with designing, developing, and supporting database environmentsStrong customer relationship managementTechnical writing and documentation experience\xa0\xa0Experience with project management methodologies including agile and scrum\xa0\xa0\xa0\xa0Expertise in Microsoft SQL Server Integration, Analysis and Reporting ServicesHands on exposure in Microsoft Azure Data Lakes, Data Factory, Synapse, Analysis Services and Power BI\xa0Exceptional analytical, conceptual and problem-solving abilities\xa0Development of Multi-dimensional and/or tabular data warehousesFamiliarity with metadata management concepts, change management/version control conceptsVersatile and knowledgeable in T-SQL programming and SSIS designSome experience in database administrationExperience with third party BI Tools and such as QlikView, Tableau, Cognos, etc. is a plus\xa0\xa0Experience in AWS Data Analytics Technologies is a plus\xa0\xa0\xa0Experience with other programming and machine learning languages is a plus', 'Strong customer relationship management', 'Experience with designing, developing, and supporting database environments', 'Review, recommend and implement changes to existing T-SQL code and stored procedures for performance improvements', 'Expertise in Microsoft SQL Server Integration, Analysis and Reporting Services']",Not Applicable,Full-time,Information Technology and Services,N/A,2021-03-24 13:05:10
Data Engineer,Spring EQ LLC,"Philadelphia, PA",4 weeks ago,Be among the first 25 applicants,"['Build upon a strong company culture and foster an environment of togetherness, support, and accountability.', 'The ideal candidate will enthusiastically contribution to a culture of innovation and continuous improvement', 'Design & Implementation -', 'Spring EQ is an Equal Opportunity Employer.', 'Experience with Amazon Web Services (AWS)', 'Communication', 'Strong Python or related-language skills', 'Data Quality and Optimization ', 'Basic understanding of columnar databases and data architecture', 'Communication - This position will gather and interpret requirements from business stakeholders to build new applications or add functionality to existing applications, and work with additional members of the Technology team to architect and build solutions', 'Work hard and have fun to get the job done', 'Data Quality and Optimization - This role will identify and respond to data quality and performance issues as well as drive solutions to improve data issues over time', 'Strive to make every customer interaction a great one', 'Integrations', ""Strive to make every customer interaction a great oneRecognize behind every loan is a person or family trusting us to handle what may be a once in a lifetime transactionWork hard and have fun to get the job doneInteract on a first name basis and recognize each team member's unique valueEncourage ways for our team members to learn, develop, diversify and grow with Spring EQPromote our team members so they can share their knowledge with othersInnovate, innovate, innovateCreate and embrace the latest technologySimplify constantly, challenging every process we use to better accomplish our goalsBuild upon a strong company culture and foster an environment of togetherness, support, and accountability."", 'Ability to multitask, problem solve and efficiently manage time', 'Experience with OLAP and the building and maintenance of data marts utilizing star or snowflake schemas', '\xa0', 'Spring EQ values personal excellence, integrity and accountability - we need candidates who demonstrate these qualities in their everyday lives', 'Recognize behind every loan is a person or family trusting us to handle what may be a once in a lifetime transaction', 'The ideal candidate will have a self-starter mentality with the ability to solve open ended business challenges', 'Innovate, innovate, innovate', 'Ability to multitask, problem solve and efficiently manage timeThe ideal candidate will have a self-starter mentality with the ability to solve open ended business challengesThe ideal candidate will enthusiastically contribution to a culture of innovation and continuous improvementTime is money - At Spring EQ we recognize the importance of working with a sense of urgency. Ideal candidates will possess the ability to thrive in a fast-paced, team-oriented environmentSpring EQ values personal excellence, integrity and accountability - we need candidates who demonstrate these qualities in their everyday lives', 'At Spring EQ, we;', 'Simplify constantly, challenging every process we use to better accomplish our goals', 'Strong understanding on SQL and data transformation principlesStrong Python or related-language skillsUnderstanding of real-time and batch data processing and how to build both types of systemsExperience with OLAP and the building and maintenance of data marts utilizing star or snowflake schemasExperience with the following databases: PostgreSQL, DynamoDB, Microsoft SQL ServerBasic understanding of columnar databases and data architectureExperience with Amazon Web Services (AWS)Familiarity with streaming and queuing technologies such as Kafka, Kinesis and SQS', 'Spring EQ was founded to help homeowners unlock the value of their home by providing visibility to it’s worth and provide quick access to that equity. Our foundation expanded to include direct mortgage lending in 2019 as interest rates reached historic low levels which set off an unprecedented round of refinancing and home buying opportunities for our borrowers. By surrounding ourselves with some of the brightest, hard-working minds in the industry, we constantly improve the loan experience for our customers.', 'Create and embrace the latest technology', 'Time is money - At Spring EQ we recognize the importance of working with a sense of urgency. Ideal candidates will possess the ability to thrive in a fast-paced, team-oriented environment', 'Strong understanding on SQL and data transformation principles', 'Understanding of real-time and batch data processing and how to build both types of systems', 'Promote our team members so they can share their knowledge with others', 'Spring EQ is hiring a Data Engineer to join our growing IT team.\xa0', 'Familiarity with streaming and queuing technologies such as Kafka, Kinesis and SQS', 'Design & Implementation - This position will be responsible for design and implementation of low-latency, high-availability, performant, and secure data processing applications which will power our backend services and front-end reporting.Integrations - This position will be responsible for the Integration with databases such as Postgres, SQL Server and DynamoDB and leverage APIs to pull and post data to other systems.Data Quality and Optimization - This role will identify and respond to data quality and performance issues as well as drive solutions to improve data issues over timeCommunication - This position will gather and interpret requirements from business stakeholders to build new applications or add functionality to existing applications, and work with additional members of the Technology team to architect and build solutions', 'Design & Implementation - This position will be responsible for design and implementation of low-latency, high-availability, performant, and secure data processing applications which will power our backend services and front-end reporting.', 'Experience with the following databases: PostgreSQL, DynamoDB, Microsoft SQL Server', 'Who Are We?', ""Interact on a first name basis and recognize each team member's unique value"", 'Integrations - This position will be responsible for the Integration with databases such as Postgres, SQL Server and DynamoDB and leverage APIs to pull and post data to other systems.', 'Encourage ways for our team members to learn, develop, diversify and grow with Spring EQ']",Entry level,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Data Engineer,GTN Technical Staffing and Consulting,"Dallas, TX",,N/A,"['', 'Delivering reporting or cloud-based data support', 'TECHNICAL REQUIREMENTS', 'Excellent communication skills and ability to engage with senior leaders and coworkers to design compelling dashboards that lead to actionable insights.', 'Salary:', 'Location: 100% REMOTE until Q3 of 2021. Local candidates preferred in Dallas!', 'JIRA', 'Position Type:', 'ABOUT THE POSITION', '5+ years in the following skill sets…', 'DETAILS', 'GTN Technical Staffing is seeking a Sr. Data Engineer for a Direct-Hire opportunity in Dallas, TX. We are looking for a Sr. Data Engineer with a strong SQL, SSIS, and PowerBI background to contribute to a Data Analytics team. The Sr. Data Engineer will be responsible for SSIS and PowerBI development with over 40 TBs of data on SQL Server 2014. Our customer will be migrating to the Cloud (Snowflake) in 2021, and any previous experience with Cloud migrations or working with Snowflake for Data Analytics is highly desired.\xa0', 'PowerBI report building tools including online environment.', 'SQL/T-SQL', 'PowerBI', 'PREFERRED', '\xa0', 'Previous production experience', 'Workflow management and project tracking', 'Residency Requirements: Candidates authorized to work in the US are encouraged to apply. No sponsorship is being offered at this time.', 'Candidates must be able to create their own complex queries to query SQL Server 2014 databases.', 'Data scrubbing, data cleansing, data validation', 'Prior experience in Python\xa0', 'Candidates must be able to create their own complex queries to query SQL Server 2014 databases.Data scrubbing, data cleansing, data validationExperience working with a star-schema data warehouse', 'Experience working with a star-schema data warehouse', 'Location:', 'Position Type: Direct-Hire', 'SnowFlake Cloud PlatformPrevious production experienceMigration from On-Prem to SnowFlake HIGHLY DESIREDPrior experience in Python\xa0', 'Residency Requirements:', 'Sr. Data Engineer – SQL / SSIS / PowerBI / Snowflake ', 'PowerBI report building tools including online environment.Delivering reporting or cloud-based data support', 'Salary: $100-120K', 'Migration from On-Prem to SnowFlake HIGHLY DESIRED', 'SnowFlake Cloud Platform', 'The role will be a split between traditional Data development, BI, and data project planning, and you will serve as a SME for Development and IT Departments. The role will require candidates to use SQL to query, scrub and validate data, and pull together key data points as requested by the business. Very strong SQL skills and a “data mindset” are required to be successful in this role.']",Mid-Senior level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Associate Data Engineer,Amgen,"Tampa, FL",1 day ago,Be among the first 25 applicants,"['', 'What You Will Do', 'BS or MS in Computer Science or Engineering related fields3+ years of experience in the data warehouse space5+ years of programming experience preferably in Python, Scala, or Java.5+ Experience architecting and building ETL pipelines; Hands-on experience with SQLExperience with data modeling.Experience working with Apache Spark, Apache AirflowExperience with AutomationHands-on development experience with DatabricksExperience with Software engineering best-practices, including but not limited to version control, CI/CD, automated testingExperience with AWS services EC2, S3, EMR, RDS, Redshift/Spectrum, Lambda, Glue, Athena', 'BS or MS in Computer Science or Engineering related fields', 'What We Expect Of You', 'Thrive', '5+ Experience architecting and building ETL pipelines; Hands-on experience with SQL', 'Automate and Optimize data pipeline and framework for the easier and cost-effective development process.', 'HOW MIGHT YOU DEFY IMAGINATION?', 'Serve as an Owner for all the data pipelineAutomate and Optimize data pipeline and framework for the easier and cost-effective development process.Lead Automation and Self-healing capabilities in Data pipelinesProactively work on challenging data integration problems by implementing optimal ETL patterns, frameworks for structured and unstructured data.Collaborate with lead architect, Business SME’s, and Data Scientists to architect data solutionsServe as lead engineer for the technical implementation of projects including planning, architecture, design, development, and testing in an agile setting', 'Preferred Qualifications', 'Experience working with Apache Spark, Apache Airflow', 'Proactively work on challenging data integration problems by implementing optimal ETL patterns, frameworks for structured and unstructured data.', 'Hands-on development experience with Databricks', 'Serve as lead engineer for the technical implementation of projects including planning, architecture, design, development, and testing in an agile setting', 'Generous Total Rewards Plan comprising health, finance and wealth, work/life balance, and career benefits', 'Experience with Software engineering best-practices, including but not limited to version control, CI/CD, automated testing', 'Experience with AWS services EC2, S3, EMR, RDS, Redshift/Spectrum, Lambda, Glue, Athena', 'Principal Data Engineer', 'What You Can Expect Of Us', '3+ years of experience in the data warehouse space', 'Responsibilities', '5+ years of programming experience preferably in Python, Scala, or Java.', 'Experience with Automation', 'A diverse and inclusive community of belonging, where teammates are empowered to bring ideas to the table and act', 'Experience with data modeling.', 'Serve as an Owner for all the data pipeline', 'Clear and disciplined strategic vision for the future that leverages superior-quality products, operational excellence, and top-shelf-talent', 'Basic Qualifications', 'Collaborate with lead architect, Business SME’s, and Data Scientists to architect data solutions', 'Clear and disciplined strategic vision for the future that leverages superior-quality products, operational excellence, and top-shelf-talentA diverse and inclusive community of belonging, where teammates are empowered to bring ideas to the table and actGenerous Total Rewards Plan comprising health, finance and wealth, work/life balance, and career benefits', 'Win', 'Lead Automation and Self-healing capabilities in Data pipelines', 'Live']",Not Applicable,Full-time,Information Technology,Biotechnology,2021-03-24 13:05:10
Data Engineer,Publicis Health Media,"Philadelphia, PA",3 weeks ago,143 applicants,"['', '·\xa0\xa0\xa0\xa0\xa0\xa0Recommend solutions to existing or anticipated issues', 'Knowledge of data architecture and modeling (schemas, ERDs)Knowledge of producing and consuming Restful APIs. Ability to define API contracts.Strong Client management skills', 'Data Visualization through Tableau and PowerBI', '·\xa0\xa0\xa0\xa0\xa0\xa0Creating novel new product and analytical offerings in coordination with BI and Publicis Health leadership teams', 'Data Engineering:\xa0', 'Publicis Health Media (PHM) is the market-leading media agency in the health category, providing unmatched health and pharmaceutical expertise, innovation, and buying clout for some of the world’s biggest healthcare brands. The work we do at PHM is shaped by our genuine passion for health and wellness: the imperative to help real patients navigate the most pivotal moments of their healthcare journeys—moments of curiosity, fear and optimism.\xa0', 'A Data Engineer\xa0will be responsible for the design, development, implementation and on-going support in building platforms to support growing demands for data-driven solutions. Must be able to work in a fast-paced environment on multiple projects simultaneously, including both enhancements as well as new project development. The candidate must be a self-starter with a sense of urgency and a commitment to quality and professionalism.', '·\xa0\xa0\xa0\xa0\xa0\xa0BI Senior Leadership to utilize the aforementioned insights/recommendations as the foundation for presentations, reports, assorted client deliverables in either a face-to-face setting or via WebEx/teleconference', 'PHM is part of Publicis Health, the world’s premier health-oriented agency network. A division of Publicis Groupe, Publicis Health manages top-tier agencies specializing in promoting innovative solutions in advertising, digital, branding, message delivery, market access, and medical communications.', '·\xa0\xa0\xa0\xa0\xa0\xa0Develop documentation and transfer of knowledge', 'Bonus skills:\xa0', 'Knowledge of data warehousing on Snowflake', 'Strong Client management skills', '·\xa0\xa0\xa0\xa0\xa0\xa0Follow implementation standards', '·\xa0\xa0\xa0\xa0\xa0\xa0Maintain professional relationships with internal (PHM) and external (client team, partner agencies/consulting groups) stakeholders by communicating effectively and delivering on all data needs', 'Hands on experience of ETL/ELT on public clouds (AWS, Azure, GCP).\xa0', 'Create and execute scalable, automated, cloud-based data pipelines linking diverse data sources and performing ETL. Examples of data sources are flat files from SFTP, relational data warehouses (MS SQL Server, AWS Redshift, Snowflake) and data lakes (flat files and Parquet).\xa0', '·\xa0\xa0\xa0\xa0\xa0\xa0As we execute on the behalf of the client, manage and ensure the accuracy of the many internal data sources and (where applicable) third-party vendors/data streams that comprise our custom measurement platform while keeping workstreams on task within the established timelines and delivery dates', '·\xa0\xa0\xa0\xa0\xa0\xa0The Business Intelligence (BI) team’s ability to synthesize data available into insights/recommendations about the success of our programs in transforming the behavior of our target audience (most likely healthcare professionals, patients/prospects or a combination of the two) while identifying opportunities for program optimization and enhancement', 'Internally, we align around our mission to connect people with life-saving health and wellness solutions every day and we celebrate our connection to one another and the communities around us through our #PHMLove movement.\xa0', 'Advanced knowledge of Data wrangling on Big Data through Python, Pandas. Working knowledge of Apache Spark (Databricks) a plus.\xa0', '·\xa0\xa0\xa0\xa0\xa0\xa0Design, Build, Test, Debug, Monitor, and Troubleshoot ETL and software processes', '·\xa0\xa0\xa0\xa0\xa0\xa0Extract, transform, and load data from multiple data sources using ETL tools', 'Create and execute scalable, automated, cloud-based data pipelines linking diverse data sources and performing ETL. Examples of data sources are flat files from SFTP, relational data warehouses (MS SQL Server, AWS Redshift, Snowflake) and data lakes (flat files and Parquet).\xa0Advanced knowledge of Data wrangling on Big Data through Python, Pandas. Working knowledge of Apache Spark (Databricks) a plus.\xa0Hands on experience of ETL/ELT on public clouds (AWS, Azure, GCP).\xa0Azure SQL advanced querying, creating stored procedures, Indexes etc.\xa0', 'Azure SQL advanced querying, creating stored procedures, Indexes etc.', '\xa0', '·\xa0\xa0\xa0\xa0\xa0\xa0Analyze business requirements and partner with Business Intelligence (BI) team to provide a strategic solution', '·\xa0\xa0\xa0\xa0\xa0\xa0Serve as an escalation point for any issues that may arise', 'Your day to day will include:', '\ufeff', 'Knowledge of data architecture and modeling (schemas, ERDs)', 'Knowledge of media and advertising industry', 'Qualifications', 'Minimum Bachelor’s Degree in Computer Science, Information Technology, or its equivalent', 'As health media specialists, we channel this passion into a dogged pursuit of marketing solutions that help our clients and brands navigate the complexity of the modern healthcare landscape. From deep category expertise to the latest innovations in the industry, our team’s best-in-class health media aptitude allows us to deliver on that promise for our clients.\xa0', '·\xa0\xa0\xa0\xa0\xa0\xa0Collaborate with Senior Data Architects / Engineers to assess data management and architecture demands, identify functional requirements to ensure systems are designed, developed, and implemented to solve for business challenges and opportunities in the most efficient and cost-effective way.', 'Company Description', 'This roll will support:', '·\xa0\xa0\xa0\xa0\xa0\xa0Advanced database administration and development including stored procedures, user defined functions, triggers and ETL packages as well as security and roles', 'Knowledge of producing and consuming Restful APIs. Ability to define API contracts.', 'Job Description', '·\xa0\xa0\xa0\xa0\xa0\xa0Optimization and tuning of existing SQL stored procedures to improve performance', 'PHM has its finger on the pulse of the industry—we’re a creator culture with the deep insights to uncover what’s important now and next in media. Our annual HealthFront, the only industry upfront dedicated to health, profiles industry trends and disruptors and helps drive our client’s business results in the dynamic health & wellness marketplace.\xa0', '·\xa0\xa0\xa0\xa0\xa0\xa0Troubleshoot database issues, identify root causes, and implement optimal solutions', 'Data Visualization through Tableau and PowerBIKnowledge of data warehousing on SnowflakeKnowledge of media and advertising industry', 'Additional skills:']",Associate,Full-time,Advertising,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer (remote),Spider Strategies,"Arlington, VA",2 weeks ago,163 applicants,"['Experience with ETL and business intelligence/reporting tools is helpful but not required.', 'The preferred candidate has an understanding of Military force structure and Army-related regulations/publications.', 'Experience with programming languages like JavaScript or Python is helpful but not required.', 'Looking for an exciting, challenging opportunity to take your Data Engineer career to the next step? Seeking a fast-paced, government contracting environment where skill and motivation are rewarded, teamwork and a can-do attitude are expected, and a work-life balance is valued? Then we have the perfect position for you!', 'Data Engineer (remote)', 'This position involves working with data source owners to obtain data from systems of record. You will clean and transform data, writing and maintaining scripts in the process. You will build, maintain, and link datasets in the customer BI platform. You will also assist with building dashboards and reports, allowing for the most informed decisions to be made by senior leaders.', 'Excellent written and oral communication skills are a must in order to prepare concise recommendations, reports, and briefings to senior-level agency officials.', 'You will support our customer as a Data Engineer and technical consultant on a strategic dashboard and business intelligence platform, focusing on manipulation and presentation of data as per client requirements.', 'Capability to plan, organize, and complete studies and analyses related to assigned actions.', 'Details', 'This is a full-time remote work position.', 'This position requires a thorough understanding of databases, SQL, primary keys, relational integrity, bash/unix scripting, SSH, (S)FTP, and how to manipulate raw data sets (Excel, CSV, XML, JSON, etc.).', ' Must have a DOD SECRET (or interim) clearance and prior experience working with the US Army.**Job DescriptionYou will support our customer as a Data Engineer and technical consultant on a strategic dashboard and business intelligence platform, focusing on manipulation and presentation of data as per client requirements.This position involves working with data source owners to obtain data from systems of record. You will clean and transform data, writing and maintaining scripts in the process. You will build, maintain, and link datasets in the customer BI platform. You will also assist with building dashboards and reports, allowing for the most informed decisions to be made by senior leaders.This is a full-time remote work position.DetailsThis position requires a thorough understanding of databases, SQL, primary keys, relational integrity, bash/unix scripting, SSH, (S)FTP, and how to manipulate raw data sets (Excel, CSV, XML, JSON, etc.).The preferred candidate has an understanding of Military force structure and Army-related regulations/publications.Experience with ETL and business intelligence/reporting tools is helpful but not required.Experience with programming languages like JavaScript or Python is helpful but not required.Excellent written and oral communication skills are a must in order to prepare concise recommendations, reports, and briefings to senior-level agency officials.Capability to plan, organize, and complete studies and analyses related to assigned actions.Spider Strategies is a leading provider of performance management software and consulting and supports some of the world\'s largest companies and U.S. Federal Agencies. In addition to rewarding work, working at Spider Strategies is rewarding. Spider Strategies was recently named one of the Top 10 ""Best Places to Work"" in the Washington, DC area by the Washington Business Journal -- this is Spider\'s 4th time to receive this recognition.', 'Job Description', 'Spider Strategies has an opening for a talented, experienced Data Engineer to join our government contracting team.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Maania Consultancy Services,"Reston, VA",6 days ago,Be among the first 25 applicants,"['', 'Experience with tactical data communications.', 'Developing and operating data processing systemsExploratory data analysis (EDA), ETL and data processing using Python and PandasDeveloping and Maintaining PostgreSQL databasesVirtualization systems (e.g., Docker).', 'Required Skills:', '.', 'Desired Skills:', 'Exploratory data analysis (EDA), ETL and data processing using Python and Pandas', 'Experience with machine learningExperience with dataExperience with tactical data communications.', 'Experience with machine learning', 'Developing and Maintaining PostgreSQL databases', 'Developing and operating data processing systems', 'Virtualization systems (e.g., Docker).', 'Experience with data']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Robert Half,"California, United States",,N/A,"['', 'FOR immediate consideration please email Valerie Nielsen on LinkedIn with a copy of your resume or send over an email****Position: Data EngineerSalary: 150k base plus bonusLocation: Ideally in a 2 hour time frame from California (PST) (fully remote is open but preference for candidates local to Southbay - onsite once a week would be ideally since this person will be a leadPlease note: this is a direct hire (not contract)Data EngineerA direct client of Robert Half Technology is looking for a well-rounded, Data Engineer to join their team on a direct hire basis in an effort to refine and grow their team. Our client is in the SaaS industry and the growth of their business is relying on the further development of their current team and massive growth during this time. orRequirementsMust Haves:2-5 years leading/mentoring/managing a small teamETL, Cloud experience (they used Amazon Redshift but any is ok)Datawarehouse, data engineeringHeavy Python, some R would be nice Marketing experience is strong preference but not a deal breakerExcellent codingSomeone who is looking to build our their management careerFOR immediate consideration please email Valerie Nielsen on LinkedIn with a copy of your resume****Robert Half Technology matches IT professionals with remote or on-site jobs on a temporary, project or full-time basis. From roles in software and applications to IT infrastructure and operations, we provide you unparalleled access to exciting career opportunities.Our experienced staffing professionals can promote you to employers and advocate on your behalf. We provide access to top jobs, competitive compensation and benefits, and free online training. For more opportunities, get the Robert Half app and receive instant notifications when our AI matches you with jobs.When you work with us, you’re working with the best. Robert Half has been recognized as one of FORTUNE’s “Most Admired Companies” every year since 1998 and was named to Forbes’ inaugural list of America’s Best Temporary Staffing Firms.Questions? Call your local office at 1.888.490.4429. All applicants applying for U.S. job openings must be authorized to work in the United States. Benefits are available to temporary professionals. Visit© 2020 Robert Half Technology. An Equal Opportunity Employer. M/F/Disability/Veterans. By clicking “Apply Now,” you’re agreeing to Robert Half’s Terms of Use (', 'FOR immediate consideration please email Valerie Nielsen on LinkedIn with a copy of your resume or send over an email****Position: Data EngineerSalary: 150k base plus bonusLocation: Ideally in a 2 hour time frame from California (PST) (fully remote is open but preference for candidates local to Southbay - onsite once a week would be ideally since this person will be a leadPlease note: this is a direct hire (not contract)Data EngineerA direct client of Robert Half Technology is looking for a well-rounded, Data Engineer to join their team on a direct hire basis in an effort to refine and grow their team. Our client is in the SaaS industry and the growth of their business is relying on the further development of their current team and massive growth during this time. orRequirementsMust Haves:2-5 years leading/mentoring/managing a small teamETL, Cloud experience (they used Amazon Redshift but any is ok)Datawarehouse, data engineeringHeavy Python, some R would be nice Marketing experience is strong preference but not a deal breakerExcellent codingSomeone who is looking to build our their management career', 'Datawarehouse, data engineering', '2-5 years leading/mentoring/managing a small team', 'ETL, Cloud experience (they used Amazon Redshift but any is ok)', 'Must Haves:', 'Salary: 150k base plus bonus', 'Requirements', 'Location: Ideally in a 2 hour time frame from California (PST)', 'Position: Data Engineer', 'Description', '2-5 years leading/mentoring/managing a small teamETL, Cloud experience (they used Amazon Redshift but any is ok)Datawarehouse, data engineeringHeavy Python, some R would be nice Marketing experience is strong preference but not a deal breakerExcellent codingSomeone who is looking to build our their management career', 'Someone who is looking to build our their management career', 'Marketing experience is strong preference but not a deal breaker', 'Excellent coding', 'Data Engineer', 'FOR immediate consideration please email Valerie Nielsen on LinkedIn with a copy of your resume****Robert Half Technology matches IT professionals with remote or on-site jobs on a temporary, project or full-time basis. From roles in software and applications to IT infrastructure and operations, we provide you unparalleled access to exciting career opportunities.Our experienced staffing professionals can promote you to employers and advocate on your behalf. We provide access to top jobs, competitive compensation and benefits, and free online training. For more opportunities, get the Robert Half app and receive instant notifications when our AI matches you with jobs.When you work with us, you’re working with the best. Robert Half has been recognized as one of FORTUNE’s “Most Admired Companies” every year since 1998 and was named to Forbes’ inaugural list of America’s Best Temporary Staffing Firms.Questions? Call your local office at 1.888.490.4429. All applicants applying for U.S. job openings must be authorized to work in the United States. Benefits are available to temporary professionals. Visit© 2020 Robert Half Technology. An Equal Opportunity Employer. M/F/Disability/Veterans. By clicking “Apply Now,” you’re agreeing to Robert Half’s Terms of Use (', 'Heavy Python, some R would be nice ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,TA Digital ,"Baltimore, MD",6 days ago,77 applicants,"['', 'Experience interpreting and communicating analytic results to analytical and non-analytical business partners\u202f ', 'Research and evaluate innovative analytical methodologies, approaches, and solutions ', 'Interprets and communicates analytic results to analytical and non-analytical business partners ', 'Experience building large-scale predictive models', 'Experience in Big Data environment specifically PySpark and/or Databricks', ""Master's Degree in a quantitative discipline, such as Computer Science, Mathematics or Statistics and/or related quantitative field desired"", 'Collaborate with analytic and data teams to set objectives, approaches, and work plans ', 'Location: ', 'you are ready for a bigger voice, larger platform or place for a stronger impact', 'Requirements', 'Demonstrated experience with SAS, SQL, MATLAB, DataRobot, Python or similar statistical and scripting tools ', 'Naturally curious and comfortable with ambiguity and demonstrate critical thinking ', 'Compensation:', 'Clear and concise oral and written communication skills, with a proven ability to translate complex methodologies and analytical results to non-technical business insights ', 'Understands department, segment, and organizational strategy and operating objectives, including their linkages to related areas', 'Description', 'Strong interpersonal and consulting skills ', 'About Us', 'Experience developing and validating statistical forecasting models or predictive models\u202f ', 'Responsibilities', ' there are too many layers between you and decision makers that your current organization will not make the right decision even after you identify problems and recommend viable solutions you are ready for a bigger voice, larger platform or place for a stronger impact ', 'EOE & OFCCP Compliant regardless of:', ' Develop and validate statistical forecasting models or predictive models and tools  Research and evaluate innovative analytical methodologies, approaches, and solutions  Collaborate with analytic and data teams to set objectives, approaches, and work plans  Interprets and communicates analytic results to analytical and non-analytical business partners  Understands department, segment, and organizational strategy and operating objectives, including their linkages to related areas Make decisions regarding own work methods, occasionally in ambiguous situations, and requires minimal direction and receives guidance where needed ', 'there are too many layers between you and decision makers', '2+ years of experience in advanced data analytics ', "" Master's Degree in a quantitative discipline, such as Computer Science, Mathematics or Statistics and/or related quantitative field desired 2+ years of experience in advanced data analytics  Demonstrated experience with SAS, SQL, MATLAB, DataRobot, Python or similar statistical and scripting tools  Experience in Big Data environment specifically PySpark and/or Databricks Experience developing and validating statistical forecasting models or predictive models\u202f  Experience interpreting and communicating analytic results to analytical and non-analytical business partners\u202f  Naturally curious and comfortable with ambiguity and demonstrate critical thinking  Clear and concise oral and written communication skills, with a proven ability to translate complex methodologies and analytical results to non-technical business insights  Strong interpersonal and consulting skills  Experience building large-scale predictive models "", 'If You Feel', 'Benefits', 'Make decisions regarding own work methods, occasionally in ambiguous situations, and requires minimal direction and receives guidance where needed', 'Data Engineer', 'that your current organization will not make the right decision even after you identify problems and recommend viable solutions', 'Develop and validate statistical forecasting models or predictive models and tools ']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer,neteffects,"St Louis, MO",6 days ago,141 applicants,"['', '\ufeffAt Neteffects, we are looking for a Data Engineer for our Direct client. The position is remote for now.', 'Required Skills & Experience:', '• Ability to construct a storyline, perform analyses, and create presentations to effectively communicate complex concepts and recommendations', 'Python', 'No C2C', '• In addition to strong technical skills, the successful candidate must excel in time management; and have a proven track record in problem solution leveraging complex data. • Ideal candidates will have a strong developmental desire to continually learn new technical skills to remain on the cutting edge of capabilities for data analysis and visualization.', '• Understanding of Database systems and management of large data sets', 'Experience and skills with project management and communication of strategy with both technical and non-technical audiences. Additional Info: Develop Business Intelligence Visualization SQL R Metrics Create Tools Manipulation Data AcquisitionExcellent communication skills both written and oral and documentation skills.', ""• Bachelor's degree in Computer Science, Data Science, Agriculture/Life Sciences, or related field."", '• Experience working with multidisciplinary research and field teams', '\xa0The Data Engineer should be capable of developing key metrics and data visualizations that will enable prescriptive operations and optimize pipeline processes and transparency through increased data visibility.', ' Data Engineer ', '• Advanced experience with Python', 'data analysis', '\ufeff', 'Desired Skills/Experience:', 'Excellent communication skills both written and oral and documentation skills.', '• Strong data analysis skills utilizing tools such as Spotfire, Tableau, Pipeline Pilot, or SQL; API consumption/development experience.', 'Spotfire, Tableau, Pipeline Pilot, or SQL; API', 'data visualizations', 'Experience and skills with project management and communication of strategy with both technical and non-technical audiences. Additional Info: Develop Business Intelligence Visualization SQL R Metrics Create Tools Manipulation Data Acquisition', 'key metrics']",Mid-Senior level,Contract,Information Technology,Staffing and Recruiting,2021-03-24 13:05:10
Data Engineer,Dice,"Chesterfield, VA",2 days ago,Be among the first 25 applicants,[''],Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Principal Data Engineer,bp,"Houston, TX",2 days ago,Be among the first 25 applicants,"['', ' Strong stakeholder management and ability to lead large organizations through influence ', ' Experience in retail and / or supply chain data ', ' Actively contributes to improve developer velocity. ', ' Experience in AWS and / or Azure native data platforms ', 'Essential Experience And Job Requirements', ' deploying our integrated capability and standards in service of our net zero and \u200esafety ambitions', 'Time Type', 'Country', 'INNOVATION & ENGINEERING', ' Advocates for and ensures their team adheres to software engineering best practices (e.g. technical design, technical design review, unit testing, monitoring & alerting, checking in code, code review, documentation), ', ' Creates positive engagement and drives an inclusive work environment with team and stakeholders through the quality of interactions and collaboration across multiple business entities. ', ' driving our digital transformation and pioneering new business models', 'Job Advert', ' Responsible for deploying secure and well-tested software that meets privacy and compliance requirements. ', ' Architects and designs reliable and scalable data infrastructure. ', ' Experience in retail and / or supply chain data  Experience in AWS and / or Azure native data platforms ', 'Experience Level', 'Legal disclaimer', ' protecting us by assuring management of our greatest physical and digital risks', 'Travel Required', ' Development experience in one or more object-oriented programming languages (e.g. Python, Go, Java, C++) ', 'Because Together We Are', ' Leads, grows and develops a team of data engineers that writes, deploys and maintains software to build, integrate, manage, maintain, and quality-assure data at bp. ', ' BS degree in computer science or related field ', ' Experience designing and implementing large-scale distributed systems ', ' Deep knowledge and hands-on experience in technologies across all data lifecycle stages ', ' Effectively works with cross-disciplinary collaborators and stakeholders across multiple business entities. ', 'Relocation available', 'About BP', ' collaborating to deliver competitive customer-focused energy solutions', 'Job Profile Summary', ' Empathetic, curious, creative and inclusive', ' No prior experience in the energy industry required ', 'Entity', 'Job Family Group', ' Experience (typically 6+ years) leading, growing and developing a data engineering team of around 30-150 people ', ' Responsible for service reliability and following site-reliability engineering best practices: on-call rotations for services they oversee, responsible for defining and maintaining SLAs. ', 'Working With Us, You Can Do This By', ' Engineers, technologists, scientists and entrepreneurs\u200e', ' Experience (typically 6+ years) leading, growing and developing a data engineering team of around 30-150 people  Deep and hands-on experience (typically 12+ years) designing, planning, productionizing, maintaining and documenting reliable and scalable data infrastructure and data products in complex environments  Development experience in one or more object-oriented programming languages (e.g. Python, Go, Java, C++)  Experience designing and implementing large-scale distributed systems  Deep knowledge and hands-on experience in technologies across all data lifecycle stages  Strong stakeholder management and ability to lead large organizations through influence  Continuous learning and improvement mindset  BS degree in computer science or related field  No prior experience in the energy industry required ', ' Originators, builders, guardians and disruptors', ' Deep and hands-on experience (typically 12+ years) designing, planning, productionizing, maintaining and documenting reliable and scalable data infrastructure and data products in complex environments ', ' originating, scaling and commercialising innovative ideas, and creating ground-breaking new \u200ebusinesses from them', ' Continuous learning and improvement mindset ', ' Leads, grows and develops a team of data engineers that writes, deploys and maintains software to build, integrate, manage, maintain, and quality-assure data at bp.  Creates positive engagement and drives an inclusive work environment with team and stakeholders through the quality of interactions and collaboration across multiple business entities.  Effectively works with cross-disciplinary collaborators and stakeholders across multiple business entities.  Architects and designs reliable and scalable data infrastructure.  Advocates for and ensures their team adheres to software engineering best practices (e.g. technical design, technical design review, unit testing, monitoring & alerting, checking in code, code review, documentation),  Responsible for deploying secure and well-tested software that meets privacy and compliance requirements.  Responsible for service reliability and following site-reliability engineering best practices: on-call rotations for services they oversee, responsible for defining and maintaining SLAs.  Actively contributes to improve developer velocity. ']",Mid-Senior level,Full-time,Information Technology,Oil & Energy,2021-03-24 13:05:10
Data Engineer,Mylio,"Bellevue, WA",2 weeks ago,96 applicants,"['', 'Ideal candidates will have the following:', '3+ years PostgreSQL experience', '5+ years as a Data Engineer or a similar role working with data and databases.\xa03+ years PostgreSQL experienceStrong expertise in creating complex SQL queries AND design (schemas, data types, window functions, extensions, user-defined functions, conversions, etc.),Experience in data warehousing, modeling, and tuningStrong ability to work directly on developing scripts that connect that data warehouse data via APIs to Google Analytics, Salesforce, and other reporting tools such as Microsoft BIStrong expertise with ETLStrong working knowledge of\xa0Salesforce.com is preferred', '5+ years as a Data Engineer or a similar role working with data and databases.\xa0', 'Experience in data warehousing, modeling, and tuning', 'Strong expertise with ETL', 'If you are looking to be part of something where you can make a difference, help build an exciting product, and work with smart people…Mylio might be a great place for you. You’ll have responsibility, impact, and upside that you could normally only get in a high-risk, early-stage startup. We offer a collaborative, casual environment with strong benefits and a great work-life balance.', 'Strong ability to work directly on developing scripts that connect that data warehouse data via APIs to Google Analytics, Salesforce, and other reporting tools such as Microsoft BI', 'Strong working knowledge of\xa0Salesforce.com is preferred', 'Only local Greater Seattle area candidates will be considered. No relocation is available. Please do not apply if you are out of the area.', 'Mylio is changing the way the world remembers!\xa0We are seeking a Data Engineer to help us grow and to help people take back control of their photos.\xa0\xa0Mylio is a category-defining product that allows you to organize the memories of a lifetime. Manage all your photos and documents on your phone, your tablet, your computer, wherever you are even with limited or no connectivity. We work across multiple platforms and devices and solve a major problem that many people still face….being overwhelmed and unorganized with their photos.\xa0', 'NOTE: At this time, remote work from the Greater Seattle Area is acceptable. No out-of-state candidates will be considered. The ability to come into the office as needed and eventually primarily work from Bellevue is required', 'Strong expertise in creating complex SQL queries AND design (schemas, data types, window functions, extensions, user-defined functions, conversions, etc.),', '\xa0']",Associate,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Data Engineer-Data Platform,TikTok,"Mountain View, CA",3 weeks ago,143 applicants,"['', 'Design and build data transformations efficiently and reliably for different purposes (e.g. reporting, growth analysis, multi-dimensional analysis)', 'Experience with performing data analysis, data ingestion and data integration', 'Passionate and self-motivated about technologies in the Big Data area', 'Experience in the Big Data technologies(Hadoop, M/R, Hive, Spark, Metastore, Presto, Flume, Kafka, ClickHouse, Flink etc.)', ""As a data engineer in the data platform team, you will have the opportunity to build, optimize and grow one of the largest data platforms in the world. You'll have the opportunity to gain hands-on experience on all kinds of systems in the data platform ecosystem. Your work will have a direct and huge impact on the company's core products as well as hundreds of millions of users."", 'TikTok is committed to providing reasonable accommodations during our recruitment process. If you need assistance or an accommodation, please reach out to us at usrc@tiktok.com', '\xa0', 'Design and build data transformations efficiently and reliably for different purposes (e.g. reporting, growth analysis, multi-dimensional analysis)Design and implement reliable, scalable, robust and extensible big data systems that support core products and businessEstablish solid design and best engineering practice for engineers as well as non-technical people', 'BS or MS degree in Computer Science or related technical field or equivalent practical experience', 'Responsibilities', 'Qualifications', 'Experience with schema design, data modeling and SQL queries', 'Design and implement reliable, scalable, robust and extensible big data systems that support core products and business', 'Experience with ETL(Extraction, Transformation & Loading) and architecting data systems', 'Establish solid design and best engineering practice for engineers as well as non-technical people', 'BS or MS degree in Computer Science or related technical field or equivalent practical experienceExperience in the Big Data technologies(Hadoop, M/R, Hive, Spark, Metastore, Presto, Flume, Kafka, ClickHouse, Flink etc.)Experience with performing data analysis, data ingestion and data integrationExperience with ETL(Extraction, Transformation & Loading) and architecting data systemsExperience with schema design, data modeling and SQL queriesPassionate and self-motivated about technologies in the Big Data area', ""TikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too."", 'TikTok is the leading destination for short-form mobile video. Our mission is to inspire creativity and bring joy. TikTok has global offices including Los Angeles, New York, London, Paris, Berlin, Dubai, Mumbai, Singapore, Jakarta, Seoul and Tokyo.']",Mid-Senior level,Full-time,Engineering,Internet,2021-03-24 13:05:10
Data Engineer,Human API,"Myrtle Point, OR",3 weeks ago,Be among the first 25 applicants,"['', 'Experience with Databricks, Apache Spark, PrestoDB, or Kafka desirable', ""Own problems end-to-end - we have a very ownership driven cultureAre product focused - solving the underlying product problem is more important than writing lots of codeHave a good sense of humorWant to make a positive difference in people's lives"", 'Incorporating appropriate use of ML techniques in the pipeline', 'Conventional(-ish) ETL reporting and data engineering.', 'Experience working directly on one or more of the following: database schema design, query optimization, development of ETL / analytics / reporting systems, big-data systems, large-scale text-parsing systems, stream processing, or ML pipeline engineering.Experience with large scale distributed systems desirableExperience with Databricks, Apache Spark, PrestoDB, or Kafka desirableA positive attitude, willingness to learn, and desire for self-improvement', 'Are product focused - solving the underlying product problem is more important than writing lots of code', 'Have a good sense of humor', 'Experience with large scale distributed systems desirable', 'Experience working directly on one or more of the following: database schema design, query optimization, development of ETL / analytics / reporting systems, big-data systems, large-scale text-parsing systems, stream processing, or ML pipeline engineering.', 'We Really Like People Who', 'Bespoke data pipeline logic and processes.', 'Your team will own', 'You should have', 'Performance and scaling for internal and external real-time data services.', 'Bespoke data pipeline logic and processes.Performance and scaling for internal and external real-time data services.Conventional(-ish) ETL reporting and data engineering.Incorporating appropriate use of ML techniques in the pipeline', 'By submitting your interest in this job, you agree to receive text notifications with additional steps to complete your job application. You will receive up to 6 messages from the number ""63879"". Message & data rates may apply. Please refer to our privacy policy for more information.', 'Own problems end-to-end - we have a very ownership driven culture', 'A positive attitude, willingness to learn, and desire for self-improvement', ""Want to make a positive difference in people's lives""]",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Zaxby's Franchising LLC,"Athens, GA",1 week ago,190 applicants,"['', 'Strong knowledge of relational databases and ERD developmentWorking knowledge of Snowflake, Azure Blob Storage, and Azure File StorageStrong knowledge of Power BI, and Power BI Report BuilderPreferred experience or understanding of IBM PureData Systems and IBM CognosWorking knowledge of legacy ETL tools such as SQL Server Integration Services (SSIS) and IBM DatastageWorking knowledge of cloud-based, on premise, and hybrid data and application architecturesAbility to create software solutions using the .Net Framework and Core Framework for application service developmentFundamental understanding of common structured and unstructured data storage solutionsStrong ability to articulate and explain situations, issues and solutions to members of the organization not regularly involved in data managementAbility to join and contribute effectively to large meetings with individuals from various departments within the organization and at various levelsProficient in common offerings within Office 365 product suite (Word, PowerPoint, Excel)', 'Ability to create software solutions using the .Net Framework and Core Framework for application service development', 'Seek data sources that provide high value to the overall brand data ecosystem', 'QUALIFICATIONS', 'EDUCATION AND EXPERIENCE GUIDELINES', 'Implement structured and non-structured data storage solutions', 'Essential duties may include but are not limited to the following:', 'Fundamental understanding of common structured and unstructured data storage solutions', 'Implement on premise and cloud based resources to meet any given need for data storage and transmission', 'Evaluate and implement best practices for Master Data Management', 'Validate, massage and clean data from all available sources to ensure the highest degree of accuracy and usability', 'Develop processes and procedures for acquiring data from disparate sourcesEvaluate current ETL/ELT processes for proper fit of accomplishing business needsResearch and stay abreast of industry leading technologies that improve business intelligence and create more effective data structuresEvaluate and implement best practices for Master Data ManagementImplement on premise and cloud based resources to meet any given need for data storage and transmissionDevelop software solutions that can send and receive information via protocols such as HTTPS and SFTPSupport current data storage solutions and migration processes to ensure maximum uptime and usability for the brandCreate, support, and distribute reports and visualizations using applicable tools available to the Zaxby’s brandImplement structured and non-structured data storage solutionsSeek data sources that provide high value to the overall brand data ecosystemParticipate in requirements gathering and brainstorming sessions, and be willing to suggest and provide innovative solutions to complex business problemsPartner with various groups throughout the organization to create collaborative solutions to meet business needsCollaborate with other teams within IT to create a cohesive integration of systems and dataValidate, massage and clean data from all available sources to ensure the highest degree of accuracy and usabilityUnderstand and perform to standards within project plans set forth by leaders of the organization and the Project Management Office', 'Working knowledge of Snowflake, Azure Blob Storage, and Azure File Storage', 'The Data Engineer will be responsible for implementing, vetting, and supporting new and existing solutions for ZFL’s data ecosystem.\xa0This individual will use industry-known ETL/ELT tools along with best practices to properly build designed data structures and ETL/ELT processes that provide valuable information based upon what data is available.\xa0He or she will have a thorough understanding of data storage, transfer, and reporting technologies and their capabilities.\xa0There will be a focus on providing data that can be consumed by various reporting and business intelligence tools throughout the organization to support strategic initiatives.\xa0The candidate will need to be comfortable with engaging different areas of the business to gather requirements, evaluate circumstances, and produce results under the guidance of a data architect.', 'Strong knowledge of Power BI, and Power BI Report Builder', 'Education', 'Collaborate with other teams within IT to create a cohesive integration of systems and data', 'Support current data storage solutions and migration processes to ensure maximum uptime and usability for the brand', 'Strong knowledge of relational databases and ERD development', 'Develop software solutions that can send and receive information via protocols such as HTTPS and SFTP', 'Preferred experience or understanding of IBM PureData Systems and IBM Cognos', 'Strong ability to articulate and explain situations, issues and solutions to members of the organization not regularly involved in data management', 'Experience', 'Understand and perform to standards within project plans set forth by leaders of the organization and the Project Management Office', '\xa0', 'Data Engineer (Business Intelligence)', 'Working knowledge of legacy ETL tools such as SQL Server Integration Services (SSIS) and IBM Datastage', 'ESSENTIAL JOB FUNCTIONS', 'Develop processes and procedures for acquiring data from disparate sources', 'Proficient in common offerings within Office 365 product suite (Word, PowerPoint, Excel)', 'Create, support, and distribute reports and visualizations using applicable tools available to the Zaxby’s brand', 'Partner with various groups throughout the organization to create collaborative solutions to meet business needs', 'Working knowledge of cloud-based, on premise, and hybrid data and application architectures', 'Zaxby’s Franchising LLC is an equal opportunity employer and does not discriminate in employment decisions based on any factor protected by federal, state or local law.', 'Ability to join and contribute effectively to large meetings with individuals from various departments within the organization and at various levels', 'Experience: \xa0\xa0\xa0\xa0\xa0\xa03+ years of experience in Data Engineering (including data structure and ETL/ELT development)', 'Research and stay abreast of industry leading technologies that improve business intelligence and create more effective data structures', 'Evaluate current ETL/ELT processes for proper fit of accomplishing business needs', 'Participate in requirements gathering and brainstorming sessions, and be willing to suggest and provide innovative solutions to complex business problems', 'Education:\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Bachelor’s degree in Computer Science, Information Systems, Data Science, or related field']",Entry level,Full-time,Information Technology,Restaurants,2021-03-24 13:05:10
Data Engineer/ETL Developer,Entech,"Wilmington, DE",6 days ago,80 applicants,"['', 'Validate requests, data and any dependencies for compliance with established data standards, policies and procedures', 'Format data, as needed, to meet data standards, policies and procedures', 'Identify data dependencies', 'Data modeling and mining', 'Knowledgeable in the operation of business application(s) utilized to process business data requests', 'Building and designing large-scale applicationsData modeling and miningStatistical modeling and regression analysisDistributed computing and splitting algorithms to yield predictive accuracyProven communication and leadership skills.Reviews and processes business requests to create, update or delete business dataGathers data from content owner and transfers data into a standard format for systemsUnderstands business data standards, policies and procedures and applies them appropriately to dataRecognizes business data dependenciesKnowledgeable in the treatment of data dependenciesKnowledgeable in the operation of business application(s) utilized to process business data requestsReview data requests for requirements and outcomesIdentify data dependenciesValidate requests, data and any dependencies for compliance with established data standards, policies and proceduresFormat data, as needed, to meet data standards, policies and proceduresInteract with requisite business application(s)', 'Requirements:', 'Scipy', 'Reviews and processes business requests to create, update or delete business data', 'Interact with requisite business application(s)', ""Bachelor’s degree with a focus on Computer Science or Engineering, Master's Degree preferred"", 'Gathers data from content owner and transfers data into a standard format for systems', 'Basic coding fundamentals: Hands on experience (school, personal project or internship experience) Java, Python, SQL, R or other coding languages- must be able to demonstrate code', 'Understands business data standards, policies and procedures and applies them appropriately to data', 'Creative thinking, excitement to share ideas, receive feedback and learn continuously', 'Ability to listen and learn- active listening, understanding full situations and proposing solutions', ""A passion for\xa0Data Engineering- an excitement and eagerness to learn and work with Database Architecture, Data Modeling and mining, statistical analysis.ETL Development experienceBasic coding fundamentals: Hands on experience (school, personal project or internship experience) Java, Python, SQL, R or other coding languages- must be able to demonstrate codeAbility to listen and learn- active listening, understanding full situations and proposing solutionsCreative thinking, excitement to share ideas, receive feedback and learn continuouslyAbility to operate independently- after training and mentorship able to work at a problem independently, knowing when to elevate for leadership insights, and when to keep working to find a solution3 - 5 years professional experienceBachelor’s degree with a focus on Computer Science or Engineering, Master's Degree preferred"", 'ETL Development experience', 'Statistical modeling and regression analysis', 'Entech does not offer sponsorship for this role, and is a Full Time Salaried position', 'A passion for\xa0Data Engineering- an excitement and eagerness to learn and work with Database Architecture, Data Modeling and mining, statistical analysis.', '3 - 5 years professional experience', 'Scikit-Learn', 'Proven communication and leadership skills.', 'Jupyter Notebooks', 'Recognizes business data dependencies', 'Review data requests for requirements and outcomes', 'Seaborn', 'Distributed computing and splitting algorithms to yield predictive accuracy', 'Role & Responsibilities:', 'Working Environment:', 'Ability to operate independently- after training and mentorship able to work at a problem independently, knowing when to elevate for leadership insights, and when to keep working to find a solution', 'Linux & Docker', 'Numpy', 'Keras', 'Building and designing large-scale applications', 'PandasScikit-LearnKerasLinux & DockerJupyter NotebooksNumpyScipySeaborn', ""The ideal candidate's favorite words are learning, data, scale, and agility. You will leverage your strong collaboration skills and ability to extract valuable insights from highly complex data sets to ask the right questions and find the right answers.\xa0"", 'Knowledgeable in the treatment of data dependencies', 'Pandas']",Associate,Full-time,Engineering,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Beghou Consulting,"Boston, MA",7 days ago,99 applicants,"['', ' We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.', 'Knowledge and experience configuring AWS or Azure cloud infrastructure and managed services', 'All applicants must be currently authorized to work in the United States on a full-time basis.\xa0Beghou\xa0Consulting will not sponsor applicants for work visas.', 'We treat our employees with respect and appreciation, not only for what you do but who you are.We value the many talents and abilities of our employees and promote a supportive, collaborative and dynamic work environment that encourages both professional and personal growth.You will have the opportunity to work with and learn from senior staff and partners, allowing everyone to work together to develop, achieve, and succeed with every project.We have had steady growth throughout our history because the people we hire are committed not only to delivering quality results for our clients, but also to becoming leaders in sales and marketing analytics.', 'Software development fundamentals, including participating in Agile development, use of version control systems such as Git or DevOps, code reviews, emphasis on testing, and dedication to documentation', 'Experience with relational database technologies, such as PostgreSQL, Oracle, MySQL, Redshift, Snowflake', '3+ years’ experience in data engineering or application development using Python, including use of pandas or PySparkExperience with relational database technologies, such as PostgreSQL, Oracle, MySQL, Redshift, SnowflakeKnowledge and experience configuring AWS or Azure cloud infrastructure and managed servicesSoftware development fundamentals, including participating in Agile development, use of version control systems such as Git or DevOps, code reviews, emphasis on testing, and dedication to documentation', 'Develop best practice guidance and supporting materials for data management and advanced analytics pipelines', 'Build and enhance data integration, management, and analytics tools and pipelines using pandas and Spark', 'Experience configuring AzureAD/SAML/Okta/Oauth and administering AWS or Azure security best practices', 'What you should know: ', 'We are seeking candidates in Boston, Chicago, New York, or San Francisco.', '\xa0', 'Experience configuring AzureAD/SAML/Okta/Oauth and administering AWS or Azure security best practicesWeb application development experience using Flask, Django, JavaScript, Ajax, or CSS/HTMLContainer orchestration systems experience using Docker, Kubernetes, AWS ECS)Experience with WYSIWYG ETL tools (Azure Data Factory, Informatica, SnapLogic, Boomi)Experience with building systems in event-driven or streaming architecturesLife Sciences industry experience', 'Experience with WYSIWYG ETL tools (Azure Data Factory, Informatica, SnapLogic, Boomi)', 'We treat our employees with respect and appreciation, not only for what you do but who you are.', 'You’ll need to have:', 'Web application development experience using Flask, Django, JavaScript, Ajax, or CSS/HTML', 'As a result of our tremendous growth, Beghou Consulting is seeking experienced individuals to join our team of skilled data practitioners. We seek creative, adaptable, and analytical candidates who are committed to delivering innovative solutions to life sciences companies that exceed expectations. ', 'We value the many talents and abilities of our employees and promote a supportive, collaborative and dynamic work environment that encourages both professional and personal growth.', 'All applicants must be currently authorized to work in the United States on a full-time basis.\xa0Beghou\xa0Consulting will not sponsor applicants for work visas. We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.', 'You will have the opportunity to work with and learn from senior staff and partners, allowing everyone to work together to develop, achieve, and succeed with every project.', 'We’ll trust you to:', 'Life Sciences industry experience', 'Emphasize and ensure reliability and stability of our enterprise data platform and tools used by internal teams and life sciences clientsBuild and enhance data integration, management, and analytics tools and pipelines using pandas and SparkDevelop best practice guidance and supporting materials for data management and advanced analytics pipelines', 'Container orchestration systems experience using Docker, Kubernetes, AWS ECS)', 'We have had steady growth throughout our history because the people we hire are committed not only to delivering quality results for our clients, but also to becoming leaders in sales and marketing analytics.', 'Emphasize and ensure reliability and stability of our enterprise data platform and tools used by internal teams and life sciences clients', 'As a Data Engineer, your goal is to develop and enhance an in-house enterprise data platform and tools used to ingest, transform, and analyze data to yield value for our clients. You will build data integrations, connectors, and analytics applications using tools like Python and Spark and support deployment to client cloud environments. You will partner with our internal consulting teams to develop data integration and analytics pipelines and will help deliver uncovered insights to our clients. You will collaborate with our data platform and technology team to craft and evolve the tools and proprietary systems we offer.\xa0', 'We’d love to see:', '3+ years’ experience in data engineering or application development using Python, including use of pandas or PySpark', 'Experience with building systems in event-driven or streaming architectures']",Associate,Full-time,Information Technology,Management Consulting,2021-03-24 13:05:10
Data Engineer,"Enjoy Technology, Inc.","Palo Alto, CA",1 week ago,140 applicants,"['', 'Develop, manage, operationalize & improve current data systems & processes.', 'BS or MS in Computer Science, Software Engineering, or related field', ' BS or MS in Computer Science, Software Engineering, or related field 5+ years of experience building large scale data warehouse, big data platform as well as real-time systems Strong Database knowledge, Snowflake and postgres preferred Proficient in Complex SQL development and optimization. Proficient in Python development. Proficient with SDLC methodologies Experience in Data modeling & schema design. Strong understanding of master data management & data governance Experience with Unix/Shell scripting & Cloud tools Experience with Web development technologies Experience with Data workflow tools like Airflow, Pentaho etc Experience with implementing and operationalizing data quality architecture. Experience working with unstructured data format (Json/XML/Yaml) Experience with ETL development methodologies & tools. Exceptional Excel / Data Management skills Experience in ECommerce and Retail will be a plus ', 'All offers of employment are subject to background checks and drug screens prior to start date. Enjoy will consider for employment qualified applicants with criminal histories in a manner consistent with applicable local, state, and federal laws and Fair Chance Ordinances.', 'Experience in ECommerce and Retail will be a plus', 'Have a high sense of urgency to deliver projects as well as troubleshoot and fix data issues', 'Own and manage data engineering roadmap, architecture and deliverables by working with various engineering and business teams align on priorities', 'Develop, build & maintain mission critical data pipelines that gather data from various internal & external sources', 'Strong Database knowledge, Snowflake and postgres preferred', 'Communicate updates with the leadership team, as well as business & engineering stakeholders.', 'Experience with implementing and operationalizing data quality architecture.', 'Collaborate with onshore and offshore data team members', 'This job description is not intended as a comprehensive or exhaustive list of all duties, responsibilities, and qualifications necessary for the position described herein. The duties, responsibilities, and qualifications of the position are subject to change without notice.', 'Exceptional Excel / Data Management skills', 'As an equal opportunity employer, Enjoy is proud to maintain a workplace characterized by mutual respect, inclusivity, and the celebration of diversity. All qualified applicants are welcome and will be considered for employment without regard to race, color, ethnicity, national origin, religion, gender, sex, sexual orientation, gender identity, genetics, disability, veteran status, or ', ' Own and manage data engineering roadmap, architecture and deliverables by working with various engineering and business teams align on priorities Develop, build & maintain mission critical data pipelines that gather data from various internal & external sources Create optimized and scalable data models Develop, manage, operationalize & improve current data systems & processes. Always be on the lookout to automate and improve existing data processes for quicker turnaround and high productivity Collaborate with onshore and offshore data team members Develop a deep understanding of business & business processes. Communicate updates with the leadership team, as well as business & engineering stakeholders. Have a high sense of urgency to deliver projects as well as troubleshoot and fix data issues ', 'Experience with Data workflow tools like Airflow, Pentaho etc', 'Experience with ETL development methodologies & tools.', 'Responsibilities', 'Strong understanding of master data management & data governance', 'Qualifications', 'Proficient in Complex SQL development and optimization.', 'Develop a deep understanding of business & business processes.', 'Proficient in Python development.', 'Always be on the lookout to automate and improve existing data processes for quicker turnaround and high productivity', 'Create optimized and scalable data models', 'Proficient with SDLC methodologies', 'Experience in Data modeling & schema design.', 'Experience working with unstructured data format (Json/XML/Yaml)', '5+ years of experience building large scale data warehouse, big data platform as well as real-time systems', 'Job Description', 'Experience with Web development technologies', 'other applicable protected characteristic under local, state, or federal law.', 'Experience with Unix/Shell scripting & Cloud tools']",Entry level,Full-time,Information Technology,Retail,2021-03-24 13:05:10
Data Engineer,Shipt,"Birmingham, AL",6 days ago,Be among the first 25 applicants,"['We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.', ' An understanding of technologies and design patterns in fields such as: micro services, streaming / queuing systems, SQL and key-value stores, and high-performance solutions (vectorization, task and data parallelism) ', 'At Shipt, we are transforming the grocery shopping experience and giving time back to consumers. Shipt shoppers handpick fresh groceries and household essentials, then deliver them to your door in as little as one hour.Data pipelines to move data from the enterprise service bus messaging to our data lake and ultimately data warehouse analytical storesTest the data pipeline code to ensure quality buildsCollect and monitor the metrics necessary to quantify system performance and forecast future capacity needs.Your Responsibilities Develop Data Pipeline - working within the Data Warehouse team and with other members of the Engineering organizations to build services that subscribe and collect messages from our next generation services for entity CRUD and business activity. describe, document intended use and finally surface data as actionable information.  Ideate and Collaborate on Solutions- be a thought leader within the Tech organization to build new and improved data tools and services that can scale with the company  Invest in the Process - execute and continuously improve our development process Requirements 4+ years in Data Engineering and/or Engineering  Experience working in a web-scale data environment (for example, millions to billions of messages per day)  Experience working in an environment with a bias towards action  Strong development skills especially with tools such as Tableau, Looker, ChartIO and their relations.  An understanding of technologies and design patterns in fields such as: micro services, streaming / queuing systems, SQL and key-value stores, and high-performance solutions (vectorization, task and data parallelism)  Expertise with Presto, Snowflake and Redshift (or their relations) is a major plus  Expertise with AWS tech and deployments is a major plus (we currently use Jenkins, Docker, AWS Lambda, and AWS Batch)  A Bachelor’s Degree in CS, Information Systems, a related field or equivalent work experience We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.', 'Your Responsibilities', 'Requirements', ' 4+ years in Data Engineering and/or Engineering  Experience working in a web-scale data environment (for example, millions to billions of messages per day)  Experience working in an environment with a bias towards action  Strong development skills especially with tools such as Tableau, Looker, ChartIO and their relations.  An understanding of technologies and design patterns in fields such as: micro services, streaming / queuing systems, SQL and key-value stores, and high-performance solutions (vectorization, task and data parallelism)  Expertise with Presto, Snowflake and Redshift (or their relations) is a major plus  Expertise with AWS tech and deployments is a major plus (we currently use Jenkins, Docker, AWS Lambda, and AWS Batch)  A Bachelor’s Degree in CS, Information Systems, a related field or equivalent work experience ', ' Strong development skills especially with tools such as Tableau, Looker, ChartIO and their relations. ', ' Expertise with AWS tech and deployments is a major plus (we currently use Jenkins, Docker, AWS Lambda, and AWS Batch) ', ' Develop Data Pipeline - working within the Data Warehouse team and with other members of the Engineering organizations to build services that subscribe and collect messages from our next generation services for entity CRUD and business activity. describe, document intended use and finally surface data as actionable information. ', ' Ideate and Collaborate on Solutions- be a thought leader within the Tech organization to build new and improved data tools and services that can scale with the company ', ' Expertise with Presto, Snowflake and Redshift (or their relations) is a major plus ', ' Experience working in a web-scale data environment (for example, millions to billions of messages per day) ', ' Experience working in an environment with a bias towards action ', 'Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.', ' A Bachelor’s Degree in CS, Information Systems, a related field or equivalent work experience ', ' 4+ years in Data Engineering and/or Engineering ', ' Invest in the Process - execute and continuously improve our development process ', ' Develop Data Pipeline - working within the Data Warehouse team and with other members of the Engineering organizations to build services that subscribe and collect messages from our next generation services for entity CRUD and business activity. describe, document intended use and finally surface data as actionable information.  Ideate and Collaborate on Solutions- be a thought leader within the Tech organization to build new and improved data tools and services that can scale with the company  Invest in the Process - execute and continuously improve our development process ', 'As a Member Of The Data Warehouse Team, You Will Be Developing, Maintaining And Supporting']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer,Seismic,"San Diego, CA",1 week ago,Be among the first 25 applicants,"['', ' Generous PTO, paid holidays, and paid sick leave Competitive Medical, Dental and Vision Plans Robust 401(k) fund options with company matching Catered meals, happy hours, healthy snacks, and coffee bar Seismic Cares volunteer program #OneSeismic culture that celebrates wins, encourages autonomy, ownership, and transparency ', 'DBT experience a plus', ' Bachelor of Arts/Science degree from an accredited university in the field of computer science, MIS or comparable related technical discipline 1-2 years of hands-on experience with database development, modeling and governance within a RDBMS environment required. Snowflake, Google BigQuery, or other data warehouse system experience required. Excellent command of SQL programming skills required. Demonstrable understanding of coding and scripting languages - Java, Python, JavaScript, etc Experience working with a variety of file formats such as JSON, CSV and Parquet required Experience with cloud services such as aws, azure and gcp a plus. DBT experience a plus ', 'Experience with cloud services such as aws, azure and gcp a plus.', 'Play a direct role in the ETL process to model datasets to be used in enterprise-wide BI reporting, often using new data sources', '#OneSeismic culture that celebrates wins, encourages autonomy, ownership, and transparency', 'Provide architecture designs to store, process and publish data at every step of the data pipeline from ingestion from various data sources to designing data warehouses', 'Document processes, architecture of systems/data flows, project plans, using the agile methodology', 'What You Possess', '1-2 years of hands-on experience with database development, modeling and governance within a RDBMS environment required.', 'Competitive Medical, Dental and Vision Plans', 'Seismic Cares volunteer program', 'Robust 401(k) fund options with company matching', 'Bachelor of Arts/Science degree from an accredited university in the field of computer science, MIS or comparable related technical discipline', 'What You Will Be Doing', 'Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities and activities may change at any time with or without notice.', 'Experience working with a variety of file formats such as JSON, CSV and Parquet required', 'What We Have For You', ' Provide architecture designs to store, process and publish data at every step of the data pipeline from ingestion from various data sources to designing data warehouses Play a direct role in the ETL process to model datasets to be used in enterprise-wide BI reporting, often using new data sources Evaluate stakeholder requirements for upcoming projects and assist with project scope from a technical perspective Document processes, architecture of systems/data flows, project plans, using the agile methodology Advise leaders, analysts, and developers with the intention of furthering data quality, standards, best practices and uses. ', 'Snowflake, Google BigQuery, or other data warehouse system experience required.', 'Demonstrable understanding of coding and scripting languages - Java, Python, JavaScript, etc', 'Advise leaders, analysts, and developers with the intention of furthering data quality, standards, best practices and uses.', 'Evaluate stakeholder requirements for upcoming projects and assist with project scope from a technical perspective', 'Generous PTO, paid holidays, and paid sick leave', 'Excellent command of SQL programming skills required.', 'Catered meals, happy hours, healthy snacks, and coffee bar']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer,AllianceBernstein,"Nashville, TN",2 weeks ago,43 applicants,"['', ' Learning the equity investment business and engaging directly with end users.  Automating complex data loads and pipelines.  Onboard alternative datasets including learning how to web scrape.  Best practices managing large data sets.  Building technical skills including SQL, Python, and PowerBI.  Applying cloud based technologies including data lakes and data pipelines. ', 'Skills', ' 5+ years programming in SQL with experience in relational schema designs and optimizing query performance ', ' Automating complex data loads and pipelines. ', ' ETL experience is a strong plus ', ' Experience working in the financial industry or knowledge of basic financial statement concepts ', 'Describe The Role', ' BS in Computer Science/Engineering, Finance, Mathematics/Statistics or a related major ', 'People of color, women, and those who identify as LGBTQ people are encouraged to apply. AB does not discriminate against any employee or applicant for employment on the basis of race, color, religion, creed, ancestry, national origin, sex, age, disability, marital status, citizenship status, sexual orientation, gender identity, military or veteran status or any other basis that is prohibited by applicable law. AB’s policies, as well as practices, seek to ensure that employment opportunities are available to all employees and applicants, based solely on job-related criteria. ', ' 2+ years using Python or another object oriented language (C#, Java) ', ' Automation of data ingestion supporting various sources and formats both external and internal. ', ' Self starter as well as a good team player ', ' BS in Computer Science/Engineering, Finance, Mathematics/Statistics or a related major  5+ years programming in SQL with experience in relational schema designs and optimizing query performance  2+ years using Python or another object oriented language (C#, Java)  ETL experience is a strong plus  Working with NoSQL is a strong plus  Building visualizations using Tableau, Qlik, or PowerBI is a strong plus ', ' Onboard alternative datasets including learning how to web scrape. ', ' Provide support for overnight jobs. ', ' Assist with ad-hoc data and research requests from the investment team. ', ' Learning the equity investment business and engaging directly with end users. ', ' Azure experience building data pipelines ', 'IT Group Description', ' Candidate must be willing to take ownership of projects and show strong client commitment ', 'Qualifications, Experience, Education', ' Best practices managing large data sets. ', ' Building visualizations using Tableau, Qlik, or PowerBI is a strong plus ', ' Experience working in the financial industry or knowledge of basic financial statement concepts  Azure experience building data pipelines  Experience using Airflow ', ' What makes this role unique or interesting? ', ' Solid analytical and technical skills  Candidate must be willing to take ownership of projects and show strong client commitment  Must demonstrate good communication skills and be comfortable working closely with business users  Self starter as well as a good team player  A strong desire to document and share work done to aid in long term support ', ' Experience using Airflow ', ' Solid analytical and technical skills ', ' Automation of data ingestion supporting various sources and formats both external and internal.  Implementing a quality control framework for ensuring data consistency.  Cataloging new data sets to facilitate data discovery, lineage, and self-service.  Building business intelligence dashboards to provide data insights.  Assist with ad-hoc data and research requests from the investment team.  Provide support for overnight jobs. ', ' Working with NoSQL is a strong plus ', ' Building business intelligence dashboards to provide data insights. ', ' Cataloging new data sets to facilitate data discovery, lineage, and self-service. ', ' Implementing a quality control framework for ensuring data consistency. ', ' A strong desire to document and share work done to aid in long term support ', 'This Role Provides Opportunity In The Following Areas', ' Must demonstrate good communication skills and be comfortable working closely with business users ', ' The key job responsibilities include, but are not limited to: ', 'Special Knowledge (nice To Have, But Not Required)', 'Job Description', ' Applying cloud based technologies including data lakes and data pipelines. ', ' What is the professional development value of this role? ', ' Building technical skills including SQL, Python, and PowerBI. ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,UpKeep,"Los Angeles, CA",2 days ago,50 applicants,"['', ' Customer Stories ', 'Experience implementing and administering BI toolsExperience working with API’s to collect or ingest data in batch or real-timeExperience using variety of Amazon Web Services (EC2, ELB, RDS)', 'Architect, implement, and maintain a comprehensive data environment including data warehousing, ETL pipelines, and BI tools', 'Translate business requirements into data models that are easy to understand and used by different disciplines across the company', 'Optimize data ingestion, storage, and processing architecture to meet product, business, and performance needsIdentify opportunities to automate and improve existing data processes with long-term scalability in mind ', '3+ years of data engineering experience', 'Optimize data ingestion, storage, and processing architecture to meet product, business, and performance needs', ""Examine and troubleshoot data stored in SQL in collaboration with UpKeep's Postgres DBA "", ' Data Engineer Qualifications ', 'Translate business requirements into data models that are easy to understand and used by different disciplines across the companyDesign and develop tools to enable teams to consume and understand data faster ', 'Identify opportunities to automate and improve existing data processes with long-term scalability in mind ', 'BS in Computer Science, Mathematics, or equivalent is preferred ', 'Design and develop tools to enable teams to consume and understand data faster ', 'Document and promote data engineering best practices ', 'Experience using variety of Amazon Web Services (EC2, ELB, RDS)', 'Have architected distributed systems with infrastructure automation, monitoring, and alerting ', ""Uphold and own UpKeep's data quality, privacy, and security in partnership with UpKeep's Engineering, TechOps, Legal, and IT teams"", ' Product Overview ', 'Proficiency in SQL Databases; including advanced analytical query (Postgres preferred)', ' Employee Benefits ', ""Uphold and own UpKeep's data quality, privacy, and security in partnership with UpKeep's Engineering, TechOps, Legal, and IT teamsDocument and promote data engineering best practices "", ' Data Engineer Role ', 'Experience implementing and administering BI tools', ' Demo Video ', 'Experience working with API’s to collect or ingest data in batch or real-time', ' The Company ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Compri Consulting,Denver Metropolitan Area,,N/A,"['', 'Required:', '-Salesforce.', '-Azure SQL Server database administration (production and development).', 'Desired:', '-Computer Science degree.', '-MS Dynamics', '-ETL / ELT processes.', '-Senior T-SQL coding.', 'Client located in Lakewood, Colorado is seeking a Data Engineer for a direct hire position.\xa0Responsibilities include data architecture, data ingestion, data processing, data presentation and database administration.', '\xa0']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Automotus,United States,2 weeks ago,Over 200 applicants,"['', '- Strong machine learning skills: first-principles knowledge of canonical algorithms and experience using popular libraries such as Tensorflow and Pytorch', ""Our team is small but growing fast. We're currently looking for our first data engineer, who will be a critical asset in designing, building, and implementing the data pipelines that form the foundation of our product."", 'Location', '- Experience building and maintaining a data warehouse in production', '- Design and implement ML models to deliver predictive insights to our customers', 'Automotus helps cities better understand, manage, and monetize their curbs. We use computer vision deployed at the edge to guide planning decisions, autonomously charge companies for parking by the minute, and automate enforcement, while also helping fleets save money by operating more efficiently and avoiding parking citations.', '- At least two years of professional software engineering experience, with proficiency in at least one major language (Python, Golang, Typescript preferred)', '- Experience working with time-series and geospatial datasets', ""- Function as a leader, contributing towards establishing a culture of excellence as Automotus' engineering team grows"", 'Preferred Skills/Experience', '- Architect, build, and maintain flexible & scalable data infrastructure', '- Applied statistics skills such as experimental design and hypothesis testing', 'Overview', '- Competitive salary', '- Strong familiarity with SQL databases', '- Volunteer time off policy (VTO)', 'Requirements', '- Experience designing, implementing, and maintaining extensible and scalable ETL pipelines', ""- Bachelor's Degree in Computer Science or a related field"", 'Automotus is based in Los Angeles, CA, but we operate as a fully distributed team. If the time zone where you are is within 4 hours of GMT-8, feel free to apply!', '- Experience with data transformation tools like Spark or Hadoop', '- Career development and upward mobility', 'What We Offer', 'Responsibilities', '- Self-directed vacation policy', '- Experience working on IoT products', ""Given the size of the team and the fluid nature of technical roles in an early-stage organization, we're most interested in applicants who have killer data engineering skills, but aren't opposed to getting their hands dirty elsewhere in our stack."", '- Health, dental, and vision coverage', 'We encourage women and people from underrepresented groups to apply.', '- Experience deploying data services in the cloud (AWS preferred)']",Entry level,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Data Engineer (Remote),Automox,"New York, NY",5 days ago,45 applicants,"['', ' Overview ', 'Experience partnering with business intelligence and analytics teams ', 'Experience with both relational SQL and NoSQL databases', 'Proficiency with and willingness to learn programming languages such as Golang, Python, Java, Scala', 'Work on projects that are critical to Automox’s mission and have high visibility across the companyBuild, enable, and maintain high-quality, reliable data infrastructurePartner with data science, engineering, and product teams to deliver new capabilities to customersLeverage modern engineering practices and tools in a cloud-native SaaS environmentHave an opinion, test your ideas, work with talented yet humble people, have fun, get stuff done', 'Build analytics tools as part of a data platform to enable data access, analytics workflows, and actionable insights into customer’s IT environments and the company’s operational efficiency', 'Stock options', 'Build, enable, and maintain high-quality, reliable data infrastructure', 'Parental Benefits: Adoption benefits, Parental leave', ""Our salary ranges are based on national averages and are determined based on the level of the position we are hiring for. The ranges are wide to leave room for variability in a candidate's skills, experience, and location all which impact where someone might come in on the range."", ' Benefits ', 'Have an opinion, test your ideas, work with talented yet humble people, have fun, get stuff done', 'Expertise with deploying and maintaining distributed data processing frameworks, database technologies, and streaming/messaging systems', 'Work on projects that are critical to Automox’s mission and have high visibility across the company', 'Knowledge of data science principles, statistics, or machine learning can be beneficial but should not be considered a required skill set', 'All employees are part of our Company bonus plan. Our bonus is a mix of company performance and individual contributions.', ' Equity ', ' Salary ', ' Bonus ', 'Ability to assemble large, complex data sets that satisfy the needs of both internal and external customers', 'Comfortable working in a remote/distributed work environment requiring excellent verbal, written, and asynchronous communication skills', 'Time off: We have a flexible PTO policy with an additional 9 paid holidays.', 'Perks: Monthly internet and wellness stipend, money to set up your home office, and no commute.', 'Partner with data science, engineering, and product teams to deliver new capabilities to customers', '$105,000 - $150,000/year', 'Track record of creating and maintaining data pipelines', 'Track record of creating and maintaining data pipelinesExpertise with deploying and maintaining distributed data processing frameworks, database technologies, and streaming/messaging systemsAbility to assemble large, complex data sets that satisfy the needs of both internal and external customersBuild analytics tools as part of a data platform to enable data access, analytics workflows, and actionable insights into customer’s IT environments and the company’s operational efficiencyFamiliarity with cloud environments and technologies (AWS preferred)Experience with both relational SQL and NoSQL databasesProficiency with and willingness to learn programming languages such as Golang, Python, Java, ScalaExperience partnering with business intelligence and analytics teams Comfortable working in a remote/distributed work environment requiring excellent verbal, written, and asynchronous communication skillsKnowledge of data science principles, statistics, or machine learning can be beneficial but should not be considered a required skill set', 'Healthcare options through Cigna and Guardian including Medical, Dental, Vision, Basic Life insurance, Voluntary Life Insurance, Basic STD & LTD, HSA, FSA, 401(k) and more. Automox has a generous employer contribution towards all health plans with low premiums for all employees.', 'Familiarity with cloud environments and technologies (AWS preferred)', 'Leverage modern engineering practices and tools in a cloud-native SaaS environment']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
SQL Data Engineer,Piper Companies,"Horsham, PA",19 hours ago,Be among the first 25 applicants,"['', 'Remote during Covid', ' Experience working in an Agile environment.', ' Experience working with Redshift and/or Snowflake preferred.', ' Implement performance and tuning activities in the ODS', ' Write complex SQL queries to move data from various source systems (flat files, SQL Server databases, xls files, JSON files, etc.) into the ODS.', ' Deep knowledge of ODS concepts, data loading issues and best practices.', 'Qualifications For The SQL Data Engineer Include', 'Responsibilities For The SQL Data Engineer Include', ' Strong data manipulation and data analysis skills.', 'Security Clearance', ' Horsham, PA ', ' Responsible for moving SSIS packages for SQL server 2016 to 2019 migration.', 'Comprehensive benefit package; Medical, Dental, Vision, 401k, PTORemote during Covid', ' Experience in a financial services industry preferred.', ' Create error handling and logging processes in the ODS', ' Work closely with the data modelers, data analysts, and QA team to ensure that the data is properly loaded into the ODS', ' 5+ years’ experience in creating and modifying SSIS packages.', ' Ability to work both independently as well as a team.', ' 5+ years’ experience performing data engineering in a ODS environment.', 'Compensation And Benefits', 'Job Category', 'Comprehensive benefit package; Medical, Dental, Vision, 401k, PTO', ' Experience with Boomi would be highly desirable.', ' Create reconciliation processes to ensure data is loaded into the ODS completely and accurately.', 'SQL Data Engineer ', ' Deep knowledge of database structures, normalization, de-normalization and entity relationships.', ' Experience with SQL Server upgrade would be very helpful.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Insight Global,"Atlanta, GA",,N/A,"['', 'Insight Global is hiring a Spark Data Engineer to build the data pipelines and processing needed to deliver on the company’s data science and mobile initiatives. This engineer will join a team that fills a key role in the continued growth of both Insight Global and our internal data team. If you are excited about the prospect of building things that help thousands of people find jobs and the hope and joy which comes along with that mission, this is the role and company for you.', 'Job Overview:', '•SQL ', '•Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.', '•3+ years professional experience in Spark using Python and associated libraries', 'Minimum Requirements: ', '•Good communication and collaboration skills, and the ability to work with business and IT stakeholders', 'At Insight Global, our goal is to connect our consultants with our customers and empower everyone to develop personally, professionally, and financially; so they can be the light to the world around them.', 'Specific Duties & Job Responsibilities: ', '•Spark programming, Databricks experience preferred', '•Python', 'Required Skills:', '•Cloud based development (Azure preferred)', '•Work with the IG Data Engineering team to develop IG’s data pipeline and processing capabilities in a Databricks and Azure environment', '•Build analytics tools that utilize the data pipeline to provide actionable insights into consultant & customer recommendations, operational efficiency, and other key business performance metrics.', '•Work with stakeholders and other teams to assist with data-related technical issues and support their data needs.', '•Hands on experience designing and developing ETL / ELT processes and pipelines', '•Assemble large, complex data sets to meet the data needs for IG’s data science, mobile application, and business reporting capabilities', '•3+ years of professional data engineering experience', '•2+ years professional experience designing and developing with a cloud service (Azure preferred)\xa0', '•2+ years professional experience with SQL and RDBMS development']",Associate,Full-time,Information Technology,Staffing and Recruiting,2021-03-24 13:05:10
Data Engineer,Calm,San Francisco Bay Area,3 weeks ago,Over 200 applicants,"['', 'Calm is also committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures. Please inform Calm’s Recruiting team if you need any assistance completing any forms or to otherwise participate in the application process.', 'Experience writing production-level code in Python or Go\xa0', 'We pay your medical, dental, & vision insurance premiums', 'Commuter benefits', 'About Calm', 'And much more!', 'Life insurance and disability benefits', 'Experience building and maintaining critical, reliable ETL pipelines', 'We have a simple mission at Calm: To make the world a happier and healthier place.', 'Work cross-functionally with our product, marketing and growth teams on complex and exciting projects that propel Calm’s business', 'Unlimited PTO', 'Create automated, highly reliable data pipelines', 'Test all code written and ensure production readiness before shipping', 'Experience working with at least one data processing tool such as Spark, Redshift/Snowflake, Airflow/Luigi, etcBasic working knowledge of other big data technologies\xa0Experience building and maintaining critical, reliable ETL pipelinesExperience writing production-level code in Python or Go\xa0Fluent in SQLProactive communicator who can translate between technical and non-technical stakeholdersExcellent sense of how to drive business impactTeam player who gives and takes feedback in a thoughtful way, and loves to help others.Work cross-functionally with our product, marketing and growth teams on complex and exciting projects that propel Calm’s businessBonus points if you have some devops related-experience working with, e.g., kubernetes, terraform, etc. Or if you have previous experience as a Data Scientist or analystAt least 2 years of data engineering experience', 'Over 75 Million people have downloaded the app and we are growing by 100,000 new downloads a day. The company is profitable and headquartered in San Francisco, CA.', 'Write well-tested, production ready code in Python, Go, and/or SQL', 'Fluent in SQL', 'Team player who gives and takes feedback in a thoughtful way, and loves to help others.', 'Bonus points if you have some devops related-experience working with, e.g., kubernetes, terraform, etc. Or if you have previous experience as a Data Scientist or analyst', 'The heart of Calm is digital but the brand is expanding offline into a variety of products and services that bring more peace, clarity and perspective into people’s busy lives. We are building Calm into the Nike of the Mind. We believe Calm can become one of the most valuable and meaningful brands in the world.', '\xa0', 'Apple equipment', 'Build data integrations within our data platform and between partners\xa0Write well-tested, production ready code in Python, Go, and/or SQLImprove the efficiency, reliability, and latency of our data systemCreate automated, highly reliable data pipelinesHelp define and craft our data modelOnboard onto Airflow and Redshift and assist with improvements and maintenanceDefine, design, and build data testing and quality frameworksTest all code written and ensure production readiness before shippingWork cross-functionally with our product, marketing and growth teams on complex and exciting projects that propel Calm’s businessStays up-to-date with high-potential new technologies, and can evaluate and present to the team for Calm’s use case', 'Competitive salary and equityUnlimited PTOWe pay your medical, dental, & vision insurance premiums401KCommuter benefitsLife insurance and disability benefitsApple equipmentOpportunity to work with a product focused on making the world happier and healthierAnd much more!', 'Calm is proud to be an Equal Employment Opportunity employer. We do not discriminate based upon race, religion, color, national origin, sex, sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, genetic information, political views or activity, or other applicable legally protected characteristics.\xa0', 'Calm was co-founded by Alex Tew (Million Dollar Homepage) and Michael Acton Smith (Mind Candy, Moshi Monsters, Firebox).', 'Build data integrations within our data platform and between partners\xa0', 'Stays up-to-date with high-potential new technologies, and can evaluate and present to the team for Calm’s use case', 'Mission', 'Define, design, and build data testing and quality frameworks', 'Experience working with at least one data processing tool such as Spark, Redshift/Snowflake, Airflow/Luigi, etc', 'Outcomes', 'At least 2 years of data engineering experience', 'Basic working knowledge of other big data technologies\xa0', 'Improve the efficiency, reliability, and latency of our data system', ""The Data Engineering team’s mission is to make Calm’s data reliable, trustworthy and easy to use.\xa0To do that, we’re building a data ecosystem that enables the entire organization to use data to make our product better, facilitate decision making and help drive business value.Data Engineers at Calm work closely with internal stakeholders, defining requirements and designing and building solutions that meet those requirements.\xa0We are a group of strong communicators, who care about our stakeholders and each other. We love to work together as a team to find simple solutions to complex problems.\xa0Our current stack includes Airflow, DBT, Tableau, Redshift, SQS, SNS, and Sagemaker all deployed on Docker and Kubernetes on AWS - but we’re always open to the right technology for the job.You can read more\xa0about our interview process,\xa0our\xa0engineering organization, and\xa0what we've worked on recently."", 'Help define and craft our data model', '401K', 'Excellent sense of how to drive business impact', 'Onboard onto Airflow and Redshift and assist with improvements and maintenance', 'Benefits', 'Calm is deeply committed to diversity, equity and inclusion, both in our hiring practices and in our experiences as a Calm employee. We strive to create a mindful and respectful environment where everyone can bring their authentic self to work, and experience a culture that is free of harassment, racism, and discrimination.\xa0', 'Proactive communicator who can translate between technical and non-technical stakeholders', 'Competitive salary and equity', 'Competencies', 'Opportunity to work with a product focused on making the world happier and healthier']",Mid-Senior level,Full-time,Engineering,"Health, Wellness and Fitness",2021-03-24 13:05:10
Data Engineer,Capital TechSearch,"Bethesda, MD",,N/A,"['', 'Assess, design, and implement data engineering and ETL solutions using Python, Spark, and AWSContribute your vision to new and existing data products and initiatives.Perform development work training and evaluating Machine Learning models.', 'AWSDockerKubernetesNodeJSExperience implementing Machine Learning applications with Python libraries such as scikit-learn', 'AWS', 'Experience implementing Machine Learning applications with Python libraries such as scikit-learn', 'Perform development work training and evaluating Machine Learning models.', '4+ years of software development experience', 'Apache Spark', 'In this role you will join a small team of engineers creating and implementing unique data products and applications for the commercial retail apparel industry.', 'NodeJS', '\xa0Required Expertise:', 'Agile Scrum', 'The ideal candidate has real-world experience performing data engineering with commercial data products.', 'Contribute your vision to new and existing data products and initiatives.', 'Kubernetes', 'Responsibilities Include:', 'Docker', 'High competency with Python', 'Assess, design, and implement data engineering and ETL solutions using Python, Spark, and AWS', 'Data Engineer', '\ufeffExpertise in the following is a plus:', 'Required Expertise:', 'SQL-based Data Warehouse services (Amazon Redshift, Snowflake, etc)', '4+ years of software development experienceHigh competency with PythonSQL-based Data Warehouse services (Amazon Redshift, Snowflake, etc)Apache SparkAgile Scrum']",Mid-Senior level,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Senior Data Engineer,"Seaglass Technology Partners, LLC",United States,1 day ago,Be among the first 25 applicants,"['Required:', '', 'AWS Redshift', 'Airflow', 'Experience with relational database systems', 'Python', 'Experience with data transformation technologies', 'Preferred:', '5+ years of data engineering experiencePython scripting experienceExperience with relational database systemsExperience with data transformation technologiesPythonSpark', 'Experience with cloud data systems (AWS or GCP preferred)DockerKubernetesAirflowAWS RedshiftSnowflake', 'Python scripting experience', 'Docker', 'Kubernetes', '5+ years of data engineering experience', 'Spark', 'Snowflake', 'Experience with cloud data systems (AWS or GCP preferred)']",Not Applicable,Full-time,Information Technology,Staffing and Recruiting,2021-03-24 13:05:10
Data Engineer,HealthVerity,"Philadelphia, PA",2 weeks ago,Be among the first 25 applicants,"['', ' 1+ years of experience with AWS EMR, AWS S3 service.', ' Comfortable on the command line and consider it an essential tool', ' Comfortable using *nix command line (shell scripting, AWK, SED)', ' Experienced in writing scalable applications on distributed architectures', ' Troubleshoot and resolve issues relating to data integrity', ' Experienced in writing scalable applications on distributed architectures Data driven, testing and measuring as much as you can Eager to both review peer code and have your code reviewed Comfortable on the command line and consider it an essential tool Confident in SQL, you know it, write smart queries, it’s no big deal', ' About You ', ' Required Skills And Experience ', ' Help establish procedures and best practices for transforming and storing data', ' Experience with Apache Zeppelin', ' Standardizing on common data models across data types', ' Work with some of the most exciting open-source tools like Spark, Hadoop, Docker, Airflow, Zeppelin', ' 3+ years of experience with PySpark and Spark-SQL (writing, testing, debugging spark routines)', ' Lead requirements gathering around data pipeline automation improvements', ' Orchestrating an industry-leading HIPAA privacy layer', ' Marvel at the speed with which your creation makes it into production', ' Ingesting and managing billions of healthcare records from a wide variety of partners', "" Work with the team to load data into HealthVerity's data warehouse Troubleshoot and resolve issues relating to data integrity Help establish procedures and best practices for transforming and storing data Lead requirements gathering around data pipeline automation improvements Work with some of the most exciting open-source tools like Spark, Hadoop, Docker, Airflow, Zeppelin Leverage distributed computing and serverless architecture such as AWS EMR & AWS Lambda, to develop pipelines for transforming data Marvel at the speed with which your creation makes it into production Research and implement new technologies with a team of developers to execute strategies and implement solutions Produce peer reviewed quality software Solve complex problems related to the real-time discovery of large data"", ' Confident in SQL, you know it, write smart queries, it’s no big deal', ' 3+ years of experience with Python', ' Experience with Apache Airflow Experience with Apache Zeppelin Experience with healthcare data', ' Building a culture that supports rapid iteration and new possibilities', ' Experience with Apache Airflow', ' Experience with MySQL and Postgres', ' Innovating our proprietary de-identification and data science algorithms', ' Produce peer reviewed quality software', ' Eager to both review peer code and have your code reviewed', ' Comfortable using AWS CLI and boto3', ' 3+ years of work experience', "" Work with the team to load data into HealthVerity's data warehouse"", ' Empowering clients with highly rewarding data discovery and licensing tools Ingesting and managing billions of healthcare records from a wide variety of partners Standardizing on common data models across data types Orchestrating an industry-leading HIPAA privacy layer Innovating our proprietary de-identification and data science algorithms Building a culture that supports rapid iteration and new possibilities', ' Research and implement new technologies with a team of developers to execute strategies and implement solutions', ' 3+ years of work experience 3+ years of experience with Python 3+ years of experience with PySpark and Spark-SQL (writing, testing, debugging spark routines) 1+ years of experience with AWS EMR, AWS S3 service. Comfortable using AWS CLI and boto3 Comfortable using *nix command line (shell scripting, AWK, SED) Experience with MySQL and Postgres', ' Empowering clients with highly rewarding data discovery and licensing tools', ' Leverage distributed computing and serverless architecture such as AWS EMR & AWS Lambda, to develop pipelines for transforming data', ' Experience with healthcare data', ' Data driven, testing and measuring as much as you can', ' Solve complex problems related to the real-time discovery of large data']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Python Data Engineer,RightClick,"New York, United States",2 days ago,74 applicants,"['-Competitive compensation package ', 'Our client is an explosive startup within the pharma space that is looking to add IT professionals to their growing offices. The company we represent is dedicated to transformative developments within the healthcare industry. Our client is looking to identify a Python Data Engineer to help build the cloud-based data processing platform for a home-grown Data Lake. ', 'What You Will Need as a Python Data engineer: ', 'What You Will Do as a Python Data Engineer:', ' ', 'Our client is unable to transfer or sponsor visas at this time.', '-Lay out the design of various components that make up the Data Warehouse platform ', 'What You Will Do as a Python Data Engineer: ', '-Expert knowledge in Python programming ', '-Start-up environment that breeds innovation ', 'Our client is not offering relocation packages. ', '-Automate and maintain data processing pipelines ', 'What You Will Get: ', '-BS degree in Computer Science or related field preferred ', '-5+ years of experience building data processing platforms ', '-Build the infrastructure required for optimal ingestion and refining of data sources ', 'What You Will Get:', 'What You Will Need as a Python Data engineer:', '-Work on big data analytics projects to derive actionable insights ', '-Autonomy within the role ', '-Working knowledge of MPP databases, SQL, Linux and the Hadoop/Spark ecosystem ']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,KLC Talent Match,"Ohio, United States",2 days ago,40 applicants,"['', 'Design and implement large scale (big data) multi-variable test plans to train AI models to detect tire wear and damage over a range of surfaces and conditions.', 'This position will lead all data collection and analysis, create and develop new projects, and investigate underlying tire physics which drive AI models.', 'Research underlying tire physics, AI models, and develop improvements.', 'Act as the primary interface with outside researchers/companies and the research team.', 'Manage all related AI projects and propose new projects/improvements']",Mid-Senior level,Full-time,Project Management,Automotive,2021-03-24 13:05:10
Data Engineer,"TrueChoice Solutions, Inc.",United States,5 days ago,122 applicants,"['', 'Delivering test plans, monitoring, debugging, and technical documents as a part of the development cycle.', 'TrueChoice Solutions is a private, rapidly growing Software as a Service (SaaS) preference analytics marketing software company that develops and markets unique, sophisticated applications that give its Fortune 500 clients insights about “how” and “why” buyers make purchase decisions. The company’s clients use these insights to help increase sales, client retention, and profitability. TrueChoice has an impressive list of Fortune 500 clients across multiple industries with offices in the UK, Continental Europe, and New York City.', 'Position Overview:', 'Expand our data platform and critical ETL pipelines while advancing best practices [think resiliency and maintainability] for not only the analytics team but the rest of the organization.', '3 or more years of data engineering experience Strong familiarity and experience with building out ETL pipelines.Top-notch understanding of basic statistics and issues surrounding data quality.Experience running and supporting the production of enterprise data platforms.Experience with relational and non-relational databases.Experience building data pipelines in AWS.Proficiency in most of the following: R, Python, Java, Git, SQL/NoSQL, and Bash (C# and .NET experience good to have)Excellent written and verbal communication skills, as well as top organizational, time management, and documentation skills.', 'Identifying, designing, and implementing internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability.', 'Proficiency in most of the following: R, Python, Java, Git, SQL/NoSQL, and Bash (C# and .NET experience good to have)', 'Position Requirements:', 'Experience building data pipelines in AWS.', 'Strong familiarity and experience with building out ETL pipelines.', 'Company Overview:', 'Duties & Responsibilities: ', 'TrueChoice Solutions is currently seeking a qualified candidate to fill the role of Data Engineer within our growing organization. The ideal candidate is a self-motived teammate, skilled in a broad set of data processing techniques with the ability to adapt and learn quickly, provide results with limited direction, and choose the best possible data processing solution is a must. This is a full-time position.', 'Creating data tools for analytics and working with stakeholders across all departments to assist with data-related technical issues and supporting their data infrastructure needs.Expand our data platform and critical ETL pipelines while advancing best practices [think resiliency and maintainability] for not only the analytics team but the rest of the organization.Identifying, designing, and implementing internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability.Delivering data integration solutions to downstream marketing and campaign software.Delivering quality production-ready code in an agile environment.Delivering test plans, monitoring, debugging, and technical documents as a part of the development cycle.Effectively document, present, and explain deliverables to senior leadership.', 'Delivering data integration solutions to downstream marketing and campaign software.', 'Experience running and supporting the production of enterprise data platforms.', 'Experience with relational and non-relational databases.', '3 or more years of data engineering experience ', 'Effectively document, present, and explain deliverables to senior leadership.', 'Delivering quality production-ready code in an agile environment.', 'Position Requirements', 'Excellent written and verbal communication skills, as well as top organizational, time management, and documentation skills.', 'Creating data tools for analytics and working with stakeholders across all departments to assist with data-related technical issues and supporting their data infrastructure needs.', 'Top-notch understanding of basic statistics and issues surrounding data quality.']",Entry level,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Data Engineer,BoomTown - Real Estate Platform,"Charleston, South Carolina Metropolitan Area",1 week ago,60 applicants,"['', 'BoomTown is proud to be an Equal Opportunity Employer.\xa0We evaluate qualified applicants without regard to race, color, national origin, religion, sex, sexual orientation, gender identity, disability, protected veteran status, and other statuses protected by law.', 'BoomTown is looking for a Data Engineer to join a cross-functional team that serves as a virtual center of data excellence for the company.\xa0\xa0', 'Work closely with our operations and other development teams to optimize source data acquisition processes and strategies', 'To help you stay energized, engaged and inspired, we offer a wide range of benefits including a 401k retirement plan, comprehensive healthcare, discretionary PTO after a year, and a quarterly wellness reimbursement towards health and fitness.\xa0\xa0', 'BS degree in Computer Science or related technical field\xa0\xa02-4 years of relevant work experienceA deep understanding of general ETL processing and tools as well as data warehousing concepts and workflowsStrong knowledge of SQL and Python\xa0Significant experience with large scale data sources, structures and processes', 'Participate in technology evaluation and selection initiatives that grow the company’s core data and analytics capacity\xa0', 'Responsibilities:', 'Experience with Amazon Web Services, particularly Redshift, S3\xa0or comparable services from another cloud providerExperience with ETL orchestration tools to run large scale ETL workflowsExperience deploying infrastructure as code with tools such as AWS CloudFormation and TerraformExperience working with web data sourcesExperience working in an Agile development environment will be a plus.', '2-4 years of relevant work experience', 'BS degree in Computer Science or related technical field\xa0\xa0', 'Experience with Amazon Web Services, particularly Redshift, S3\xa0or comparable services from another cloud provider', 'Experience working in an Agile development environment will be a plus.', 'Continually research and learn the latest approaches to building analytical solutions in highly variant data environments\xa0', 'Build ETL processes that integrate data from multiple, highly variant sources into analytic models and data stores that feed multiple end user solutions and perspectives', 'With our product suite, you get access to world-class lead generation, consumer-websites, CRM technology, and a powerful mobile app. Together, they create a seamless experience so you can attract, convert, and win new clients easily.\xa0', 'Strong knowledge of SQL and Python\xa0', 'What is BoomTown?', 'Significant experience with large scale data sources, structures and processes', 'Take initiative on assigned day-to-day tasks and keep pace with the team to get the job done while knowing when to ask for help.', 'A deep understanding of general ETL processing and tools as well as data warehousing concepts and workflows', 'Be a proactive member of an autonomous, cross-disciplined team with a goal of building best in class data solutions that delight internal and external customers.Build ETL processes that integrate data from multiple, highly variant sources into analytic models and data stores that feed multiple end user solutions and perspectivesContinually research and learn the latest approaches to building analytical solutions in highly variant data environments\xa0Work closely with our operations and other development teams to optimize source data acquisition processes and strategiesParticipate in technology evaluation and selection initiatives that grow the company’s core data and analytics capacity\xa0Understand product descriptions for new features and how they fit into the greater BoomTown system while learning how our customers use themTake initiative on assigned day-to-day tasks and keep pace with the team to get the job done while knowing when to ask for help.', 'Experience deploying infrastructure as code with tools such as AWS CloudFormation and Terraform', 'Be a proactive member of an autonomous, cross-disciplined team with a goal of building best in class data solutions that delight internal and external customers.', 'Experience working with web data sources', 'As the Data Engineer, you will design and build analytic data stores and integration processes that meet the data needs of various internal and external constituents.\xa0The Data Engineer will be responsible for designing, building, monitoring, conceptualizing, innovating, and collaborating with team members and customers.\xa0You will be in a fast-growing and rapidly changing environment that will demand a continuous improvement mindset and a hunger for the next challenge.', 'Required Qualifications:', 'Understand product descriptions for new features and how they fit into the greater BoomTown system while learning how our customers use them', 'Our Benefits:', 'Experience with ETL orchestration tools to run large scale ETL workflows', 'BoomTown is a software platform designed to help real estate professionals generate leads, manage contacts, and run their business better. Over 40,000 of the industry’s best use our products to close more deals. Their success is trademarked with growth. It’s why the Real Trends Thousand is dominated by BoomTown clients. We bring the technology and experts to turn opportunities into closings.', 'Preferred Qualifications:']",Mid-Senior level,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Data Engineer,Flashii ,"Cupertino, CA",5 days ago,106 applicants,"['', 'Automation', 'Airflow', 'Looking for a Data Engineer skilled in the following:', 'Data ingest', 'Data QA', 'Remote', 'Long-Term Contract', 'Strong in Python and SQL', 'Pipeline creation', 'Requirements:', 'Strong in Python and SQLData ingestPipeline creationData QAAutomationSparkAirflow', 'Spark', 'Must be US Citizens/GC Holders']",Mid-Senior level,Contract,Information Technology,Staffing and Recruiting,2021-03-24 13:05:10
Data Integration Engineer,Peloton Interactive,"New York, NY",7 days ago,Be among the first 25 applicants,"['', 'Job Responsibilities', 'Own the Boomi development process from requirements gathering to full implementation.', 'Peloton does not accept unsolicited agency resumes. Agencies should not forward resumes to our jobs alias, Peloton employees or any other organization location. Peloton is not responsible for any agency fees related to unsolicited resumes.', 'Lead the research, development & implementation of special projects, as needed.', 'Strong verbal and written communication skills', 'SQL language proficiency with at least one of the common database platforms such as SQL Server, Oracle, MySQL, and/or DB2 database management systems', 'You are a proactive problem-solver, even in areas of uncertainty and ambiguity.', 'Dell Boomi ', 'supply chain systems data integrations', 'Strong understanding of integration architecture options such as SoA and APIs', 'Experience with REST and SOAP web services', 'Boomi Developer/Architect certified', 'Collaborate with the development team to architect efficient and stable integrations.', 'You have excellent analytical and critical reasoning skills.', 'Monitor, troubleshoot, and resolve problems with integrations.', 'Ability to work well under pressure; manage tight deadlines and situations where conflicting priorities arise.', 'Minimum of 3 years experience working with integration platforms (Boomi, MuleSoft etc.).', ' You have the ability to analyze business problems and develop both long-term sustainable solutions and also quick tactical solutions. You can articulate complex concepts in a way that is understandable to non-technical stakeholders. You have excellent analytical and critical reasoning skills. You are a proactive problem-solver, even in areas of uncertainty and ambiguity. You possess strong collaboration skills and approach problems with positive intent while driving towards resolution. ', 'Bachelor’s degree', 'You can articulate complex concepts in a way that is understandable to non-technical stakeholders.', 'Strong analytical and critical thinking skills', 'Experience developing applications that utilize Boomi Integration', ' Bachelor’s degree Strong verbal and written communication skills Strong analytical and critical thinking skills Adapt and proactive at problem-solving and conflict resolution. Minimum of 3 years experience working with integration platforms (Boomi, MuleSoft etc.). Proven ability to implement at least 1 to 2 full integration projects involving on-premise and Cloud applications. Functional experience with ERP systems (i.e., NetSuite, SAP) Experience with REST and SOAP web services SQL language proficiency with at least one of the common database platforms such as SQL Server, Oracle, MySQL, and/or DB2 database management systems Ability to work well under pressure; manage tight deadlines and situations where conflicting priorities arise. ', 'Functional experience with ERP systems (i.e., NetSuite, SAP)', 'Provide guidance on the monitoring, troubleshooting, and resolution of data quality issues in collaboration with the appropriate stakeholders; and', 'About Peloton', ' Integration Engineer', 'Document and analyze current business processes and underlying systems/applications.', 'Basic Job Requirements', 'Adapt and proactive at problem-solving and conflict resolution.', 'You have the ability to analyze business problems and develop both long-term sustainable solutions and also quick tactical solutions.', 'Hands-on experience on Integration Platforms such as Boomi, MuleSoft; AnyPoint, Pentaho, and Informatica', ' Own the Boomi development process from requirements gathering to full implementation. Monitor, troubleshoot, and resolve problems with integrations. Collaborate with the development team to architect efficient and stable integrations. Document and analyze current business processes and underlying systems/applications. Lead the research, development & implementation of special projects, as needed. Provide guidance on the monitoring, troubleshooting, and resolution of data quality issues in collaboration with the appropriate stakeholders; and Communicate and collaborate effectively with technical peers and business users. ', 'You possess strong collaboration skills and approach problems with positive intent while driving towards resolution.', 'Experience with performance tuning optimization within Boomi', 'Proven ability to implement at least 1 to 2 full integration projects involving on-premise and Cloud applications.', 'Preferred Experience', ' Boomi Developer/Architect certified Experience developing applications that utilize Boomi Integration Strong understanding of integration architecture options such as SoA and APIs Experience with performance tuning optimization within Boomi Functional experience with ERP systems (i.e., NetSuite, SAP) Hands-on experience on Integration Platforms such as Boomi, MuleSoft; AnyPoint, Pentaho, and Informatica ', 'Who You Are', 'Communicate and collaborate effectively with technical peers and business users.']",Entry level,Full-time,Information Technology,"Health, Wellness and Fitness",2021-03-24 13:05:10
Data Engineer,"WBCP, Inc","Fairfield, CA",1 week ago,Be among the first 25 applicants,"['', 'THE DEPARTMENT OF INFORMATION TECHNOLOGY', 'for more information see the complete job announcement: https://wbcpinc.com/wp-content/uploads/2021/03/Announcement-Data-Engineer-Solano-v6-FINAL.pdf', 'The County of Solano has three openings for Data Engineers to assist the Department of Information Technology (DoIT) to build a data warehouse from the ground up', 'County of Solano, California', 'THREE (3) POSITIONS OPEN', 'If you are looking for an amazing career opportunity, in a dynamic and fast paced environment, working with a forward thinking, team-oriented organization, then apply today!', 'DATA ENGINEERS']",Entry level,Full-time,Information Technology,Human Resources,2021-03-24 13:05:10
Data Engineer,Novetta,"Washington, DC",2 days ago,Be among the first 25 applicants,"['', 'Experience with CI/CD and DevSecOps Environments, developing and deploying Data Quality Pipelines', ' Must have the ability to obtain and maintain DHS suitability within a reasonable, customer-mandated time frame. ', 'Excellence in Execution ', 'in machine learning, data analytics, full-spectrum cyber, cloud engineering, open source analytics, and multi-INT fusion', 'Basic Qualifications: ', 'Desired Skills:', 'Migration to the cloud', ' We strive daily to exceed expectations and achieve customer mission success.Employee Focus ', ' for ', 'Earn a REFERRAL BONUS for the qualified people you know. ', ' We know that discovering new and innovative ways to solve problems is critical to our success and makes us a great company.Excellence in Execution ', 'Big Data (e.g., NiFi, Hadoop, Spark, PySpark, Dask, etc.)', 'AWS or Azure Certifications', 'Department of Homeland Security or other government law enforcement agency', "" We invest in our employees' professional development and training, respecting individuality, and fostering a culture of diversity and inclusion.Innovation "", 'Join our team dedicated to developing and executing innovative solutions in support of customer mission success. Novetta is committed to cultivating a diverse, inclusive workplace culture, embracing our differences and perspectives to build a stronger, more successful company.', 'Data Management and Governance', ""Integrity  We hold ourselves accountable to the highest standards of integrity and ethics.Customer Success  We strive daily to exceed expectations and achieve customer mission success.Employee Focus  We invest in our employees' professional development and training, respecting individuality, and fostering a culture of diversity and inclusion.Innovation  We know that discovering new and innovative ways to solve problems is critical to our success and makes us a great company.Excellence in Execution  We take pride in flawless execution as we build a company that is best in class.Earn a REFERRAL BONUS for the qualified people you know. For more details or to submit a referral, visit bit.ly/NovettaReferrals.Novetta is an equal opportunity/affirmative action employer.All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected veteran status, age, or any other characteristic protected by law."", 'Agile methodologies', 'Innovation ', 'Our culture is shaped by a commitment to our core values:', ' AWS or Azure Certifications ', 'Must have the ability to obtain and maintain DHS suitability within a reasonable, customer-mandated time frame.', 'Defense, Intelligence Community, and Federal Law Enforcement customers. Novetta is headquartered in McLean, VA with over 1,300 employees across the U.S.', 'Ability to manage and organize data while identifying trends and inconsistencies that will impact government missions', 'Employee Focus ', 'Novetta is an equal opportunity/affirmative action employer.', ' Department of Homeland Security or other government law enforcement agency Migration to the cloud Data Management and Governance Big Data (e.g., NiFi, Hadoop, Spark, PySpark, Dask, etc.) Evaluating and assessing data exchange processes Agile methodologies ', 'Evaluating and assessing data exchange processes', 'Experience in cloud, containerization (Docker, OpenShift, etc), indexing, scaling databases, and graph analysis to build in automation, efficiency, and implementation of security controls', 'Novetta, from complexity to clarity.', ' Minimum of five (5) years progressive technical or functional experience in a selected technical area or specialty skill - Talend ETL, Hadoop, NiFi, Spark, PySpark, Python, R etc Experience in cloud, containerization (Docker, OpenShift, etc), indexing, scaling databases, and graph analysis to build in automation, efficiency, and implementation of security controls Experience with CI/CD and DevSecOps Environments, developing and deploying Data Quality Pipelines Experience with relational and non relational databases (SQL, NOSQL, MongoDB, etc) Ability to manage and organize data while identifying trends and inconsistencies that will impact government missions Experience preparing data for predictive and prescriptive modeling; uses data to discover tasks that can be automated; delivers updates to stakeholders based on analytics ', 'Minimum of five (5) years progressive technical or functional experience in a selected technical area or specialty skill - Talend ETL, Hadoop, NiFi, Spark, PySpark, Python, R etc', 'Experience preparing data for predictive and prescriptive modeling; uses data to discover tasks that can be automated; delivers updates to stakeholders based on analytics', 'Job Description: ', 'Experience with relational and non relational databases (SQL, NOSQL, MongoDB, etc)', ' We take pride in flawless execution as we build a company that is best in class.Earn a REFERRAL BONUS for the qualified people you know. For more details or to submit a referral, visit bit.ly/NovettaReferrals.Novetta is an equal opportunity/affirmative action employer.All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected veteran status, age, or any other characteristic protected by law.', 'Customer Success ', 'Novetta delivers highly scalable advanced analytics and secure technology solutions to address challenges of national and global significance. Focused on mission success, Novetta pioneers disruptive technologies ', ' We hold ourselves accountable to the highest standards of integrity and ethics.Customer Success ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer I,Consulting Solutions,"Fort Worth, TX",,N/A,"['', 'This is a great opportunity to get real industry experience with a planned track to more senior roles in an established, enterprise environment. ', 'Applied experience in Agile, SAFe, Scrum or Lean Six Sigma', 'Collaborate with business stakeholders to develop an understanding of their business requirements and operational processes;', 'The Data Engineer I designs, builds and implements data integration and data services across the enterprise; specially in our Microsoft Azure cloud environment.\xa0As part of the Enterprise Data & Analytics Team, the Data Engineer I directly contributes to modernizing our data integration practices based on the Enterprise Data and Analytics strategy, Enterprise Architecture and Data Governance outcomes. ', 'Develop data pipeline automation and integration capabilities to implement the data lifecycle, considering how data is created, transformed, stored, archived, analyzed and shared in the organization;', 'Ability to quickly adapt to changing priorities and generating innovative solutions', 'Preferred Qualifications', 'Familiarity with analytics solutions and data science capabilities', 'Responsibilities:', 'Required Qualifications', 'Develop data pipeline automation and integration capabilities to implement the data lifecycle, considering how data is created, transformed, stored, archived, analyzed and shared in the organization;Collaborate with business stakeholders to develop an understanding of their business requirements and operational processes;Implement and act on the recommendations, outcomes and designs from Data Governance and Enterprise Architecture;Contribute to process for making data architecture decisionsComplete data exploration and profiling by leveraging data analysis, design and presentation tools;Contribute to the design, development, and maintenance of ongoing automation, integration, services, etc. to provide the right data to the right place at the right time;Develop and implement automated solutions to optimize efficiency and quality of First Command enterprise data and business processes;Partner with Information Technology and business unit groups to develop and implement comprehensive automation and business critical data which will be leveraged for solutions that provide consistent, clean and integrated data which enables business intelligence;Implement methods to include unstructured and big data, such as social media, emails, pictures, videos, voice and sensor data, client surveys and feedback.', 'Education', '0-2 years of applied experience in data integration, ETL, and data management or comparable positions that handle large/complex data sets, developing automation, and fostering business partner relationships', 'Familiarity with data science and analytics tools such as SPSS, SAS, Tableau, PowerBI', 'Certifications related to Data Integration a plus', 'Develop and implement automated solutions to optimize efficiency and quality of First Command enterprise data and business processes;', 'Excellent written communication and presentation skills', '0-2 years of applied experience in data integration, ETL, and data management or comparable positions that handle large/complex data sets, developing automation, and fostering business partner relationshipsProficient in Databricks and/or Microsoft Azure Data Factory; SSIS and Informatica are a plusProficient in SQLExcellent written communication and presentation skillsProficient in transforming manual processes to industry standards and automationProficient in understanding of data mapping and lineage strategiesProficient in understanding in conceptual, logical and physical data designProficient in understanding of data management practices, data architecture principles, and data governance processAbility to quickly adapt to changing priorities and generating innovative solutions', 'Implement methods to include unstructured and big data, such as social media, emails, pictures, videos, voice and sensor data, client surveys and feedback.', 'Financial services industry experience or other highly regulated industry experience a plus', 'Proficient in Databricks and/or Microsoft Azure Data Factory; SSIS and Informatica are a plus', 'Complete data exploration and profiling by leveraging data analysis, design and presentation tools;', 'Proficient in understanding of data management practices, data architecture principles, and data governance process', 'Responsibilities', 'Familiarity with object-oriented programming and the software development process', 'Proficient in SQL', 'Working in the Scaled Agile Framework, the Data Engineer I will use new and established automation and integration tools, the Enterprise Information Management Strategy (EIMS), frameworks, critical thinking, and problem-solving skills to partner with the First Command lines of business to rationalize data needs that are critical to our data transformation.\xa0', 'Partner with Information Technology and business unit groups to develop and implement comprehensive automation and business critical data which will be leveraged for solutions that provide consistent, clean and integrated data which enables business intelligence;', 'Contribute to the design, development, and maintenance of ongoing automation, integration, services, etc. to provide the right data to the right place at the right time;', 'Contribute to process for making data architecture decisions', 'Proficient in transforming manual processes to industry standards and automation', 'US Citizens and Green Card holders will only be considered as this is for W2 basis only. All other visa types are not eligible for this position. Also, messages from third party vendors will be ignored. ', 'Implement and act on the recommendations, outcomes and designs from Data Governance and Enterprise Architecture;', 'Proficient in understanding of data mapping and lineage strategies', 'BS preferred; MBA or MS or equivalent a plus.', 'Financial services industry experience or other highly regulated industry experience a plusCertifications related to Data Integration a plusApplied experience in Agile, SAFe, Scrum or Lean Six SigmaFamiliarity with analytics solutions and data science capabilitiesFamiliarity with object-oriented programming and the software development processFamiliarity with data science and analytics tools such as SPSS, SAS, Tableau, PowerBI', 'Proficient in understanding in conceptual, logical and physical data design']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Tencent,"Los Angeles, CA",4 weeks ago,197 applicants,"['·\xa02+ years’ experience in custom ETL design, implementation and maintenance.', '·\xa02+ years of Python development experience.', '·\xa02+ years of SQL experience.', 'Tencent Games is looking for Data Engineer (DE) with 2+ years’ experience:', '·\xa0Educate your partners: Use your data and analytics experience to ‘see what’s missing’, identifying and addressing gaps in their existing logging and processes.', '·\xa0Leverage data and business principles to solve large scale web, mobile and data infrastructure problems.', '·\xa02+ years of experience with workflow management engines (i.e. Airflow, Luigi, Prefect, Dagster, digdag.io, Google Cloud Composer, AWS Step Functions, Azure Data Factory, UC4, Control-M).', '·\xa0Design, build and launch extremely efficient and reliable data pipelines to move data across a number of platforms including Data Warehouse, online caches and real-time systems.', 'Requirements:', '·\xa0Build data expertise and own data quality for your areas.', '·\xa0Experience querying massive datasets using Spark, Hadoop, Presto, Hive, Impala, etc.', 'Responsibility:', '·\xa0Communicate, at scale, through multiple mediums: Presentations, dashboards, company-wide datasets, bots and more.', '\xa0']",Mid-Senior level,Full-time,Information Technology,Internet,2021-03-24 13:05:10
Senior Data Engineer,The Washington Post,"Washington, DC",23 hours ago,Be among the first 25 applicants,"['', 'The Washington Post', 'BA/BS in Computer Science or related technical field or equivalent practical experience.', 'Experience with AWS data warehouse technologies including GLUE ETL, Lambda, S3, EMR, and EC2', 'Preferred Qualifications', 'Architect, build and deploy on AWS', ' The Post strives to provide its readers with high-quality, trustworthy news and information while constantly innovating. That mission is best served by a diverse, multi-generational workforce with varied life experiences and perspectives. All cultures and backgrounds are welcomed. ', 'Model, query, and analyze large, business-critical datasets', 'Plan and then execute successful complex technical projects in an Agile process.', 'Experience using stream and batch processing technologies including Hadoop, Spark, and Kafka', '#washpostlife', 'Participate in all stages of software development - from early brainstorming to coding and bug fixingModel, query, and analyze large, business-critical datasetsArchitect, build and deploy on AWSPlan and then execute successful complex technical projects in an Agile process.Help in on call activities when the ETL is broken or have data issues.Assist business with ad hoc reports and business requirements.', ' Impact Tomorrow', 'Assist business with ad hoc reports and business requirements.', 'Experience using stream and batch processing technologies including Hadoop, Spark, and KafkaExperience with AWS data warehouse technologies including GLUE ETL, Lambda, S3, EMR, and EC2Experience with multiple database technologies including Postgres, MySQL, MongoDB, and DynamoDB', '6+ years ETL/ELT experience', 'Experience with multiple database technologies including Postgres, MySQL, MongoDB, and DynamoDB', 'Responsibilities', '3+ years of Python programming skills', 'Qualifications', '6+ years of experience modeling, querying and analyzing large datasets using SQL', 'Participate in all stages of software development - from early brainstorming to coding and bug fixing', 'Strong Hive and Pyspark skills', 'Experience building business-critical, data processing pipelines on AWS', 'Job Description', 'BA/BS in Computer Science or related technical field or equivalent practical experience.6+ years of experience modeling, querying and analyzing large datasets using SQL6+ years ETL/ELT experience3+ years of Python programming skillsStrong Hive and Pyspark skillsExperience building business-critical, data processing pipelines on AWS', 'Help in on call activities when the ETL is broken or have data issues.']",Associate,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer,Cresset,"Reston, VA",2 weeks ago,Be among the first 25 applicants,"['', 'Apply for this Position', ' Excellent communication skills and ability to present data analytics to senior leadership ', ' Acquire and integrate data from multiple sources/systems for analysis ', ' Bachelor’s degree required in computer science, engineering and/or similar field ', ' Bachelor’s degree required in computer science, engineering and/or similar field  3+ years of experience in performing data analysis on cross functional business processes involving multiple IT systems  Experience building API integrations, including learning vendor API documentation, and staying current with updates to the tech stack, requirements, and advancements  Experience with Sisense preferred  Ability to code in SQL and Python  Structured & unstructured data expertise  Strong analytical experience and understanding technology processes of information and data  Excellent communication skills and ability to present data analytics to senior leadership  Strong teamwork and interpersonal skills to collaborate with people across different functions  Self-motivated and detail oriented  Knowledge of RIA industry preferred ', ' Assist in developing a data reporting and analytics framework, including industry best practices  Improve the performance of queries and analysis through automation  Collaborate with business partners to prioritize requests/needs and provide a holistic view of the analysis  Acquire and integrate data from multiple sources/systems for analysis  Identify data relationships such as trends, patterns, and correlations in order to answer business questions as well as provide actionable recommendations  Present data in a clear and concise manner allowing the internal partners to quickly understand the results and make data driven decisions  Perform advanced data analytics (e.g., data mining, statistical analysis, predictive analytics) ', 'About Cresset', ' Collaborate with business partners to prioritize requests/needs and provide a holistic view of the analysis ', ' Thank You ', ' Knowledge of RIA industry preferred ', ' Experience building API integrations, including learning vendor API documentation, and staying current with updates to the tech stack, requirements, and advancements ', ' Present data in a clear and concise manner allowing the internal partners to quickly understand the results and make data driven decisions ', ' Strong analytical experience and understanding technology processes of information and data ', ' Assist in developing a data reporting and analytics framework, including industry best practices ', ' Perform advanced data analytics (e.g., data mining, statistical analysis, predictive analytics) ', ' 3+ years of experience in performing data analysis on cross functional business processes involving multiple IT systems ', ' Strong teamwork and interpersonal skills to collaborate with people across different functions ', 'Qualifications', ' Experience with Sisense preferred ', ' Ability to code in SQL and Python ', ' Improve the performance of queries and analysis through automation ', 'By submitting your interest in this job, you agree to receive text notifications with additional steps to complete your job application. You will receive up to 6 messages from the number ""63879"". Message & data rates may apply. Please refer to our privacy policy for more information.', ' Structured & unstructured data expertise ', ' Self-motivated and detail oriented ', 'Job Description', ' Identify data relationships such as trends, patterns, and correlations in order to answer business questions as well as provide actionable recommendations ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer (100% remote!),Optello,"Denver, CO",4 days ago,36 applicants,"['', ' Contributing to product development efforts with goals of reliable delivery, high quality (eg. bug free), technical excellence and continuous delivery', ' Practicing general software engineering principles (SOLID principles), best practices (code review), engineering patterns (e.g. design patterns), and Test Driven Development', 'Your Right to Work', ' Contributing to building new predictive and prescriptive analytics to compliment existing descriptive and diagnostic analytics', ' Collaborating with distributed team of engineers and product owners', ' Apache PIG', ' Relational databases', ' Azure App Functions', ' Spark', 'Email Your Resume In Word To', ' Enhancing the data pipeline and platform using architectural and design patterns such as data lake Implementing scalable data services using serverless Azure resources such as Data Factory, Synapse, Databricks, Azure Functions and traditional SQL Collaborating with data scientists to implement AI and machine learning solutions, and DevOps to automate workflows and processes Contributing to building new predictive and prescriptive analytics to compliment existing descriptive and diagnostic analytics Contributing to product development efforts with goals of reliable delivery, high quality (eg. bug free), technical excellence and continuous delivery Implementing technical design for feature enhancements using appropriate design patterns, data and object models Collaborating with distributed team of engineers and product owners Practicing general software engineering principles (SOLID principles), best practices (code review), engineering patterns (e.g. design patterns), and Test Driven Development', ' Prior experience in a similar role, ideally within the health tech space but one where analytics was the focal point of the technology Experience with the following languages and technologies: Relational databases SQL ETL (Azure Data Factory / SSIS, Databricks) Azure App Functions Azure Data lake Spark Apache PIG', ' Competitive compensation + benefits', ' Competitive compensation + benefits Generous annual bonus structure', ' Collaborating with data scientists to implement AI and machine learning solutions, and DevOps to automate workflows and processes', ' Experience with the following languages and technologies:', ' SQL', 'Please do NOT change the email subject line in any way. You must keep the JobID: linkedin : JG9-1622345 -- in the email subject line for your application to be considered.***', ' Generous annual bonus structure', 'Optello is proud to be an Equal Opportunity Employer', ' Azure Data lake', ' Implementing technical design for feature enhancements using appropriate design patterns, data and object models', ' Implementing scalable data services using serverless Azure resources such as Data Factory, Synapse, Databricks, Azure Functions and traditional SQL', ' Enhancing the data pipeline and platform using architectural and design patterns such as data lake', ' ETL (Azure Data Factory / SSIS, Databricks)', ' Prior experience in a similar role, ideally within the health tech space but one where analytics was the focal point of the technology']",Entry level,Full-time,Information Technology,Construction,2021-03-24 13:05:10
Data Engineer Sr,PNC,"Philadelphia, PA",4 weeks ago,Be among the first 25 applicants,"['', 'Customer Focused - Knowledgeable of the values and practices that align customer needs and satisfaction as primary considerations in all business decisions and able to leverage that information in creating customized customer solutions.', ""Customer Focused - Knowledgeable of the values and practices that align customer needs and satisfaction as primary considerations in all business decisions and able to leverage that information in creating customized customer solutions.Managing Risk - Assessing and effectively managing all of the risks associated with their business objectives and activities to ensure they adhere to and support PNC's Enterprise Risk Management Framework."", 'Leads in developing, supporting and implementing data solutions for multiple applications in order to meet business objectives and user requirements. Leverages technical knowledge and industry experience to design, build and maintain technology solutions.', 'Oversees the development and implementation of data solutions for multiple applications to ensure its scalability, availability and maintainability.', 'Disability Accommodations Statement', 'Equal Employment Opportunity (EEO)', 'Education', 'Leads in designing and building data service infrastructure on multiple data platforms, according the workflow.', 'California Residents ', 'Managing Risk', 'Leads data requirement analysis and the data preparation process development for targeted data solutions.', 'Leads in developing, supporting and implementing data solutions for multiple applications in order to meet business objectives and user requirements. Leverages technical knowledge and industry experience to design, build and maintain technology solutions.Leads data requirement analysis and the data preparation process development for targeted data solutions.Leads in designing and building data service infrastructure on multiple data platforms, according the workflow.Oversees the development and implementation of data solutions for multiple applications to ensure its scalability, availability and maintainability.Consults on data migration and transformation to ensure the accuracy and security of data solutions.', 'Work Experience', 'Consults on data migration and transformation to ensure the accuracy and security of data solutions.', 'Position Overview', ""Managing Risk - Assessing and effectively managing all of the risks associated with their business objectives and activities to ensure they adhere to and support PNC's Enterprise Risk Management Framework."", 'Job Description', 'Customer Focused', 'Competencies']",Not Applicable,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,"Agile Resources, Inc.","Atlanta, GA",,N/A,"['', 'BS degree in Computer Science or related technical fieldExperience with Amazon Web Services, particularly Redshift, S3\xa0or comparable services from another cloud providerExperience with ETL orchestration tools to run large scale ETL workflowsExperience deploying infrastructure as code with tools such as AWS CloudFormation and TerraformExperience working with web data sourcesExperience working in an Agile development environment will be a plus.', 'Role Type:', '.', 'Pay:\xa0up to $115k DoE', 'Our Ideal Candidate has the Following:', 'Strong knowledge of SQL and Python', 'Role Type:\xa0Direct Hire / Full-time / Permanent', 'Pay:\xa0', 'BS degree in Computer Science or related technical field', '2-4 years of relevant work experience', 'Experience with Amazon Web Services, particularly Redshift, S3\xa0or comparable services from another cloud provider', 'Experience working in an Agile development environment will be a plus.', 'Our client is a software company in South Carolina which builds custom AWS-hosted software solutions which service the real estate industry.', '\xa0', 'Significant experience with large scale data sources, structures and processes', 'Remote:\xa0', 'A deep understanding of general ETL processing and tools as well as data warehousing concepts and workflows', 'Experience deploying infrastructure as code with tools such as AWS CloudFormation and Terraform', '\ufeffKeywords', 'Experience working with web data sources', 'Preferred Skills (not required):', '2-4 years of relevant work experienceA deep understanding of general ETL processing and tools as well as data warehousing concepts and workflowsStrong knowledge of SQL and PythonSignificant experience with large scale data sources, structures and processes', 'Experience with ETL orchestration tools to run large scale ETL workflows', '\ufeffKeywords: data, python, ETL, SQL, data warehouse, BI', 'Remote:\xa0100% remote role']",Mid-Senior level,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Data Engineer,Slalom,"San Diego, CA",2 weeks ago,56 applicants,"['', ' Strong aptitude for learning new technologies and analytics techniques', ' Familiarity with streaming data ingestion', ' Gain hands-on experience with new data platforms and programming languages (e.g. Python, Hive, Spark)', ' Proficient in a source code control system, such as Git', 'Slalom is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law.', ' Consulting experience', ' Familiarity with implementing analytics solutions with one or more Hadoop distributions (Cloudera, Hortonworks, MapR, HDInsight, EMR) Familiarity with streaming data ingestion Proficient in Python and/or Java Consulting experience Familiarity or strong desire to learn quantitative analysis techniques (e.g., predictive modeling, machine learning, segmentation, optimization, clustering, regression)', ' Proficient in SQL', ' Participate in development of cloud data warehouses and business intelligence solutions', ' Work as part of a team to develop Cloud Data and Analytics solutions Participate in development of cloud data warehouses and business intelligence solutions Data wrangling of heterogeneous data and explore and discover new insights Gain hands-on experience with new data platforms and programming languages (e.g. Python, Hive, Spark)', 'Job Title: Data Engineer', ' 3+ years of related work experience in Data Engineering or Data Warehousing Hands-on experience with leading commercial Cloud platforms, including AWS, Azure, and Google Proven experience with data warehousing, data ingestion, and data profiling Proficient in SQL Strong aptitude for learning new technologies and analytics techniques Highly self-motivated and able to work independently as well as in a team environment Understanding of agile project approaches and methodologies Proficient in a source code control system, such as Git Proficient in the Linux shell, including utilities such as SSH', ' Familiarity or strong desire to learn quantitative analysis techniques (e.g., predictive modeling, machine learning, segmentation, optimization, clustering, regression)', ' Proficient in Python and/or Java', ' 3+ years of related work experience in Data Engineering or Data Warehousing', ' Hands-on experience with leading commercial Cloud platforms, including AWS, Azure, and Google', ' Highly self-motivated and able to work independently as well as in a team environment', 'Responsibilities', ' Proven experience with data warehousing, data ingestion, and data profiling', ' Familiarity with implementing analytics solutions with one or more Hadoop distributions (Cloudera, Hortonworks, MapR, HDInsight, EMR)', 'Qualifications', ' Work as part of a team to develop Cloud Data and Analytics solutions', ' Understanding of agile project approaches and methodologies', ' Data wrangling of heterogeneous data and explore and discover new insights', 'Preferred Experience', ' Proficient in the Linux shell, including utilities such as SSH']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,PRI Global,United States,1 week ago,111 applicants,"['', '3+ years of experience in Computer Engineering, Software Development', 'Responsibilities:', 'Direct Client', 'Required Skills:', 'Data Engineer', 'Hands-on experience with ETL tools & automation', '100% Remote', 'Healthcare is Mandatory ', ""Bachelor's degree in Computer Science, Computer Engineering, Software Engineering, related field or equivalent experience"", 'SQL (Oracle, MSSQL, MySQL, etc) and NoSQL (Mongo)Scripting languages such as PythonStrong understanding of data modeling, algorithms, and data transformation techniques3+ years of experience in Computer Engineering, Software DevelopmentHands-on experience with ETL tools & automationHealthcare is Mandatory Ability to develop in multiple programming and scripting languages', 'SQL (Oracle, MSSQL, MySQL, etc) and NoSQL (Mongo)', 'Ability to develop in multiple programming and scripting languages', 'Education Requirement:', 'Scripting languages such as Python', 'Strong understanding of data modeling, algorithms, and data transformation techniques', '\xa0']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer ,Fluence,"Arlington, VA",3 weeks ago,158 applicants,"['', 'Engineer and support analytics and machine learning tooling.', 'o\xa0\xa0Standard industry data formats such as CSV, Parquet, Avro, etc.\xa0', 'Fluence, a Siemens and AES company, is the global market leader in energy storage technology solutions and services, combining the agility of a technology company with the expertise, vision and financial backing of two well-established and respected industry giants. Building on the pioneering work of AES Energy Storage and Siemens energy storage, our goal is to create a more sustainable future by transforming the way we power our world. Providing design, delivery and integration, Fluence offers proven energy storage technology solutions that address the diverse needs and challenges of customers in a rapidly transforming energy landscape. ', 'Leading ', 'In this role, you will engineer and support analytics and machine learning tooling. You’ll create data tools for the Fluence team, including business analysts and data scientists, that help them discover insights across our organization. You’ll work with data and analytics experts to create industry leading functionality.\xa0Create and optimize Fluence data pipelines. This involves assembling large, complex data sets that meet our customers’ diverse and evolving requirements. You’ll create and maintain the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of sources using SQL and other query languages. You’ll configure data collection and aggregation pipelines in both bare metal Linux and AWS environments and design procedures for maintaining these pipelines.\xa0Increase data visibility and automation. You’ll identify, design, and implement internal process improvements: enhance data access and visibility, automate manual processes, optimize data delivery and design for scalability.\xa0Engineer and support analytics and machine learning tooling. You’ll create data tools for the Fluence team, including business analysts and data scientists, that help them discover insights across our organization. You’ll work with data and analytics experts to create industry leading functionality.\xa0', 'Working on transforming a fundamental part of our society is exciting and fulfilling. It requires creativity, diversity of ideas and backgrounds, and building trust to effect change and move with speed. We respect our coworkers and customers. We listen to what others have to say, and we are inclusive.', 'In this role, you will engineer and support analytics and machine learning tooling. You’ll create data tools for the Fluence team, including business analysts and data scientists, that help them discover insights across our organization. You’ll work with data and analytics experts to create industry leading functionality.\xa0', 'ABOUT FLUENCE', 'o\xa0\xa0AWS cloud services such as EC2, S3, Lambda, and Fargate.\xa0', 'Knowledge needed manage and process large amounts of time series data, including real-time processing, analytics, machine learning data set creation, and retrieval systems for archived data.\xa0', 'Create and optimize Fluence data pipelines.', 'Fluence currently has more than 2.4 gigawatts of projects in operation or awarded across 24 countries and territories worldwide. We topped the Navigant Research utility-scale energy storage leaderboard in 2018 and were named one of Fast Company’s Most Innovative Companies in 2019. In 2020, our sixth-generation Tech Stack won Commercial Technology of the Year at the 22nd annual S&P Global Platts Global Energy Awards.', 'Belief that the work we do must always improve lives by lowering the cost of electricity, improving the resilience of the electric system, and enhancing grid sustainability.\xa0', 'Communication skills to explain recommendations from complex data analysis to teammates, colleagues, and leadership.\xa0', 'We are curious, adaptable, and self-critical; we use these traits to meet our team’s and customers’ needs. We always want to be working on the issues with the greatest impact.\xa0', 'o\xa0\xa0Relational and non-relational databases, including expertise in Postgres, MariaDB, Snowflake, or Presto/Athena.\xa0', 'Engineer and support analytics and machine learning tooling. You’ll create data tools for the Fluence team, including business analysts and data scientists, that help them discover insights across our organization. You’ll work with data and analytics experts to create industry leading functionality.\xa0', 'Succeeding in our mission requires creativity of thought, diversity of backgrounds, and an environment of safety and trust. These aspects of our culture enable us to effect change and move with speed. Workspace respect, balance, and inclusivity is foundational to our success, and our experiences at work are an essential part of what we create.\xa0', 'Cooperation skills to work collaboratively in a fast-paced entrepreneurial environment, while also taking ownership of responsibilities and pursuing them diligently.\xa0', 'Advanced working knowledge of SQL and experience working with relational databases, query authoring, optimization and familiarity with a variety of databases.\xa0Experience building and optimizing data pipelines, both with bare metal and cloud services.\xa0Knowledge needed manage and process large amounts of time series data, including real-time processing, analytics, machine learning data set creation, and retrieval systems for archived data.\xa0Awareness of modern clustering, containerization, services, and serverless architectures.\xa0Cooperation skills to work collaboratively in a fast-paced entrepreneurial environment, while also taking ownership of responsibilities and pursuing them diligently.\xa0Communication skills to explain recommendations from complex data analysis to teammates, colleagues, and leadership.\xa0Belief that the work we do must always improve lives by lowering the cost of electricity, improving the resilience of the electric system, and enhancing grid sustainability.\xa0Knowledge of basic Linux system administration concepts—working with the command line and operating system resource management.\xa0A degree in Computer Science, Information Systems or another quantitative field with relevant experience.\xa0You should also have experience with the following technology:\xa0', 'Fluence IS AN EQUAL OPPORTUNITY EMPLOYER and fully subscribes to the principles of Equal Employment Opportunity to ensure that all applicants and employees are considered for hire, promotion, and job status without regard to race, color, religion, sex, national origin, age, disability, veteran status, sexual orientation, marital or familial status.', 'Fluence is defined by its unwavering commitment to safety, quality, and integrity. We take personal ownership in what we do, and we prioritize the development of trust in our relationships. We firmly believe in having honest, forthcoming, respectful, and fair communications. \xa0We are curious, adaptable, and self-critical; we use these traits to meet our team’s and customers’ needs. We always want to be working on the issues with the greatest impact.\xa0Succeeding in our mission requires creativity of thought, diversity of backgrounds, and an environment of safety and trust. These aspects of our culture enable us to effect change and move with speed. Workspace respect, balance, and inclusivity is foundational to our success, and our experiences at work are an essential part of what we create.\xa0', 'Experience building and optimizing data pipelines, both with bare metal and cloud services.\xa0', 'GET IN TOUCH', 'Please send your resume and cover letter to careers@fluenceenergy.com.', 'Awareness of modern clustering, containerization, services, and serverless architectures.\xa0', 'A degree in Computer Science, Information Systems or another quantitative field with relevant experience.\xa0', 'Create and optimize Fluence data pipelines. This involves assembling large, complex data sets that meet our customers’ diverse and evolving requirements. You’ll create and maintain the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of sources using SQL and other query languages. You’ll configure data collection and aggregation pipelines in both bare metal Linux and AWS environments and design procedures for maintaining these pipelines.\xa0', 'Do others come to you for your subject matter expertise? Are you excited by the challenge of working in a start-up atmosphere with a purpose? ', 'Fun', 'Advanced working knowledge of SQL and experience working with relational databases, query authoring, optimization and familiarity with a variety of databases.\xa0', 'engineer and support analytics and machine learning tooling.', 'o\xa0\xa0Batched and real-time data stream-processing systems.\xa0', 'Location: Arlington, VA (preferred) or continental US', 'Increase data visibility and automation. You’ll identify, design, and implement internal process improvements: enhance data access and visibility, automate manual processes, optimize data delivery and design for scalability.\xa0', 'You should also have experience with the following technology:\xa0', 'Knowledge of basic Linux system administration concepts—working with the command line and operating system resource management.\xa0', 'Our Corporate Culture', 'Data Engineer', 'Here at Fluence, we strive to continuously improve, be intellectually curious and be adaptive to our customers and employee’s needs. Collaboration is key, both in our partnerships with our customers, and with each other. Fluence prioritizes the most critical efforts that allow for the greatest impact. ', 'As an ideal candidate you have:\xa0\xa0', 'Fluence is seeking a Data Engineer to help implement and monitor large-scale data systems that will help create more sustainable global energy systems.', 'Increase data visibility and automation.', 'Fluence is defined by its unwavering commitment to safety, quality, and integrity. We take personal ownership in what we do, and we prioritize the development of trust in our relationships. We firmly believe in having honest, forthcoming, respectful, and fair communications. \xa0', 'Responsible', 'o\xa0\xa0Automation and scripting using Python (required).\xa0', 'Agile']",Mid-Senior level,Full-time,Information Technology,Renewables & Environment,2021-03-24 13:05:10
Data Engineer II,Rivian,"Irvine, CA",1 day ago,Be among the first 25 applicants,"['', '2-5 years of hands-on IT experience in Data Warehouse, ETL & Reporting. Extensive experience of executing multiple data warehouse projects in variety of business domain', 'Design and develop secure, scalable, high-performance and reliable (cost effective) big data and analytics solutions that enable tableau dashboards and reports for cross-functional teams and executives.', 'Hands-on experience with Airflow, Alteryx and Tableau. (Certification is a plus)', 'Spread the data culture across the company by enabling best practices, standards, governed processes and relevant technologies in the Data platforms.', 'Role Summary', 'Establish technical partnerships with data scientists/engineers, and product owners/managers through brainstorming sessions, design reviews, and retrospectives.', 'Own the data management technical roadmap and execution aligning with peer platform organizations and product delivery, Ensure alignment with over-arching enterprise platform initiatives', ' Excellent communication and interpersonal skills, with the ability to break down complex technical problems into simple elegant solutions understandable by non-technical business partners. 2-5 years of hands-on IT experience in Data Warehouse, ETL & Reporting. Extensive experience of executing multiple data warehouse projects in variety of business domain 3+ years of experience in understanding variety of complex business use cases and modelling the data in the data warehouse. Very strong understanding of core data warehouse concepts. 3+ years of hands-on experience in productionizing and deploying Big Data platforms and applications, current experience working with: Relational/SQL, distributed columnar data stores/NoSQL databases. 3+ years of deep experience using S3, Python, SQL (Redshift) and Spark. Strong understanding of AWS Services like S3, Glue, Kinesis, Lambdas and Step functions. Strong experience in R or similar technologies with a background in data science a plus. Hands-on experience with Airflow, Alteryx and Tableau. (Certification is a plus) Familiarity with graph databases, timeseries databases, NiFi, Spark streaming, Kafka, Hive, Impala, Avro, and more is a plus. Strong understanding of Supply Chain and Financials business processes like Order Bookings, Invoicing/Billings, Forecasting, Demand Planning, Financial Planning. Purchasing, Payables, Manufacturing etc. ', 'Define templates and process for the analysis of data models, data flows, and integration patterns for structural deficiencies/soundness to build a robust and complete future state model', 'Lead by example to review code, look for code issues, provide meaningful and relevant feedback to engineers, stay up to date with system changes and latest technologies', 'Comfortably present and acquire consensus using multiple methods: Written diagrams and text, in-person meetings, in-person and remote presentations to both technical and business audiences', 'Excellent communication and interpersonal skills, with the ability to break down complex technical problems into simple elegant solutions understandable by non-technical business partners.', 'Strong understanding of Supply Chain and Financials business processes like Order Bookings, Invoicing/Billings, Forecasting, Demand Planning, Financial Planning. Purchasing, Payables, Manufacturing etc.', 'Responsibilities', ' Design and develop secure, scalable, high-performance and reliable (cost effective) big data and analytics solutions that enable tableau dashboards and reports for cross-functional teams and executives. Use Data & Analytics to answer business questions that leady to insights and actionable outcomes. Understand and deploy processes for: SDC, CI/CD, quality checks, error handling, error notifications, security, extensibility and maintainability of big data and analytics platform software and services Define templates and process for the analysis of data models, data flows, and integration patterns for structural deficiencies/soundness to build a robust and complete future state model Lead by example to review code, look for code issues, provide meaningful and relevant feedback to engineers, stay up to date with system changes and latest technologies Comfortably present and acquire consensus using multiple methods: Written diagrams and text, in-person meetings, in-person and remote presentations to both technical and business audiences Establish technical partnerships with data scientists/engineers, and product owners/managers through brainstorming sessions, design reviews, and retrospectives. Own the data management technical roadmap and execution aligning with peer platform organizations and product delivery, Ensure alignment with over-arching enterprise platform initiatives Spread the data culture across the company by enabling best practices, standards, governed processes and relevant technologies in the Data platforms. ', 'Qualifications', 'Strong understanding of AWS Services like S3, Glue, Kinesis, Lambdas and Step functions.', '3+ years of hands-on experience in productionizing and deploying Big Data platforms and applications, current experience working with: Relational/SQL, distributed columnar data stores/NoSQL databases.', 'Use Data & Analytics to answer business questions that leady to insights and actionable outcomes.', '3+ years of experience in understanding variety of complex business use cases and modelling the data in the data warehouse. Very strong understanding of core data warehouse concepts.', '3+ years of deep experience using S3, Python, SQL (Redshift) and Spark.', 'Familiarity with graph databases, timeseries databases, NiFi, Spark streaming, Kafka, Hive, Impala, Avro, and more is a plus.', 'Understand and deploy processes for: SDC, CI/CD, quality checks, error handling, error notifications, security, extensibility and maintainability of big data and analytics platform software and services', 'Strong experience in R or similar technologies with a background in data science a plus.']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,East West Bank,"Pasadena, CA",4 weeks ago,Over 200 applicants,"['', '·\xa0\xa0\xa0\xa0\xa0\xa0Self-driven to identify areas of improvement.', '·\xa0\xa0\xa0\xa0\xa0\xa0Define new data collection and analysis processes', '·\xa0\xa0\xa0\xa0\xa0\xa0Product knowledge in ', '·\xa0\xa0\xa0\xa0\xa0\xa0Interpret and analyze results from data extractions to identifying patterns and trends', '·\xa0\xa0\xa0\xa0\xa0\xa0Proficiency with Excel.', '·\xa0\xa0\xa0\xa0\xa0\xa0Maintain an understanding of business operations, operational risks and regulatory requirements ', '·\xa0\xa0\xa0\xa0\xa0\xa0Can-do attitude, self-motivated and strong work ethic.', '·\xa0\xa0\xa0\xa0\xa0\xa0Able to work under pressure while managing competing demands and tight deadlines.', '·\xa0\xa0\xa0\xa0\xa0\xa0Strong computing skills with at least one of the following: C#, Java, Python, R, T-SQL with a strong interest to learn.', '·\xa0\xa0\xa0\xa0\xa0\xa0Extract and collect large data sets from various sources and formats', '·\xa0\xa0\xa0\xa0\xa0\xa0Well organized with meticulous attention to detail.', '·\xa0\xa0\xa0\xa0\xa0\xa0Test adherence with Bank’s policies and controls, as well as regulatory requirements', '·\xa0\xa0\xa0\xa0\xa0\xa0Must be team-oriented with experience working on interdepartmental team projects.', '·\xa0\xa0\xa0\xa0\xa0\xa0Anticipate changes in the internal and external environment and adapt the testing program accordingly ', '·\xa0\xa0\xa0\xa0\xa0\xa0Bachelor’s degree required in Computer Science, Information Technology, Management Information Systems or Business Management.', '·\xa0\xa0\xa0\xa0\xa0\xa0Quantitative, analytical, process oriented and troubleshooting skills', '·\xa0\xa0\xa0\xa0\xa0\xa0Collect and document business requirements.\xa0Maintain functional and technical artifacts including design documents, data mappings, architecture, data models, and dictionaries.', '·\xa0\xa0\xa0\xa0\xa0\xa0Build high-performance algorithms, predictive models, and prototypes.', '·\xa0\xa0\xa0\xa0\xa0\xa0Assuring the integrity of data, including data extraction, storage, manipulation, processing and analysis', '·\xa0\xa0\xa0\xa0\xa0\xa0Test and validate that key assumptions, data sources, and procedures utilized in measuring and monitoring risk and internal controls can be relied upon on an ongoing basis; and, in the case of transaction testing, to assess that controls are working as intended.', 'Responsibilities', 'o\xa0\xa0Loans', '·\xa0\xa0\xa0\xa0\xa0\xa0Analytical and problem solving skills including troubleshooting.', 'o\xa0\xa0Deposits', 'Qualifications', '·\xa0\xa0\xa0\xa0\xa0\xa0Create reports, processes and tools to monitor key risk indicators for business units across the organization.', 'East West Bank is seeking a Data Engineer.\xa0The Data Engineer works with banking data and business units to propose, design, and deliver business solutions. The data engineer will collect and analyze data wherever it resides, including in spreadsheets, files, databases, and APIs. The data engineer will create reports, dashboards, and other visualizations and they will develop applications that collect data from users to enrich enterprise data sets to improve their completeness and accuracy.\xa0\xa0', 'o\xa0\xa0Banking operations', '·\xa0\xa0\xa0\xa0\xa0\xa0Design, implement, deploy and maintain Data solutions.\xa0These solutions are written in T-SQL, Python, C#, Java and R.', '·\xa0\xa0\xa0\xa0\xa0\xa0Master’s degree in Mathematics, Statistics, Computer Science, Data Science or relevant.', '·\xa0\xa0\xa0\xa0\xa0\xa0Report results back to management and relevant team members', '·\xa0\xa0\xa0\xa0\xa0\xa0Provides recommendations to streamline tasks and create a more efficient working environment', '·\xa0\xa0\xa0\xa0\xa0\xa0Build reports in SSRS, Power BI, Tableau, Excel and other visualization tools.', '·\xa0\xa0\xa0\xa0\xa0\xa0Evaluate internal controls and identify deficiencies through the testing of data source, systems, and processes', '·\xa0\xa0\xa0\xa0\xa0\xa0Translate, cleanse and normalize large datasets.', '·\xa0\xa0\xa0\xa0\xa0\xa0Collaborate with team members and business units to Deliver Data-Oriented solutions to the enterprise.']",Associate,Full-time,Information Technology,Banking,2021-03-24 13:05:10
Data Engineer,Idaho Central Credit Union,"Chubbuck, ID",2 weeks ago,91 applicants,"['', 'Ability to maintain confidentiality of Credit Union and member records at all times.', 'The Data Engineer is responsible for architecting, managing, optimizing, overseeing and monitoring data retrieval, storage and distribution. This role is responsible for the technical aspects and enhancements of the data integration tools, enterprise data warehouse, data lake, and enterprise reporting tools.', 'Does success motivate you to want to do more and be better?', 'The above statements reflect the general details considered necessary to describe the essential functions of the job and should not be construed as a detailed description of all the work requirements that may be inherent of the job.', 'Other duties as assigned.', 'Do you enjoy working with other people and finding solutions when everyone else only see problems?', '2+ years’ experience working in cloud computing with Azure experience required', 'Source-code management tools such as GitHub', 'Work closely with team members and BI Analysts to turn data into critical information and knowledge that can be used to make sound business decisions.', 'Ability to work with other department supervisors.', 'SQL development', 'Advocate of CI/CD methodologies and agile ways of working.', 'Must be eligible for membership at Idaho Central Credit Union to obtain employment.', 'Build conceptual and logical data models.', 'Ability to work with and communicate with all Credit Union personnel in the various departments.', 'Design and development focus include but is not limited to dimensional data model design, ETL/ELT frameworks and processing, meta-data management, operational data integration, master data management and data quality, data auditing and profiling, and business intelligence reporting solutions.', 'EOE/Minorities/Females/Vet/Disability', 'Responsible for the full life cycle development, implementation, support, architecture and tuning of the Enterprise Data Warehouse, including Data Marts, Data Lake and data pipelines.', 'Support the integration of enterprise application databases and real time processing into the data warehouse.', 'Microsoft SQL Server, Oracle, and other relational and NoSQL databases.SQL developmentExperience with business intelligence visualization tools and business intelligence tools, such as, Microsoft Power BIData warehousing, Data Lake, data modeling, data pipelinesELT/ETL development.Knowing programming languages such as Java, R, Python is a plus but is not required.Source-code management tools such as GitHub2+ years’ experience working in cloud computing with Azure experience requiredKnowledge of data processing, hardware platforms, and enterprise software Technical experience with enterprise systems, databases, and user support.Advocate of CI/CD methodologies and agile ways of working.Ability to maintain confidentiality of Credit Union and member records at all times.Self-motivated with the ability to prioritize, meet deadlines ,and manage changingWillingness to work occasionally outside of normal business hours.Excellent English oral and written communication skills.Ability to work with other department supervisors.Strong analytical and problem-solving Ability to maintain confidentiality of Credit Union and member records at all times.Ability to work with and communicate with all Credit Union personnel in the various departments.', 'Can you handle multiple projects at the same time and smile? ', 'Experience with business intelligence visualization tools and business intelligence tools, such as, Microsoft Power BI', 'Identify, design, and implement internal process improvements including automating manual processes, optimize data delivery and designing cloud infrastructure for greater scalability, etc..', 'Ensure all data sources are accurate, congruent, reliable, and secure.', 'Technical skill-sets needed include but not limited to SQL, ELT/ETL development, configuring and maintaining data warehousing solutions, data modeling, data lake technologies, data architecture, and BI reporting development.', 'Responsible for architecting, configuring, analyzing, and maintaining all data structures used for BI analytics and reporting.', 'Perform tasks requiring manual dexterity (processing paperwork, filing, stapling, sorting, collating, typing, counting cash, etc.). Sit for extended periods of time. Lift 10-20 pounds of applicable supplies including but not limited to copy paper, cash drawers, marketing material, etc. Repetitive motion using wrists, hands, and fingers. Reach keyboards. Ability to operate basic office machines (calculator, computer, telephone, copy machine, fax machine, etc.).', 'Technologies involved but not limited to Azure Data Lake, Azure Data Factory, Azure Data-bricks, Azure Synapse, SQL Server, Power BI, Oracle database, AI/ML', 'Idaho Central Credit Union is a dynamic financial institution that is focused on helping our members achieve financial success. Established in 1940 we have become the largest and fastest growing financial institution in the state of Idaho. ICCU was voted for large companies the Best Place to work in Idaho. We have also been named by S&P Global Market Intelligence as the top performing credit union in the nation. We are a talent based organization looking for talented individuals to help our members achieve financial success.', 'Write technical policies, procedures, and documentation for the systems including data flow diagrams, architecture diagrams, etc.', 'Contribute to a team responsible for the design, development and implementation of mission critical business intelligence reports and applications to enterprise scale.', 'Work closely with team members and BI Analysts to turn data into critical information and knowledge that can be used to make sound business decisions.Responsible for architecting, configuring, analyzing, and maintaining all data structures used for BI analytics and reporting.Ensure all data sources are accurate, congruent, reliable, and secure.Responsible for the full life cycle development, implementation, support, architecture and tuning of the Enterprise Data Warehouse, including Data Marts, Data Lake and data pipelines.Define and build data integration processes to be used across the organization.Build conceptual and logical data models.Identify, design, and implement internal process improvements including automating manual processes, optimize data delivery and designing cloud infrastructure for greater scalability, etc..Ensure all data accesses, along with data at rest or in transit is secure and follows the best in class data governance standards.Support the integration of enterprise application databases and real time processing into the data warehouse.Technical skill-sets needed include but not limited to SQL, ELT/ETL development, configuring and maintaining data warehousing solutions, data modeling, data lake technologies, data architecture, and BI reporting development.Technologies involved but not limited to Azure Data Lake, Azure Data Factory, Azure Data-bricks, Azure Synapse, SQL Server, Power BI, Oracle database, AI/MLThis person will work closely with external professional services and third-party vendors to have a full knowledge of systems and help establish road-map for future enhancements.Write technical policies, procedures, and documentation for the systems including data flow diagrams, architecture diagrams, etc.Contribute to a team responsible for the design, development and implementation of mission critical business intelligence reports and applications to enterprise scale.Design and development focus include but is not limited to dimensional data model design, ETL/ELT frameworks and processing, meta-data management, operational data integration, master data management and data quality, data auditing and profiling, and business intelligence reporting solutions.Other duties as assigned.', 'Microsoft SQL Server, Oracle, and other relational and NoSQL databases.', 'Excellent English oral and written communication skills.', 'Do you take pride in the work you accomplish?', 'Self-motivated with the ability to prioritize, meet deadlines ,and manage changing', 'Ensure all data accesses, along with data at rest or in transit is secure and follows the best in class data governance standards.', 'Knowledge of data processing, hardware platforms, and enterprise software Technical experience with enterprise systems, databases, and user support.', 'Willingness to work occasionally outside of normal business hours.', 'This person will work closely with external professional services and third-party vendors to have a full knowledge of systems and help establish road-map for future enhancements.', 'Data warehousing, Data Lake, data modeling, data pipelines', 'ELT/ETL development.', 'Bachelor’s degree in computer science or equivalent degree is required. Five plus years of experience in a Data Engineering or similar role. Certifications in data analytics and/or data engineering a plus. A demonstrated cooperative and positive attitude toward members and other Credit Union staff. Ability to determine member needs and cross sell Credit Union services. Professional in appearance, attendance, quality, and quantity of work performed. Ability to work under pressure and conflicting situations. Must be willing to comply with the Bank Secrecy Act and USA Patriot Act as implemented by Idaho Central Credit Union.', 'Experience In', '(Keywords Bank, Banking, Finance, Information Technology, Workstations)', 'Knowing programming languages such as Java, R, Python is a plus but is not required.', 'Do you enjoy working with other people and finding solutions when everyone else only see problems?Are you compelled to initiate action and remain proactive in getting things done?Do you take pride in the work you accomplish?Does success motivate you to want to do more and be better?Can you handle multiple projects at the same time and smile? ', 'Strong analytical and problem-solving Ability to maintain confidentiality of Credit Union and member records at all times.', 'If you can answer yes to these questions Idaho Central Credit Union is ready for you to join our team!', 'Define and build data integration processes to be used across the organization.', 'Are you compelled to initiate action and remain proactive in getting things done?']",Entry level,Full-time,Information Technology,Banking,2021-03-24 13:05:10
Data Engineer I,Thermo Fisher Scientific,"Frederick, MD",6 days ago,Be among the first 25 applicants,"['', 'Ability to interact professionally with a diverse group including VPs, directors, managers, subject matter experts and end-users.', 'Good scripting and programming skills', 'Key Responsibilities', 'Develop/Maintain and Administer PowerBI dashboards, gateways and associated infrastructure', 'Excellent interpersonal and communication skills (both verbal and written).', 'BA/BS degree in finance, mathematics, computer science preferred or equivalent work experience ', 'Strong organizational and communication skills, and proven ability to adapt style to different situations and peopleMust be a business partner, not merely a technical expert – this position plays an active role providing actionable insight into the Clinical Trials DivisionStrong analytical skills – and ability to use those skills to influence and drive changeExcellent interpersonal and communication skills (both verbal and written).Ability to interact professionally with a diverse group including VPs, directors, managers, subject matter experts and end-users.Self-motivated; bias for actionGlobal experience10% travel requirement', 'Experience with common data science toolkits', 'Minimum Requirements/Qualifications', 'Extending company’s data with third party sources of information when needed', 'Direct interaction with the business to understand and analyze business problems, derive insights and recommend solutions', '2-4 years of data engineering experience', 'Direct interaction with the business to understand and analyze business problems, derive insights and recommend solutionsWorking independently, or with functional analysts, to create reports, data sets and mechanisms to provide visibility to business dataExecuting ad-hoc analysis and presenting results in a clear mannerExtending company’s data with third party sources of information when neededCollaborate with IT and business partners to ensure data quality, integrity, and accuracy across the global CTD data storesEstablish good working relationships with peers in other divisions and explore joint system and process improvement opportunities.Maintain MS SQL Server and Oracle database environments (security, tables, views, packages, SQL Agent jobs, SSAS database, Integration Services)Develop/Maintain and Administer PowerBI dashboards, gateways and associated infrastructure', 'Working independently, or with functional analysts, to create reports, data sets and mechanisms to provide visibility to business data', 'Data-oriented personality', 'Proficiency in using query languages such as SQL', 'Self-motivated; bias for action', 'Great communication skills', 'Strong organizational and communication skills, and proven ability to adapt style to different situations and people', 'Maintain MS SQL Server and Oracle database environments (security, tables, views, packages, SQL Agent jobs, SSAS database, Integration Services)', 'Experience with NoSQL databases', 'Experience with data visualization tools, such as PowerBI, Tableau or Cognos', 'Strong analytical skills – and ability to use those skills to influence and drive change', 'Key Success Factors', 'Ability to partner with management at all levels and to lead major projects and initiatives', 'Good applied statistics skills, such as distributions, statistical testing, regression, etc.', 'Executing ad-hoc analysis and presenting results in a clear manner', 'Collaborate with IT and business partners to ensure data quality, integrity, and accuracy across the global CTD data stores', 'Establish good working relationships with peers in other divisions and explore joint system and process improvement opportunities.', '2-4 years of data engineering experienceData-oriented personalityGreat communication skillsExperience with data visualization tools, such as PowerBI, Tableau or CognosProficiency in using query languages such as SQLExperience with NoSQL databasesGood applied statistics skills, such as distributions, statistical testing, regression, etc.Experience with common data science toolkitsGood scripting and programming skillsAbility to partner with management at all levels and to lead major projects and initiativesStrong communication skills and ability to work effectively across a matrix organizationBA/BS degree in finance, mathematics, computer science preferred or equivalent work experience ', '10% travel requirement', 'Strong communication skills and ability to work effectively across a matrix organization', 'Must be a business partner, not merely a technical expert – this position plays an active role providing actionable insight into the Clinical Trials Division', 'Global experience', 'Position Summary']",Not Applicable,Full-time,Other,Pharmaceuticals,2021-03-24 13:05:10
Data Engineer,Envision,Greater St. Louis,1 week ago,129 applicants,"['', '• Bioinformatics experience, especially large scale storage and data mining of variant data, variant annotation, and genotype to phenotype correlation', 'Data Engineer, (remote *)', 'Required experience:', 'Bonus points for:', 'You may apply on LinkedInor on our web site: http://www.envision.com/jobs/index.html#/jobs/71767', 'You must be able to show where you worked remotely and outline the tools you used to work remotely.', '• A proven ability to build and maintain cloud based infrastructure on a major cloud provider like AWS, Azure or Google Cloud Platform', '• Experience data modeling for large scale databases, either relational or NoSQL', '• Proven experience (2 years) building and maintaining data-intensive APIs using a RESTful approach', '• Experience working with scientific datasets, or a background in the application of quantitative science to business problems', '*Remote: For exceptional candidates who can show previous remote work on their resume, we will allow remote. You must be able to show where you worked remotely and outline the tools you used to work remotely.*', '• Familiarity with creating and maintaining containerized application deployments with a platform like Docker', '• Experience with protocol buffers and gRPC', 'No C2C, must be our\xa0W2 employee', '• Experience with stream processing using Apache Kafka', '• At least 2 years experience with Go', '• A level of comfort with Unit Testing and Test Driven Development methodologies', '• Experience with: Google Cloud Platform, Apache Beam and or Google Cloud Dataflow, Google Kubernetes Engine or Kubernetes']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,CitiusTech,"New York, NY",1 week ago,51 applicants,"['', ' Perform analysis of vast data stores and uncover insights.', 'NYC, NY', 'Overview:', ' Loading from disparate data sets and reconciliation', 'Mandatory Skills:', 'Long Term!', 'Job Description:', ' High-speed querying.', 'Thanks,', ' Performance and code optimization', ' Strong visual and verbal communication skills', ' Work in a fast-paced, creative atmosphere to develop new ideas that adapt to evolving user needs', "" Create salable and high-performance API's for data tracking/data querying"", 'Ganesh.nadar@citiustech.com', ' Knowledge of ETL concepts and Shell – Dos, Bash scripting is good to have', ' Maintain security and data privacy.', '617-795-3066', ' Pre-processing using Python', ' Strong experience on Python, Spark and SQL is a MUST. PySpark experience will be an added advantage', 'Data Engineer', 'Ganesh Nadar']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Yum! Brands,"Irvine, CA",4 weeks ago,41 applicants,"['', ' Strong understanding of ETL processing with large data stores using AWS Data Lake Formation and AWS Glue', ' Design and code data pipeline features and data processing jobs for collecting various data from customer, operations and financials systems Build out a robust big data ingestion framework with automation, self-heal capabilities and ability to handle data drifts Work with real-time data streams & API’s from multiple internal/external sources Write ETL pipelines to implement pre-defined business rules and metrics Build visual data quality metrics in the data warehouse load processes Provide scalable solutions to manage large file imports', ' Work with real-time data streams & API’s from multiple internal/external sources', ' Familiarity with AWS Data and Analytics technologies such as Athena, Redshift Spectrum', ' Experience in ETL and ELT workflow management', ' Discounts for life’s adventures (ex: theme parks, wireless plans, etc.)', ' Experience integrating data using streaming technologies such as Kinesis Firehose, Kafka', ' 4+ years of experience designing and delivering large scale, 24-7, mission-critical data pipelines and features using modern big data architectures 4+ years of hands-on experience in ETL tools such as Talend & Python and SQL. 3+ years of experience working in Redshift and AWS Lambda. Strong understanding of ETL processing with large data stores using AWS Data Lake Formation and AWS Glue Strong experience in data quality tools such as Informatica DQ or Talend DQ tools Experience in ETL and ELT workflow management Familiarity with AWS Data and Analytics technologies such as Athena, Redshift Spectrum Intermediate knowledge of BI best practices, relational structures, dimensional data modeling, structured query language (SQL) skills, data warehouse and reporting techniques BA/BS in related field, or equivalent experience Experience integrating data from multiple data sources and file types such as JSON, Parquet and Avro formats. Experience integrating data using API integration from Salesforce Experience integrating data using streaming technologies such as Kinesis Firehose, Kafka Knowledge working with git or any other version control system. Experience working in Data Ops environment', ' Company paid life insurance', ' BA/BS in related field, or equivalent experience', ' Experience integrating data using API integration from Salesforce', 'Check Out Some Of Our Great Benefits', ' 3+ years of experience working in Redshift and AWS Lambda.', ' Grow Yourself Week which is devoted to your personal development', ' Intermediate knowledge of BI best practices, relational structures, dimensional data modeling, structured query language (SQL) skills, data warehouse and reporting techniques', ' Experience integrating data from multiple data sources and file types such as JSON, Parquet and Avro formats.', ' Onsite childcare through Bright Horizons', ' Healthcare and dependent care flexible spending accounts', ' 4 weeks of vacation per year plus holidays 2 paid days off per year to volunteer Onsite childcare through Bright Horizons Onsite dining center (yes, you can eat KFC, Taco Bell or Pizza hut every day!) Onsite dry cleaning, laundry services, concierge Onsite gym with fitness classes and personal trainer sessions Tuition reimbursement, education benefits and scholarship opportunities Discounts for life’s adventures (ex: theme parks, wireless plans, etc.) Generous parental leave for all new parents and adoption assistance program 401(k) with a 6% matching contribution from Yum! Brands with immediate vesting Comprehensive medical, vision and dental including prescription drug benefits and 100% preventive care Recognition based culture and unique, fun events year round Healthcare and dependent care flexible spending accounts Company paid life insurance Grow Yourself Week which is devoted to your personal development', ' Comprehensive medical, vision and dental including prescription drug benefits and 100% preventive care', ' Onsite dining center (yes, you can eat KFC, Taco Bell or Pizza hut every day!)', ' 4 weeks of vacation per year plus holidays', ' Build visual data quality metrics in the data warehouse load processes', ' Recognition based culture and unique, fun events year round', ' 401(k) with a 6% matching contribution from Yum! Brands with immediate vesting', ' Tuition reimbursement, education benefits and scholarship opportunities', ' Provide scalable solutions to manage large file imports', ' 4+ years of hands-on experience in ETL tools such as Talend & Python and SQL.', ' Onsite dry cleaning, laundry services, concierge', ' Design and code data pipeline features and data processing jobs for collecting various data from customer, operations and financials systems', ' Experience working in Data Ops environment', ' Generous parental leave for all new parents and adoption assistance program', ' Write ETL pipelines to implement pre-defined business rules and metrics', ' Strong experience in data quality tools such as Informatica DQ or Talend DQ tools', ' 4+ years of experience designing and delivering large scale, 24-7, mission-critical data pipelines and features using modern big data architectures', ' Onsite gym with fitness classes and personal trainer sessions', ' Knowledge working with git or any other version control system.', ' Build out a robust big data ingestion framework with automation, self-heal capabilities and ability to handle data drifts', ' 2 paid days off per year to volunteer', 'Required Skills']",Not Applicable,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,eNGINE ,"Pittsburgh, PA",1 week ago,83 applicants,"['', 'This person’s responsibilities will include designing and implementing data solutions for database-driven applications and services.', 'eNGINE builds Technical Teams. We are a Solutions and Placement firm shaped by decades of interaction with Technical professionals. Our inspiration is continuous learning and engagement with the markets we serve, the talent we represent, and the teams we build. Our Consulting Workforce is encouraged to enjoy career fulfillment in the form of challenging projects, schedule flexibility, and paid training/certifications. Successful outcomes start and finish with eNGINE.', 'Candidates LOCAL TO PITTSBURGH ONLY! NO C2C', 'eNGINE is hiring Data Engineer to support our Pittsburgh based client. This person must have professional experience with PL/SQL programming,\xa0developing scripts, and ETL development. Experience with Oracle technologies including and Cloud-based data warehouse technologies is a plus.', '\xa0']",Not Applicable,Contract,Information Technology,Staffing and Recruiting,2021-03-24 13:05:10
Data Engineer,DATAECONOMY,"Charlotte, NC",,N/A,"['', 'Leverages DevOps techniques and Experience with DevOps tools - GitHub, Jira, Jenkins, Crucible for Continuous Integration, Continuous Deployment and build automation.', 'Job Responsibilities', 'Proven track record of customer satisfaction and delivery success and ability to establish and maintain appropriate relationships with business and IT stakeholders', 'Passionate for continuous learning, experimenting, applying and contributing towards cutting edge open source technologies and software paradigms', 'Mode of Employment: Full-time', 'Ability to work independently and drive solutions end to end leveraging various technologies to solve data problems and develop solutions.', 'Ability to work in an advisory capacity to identify key technical business problems, develop and evaluate alternative solutions and make recommendations', 'Ability to research and assess open source technologies and components to recommend and integrate into the design and implementation', 'Experience developing pipelines for both Cloud and Hybrid Cloud infrastructures.', 'Perform unit tests and conduct reviews with other team members to ensure code is designed with high code coverage', 'Knowledge of NoSQL, RDBMS, SQL, JSON, XML and ETL skills are must.Understanding of data transformations, cleansing, and deduplications.Advanced knowledge of SQL (PSQL or TSQL).Experience developing pipelines for both Cloud and Hybrid Cloud infrastructures.Experience in AWS utilizing services such as S3, AWS CLI, and RDS.Experience using modern ETL tools like Talend and Nifi.Experience implementing Data Warehouse in SnowflakeExperience working in an Agile delivery environmentAbility to work independently and drive solutions end to end leveraging various technologies to solve data problems and develop solutions.Perform unit tests and conduct reviews with other team members to ensure code is designed with high code coveragePassionate for continuous learning, experimenting, applying and contributing towards cutting edge open source technologies and software paradigmsAbility to research and assess open source technologies and components to recommend and integrate into the design and implementationProven track record of customer satisfaction and delivery success and ability to establish and maintain appropriate relationships with business and IT stakeholdersAbility to work in an advisory capacity to identify key technical business problems, develop and evaluate alternative solutions and make recommendationsExtensive experience in all aspects of the software development life cycle', 'Location: Charlotte, North Carolina, United States', 'Design, build and maintain Big Data workflows/pipelines to process billions of records in large-scale data environments with experience in end-to-end design and build process of Near-Real Time and Batch Data Pipelines.', 'Leads code review sessions to validate adherence with development standards and benchmark application performance by capacity testing.', 'DATA ENGINEER', '\xa0', 'Experience with software testing frameworks.', 'Experience using modern ETL tools like Talend and Nifi.', 'Engage in application design and data modeling discussions also participate in developing and enforcing data security policies', 'Understanding of data transformations, cleansing, and deduplications.', 'Develop, implement and optimize streaming, data lake, and big data analytics solutions', 'Qualifications', ""Urgent need for DATA ENGINEER for a join on our client's project."", 'Experience implementing Data Warehouse in Snowflake', '\xa0Job Responsibilities', 'Support reusable framework and data governance processes by partnering with LOBs for any code/requirements remediation', 'Advanced knowledge of SQL (PSQL or TSQL).', 'Extensive experience in all aspects of the software development life cycle', 'Knowledge of NoSQL, RDBMS, SQL, JSON, XML and ETL skills are must.', 'Experience working in an Agile delivery environment', 'Please Note: Must be US Citizen /GC/ H4 EAD / H1B Transfer Candidates', 'Design, build and maintain Big Data workflows/pipelines to process billions of records in large-scale data environments with experience in end-to-end design and build process of Near-Real Time and Batch Data Pipelines.Leads code review sessions to validate adherence with development standards and benchmark application performance by capacity testing.Experience with software testing frameworks.Leverages DevOps techniques and Experience with DevOps tools - GitHub, Jira, Jenkins, Crucible for Continuous Integration, Continuous Deployment and build automation.Develop, implement and optimize streaming, data lake, and big data analytics solutionsSupport reusable framework and data governance processes by partnering with LOBs for any code/requirements remediationEngage in application design and data modeling discussions also participate in developing and enforcing data security policies', 'If you are interested, please share your resume to jason@dataeconomy.io or call 614-734-1434', 'Experience in AWS utilizing services such as S3, AWS CLI, and RDS.', 'Hourly Billing Rate: Open - Must be competitive']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer - PySpark - AWS,Fannie Mae,"Herndon, VA",2 days ago,58 applicants,"['', 'Serve as a technical lead for an Agile team and actively participate in all Agile ceremonies. Participate in all team ceremonies including planning, grooming, product demonstration and team retrospectives.', '2+ years of development experience using Pyspark', 'Create or Update documentation in support of development efforts. Documents may include detailed specifications, implementation guides, architecture diagrams or design documents.', 'Participate in code reviews with peers and managers to ensure that each increment adheres to original vision as described in the user story and all standard resource libraries and architecture patterns as appropriate.', 'Preferred Qualifications', 'Advanced proficiency in Object Oriented Design (OOD) and analysis. Advanced proficiency in application of analysis/design engineering functions. Advanced proficiency in application of non-functional software qualities such as resiliency, maintainability, etc. Advanced proficiency in advanced behavior-driven testing techniques.', 'Leverage Fannie Mae DevOps tool stack to build, inspect, deploy, test and promote new or updated features.', 'Hands-on experience in implementation and development using cloud technologies (AWS)', 'Mentor less experienced technical staff; may use high end development tools to assist or facilitate development process.', 'Bachelor’s Degree or Equivalent Experience (required)', 'Advanced proficiency in unit testing as well as coding in 1-2 languages (e.g. Java, etc).', 'Set up and configure a continuous integration environment.', 'Experience with Agile Development Methodology', 'Work with product owners and other development team members to determine new features and user stories needed in new/revised applications or large/complex development projects.', 'Experience with application integrations such as RESTful Web Services, and File/Data transfers, etc.', 'THE IMPACT YOU WILL MAKE', 'Experience with CI/CD with knowledge of Git Hub, Maven and Jenkins', 'Responsibilities', 'Qualifications', 'Bachelor’s Degree or Equivalent Experience (required)4-6 years of related experience2+ years of development experience using Pyspark', 'Apache Spark experienceETL experience Pyspark, AWS GlueHands-on experience in implementation and development using cloud technologies (AWS)Knowledge of UNIX (Linux) environment, scripting (bash, shell) and AutosysExperience with application integrations such as RESTful Web Services, and File/Data transfers, etc.Experience with CI/CD with knowledge of Git Hub, Maven and JenkinsExperience with Agile Development Methodology', '4-6 years of related experience', 'Company Description', 'Knowledge of UNIX (Linux) environment, scripting (bash, shell) and Autosys', 'ETL experience Pyspark, AWS Glue', 'Basic Qualifications', 'Work with product owners and other development team members to determine new features and user stories needed in new/revised applications or large/complex development projects.Create or Update documentation in support of development efforts. Documents may include detailed specifications, implementation guides, architecture diagrams or design documents.Participate in code reviews with peers and managers to ensure that each increment adheres to original vision as described in the user story and all standard resource libraries and architecture patterns as appropriate.Respond to trouble/support calls for applications in production in order to make quick repair to keep application in production.Serve as a technical lead for an Agile team and actively participate in all Agile ceremonies. Participate in all team ceremonies including planning, grooming, product demonstration and team retrospectives.Mentor less experienced technical staff; may use high end development tools to assist or facilitate development process.Leverage Fannie Mae DevOps tool stack to build, inspect, deploy, test and promote new or updated features.Set up and configure a continuous integration environment.Advanced proficiency in unit testing as well as coding in 1-2 languages (e.g. Java, etc).Advanced proficiency in Object Oriented Design (OOD) and analysis. Advanced proficiency in application of analysis/design engineering functions. Advanced proficiency in application of non-functional software qualities such as resiliency, maintainability, etc. Advanced proficiency in advanced behavior-driven testing techniques.', 'Apache Spark experience', 'Respond to trouble/support calls for applications in production in order to make quick repair to keep application in production.', 'Job Description', 'THE EXPERIENCE YOU BRING TO THE TEAM']",Entry level,Full-time,Information Technology,Financial Services,2021-03-24 13:05:10
Data Engineer,Apex Systems,"Florida, United States",,N/A,"['4. ETL Development', '5. Expert in\xa0SQL\xa03+ years', '3. Power BI', '1.\xa0ADF\xa0and\xa0Azure\xa0Data\xa0Lake\xa0Experience 1-2 years here', 'What are the top MUST HAVES:', '2. OK with coming onsite']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer (Philadelphia),"J Harlan Group, LLC","Philadelphia, PA",2 days ago,Be among the first 25 applicants,"['', 'Data Engineer (Philadelphia)', 'Practical familiarity with microservices architecture concepts, including API gateways, security, scaling and resiliency, and operational monitoring', 'Designing clean, efficient, robust and reliable solutions that deliver on requirementsAdvising on data centric designs, that are secure, stable and scalableStrong understanding and practical application of the most appropriate AWS tooling to utlise and support data movementDeveloping, maintaining and continuously improving DevOps methods and practicesProviding guidance on quality engineering which includes test class and automation frameworks.', 'Development experience in a large organization within a complex technology landscape, that includes AWS experience', 'Advising on data centric designs, that are secure, stable and scalable', 'Strong understanding and practical application of the most appropriate AWS tooling to utlise and support data movement', 'Hands on experience authoring and configuring AWS data pipelines and building microservices and components for efficient data delivery across systems', 'Designing clean, efficient, robust and reliable solutions that deliver on requirements', 'Able to demonstrate sound business judgment', 'About The Client', 'Able to digest complexity while maintaining an understanding of the “big picture” of business needs', 'Strong communicators who excel at rapid synthesis', 'Key Responsibilities Include', 'Analytic and relentless in pursuit of the right answer', 'Experience working in a fast-paced agile delivery environment and managing work via JIRA or a similar tracking tool', 'Strong analytical and problem-solving skills.', 'Analytic and relentless in pursuit of the right answerStrong communicators who excel at rapid synthesisAble to demonstrate sound business judgmentAble to digest complexity while maintaining an understanding of the “big picture” of business needsTeam players who are energized by a collaborative enterprise', 'Providing guidance on quality engineering which includes test class and automation frameworks.', 'Ability to manage user expectations and complete work requests within the stated or expected deadlines.', 'Development experience in a large organization within a complex technology landscape, that includes AWS experienceStrong understanding of data modelling and data architecture principlesHands on experience authoring and configuring AWS data pipelines and building microservices and components for efficient data delivery across systemsGood understanding of authentication, authorization and security patternsPractical familiarity with microservices architecture concepts, including API gateways, security, scaling and resiliency, and operational monitoringExperience working in a fast-paced agile delivery environment and managing work via JIRA or a similar tracking toolHands on experience working with IDE, version control and CI/CD infrastructure.Ability to manage user expectations and complete work requests within the stated or expected deadlines.Strong analytical and problem-solving skills.Excellent verbal and written communication skills.', 'Global Asset Manager', 'Strong understanding of data modelling and data architecture principles', 'Team players who are energized by a collaborative enterprise', 'Excellent verbal and written communication skills.', 'The Ideal Candidate Would Have a Background Including', 'Principal Responsibilities', 'Successful Candidates Are', 'Good understanding of authentication, authorization and security patterns', 'Developing, maintaining and continuously improving DevOps methods and practices', 'Hands on experience working with IDE, version control and CI/CD infrastructure.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
"Data Engineer, PySpark",Odyssey Information Services,"Plano, TX",1 day ago,Over 200 applicants,"['', 'Must be able to organize and schedule work effectively', 'Willingness and ability to learn in a fast-paced environment', 'Delivering business intelligence and dashboard solutions', 'Proficient in data wrangling technologies like SQL and Python', '5 years of relevant experience', 'Must be able to work well under time constraints', 'Ability to work independently and problem solve with little to no direction', 'As a Data Engineer, the primary role will be to maintain the data visualization technologies and platform and provide support in advancing the capabilities to synthesize volumes of complex data and refine that raw data into actionable visual insights meaningful and important to the business.', 'Analyzing, compiling, cleansing, interpreting, joining and staging data', 'An expert who can tell a story with data, as well as visualize it with effective communication tools', 'Strong background in data relationships, modeling, semantic layer and mining', 'Working in an Agile development environment', 'Must be able to handle multiple tasks with changing priorities, communicating changes in scope and schedule to all parties concerned', ""This is a place where divisional analytical users can visit to understand how we are performing toward our goals, learn about the shape of our business metrics, and regularly check in on measurements that are central to our company's mission. We are looking for a well-rounded engineer who has good business and design sense."", ""Bachelor's Degree in Computer Science, Engineer, Mathematics or similar required, master's degree preferred"", 'Experience managing multiple, simultaneous projects', 'Requirements', '\xa0', 'Ability to teach and train effectively', 'Proven ability to produce high-quality data visualizations in a fast-paced environment', 'Experience working with complex and large volumes of data', ""Bachelor's Degree in Computer Science, Engineer, Mathematics or similar required, master's degree preferred5 years of relevant experienceDelivering business intelligence and dashboard solutionsAnalyzing, compiling, cleansing, interpreting, joining and staging dataWorking in an Agile development environmentResearch product and technical data in order to recommend products, technologies, and processes for ongoing projectsProven ability to produce high-quality data visualizations in a fast-paced environmentAn expert who can tell a story with data, as well as visualize it with effective communication toolsStrong background in data relationships, modeling, semantic layer and miningProficient in data wrangling technologies like SQL and PythonExperience working with complex and large volumes of dataAbility to filter the signal from the noise in datasetsAbility to work independently and problem solve with little to no directionWillingness and ability to learn in a fast-paced environmentAbility to teach and train effectivelyExperience managing multiple, simultaneous projectsMust be able to organize and schedule work effectivelyMust be able to work well under time constraintsMust be able to handle multiple tasks with changing priorities, communicating changes in scope and schedule to all parties concernedMust be able to maintain confidentiality"", 'Must be able to maintain confidentiality', 'Ability to filter the signal from the noise in datasets', 'Research product and technical data in order to recommend products, technologies, and processes for ongoing projects']",Mid-Senior level,Full-time,Information Technology,Hospital & Health Care,2021-03-24 13:05:10
Data Engineer,Chase International Partners,"Boston, MA",3 weeks ago,129 applicants,"['', 'High energy and collaborative style; flexibility and willingness to take on a broad range of engineering and support assignments', 'Understanding of DevOps and CI/CD practices and tools', 'The staff will partner with business users, senior architects, and infrastructure engineers to form complete end-to end-solutions.\xa0 They thrive in an evolving fast-paced environment, and bring a work style marked by high energy, flexibility, quick learning, and collaboration.', 'Job Skills and Abilities', 'Data modeling and design understanding, including conceptual, logical, and physical modeling', 'Mix of education and experience with SQL Server, SQL Server Integration Services, and Azure Data Services (Data Factory, Databricks)In-depth SQL, plus Python, R and/or similar analytical languagesData modeling and design understanding, including conceptual, logical, and physical modelingUnderstanding of DevOps and CI/CD practices and toolsHigh energy and collaborative style; flexibility and willingness to take on a broad range of engineering and support assignmentsTrack record and skill sets demonstrated on high-intensity, high-complexity projectsComfort and confidence interacting with business usersAbility to use analytical skills to translate business ideas into technology solutionsExperience in Agile delivery frameworkExperience working with distributed external resources and vendor teams (onshore and offshore)Clear communicator of technical details both verbally and in writing', 'Mix of education and experience with SQL Server, SQL Server Integration Services, and Azure Data Services (Data Factory, Databricks)', 'My client, a leader within the alternative investment space is growing. We have seen them grow in 4 years from 300 staff to 650 plus and $30 bil AUM to over $70 bil AUM. They have been hiring all year and still growing, an amazing company to join.', 'Experience working with distributed external resources and vendor teams (onshore and offshore)', 'Clear communicator of technical details both verbally and in writing', 'The Data Engineer will be responsible for building and maintaining data management processes for our cloud-based Investment Data Analytics Platform, a strategic asset that drives quantitative-driven research and investment decision-making.\xa0 ', 'Experience in Agile delivery framework', 'Hired staff will design and build data validations, transformations, normalizations, reports and extracts, and integration processes that deliver unstructured and structured content to our cloud-based Data Lake and SQL Warehouse.\xa0\xa0 ', 'Comfort and confidence interacting with business users', 'Track record and skill sets demonstrated on high-intensity, high-complexity projects', 'In-depth SQL, plus Python, R and/or similar analytical languages', 'Ability to use analytical skills to translate business ideas into technology solutions']",Mid-Senior level,Full-time,Information Technology,Venture Capital & Private Equity,2021-03-24 13:05:10
Data Analyst/ Data Engineer,Avenue Code,"San Francisco, CA",3 weeks ago,Over 200 applicants,"['', '""We were awarded as one of the bests companies to work on (GPTW Award -', ' Data modeling and data visualization. Answer client’s business questions by dissecting their data, using measurement techniques, drafting KPIs, and building reports and dashboards. Collaborate with clients and team members on data visualizations using tools such as Tableau, Qlik, PowerBI, Looker, per clients’ needs. Generate requirements for application designs while pinpointing the best type of visualization to meet your client’s needs. Run data and dashboard quality assurance throughout the design phase in collaboration with your team. ', 'More reasons to be an Avenue Coder?', 'Data modeling and data visualization.', 'Required Qualifications', ' Degree in computer science or a similar field Solid professional experience as a Data Analyst and/or Business Analyst Strong experience querying data with SQL  Strong grasp of statistics and experience conducting rigorous data analysis  Experience with Business Intelligence tools, like Tableau, Qlik, PowerBI, Metabase or Looker  Experience with visualization best practices  Experience using git  Troubleshooting and analytical skills  Good English ', '2015, 2016 and 2020);', 'Degree in computer science or a similar field', 'Good English', 'Experience with visualization best practices ', 'Experience using git ', 'Answer client’s business questions by dissecting their data, using measurement techniques, drafting KPIs, and building reports and dashboards.', 'About The Opportunity', 'Responsibilities', 'Run data and dashboard quality assurance throughout the design phase in collaboration with your team.', 'Strong grasp of statistics and experience conducting rigorous data analysis ', 'Strong experience querying data with SQL ', 'Solid professional experience as a Data Analyst and/or Business Analyst', 'Troubleshooting and analytical skills ', 'Generate requirements for application designs while pinpointing the best type of visualization to meet your client’s needs.', 'Collaborate with clients and team members on data visualizations using tools such as Tableau, Qlik, PowerBI, Looker, per clients’ needs.', ' ""We were awarded as one of the bests companies to work on (GPTW Award - 2015, 2016 and 2020);  Working with modern technologies; Flexible hours"" ', ' Working with modern technologies;', 'Experience with Business Intelligence tools, like Tableau, Qlik, PowerBI, Metabase or Looker ', 'Flexible hours""']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Twitch,"San Francisco, CA",4 weeks ago,Over 200 applicants,"['', 'Delight data consumers throughout Twitch by ensuring they have the data they need to inform decisions, where and when they need it.', 'Prioritize projects from a diverse set of partners', 'You Will', 'Experience with Python', ' 401(k) , Maternity & Parental Leave ', 'Amazon Employee Discount', ' Define and own organization-level data architecture for a trusted, governed, dimensionally-modeled repository of data that enables Twitch staff to quickly and reliably answer their questions.', ' Optimize business, engineering, and data processes via data architecture, engineering, testing, and operational excellence best practices.', 'About The Role', 'Protect data sources against data quality issues: work with data producers to ensure data passes acceptance tests; design, develop and maintain data quality monitoring and assurance framework; and continuously improve the processes for developing new ones, raising the level of quality expected from our work.', 'Experience with Amazon Web Services: Redshift, S3, Glue, EMR, or Athena', 'Flexible PTO', ' 3+ years using relational database concepts with a working knowledge of SQL, SQL Tuning, data modeling best principles, OLAP, Big Data technologies', ' Commuter Benefits ', 'Breakfast, Lunch & Dinner Served Daily', 'Experience with development best practices, including query optimization, version control, code reviews, and documentation', 'Medical, Dental, Vision & Disability Insurance', ' 3+ years of experience generating data pipelines from multiple data sources, in collaboration with diverse team members', 'About Us', ' Delight data consumers throughout Twitch by ensuring they have the data they need to inform decisions, where and when they need it.  Define and own organization-level data architecture for a trusted, governed, dimensionally-modeled repository of data that enables Twitch staff to quickly and reliably answer their questions. Prioritize projects from a diverse set of partners Protect data sources against data quality issues: work with data producers to ensure data passes acceptance tests; design, develop and maintain data quality monitoring and assurance framework; and continuously improve the processes for developing new ones, raising the level of quality expected from our work. Improve data discovery: create data exploration processes and promote adoption of data sources across the company.  Optimize business, engineering, and data processes via data architecture, engineering, testing, and operational excellence best practices. ', 'Free Snacks and Beverages ', ' 3+ years of experience in data engineering, software engineering, or other related roles', 'Improve data discovery: create data exploration processes and promote adoption of data sources across the company.', 'You Have:', 'Perks', '  3+ years of experience in data engineering, software engineering, or other related roles  3+ years using relational database concepts with a working knowledge of SQL, SQL Tuning, data modeling best principles, OLAP, Big Data technologies  3+ years of experience generating data pipelines from multiple data sources, in collaboration with diverse team members Experience with development best practices, including query optimization, version control, code reviews, and documentation Experience with Amazon Web Services: Redshift, S3, Glue, EMR, or Athena Experience with Python ', ' Medical, Dental, Vision & Disability Insurance  401(k) , Maternity & Parental Leave  Flexible PTO  Commuter Benefits  Amazon Employee Discount Monthly Contribution and Discounts for Wellness Related Activities & Programs (e.g., gym memberships, off-site massages), Breakfast, Lunch & Dinner Served Daily Free Snacks and Beverages  ', 'Monthly Contribution and Discounts for Wellness Related Activities & Programs (e.g., gym memberships, off-site massages),']",Not Applicable,Full-time,Information Technology,Internet,2021-03-24 13:05:10
"Data Engineer, Data Platform",MasterClass,"San Francisco, CA",4 weeks ago,88 applicants,"['', 'Work full-time in our San Francisco office', 'Attain SLA’s for data sets and processes running in production', 'What We Are Looking For', 'exceptional Data Engineer ', '2+ years of experience in Data Engineering and Data Warehousing', 'Who We Are', 'Experience integrating and development in distributed/RT systems ', 'Responsibilities Of The Role', ' Proactively execute implementations of our data engineering/data warehouse initiatives that holds video streaming, subscription, enterprise, CRM, and financial datasets and systems Understand business needs, build data models, develop scalable and reliable solutions Maintain data quality; use best practices for standard naming conventions and coding practices to ensure consistency of data models and tracking Attain SLA’s for data sets and processes running in production Continuously improve our various tools within data infrastructure Design and develop scalable implementations  Build strong cross-functional partnerships with Data Scientists, Analysts, Product Managers and Software Engineers to understand data needs and implement on those needs ', 'Understand business needs, build data models, develop scalable and reliable solutions', 'Strong communication skills, with the ability to execute projects proactively and accurately', 'Requirements', 'At MasterClass, we believe we put our best work forward when our employees bring together ideas that are diverse in thought. We are proud to be an equal opportunity workplace and are committed to equal employment opportunity regardless of race, color, religion, national origin, age, sex, marital status, ancestry, physical or mental disability, genetic information, veteran status, gender identity or expression, sexual orientation, or any other characteristic protected by applicable federal, state or local law. In addition, MasterClass will provide reasonable accommodations for qualified individuals with disabilities. If you have a disability or special need, we would like to know how we can better accommodate you.', 'Maintain data quality; use best practices for standard naming conventions and coding practices to ensure consistency of data models and tracking', 'Proactively execute implementations of our data engineering/data warehouse initiatives that holds video streaming, subscription, enterprise, CRM, and financial datasets and systems', 'Experience integrating with external systems/vendor APIs for ingestion of datasets in data platform', 'Advanced proficiency with SQL, Python, Postgres, REST/GraphQL', 'Design and develop scalable implementations ', 'Continuously improve our various tools within data infrastructure', "" 2+ years of experience in Data Engineering and Data Warehousing Bachelor's degree in a quantitative field, e.g. Computer Science, Math, Physics Advanced proficiency with SQL, Python, Postgres, REST/GraphQL Experience in working within cloud based Data warehouse/Data platform systems, developing ETL/ELT pipelines and usage of RDBMS/NoSQL/other types of databases Experience integrating with external systems/vendor APIs for ingestion of datasets in data platform Experience integrating and development in distributed/RT systems  Strong communication skills, with the ability to execute projects proactively and accurately Work full-time in our San Francisco office "", ""Bachelor's degree in a quantitative field, e.g. Computer Science, Math, Physics"", 'Experience in working within cloud based Data warehouse/Data platform systems, developing ETL/ELT pipelines and usage of RDBMS/NoSQL/other types of databases', 'Build strong cross-functional partnerships with Data Scientists, Analysts, Product Managers and Software Engineers to understand data needs and implement on those needs']",Associate,Full-time,Engineering,E-Learning,2021-03-24 13:05:10
Data Engineer,"Munich Reinsurance America, Inc.","Princeton, NJ",2 weeks ago,80 applicants,"['Experience in insurance industry (preferred experience working with insurance auto/mobility data)', 'Excellent analytical, problem solving and organizational skills.', 'Expert SQL skills including experience with distributed and spatial queries', 'Strong oral and written communication and interpersonal skills', ""Bachelor's degree (MS preferred) in Computer Science, Statistics, Math or equivalent combination of education and experience1-3 years of experience with statistical modeling, data modeling, data intake, and data-curation proceduresExperience in insurance industry (preferred experience working with insurance auto/mobility data)Expert SQL skills including experience with distributed and spatial queries1+ years of hands-on R or Python experience is highly preferredExperience in data pipeline development using Databricks and Azure Data FactoryExperience with data visualization tools such as Power BIDrive and dedication, as well as creativity and hands-on attitudeCuriosity in searching for new solutions outside of traditional approachesDemonstrated ability to experiment with and learn new technologiesStrong oral and written communication and interpersonal skillsExcellent analytical, problem solving and organizational skills."", 'Curiosity in searching for new solutions outside of traditional approaches', '1-3 years of experience with statistical modeling, data modeling, data intake, and data-curation procedures', 'Experience in data pipeline development using Databricks and Azure Data Factory', 'Drive and dedication, as well as creativity and hands-on attitude', 'Munich Re America Services (MRAS) is a shared service organization that delivers services to all Munich Re US P&C Companies and other group entities.Job DescriptionAccess to increasing volumes of data, computing power, and data analytics capabilities is transforming the insurance industry and has an impact on all parts of the insurance value chain. Data is an important asset to Munich Re and there is an increasing need for data to develop and strengthen our business. The Munich Re America Services Data and Analytics team provides support across our businesses to achieve these goals. Our Data Engineering team supports the extraction, architecting, modeling, transformation, and interpretation of data across a range of business initiatives. The team works closely with our underwriting, actuarial, analytics, IT, and claims teams to create data assets and to automate and develop end-user tools that support data-driven decision-making. We are looking for a Data Engineer to join our team with a specific focus on supporting our work in the mobility space which is a growing area of emphasis where data is critical to success. This position will be responsible for executing on projects focusing on the understanding, extraction, cleaning, and transformation of data. This will include working with a variety of complex data sources, at various granular levels to build high quality datasets for analysis as well as building pipelines to automate processes and develop data architecture designs that support future growth. Through this work you will develop an understanding of the business models across a range of Munich Re companies and how data is being used to enable new underwriting, product development, and claims solutions. As the scope is broad and the business environment is dynamic, we are seeking someone with the creativity, drive, and dedication to initiate and execute new solutions. Key Responsibilities of this position include: ', 'Demonstrated ability to experiment with and learn new technologies', 'Experience with data visualization tools such as Power BI', 'Drive data architecture design decisions considering future growth', 'Independently perform the extraction, cleaning, transformation and loading of various data file formats, databases and cloud/web sources', 'Execute on projects to provide relevant datasets in support of business initiatives', 'At Munich Re US, we see Diversity and Inclusion as a solution to the challenges and opportunities all around us. Our goal is to foster an inclusive culture and build a workforce that reflects the customers we serve and the communities in which we live and work. We strive to provide a workplace where all of our colleagues feel respected, valued and empowered to achieve their very best every day. We recruit and develop talent with a focus on providing our customers the most innovative products and services. We are an equal opportunity employer. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.Qualifications / Job Requirements: ', 'Develop expert knowledge of data and analytics infrastructure within Munich Re', 'Execute on projects to provide relevant datasets in support of business initiativesIndependently perform the extraction, cleaning, transformation and loading of various data file formats, databases and cloud/web sourcesWork with various complex databases containing data from multiple sources, at various levels of granularity to deliver high-quality dataBuild data pipelines for the automation of data processesDrive data architecture design decisions considering future growthDevelop expert knowledge of data and analytics infrastructure within Munich Re', 'Build data pipelines for the automation of data processes', 'Work with various complex databases containing data from multiple sources, at various levels of granularity to deliver high-quality data', ""Bachelor's degree (MS preferred) in Computer Science, Statistics, Math or equivalent combination of education and experience"", '1+ years of hands-on R or Python experience is highly preferred']",Mid-Senior level,Full-time,Information Technology,Insurance,2021-03-24 13:05:10
Data Engineer ,Concero,"St Louis, MO",,N/A,"['', '#3518', '• Must be presently authorized to work in the U.S. without a requirement for work authorization sponsorship by our company for this position now or in the future', '• Experience with scripting languages such as Bash Shell, Python, or PowerShell', '• Must be able to work in a fast-paced, collaborative, team-oriented environment', '• Must have good oral and written communication skills to communicate with various IT teams and business representatives effectively', 'Equal Opportunity Employer/Disability/Veterans', 'Required:', '• Must have experience in full lifecycle development and end-to-end testing', ""• Bachelor's degree in Computer Science, Computer Information Systems, Management Information Systems, or related field preferred"", '• Knowledge of Teradata data warehousing and SAS business analytics technologies to support migration to Microsoft Azure cloud data platform', '• Must have 3 or more years of related experience', 'Preferred:', '• Professional data engineering experience working with large datasets and complex data environments, processes, and associated solutions', '• Experience with configuration management and build automation capabilities such as Git/Jenkins', '• Experience writing and tuning complex SQLs, and the ability to analyze and make recommendations on results', '• Experience working with Apache Spark', '• Experience with Azure, AWS, or other Public Cloud environments', '• Basic understanding of streaming data, unstructured and structured data ingestion, and other real-time data acquisition methodologies', '• Must have professional development experience in one of the following languages: Scala, Java, or C#', '• Must be committed to incorporating security into all decisions and daily job responsibilities', '• Experience with Microsoft Azure platform technologies including Databricks/Spark applications, Event Hub, Data Factory, Synapse Analytics, Azure Functions (Scala), Cosmos DB, and DevOps', '• Experience with functional programming and functional programming design patterns', '• Experience making enhancements or building new data pipelines on an already existing data platform', '• Knowledge and working experience with Agile methodologies', 'In this role on the Cloud Data Engineering team, you will be a part of a team responsible for migrating existing data platform solutions to the cloud. You will also collaborate with others to develop new data pipelines. As a data engineer in the Cloud Data Engineering team, you will need to work in a data-first mindset. You will use Microsoft Azure Cloud PaaS technologies to engineer our data solutions in a way that allows us to optimize our information, make better decisions, and meet our customer’s needs. If you have a passion for engineering, working with massive amounts of data, and empowering smart business decisions, this is the role for you.', '• Must have good troubleshooting and problem-solving skills', '• Experience with Big Data solutions and No SQL DBMSs']",Mid-Senior level,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Data Engineer,CLEAR (clearme.com),"New York, NY",3 weeks ago,27 applicants,"['', 'What You Will Do', 'Experience with NoSQL databases such as Redis, Cassandra, CouchDB, MongoDB, Elasticsearch', 'Work with our Data Warehouse, Data Science, and Product teams to ensure that we have high quality data that meets the needs of the business.', 'Build out our data pipeline architecture, and optimize data flow and collection for cross functional teams.', 'Lead the development of a Data Lake solution that can be used for reporting and analytics across the entire organization.', 'Experience with queuing systems: SQS, RabbitMQ', 'You have experience with big data tools such as Hadoop, Spark, Hive, Hudi, Presto, Sqoop', '5+ years of experience with languages such as Python, Java, and Scala', 'Experience with stream-processing systems: Kafka, Storm, Spark Streaming', 'Drive data acquisition and technology improvements to help our systems evolve with our needs.', 'You have experience with SQL Databases such as: Redshift, SQL Server, Snowflake, Big Query, Oracle, Postgres, MySQL', ' Build out our data pipeline architecture, and optimize data flow and collection for cross functional teams. Lead the development of a Data Lake solution that can be used for reporting and analytics across the entire organization. Work closely with our engineering teams to integrate data sources across a multitude of micro-services. Work with our Data Warehouse, Data Science, and Product teams to ensure that we have high quality data that meets the needs of the business. Drive data acquisition and technology improvements to help our systems evolve with our needs. ', 'Experience with data pipeline and workflow management tools: Airflow, Luigi, Oozie, Azkaban, etc.', 'You have 3+ years working in an AWS environment, with experience using one or more of the following: Kinesis, EMR, RDS, S3, Glue, Athena, DynamoDB', 'Work closely with our engineering teams to integrate data sources across a multitude of micro-services.', 'Have experience developing against internal and external API’s to consume data from disparate structured and unstructured sources', 'Who You Are', ' You have 3+ years working in an AWS environment, with experience using one or more of the following: Kinesis, EMR, RDS, S3, Glue, Athena, DynamoDB Have experience developing against internal and external API’s to consume data from disparate structured and unstructured sources 5+ years of experience with languages such as Python, Java, and Scala You have experience with big data tools such as Hadoop, Spark, Hive, Hudi, Presto, Sqoop Experience with stream-processing systems: Kafka, Storm, Spark Streaming You have experience with SQL Databases such as: Redshift, SQL Server, Snowflake, Big Query, Oracle, Postgres, MySQL Experience with NoSQL databases such as Redis, Cassandra, CouchDB, MongoDB, Elasticsearch Experience with data pipeline and workflow management tools: Airflow, Luigi, Oozie, Azkaban, etc. Experience with queuing systems: SQS, RabbitMQ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Tangoe,United States,1 week ago,111 applicants,"['As a Federal Contractor, Tangoe is required to participate in the E-Verify Program to confirm eligibility to work in the United States. For information please visit Tangoe’s career page at https://www.tangoe.com/company/careers/.', 'As an experienced data pipeline builder and data wrangler who enjoys building data systems from the ground up you’re excited at the prospect of optimizing/re-designing our data architecture to support our next generation of products and data initiatives using various data pipeline and workflow management tools.', 'Integrity · Excellence · Courage · Selflessness', 'inventory', 'More Information About Tangoe:', 'cloud', 'Our values: Integrity · Excellence · Courage · Selflessness', 'The law requires Tangoe to post a notice describing the Federal laws prohibiting job discrimination. For information\xa0regarding your legal rights and protections, please visit Tangoe’s career page at https://www.tangoe.com/company/careers/.', 'We are looking for a savvy Data Engineer to join our team of analytics experts on a company-wide project to bring all our platform data into one data warehouse.\xa0Working closely with a Senior Data Architect you’ll be responsible for expanding and optimizing our “big data” data pipelines, architecture and data sets, and building processes supporting data transformation, data structures, metadata, dependency and workload management. The infrastructure is to be built from a wide variety of data sources using SQL and AWS ‘big data’ technologies.\xa0You’ll also build the analytics tools to utilize the data pipeline, providing stakeholders with insights into an array of information.', ""A competitive salary, along with a whole stack of other benefits too!\xa0We're ambitious, friendly, driven, and passionate. If you want to be a part of a like-minded team in an evolving, challenging marketplace, then it’s worth us having a chat.\xa0"", 'What you’ll do:', 'What we offer:', '\xa0\xa0', 'Who you are: ', 'telecom', '\xa0pay their bills', 'This is a 100% remote role that can be based anywhere in the USA.\xa0', 'Technically you’re highly proficient in relational SQL and NoSQL databases, including Postgres, Oracle and Cassandra and confident working with cloud services (AWS, Hadoop or Hive). You’re experienced with object-oriented/object function scripting languages: Python, Java, NodeJS and with stream-processing systems: Storm, Spark-Streaming, Amazon Kinesis, etc. You excel in performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. It goes without saying that you have exceptional analytic skills, confident working with both structured and unstructured datasets and have a successful history of manipulating, processing and extracting value from large disconnected datasets.\xa0', 'Tangoe reaffirms its commitment to providing equal opportunities for employment and advancement to qualified employees and applicants. Individuals will be considered for positions for which they meet the minimum qualifications and are able to perform without regard to race, color, gender, age, religion, disability, national origin, veteran status, sexual orientation, gender identity, current unemployment status, or any other basis protected by federal, state or local laws. Tangoe is an Equal Opportunity Employer - Minority/Female/Disability/Veteran/Current Unemployment Status.', 'This is a constantly evolving area of our business as we’re always looking how we can improve our services to our clients. Challenges will be ongoing and varied, but this is where you’re happiest, creating a warehouse that will hold up to stringent testing by internal and external stakeholders and add immense value to the organization and our F500 clients.', '\xa0', 'Notice to Recruiters and Agencies: Tangoe prefers to recruit candidates directly, rather than through a third party recruiter or agency. Do not submit or present your candidate(s) through any means (e-mail, fax, phone, mail, verbal referral) to Tangoe or any employee of Tangoe. In the case of candidate(s) submitted or presented to Tangoe by a recruiter or agency without a signed agreement in place for the specific position or without a specific open requisition, Tangoe explicitly reserves the right to pursue and hire those candidate(s) without any financial obligation to the recruiter or agency. To request consideration as a Recruiting Vendor, please contact our recruiting department.', 'orders', 'Notice to Recruiters and Agencies:', 'Team-working is a key part of this role as you’ll be working with a range of stakeholders including the Executive, Product, Data and Design teams, assisting with their data-related technical issues and supporting their data infrastructure needs. You’ll also support our software developers, database architects, data analysts and data scientists on data initiatives ensuring optimal data delivery architecture is consistent throughout ongoing projects, so you’re happy being part of a great team.', 'savings', 'mobile', 'Tangoe will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay. For information please visit Tangoe’s career page at https://www.tangoe.com/company/careers/.', 'Data Engineer', 'Where you’ll work:', 'The Legal Stuff in really small print:', 'Who we are:', 'Tangoe is powered by cutting-edge technology and the top people in the IT lifecycle management industry. Customers count on us to\xa0pay their bills, process their\xa0orders, manage their\xa0inventory, and find them\xa0savings\xa0for their global\xa0telecom,\xa0mobile, and\xa0cloud\xa0technology environments.\xa0In other words, we’re really rather fabulous, and Gartner rates us as “Visionary”!', 'Tangoe is committed to working with and providing reasonable accommodation to individuals with physical and mental\xa0disabilities. If you need special assistance or an accommodation while seeking employment, please call\xa0(973) 257-0300.\xa0We will make a determination on your request for reasonable accommodation on a case-by-case basis.']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Systematix,United States,2 weeks ago,Over 200 applicants,"['', 'Proven experience working in a cross-functional team of technical and non-technical personnel', 'Experience with building data or machine learning pipeline', 'Organizing, cleaning, and analyzing disparate data sets and find patterns and attributes that could be used to build machine learning modelsGenerating proofs-of-concept with exploratory data sets and models to demonstrate business value and define development roadmaps for future workDesigning and generating data flows as well as expanding and optimizing our current data pipeline architectureImplementing mock-up end-user interface for reporting and data visualizationCollaborating with subject matter experts and software engineers to define and implement suitable data platforms to support exploratory work as well as deliver data science products to business usersIntegrating access management tools which allow data access to be configured at the user level', 'Experience with NLP/text analysis, preferred', 'ABOUT SYSTEMATIX:', 'ABOUT THE REQUIRED SKILLS:', 'If you are interested in finding out more please contact us or submit your resume to\xa0', 'Organizing, cleaning, and analyzing disparate data sets and find patterns and attributes that could be used to build machine learning models', 'Data and Machine Learning Engineer', 'Systematix, we pride ourselves in taking care of our consultant partnerships - by doing the little things that matter - like taking the time to get to know you, knowing when an opportunity is the perfect fit, informing you every step of the way, and building the foundation for long term relationships. We do the same with out clients, concerning ourselves more with the project than the position, so that we have the information you need to ensure an opportunity is right for you. We only work with people who enjoy making a difference.', 'Degree in Computer Science or Engineering.Proficiency in Python software development and one or more programming tools (Pandas, Numpy...)Experience in acquiring, cleaning, and structuring data from multiple sources, ETL processes, data imputationExperience in creating and maintaining relational databases, SQL/NoSQL, as well as key-value storesExperience with building data or machine learning pipelineWritten and verbal technical communication skills with an ability to present complex technical information in a clear and concise manner to a variety of audiencesExperience with NLP/text analysis, preferredExperience with data Integration for Business Intelligence tools, such as Power BI, preferredExperience with AWS technology stack, preferredExcellent analytical, problem solving and decision making skills.Proven technical skills and a team player, with strong collaboration skills.Proven experience working in a cross-functional team of technical and non-technical personnel', 'Generating proofs-of-concept with exploratory data sets and models to demonstrate business value and define development roadmaps for future work', 'We are Systematix and we are currently looking for a\xa0Data and Machine Learning Engineer\xa0with strong data pipeline skills to fill a contract role with one of our key clientele.', 'This is a 6 month remote contract position.', 'Integrating access management tools which allow data access to be configured at the user level', 'At Systematix, we bring people and projects together!', 'Collaborating with subject matter experts and software engineers to define and implement suitable data platforms to support exploratory work as well as deliver data science products to business users', 'Experience with AWS technology stack, preferred', 'Designing and generating data flows as well as expanding and optimizing our current data pipeline architecture', 'Proficiency in Python software development and one or more programming tools (Pandas, Numpy...)', 'Degree in Computer Science or Engineering.', 'Experience with data Integration for Business Intelligence tools, such as Power BI, preferred', 'ABOUT THE ROLE:', 'Written and verbal technical communication skills with an ability to present complex technical information in a clear and concise manner to a variety of audiences', 'ABOUT THE PROJECT:', 'APPLY NOW:', 'This is a contract position, where you will be a significant contributor to their product development team, working in tandem with their development group.\xa0As Data Engineer, you will be tasked with the design and development of data pipeline.', 'Experience in creating and maintaining relational databases, SQL/NoSQL, as well as key-value stores', 'Proven technical skills and a team player, with strong collaboration skills.', 'Implementing mock-up end-user interface for reporting and data visualization', 'Specifically, your responsibilities in this role are:', 'Excellent analytical, problem solving and decision making skills.', 'Our client is one of the world’s leading Life Sciences organizations, with offices worldwide.\xa0Due to its ever-expanding technology footprint, providing best in class software solutions for the equipment they design and manufacture, the future of its development technology roadmap needs a guiding hand.', 'If you are interested in finding out more please contact us or submit your resume to\xa0jobs@systematix.com.', 'Experience in acquiring, cleaning, and structuring data from multiple sources, ETL processes, data imputation', 'jobs@systematix.com.']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Advantis Global,"Austin, TX",2 days ago,129 applicants,"['', 'ETL', 'Attention to detail and excellent communication skills', 'Developing robust, low latency and fault tolerant pipelines to support business critical systems', ' Helping us leverage large-scale data stores by building out ETL pipelines and utilities in Spark and Hive  Developing robust, low latency and fault tolerant pipelines to support business critical systems Aggregating key metrics for business partners to inform key decisions Working with cloud technologies to deploy your applications ', ' Experience with Docker and Kubernetes preferred Experience with AWS a bonus', 'Familiarity with workflow scheduling/orchestration tools (Oozie, Jenkins)', 'Aggregating key metrics for business partners to inform key decisions', 'About This Opportunity', 'Experience with Docker and Kubernetes preferred', 'Opportunity For You', 'Working with cloud technologies to deploy your applications', 'Strong skills in SQL, Java and/or Python', 'Experience with AWS a bonus', 'Key Success Factors', 'Helping us leverage large-scale data stores by building out ETL pipelines and utilities in Spark and Hive ', ' Strong skills in SQL, Java and/or Python Experience with Apache Big Data Frameworks (Hadoop, Spark, Hive)  Familiarity with workflow scheduling/orchestration tools (Oozie, Jenkins) Strong documentation and technical writing skills  Attention to detail and excellent communication skills ETL ', 'Experience with Apache Big Data Frameworks (Hadoop, Spark, Hive) ', 'Strong documentation and technical writing skills ']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Staff Data Engineer,Venmo,"Scottsdale, AZ",18 hours ago,Be among the first 25 applicants,"['', 'Professional 5+ years of industry and academic experiences in a quantitative role with an advanced degree. ', 'Build, forecast, and report on metrics that drive strategy and facilitate decision making for key business initiatives. ', 'Passion and intellectual curiosity for the FinTech domain. ', 'Staff', 'Expert in ETL, big data platforms and tooling (AWS, Snowflake, Kafka, Luigi, Hadoop, Hive, Spark, Cassandra, Airflow, etc). ', 'About Us', 'Data Engineer', 'Excellent written and verbal communication to take ownership of meeting with cross-functional team leads on a regular basis. ', 'Familiarity with statistics and mathematical analysis, prior experience in data visualization tools such as Tableau or Looker is plus. ', 'Comfortable in working in ambiguous, fast-paced and high-growth dynamic environment. ', 'Advanced proficiency with SQL and highly skilled in at least one scripting language such as Python. ', ' Professional 5+ years of industry and academic experiences in a quantitative role with an advanced degree.  Advanced proficiency with SQL and highly skilled in at least one scripting language such as Python.  Expert in ETL, big data platforms and tooling (AWS, Snowflake, Kafka, Luigi, Hadoop, Hive, Spark, Cassandra, Airflow, etc).  Strong fundamentals of data structures, algorithms and design patterns.  Familiarity with statistics and mathematical analysis, prior experience in data visualization tools such as Tableau or Looker is plus.  Build, forecast, and report on metrics that drive strategy and facilitate decision making for key business initiatives.  Passion and intellectual curiosity for the FinTech domain.  Comfortable in working in ambiguous, fast-paced and high-growth dynamic environment.  Excellent written and verbal communication to take ownership of meeting with cross-functional team leads on a regular basis.  ', 'Strong fundamentals of data structures, algorithms and design patterns. ', 'What We’re Looking For', 'Applied Science team ']",Associate,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Data Platform Engineer,Stash,"Remote, OR",2 days ago,Be among the first 25 applicants,"['', 'Flexible Remote ', 'Who We’re Looking For', 'Experience with working with a cluster manager (YARN / Mesos / Kubernetes).', 'BS / MS in Computer Science, Engineering, Mathematics, or a related field', 'Awards & Recognition', 'MarCom Awards Double Gold & Platinum Winner (2018)', 'W3 Awards Winner for Best User Experience (2017)', 'Instrument, test, profile, and code review software and infrastructure', 'Experience working with Apache Airflow, Luigi, or Azkaban', 'You have built large-scale data products and understand the tradeoffs made when building these features', 'Benefits & Perks', ' Experience working with Apache Airflow, Luigi, or Azkaban Experience working with AWS Glue ', 'Equity in Stash ', 'Team outings that do not involve trust falls...', ""Built in NYC's Best Places to work (2019)"", ""What You'll Do"", 'Own the end to end delivery of software components from inception to production', 'Continuously deploy to our cloud-based infrastructure', 'Experience working with AWS Glue', ' 3+ years of professional experience working in data platform engineering BS / MS in Computer Science, Engineering, Mathematics, or a related field You have built large-scale data products and understand the tradeoffs made when building these features You have a deep understanding of system design, data structures, and algorithms Experience (or a strong interest in) working with Python, Scala, Go, or statically typed, compiled programming languagesor Go. Experience working with highly available and distributed databases (CockroachDB / YugabyteDB / MongoDB / Cassandra) Experience with working with a cluster manager (YARN / Mesos / Kubernetes). Experience with distributed computing and working with Spark, Hadoop, Hive, or MapReduce Framework Experience working on a cloud platform such as AWS Experience with ETL in general ', 'No recruiters, please.', ' Architect, design, and implement highly available data pipelines and distributed storage solutions Own the end to end delivery of software components from inception to production Instrument, test, profile, and code review software and infrastructure Continuously deploy to our cloud-based infrastructure Scale software infrastructure for a quickly-growing user base ', '______________________________________________________________________________________________', 'Flexible Vacation', '3+ years of professional experience working in data platform engineering', 'Scale software infrastructure for a quickly-growing user base', 'Experience working with highly available and distributed databases (CockroachDB / YugabyteDB / MongoDB / Cassandra)', 'Experience working on a cloud platform such as AWS', 'Family-Friendly Medical, Dental, and Vision Insurance Plans', 'Experience with ETL in general', 'Gold Stars', 'Tearsheet Challenge Awards, Best Banking Card Product - Stock-Back® Card, 2020', 'Experience (or a strong interest in) working with Python, Scala, Go, or statically typed, compiled programming languagesor Go.', 'Forbes Fintech 50 (2019 & 2020)', '401k', 'Experience with distributed computing and working with Spark, Hadoop, Hive, or MapReduce Framework', 'Learning & Development & Ergonomic Work Space Stipends', 'Commuter Benefits and Flexible Spending Account (FSA)', 'Employee referral bonuses', 'Architect, design, and implement highly available data pipelines and distributed storage solutions', 'LendIt Fintech Innovator of the Year (2019 & 2020)', 'You have a deep understanding of system design, data structures, and algorithms', 'Webby Award Winner for Best Mobile Sites & Apps in the Financial Services and Banking category (2017)', ' Equity in Stash  Flexible Vacation Flexible Remote  Family-Friendly Medical, Dental, and Vision Insurance Plans 401k Learning & Development & Ergonomic Work Space Stipends Commuter Benefits and Flexible Spending Account (FSA) Employee referral bonuses Team outings that do not involve trust falls... ', "" Forbes Fintech 50 (2019 & 2020) Best Digital Bank, Finovate Awards (2020) Tearsheet Challenge Awards, Best Banking Card Product - Stock-Back® Card, 2020 LendIt Fintech Innovator of the Year (2019 & 2020) Built in NYC's Best Places to work (2019) MarCom Awards Double Gold & Platinum Winner (2018) Webby Award Winner for Best Mobile Sites & Apps in the Financial Services and Banking category (2017) W3 Awards Winner for Best User Experience (2017) "", 'Best Digital Bank, Finovate Awards (2020)']",Entry level,Full-time,Engineering,Computer Software,2021-03-24 13:05:10
Data Engineer (Remote),Last Call Media,"Remote, OR",2 weeks ago,Be among the first 25 applicants,"['', 'Able to empathize, which helps us understand the needs of the client, the customer, the product, and the team at LCM. ', 'Experienced with communicating directly with clients', 'Provide guidance on the collection of web analytics data (eg: Implementation advice for GTM).', 'Technical Architecture experience.', 'Infrastructure as code tools, especially Terraform.', 'General comfort with Linux environments.', 'What You’ll Bring', 'Extracting data from Google Analytics/BigQuery, JSON files, and various web APIs.', 'A proven track record of building robust web data pipelines. 2+ years of similar experienceAn obvious drive to grow and learn from the highly-skilled team around you.Experience and/or a desire to work remotely.Ability to work efficiently, sometimes under tight deadlines', 'General comfort with Linux environments.Familiarity with DevOps principles, such as GitOps and infrastructure as code.Technical Architecture experience.Javascript experience.', 'Comfortable asking for help', 'Highly communicative', 'Work with team members to optimize and extend an existing data warehouse.', 'Experience and/or a desire to work remotely.', 'Last Call Media', 'Extracting data from Google Analytics/BigQuery, JSON files, and various web APIs.Utilizing Google Analytics/Tag Manager to collect web analytics data.Writing ETLs, particularly in Python for Apache Airflow or AWS Step functions.Building and maintaining AWS infrastructure, including ECS containers and Lambda functions.Building and optimizing data warehouses, especially Postgres databases.Infrastructure as code tools, especially Terraform.', 'Familiarity with DevOps principles, such as GitOps and infrastructure as code.', 'Application Process', 'Able to work independently ', 'Able to empathize, which helps us understand the needs of the client, the customer, the product, and the team at LCM. Highly communicativeAble to work independently Comfortable asking for helpExperienced with communicating directly with clientsEager and motivated to learn new conceptsA team player in a collaborative environment A fast learner', 'It’d Be Nice If You Also Had', 'Writing ETLs, particularly in Python for Apache Airflow or AWS Step functions.', 'An obvious drive to grow and learn from the highly-skilled team around you.', 'Provide guidance on feasibility and advisability of upcoming data collection projects.', 'What You’ll Do', 'Javascript experience.', 'A team player in a collaborative environment ', 'Building and maintaining AWS infrastructure, including ECS containers and Lambda functions.', 'A fast learner', 'Eager and motivated to learn new concepts', 'comprehensive benefits', 'Work with team members to optimize and extend an existing data warehouse.Gather new data collection requirements and implement or change ETL processes.Optimize existing ETL processes.Provide guidance on feasibility and advisability of upcoming data collection projects.Provide guidance on the collection of web analytics data (eg: Implementation advice for GTM).', 'All Of Us At LCM Pride Ourselves On Being', 'Optimize existing ETL processes.', 'A proven track record of building robust web data pipelines. ', 'Gather new data collection requirements and implement or change ETL processes.', 'The Role', 'Ability to work efficiently, sometimes under tight deadlines', '2+ years of similar experience', 'Building and optimizing data warehouses, especially Postgres databases.', 'Utilizing Google Analytics/Tag Manager to collect web analytics data.']",Mid-Senior level,Full-time,Engineering,Computer Software,2021-03-24 13:05:10
Software Engineer (Entry Level),"Rubrik, Inc.",San Francisco Bay Area,21 hours ago,Over 200 applicants,"['', 'Rubrik is seeking a passionate and talented software developer with infrastructure and DevOps knowledge, who has solid problem-solving and programming skills, ideally with experience in build and continuous integration systems (Jenkins, TeamCity, Argo-CI, Bamboo, etc..), and eager to enable efficient and fast releases by solving issues in the continuous integration systems.', 'Knowledge of containers and container orchestration and virtualization technology , i.e: Kubernetes and VMware.', '\xa0\xa0\xa0\xa0\xa0Maintain and support Jenkins setup in the Clouds as well as on-prem on Kubernetes.', 'Rubrik is one of the fastest growing companies in Silicon Valley, revolutionizing data protection and management in the emerging multi-cloud world. We are the leader in cloud data management, delivering a single platform to manage and protect data in the cloud, at the edge, and on-premises. Enterprises choose Rubrik to simplify backup and recovery, accelerate cloud adoption, enable automation at scale, and secure against cyberthreats. We’ve been recognized as a Forbes Cloud 100 Company two years in a row and as a LinkedIn Top 10 startup.\xa0', 'Experience: at least three years', 'Experience in managing Jenkins 2.0 pipeline as code, Jenkins configuration, setup and plugins\xa0', 'Provide recommendations to monitor and improve release and continuous integration infrastructure, and develop solutions for monitoring, metrics, and automation.\xa0Provide timely and world class services of requests from key stakeholders: software development teams and project management. Typical requests would cover; CI systems, branching, build systems and testing across multiple platform environments, code signings, patches and more.', 'Minimum Qualifications:', 'Design, develop and drive improvements and solutions to complex technical challenges around engineering productivity and quality, build scalability and stability, test, and release infrastructure and CI/CD.Partner with software development and project management teams to prepare for customization or enhancements required for new engineering requirements, to enable better code quality, testability and release.', '\ufeffABOUT THE JOB', 'ABOUT RUBRIK', 'Solid python programming skills and experience with scripting languages, e.g. Flask and/or django, Groovy, Shell script, and troubleshooting Scala builds.', 'Responsibilities:', 'Experience with multiple build tools - Bazel, GCC, Visual Studio, Make.', 'Experience with version control, build management and CI tools, e.g. Git, Argo-CI, Artifactory, Bamboo or TeamCity.', 'Partner with software development and project management teams to prepare for customization or enhancements required for new engineering requirements, to enable better code quality, testability and release.', 'You are a key stakeholder who has a sharp focus on developing, managing and automating a reliable, robust, high quality continuous integration infrastructure and services, debugging and resolving build and test issues to ensure on-time releases. The successful candidate has experience in automating tasks and working on large scale software development and releases within multi-platform and hybrid environments (On-prem and Clouds). He/she demonstrates effective communication and collaboration skills across projects and functional teams, and thrives in a fast paced environment. Strong sense of ownership and work ethics, collaboration, integrity are important traits required to succeed in this role.', 'Experience with multiple OS environments - Linux, Windows, Mac OSX, various Unix flavors.', 'Strong work ethics and adept at working with teams of highly talented individuals and at different time zones.', 'Experience: at least three yearsSolid python programming skills and experience with scripting languages, e.g. Flask and/or django, Groovy, Shell script, and troubleshooting Scala builds.Strong work ethics and adept at working with teams of highly talented individuals and at different time zones.Experience with version control, build management and CI tools, e.g. Git, Argo-CI, Artifactory, Bamboo or TeamCity.Knowledge of containers and container orchestration and virtualization technology , i.e: Kubernetes and VMware.Experience with multiple OS environments - Linux, Windows, Mac OSX, various Unix flavors.', 'ABOUT THE TEAM', 'Excellent follow up and communication skills. Ability to keep things conversational and organized while getting things done.', 'Design, develop and drive improvements and solutions to complex technical challenges around engineering productivity and quality, build scalability and stability, test, and release infrastructure and CI/CD.', 'As a member of the Engineering Platform and Developer Productivity team at Rubrik, you will be focused on solving challenging frameworks, scalability & stability problems that will enable the company to deliver a product at high-scale without compromising on quality, velocity, and coding standards. Rubrik positions you with a unique opportunity to innovate as we scale our organization to support a rapidly increasing customer base and engineering organization. As our products add more features and functionalities to the portfolio, you will be provided with more unique and diverse problems that require innovation that will help in accelerating your career.', 'Excellent follow up and communication skills. Ability to keep things conversational and organized while getting things done.Experience with multiple build tools - Bazel, GCC, Visual Studio, Make.Experience in managing the CI/CD lifecycle in a cloud environment, GCP, Azure and AWS, familiar with deployment and provisioning tools such as Ansible, Puppet.\xa0Experience in managing Jenkins 2.0 pipeline as code, Jenkins configuration, setup and plugins\xa0', 'Experience in managing the CI/CD lifecycle in a cloud environment, GCP, Azure and AWS, familiar with deployment and provisioning tools such as Ansible, Puppet.\xa0', 'Preferred Qualifications:', 'Provide recommendations to monitor and improve release and continuous integration infrastructure, and develop solutions for monitoring, metrics, and automation.\xa0', 'Provide timely and world class services of requests from key stakeholders: software development teams and project management. Typical requests would cover; CI systems, branching, build systems and testing across multiple platform environments, code signings, patches and more.']",Entry level,Full-time,Engineering,Computer Software,2021-03-24 13:05:10
Data Engineer,Summit2Sea Consulting,"Arlington, VA",7 days ago,50 applicants,"['', 'Databricks developer certification', ""We value the individual and share our company's success across our team."", ' Experience with AWS Glue, StreamSet, MLOps, building ML models Databricks developer certification ', 'Life Insurance', 'Strong competency in building data pipelines involving ETL / ELT on multiple sources and destinations', 'Ability to obtain a Secret clearance is required', 'Quarterly Recruiting Bonus', 'Team player with critical thinking and a can-do attitude', 'Strong competency in design and implementation of complex data transformations', 'Medium to strong competency in AWS and Databricks environments ', 'Summit2Sea Consulting is an Equal Opportunity Employer (EOE) and E-Verify employer. Qualified applicants are considered for employment without regard to age, race, color, religion, sex, national origin, sexual orientation, disability, or veteran status. If you need assistance or an accommodation during the application process because of a disability, it is available upon request. The company is pleased to provide such assistance, and no applicant will be penalized as a result of such a request.', 'New Sales Bonus', ""This position is currently remote due to COVID-19.*RequirementsMust Have  Strong competency in building data pipelines involving ETL / ELT on multiple sources and destinations Strong competency in design and implementation of complex data transformations Medium to strong competency in AWS and Databricks environments  Medium to strong competency in python language and libraries, including pandas, numpy, pyspark Team player with critical thinking and a can-do attitude Ability to obtain a Secret clearance is required Nice to Have  Experience with AWS Glue, StreamSet, MLOps, building ML models Databricks developer certification BenefitsUpper Tier Compensation includes a base salary and bonuses based upon performance, business development, employee referrals and knowledge sharing. We value the individual and share our company's success across our team.Summit2Sea is committed to offering our employees a benefits package that is competitive and comprehensive enough to meet their goals and needs. As a valued member of the S2S team, employees are provided with a collection of benefits to include paid holidays, health and dental care to name a few.\u200dCompensation Components"", 'Requirements', 'Build and deliver high-impact solutions for our clients', 'Work to understand complex client environment and their unique business problems', 'Competitive Base Salary', ' Strong competency in building data pipelines involving ETL / ELT on multiple sources and destinations Strong competency in design and implementation of complex data transformations Medium to strong competency in AWS and Databricks environments  Medium to strong competency in python language and libraries, including pandas, numpy, pyspark Team player with critical thinking and a can-do attitude Ability to obtain a Secret clearance is required ', 'Benefit Components', ""This position is currently remote due to COVID-19.*RequirementsMust Have  Strong competency in building data pipelines involving ETL / ELT on multiple sources and destinations Strong competency in design and implementation of complex data transformations Medium to strong competency in AWS and Databricks environments  Medium to strong competency in python language and libraries, including pandas, numpy, pyspark Team player with critical thinking and a can-do attitude Ability to obtain a Secret clearance is required Nice to Have  Experience with AWS Glue, StreamSet, MLOps, building ML models Databricks developer certification BenefitsUpper Tier Compensation includes a base salary and bonuses based upon performance, business development, employee referrals and knowledge sharing. We value the individual and share our company's success across our team.Summit2Sea is committed to offering our employees a benefits package that is competitive and comprehensive enough to meet their goals and needs. As a valued member of the S2S team, employees are provided with a collection of benefits to include paid holidays, health and dental care to name a few.\u200dCompensation ComponentsCompetitive Base SalaryQuarterly Recruiting BonusNew Sales BonusKnowledge Contribution BonusBenefit ComponentsPaid HolidaysVacation/Sick Leave/Personal Time OffHealth InsuranceDental InsuranceLife Insurance401KSummit2Sea Consulting is an Equal Opportunity Employer (EOE) and E-Verify employer. Qualified applicants are considered for employment without regard to age, race, color, religion, sex, national origin, sexual orientation, disability, or veteran status. If you need assistance or an accommodation during the application process because of a disability, it is available upon request. The company is pleased to provide such assistance, and no applicant will be penalized as a result of such a request."", 'Health Insurance', 'Compensation Components', 'Experience with AWS Glue, StreamSet, MLOps, building ML models', '401K', 'Have you been looking to shift your career into high gear? This is your opportunity to take your ambitions and convert them into a solid career in a supportive and innovative environment!', 'Paid Holidays', ""Impactful Work You'll Do "", 'Help build a new breed of products that combine ML and RPA technologies to automate business processes', 'Benefits', 'Vacation/Sick Leave/Personal Time Off', ' Work to understand complex client environment and their unique business problems Build and deliver high-impact solutions for our clients Help build a new breed of products that combine ML and RPA technologies to automate business processes ', 'Nice to Have ', 'Must Have ', 'Knowledge Contribution Bonus', 'Data Engineer.', 'Dental Insurance', 'Medium to strong competency in python language and libraries, including pandas, numpy, pyspark']",Entry level,Full-time,Analyst,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Modis,United States,5 days ago,125 applicants,"['', 'SQL Servers or postgres', 'Working with attunity – this person does not need it, but huge plus if they have it!', 'CI/CD Pipelines', 'Python', 'Build data replication pipelines and microservices', 'Need to get data pipelines on prem to cloud', '4-7 years of experiencePythonNeed to get data pipelines on prem to cloudBuild data replication pipelines and microservicesAWS (other cloud is fine, but AWS preferred)Working with attunity – this person does not need it, but huge plus if they have it!CI/CD PipelinesSQL Servers or postgresThey are not working on big data tools like Hadoop or spark, and not looking for a data scientist either', 'AWS (other cloud is fine, but AWS preferred)', 'They are not working on big data tools like Hadoop or spark, and not looking for a data scientist either', '4-7 years of experience']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Fanzz,"Sandy, UT",4 weeks ago,Be among the first 25 applicants,[],Entry level,Full-time,Information Technology,Automotive,2021-03-24 13:05:10
Data Engineer,The Scotts Miracle-Gro Company,"Marysville, OH",3 weeks ago,62 applicants,"['', 'Experience with Neo4j Graph Databases, MuleSoft, and Graphql a plus', 'Computer Science, Computer Applications, and Information Systems degree or the equivalent combination of training, education, or experience5+ years of Python or Java development experience5+ years of SQL and No-SQL experience5+ years of experience with schema design and dimensional data modelingAbility in managing and communicating data warehouse plans to internal clients', '5+ years of developing and maintaining data pipelines', 'Education/Training:', 'What you’ll do in this role:', 'What we do for you (just to list a few cool ones):', 'Excellent problem solving and troubleshooting skills', '5+ years of developing and maintaining data pipelinesProcess oriented with great documentation skillsExcellent problem solving and troubleshooting skillsExperience with Apache AirflowExperience with AWS platform with emphasis on Amazon S3, Glue, Athena, EMR, Redshift, and Lake Formation Experience with data cleansing, cataloging, and validationExperience with Neo4j Graph Databases, MuleSoft, and Graphql a plus', 'Create and maintain optimal data pipeline architecture including new API integrations to support continuing increases in data volume and complexity.', 'Implements processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it.', 'Assemble large, complex data sets that meet functional / non-functional business requirements.', 'The Sr. Analyst, Data Engineer role will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.', 'Offer extremely competitive benefits including: Health, Dental and Vision coverage ', 'Our commitment to diversity and inclusion includes employee resource groups: Scotts Women’s Network, Scotts Black Employee Network, Scotts Veterans Network, GroPride Network, and Scotts Young Professionals', 'Experience with data cleansing, cataloging, and validation', 'Analyze the impact of any new requirements on the existing solution', 'Business Intelligence ', 'We are looking for a Data Engineer who will join our Business Intelligence team in Marysville, OH.', 'First day of hire, 401K match (up to 7.5%) and discounted stock purchasing program (15% discount) ', '5+ years of SQL and No-SQL experience', 'Non-Technical Skills:', 'Beautiful campus and corporate offices designed like a log cabin offering free coffee, chef run cafe and the best crushed ice! ', 'Ability to meet with the business and technical teams to provide understandable technical direction that demonstrates understanding of and alignment with business objectives.', 'Computer Science, Computer Applications, and Information Systems degree or the equivalent combination of training, education, or experience', 'Ability in managing and communicating data warehouse plans to internal clients', 'Experience with Apache Airflow', 'Nutrition reimbursement program (up to $200 per associate and per spouse) ', 'Ability to formulate an in-depth technical strategy meeting the needs of supported business functions', 'Data Engineer ', 'Estimation of efforts for issue-resolution and change requests. ', 'Experience with AWS platform with emphasis on Amazon S3, Glue, Athena, EMR, Redshift, and Lake Formation ', 'Complex problem solving & analysis', 'Work with business stakeholders to assist with data-related technical issues and support their data infrastructure needs.', 'Actively engage with key internal and external technology partners.', 'A place on an engaged team of passionate colleagues in a growing and dynamic industry', 'Relationship building with technical and non-technical personnel', 'Process oriented with great documentation skills', 'Complex problem solving & analysisAbility to formulate an in-depth technical strategy meeting the needs of supported business functionsAbility to meet with the business and technical teams to provide understandable technical direction that demonstrates understanding of and alignment with business objectives.Relationship building with technical and non-technical personnel', 'Create and maintain optimal data pipeline architecture including new API integrations to support continuing increases in data volume and complexity.Implements processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it.Assemble large, complex data sets that meet functional / non-functional business requirements.Work with business stakeholders to assist with data-related technical issues and support their data infrastructure needs.Actively engage with key internal and external technology partners.Analyze the impact of any new requirements on the existing solutionEstimation of efforts for issue-resolution and change requests. ', '5+ years of Python or Java development experience', '5+ years of experience with schema design and dimensional data modeling', '13 paid holidays and generous vacation policy ', 'Onsite wellness center which includes: 24,000 sq. ft. fitness center, Walgreens pharmacy and Doctor’s office ', 'Marysville, OH', 'What we do for you (just to list a few cool ones)', 'A place on an engaged team of passionate colleagues in a growing and dynamic industryOffer extremely competitive benefits including: Health, Dental and Vision coverage First day of hire, 401K match (up to 7.5%) and discounted stock purchasing program (15% discount) 13 paid holidays and generous vacation policy Onsite wellness center which includes: 24,000 sq. ft. fitness center, Walgreens pharmacy and Doctor’s office Nutrition reimbursement program (up to $200 per associate and per spouse) Beautiful campus and corporate offices designed like a log cabin offering free coffee, chef run cafe and the best crushed ice! Our commitment to diversity and inclusion includes employee resource groups: Scotts Women’s Network, Scotts Black Employee Network, Scotts Veterans Network, GroPride Network, and Scotts Young Professionals', 'What you’ll need to be successful:', 'This position will be based at The Scotts Miracle-Gro world headquarters in Marysville, a suburb of Columbus, OH.\xa0 Not familiar with Columbus? Visit columbusregion.com/columbus-2020/']",Associate,Full-time,Research,Consumer Goods,2021-03-24 13:05:10
Staff Data Engineer,Walmart Global Tech,"Sunnyvale, CA",4 weeks ago,Over 200 applicants,"['', 'About Global Tech\xa0', 'Working virtually this year has helped us make quicker decisions, remove location barriers across our global team, be more flexible in our personal lives and spend less time commuting.\xa0Today, we are reimagining the tech workplace of the future by making a permanent transition to virtual work for most of our team. Of course, being together in person is an important part of our culture and shared success. We’ll collaborate in person at a regular cadence and with purpose.\xa0\xa0\xa0', ""Lead development efforts for robust data pipelines and design efforts for working with Walmart's customer data. The ideal candidate will be a combination of solid developer with strong technical design lead."", 'Have robust experience with Data engineering, database engineering, business intelligence, or business analytics, ETL tools and working with large data sets in the cloud', ""4+ years' experience in software engineering or related field"", ""Have robust experience with Data engineering, database engineering, business intelligence, or business analytics, ETL tools and working with large data sets in the cloudA Master’s degree in Computer Science or related field 4+ years' experience in software engineering or related field"", 'A Master’s degree in Computer Science or related field ', 'We’re virtual', 'The Customer Backbone team with Walmart Global Tech is hiring for a Staff Data Engineer role to be based with our team in Sunnyvale, CA. ', 'Imagine working in an environment where one line of code can make life easier for hundreds of millions of people and put a smile on their face. That’s what we do at Walmart Global Tech. We’re a team of 15,000+ software engineers, data scientists and service professionals within Walmart, the world’s largest retailer, delivering innovations that improve how our customers shop and empower our 2.2 million associates. To others, innovation looks like an app, service or some code, but Walmart has always been about people. People are why we innovate, and people power our innovations. Being human-led is our true disruption.\xa0', ""You'll blow us away if you....."", 'In this role, you will.....']",Mid-Senior level,Full-time,Engineering,Computer Software,2021-03-24 13:05:10
Data Engineer,Alignment Healthcare,"Orange County, CA",5 days ago,95 applicants,"['', 'Position Summary:', 'Industry experience as a Data Engineer or related specialty (e.g., Software Engineer, Business Intelligence Engineer, Data Scientist) with a track record of manipulating, processing, and extracting value from large datasets.Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data setsExperience building data products incrementally and integrating and managing datasets from multiple sources', 'Problem solving skills and Ability to meet deadlines are a must', 'Excellent communication, analytical and collaborative problem-solving skills', 'Demonstrable track record dealing well with ambiguity, prioritizing needs, and delivering results in an agile, dynamic startup environmentProblem solving skills and Ability to meet deadlines are a mustMicrosoft Azure Certification is a plus', 'Identifying the data quality issues to address them immediately to provide great user experience', 'Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets', 'Data Warehousing Experience with SQL Server, Oracle, Redshift, Teradata, etc.', 'Linux/UNIX including to process large data sets.Experience with Azure, AWS or GCP is a plusMicrosoft Azure Certification is a plus', 'Demonstrated ability in data modeling, ETL development, and data warehousing.', 'Essential Physical Functions:', 'Designing, implementing and supporting a platform that can provide ad-hoc access to large datasets', '**Hands-on experience with modern technologies, including Spark and Scala, are required for this position**', 'Experience leading large-scale data warehousing and analytics projects, including using Azure or AWS technologies – SQL Server, Redshift, S3, EC2, Data-pipeline, Data Lake, Data Factory and other big data technologies', 'Preferred', 'Healthcare domain and data experience', 'Healthcare EDI experience is a plus', 'Industry experience as a Data Engineer or related specialty (e.g., Software Engineer, Business Intelligence Engineer, Data Scientist) with a track record of manipulating, processing, and extracting value from large datasets.', 'Experience with Azure, AWS or GCP is a plus', 'This position will play a key role in building and operating a cloud-based data platform and its pipelines using big data technologies.', 'Microsoft Azure Certification is a plus', 'Minimum Experience:', 'Bachelors or Masters in Computer Science, Engineering, Mathematics, Statistics, or related field', 'Education/Licensure:', 'Experience with Big Data Technologies (NoSQL databases, Hadoop, Hive, Hbase, Pig, Spark, Elasticsearch etc.)', 'Preferred:', 'Alignment Healthcare was founded with a mission to revolutionize health care with a serving heart culture. Through its unique integrated care delivery models, deep physician partnerships and use of proprietary technologies, Alignment is committed to transforming health care one person at a time.', 'While performing the duties of this job, the employee is regularly required to talk or hear. The employee regularly is required to stand, walk, sit, use hand to finger, handle or feel objects, tools, or controls; and reach with hands and arms.', 'We are experiencing rapid growth (backed by top private equity firms), our Data Services and BI team is looking for the best and brightest leaders. Data drives the way we make decisions. We love our customers and understanding them better makes it possible to provide the best clinical outcome and care experience.', 'As a Data Engineer, you will develop a new data engineering platform that leverage a new cloud architecture, and will extend or migrate our existing data pipelines to this architecture as needed. You will also be assisting with integrating the SQL data warehouse platform as our primary processing platform to create the curated enterprise data model for the company to leverage. You will be part of a team building the next generation data platform and to drive the adoption of new technologies and new practices in existing implementations. You will be responsible for designing and implementing the complex ETL pipelines in cloud data platform and other solutions to support the rapidly growing and dynamic business demand for data, and use it to deliver the data as service which will have an immediate influence on day-to-day decision making.', 'Work Environment:', '\xa0', 'Building and migrating the complex ETL pipelines from on premise system to cloud and Hadoop/Spark to make the system grow elastically', 'Alignment Healthcare is a data and technology driven healthcare company focused partnering with health systems, health plans and provider groups to provide care delivery that is preventive, convenient, coordinated, and that results in improved clinical outcomes for seniors.', 'Other:', 'By becoming a part of the Alignment Healthcare team, you will provide members with the quality of care they truly need and deserve. We believe that great work comes from people who are inspired to be their best. We have built a team of talented and experienced people who are passionate about transforming the lives of the seniors we serve. In this fast-growing company, you will find ample room for growth and innovation alongside the Alignment community.', 'The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.', 'Experience in using Python, .net, Java and/or other data engineering languages', 'Work Environment:The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job.\xa0Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.\xa0\xa0\xa0\xa0\xa0\xa0\xa0', 'Modelling data and metadata to support machine learning and AI', 'Experience building data products incrementally and integrating and managing datasets from multiple sources', 'Linux/UNIX including to process large data sets.', 'Minimum Requirements:', 'To perform this job successfully, an individual must be able to perform each essential duty satisfactorily.\xa0The requirements listed below are representative of the knowledge, skill, and/or ability required.\xa0Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.', 'Interfacing with business customers, gathering requirements and developing new datasets in data platform', 'The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job.\xa0Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.\xa0\xa0\xa0\xa0\xa0\xa0\xa0', '(May include but are not limited to)', 'Experience leading large-scale data warehousing and analytics projects, including using Azure or AWS technologies – SQL Server, Redshift, S3, EC2, Data-pipeline, Data Lake, Data Factory and other big data technologiesExperience providing technical leadership and mentor other engineers for the best practices on the data engineering space', 'Experience providing technical leadership and mentor other engineers for the best practices on the data engineering space', 'Demonstrable track record dealing well with ambiguity, prioritizing needs, and delivering results in an agile, dynamic startup environment', 'The employee frequently lifts and/or moves up to 10 pounds. Specific vision abilities required by this job include close vision and the ability to adjust focus.', 'Knowledge and experience of SQL Sever and SSIS.', '2+ years relevant experience in cloud based data engineering.', 'Data Engineer', 'Extracting and combining data from various heterogeneous data sources', 'Minimum Experience:2+ years relevant experience in cloud based data engineering.Demonstrated ability in data modeling, ETL development, and data warehousing.Data Warehousing Experience with SQL Server, Oracle, Redshift, Teradata, etc.Experience with Big Data Technologies (NoSQL databases, Hadoop, Hive, Hbase, Pig, Spark, Elasticsearch etc.)Experience in using Python, .net, Java and/or other data engineering languagesEducation/Licensure:Bachelors or Masters in Computer Science, Engineering, Mathematics, Statistics, or related fieldOther:Knowledge and experience of SQL Sever and SSIS.Excellent communication, analytical and collaborative problem-solving skillsPreferred:Healthcare domain and data experienceHealthcare EDI experience is a plus', 'General Duties/Responsibilities:', 'Interfacing with business customers, gathering requirements and developing new datasets in data platformBuilding and migrating the complex ETL pipelines from on premise system to cloud and Hadoop/Spark to make the system grow elasticallyIdentifying the data quality issues to address them immediately to provide great user experienceExtracting and combining data from various heterogeneous data sourcesDesigning, implementing and supporting a platform that can provide ad-hoc access to large datasetsModelling data and metadata to support machine learning and AI', 'While performing the duties of this job, the employee is regularly required to talk or hear. The employee regularly is required to stand, walk, sit, use hand to finger, handle or feel objects, tools, or controls; and reach with hands and arms.The employee frequently lifts and/or moves up to 10 pounds. Specific vision abilities required by this job include close vision and the ability to adjust focus.', 'API development experience is a plus']",Mid-Senior level,Full-time,Information Technology,Hospital & Health Care,2021-03-24 13:05:10
Data Engineer,Keyrus,New York City Metropolitan Area,2 days ago,29 applicants,"['', 'Any understanding of IT processes and systems (like Alteryx server) is highly preferred', 'At least 3+ years working with ETL related technologies such as Alteryx, SSIS, Talend, ODI, and etc.Strong knowledge of data manipulation, data blending, and wrangling, with experience in data querying (if you can write complex queries and nested queries, that’d be great!)Strong knowledge of relational databases such as MSSQL and Oracle\xa0Any understanding of IT processes and systems (like Alteryx server) is highly preferredMinimum of a Bachelor’s degree in Computer Science, Information Systems, IT, or a related field', 'Strong knowledge of data manipulation, data blending, and wrangling, with experience in data querying (if you can write complex queries and nested queries, that’d be great!)', 'Although the role will be utilizing Alteryx, we do not require Alteryx experience. We are looking for candidates who have strong backend/database backgrounds (i.e. SQL Developers, Database Analysts, ETL Developers) and are passionate about data and are excited to learn new technologies!', 'At least 3+ years working with ETL related technologies such as Alteryx, SSIS, Talend, ODI, and etc.', 'We are looking for a Data Engineer to work with our client, who is one of the largest food and beverage retailers in the U.S.\xa0The role will be responsible for continuing to develop the analytics infrastructure for the company’s various business teams from finance to supply chain, as they revamp and migrate their legacy systems to Alteryx. As there are no dedicated business analysts on this team, this role will wear multiple hats and work closely with business stakeholders to gather requirements and translate business logic into technical specifications for development.', '**This is a full time role with our client (not Keyrus). We are unfortunately unable to work with candidates who now or in the future will require work sponsorship.**', 'Minimum of a Bachelor’s degree in Computer Science, Information Systems, IT, or a related field', '\ufeff', 'Strong knowledge of relational databases such as MSSQL and Oracle\xa0', 'Requirements:']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Larry H. Miller Group of Companies,"Sandy, UT",4 weeks ago,Be among the first 25 applicants,[],Entry level,Full-time,Information Technology,Automotive,2021-03-24 13:05:10
Data Engineer,Womply,"Lehi, UT",6 days ago,131 applicants,"['', 'Healthy Skepticism to challenge the status quo so we can improve', 'Team-oriented, self-motivated, success-driven, roll-up-your-sleeves attitude', 'Our mission is to help local businesses thrive in a digital world. Founded in 2011, Womply is a local commerce platform that provides apps, APIs, marketing, and financial tools to make local commerce happen for over 1M American businesses and their customers. All of Womply’s products and services are powered by the Womply Commerce Graph, a proprietary data asset that offers the most complete view of local commerce.', 'Machine Learning', 'Individuals seeking employment at Womply are considered without regard to race, color, religion, national origin, age, sex, marital status, ancestry, physical or mental disability, veteran status, or sexual orientation.', '5+ years in software engineeringExperience with Data Warehousing, Architecting Pipelines, and Data ModelingTeam-oriented, self-motivated, success-driven, roll-up-your-sleeves attitudeStrong intellectual curiosity and demonstrated ability to understand and question the dataHealthy Skepticism to challenge the status quo so we can improveTechnically proficient in:\xa0Languages - Python / SQL Technologies - Snowflake, AWS, Airflow', 'Vision - Your and your team will leverage our data foundation to design and implement innovative solutions to our hardest data problems', 'You will work with a team of Data Engineers to deliver large-scale projects, set and drive roadmap execution through resource planning and allocation. Your focus is to help us evolve our data-driven philosophy and become a world-class data organization. Your team owns the design, execution, and ongoing support of critical data warehousing projects enabling accurate reporting and advanced analytics for all of Womply’s internal business units.', 'You will have to be self-sufficient - we are a startup, so everyone might do a bit of everything to get things done. We look for people who take pride in their work, execute on it, and deliver phenomenal results.', 'You must have:', 'Technically proficient in:\xa0', 'In order to be successful in this role, you will be responsible for:', 'Technologies - Snowflake, AWS, Airflow', 'Nice to Have:', 'Languages - Python / SQL ', 'Experience with Data Warehousing, Architecting Pipelines, and Data Modeling', 'The Data Team at Womply advances the state of our data, and empowers the company to make better decisions from our data. We’re seeking a talented and motivated Data Engineer to join our team. As a Data Engineer, you will hold the keys to the infrastructure that powers our current and future Data Products. Our team of data engineers are helping us build and leverage the latest technologies to tap into our firehose of data to open up new paths for analysis and discovery. You will have the opportunity to make a big impact, and work with extremely talented peers on a fast paced, high energy team.', '\xa0', 'Execution - Building and maintaining the data pipelines from various data sources, while maintaining high accuracy, consistency, and reliability', '5+ years in software engineering', 'Data Engineer ', 'PLEASE NOTE - Direct applicants ONLY. Any recruiter/3rd party submissions we receive will be considered a gift.', 'Data Science', 'Come build something amazing at Womply', 'Strong intellectual curiosity and demonstrated ability to understand and question the data', 'Data ScienceMachine Learning', 'We’re a fanatically values-based company with $50 million raised to accelerate our growth. Womply is a remote-first company with one physical office in Lehi, Utah. We’re always looking for top talent in product, engineering, DevOps, design, data science, sales, marketing, business development, account management, and more. If you want to make a big impact, let’s talk. Learn more at www.womply.com/careers.\xa0', 'Partnership - You establish strong relationships with business and technical leaders across the organization', 'Vision - Your and your team will leverage our data foundation to design and implement innovative solutions to our hardest data problemsExecution - Building and maintaining the data pipelines from various data sources, while maintaining high accuracy, consistency, and reliabilityPartnership - You establish strong relationships with business and technical leaders across the organization']",Mid-Senior level,Full-time,Information Technology,Internet,2021-03-24 13:05:10
Data Engineer,The Motley Fool,"Alexandria, VA",3 weeks ago,94 applicants,"['', 'Experience supporting and working with cross-functional teams in a dynamic environment.', 'Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.', 'Build processes supporting data transformation, data structures, metadata, dependency and workload management.', 'Preferred Qualifications', 'Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.', 'Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.', 'Primary Responsibilities', 'Assemble large, complex data sets that meet functional / non-functional business requirements.', 'Ability to work with stakeholders to translate business requirements into technical requirements. ', 'Create and maintain optimal data pipeline architecture.Assemble large, complex data sets that meet functional / non-functional business requirements.Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, and re-designing infrastructure for greater scalability.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL.Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.Work with data and analytics experts to strive for greater functionality in our data systems.', 'Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.', 'Experience building and optimizing ‘Big Data’ data pipelines, architectures, and data sets.', 'Experience with some cloud services like Azure and AWS.', 'Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.', 'Description', 'Experience with relational SQL databases.', 'Experience with streaming tools like Kafka/Kinesis and Spark Structured Streaming a plus.', 'Experience with serverless technologies like AWS Lambda a plus.', 'Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL.', 'Experience with relational SQL databases.Experience with some cloud services like Azure and AWS.Experience with object-oriented/object function scripting languages like Python.Experience with streaming tools like Kafka/Kinesis and Spark Structured Streaming a plus.Experience with serverless technologies like AWS Lambda a plus.Experience working with or understanding formal ETL tools like SSIS a plus. ', 'Experience working with or understanding formal ETL tools like SSIS a plus. ', 'Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, and re-designing infrastructure for greater scalability.', 'Strong project management and organizational skills.', 'Strong analytical skills and detailed oriented.', 'A successful history of manipulating, processing and extracting value from large disconnected datasets.', 'Work with data and analytics experts to strive for greater functionality in our data systems.', 'Experience with object-oriented/object function scripting languages like Python.', 'Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.Experience building and optimizing ‘Big Data’ data pipelines, architectures, and data sets.Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.Strong analytical skills and detailed oriented.Build processes supporting data transformation, data structures, metadata, dependency and workload management.A successful history of manipulating, processing and extracting value from large disconnected datasets.Strong project management and organizational skills.Experience supporting and working with cross-functional teams in a dynamic environment.Ability to work with stakeholders to translate business requirements into technical requirements. ', 'They Should Also Have Experience Using The Following Software/tools', 'Create and maintain optimal data pipeline architecture.']",Entry level,Full-time,Information Technology,Online Media,2021-03-24 13:05:10
Data Engineer,TEKsystems,"Allentown, PA",1 week ago,Be among the first 25 applicants,"['', 'Description', 'Top Skills Details', ' Experience leading a Data Warehouse and/or Data Engineering team. Must be able to build a data lake and data warehouse strategy', 'About TEKsystems', ' Cloud Analytics experience (vendor agnostic - can be AWS, GCP, Azure or Snowflake)', ' Data engineering experience - data ingestion and building data pipelines', ' Experience leading a Data Warehouse and/or Data Engineering team. Must be able to build a data lake and data warehouse strategy Data engineering experience - data ingestion and building data pipelines Cloud Analytics experience (vendor agnostic - can be AWS, GCP, Azure or Snowflake)']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Lorien,Greater St. Louis,5 days ago,Over 200 applicants,"['', 'data analysis skills utilizing either Spotfire and/or Tableau', 'Visualization-data analysis skills utilizing either Spotfire and/or Tableau', 'Use Python and SQL to manipulate data to support the projectDevelop key metrics and data visualizations that will enable prescriptive operations and optimize pipeline processes and transparency through increased data visibility ', 'Understanding of Database systems and management of large data sets ', ""Bachelor's degree or higher in Computer Science, Data Science, Data Analytics, Ag/Life Sciences, or related fieldStrong Python skills \xa0with the ability to manipulate dataStrong SQL querying skillsGood data management skillsVisualization-data analysis skills utilizing either Spotfire and/or TableauIn addition to strong technical skills, the successful candidate must excel in time management; and have a proven track record in problemsolution leveraging complex data. Ideal candidates will have a strong developmental desire to continually learn new technical skills to remain on the cutting edge of capabilities for data analysis and visualization."", 'Database systems and management of large data sets ', 'Considered a plus for the Data Engineer', 'Strong Python skills \xa0with the ability to manipulate data', '\xa0Lorien ', 'Understanding of Database systems and management of large data sets Experience working with multidisciplinary research and field teamsAbility to construct a storyline, perform analyses, and create presentations to effectively communicate complex concepts and recommendationsExperience and skills with project management and communication of strategy with both technical and non-technical audiences.Info: Develop Business Intelligence Visualization SQL R Metrics Create Tools Manipulation Data Acquisition is a plus', 'manipulate data', 'Considered a plus for the Data Engineer: ', 'Please apply online or email ian.mulloy@lorienglobal.com.\xa0If you don’t meet these requirements, but are interested in other Impellam NA, Corestaff Services or Lorien opportunities, please register with us online at ess.impellam.com.', 'Data Enginee', 'Responsibilities for the Data Engineer:', 'Strong Python', 'In addition to strong technical skills, the successful candidate must excel in time management; and have a proven track record in problemsolution leveraging complex data. ', 'Ideal candidates will have a strong developmental desire to continually learn new technical skills to remain on the cutting edge of capabilities for data analysis and visualization.', 'solution leveraging complex data', 'Good data management skills', 'Experience and skills with project management and communication of strategy with both technical and non-technical audiences.', ' SQL', ' strong Python and SQL skills', ""Bachelor's degree or higher in Computer Science, Data Science, Data Analytics, Ag/Life Sciences, or related field"", 'Ability to construct a storyline, perform analyses, and create presentations to effectively communicate complex concepts and recommendations', 'Use Python and SQL to manipulate data to support the project', 'Due to contractual obligations candidates must be on our W2.\xa0Depending on skill level candidates can work 100% remote, but will need to travel to the office once every few months.', 'Data Engineer', 'Develop key metrics and data visualizations that will enable prescriptive operations and optimize pipeline processes and transparency through increased data visibility ', 'Strong SQL querying skills', 'Info: Develop Business Intelligence Visualization SQL R Metrics Create Tools Manipulation Data Acquisition is a plus', 'Data Engineer/Data Analyst with strong Python and SQL skills is needed manipulate data for developing and deploying prototype solutions and working with IT and data teams to deliver robust solutions in a continuously evolving technical environment for a dynamic company. This is a long term contract; potential contract to hire position.\xa0Due to contractual obligations candidates must be on our W2.\xa0Depending on skill level candidates can work 100% remote, but will need to travel to the office once every few months.', '\xa0Lorien is an Equal Opportunity Employer - All qualified applicants will receive consideration without regard to race, color, religion, gender, national origin, age, disability, veteran status, or any other factor determined to be unlawful under applicable law.', 'Develop key metrics and data visualizations', 'Requirements for the Data Engineer:', 'Python and SQL to manipulate data', 'Experience working with multidisciplinary research and field teams']",Mid-Senior level,Contract,Information Technology,Computer Software,2021-03-24 13:05:10
Data Engineer,Neuberger Berman,"Chicago, IL",21 hours ago,29 applicants,"['', 'Build automated pipelines for developing, testing, and deploying data analytics applications', 'Summary:', 'Experience with DevOps tool-sets such as Confluence, JIRA, and Git.', 'Experience working within an Agile software development framework with strongly disciplined approach to software development', 'Set up strong foundational procedures, guidelines, and standards for data analytics and processing', 'Experience working with financial data sets a plus', 'Responsibilities:', 'Process, clean, and verify the integrity of data used for analysis', 'Bachelor degree or equivalent in Computer Science, Data Science, or Engineering', 'Qualifications:', 'We are looking for a hands-on data developer / engineer with strong MS SQL Server experiences. The candidate requires knowledge of RDBMS modeling and development experiences as well as some data science technologies.', 'A team-player who is eager to learn with strong analytical and communications skills', 'Conduct ad-hoc analysis and present results in a clear manner', '5 years of experience of MS SQL Server with knowledge of OLTP and Dimensional Modeling design and development. Familiar with MS SQL Server performance optimization techniques.', 'Neuberger Berman is an equal opportunity/affirmative action employer. The Firm and its affiliates do not discriminate in employment because of race, creed, national origin, religion, age, color, sex, marital status, sexual orientation, gender identity, disability, citizenship status or protected veteran status, or any other characteristic protected by local, state, or federal laws, rules, or regulations.\xa0If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact onlineaccommodations@nb.com.', 'Experience with Tableau \xa0reporting and analytical tools', 'Experience with T-SQL and various of MS SQL 2012+ advance programming features.', 'If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact onlineaccommodations@nb.com.', 'Work closely with business stakeholders to understand their analytics, and construct efficient and scalable algorithms to implement them', 'Integrate new software tools for data analysis into the existing tool-set', 'Bachelor degree or equivalent in Computer Science, Data Science, or Engineering5 years of experience of MS SQL Server with knowledge of OLTP and Dimensional Modeling design and development. Familiar with MS SQL Server performance optimization techniques.Experience with T-SQL and various of MS SQL 2012+ advance programming features.Experience with Tableau \xa0reporting and analytical toolsExperience working with financial data sets a plusExperience with data integration and workflow tools.\xa0 \xa0Hands-on with large data pre-processing (ETL), and data cleansing.Experience with DevOps tool-sets such as Confluence, JIRA, and Git.Experience working within an Agile software development framework with strongly disciplined approach to software developmentExperience with Azure and Snowflake is a plus.A team-player who is eager to learn with strong analytical and communications skills', 'Neuberger Berman is an equal opportunity/affirmative action employer. The Firm and its affiliates do not discriminate in employment because of race, creed, national origin, religion, age, color, sex, marital status, sexual orientation, gender identity, disability, citizenship status or protected veteran status, or any other characteristic protected by local, state, or federal laws, rules, or regulations.', 'Experience with data integration and workflow tools.\xa0 \xa0Hands-on with large data pre-processing (ETL), and data cleansing.', 'Work closely with business stakeholders to understand their analytics, and construct efficient and scalable algorithms to implement themSet up strong foundational procedures, guidelines, and standards for data analytics and processingIntegrate new software tools for data analysis into the existing tool-setBuild automated pipelines for developing, testing, and deploying data analytics applicationsConduct ad-hoc analysis and present results in a clear mannerProcess, clean, and verify the integrity of data used for analysis', 'Experience with Azure and Snowflake is a plus.']",Associate,Full-time,Finance,Financial Services,2021-03-24 13:05:10
"Data Engineer, Product Analytics",Carvana,"Phoenix, AZ",2 days ago,105 applicants,"['', 'Additionally, You Will', ' Full-Time Position with a competitive salary. Medical, Dental, and Vision benefits. 401K with company match. A multitude of perks including student loan payments, discounts on vehicles, benefits for your pets, and much more. A great wellness program to keep you healthy and happy both physically and mentally. Access to opportunities to expand your skill set and share your knowledge with others across the organization. A company culture of promotions from within, with a start-up atmosphere allowing for varied and rapid career development. A seat in one of the fastest-growing companies in the country. ', 'You are fearless of being un-knowledgeable about a particular subject area/technology; you yearn to learn and ask questions, with a strong desire to grow through challenging work and new technologies', 'What You Should Have', 'Search and browse our inventory of over 20,000 vehicles that we own and certify.', 'Other duties as assigned.', 'Must be able to read, write, speak and understand English.', '401K with company match.', 'A multitude of perks including student loan payments, discounts on vehicles, benefits for your pets, and much more.', ' Search and browse our inventory of over 20,000 vehicles that we own and certify. Narrow down search results using highly intelligent filtering tools/components. View vehicle details, Carfax reports, and 360 rotating studio images for every vehicle. Secure financing in minutes using Carvana’s in-house service or their own bank. Interact with GUI components to easily customize loan length, down payment, and monthly payment. Generate, upload, and eSign all documents online (no ink necessary). Schedule front door delivery or pick up at one of our vending machines. Trade-in their existing vehicle or just sell it to Carvana (no purchase necessary). ', 'A great wellness program to keep you healthy and happy both physically and mentally.', 'You have used Python for basic to advanced data processing tasks', 'Secure financing in minutes using Carvana’s in-house service or their own bank.', 'Take part in cross-functional project teams with data scientists, product engineers and analysts, among others.', 'Move and shape structured and semi-structured data between various data platforms, including MS SQL Server, AWS S3, Azure DataLake, Azure Blob and Azure SQL DB', 'Interact with GUI components to easily customize loan length, down payment, and monthly payment.', 'Use tools like SSDT, Python and Databricks to build data pipelines from our production systems into easily digestible objects for reporting', 'You actively keep up with industry best practices', 'You have experience working with streaming data pipelines', ' You have experience coding in JAVA You have experience working with streaming data pipelines ', 'You have a strong understanding of data integration concepts, business intelligence, data warehousing and working with large data', 'You have worked with and feel comfortable with MS SQL Server', 'About Carvana', 'Medical, Dental, and Vision benefits.', 'Become familiar with the multitude of ways data flows through our systems in order to contribute creative problem solving solutions, as well as to ensure the integrity of various data stores and ETLs', 'Narrow down search results using highly intelligent filtering tools/components.', 'Your attitude rocks', 'Generate, upload, and eSign all documents online (no ink necessary).', 'You have experience coding in JAVA', 'A company culture of promotions from within, with a start-up atmosphere allowing for varied and rapid career development.', ' Use tools like SSDT, Python and Databricks to build data pipelines from our production systems into easily digestible objects for reporting Move and shape structured and semi-structured data between various data platforms, including MS SQL Server, AWS S3, Azure DataLake, Azure Blob and Azure SQL DB Become familiar with the multitude of ways data flows through our systems in order to contribute creative problem solving solutions, as well as to ensure the integrity of various data stores and ETLs Utilize test and production environments, adhering to change management requirements for system implementations Participate in team design sessions and code reviews Take part in cross-functional project teams with data scientists, product engineers and analysts, among others. Work with Product and Analytics leaders to define business intelligence needs and translate them into functional requirements  Work with end users and other IT teams to resolve operational issues and mitigate risks as applicable Help architect, design and prototype solutions to support business strategies and deliver business value Other duties as assigned. ', 'You hold (or have the equivalent experience to) a degree in Management Information Systems, Computer Science or a related field', 'Schedule front door delivery or pick up at one of our vending machines.', 'Work with Product and Analytics leaders to define business intelligence needs and translate them into functional requirements ', ' Must be able to read, write, speak and understand English. ', 'View vehicle details, Carfax reports, and 360 rotating studio images for every vehicle.', 'You have an extensive background in SQL and relational databases', 'You have solid understanding of indexing concepts', 'A seat in one of the fastest-growing companies in the country.', 'You are excited for performance tuning and process streamlining (woo!)', 'About The Team And Position', 'Access to opportunities to expand your skill set and share your knowledge with others across the organization.', 'Utilize test and production environments, adhering to change management requirements for system implementations', 'Participate in team design sessions and code reviews', 'Work with end users and other IT teams to resolve operational issues and mitigate risks as applicable', 'Help architect, design and prototype solutions to support business strategies and deliver business value', 'You’re a forward thinker; you will predict how future company evolution will affect processes implemented today', 'You practice excellent written and verbal communication', 'You know what a query plan is and how to view one', 'Other Requirements', 'Full-Time Position with a competitive salary.', 'Trade-in their existing vehicle or just sell it to Carvana (no purchase necessary).', 'What You’ll Be Doing', 'What We’ll Offer In Return', ' You hold (or have the equivalent experience to) a degree in Management Information Systems, Computer Science or a related field You have an extensive background in SQL and relational databases You have used Python for basic to advanced data processing tasks You have a strong understanding of data integration concepts, business intelligence, data warehousing and working with large data You have worked with and feel comfortable with MS SQL Server You know what a query plan is and how to view one You have solid understanding of indexing concepts You are excited for performance tuning and process streamlining (woo!) You actively keep up with industry best practices You’re a forward thinker; you will predict how future company evolution will affect processes implemented today You are fearless of being un-knowledgeable about a particular subject area/technology; you yearn to learn and ask questions, with a strong desire to grow through challenging work and new technologies You practice excellent written and verbal communication Your attitude rocks Bonus: ', 'Legal stuff', 'Bonus: ']",Entry level,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
"Data Engineer/Senior Data Engineer, IT Applications",American Airlines,"Fort Worth, TX",4 weeks ago,Be among the first 25 applicants,"['', 'Experience in Cloud; IBM or Azure', 'Develop POC’s when necessary', 'DevOps CI/CD using Jenkins or other competing tools in the market', 'Skills, Licenses & Certifications', ""Bachelor's degree in Computer Science, Computer Engineering, Technology, Information Systems (CIS/MIS), Engineering or related technical discipline, or equivalent experience/training3+ years’ of hands on experience in data warehouse development including framework and application solutions leveraging DataStage and Teradata3+ years’ experience with UNIX Shell Scripting and SQLExperience delivering data solutions for, or within, an analytic or business intelligence environmentSource code management in Git or Subversion"", 'Interact with business and technologies peers', 'Interpret business data and data access requirements', 'Complete source to target mappings', ""All you'll need for success"", 'Contribute to continuous improvement of On Premise Data warehouse and BigData applications and the Cloud applications', 'Works in conjunction with Product Owner and Agile teamInteract with business and technologies peersInterpret business data and data access requirementsProvide appropriate estimates on development tasks and capacity requirementsComplete source to target mappingsDevelop, code, test, and implement data solutions according to business requirementsDesign job/jobstream flows via scheduling tool, ensuring proper dependencies within enterprise schedulesProvide assistance and resolution for any production related issuesBe accountable for application performance monitoring and tuningContribute to continuous improvement of On Premise Data warehouse and BigData applications and the Cloud applicationsDevelop POC’s when necessary', 'Intro', 'Experience delivering data solutions for, or within, an analytic or business intelligence environment', 'Works in conjunction with Product Owner and Agile team', '3+ years’ of hands on experience in data warehouse development including framework and application solutions leveraging DataStage and Teradata', 'Travel Perks: Ready to explore the world? You, your family and your friends can reach 365 destinations on more than 6,800 daily flights across our global network.', ""What You'll Do"", 'Develop, code, test, and implement data solutions according to business requirements', 'Be accountable for application performance monitoring and tuning', 'Source code management in Git or Subversion', 'Strong problem-solving ability with a positive ""can-do"" attitude', 'Additional Benefits: Other great benefits include our Employee Assistance Program, pet insurance and discounts on hotels, cars, cruises and more', 'Demonstrated achievement in developing analytical data layers/applications with large data volume', ""What You'll Get"", 'Demonstrated achievement in developing analytical data layers/applications with large data volumeStrong problem-solving ability with a positive ""can-do"" attitudeDevOps CI/CD using Jenkins or other competing tools in the marketA passion for technology, continuous improvement, quality and helping others grow', 'Preferred Qualifications- Education & Prior Job Experience', 'Health Benefits: On day one, you’ll have access to your health, dental, prescription and vision benefits to help you stay well. And that’s just the start, we also offer virtual doctor visits, flexible spending accounts and more. ', 'Experience in Big Data development including Python, Spark, Scala, Parquet', 'Provide assistance and resolution for any production related issues', 'Minimum Qualifications- Education & Prior Job Experience', 'Design job/jobstream flows via scheduling tool, ensuring proper dependencies within enterprise schedules', '3+ years’ experience with UNIX Shell Scripting and SQL', 'This position is a member of the Information Technology Team, within the RPT Commercial Data Engineering & Business Analytics group supporting Revenue Management Product.The role of the Date Engineer, IT Applications will be to translate business requirements into solutions enabling business value in areas which may include analytics, data pipelines, and complex batch processing of airline commercial data.Success in this role is defined by the ability to leverage strong data application skills to open new capabilities being defined by our business community. You will collaborate with our business partners, fellow developers, and platform architects to achieve these', '5 years of hands on experience in data warehouse development including framework and application solutions leveraging DataStage and Teradata', ""Why you'll love this job"", ""Bachelor's degree in Computer Science, Computer Engineering, Technology, Information Systems (CIS/MIS), Engineering or related technical discipline, or equivalent experience/training"", 'Wellness Programs: We want you to be the best version of yourself - that’s why our wellness programs provide you with all the right tools, resources and support you need.', 'Provide appropriate estimates on development tasks and capacity requirements', 'A passion for technology, continuous improvement, quality and helping others grow', '5 years of hands on experience in data warehouse development including framework and application solutions leveraging DataStage and TeradataExperience in Big Data development including Python, Spark, Scala, ParquetExperience in Cloud; IBM or Azure', '401(k) Program: Available upon hire and, depending on the workgroup, employer contributions to your 401(k) program are available after one year.', 'Feel Free to be yourself at American', 'Travel Perks: Ready to explore the world? You, your family and your friends can reach 365 destinations on more than 6,800 daily flights across our global network.Health Benefits: On day one, you’ll have access to your health, dental, prescription and vision benefits to help you stay well. And that’s just the start, we also offer virtual doctor visits, flexible spending accounts and more. Wellness Programs: We want you to be the best version of yourself - that’s why our wellness programs provide you with all the right tools, resources and support you need.401(k) Program: Available upon hire and, depending on the workgroup, employer contributions to your 401(k) program are available after one year.Additional Benefits: Other great benefits include our Employee Assistance Program, pet insurance and discounts on hotels, cars, cruises and more']",Not Applicable,Full-time,Information Technology,Airlines/Aviation,2021-03-24 13:05:10
Data Engineer,Benefits Data Trust,"Philadelphia, PA",3 weeks ago,Be among the first 25 applicants,"['', ' Engage with colleagues and collaborators using curiosity, critical thinking, a drive to completion, empathy, and a focus on impact', 'Data Encapsulation & Transfer methodologies ', 'Data Encapsulation & Transfer methodologies – understands standards for file formats and transfer methods', 'Successful candidates will demonstrate the following through their experience (typically 3-5 years as a Data Engineer):', 'About BDT', ' Build out our new GCP data platform and collaborate on architectural patterns for it with the Data Engineering team', 'Automation, monitoring, and alerting – creating these tools based on existing designs and frameworks; resolving bugs and issues', 'Build out our new GCP data platform', 'Due to COVID-19, BDT is operating under a remote-working protocol, following governmental requirements and CDC guidelines. ', 'Director of Data Science.', ' Support the development of machine learning models with productionizing, monitoring and alerting tools', ' Follow existing data access and performance design standards for the data platform, software engineering, and all products and services accessing BDT information', 'Also interested in relevant experience including:', "" Consult to software engineers on data-related changes to BDT's suite of software applications, including schema/model design, table structure, and data collection "", 'o Machine learning techniques, productionizing machine learning models, and/or creating models', 'Cloud engineering – working towards certification on any of the major hyperscale cloud platforms', 'Cloud engineering', 'Data Modeling and Warehousing', 'Data Modeling and Warehousing – proficient understanding of relational data structures and schemas; some familiarity with semi-structured, unstructured (big data) schemas', 'Systems Engineering', 'Implement continuous improvements ', 'o Experience with BI implementations/uplifts (we currently use Looker) and/or Data Governance models and methods o Machine learning techniques, productionizing machine learning models, and/or creating models', 'Automation, monitoring, and alerting ', ' Write, update, and maintain ETL jobs across our data pipelines (mostly in Airflow)', "" Build out our new GCP data platform and collaborate on architectural patterns for it with the Data Engineering team Support the development of machine learning models with productionizing, monitoring and alerting tools Write, update, and maintain ETL jobs across our data pipelines (mostly in Airflow) Implement continuous improvements using our existing tools/technologies, which include SQL, Airflow, Python, Docker/Kubernetes, and others such as Terraform and Apache Beam. May also be expected to research and select other tools when the situation demands Collaborate with internal customers to identify ongoing platform improvements (teams including Analytics, Projects, Policy, Software Engineering, and others throughout the organization) Consult to software engineers on data-related changes to BDT's suite of software applications, including schema/model design, table structure, and data collection  Engage with colleagues and collaborators using curiosity, critical thinking, a drive to completion, empathy, and a focus on impact Follow existing data access and performance design standards for the data platform, software engineering, and all products and services accessing BDT information"", 'Support the development of machine learning models ', 'Consult to software engineers on data-related changes ', 'Cloud-based Solution Implementation, of data platforms and infrastructure, including event-driven architectures, microservices and pattern design, supporting compliance and regulated environments (including PII and PHI)', ' Collaborate with internal customers to identify ongoing platform improvements (teams including Analytics, Projects, Policy, Software Engineering, and others throughout the organization)', 'Engage with colleagues and collaborators ', 'Collaborate with internal customers ', 'Write, update, and maintain ETL jobs ', 'o Experience with BI implementations/uplifts (we currently use Looker) and/or Data Governance models and methods ', 'Data Engineer ', ' Implement continuous improvements using our existing tools/technologies, which include SQL, Airflow, Python, Docker/Kubernetes, and others such as Terraform and Apache Beam. May also be expected to research and select other tools when the situation demands', 'Communication and Relationship-building – ', 'Cloud-based Solution Implementation', 'Follow existing data access and performance design standards ', 'Workflow and pipeline development to ensure reliability, availability, and consistency', ' Communication and Relationship-building – with technical peers and some stakeholders Cloud-based Solution Implementation, of data platforms and infrastructure, including event-driven architectures, microservices and pattern design, supporting compliance and regulated environments (including PII and PHI) Workflow and pipeline development to ensure reliability, availability, and consistency Systems Engineering – on-system service management, typically in *nix environments Data Modeling and Warehousing – proficient understanding of relational data structures and schemas; some familiarity with semi-structured, unstructured (big data) schemas Automation, monitoring, and alerting – creating these tools based on existing designs and frameworks; resolving bugs and issues Cloud engineering – working towards certification on any of the major hyperscale cloud platforms Data Encapsulation & Transfer methodologies – understands standards for file formats and transfer methods Also interested in relevant experience including: ', 'Communication and Relationship-building – with technical peers and some stakeholders', 'Workflow and pipeline development ', 'Benefits Data Trust (BDT)', 'Systems Engineering – on-system service management, typically in *nix environments']",Entry level,Full-time,Information Technology,Nonprofit Organization Management,2021-03-24 13:05:10
Data Engineer,Emergent Professional Resources L.P. (EPR),"Houston, TX",5 days ago,35 applicants,"['We are HIRING a Data Engineer to join the development team of a local real estate company here in Houston. Working collectively with other developers, business analysts and specialist, the Data Engineer will provide solutions relating to database development and reporting for the company.', '\xa0At this time, we are considering local candidates only and not able to work C2C on this role. If you or someone you know is interested, please reach out to me directly at mfitzwater@eprinc.com', 'Qualifications:', 'Excellent written and verbal communication', '3-5 years of experience relating to data integration, analysis and reporting ', 'Bachelors degree or Masters in Computer Science or related', '3-5 years of experience relating to data integration, analysis and reporting Experience with .NET framework and C#Experience with a variety of reporting tools, and integration tools ( SSRS, Power BI, SSIS)Excellent written and verbal communicationBachelors degree or Masters in Computer Science or related', 'mfitzwater@eprinc.com', 'Experience with a variety of reporting tools, and integration tools ( SSRS, Power BI, SSIS)', 'Experience with .NET framework and C#', '\xa0']",Mid-Senior level,Full-time,Information Technology,Real Estate,2021-03-24 13:05:10
Data Engineer,IntegriChain,Greater Philadelphia,4 weeks ago,27 applicants,"['', 'Maintain, improve, and develop expertise in existing production data, models, and algorithms.', 'Define, document, and maintain a data dictionary including:\xa0data definitions, data sources, business meaning and usage of information.', 'Duties:', 'Identify and validate opportunities to reuse existing data and algorithms.', '2 - 3+ years of experience building data pipelines and using ETL tools.\xa0Prefer python programming experience.', 'Experience with common GitHub developer practices and paradigms.', 'Experience in building AWS data pipelines using python, S3 data lake is a plus.\xa0', '**Recruiting Agencies:\xa0Please do not send unsolicited resumes to our employees, job listings, or the recruiting team. IntegriChain is not responsible for any fees related to unsolicited resumes.\xa0', 'Share team responsibilities; such as contributing to development of data warehouses and productizing algorithms created by Data Science team members.', 'Develop, and refine both streaming and batch processing data pipeline frameworks.', 'Qualifications and Competencies:', 'Knowledge of aws services and airflow is a plus.\xa0', 'Knowledge of redshift or any other columnar database is prefered.', '1+ years experience developing modern, industry standard big data frameworks with AWS or other cloud services.', '\xa0', '\ufeffMission:', 'Experience working with agile methodologies and cross-functional teams.\xa0', 'Develop, support, and refine new data pipelines, data models, business logic, data schemas as code, and analytics to product specifications.', 'Prototype and optimize data type checks to ensure data uniformity prior to load.', 'Knowledge of speciality pharmaceutical and\xa0retail pharmacy is a plus.', 'Learn and utilize\xa0business data domain knowledge and its correlation to\xa0underlying data sources.', '3+ years of experience in at least one basic relational database platform (sql server, oracle, postgres, mysql) and languages (PL/SQL, SQL).', 'Deliver modern data pipelines and create custom data extracts that meet the needs of both internal and external customers.', 'Collaborate on\xa0design and implementation of data standardization procedures.', 'Please do not send unsolicited resumes to our employees, job listings, or the recruiting team. IntegriChain is not responsible for any fees related to unsolicited resumes.\xa0', ""Bachelor's Degree in technical background\xa0or equivalent work experience.2 - 3+ years of experience building data pipelines and using ETL tools.\xa0Prefer python programming experience.3+ years of experience in at least one basic relational database platform (sql server, oracle, postgres, mysql) and languages (PL/SQL, SQL).1+ years experience developing modern, industry standard big data frameworks with AWS or other cloud services.Experience with common GitHub developer practices and paradigms.Experience working with agile methodologies and cross-functional teams.\xa0Knowledge of redshift or any other columnar database is prefered.Knowledge of aws services and airflow is a plus.\xa0Experience in building AWS data pipelines using python, S3 data lake is a plus.\xa0Knowledge of speciality pharmaceutical and\xa0retail pharmacy is a plus."", '**Recruiting Agencies:\xa0', 'Develop, support, and refine new data pipelines, data models, business logic, data schemas as code, and analytics to product specifications.Prototype and optimize data type checks to ensure data uniformity prior to load.Develop, and refine both streaming and batch processing data pipeline frameworks.Maintain, improve, and develop expertise in existing production data, models, and algorithms.Learn and utilize\xa0business data domain knowledge and its correlation to\xa0underlying data sources.Define, document, and maintain a data dictionary including:\xa0data definitions, data sources, business meaning and usage of information.Identify and validate opportunities to reuse existing data and algorithms.Works with stakeholders to gather requirements on merging, de-duplicating, standardizing data.Collaborate on\xa0design and implementation of data standardization procedures.Share team responsibilities; such as contributing to development of data warehouses and productizing algorithms created by Data Science team members.', ""Bachelor's Degree in technical background\xa0or equivalent work experience."", 'Works with stakeholders to gather requirements on merging, de-duplicating, standardizing data.']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,"The Squires Group, Inc.","Arlington, VA",4 weeks ago,127 applicants,"['', ' Overview ', ' Informatica Axon (preferred) ', ' 3+ years of experience with the following: Informatica Data Quality  Data Warehouse Tools  Data Audit and Profiling  Data Warehouse ETL Testing  Informatica Axon (preferred)  Informatica Enterprise Data Catalog (preferred) ', ' Well versed in the fundamentals of building ETLs and data models ', ' Informatica Enterprise Data Catalog (preferred) ', ' Solid AWS experience is a plus ', 'U.S. Citizens with an Active or Interim Secret Clearance.', ' Informatica Data Quality  Data Warehouse Tools  Data Audit and Profiling  Data Warehouse ETL Testing  Informatica Axon (preferred)  Informatica Enterprise Data Catalog (preferred) ', ' Check out our Referral Program! ', 'Secret Clearance', ' The Squires Group, Inc. is an Equal Opportunity/Affirmative Action Employer. ', ' Per our Federal Government Contract, candidates must be U.S. Citizens with an Active or Interim Secret Clearance ', ' Informatica Data Quality ', ' Golden Rule - We treat our consultants the way we want to be treated: with integrity, professionalism, and trust.', 'Overview ', ' Bachelor’s Degree AND 3 years of professional experience (or 3 additional years in lieu of a degree) ', 'U.S. Citizens with an Active or Interim ', ' We Care About You – We help you meet your career goals and continuously support your efforts in the field.', ' Bachelor’s Degree AND 3 years of professional experience (or 3 additional years in lieu of a degree)  3+ years of experience with the following: Informatica Data Quality  Data Warehouse Tools  Data Audit and Profiling  Data Warehouse ETL Testing  Informatica Axon (preferred)  Informatica Enterprise Data Catalog (preferred)  Well versed in the fundamentals of building ETLs and data models  Solid AWS experience is a plus  Per our Federal Government Contract, candidates must be U.S. Citizens with an Active or Interim Secret Clearance ', ' Our Commitment to You - We offer competitive pay, multi-year projects, and a list of exciting clients.', 'Qualifications', '4 Reasons To Join The Squires Group, Inc.', ' Data Warehouse ETL Testing ', 'Data Engineer', ' Data Audit and Profiling ', ' #Di', 'Arlington, VA', ' Data Warehouse Tools ', ' Work-Life Balance - We work hard; we work smart and have quality time for family and ""life.""']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Pixability,"Boston, MA",2 days ago,72 applicants,"['', '5+ years experience in a Data Engineering or similar role', '5+ years data modeling, optimization, and visualization in a ""big data"" environment', 'Our Award-Winning Culture', 'Who We Are', "" 2+ years in Hive/MapReduce and/or Spark AWS Database or Data Analytics Certification Snowflake (SnowPro) Certification Looker Certification (LookML or Looker Business Analyst) Experience in advertising or marketing technologies, such as Google Ads and YouTube Bachelor's Degree or higher 2+ years of software engineering development using Python or Golang "", 'Must Haves', 'Pixability does not accept/pay fees for unsolicited resumes from third-party agencies/vendors', ' 5+ years experience in a Data Engineering or similar role 5+ years data modeling, optimization, and visualization in a ""big data"" environment 5+ years working with data analytics and BI tools such as Looker Expert knowledge of SQL and a variety of databases Known for extracting value from large data sets and turning them into actionable insights Able to solve the most complex business and customer reporting needs Experience working in an agile team environment Authorized to work in the US or Canada ', 'Experience working in an agile team environment', 'Looker Certification (LookML or Looker Business Analyst)', 'The Team', 'Nice to Haves', 'AWS Database or Data Analytics Certification', 'Able to solve the most complex business and customer reporting needs', 'Known for extracting value from large data sets and turning them into actionable insights', '2+ years of software engineering development using Python or Golang', '2+ years in Hive/MapReduce and/or Spark', 'Snowflake (SnowPro) Certification', 'Expert knowledge of SQL and a variety of databases', 'Experience in advertising or marketing technologies, such as Google Ads and YouTube', 'Authorized to work in the US or Canada', ""Bachelor's Degree or higher"", 'The Role', 'As part of our dedication to the diversity of our workforce, Pixability is committed to Equal Employment Opportunity without regard for race, ethnicity, gender, protected veteran status, disability, sexual orientation, gender identity or religion.', '5+ years working with data analytics and BI tools such as Looker']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer,In-Finite Search Solutions,Greater Cleveland,,N/A,"['', 'Implement processes and systems to monitor data quality, ensuring production data accuracy and assist in data analysis to troubleshoot and resolve data issuesdesign and create AI models for large scale test plansPython, C++ AI, Project Management', 'BS, MA or PhD', 'Responsibilities:', 'Our team is looking for a data driven person who has engineered and supported critical applications in an automotive environment and is experienced in analytics, business intelligence, data warehousing, artificial intelligence and setting up data pipelines across the enterprise. This seasoned professional will need to interface with multiple client groups and IT management, providing them with data solutions, timely and accurate status updates, short-term / long-term plans, release plans, etc. in a clear consistent manner that emphasizes performance metrics, strategic planning, continuous improvement and adherence to industry best practices.', 'Integrate data from a variety of systems into refined data products available to the rest of the enterprise to support both real-time and batch processing.', 'Implement processes and systems to monitor data quality, ensuring production data accuracy and assist in data analysis to troubleshoot and resolve data issues', 'design and create AI models for large scale test plans', 'Review and understand data requirements for operational and analytic projects, with a special emphasis on developing scalable data solutions involving data integration, reporting, analytics and data warehousing.', 'Collaborate with business and IT partners to refine data requirements and perform data analysis activities including data profiling, creation of data dictionaries, data transformation rules and integration requirements.', 'Develop and maintain data pipelines and build out new API integrations to support continuing increases in data volume and complexity.', 'Python, C++ AI, Project Management', 'Project Description:']",Mid-Senior level,Full-time,Analyst,Staffing and Recruiting,2021-03-24 13:05:10
Data Engineer - 21-01599,Infinity Consulting Solutions,"New York, NY",21 hours ago,Be among the first 25 applicants,"['', 'You have experience with continuous integration/deployment and Scala build tool.', 'Proven ability to work with individuals at all organization levels.', 'You have experience with key AWS services, such as EC2, RDS (Postgres, Aurora), Lambda, Athena', 'Experience in agile project development.', ' Code and test reliable and resilient real-time Spark Applications. Learn our architecture and start to contribute immediately as we value and encourage brainstorming and input from everyone on the team. Participate in all phases of the SDLC including but not limited to architecture, technical design and documentation, testing, implementation and product launch. Write unit/integration tests as part of the development initiative providing great test coverage which will enable continuous delivery of code. Implement readable, maintainable, and highly performant Scala/Python code. Use many of the available AWS services to build applications. Create and maintain technical documentation and architecture diagrams. Support projects through their entire lifecycle from analysis to production rollout. Plan and coordinate project schedules, goals, and milestones. Collaborate with the business stake holders on the feature set specifications. Collaborate with team members on the implementation and planning. ', 'Good knowledge of SQL. The ability to recognize when you require input from the DBA team.', 'Write unit/integration tests as part of the development initiative providing great test coverage which will enable continuous delivery of code.', 'Implement readable, maintainable, and highly performant Scala/Python code.', 'Collaborate with the business stake holders on the feature set specifications.', 'Create and maintain technical documentation and architecture diagrams.', 'Participate in all phases of the SDLC including but not limited to architecture, technical design and documentation, testing, implementation and product launch.', 'Nice To Haves', 'Ability to work on concurrent projects, when required.', 'Learn our architecture and start to contribute immediately as we value and encourage brainstorming and input from everyone on the team.', 'Support projects through their entire lifecycle from analysis to production rollout.', 'Plan and coordinate project schedules, goals, and milestones.', 'Spark streaming and optimization', 'Strong Linux knowledge.', 'Code and test reliable and resilient real-time Spark Applications.', 'Support production issues when required.', 'Requirements', 'Must know how to use GIT.', 'Excellent verbal and written communication skills with employees both onsite and in remote locations.', 'Ability to develop a detailed project plan when working on a project.', "" Data Engineering - 3-5 years' experience with AWS, Scala, Spark, Kafka, S3, Python Spark streaming and optimization You have experience with continuous integration/deployment and Scala build tool. Good Debugging Skills, excellent troubleshooting skills Must be self-motivated and willing to learn. You love to stay on top of new technology and share with others. Good knowledge of SQL. The ability to recognize when you require input from the DBA team. Excellent verbal and written communication skills with employees both onsite and in remote locations. Proven ability to work with individuals at all organization levels. Ability to work on concurrent projects, when required. Ability to develop a detailed project plan when working on a project. Support production issues when required. Experience in agile project development. Must know how to use GIT. Strong Linux knowledge. "", 'Collaborate with team members on the implementation and planning.', 'You have worked in the Financial Markets.', ""Data Engineering - 3-5 years' experience with AWS, Scala, Spark, Kafka, S3, Python"", 'Our Successful Candidate Will', 'Good Debugging Skills, excellent troubleshooting skills', 'Experience working with equity financial data (Quotes, Trades, Company Information)', 'Use many of the available AWS services to build applications.', 'Must be self-motivated and willing to learn. You love to stay on top of new technology and share with others.', 'You have worked with large data sets.', 'Job Description', ' You have experience with key AWS services, such as EC2, RDS (Postgres, Aurora), Lambda, Athena Experience working with equity financial data (Quotes, Trades, Company Information) You have worked with large data sets. You have worked in the Financial Markets.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer (Remote),Jun Group,"New York, NY",3 weeks ago,39 applicants,"['', "" Experience working with streaming and batch data processing tools like Apache Beam, Spark, Flink, etc. You've built and maintained an ETL pipeline using a data warehouse like BigQuery or Redshift You are comfortable writing SQL and manipulating large structured or unstructured datasets for analysis Practical knowledge of how to build efficient end-to-end ML workflows Familiarity with AWS or Google Cloud big data products "", 'Macbook Pros and any other equipment you need to work effectively from home', 'Annual personal development budget to attend a conference of your choice', 'Who You Are', 'A highly competitive compensation package', 'Designated time to work on company-related projects you feel strongly about', 'You eagerly dig into complex engineering problems', 'Monthly company events ', ""What You'll Do"", ""You've built and maintained an ETL pipeline using a data warehouse like BigQuery or Redshift"", ' Contribute to exciting greenfield projects Own all things data - including our ETL processes, reporting APIs, and internal dashboards Collaborate with our engineering team to improve our existing machine learning models and tooling Experiment with new tech to find the right tool for the job Use Kanban to manage multiple releases per week Maintain high code quality through code reviews and automated tests ', 'Collaborate with our engineering team to improve our existing machine learning models and tooling', 'You are comfortable writing SQL and manipulating large structured or unstructured datasets for analysis', 'Practical knowledge of how to build efficient end-to-end ML workflows', 'Jun Group will only consider candidates for this position who are currently legally authorized to work in the United States.', 'What We Offer', 'Qualifications', 'Experiment with new tech to find the right tool for the job', 'You want to be part of a small team inside a large company with massive opportunity for growth', 'Paid vacation, work from home, and sick days', 'Familiarity with AWS or Google Cloud big data products', ' A highly competitive compensation package 401k with company match Paid vacation, work from home, and sick days Annual personal development budget to attend a conference of your choice Designated time to work on company-related projects you feel strongly about Macbook Pros and any other equipment you need to work effectively from home Monthly company events  ', 'Experience working with streaming and batch data processing tools like Apache Beam, Spark, Flink, etc.', '401k with company match', 'Maintain high code quality through code reviews and automated tests', 'You enjoy collaboration with other teams including product, biz dev, and our in-house QA team', ' You enjoy a fun, creative, and engaging working atmosphere free of brilliant jerks You want to be part of a small team inside a large company with massive opportunity for growth You enjoy collaboration with other teams including product, biz dev, and our in-house QA team You eagerly dig into complex engineering problems ', 'Use Kanban to manage multiple releases per week', 'Own all things data - including our ETL processes, reporting APIs, and internal dashboards', 'Contribute to exciting greenfield projects', 'You enjoy a fun, creative, and engaging working atmosphere free of brilliant jerks']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer,EPITEC,"Dearborn, MI",6 days ago,70 applicants,"['', 'Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.', 'Build processes supporting data transformation, data structures, metadata, dependency and workload management.', 'POSITION:', 'Dearborn, Michigan', ""Epitec represents the world’s top companies and works to fill their open jobs with the world’s best talent. That’s led to Epitec servicing an impressive list of Fortune 100 companies. We've also won many awards, including one of Crain’s Detroit Business “Cool Places to Work,” and 101 Best & Brightest – local, national and elite winner. And that’s just the beginning, as we work to innovate the way the world thinks about employment."", ""This position will be part of the Data Supply Chain (DSC) product group Data fulfillment team using tools like Attunity, Sqoop etc. The product team's objective is to replicate data from hundreds of database sources within the company to the DSC Hadoop environment and do transformations to make it usable for data scientists.This position will require an individual who has a strong background with multiple database technologies, who is process oriented and has knowledge of Java and expertise in the Hadoop environment. "", 'We started Epitec with a single focus, “Placing People First.” Knowing every good endeavor begins with listening and understanding, we’ve set about challenging every part of the employment process. Bringing the proper connections together for the perfect fit.', 'Experience building and optimizing ""big data"" data pipelines, architectures and data sets.', 'JOB SUMMARY FOR SOFTWARE DEVELOPER:', 'BENEFITS', 'Strong analytic skills related to working with unstructured datasets.', 'JOB TYPE:', 'Long-term contract', 'What is the result?', 'Experience with big data tools: Hadoop, Spark, Kafka, etc.', '\xa0', 'This position will require an individual who has a strong background with multiple database technologies, who is process oriented and has knowledge of Java and expertise in the Hadoop environment. ', 'Experience with relational SQL and NoSQL databases', 'Experience with object-oriented/object function scripting languages: Python, Java', 'LOCATION:', 'Software Developer', 'Advanced working SQL knowledge and experience working with relational databases', 'Experience supporting and working with cross-functional teams in agile mode.', 'Candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, -Statistics, Informatics, Information Systems or another quantitative field', 'How is Epitec different?', 'Why should you choose Epitec?', ""This position will be part of the Data Supply Chain (DSC) product group Data fulfillment team using tools like Attunity, Sqoop etc. The product team's objective is to replicate data from hundreds of database sources within the company to the DSC Hadoop environment and do transformations to make it usable for data scientists."", 'Epitec gets to know our prospective employees, using these insights to locate the perfect placement for you. We are there, every step of the way. Providing a best-in-class compensation package combined with the opportunity to grow financially and personally through your work.', 'REQUIRED EXPERIENCE FOR SOFTWARE DEVELOPER:', 'Epitec', 'Medical, Dental, PTO, Holiday tailored to meet your needs. 401K/Match, $15,000 Life Insurance, Award-Winning Employee Care Program. Established and highly regarded reputation with Green Card and H1 processing.', 'Candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, -Statistics, Informatics, Information Systems or another quantitative fieldExperience with big data tools: Hadoop, Spark, Kafka, etc.Experience with relational SQL and NoSQL databasesExperience with object-oriented/object function scripting languages: Python, JavaAdvanced working SQL knowledge and experience working with relational databasesExperience building and optimizing ""big data"" data pipelines, architectures and data sets.Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.Strong analytic skills related to working with unstructured datasets.Build processes supporting data transformation, data structures, metadata, dependency and workload management.Experience supporting and working with cross-functional teams in agile mode.']",Mid-Senior level,Full-time,Engineering,Information Technology and Services,2021-03-24 13:05:10
Associate Data Engineer,The Hartford,"Remote, OR",3 weeks ago,48 applicants,"['', 'Support the development of advanced monitoring metrics, which includes becoming the subject matter expert of the data and ownership of metric definitions.', 'Solve a range of core business and technical questions through data analysis', 'Bachelor degree or equivalent experience in related field required ', 'Experience writing SQL queries', 'Experience in Tableau is a plus', 'Extend data analytics using tools like Python', 'Design and develop data solutions extracting data from Oracle and SQL ServerSolve a range of core business and technical questions through data analysisSupport the development of advanced monitoring metrics, which includes becoming the subject matter expert of the data and ownership of metric definitions.Collaboration within the team to support data needs and team members’ analyses.Extend data analytics using tools like Python', 'Responsibilities', 'Collaboration within the team to support data needs and team members’ analyses.', 'Experience in Data Analysis and/or Data Engineer competencies (e.g. SQL Server, Oracle)', 'Knowledge of relational database design and concepts', 'Bachelor degree or equivalent experience in related field required Knowledge of relational database design and conceptsExperience in Data Analysis and/or Data Engineer competencies (e.g. SQL Server, Oracle)Experience writing SQL queries', 'Requirements', 'Preferred Experience', 'Prefer working knowledge of ETL processExperience in Tableau is a plus', 'Prefer working knowledge of ETL process', 'Design and develop data solutions extracting data from Oracle and SQL Server']",Associate,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Data Engineer (Remote) ,ADESA,"Carmel, IN",2 days ago,43 applicants,"['', 'Bonus:', 'Experience with cloud warehousing and analytics (ex. Snowflake, Redshift, BigQuery)Experience with semi-structured and unstructured dataOOP or functional programming experience (ex. Python, Java, C++, Scala, R, etc.)Working knowledge of message queuing and stream processing', 'Experience with semi-structured and unstructured data', 'Employee stock purchase program', 'Location: Remote, Canada or North America (EST Time Zone Preferred)', 'Competitive compensation', 'About Our Candidate: ', 'About Our Team:', 'Data Warehouse and Business Intelligence experienceStrong problem-solving skills, with the ability to analyze and break down problemsAdvanced SQL and RDBMS experience (ex. Snowflake, RedShift, Oracle, SQL Server, MySQL, etc.)Experience engineering data ingestion and transformation solutions (ex. Azure Data Factory, Informatica, SSIS, Talend, Pentaho, Python, Databricks, stored procedures, etc.)Advanced ability to visualize data (Tableau experience preferred)Understanding and practice of Agile and DevOps principles', 'Paid holidays and generous paid time off', 'Design and recommend modern data warehousing solutions', 'Experience engineering data ingestion and transformation solutions (ex. Azure Data Factory, Informatica, SSIS, Talend, Pentaho, Python, Databricks, stored procedures, etc.)', 'We’re an analytics company\xa0leveraging data to inform and empower our customers with clear, actionable insights.', 'Experience with cloud warehousing and analytics (ex. Snowflake, Redshift, BigQuery)', 'ADESA, a KAR Global brand, is a leader in the wholesale auto auction industry, serving customers both online and at its 70+ auction locations across North America. ADESA serves both commercial and dealer customers, including vehicle manufacturers and their finance companies, banks, credit unions, rental agencies and fleet management companies.', 'KAR Global powers the world’s most trusted automotive marketplaces through innovation, technology and people. Our end-to-end platform serves the remarketing needs of the world’s largest OEMs, dealers, fleet operators, rental companies and financial institutions.', '401(k) with employer match', 'Understanding and practice of Agile and DevOps principles', 'Working knowledge of message queuing and stream processing', 'Strong problem-solving skills, with the ability to analyze and break down problems', 'And we’re an auction company\xa0powering the world’s most advanced and integrated mobile, digital and physical auction marketplaces.', 'Competitive compensationInsurance coverage that includes medical, dental, vision and life insuranceFlexible spending accountWellness program401(k) with employer matchEmployee stock purchase programPaid holidays and generous paid time offPaid parental leaveLearning and development resources', 'Learning and development resources', 'Implement simple, intuitive data engineering and visual solutions', 'We are seeking a Data Engineer for our Business Intelligence team who is excited to work in a collaborative and fast paced environment, is self-motived, and is passionate about the ever-evolving data and technology space.\xa0This role will provide the opportunity to work hand in hand with our business partners, including executive leadership, to implement solutions that unlock the power of data driven decision making across the organization and industry.', 'We’re a technology company\xa0delivering next generation tools to accelerate and simplify remarketing.We’re an analytics company\xa0leveraging data to inform and empower our customers with clear, actionable insights.And we’re an auction company\xa0powering the world’s most advanced and integrated mobile, digital and physical auction marketplaces.', 'Collaborate with business partners to understand processes and the relationship to data velocity, availability, and quality', 'ADESA handles every aspect of the used-vehicle lifecycle. Its sellers need to liquidate an asset, and its buyers need to source inventory for their retail lot. ADESA brings them together.', 'Design solutions with a focus on cloud, PaaS, SaaS, and serverless services', 'Insurance coverage that includes medical, dental, vision and life insurance', 'The ADESA Business Intelligence team is building a modern platform to drive data driven decision-making across ADESA and KAR.\xa0ADESA is undergoing a technology transformation, both at our physical auctions and online platforms, and is in demand of a data and analytics platform to support our expanding portfolio of business opportunities.', 'What You Need to Be Successful: ', 'OOP or functional programming experience (ex. Python, Java, C++, Scala, R, etc.)', 'We’re a technology company\xa0delivering next generation tools to accelerate and simplify remarketing.', 'Mentor others to improve analytical skillsets across the organization', 'Collaborate with business partners to understand processes and the relationship to data velocity, availability, and qualityIdentify improvement areas to enhance existing data processes and offeringsDesign and recommend modern data warehousing solutionsImplement simple, intuitive data engineering and visual solutionsMentor others to improve analytical skillsets across the organizationDesign solutions with a focus on cloud, PaaS, SaaS, and serverless services', 'Data Warehouse and Business Intelligence experience', 'Identify improvement areas to enhance existing data processes and offerings', 'Advanced ability to visualize data (Tableau experience preferred)', 'Wellness program', 'Advanced SQL and RDBMS experience (ex. Snowflake, RedShift, Oracle, SQL Server, MySQL, etc.)', 'Who We Are:', 'What You Will Be Doing: ', 'Flexible spending account', 'Paid parental leave', 'ADESA offers technology-based solutions that enable dealers to maximize ROI. From condition reports, to purchase guarantees, to transportation and more—our goal is to be the go-to resource for your business throughout the remarketing cycle by using data-driven resources and top-notch customer service.', 'We want you to be well and thrive. Our benefits package includes:']",Mid-Senior level,Full-time,Information Technology,Automotive,2021-03-24 13:05:10
Data Engineer,Dr. Squatch,Los Angeles Metropolitan Area,,N/A,"['', 'Interest in conducting reproducible analysis that is peer-reviewed, version-controlled, and well-documented', 'Take ownership over our data architecture, managing, selecting, and improving the tools we use to store, load, transform, and visualize data', 'You proactively help others, stay positive and have a good sense of humor', 'We were recently listed as the 325th fastest growing company in the nation by Inc. Magazine', 'About You:', 'Architect and maintain pipelines for moving data into various third-party services, furthering our personalization capabilities', '2+ years of experience in a data/analytics engineering role', 'This is a full-time role with company benefits based in Marina del Rey, California.', 'Experience working with business intelligence solutions (required)', 'Looker', 'We are Scrappy', 'The Company (drsquatch.com):', 'Responsibilities:', 'Expert SQL skills (required)', 'dbt', 'Qualifications:', 'Implement testing, validation, and documentation to flag and resolve issues with poor-quality data', 'Tools we currently use:', 'Play to Win', 'Take ownership over our data architecture, managing, selecting, and improving the tools we use to store, load, transform, and visualize dataOptimize our data modeling layer (dbt), reducing runtime and decreasing unnecessary complexityBe a champion for efficient, effective data modeling, reviewing pull requests, suggesting improvements, and helping/coaching others on the team to write better codeDesign, build and maintain scalable data models (in dbt) to power self-service business intelligence tools and support fast analysis by our analystsImplement testing, validation, and documentation to flag and resolve issues with poor-quality dataPartner with our data engineering team to improve the quality of data that we load to our warehouse via custom pipelines (e.g., data not loaded via Fivetran/Stitch)Proactively seek out and explore new technologies to advance our data capabilitiesArchitect and maintain pipelines for moving data into various third-party services, furthering our personalization capabilities', 'This role will report to the Director of Data & Analytics.', '2+ years of experience in a data/analytics engineering roleExpert SQL skills (required)Experience working with cloud data warehouses (required)Experience working with business intelligence solutions (required)Experience using dbt (strongly preferred)Proficiency with a scripting language like Python (strongly preferred)Enthusiasm for writing clean codeInterest in conducting reproducible analysis that is peer-reviewed, version-controlled, and well-documentedPreferred, but not required: Experience in ecommerce/direct to consumer businesses', 'Team First Mentality', 'You can get things done without perfect resources, are innovative and work with a sense of urgency', 'Partner with our data engineering team to improve the quality of data that we load to our warehouse via custom pipelines (e.g., data not loaded via Fivetran/Stitch)', 'You work independently, ensuring work is completed on time regardless of the challenges that come up', 'Experience using dbt (strongly preferred)', 'You have high standards, take ownership of your work, and are invested in the outcome', 'Proactively seek out and explore new technologies to advance our data capabilities', 'Snowflake', 'SnowflakeFivetran, StitchdbtLooker', 'Enthusiasm for writing clean code', 'Fivetran, Stitch', 'You can get things done without perfect resources, are innovative and work with a sense of urgencyYou have high standards, take ownership of your work, and are invested in the outcomeYou proactively help others, stay positive and have a good sense of humorYou enjoy working with data and applying the insights you find in itYou work independently, ensuring work is completed on time regardless of the challenges that come up', 'Design, build and maintain scalable data models (in dbt) to power self-service business intelligence tools and support fast analysis by our analysts', 'Preferred, but not required: Experience in ecommerce/direct to consumer businesses', 'The Opportunity:', 'Dr. Squatch is looking for a talented Data Engineer to join our Data team. The Data Engineer will be responsible for ensuring high-quality, accurate data modeling (we use dbt), working with our outsourced data engineering team to ensure that our raw data is complete and accurate, and helping data analysts and others on the Data team transform raw data into clean, modeled data that is ready for analysis by end users. This role will have a high level of autonomy with the ability to change to the tools we use, alter the processes we maintain, and make other decisions related to data/analytics engineering that improve the capabilities and outputs of our team.', 'Be a champion for efficient, effective data modeling, reviewing pull requests, suggesting improvements, and helping/coaching others on the team to write better code', 'You enjoy working with data and applying the insights you find in it', 'Optimize our data modeling layer (dbt), reducing runtime and decreasing unnecessary complexity', 'Proficiency with a scripting language like Python (strongly preferred)', 'Experience working with cloud data warehouses (required)', ""Dr. Squatch is a high-growth startup changing the game in men's personal care through our all-natural products. We were recently listed as the 325th fastest growing company in the nation by Inc. Magazine and are looking to add talented and motivated people to our team! Our core values come naturally and make us a better, more whole and unique team. We are Scrappy - we get things done, we find a way, we act with urgency and we maintain a start-up mentality. We Play to Win - we have high standards, we encourage ownership of work, we are “hungry” and we invest in the outcome of our work. We have a Team First Mentality - we are humble, help others outside our own wheelhouse, stay positive and have fun. We offer a competitive salary in a growth focused & collaborative team environment. Perks include office gym and pool, snacks, unlimited PTO, and free soap. We're passionate about improving the lives of men and are looking for people who want to join us in our mission!""]",Associate,Full-time,Engineering,Consumer Goods,2021-03-24 13:05:10
Data Engineer,Zycron,"Farmers Branch, TX",3 weeks ago,127 applicants,"['', '2 years combined experience with object-oriented scripting languages such as C#, Python, Go, or Java.', 'Troubleshoot and own defects identified by the QA team and customers.', 'Zycron is currently seeking a\xa0Data Engineer\xa0for our client in the Dallas, TX area.\xa0This is a direct hire/full time position with generous salary, bonus and benefits.\xa0The client is not able to sponsor visas at this time.\xa0No CTC.', 'Participate in the data management life cycle process from requirements through production support.', 'Data Engineer\xa0', '3+ years of total data engineering experience.', '3+ years of total data engineering experience.3 years combined experience with SQL and/or No SQL databases.2 years combined experience with object-oriented scripting languages such as C#, Python, Go, or Java.2 years experience using\xa0SQL to\xa0query.1 year experience working with a version control system..Experience exposing data as a service using RESTful APIs is preferred.Experience with Google Big Query or SQL Server is a preferred.Experience with big data tools: Hadoop, Spark, Kafka, etc. is a plus.Experience with at least one of Google Cloud, AWS Cloud, or Azure Cloud preferred.A strong desire for continuous growth and learning.', 'The\xa0Data Engineer\xa0will\xa0deliver quality data intelligence solutions to the organization, assist our analytics team with drawing insights, and participate in creating our data platform. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up.', 'Experience with big data tools: Hadoop, Spark, Kafka, etc. is a plus.', 'Prepare design documents, create unit tests, apply version control, and perform related operational duties.', 'Zycron, a Brand of BG Staffing, Inc. (NYSE American: BGSF), is one of the largest IT solutions firms headquartered in Tennessee. We provide client-specific solutions from staffing to outsourcing across all industries, with extensive experience in health care, energy and utilities, and state and local government. To learn more about our services visit\xa0www.zycron.com', '\xa0', 'Create technical documentation.', 'Job ID Number:', ""Only candidates with backgrounds who match our client's requested experience will be contacted. Do not take this as a poor reflection on your experience, just a decision for the specific needs of our client's project/job. We look forward to working with you."", 'Develop, test, and deploy solutions for data warehousing.', '3 years combined experience with SQL and/or No SQL databases.', 'Participate in Agile ceremonies like standup, grooming and retrospectives.', 'Minimum Qualifications (Knowledge, Skills, and Abilities)', 'Experience exposing data as a service using RESTful APIs is preferred.', '2 years experience using\xa0SQL to\xa0query.', '1 year experience working with a version control system..', 'Build, manage, and optimize data pipelines for data structures encompassing data transformation, data models, schemas, metadata, data quality, and workload management.', 'Data Engineer', 'Job ID Number:\xa0117375 (Please reference in call or email)', 'Create and support data processing solutions using a wide array of technologies across Google, Amazon, and Azure cloud environments.', 'A strong desire for continuous growth and learning.', 'Essential Duties & Responsibilities', 'Experience with at least one of Google Cloud, AWS Cloud, or Azure Cloud preferred.', 'Experience with Google Big Query or SQL Server is a preferred.', 'Create and support data processing solutions using a wide array of technologies across Google, Amazon, and Azure cloud environments.Develop, test, and deploy solutions for data warehousing.Participate in the data management life cycle process from requirements through production support.Create technical documentation.Participate in Agile ceremonies like standup, grooming and retrospectives.Prepare design documents, create unit tests, apply version control, and perform related operational duties.Build, manage, and optimize data pipelines for data structures encompassing data transformation, data models, schemas, metadata, data quality, and workload management.Troubleshoot and own defects identified by the QA team and customers.Work closely with business analysts and product owners to understand requirements.', 'Work closely with business analysts and product owners to understand requirements.']",Mid-Senior level,Full-time,Information Technology,Apparel & Fashion,2021-03-24 13:05:10
Data Engineer,Doyensys Inc,"Plano, TX",1 day ago,Be among the first 25 applicants,[''],Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Junior Data Engineer,Idexcel,"Herndon, VA",1 hour ago,170 applicants,"['knowledge in  (Amazon AWS, Microsoft Azure)', 'At least 1 years of experience in open source programming languages for large scale data analysis (Spark, Hadoop, HDFS, AVRO)', 'At least 1 years of Java, Scala or Python development for modern data engineering', 'At least 1 years of experience with Real Time data stream platforms (Kafka, Spark Streaming)', 'At least 2 years of professional work experience in data management, data warehousing or unstructured data environments']",Associate,Full-time,Engineering,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Compass,"Seattle, WA",4 weeks ago,Over 200 applicants,"['', 'What We Look For', 'About This Role', ' Partner with Engineering, Product and/or the Business to understand data needs and drive innovative solutions Design, build and launch core data models into production Define and manage SLA for datasets in areas of ownership Advocate for process improvements to drive our data platform forward with a metrics-driven, continuous improvement approach ', 'Experience working with databricks is a plus', '2+ years working with Spark or an MPP system ', 'Advocate for process improvements to drive our data platform forward with a metrics-driven, continuous improvement approach', 'Define and manage SLA for datasets in areas of ownership', 'Responsibilities', 'Knowledge of, or prior experience with, real estate data integrations is a plus', 'Partner with Engineering, Product and/or the Business to understand data needs and drive innovative solutions', '2+ years experience writing SQL', ' 2+ years experience in fields such as data engineering, business intelligence, or analytics 2+ years experience writing SQL 2+ years experience in the data modeling/ETL/schema design 2+ years working with Spark or an MPP system  2+ years of experience with an object-oriented programming languages (e.g Python) is a plus Experience with business intelligence tools such as Looker or Tableau is a plus Experience working with databricks is a plus Knowledge of, or prior experience with, real estate data integrations is a plus ', 'Design, build and launch core data models into production', 'Experience with business intelligence tools such as Looker or Tableau is a plus', '2+ years experience in fields such as data engineering, business intelligence, or analytics', '2+ years of experience with an object-oriented programming languages (e.g Python) is a plus', '2+ years experience in the data modeling/ETL/schema design']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer,Iris Software Inc.,"Newark, NJ",5 days ago,115 applicants,"['', 'Work Locations: ', 'What You Will Do:', 'Interview Process:', 'Your Desired Skills:', 'Demonstrating the ability to learn new skills and work as a team', 'Your Required Skills:', 'Build and maintain new and existing applications in preparation for a large-scale architectural migration within an Agile function.', 'Bachelor’s degree Computer Science or a related field.', 'Duration: ', 'Experience with data lake/data marts/data warehouse', 'Build interfaces for supporting evolving and new applications and accommodating new data sources and types of data.', 'Direct experience supporting multiple business units for foundational data work and sound understanding of capital markets within Fixed Income', 'Knowledge of Jira, Confluence, SAFe development methodology & DevOps', '10+ years of experience in building out core financial trading systems in Core Java', 'Good exposure to Scala/Spark development work', 'Domain: ', 'Duration: Long Term/Right to hire', 'Build and maintain code to manage data received from heterogenous data formats including web-based sources, internal/external databases, flat files, heterogenous data formats (binary, ASCII).', 'Assess the impact of scaling up and scaling out and ensure sustained data management and data delivery performance.', 'Exposure to Pentaho ETL tool', 'Cloud experience (EC2/Redshift/EMR/ECS Container)', 'Design and support effective storage and retrieval of very large internal and external data set and be forward think about the convergence strategy with our AWS cloud migration', 'Job Description- 19458', 'Interview Process:\xa0WebEx interviews', 'Excellent analytical and problem-solving skills with the ability to think quickly and offer alternatives both independently and within teams.', 'Work Locations: Remote till covid/Newark NJ', 'Proven ability to work quickly in a dynamic environment.', 'Build new enterprise Datawarehouse and maintain the existing one.', 'Align with the Data/Core Product Owner and Scrum Master in assessing business needs and transforming them into scalable applications.', ""\xa0Iris's Fortune 500 direct client is looking for Data Engineer @ Start with remote/ Newark NJ"", 'Domain: Financial Services', 'Exposed to working in an Agile environment with Scrum Master/Product owner and ability to deliver']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Codoxo,Atlanta Metropolitan Area,3 weeks ago,Over 200 applicants,"['', 'Do you want to help make healthcare more effective and affordable for everyone? That’s our mission at Codoxo. The U.S. spends more on healthcare than any other country in the world, but not all of the $3.8 trillion goes to real patient care. A significant portion, up to 10% or $380 billion, is lost to fraud, waste, and abuse.', 'Codoxo’s patented artificial intelligence technology helps healthcare companies and agencies identify and act quickly to control costs. Codoxo now has six AI-powered applications that help every department across health insurance payers proactively bring down costs and reduce fraud, waste, and abuse – so more dollars to toward patient care.', 'Beneficial technical skills:\xa0', '-\tProven experience processing billions of records\xa0', '-\tAWS Glue, RDS, S3, Aurora, Data Lake\xa0', '-\tAt least 2 years of experience in the software industry\xa0', 'We are seeking a talented Data Engineer with experience in data analytics, building large reservoirs of data, and performing efficient queries. We have built a web-based application that is supported by a large dataset of healthcare data, on which we must frequently perform large queries very efficiently and return results in real-time to the user. The role of the data engineer is to bridge the data scientist with the developers and the ingestion of data. Specifically, one of the major tasks is to build and execute new data pipelines on the cloud (AWS).\xa0', '\xa0', '-\tExpert knowledge in Python\xa0', '-\tExpert knowledge in SQL', 'PLEASE NOTE BEFORE APPLYING: THIS ROLE IS LOCATED IN ATLANTA, GA AND THE CHOSEN CANDIDATE MUST BE LOCATED OR WILLING TO RELOCATE HERE AS WELL. IN ADDITION, CODOXO IS NOT ABLE TO OFFER SPONSORSHIP OR ACCOMMODATE ANY CANDIDATES THAT ARE CURRENTLY BEING SPONSORED NOW OR IN THE FUTURE', 'Required technical skills:\xa0', '-\tCloud experience (AWS)\xa0', ""-\tBachelor or master's degree in Computer Science, Engineering or related field\xa0"", '-\tExperience processing medical claims or related health care data\xa0', '-\tAuthorization to work in the USA\xa0', '-\tRelational database (PostgreSQL) programming', 'Job Description\xa0', '-\tApache Spark\xa0', 'Other Requirements/Preferences:\xa0']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Aunalytics,"South Bend, IN",1 week ago,91 applicants,"['', 'Collaborate with internal and external teams to\xa0understand business needs/issues, troubleshoot problems, conduct root cause analysis,\xa0and develop cost effective\xa0resolutions for data anomalies.\xa0', 'Build and own “one source of truth” data sets to facilitate\xa0consistency and efficiency in extracting\xa0and analyzing data from disparate data sources\xa0', 'Resourceful in getting things done, self-starter, and productive working independently or collaboratively – ours is a fast-pace entrepreneurial environment with performance expectations and deadlines.\xa0', 'Ability to communicate your ideas (verbal and written) so that\xa0team members and clients\xa0can understand them\xa0\xa0', 'Free snacks and an unlimited supply of coffee', 'Experience working with distributed and/or parallel systems experience or knowledge of concepts', 'Who We Are', 'Ability to learn quickly and contribute ideas that make the team, processes, and solutions better\xa0', 'Natural curiosity about what’s hidden in the data through exploration, attention to detail, and ability to see the big picture –\xa0similar to\xa0putting together a\xa010,000-piece\xa0puzzle.\xa0', 'Verifies accuracy of table changes and data transformation processes. Test changes prior to deployment as appropriate.\xa0', 'Build and own “one source of truth” data sets to facilitate\xa0consistency and efficiency in extracting\xa0and analyzing data from disparate data sources\xa0Ensure data integrity by developing and executing necessary processes and controls around the flow of data\xa0Innovate and improve efficiency of managing data to allow for greater speed and accuracy of producing analyses, metrics, and insights\xa0Collaborate with internal and external teams to\xa0understand business needs/issues, troubleshoot problems, conduct root cause analysis,\xa0and develop cost effective\xa0resolutions for data anomalies.\xa0Provides input into data governance initiatives\u202fto enhance current systems, ensure development of efficient application systems, influence the development of data policy, and support overall corporate and business goals\xa0Utilizes technology to analyze data from applicable systems to review data processes, identify issues, and determine actions to resolve or escalate problems that require data, system, or process improvement\xa0Verifies accuracy of table changes and data transformation processes. Test changes prior to deployment as appropriate.\xa0Recommend and implement enhancements that standardize and streamline processes, assure data quality and reliability, and reduce processing time to meet client expectations\xa0Communicate progress and completion to project team.\xa0Escalate roadblocks that may impact delivery schedule\xa0Stay\xa0up-to-date\xa0on data\xa0engineering and data science trends and developments\xa0Follow company policy and procedures which protect sensitive data and maintain compliance with established security standards and best practices\xa0Additional duties as assigned to ensure client and company success\xa0', 'Stay\xa0up-to-date\xa0on data\xa0engineering and data science trends and developments\xa0', 'Aunalytics', 'Ensure data integrity by developing and executing necessary processes and controls around the flow of data\xa0', 'Opportunity to be a part of a local company committed to making a difference in our community', 'Communicate progress and completion to project team.\xa0Escalate roadblocks that may impact delivery schedule\xa0', 'Bachelor’s degree in Computer Science, Computer Engineering, Mathematics, or related field, or\xa03\xa0plus years of relevant work experience.\xa0Experience working with relational database structures, SQL and/or flat files and performing table joins, web crawling, and web development.\xa0\xa0\xa0Proficiency in one or more of the following programming languages: PHP, Java, or Python and a familiarity with Node.js\xa0Natural curiosity about what’s hidden in the data through exploration, attention to detail, and ability to see the big picture –\xa0similar to\xa0putting together a\xa010,000-piece\xa0puzzle.\xa0Resourceful in getting things done, self-starter, and productive working independently or collaboratively – ours is a fast-pace entrepreneurial environment with performance expectations and deadlines.\xa0Ability to learn quickly and contribute ideas that make the team, processes, and solutions better\xa0Ability to communicate your ideas (verbal and written) so that\xa0team members and clients\xa0can understand them\xa0\xa0Ability to defend your professional decisions and organize proof that your ideas and processes are correct\xa0Experience working in one of the following industries: healthcare, financial services, media, or manufacturingExperience working with commercial relational database systems such as electronic medical records or other clinical systems, customer relationship management software, or accounting systemsFamiliar with various data management methodologies, data exploration techniques, data quality assurance practices, and data discovery/ visualization toolsPrior experience supporting business intelligence operations, managing technical, business, and process metadata related to data warehousingExperience working with NoSQL, Hive, MapReduce, and other Big Data technologies is preferred but not required; willing to train the right candidateExperience working with distributed and/or parallel systems experience or knowledge of conceptsShare our values:\xa0growth, relationships, integrity, and passion\xa0\xa0', 'Bachelor’s degree in Computer Science, Computer Engineering, Mathematics, or related field, or\xa03\xa0plus years of relevant work experience.\xa0', 'Provides input into data governance initiatives\u202fto enhance current systems, ensure development of efficient application systems, influence the development of data policy, and support overall corporate and business goals\xa0', 'Opportunity to work with a rapidly expanding tech company in the booming field of data science and cloud computing, alongside some of the brightest minds in the industry', 'Experience working in one of the following industries: healthcare, financial services, media, or manufacturing', 'Follow company policy and procedures which protect sensitive data and maintain compliance with established security standards and best practices\xa0', 'Opportunity to work with cutting-edge technology in a casual, fun environment', 'Competitive salary and benefits package including health, vision, dental, and life insurance', 'Experience working with NoSQL, Hive, MapReduce, and other Big Data technologies is preferred but not required; willing to train the right candidate', 'Ability to defend your professional decisions and organize proof that your ideas and processes are correct\xa0', 'Share our values:\xa0growth, relationships, integrity, and passion\xa0\xa0', 'Innovate and improve efficiency of managing data to allow for greater speed and accuracy of producing analyses, metrics, and insights\xa0', 'At the\xa0heart of our Data Solutions team are our super talented,\xa0highly-technical\xa0Data Engineers.\xa0Data Engineers are\xa0data experts who\xa0dive right into new client projects and make it their job to understand how a client’s data fits together and what that data means.\xa0\xa0Utilizing\xa0this knowledge\xa0and the industry’s newest technologies (Aunsight, Hadoop, Docker, etc.),\xa0they create data lakes (fed by real-time data streams) that become the very foundation of the work we do.\xa0\xa0Critical at all stages of the data science process,\xa0Data Engineers work cross-functionally with both external and internal teams – from business analysts\xa0to data scientists; mobile app developers to platform engineers; IT teams to high-level executives.\xa0\xa0Data Engineers also provide valuable feedback to our software team that helps to shape the development of\xa0Aunsight, our proprietary end-to-end cloud analytics\xa0platform;\xa0and the development of our proprietary mobile app,\xa0Sightglass.\xa0The best Data Engineers are patient, persistent, focused,\xa0creative,\xa0and incredibly curious.\xa0They love to learn and seek out opportunities to identify unexpected solutions or develop alternate ways to solve challenging problems.\xa0\xa0\xa0', 'Experience working with commercial relational database systems such as electronic medical records or other clinical systems, customer relationship management software, or accounting systems', 'Utilizes technology to analyze data from applicable systems to review data processes, identify issues, and determine actions to resolve or escalate problems that require data, system, or process improvement\xa0', 'Recommend and implement enhancements that standardize and streamline processes, assure data quality and reliability, and reduce processing time to meet client expectations\xa0', 'Flexible schedule and paid time off', ""What's in it for You?"", 'Additional duties as assigned to ensure client and company success\xa0', 'Prior experience supporting business intelligence operations, managing technical, business, and process metadata related to data warehousing', 'Opportunity to work with a rapidly expanding tech company in the booming field of data science and cloud computing, alongside some of the brightest minds in the industryOpportunity to work with cutting-edge technology in a casual, fun environmentOpportunity to be a part of a local company committed to making a difference in our communityChance to work with a rapidly expanding tech companyFlexible schedule and paid time offFree snacks and an unlimited supply of coffeeSocial events such as happy hours, game nights, holiday parties, birthday celebrations, movie days, ice cream sundae bars, fancy coffee carts, company softball team, etc.Competitive salary and benefits package including health, vision, dental, and life insurance', 'Essential Duties & Responsibilities:', 'Preferred Skills:', 'Aunalytics provides a leading-edge cloud platform, where businesses run their core applications on secure, high performance computing infrastructure, create integrated data-marts using enterprise analytics software, and gain on-demand access to technology and data science experts to develop algorithms that drive digital transformation.', 'Experience working with relational database structures, SQL and/or flat files and performing table joins, web crawling, and web development.\xa0\xa0\xa0', 'Position Overview', 'Familiar with various data management methodologies, data exploration techniques, data quality assurance practices, and data discovery/ visualization tools', 'Chance to work with a rapidly expanding tech company', 'Proficiency in one or more of the following programming languages: PHP, Java, or Python and a familiarity with Node.js\xa0', 'Social events such as happy hours, game nights, holiday parties, birthday celebrations, movie days, ice cream sundae bars, fancy coffee carts, company softball team, etc.']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
100% REMOTE - CONTRACT TO HIRE - Healthcare Data Integration Engineer,DISYS,United States,,N/A,"['', '· Build high performance', 'Position:', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0May design schemas, write SQL or other data markup scripting and helps to support development of Analytics and Applications that build on top of data. ', '· Maintain, improve, clean, and manipulate data in the business’s operational and analytics databases', ' (CONTRACT TO HIRE)', 'Core Tasks:', 'Location: 100% Remote', 'Duration:', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Programming and full-stack development experience would be a great addition.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Manages relationships with software and hardware vendors to understand the potential architectural impact of different vendor strategies and data acquisition. ', 'Duration: 6 Months (CONTRACT TO HIRE)', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience with FHIR is highly desirable. ', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Responsibilities may include Platform-as-a-Service and Cloud solution with a focus on data stores and associated eco systems. ', '\xa0', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Functions may include database architecture, engineering, design, optimization, security, and administration; as well as data modeling, big data development, Extract, Transform, and Load (ETL) development, storage engineering, data warehousing, data provisioning and other similar roles. ', 'Position: Sr. Data Integration Engineer', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Selects, develops and evaluates personnel to ensure the efficient operation of the function.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Healthcare data background, especially provider data (versus payer data), including clinical data. ', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience and deep background in various methods of ETL is required. ', '· Create and manage data stores at scale', '100% Remote', '· Design and deploy data platforms across multiple domains ensuring operability', '· Ensure data governance - security, quality, access and compliance', 'Location:', 'Skills/Experience Needed:', '· Transform data for meaningful analyses', 'Description & Responsibilities:', '*** No Vendors Please ***', '· Create data enrichment', '· Ensure data integrity', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Analyzes current business practices, processes and procedures as well as identifying future business opportunities for leveraging data storage and retrieval system capabilities. ', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0This specific position will be involved in loading data from Big Data infrastructure to Microsoft Azure and Snowflake and in performing mapping of data fields from a number of proprietary data formats to FHIR resources.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Duties may include management of design services, providing sizing and configuration assistance, ensuring strict data quality, and performing needs assessments.', '· Improve data efficiency, reliability and quality']",Mid-Senior level,Full-time,Information Technology,"Health, Wellness and Fitness",2021-03-24 13:05:10
Data Engineer,Jobot,"Chicago, IL",2 days ago,Be among the first 25 applicants,"['', 'GitHub', 'A Bit About Us', ' Life Insurance coverage', 'Job Details', 'Looking to hire Data Engineer!', 'About The Role', ' Full benefits Medical, Dental, Vision', 'Python', ' Generous PTO, vacation, sick, and holidays', ' ELT/ETL, reporting tools, data governance, data management, data warehousing, and dimensional modeling', ' Create a highly scalable and performant data architecture on Azure using technologies such as Kubernetes, Kafka, Debezium, Snowflake, dbt, Airflow, and Github Actions', ' 401 (K) with generous company match', 'Kafka or similar or similar distributed stream processing systems', ' ELT/ETL, reporting tools, data governance, data management, data warehousing, and dimensional modeling SQL with script-based analytic transformation tools like dbt with ability to build CTEs preferredGitHubPythonKafka or similar or similar distributed stream processing systems', ' Competitive base salary and overall compensation package', ' Design, build, and maintain data models that provide analytical insights to North America’s largest transportation companies Create a highly scalable and performant data architecture on Azure using technologies such as Kubernetes, Kafka, Debezium, Snowflake, dbt, Airflow, and Github Actions Own the modeling and transformation of semi-structured change data capture (CDC) data into a highly polished and refined business layer in our data warehouse Closely collaborate with fellow Engineers, Scientists, and Product Managers and interact with stakeholders across the organization to build data products', ' Design, build, and maintain data models that provide analytical insights to North America’s largest transportation companies', ' Closely collaborate with fellow Engineers, Scientists, and Product Managers and interact with stakeholders across the organization to build data products', ' Competitive base salary and overall compensation package Full benefits Medical, Dental, Vision Generous PTO, vacation, sick, and holidays Life Insurance coverage 401 (K) with generous company match', ' SQL with script-based analytic transformation tools like dbt with ability to build CTEs preferred', 'Essential Functions', ' Own the modeling and transformation of semi-structured change data capture (CDC) data into a highly polished and refined business layer in our data warehouse', 'Why join us?']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Jr. Data Engineer ,LPX Group,"St. Louis City County, MO",6 days ago,138 applicants,"['·\xa0\xa0\xa0\xa0\xa0\xa0Identifies, quantifies, and proposes solutions to address data integrity and data quality issues, including performing source data analysis and data profiling to assess data quality\xa0', '·\xa0\xa0\xa0\xa0\xa0\xa0Excellent written and verbal communication skills', '·\xa0\xa0\xa0\xa0\xa0\xa0This position requires less than 5% travel', '·\xa0\xa0\xa0\xa0\xa0\xa0Design and development optimizing for process maintenance and data security', '·\xa0\xa0\xa0\xa0\xa0\xa0A creative thinker with proven analytical and problem solving capabilities ', '·\xa0\xa0\xa0\xa0\xa0\xa0Experience with dimensional databases', '·\xa0\xa0\xa0\xa0\xa0\xa0High level analytical and problem solving skills, with a demonstrated ability to investigate complex issues and make informed decisions', '·\xa0\xa0\xa0\xa0\xa0\xa0Explore ways to enhance data quality and reliability', '·\xa0\xa0\xa0\xa0\xa0\xa0Identify opportunities for data acquisition', 'Desirable:', '·\xa0\xa0\xa0\xa0\xa0\xa0Familiar with Azure, specifically around Azure Data Lake, Azure SQL Server, and Azure Analysis Services', '·\xa0\xa0\xa0\xa0\xa0\xa0Analyze and organize various kinds and sources of data', '·\xa0\xa0\xa0\xa0\xa0\xa0Experience with PowerBI', '·\xa0\xa0\xa0\xa0\xa0\xa0Experience with DAX', 'Job Knowledge, Skills, and Abilities:\xa0', 'Key Responsibilities and Accountabilities:\xa0', '\xa0', 'Job Knowledge, Skills, and Abilities:', '·\xa0\xa0\xa0\xa0\xa0\xa0Capturing metadata around data quality and solution performance', '·\xa0\xa0\xa0\xa0\xa0\xa0Team player', '·\xa0\xa0\xa0\xa0\xa0\xa0Development experience with SQL Server Reporting Services and Analysis Services (SSRS and SSAS).', '·\xa0\xa0\xa0\xa0\xa0\xa0Experience with Tabular cubes', '·\xa0\xa0\xa0\xa0\xa0\xa0Conduct solution performance tuning, troubleshooting, support and capacity estimation', 'Strong willingness to learn new technologies', 'Essential:', '·\xa0\xa0\xa0\xa0\xa0\xa0Protect the integrity and quality of data solutions while securing the organizations data assets in accordance to best practices and company policy', '·\xa0\xa0\xa0\xa0\xa0\xa0Familiar with Big Data solutions', '·\xa0\xa0\xa0\xa0\xa0\xa0Efficiently use pattern development to rapidly build reliable and easily maintained solutions', '·\xa0\xa0\xa0\xa0\xa0\xa0Experience with normalized (3nf) databases', '·\xa0\xa0\xa0\xa0\xa0\xa0Provide technical support for production reports including troubleshooting and configuration management', 'Key Responsibilities and Accountabilities:', 'Desirable:\xa0', '·\xa0\xa0\xa0\xa0\xa0\xa0Experience with SQL', '·\xa0\xa0\xa0\xa0\xa0\xa0Build data systems, algorithms, prototypes, and dashboards that meet user requirements']",Associate,Contract,Information Technology,Banking,2021-03-24 13:05:10
Data Engineer,G12 Staffing,Greater St. Louis,1 week ago,103 applicants,"['● 2+ years of working knowledge in any cloud-based environment (AWS, Azure, GCP)', '● Demonstrated ability to assessing risk or assist others to determine the risk profile of the proposed solutions or workarounds', '● 5+ Years of experience of working in a development environment with 4+ years in a service/devops environment.', '● Demonstrated ability to keep cool and organized under pressure', '● Demonstrated ability to quickly understand & summarize technical issues and solutions / workarounds under consideration by engineers', '● Maturity, judgement, negotiation/influence, analytical, and leadership skills.', '● Familiarity with a DevOps or SRE culture and processes', 'Basic Qualifications:', '● Demonstrated ability to keep a team focused on getting the site back online safely as soon as possible', 'Preferred Qualifications:', '● Demonstrated ability to communicate technical issues to technical and non-technical audiences (written and oral)', '● 5+ years of strong analytical thinking and excellent troubleshooting skills', '● Familiarity with the security frameworks and application security best practices.', '● Working understanding of AWS popular services: EC2, IAM, S3, EBS, SSM, CloudWatch, CloudTrial, CloudFormation, Lambda, SQS, SNS, Redshift, Athena, EMR, Kinesis, Glue, Data Pipeline, ECR, ECS, EKS etc.', ""● Bachelor's degree or equivalent work experience"", '● Familiarity with one of the monitoring tools NewRelic/Dynatrace/DataDog.']",Mid-Senior level,Contract,Engineering,Staffing and Recruiting,2021-03-24 13:05:10
Data Engineer/Developer,JPI,"Washington, DC",1 week ago,67 applicants,"['', 'Good understanding of development lifecycle of projects with PostgreSQL database, Azure Cloud platform and Power BI and Tableau Visualization Tools', 'Experience with DHS and knowledge of DHS standards a plus', 'Responsibilities\u200b', 'Experience in developing Shell scripts on UNIX', 'Proficiency developing data extraction, transformation, and loading (ETL) processes, and performing test and validation steps', 'JPI is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status.', 'Requirements\u200b\u200b', '\u200bWork closely with software engineers and architects to extract, transform, and standardize data to prepare for ingest into target sourcesDesign and develop data services and/or pipelines as part of an Agile/Scrum teamSupport continuous process automation for data ingestWork with program management and engineers to implement and document complex and evolving requirementsPerform multiple tasks simultaneously and successful perform under changing requirements and deadlinesHelp cultivate an environment that promotes customer service excellence, innovation, collaboration, and teamworkThe person shall apply established engineering and management principles to specifications and documentation of systems developed', 'Proficiency with Python, R, and SQL languages, as well as various command line interfaces (Linux, AWS, Git Bash, etc.)', 'Demonstrated ability to communicate across all levels of the organization and communicate technical terms to non-technical audiences with an impeccable attention to detail', 'Work with program management and engineers to implement and document complex and evolving requirements', 'Perform multiple tasks simultaneously and successful perform under changing requirements and deadlines', 'Must be a US Citizen', 'Experience with DHS and knowledge of DHS standards a plusDemonstrated experience translating business and technical requirements into comprehensive data strategies and analytic solutionsDemonstrated ability to communicate across all levels of the organization and communicate technical terms to non-technical audiences with an impeccable attention to detail', 'Experience writing and modifying ETL design documentation, Test case documentation and standard operating procedures (SOP) documentation', 'Must be able to obtain a DHS Public Trust', '\u200bWork closely with software engineers and architects to extract, transform, and standardize data to prepare for ingest into target sources', 'Experienced in optimizing database querying, data manipulation and population using SQL and PL/SQL in Oracle, MySQL, PostgreSQL databases', 'Mid-level expertise in developing and managing data technologies, technical operations, reusable data services, and related tools and technologies', '\u200b\u200bBasic Requirements:', 'Additional/Desired Requirements:', 'The person shall apply established engineering and management principles to specifications and documentation of systems developed', 'Must be a US CitizenMust be able to obtain a DHS Public Trust7+ years of IT experience including experience in design, management, and solutioning of large, complex data sets and models.Experience handling multiple tasks, changing priorities, and timely actionExperience with developing data pipelines from many sources for structure and unstructured data sets in a variety of formatsProficiency developing data extraction, transformation, and loading (ETL) processes, and performing test and validation stepsProficiency with Python, R, and SQL languages, as well as various command line interfaces (Linux, AWS, Git Bash, etc.)Demonstrated experience translating business and technical requirements into comprehensive data strategies and analytic solutionsMid-level expertise in developing and managing data technologies, technical operations, reusable data services, and related tools and technologiesDemonstrated ability to adequately plan and meet delivery objectives and maintain adequate service levels in a highly dynamic, complex environmentsExperienced in optimizing database querying, data manipulation and population using SQL and PL/SQL in Oracle, MySQL, PostgreSQL databasesHands on experience with software upgrade and deployment documentationExperience in Data Warehouse/Data Mart Development Life Cycle using Dimensional modeling of STAR, SNOWFLAKE schema, Fact and Dimension tablesGood understanding of development lifecycle of projects with PostgreSQL database, Azure Cloud platform and Power BI and Tableau Visualization ToolsExperience writing and modifying ETL design documentation, Test case documentation and standard operating procedures (SOP) documentationExperience in developing Shell scripts on UNIXFamiliarity with Hive, Hadoop, Kylin, and other big data analytic tools', 'Hands on experience with software upgrade and deployment documentation', 'Help cultivate an environment that promotes customer service excellence, innovation, collaboration, and teamwork', '\xa0', '7+ years of IT experience including experience in design, management, and solutioning of large, complex data sets and models.', 'Experience in Data Warehouse/Data Mart Development Life Cycle using Dimensional modeling of STAR, SNOWFLAKE schema, Fact and Dimension tables', 'Familiarity with Hive, Hadoop, Kylin, and other big data analytic tools', 'Demonstrated ability to adequately plan and meet delivery objectives and maintain adequate service levels in a highly dynamic, complex environments', 'Experience with developing data pipelines from many sources for structure and unstructured data sets in a variety of formats', 'You will be responsible for the day-to-day operations of systems that depend on data, ensuring data is properly processed and securely transferred to its appropriate location, in a timely manner.\xa0Processing data will include managing, manipulating, storing and parsing data in a data pipeline for variety of target sources. You will also support maintenance of applications and tools that reside on these systems such as upgrades, patches, and configuration changes. The work is performed in a multidisciplinary team environment using agile methodologies. The candidate we seek must be highly motivated and enthusiastic about implementing new technologies and learning about new data in a small team environment where deadlines are important.', 'JPI is seeking a Data Engineer to support a rapidly growing Data Analytics and Business Intelligence platform focused on providing solutions that empower our Federal customers with the tools and capabilities needed to turn data into actionable insights. The ideal candidate is a critical thinker and motivated, perpetual learner and is excited to gain exposure and build skillsets across a range of technologies while solving some of our clients’ toughest challenges. As a Data Engineer, you will be integral to data operations for the development and integration of multiple data types across a range of data sets and sources.', 'Support continuous process automation for data ingest', 'Demonstrated experience translating business and technical requirements into comprehensive data strategies and analytic solutions', 'Design and develop data services and/or pipelines as part of an Agile/Scrum team', 'Experience handling multiple tasks, changing priorities, and timely action']",Associate,Full-time,Project Management,Management Consulting,2021-03-24 13:05:10
Data Engineer,Emsi,"Moscow, ID",4 days ago,Be among the first 25 applicants,"['', 'Experience with lean/agile development, kanban, and test-driven development practices, preferred.', 'Be familiar with our data clients, both in-house and external, and determine how best to get our data into their tools.', 'Customer Oriented', '2+ years Python experience, preferred', 'Attention to detail', 'Proficiency with a systems programming language.', 'Gitlab, AWS ecosystem, preferred.', 'Self-starter', 'Create and preserve knowledge with well-written documentation.', 'Those necessary for general office work', 'Experience with batch processing (AWS Batch) and/or Apache Spark (AWS EMR), preferred.', 'This position reports to the Senior Director of Data Innovation.', 'Problem solving', 'Those necessary for general office workOffice environment for majority of the time', 'Be familiar with our data clients, both in-house and external, and determine how best to get our data into their tools.Build and maintain data processing pipelines for several major data sources for job postings and professional profiles.Create and preserve knowledge with well-written documentation.Maintain Elasticsearch clusters and the servers they run on.', 'Maintain Elasticsearch clusters and the servers they run on.', 'Curious', '2+ years experience in a professional setting, preferred', 'Office environment for majority of the time', 'Build and maintain data processing pipelines for several major data sources for job postings and professional profiles.', 'Strong software engineering, programming, and QA skills', 'Organized', 'Proficiency with Linux (or other POSIX).', 'Attention to detailSelf-starterCuriousCustomer OrientedOrganizedProblem solving', 'Bachelor’s degree, preferred', 'Experience with D and/or Scala languages, preferred.', 'Strong software engineering, programming, and QA skillsProficiency with Linux (or other POSIX).Proficiency with a systems programming language.Experience with lean/agile development, kanban, and test-driven development practices, preferred.Gitlab, AWS ecosystem, preferred.Experience with D and/or Scala languages, preferred.Experience with batch processing (AWS Batch) and/or Apache Spark (AWS EMR), preferred.', '2+ years Python experience, preferred2+ years experience in a professional setting, preferredBachelor’s degree, preferred']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer,Skills Pipeline,"Madison, WI",1 week ago,83 applicants,"['Competitive medical, dental, and vision coverage', 'Integrate up-and-coming data management and software engineering technologies into existing data structures', 'A pinball machine for when you really need a break', 'Skills and qualifications:', 'Remote work throughout COVID-19 and the option to work remote two days per week once we return to the office', 'Proven ability in executing and supporting strategies to ensure the health of enterprise database environments', 'Research new uses for existing data and recommend different ways to constantly improve data reliability and quality', 'On behalf of a software company Skills Pipeline is looking for a Data Engineer.', 'standards', 'Salary paid semi-monthly', 'Responsibilities:', 'Bachelors degree in an applicable discipline or equivalent experienceProven ability in executing and supporting strategies to ensure the health of enterprise database environmentsStrong knowledge of the technical aspects of database security, enterprise backup and restore, and replication strategiesIntellectual curiosity to find new and creative ways to solve data issuesExcellent written and verbal communication, interpersonal and collaborative skillsExperience with data analysis programming languages (i.e. Python/R, SAS, etc)Bonus: experience with LAMP stack development and RESTful APIs', 'Salary paid semi-monthlyCompetitive medical, dental, and vision coverage100% employer-funded short & long-term disability coverage15 days of PTO and extra time off for holidaysRemote work throughout COVID-19 and the option to work remote two days per week once we return to the officeA pinball machine for when you really need a break', 'As a data engineer, you will play a key role in managing and improving the health and stability of their MySQL databases. You should know the ins and outs of industry standards on architecture, data modeling, security, and performance tuning, and have the ability to creatively apply solutions in collaboration with cross-functional teams to achieve business goals. The ideal candidate will excel at onboarding new customer data sets while applying their experience in data modeling and analytics to create greenhouse gas emissions and food waste dashboards for customers to track and improve their sustainability efforts.', 'Benefits + perks:\xa0', 'Experience with data analysis programming languages (i.e. Python/R, SAS, etc)', 'Design, construct, install, test, and maintain data management systemsBuild high-performance algorithms, predictive models, and prototypesEnsure that all systems meet our business requirements as well as industrystandardsTroubleshoot database-related application performance issues and recommend solutionsIntegrate up-and-coming data management and software engineering technologies into existing data structuresDevelop set processes for data modeling and analysisEmploy an array of technological languages and tools to connect systems togetherCollaborate with members of your team (eg, Software Developers, Customer Success) on the project goalsCompile and provide data sets as requested by team members or clientsResearch new uses for existing data and recommend different ways to constantly improve data reliability and qualityExecute customer onboarding verifying customer data feeds matches the uniformity of our data', 'Develop set processes for data modeling and analysis', '\xa0', 'Collaborate with members of your team (eg, Software Developers, Customer Success) on the project goals', 'If you love data architecture, working with a close team and are interested in creating a more sustainable future this could be a great opportunity for you.\xa0', 'Build high-performance algorithms, predictive models, and prototypes', 'Ensure that all systems meet our business requirements as well as industry', 'Bachelors degree in an applicable discipline or equivalent experience', 'Excellent written and verbal communication, interpersonal and collaborative skills', 'Execute customer onboarding verifying customer data feeds matches the uniformity of our data', 'Design, construct, install, test, and maintain data management systems', '15 days of PTO and extra time off for holidays', 'Troubleshoot database-related application performance issues and recommend solutions', 'Intellectual curiosity to find new and creative ways to solve data issues', 'Compile and provide data sets as requested by team members or clients', 'Bonus: experience with LAMP stack development and RESTful APIs', '100% employer-funded short & long-term disability coverage', 'Strong knowledge of the technical aspects of database security, enterprise backup and restore, and replication strategies', 'Employ an array of technological languages and tools to connect systems together']",Associate,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
eCom Data Engineer,PepsiCo,"San Francisco, CA",2 days ago,Be among the first 25 applicants,"['', '3+ years of experience with schema design and dimensional data modeling', 'Ability in managing and communicating data warehouse plans to internal clients.', 'Knowledge of machine-learning tools and techniques', 'Relocation Eligible:', 'Requires Department of Transportation (DOT) certification and successful Motor Vehicle Report (MVR) review during the pre-onboarding process', 'Experience designing, building and maintaining data processing systems', 'Experience writing production code for Python or JVM-based systems, but you know a few other languages and like the right tool for the job', 'Proven self-starter who can move projects forward by filling in the gaps on Agile teams, from leading a design session to doing some test automation, to mentoring a teammate struggling with a new technology', 'BS or MS degree in Computer Science or a related technical field', 'Qualifications/Requirements', 'Job Type:', '3+ years of Python or Java development experience, Experience with multiple data technologies and concepts such as Airflow, Kafka, Hadoop, Hive, Spark, MapReduce, SQL, NoSQL, and Columnar databases', 'Job Description', 'BS or MS degree in Computer Science or a related technical field3+ years of Python or Java development experience, Experience with multiple data technologies and concepts such as Airflow, Kafka, Hadoop, Hive, Spark, MapReduce, SQL, NoSQL, and Columnar databases3+ years of experience with schema design and dimensional data modelingAbility in managing and communicating data warehouse plans to internal clients.Experience designing, building and maintaining data processing systemsExperience writing production code for Python or JVM-based systems, but you know a few other languages and like the right tool for the jobKnowledge of machine-learning tools and techniquesProven self-starter who can move projects forward by filling in the gaps on Agile teams, from leading a design session to doing some test automation, to mentoring a teammate struggling with a new technologyRequires Department of Transportation (DOT) certification and successful Motor Vehicle Report (MVR) review during the pre-onboarding process', 'Auto req ID:']",Mid-Senior level,Full-time,Information Technology,Consumer Goods,2021-03-24 13:05:10
Data Engineer,"Edgestream Partners, L.P.","Princeton, NJ",6 days ago,Be among the first 25 applicants,"['', ' For qualified employees, the opportunity to invest in our funds.', ' Oversee and manage the data pipeline, from data download to the live trading system:', ' Collaborate closely with data scientists, the portfolio and research teams, software engineers, and trading-operations staff.', ' Advanced degree in computer science, mathematics, or physical sciences and engineering.', ' Postgres/SQL,', ' Develop production-ready software for data retrieval, storage, ETL, and analysis.', ' No prior experience in finance required.', ' Ability to communicate clearly and collaborative on larger projects.', ' Develop monitoring tools and reports at all pipeline levels,', ' Design and implement a data integration platform for managing financial data. Oversee and manage the data pipeline, from data download to the live trading system: Develop monitoring tools and reports at all pipeline levels, Ensure high data quality throughout the pipeline. Develop production-ready software for data retrieval, storage, ETL, and analysis. Work in a distributed multiprocessing environment. Onboard new data for both historical testing and real-time systems. Aid in the design of real-time data acquisition systems. Perform data analysis to validate data quality. Collaborate closely with data scientists, the portfolio and research teams, software engineers, and trading-operations staff.', ' Work in a distributed multiprocessing environment.', 'About The Job', ' Ensure high data quality throughout the pipeline.', ' Competitive salary, bonus, and incentive compensation based on overall company performance. Fantastic resources, accumulated over decades, for turning ideas into reality. Comprehensive, first-class benefits (including excellent medical and dental insurance, 401(k), HRA, FSA, life and disability insurance, catered lunch, and an onsite gym). For qualified employees, the opportunity to invest in our funds.', ' Aid in the design of real-time data acquisition systems.', 'About Us', 'Responsibilities', 'Qualifications', ' Strong programming skills, in one or many of the following:', ' Comprehensive, first-class benefits (including excellent medical and dental insurance, 401(k), HRA, FSA, life and disability insurance, catered lunch, and an onsite gym).', ' Perform data analysis to validate data quality.', ' Python, Pandas, Numpy, Matlab, R,', ' Competitive salary, bonus, and incentive compensation based on overall company performance.', ' Design and implement a data integration platform for managing financial data.', ' Advanced degree in computer science, mathematics, or physical sciences and engineering. Experience processing, cleaning, and storing (ETL) numerical and non-numerical data. Strong programming skills, in one or many of the following: Python, Pandas, Numpy, Matlab, R, Postgres/SQL, Linux/UNIX: command line tools, filesystems, signals, pipes. An interest in working on a wide spectrum of projects, including ETL pipeline engineering, data onboarding, and data analysis. Ability to communicate clearly and collaborative on larger projects. No prior experience in finance required.', 'Some Of The Benefits Of Working At Edgestream', ' Experience processing, cleaning, and storing (ETL) numerical and non-numerical data.', ' Linux/UNIX: command line tools, filesystems, signals, pipes.', ' Fantastic resources, accumulated over decades, for turning ideas into reality.', ' An interest in working on a wide spectrum of projects, including ETL pipeline engineering, data onboarding, and data analysis.', ' Onboard new data for both historical testing and real-time systems.']",Entry level,Full-time,Information Technology,Financial Services,2021-03-24 13:05:10
Data Engineer,LogistiCare,"Denver, CO",,N/A,"['', 'Tuition Reimbursement', 'This is an entry-level role and the applicant will have on-the-job trainingHealthcare or insurance experience desired but not required.Excellent time management and organizational skills.Excellent interpersonal and written communication skills.Technical skills -- Hands on experience in the administration and utilization of the technologies (or similar) below:Database technologies: DB2, Netezza, SQL Server, MySQLETL technologies: Matillion, AWS Redshift, AWS Data Migration Services, AWS GlueBI technologies (not required): Tableau, SAP BusinessObjectsCloud technologies: AWS, Azure', 'Excellent interpersonal and written communication skills.', 'ETL technologies: Matillion, AWS Redshift, AWS Data Migration Services, AWS Glue', 'Employer Paid Basic Life Insurance and AD&D', 'Voluntary Life Insurance (Employee/Spouse/Child)', 'ModivCare is an Equal Opportunity Employer.', 'The Data Engineer role will focus on driving the creation and execution of a business intelligence and data strategy. They will work directly with business and technology stakeholders and team member to analyze, define, design, develop and deliver solutions that provide actionable insights. They will drive and execute in an agile process to ensure consistent, frequent, quality deliverables are deployed.', 'Short-Term and Long-Term Disability', ""Bachelor's degree from an accredited college or university with a major in Computer Science, Information Systems, Business Administration or closely related fieldEquivalent work experience is accepted."", 'Skills & Experience', 'Paid Parental Leave', 'Develop BI solutions based on an agile SDLC.', 'Technical skills -- Hands on experience in the administration and utilization of the technologies (or similar) below:', 'Medical, Dental, and Vision insurance', 'Health Care and Dependent Care Flexible Spending Accounts', 'Facilitate the creation and execution of an ongoing BI/DW strategy inclusive of existing and available technology.', 'ModivCare offers a comprehensive benefits package to include the following:', 'Education', 'Salary- $75000-$125000 per year [Depending on candidate qualifications]', '401(k) Retirement Savings Plan with Company Match', 'We value our team members and realize the importance of benefits for you and your family.', 'Formulate and recommend standards for achieving maximum performance and efficiency of the BI/DW ecosystem.Facilitate the creation and execution of an ongoing BI/DW strategy inclusive of existing and available technology.Evangelize self-service BI and visual discovery while helping to change the Excel based culture.Interact with Reporting and Data Scientists to analyze and define requirements.Develop BI solutions based on an agile SDLC.', '\xa0', 'Healthcare or insurance experience desired but not required.', 'Evangelize self-service BI and visual discovery while helping to change the Excel based culture.', 'Excellent time management and organizational skills.', 'Employee Discounts (retail, hotel, food, restaurants, car rental and much more!!)', 'Database technologies: DB2, Netezza, SQL Server, MySQL', 'Cloud technologies: AWS, Azure', 'POSITION QUALIFICATIONS', 'Interact with Reporting and Data Scientists to analyze and define requirements.', 'ESSENTIAL FUNCTIONS', 'Pre-Tax and Post --Tax Commuter and Parking Benefits', 'Paid Time Off', 'Formulate and recommend standards for achieving maximum performance and efficiency of the BI/DW ecosystem.', ""Bachelor's degree from an accredited college or university with a major in Computer Science, Information Systems, Business Administration or closely related field"", 'Medical, Dental, and Vision insuranceEmployer Paid Basic Life Insurance and AD&DVoluntary Life Insurance (Employee/Spouse/Child)Health Care and Dependent Care Flexible Spending AccountsPre-Tax and Post --Tax Commuter and Parking Benefits401(k) Retirement Savings Plan with Company MatchPaid Time OffPaid Parental LeaveShort-Term and Long-Term DisabilityTuition ReimbursementEmployee Discounts (retail, hotel, food, restaurants, car rental and much more!!)', 'BI technologies (not required): Tableau, SAP BusinessObjects', 'Equivalent work experience is accepted.', 'This is an entry-level role and the applicant will have on-the-job training']",Mid-Senior level,Full-time,Information Technology,Hospital & Health Care,2021-03-24 13:05:10
Data Engineer (Data Science),Enverus,"Conshohocken, PA",2 weeks ago,25 applicants,"['', 'Understanding data science concepts, experience working with data scientists and operating data science models in production services. ', 'Job Knowledge - We support your professional growth as well as encourage you to share your knowledge within the organization. ', 'Develop reusable, maintainable, and efficient production-ready code. ', 'Experience with public cloud services like AWS/Azure/GCP. ', 'Self-starter who can see the big picture, and self-motivated to venture beyond the obvious. ', 'Collaboration and Teamwork - Champion an environment that supports effective teamwork by earning the trust and respect of those around you. ', 'Respect - We believe respect is the basis of all relationships both inside and out. ', 'Experience with building data science serving infrastructure and deploying data science models to production.', 'Customer Centric - always Keeping external and internal customers in mind, constantly thinking “How will this affect customers I work with?” ', 'Communication - Not only accurately communicating a message but being an active listener. This is key to our team-oriented environment. ', 'Build robust data science platform for feature discovery, feature serving, data versioning, data monitoring, and testing. ', 'Egoless culture - We recognize as an organization that the work we are doing cannot be accomplished by a single individual. As such we value tremendously everyone’s inputs at all levels. As one team, we strive to find the right balance between providing elegant solutions and time to market. ', 'Experience with monitoring and dashboard platforms such as Grafana/Datadog.', 'Experience with Docker and Kubernetes. ', 'Design flexible infrastructure and services that can grow with data science technologies and approaches. ', 'Work with data scientists to improve the efficiency of their models and bring their models to production. ', 'Egoless culture - We recognize as an organization that the work we are doing cannot be accomplished by a single individual. As such we value tremendously everyone’s inputs at all levels. As one team, we strive to find the right balance between providing elegant solutions and time to market. Job Knowledge - We support your professional growth as well as encourage you to share your knowledge within the organization. Quality of Work - You care about what you do. Customer Centric - always Keeping external and internal customers in mind, constantly thinking “How will this affect customers I work with?” Communication - Not only accurately communicating a message but being an active listener. This is key to our team-oriented environment. Respect - We believe respect is the basis of all relationships both inside and out. Reliability - Be someone your team can rely on to finish what you start. Integrity - Be honest and ethical in all your relationships and decisions. Collaboration and Teamwork - Champion an environment that supports effective teamwork by earning the trust and respect of those around you. ', 'Description', 'Experience with workflow automation and scheduling such as Airflow/Kubeflow/Prefect. ', 'Strong programming skills in Scala/Java/Python. ', 'Integrity - Be honest and ethical in all your relationships and decisions. ', 'Actively engage with the community to learn and share new ideas that can improve existing workflows and platforms. ', 'Quality of Work - You care about what you do. ', 'At least 3 years of hands-on software development experience. ', 'Experience with big data technologies such as Spark/Flink/Beam. ', 'At least 3 years of hands-on software development experience. Strong programming skills in Scala/Java/Python. Experience with big data technologies such as Spark/Flink/Beam. Experience with workflow automation and scheduling such as Airflow/Kubeflow/Prefect. Experience with monitoring and dashboard platforms such as Grafana/Datadog.Experience with Docker and Kubernetes. Experience with public cloud services like AWS/Azure/GCP. Experience with building data science serving infrastructure and deploying data science models to production.', 'Open to new technologies and ideas, but carefully evaluate the pros and cons before bringing these to production.', 'Build robust data science platform for feature discovery, feature serving, data versioning, data monitoring, and testing. Design flexible infrastructure and services that can grow with data science technologies and approaches. Work with data scientists to improve the efficiency of their models and bring their models to production. Actively engage with the community to learn and share new ideas that can improve existing workflows and platforms. Understanding data science concepts, experience working with data scientists and operating data science models in production services. Self-starter who can see the big picture, and self-motivated to venture beyond the obvious. Develop reusable, maintainable, and efficient production-ready code. Open to new technologies and ideas, but carefully evaluate the pros and cons before bringing these to production.', 'Reliability - Be someone your team can rely on to finish what you start. ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Talkspace - Online Therapy,"New York, NY",4 weeks ago,Over 200 applicants,"['', 'At ease with ambiguity', 'You will work closely with data analysts and scientists and develop a deep understanding of our product roadmap, marketing trends, and revenue targets ', 'Both learn from and teach your fellow engineers by providing constructive feedback', ""Must have a Bachelor’s or Master's degree in Computer Science"", 'You have health tech or behavioral health experience', ' You’ve worked at a data analytics company You have health tech or behavioral health experience ', 'You’ve worked at a data analytics company', 'You will own the entire data life cycle, including the project management, onboarding and implementation of any new business intelligence and/or data analytics tools ', 'Requirements', 'Python expert', "" Must have a Bachelor’s or Master's degree in Computer Science 2+ years of experience in a Data Engineering position Experience with Business Intelligence tools, preferably Looker SQL expert  Python expert At ease with ambiguity Experience (or strong interest) in working in a fast-paced environment "", 'Description', '2+ years of experience in a Data Engineering position', 'Experience (or strong interest) in working in a fast-paced environment', 'Participate in the technical design of new features', 'Experience with Business Intelligence tools, preferably Looker', ' You will be responsible for data that is used for business, product, marketing, and broad company analytics You will own the entire data life cycle, including the project management, onboarding and implementation of any new business intelligence and/or data analytics tools  You will work closely with data analysts and scientists and develop a deep understanding of our product roadmap, marketing trends, and revenue targets  Participate in the technical design of new features Both learn from and teach your fellow engineers by providing constructive feedback Collaborate with small teams of talented engineers, product managers, and designers to build the best platform for psychotherapy ', 'SQL expert ', 'You will be responsible for data that is used for business, product, marketing, and broad company analytics', 'About This Role', 'Benefits', 'Collaborate with small teams of talented engineers, product managers, and designers to build the best platform for psychotherapy']",Not Applicable,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer,The Mom Project,"Beaverton, OR",2 days ago,33 applicants,"['', '●\xa0\xa0\xa0\xa0\xa0Passion for data with demonstrated ability to use data to tell a story and influence decision making', 'Certifications and Licenses: Certified Solution Developer, MCSD, Microsoft Certified, Microsoft Certified Solution Developer', 'Commitment Level: 40 hours per week, on-site, contract through April 2022', 'What you’ll do:', '●\xa0\xa0\xa0\xa0\xa0Good understanding and application of modern data processing technology stacks. For example, Snowflake, SQL, Spark, Airflow, and others', '●\xa0\xa0\xa0\xa0\xa0Experience with Jenkins, Bitbucket/GitHub and scheduling tools like Airflow', '●\xa0\xa0\xa0\xa0\xa0Design reusable components, frameworks, and code', '●\xa0\xa0\xa0\xa0\xa0Strong programming, Python, shell scripting and SQL', '●\xa0\xa0\xa0\xa0\xa0Good understanding of file formats including JSON, Parquet, Avro, and others', '●\xa0\xa0\xa0\xa0\xa0Detail-oriented with strong information seeking skills', '●\xa0\xa0\xa0\xa0\xa0Continuous Learner', '●\xa0\xa0\xa0\xa0\xa0Broad Business Process and Systems Understanding', 'The ideal candidate will have:', 'Opportunity: The Mom Project is helping to source candidates for the above client who is looking for a Data Engineer', '●\xa0\xa0\xa0\xa0\xa0Experience with data warehouses/RDBMS like Oracle, Teradata, Snowflake', 'Opportunity:', '●\xa0\xa0\xa0\xa0\xa0Use and contribute to continuous integration pipelines', ""Education: Bachelor's degree in Computer Science, Engineering, Business, Information Technology or related field"", 'Now for the Perks!', '●\xa0\xa0\xa0\xa0\xa0Exceptional interpersonal and communication skills (written and verbal)', '●\xa0\xa0\xa0\xa0\xa0Participate in an Agile / Scrum methodology to deliver high - quality software releases every 2 weeks through Sprints', '\xa0', 'Company Industry: Retail', '●\xa0\xa0\xa0\xa0\xa0Experience with AWS components and services (E.G. EMR, S3, and Lambda)', 'Company Industry: ', 'Hours & Location:\xa0', '●\xa0\xa0\xa0\xa0\xa0Design and build product features in collaboration with business and IT\xa0', '●\xa0\xa0\xa0\xa0\xa0Familiarity with Agile project delivery methods', '●\xa0\xa0\xa0\xa0\xa0Performance/scalability tuning, algorithms, and computational complexity', '●\xa0\xa0\xa0\xa0\xa0Experience with data warehousing, dimensional modeling and ETL development', '●\xa0\xa0\xa0\xa0\xa0Demonstrable ability to quickly learn new tools and technologies\xa0', '●\xa0\xa0\xa0\xa0\xa0Troubleshoot production support issues post-deployment and come up with solutions as required', 'Commitment Level:', '●\xa0\xa0\xa0\xa0\xa0Develop architecture and design patterns to process and store high volume data sets', '●\xa0\xa0\xa0\xa0\xa02+ years’ experience in a professional organization collaborating across multiple functions', 'We are seeking an experienced Data Engineer on a contract basis to support our client’s day-to-day business needs.', 'M-F, 40 hours/week. This role will be remote while COVID restrictions are in place.\xa0The expectation is to be onsite at our Customer’s Beaverton, OR location once it is deemed safe to do so.', '●\xa0\xa0\xa0\xa0\xa0Strategic Thinking and Tactical Execution', 'Our Customer’s mission is to create groundbreaking sport innovations, by making their products more sustainable, building a creative and diverse global team, and making a positive impact in communities where we live and work. Their purpose is to bring inspiration and innovation to unite the world through sport to create a healthy planet, active communities, and an equal playing field for all.', '●\xa0\xa0\xa0\xa0\xa0Competencies: Effective Communicator', '●\xa0\xa0\xa0\xa0\xa0Analytical and Focused', 'Health Benefits: Medical, Dental, Vision, Life (including spouse &amp; child), 401k, STD/LTD, AD&D, and Commuter Benefits program.', '●\xa0\xa0\xa0\xa0\xa0Machine learning frameworks & statistical analysis with Python, R or similar']",Mid-Senior level,Contract,Information Technology,Retail,2021-03-24 13:05:10
Data Engineer,COMPASS Pathways plc,"New York, United States",2 days ago,46 applicants,"['', 'If interested, please send your CV and statement of interest to hiring@compasspathways.com', 'Expert knowledge of Python and best practices of the ecosystem. Additional languages (Java, Go, JavaScript/Typescript) are a strong plusStrong knowledge and passion for SQL. Experience debugging and optimising queriesDeep understanding of various transactional (ideally Postgres) and analytical databases, understanding of their strengths and weaknesses Experience working with distributed data processing frameworks (Spark, Flink), message queues (Kafka, Kinesis) and real-time stream processing in generalExperience working in AWS environment, being open to DevOps-related tasksExperience and understanding of modern development practices, CI/CD pipelines, infrastructure-as-code frameworks, secret managementPassion for front-end and data visualization is a plusBackground in healthtech is a strong plus', 'Deep understanding of various transactional (ideally Postgres) and analytical databases, understanding of their strengths and weaknesses ', 'Ensure availability, timeliness and correctness of our datasets (ETL, etc)', 'Experience working with distributed data processing frameworks (Spark, Flink), message queues (Kafka, Kinesis) and real-time stream processing in general', 'hiring@compasspathways.com', 'Experience and understanding of modern development practices, CI/CD pipelines, infrastructure-as-code frameworks, secret management', 'Profile', 'COMPASS Pathways is a mental health care company dedicated to accelerating patient access to evidence-based innovation in mental health (https://compasspathways.com). Our focus is on improving the lives of those who are suffering with mental health challenges and who are not helped by current treatments. We are pioneering the development of a new model of psilocybin therapy, in which our proprietary formulation of synthetic psilocybin, COMP360, is administered in conjunction with psychological support.', 'How to apply', 'Develop and maintain sophisticated data tools and platforms that will help democratizing data in the company', 'Passion for front-end and data visualization is a plus', 'Job overview', '\ufeff', 'Reports to: Data Engineer will report to Head of Data Engineering', 'Maintain and own the centralized Data Warehouse and Data Lake', 'COMPASS Pathways is proud to be an equal opportunity employer. All employment decisions are based on business needs, job requirements, and individual qualifications, without regard to race, religion, color, national origin, sex (including pregnancy, childbirth, and related medical conditions), ethnicity, age, disability, sexual orientation, gender identity, gender expression, military service, genetic information, familial or marital status, or any other status, category, or characteristic protected by applicable law.', 'Ensure the privacy, security and regulatory compliance of our datasets', 'Maintain and own the centralized Data Warehouse and Data LakeEnsure the privacy, security and regulatory compliance of our datasetsEnsure availability, timeliness and correctness of our datasets (ETL, etc)Enforce best practices of modern data warehousing, security and data engineeringEnsure Data Science team is not blocked by absence of high-quality dataDevelop and maintain sophisticated data tools and platforms that will help democratizing data in the company', 'The Data Engineer will be responsible for developing and maintaining COMPASS’ data infrastructure.', 'US applicants', 'Expert knowledge of Python and best practices of the ecosystem. Additional languages (Java, Go, JavaScript/Typescript) are a strong plus', 'Reports to', 'Strong knowledge and passion for SQL. Experience debugging and optimising queries', 'Background in healthtech is a strong plus', 'Ensure Data Science team is not blocked by absence of high-quality data', 'Experience working in AWS environment, being open to DevOps-related tasks', 'Roles and responsibilities', 'Enforce best practices of modern data warehousing, security and data engineering', 'Note that we are not able to sponsor employment visas at this time, and therefore can only accept applications from people who have employment rights in the US.', 'Equal opportunities ']",Entry level,Full-time,Information Technology,Mental Health Care,2021-03-24 13:05:10
Data Engineer,Daugherty Business Solutions,Greater St. Louis,,N/A,"['', 'Knowledge of cloud and distributed systems principles, including load balancing, networks, scaling, and in-memory versus disk.', 'Robust career development and training.', 'Collaborate and work closely with team to build data platforms.', 'Intermediate level of SQL programming and query performance tuning techniques for data integration and consumption using design for optimum performance against large data assets within an OLTP, OLAP and MPP architecture.', 'Maintain and manage Hadoop clusters in development and production environments.', 'If you require accommodations or assistance to complete the online application process, please inform any recruiter you are working with (or send an email to careers@daugherty.com) and identify the type of accommodation or assistance you are requesting. Do not include any medical or health information in this email. The recruiting team will respond to your email promptly.', 'Interest in Python, R, sh/bash and JVM-based languages including Scala and Java.', 'Interest working with AWS technologies such as Redshift, RDS, S3, EMR, ADP, Hive, Kinesis, SNS/SQS and QuickSight.Interest in Python, R, sh/bash and JVM-based languages including Scala and Java.Interest in Hadoop family languages including Pig and Hive.Familiarity with high performance data libraries including Spark, NumPy and TensorFlow.Proven ability to pick up new languages and technologies quickly.Intermediate level of SQL programming and query performance tuning techniques for data integration and consumption using design for optimum performance against large data assets within an OLTP, OLAP and MPP architecture.Knowledge of cloud and distributed systems principles, including load balancing, networks, scaling, and in-memory versus disk.Experience building data pipelines to connect analytics stacks, client data visualization tools and external data sources.Exposure to stream-processing and messaging, such as Storm, Spark-Streaming, Kafka and MQ.Understanding of DevOps and CI/CD toolset, such as Jenkins, GitLab CI, Buildbot, Drone and Bamboo.Some experience with programming Languages, such as Scala, Java, R and Python.', 'Interest working with AWS technologies such as Redshift, RDS, S3, EMR, ADP, Hive, Kinesis, SNS/SQS and QuickSight.', 'Exposure to stream-processing and messaging, such as Storm, Spark-Streaming, Kafka and MQ.', 'Familiarity with high performance data libraries including Spark, NumPy and TensorFlow.', 'As a Data Engineer you will have the opportunity to:', 'Little to no travel.', 'When you are a Daugherty employee, your job doesn’t end when a contract is up. You stay on as an indispensable member of the team with career growth opportunities tailored to your interests and talents. We want you to be eager to take on a new challenge. We are always 100% honest about what to expect, because we don’t want Daugherty to be just another job; we want Daugherty to be your dream job.\xa0', 'Interest in Hadoop family languages including Pig and Hive.', 'Excellent health, dental and vision insurance.Revenue sharing and a 401(k) retirement savings plan.Life, disability and long-term care insurance.Little to no travel.Robust career development and training.', 'Daugherty Business Solutions is an inclusive Equal Employment Opportunity employer that considers applicants without regard to gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law.', 'Contribute to the creation and maintenance of optimal data pipeline architectures.', 'Excellent health, dental and vision insurance.', '\xa0', 'We are looking for someone with:', 'Team Daugherty is hiring a Data Engineer to join us in St. Louis. The ideal candidate for this position is a problem solver with the ability to utilize insights, creativity and perspective to drive business success for our clients.', 'Contribute to the creation and maintenance of optimal data pipeline architectures.Collaborate and work closely with team to build data platforms.Maintain and manage Hadoop clusters in development and production environments.Assemble large, complex data sets that meet functional/non-functional business requirements.Work with team members and functional leads to understand existing data requirements and validation rules to support moving existing data warehouse workloads into a distributed data platform.Create custom software components (e.g. specialized UDFs) and analytics applications.Employ a variety of languages and tools to marry systems together.Recommend ways to improve data reliability, efficiency and quality.Implement & automate high-performance algorithms, prototypes and predictive models.', 'Due to COVID-19, most of our employees are working remotely. We’ve implemented a virtual hiring process and continue to interview candidates by phone or video and are onboarding new hires remotely. We value the safety of each member of our community because we know we’re all in this together.', 'Assemble large, complex data sets that meet functional/non-functional business requirements.', 'Some experience with programming Languages, such as Scala, Java, R and Python.', 'We offer members of Team Daugherty:', 'Life, disability and long-term care insurance.', 'Create custom software components (e.g. specialized UDFs) and analytics applications.', 'Revenue sharing and a 401(k) retirement savings plan.', 'Implement & automate high-performance algorithms, prototypes and predictive models.', 'Employ a variety of languages and tools to marry systems together.', 'Work with team members and functional leads to understand existing data requirements and validation rules to support moving existing data warehouse workloads into a distributed data platform.', 'Experience building data pipelines to connect analytics stacks, client data visualization tools and external data sources.', 'Interested? Apply today to take your career to the next level!', 'Recommend ways to improve data reliability, efficiency and quality.', 'Understanding of DevOps and CI/CD toolset, such as Jenkins, GitLab CI, Buildbot, Drone and Bamboo.', 'Proven ability to pick up new languages and technologies quickly.']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Alexander Technology Group,"Portsmouth, NH",2 weeks ago,77 applicants,"['', 'Advanced Microsoft Office skills:\xa0Excel, PowerPoint, Outlook', 'SAP experience strongly preferred', 'Accuracy and attention to detail', 'Commitment to meeting deadlines and continuous improvement mindset', 'If interested, please send resume to Jpolombo@alexandertg.com', 'SSIS – SQL server Integration servicesSSAS – SQL server Analysis servicesAzure DBUndergraduate degree required – Computer SciencePower BI experience 1-3 yearsSAP experience strongly preferredAdvanced Microsoft Office skills:\xa0Excel, PowerPoint, OutlookDemonstrated professionalism in written and verbal communicationsAccuracy and attention to detailCommitment to meeting deadlines and continuous improvement mindset', 'SSAS – SQL server Analysis services', 'Qualifications ', 'SSIS – SQL server Integration services', 'Power BI experience 1-3 years', 'Azure DB', 'The Alexander Technology Group is looking for a Data Engineer for a client in the Seacoast NH Area. This will be a permanent opportunity and applicants must be local to New Hampshire. ', 'Undergraduate degree required – Computer Science', 'Demonstrated professionalism in written and verbal communications']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Benchmark Analytics,"Chicago, IL",2 weeks ago,198 applicants,"['', 'The Fine Print and How to Apply:', 'Extract and transform data from large file-based and server-based datasets', 'Has at least 3 to 5 years of experience in engineering data pipeline, analytics infrastructure and integrating platforms:', 'Strong technical aptitude and technical acquisition skills (ability to work with and develop skills in technical products)', 'The satisfaction that comes with being part of a solution that makes a big difference in the world', 'Unfortunately, we are not able to sponsor employment visas at this time, so we can only accept applications from candidates who are authorized to work in the US.', 'Work with the research, product, and engineering teams to critique and iteratively improve results', 'Work with leadership and peers to drive decisions and measure progress through agile and scrum processes.', 'Thank you for your interest in our company and what we are building.Benchmark Analytics is an Equal Opportunity Employer. We value diversity of all kinds in our effort to create a stellar workforce of committed and passionate team members.Unfortunately, we are not able to sponsor employment visas at this time, so we can only accept applications from candidates who are authorized to work in the US.If you’d like to apply, please email your resume to\xa0courtney.jian@benchmarkanalytics.com.', 'Build tools to intergrade, deploy, monitor, and archive machine learning models developed by our data scientists', 'Experience building applications for end-users and in-house analysts', 'Has a strong sense of process (ability to understand how steps relate to each other to achieve end results)', 'If you are interested in the intersection of public sector operations, research and data analytics, we believe this is an unprecedented opportunity to experience it first-hand.', 'If you’d like to apply, please email your resume to\xa0courtney.jian@benchmarkanalytics.com.', 'Build dynamic, high performance system for processing millions of records daily', 'Works and communicates well with others:', 'Build and implement ETL data processes touching millions of records and spanning multiple customer systems;Design, enhance and automate end to end data platform processes;Collaborate with the Benchmark Research Team members across the organization that focus on improving product data quality, data pipeline efficiency and data platform performance.', 'Has empathy for colleagues and customers', 'Experience troubleshooting problems throughout the stack', 'We are currently seeking an experienced\xa0Data Engineer\xa0to join us to build a robust and scalable data platform as we continue to grow our predictive analytics business. ', 'As a Data Engineer,\xa0you will get to:', 'The Candidate', 'Experience with Python 3', 'Able to effectively communicate across multiple levels (team members, managers) in a fast-paced environment', 'A competitive base salary and flexible PTO policy', 'Enjoys writing queries, looking at data, and testing assumptions:', 'Able to work independently with reasonable guidance from management (except in complex and non-routine situations)', 'Excellent understanding of ETL processes and relational databases', 'We cover 75% of Medical (BCBS) Premiums, Dental & Vision Premiums, and offer 100% company sponsored Life Insurance as well as a 401k plan', 'Analyze entire ETL processes, identify inefficiencies, suggest improvement.', 'The ideal candidate will be someone who:', 'Educate and learn from internal teams on ETL data engineering best practices, relevant knowledge and specific skills', 'The excitement that comes with the need to endlessly innovate and transform our market', 'A truly unique experience to work for a fast-growing, young company', 'Able to receive and respond positively to feedback', 'Write, review and maintain code and configurations for performing data aggregations and manipulations.', 'Responsibilities', '\ufeff', 'Experience with building python-based data pipeline and ETL systems', 'Strong in data engineering development technologies (e.g., SQL, Python, Django) and data sense', 'Build automated tools for importing data from our transactional platform', 'Design, enhance and automate end to end data platform processes;', 'Thank you for your interest in our company and what we are building.', 'Experience working with and without AWS infrastructure/software', 'Benchmark Analytics is an Equal Opportunity Employer. We value diversity of all kinds in our effort to create a stellar workforce of committed and passionate team members.', 'Build a high speed, high capacity data system (multi-tenant data lake and databases)Build automated tools for importing data from our transactional platformBuild dynamic, high performance system for processing millions of records dailyManage an existing python-based toolset for ETL (django+airflow) and modeling (Scikit learn and licensed software)Build tools to intergrade, deploy, monitor, and archive machine learning models developed by our data scientistsExtract and transform data from large file-based and server-based datasetsWrite, review and maintain code and configurations for performing data aggregations and manipulations.Work with leadership and peers to drive decisions and measure progress through agile and scrum processes.Analyze entire ETL processes, identify inefficiencies, suggest improvement.Support process documentation as necessary to prepare the team and company for growth/scalePrepare presentations and reports based on the results of work/analysisEducate and learn from internal teams on ETL data engineering best practices, relevant knowledge and specific skillsWork with the research, product, and engineering teams to critique and iteratively improve results', 'Collaborate with the Benchmark Research Team members across the organization that focus on improving product data quality, data pipeline efficiency and data platform performance.', 'Support process documentation as necessary to prepare the team and company for growth/scale', 'What We Offer:', 'The Role', 'Manage an existing python-based toolset for ETL (django+airflow) and modeling (Scikit learn and licensed software)', 'A truly unique experience to work for a fast-growing, young companyThe satisfaction that comes with being part of a solution that makes a big difference in the worldThe excitement that comes with the need to endlessly innovate and transform our marketA competitive base salary and flexible PTO policyWe cover 75% of Medical (BCBS) Premiums, Dental & Vision Premiums, and offer 100% company sponsored Life Insurance as well as a 401k plan', 'Has at least 3 to 5 years of experience in engineering data pipeline, analytics infrastructure and integrating platforms:Experience with building python-based data pipeline and ETL systemsExperience building applications for end-users and in-house analystsExperience with Python 3Experience working with and without AWS infrastructure/softwareExperience troubleshooting problems throughout the stackEnjoys writing queries, looking at data, and testing assumptions:Excellent understanding of ETL processes and relational databasesStrong in data engineering development technologies (e.g., SQL, Python, Django) and data senseStrong technical aptitude and technical acquisition skills (ability to work with and develop skills in technical products)Has a strong sense of process (ability to understand how steps relate to each other to achieve end results)Has the ability to successfully mange multiple concurrent tasks/projects and meet deadlinesWorks and communicates well with others:Has empathy for colleagues and customersAble to receive and respond positively to feedbackAble to work independently with reasonable guidance from management (except in complex and non-routine situations)Able to effectively communicate across multiple levels (team members, managers) in a fast-paced environment', 'Build and implement ETL data processes touching millions of records and spanning multiple customer systems;', 'Build a high speed, high capacity data system (multi-tenant data lake and databases)', 'Prepare presentations and reports based on the results of work/analysis', 'Has the ability to successfully mange multiple concurrent tasks/projects and meet deadlines']",Entry level,Full-time,Information Technology,Law Enforcement,2021-03-24 13:05:10
Data Engineer,Research Innovations Incorporated,"Alexandria, VA",1 week ago,38 applicants,"['', 'Experience with integration methodologies and tools for Big Data applications and services', 'Create and maintain automated testing capabilities for our ETL framework and components', 'BS in Computer Science, equivalent degree, or previous work experience', 'Experience with Python', 'At RII, we believe that diversity in our workforce is critical to our success. We strive to hire great people from a wide variety of backgrounds, not just because it’s the right thing to do, but because it makes us stronger. We work to help your intellectual passions and creativity thrive. It’s one of our core values:\xa0Let your geek flag fly.', '\xa0It’s all part of another of our core values:\xa0Stay human.\xa0It’s why our comfortable and colorful offices include a community game room, pantry, massage chair, and an escape room, among other amenities. It’s why we have a community manager and regular community events.', 'WHAT YOU HAVE DONE', 'Experience with Agile Methodologies and supporting technologies enabled by Atlassian products', 'Maintain awareness of the current and emerging capabilities in ETL technologies and how these apply to our customer challenges', 'Experience working with REST APIs', 'Experience leading a team of developers', 'Experience with data quality and data profiling tools', 'Experience with Continuous Integration and Continuous Deployment concepts', 'Research Innovations, Inc. is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender, gender identity or expression, national, origin, genetics, disability status, protected veteran status, age, or any other characteristic protected by state, federal or local law.', 'Experience with runtime configuration management and automated deployment (using tools such as Ansible or Puppet)', 'EVEN BETTER', 'It’s also why, during the current pandemic, most of us aren’t actually in our lovely office. We’re all working fully remote where possible, with safety controls for those who do have to be on site. RII is committed to not overwhelming our healthcare system, preventing infection for those most at risk, reducing the impact on our communities, and keeping our employees working.', 'Document use cases, solutions, & recommendations for our customers', 'BS in Computer Science, equivalent degree, or previous work experienceDeveloped scalable ingest/ETL pipelines using Apache NiFiExperience in developing and deploying Java software architecturesExcellent general understanding of Linux operating systems, distributed systems, microservices, and database technologiesExperience working with REST APIsExperience with runtime configuration management and automated deployment (using tools such as Ansible or Puppet)Excellent general understanding data analytics platformsExperience in understanding and decomposing system level requirements into discrete and measurable tasks', 'Establish, maintain, and enhance an extensible ETL architecture in support of several customer projectsBe an engineering point-person for ingest capabilities and the interfaces of external systems we ingest data fromMaintain awareness of the current and emerging capabilities in ETL technologies and how these apply to our customer challengesDevelop extensible and scalable solutions by collaborating with customers and the broader RII development teamCreate and maintain automated testing capabilities for our ETL framework and componentsDocument use cases, solutions, & recommendations for our customers', 'Excellent general understanding data analytics platforms', 'WHAT YOU WILL BE DOING', 'Research Innovations, Inc. (RII) is breaking through the big, slow, status quo with transformative technology that fundamentally changes and improves the world. We develop cutting-edge software for all levels of the government and military. Using agile development practices and user-centered design, we create innovative software solutions for complex real-world problems.', 'Developed scalable ingest/ETL pipelines using Apache NiFi', 'Experience with the BigTable family of columnar stores (HBase, Cassandra, Accumulo)', 'We are looking for a committed Data Engineer to join our External Systems Interfaces team. Solve unique, challenging problems for our Defense and Homeland Security customers. Help develop the ETL systems that ingest large volumes of critical, real-time data that enables Big Data analytics and provides greater situational awareness and context to our users.\xa0Get s#it done.', 'MS in Computer Science, equivalent degree, or work experienceExperience with Agile Methodologies and supporting technologies enabled by Atlassian productsExperience with other ETL and adjacent technologies such as Apache Beam, Apache Spark, Apache Kafka, Apache Storm, and/or LogstashExperience with Jolt transformsExperience with messaging queue implementations supporting AQMP (e.g., RabbitMQ)Experience with PythonExperience with Continuous Integration and Continuous Deployment conceptsExperience with data quality and data profiling toolsStrong knowledge of installation, configuration, and maintenance of cloud computing and Big Data infrastructure to include Hadoop, Accumulo, Mongo, Kafka, Spark, Elasticsearch, Puppet, Ansible, and related technologiesExperience with the BigTable family of columnar stores (HBase, Cassandra, Accumulo)Experience with integration methodologies and tools for Big Data applications and servicesExperience with client-side development including JavaScript, HTML5 and AngularExperience leading a team of developers', 'Strong knowledge of installation, configuration, and maintenance of cloud computing and Big Data infrastructure to include Hadoop, Accumulo, Mongo, Kafka, Spark, Elasticsearch, Puppet, Ansible, and related technologies', 'Excellent general understanding of Linux operating systems, distributed systems, microservices, and database technologies', 'Experience with other ETL and adjacent technologies such as Apache Beam, Apache Spark, Apache Kafka, Apache Storm, and/or Logstash', 'We also offer all employees comprehensive benefits including: flexible work schedules, health insurance coverage, paid time off, 401k with a company match, paid parental leave, access to wellness programs and much more. You get this all from day one, and all 100% paid for by RII.', 'MS in Computer Science, equivalent degree, or work experience', 'This position requires the ability to obtain a US Secret Clearance. Active clearance not required to apply.', 'Establish, maintain, and enhance an extensible ETL architecture in support of several customer projects', 'Experience in developing and deploying Java software architectures', 'Develop extensible and scalable solutions by collaborating with customers and the broader RII development team', 'Experience with messaging queue implementations supporting AQMP (e.g., RabbitMQ)', 'If you are a sharp engineer with demonstrated capabilities in designing and implementing scalable solutions using extract, transform, and load technologies we want to hear from you.', 'Experience with client-side development including JavaScript, HTML5 and Angular', 'Experience with Jolt transforms', 'Be an engineering point-person for ingest capabilities and the interfaces of external systems we ingest data from', 'Experience in understanding and decomposing system level requirements into discrete and measurable tasks']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer ,ClearBridge Technology Group,"Burlington, MA",2 weeks ago,124 applicants,"['', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Minimum of 3-4 years of data engineering experience', 'o\xa0\xa0\xa0Demonstrated experience enhancing data pipelines by developing new data tables and data models', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa03 or more years of experience designing data systems particularly in Python, as well as their support infrastructure (shell scripts, job schedulers, message queues, etc.).', 'o\xa0\xa0\xa0Experience with data engineering with 1st\xa0party data (Direct to consumer)', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Prior experience working with Tableau', 'o\xa0\xa0\xa0Ability to understand the business goals of interpreting first party data for a consumer goods company', 'Our client in Burlington MA is in need of a team of Data Engineers for a 12 month contract with the strong possibility of going longer and potentially full time to work remotely (then onsite after Covid19 restrictions are lifted). The Data Engineers will work within our customer’s Data Science team which focuses on building first party data pipelines from Azure Cloud. Day to day responsibilities will include building data models and adding more features to the models. The Data Engineers will create data tables and new features to help better describe our client’s customers. They will build pipelines to which they will develop scoring through data science services to enhance our client’s customer’s onsite experience. The data engineers will need to know intuitively the business goals of this group. Develop code (Python) to take data from various data streams, data lakes, and supporting data sources and craft a unified dimensional or star schema data model for ML, analytics, and reporting', 'o\xa0\xa0\xa0Basic understanding of data science and its importance to a consumer goods company', 'o\xa0\xa0\xa0This includes machine learning data engineering', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience working with Azure Cloud as the primary data source', 'Required Skills']",Mid-Senior level,Contract,Project Management,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,ettain group,"Charlotte, NC",7 days ago,151 applicants,"['', 'Job Summary:', '6 years of experience working on project(s) involving the implementation of solutions applying development life cycles (SDLC)', 'Expertise in Java/Scala/Python, SQL, Scripting, Teradata, Hadoop (Sqoop, Hive, Pig, Map Reduce), Spark (Spark Streaming, MLib), Kafka or equivalent Cloud Bigdata components', 'Ingestion data from 3rd\xa0party and internal systems to support pro selling folks in stores and marketing department to manage marketing spend\xa0', 'Qualifications:', '4 years of experience in Hadoop or any Cloud Bigdata components (specific to the Data Engineering role)', '6 years of experience writing technical documentation in a software development environment', ""Bachelor's Degree in Engineering, Computer Science, CIS, or related field7 years of experience in Data Engineering, Data Warehousing/ETL, or Software Engineering6 years of experience working on project(s) involving the implementation of solutions applying development life cycles (SDLC)4 years of Hadoop experience"", 'The primary purpose of this role is to provide consultation and technical advice on translating business requirements and functional specifications into logical program designs. This includes facilitating the implementation and maintenance of complex business and enterprise data solutions to ensure successful deployment of released applications. This role serves as a technical expert for project teams throughout the implementation and maintenance of business and enterprise Data or Platform solutions. In addition, this role personally develops and delivers modules, stable application systems, and integrated enterprise Data or Platform solutions within various computing environments.', '\xa0', 'Experience working with Continuous Integration/Continuous Deployment tools', '7 years of experience in Data Engineering, Data Warehousing/ETL, or Software Engineering', '2 years of experience playing a lead role in projects (specific to the Data Engineering role)', ""Bachelor's Degree in Engineering, Computer Science, CIS, or related field"", '4 years of Hadoop experience', 'Internal dashboarding and integrations with 3rd\xa0party solutions to provide data to decision makers', 'Data Engineering', '4 years of experience leading teams, with or without direct reports', '6 years of IT experience developing and implementing business systems within an organization', ""Master's Degree in Computer Science, CIS, or related field6 years of IT experience developing and implementing business systems within an organization6 years of experience working with defect or incident tracking software6 years of experience writing technical documentation in a software development environment4 years of experience working with an IT Infrastructure Library (ITIL) framework4 years of experience leading teams, with or without direct reports6 years of experience working with source code control systemsExperience working with Continuous Integration/Continuous Deployment tools6 years of experience in systems analysis, including defining technical requirements and performing high level design for complex solutionsData Engineering4 years of experience in Hadoop or any Cloud Bigdata components (specific to the Data Engineering role)2 years of experience playing a lead role in projects (specific to the Data Engineering role)Expertise in Java/Scala/Python, SQL, Scripting, Teradata, Hadoop (Sqoop, Hive, Pig, Map Reduce), Spark (Spark Streaming, MLib), Kafka or equivalent Cloud Bigdata components"", '4 years of experience working with an IT Infrastructure Library (ITIL) framework', 'Preferred Qualifications:', '6 years of experience working with source code control systems', '6 years of experience in systems analysis, including defining technical requirements and performing high level design for complex solutions', ""Master's Degree in Computer Science, CIS, or related field"", '6 years of experience working with defect or incident tracking software', 'Ingestion data from 3rd\xa0party and internal systems to support pro selling folks in stores and marketing department to manage marketing spend\xa0Internal dashboarding and integrations with 3rd\xa0party solutions to provide data to decision makers']",Mid-Senior level,Contract,Information Technology,Retail,2021-03-24 13:05:10
Data Engineer,Nielsen,"Emeryville, CA",2 days ago,108 applicants,"['', 'Gracenote', 'Job Type:', 'Minimum Bachelor’s Degree in Computer Science', 'Ability and passion for learning new technology', 'FOR THIS ROLE WE WILL BE LOOKING FOR INDIVIDUALS THAT HAVE:', 'Experience with MapForce', 'Bonus Skills', ' Minimum Bachelor’s Degree in Computer Science 2+ years of software development experience Experience coding in : Java, SQL, NoSQL, Kafka, Scala, Team experience with Agile methodologies, such as Scrum and test-driven development Experience developing service oriented architectures and an understanding of design for scalability, performance and reliability Built applications using relational databases such as MySQL, Postgres, or SQL Server Ability and passion for learning new technology ', 'Experience coding in : Java, SQL, NoSQL, Kafka, Scala,', 'Team experience with Agile methodologies, such as Scrum and test-driven development', 'Primary Location', 'DevOps experience deploying and tuning the applications you’ve built', 'Built applications using relational databases such as MySQL, Postgres, or SQL Server', ' DevOps experience deploying and tuning the applications you’ve built Configuration management tools such as Ansible, Chef, or Docker Experience designing and deploy applications in AWS Experience with MapForce ', 'Configuration management tools such as Ansible, Chef, or Docker', '2+ years of software development experience', 'Data Engineer - 78162', 'Data Engineer', 'Experience designing and deploy applications in AWS', 'Experience developing service oriented architectures and an understanding of design for scalability, performance and reliability', 'About Nielsen Global Media']",Not Applicable,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Data Engineer,State Farm ®,"Dunwoody, GA",6 days ago,28 applicants,"['', 'Provide data pipeline solutions for the development, implementation, execution, validation, monitoring, and improvement of data science solutions', 'Understand the basics of machine learning models in order to optimize data science solutions', 'If A Recent College Graduate', 'What You Can Expect', 'Critical Thinking', 'Teamwork', 'We Are Looking for Candidates With', 'Expertise in data engineering practices including the identification, acquisition, cleansing, profiling, and ETL (extracting, transformation, and loading) of data used in data science discovery and deployment solutions', 'Technical/Functional Expertise', 'SFARM', ' Transcript (unofficial copy accepted)Office Location: Bloomington, Atlanta, or Dallas will be considered.', 'Identify critical and emerging technologies, techniques, tools, data sources, and platforms in the data engineering field, including cloud-based solutions, that will support and extend quantitative analytic deployment solutions', 'Adaptability ', ' Resume', 'Overview', 'State Farm Careers!', 'Conform with State Farm strategic analytic data related policies, environments, and direction', 'A Day In The Life Of A Data Engineer', ""The following must be attached to candidate's application at the time of submission -please attach both of the following as one complete document under the resume section:"", "" Resume Transcript (unofficial copy accepted)Office Location: Bloomington, Atlanta, or Dallas will be considered.Applicants are required to be eligible to lawfully work in the U.S. immediately; employer will not sponsor applicants for U.S. work authorization (e.g. H-1B visa) for this opportunity**QualificationsWe Are Looking for Candidates WithExperience and strong knowledge in the Property & Casualty (P&C) data concepts, usage and design, ability to work collaboratively and adapt to change quickly.Expertise in data engineering practices including the identification, acquisition, cleansing, profiling, and ETL (extracting, transformation, and loading) of data used in data science discovery and deployment solutionsExpertise in providing pipeline solutions for the development, implementation, execution, validation, monitoring, and improvement of data science solutionsExpertise in providing reusable data pipeline solutionsUnderstanding of the basics of machine learning models in order to optimize data science solutionsAn understanding of critical and emerging technologies, techniques, tools, data sources, and platforms in the data engineering field, including cloud-based solutions, that will support and extend quantitative analytic deployment solutionsAn ability to develop and maintain an effective network of both scientific and business contacts and knowledge to obtain relevant information and intelligence around the market and emergent opportunitiesCompetenciesTechnical/Functional ExpertiseCritical ThinkingTeamworkAdaptability What You Can ExpectNext Steps: Competitive candidates may be invited to participate in pre-employment testing and/or the interview process. This is where the excitement begins!What’s In It For YouCompetitive Benefits, Pay and Bonus Potential: Who doesn’t want money, right? Volunteer opportunities: Get involved and give back to the community! Continuing Education Support: We support opportunities for you to learn and grow!A Learning Culture: Mentoring, Professional Designations, Employee Development, and more!We embrace Diversity and Inclusion: We are one team and it is simply the right thing to do!Incumbency PeriodThere is a 24-month incumbency period (beginning on the employee's effective date) for any lateral job movements and a 12-month incumbency period for any promotional opportunities, which must be met before the employee may post for other State Farm positions. The incumbency period does not affect the at-will relationship between State Farm and the employee and does not create an employment contract, nor contractual rights.Last Date to Apply: 3/31/2021Learn more about our benefits at State Farm Careers!We are not just offering a job but a meaningful career! We’re here to help life go right®Come join our passionate team!SFARM"", 'may', 'Expertise in providing reusable data pipeline solutions', 'Demonstrate up-to-date expertise in data engineering practices and provides solutions for the identification, acquisition, cleansing, profiling, and ETL (extracting, transformation, and loading) of data used in data science discovery and deployment solutions', 'An ability to develop and maintain an effective network of both scientific and business contacts and knowledge to obtain relevant information and intelligence around the market and emergent opportunities', 'Demonstrate up-to-date expertise in data engineering practices and provides solutions for the identification, acquisition, cleansing, profiling, and ETL (extracting, transformation, and loading) of data used in data science discovery and deployment solutionsProvide data pipeline solutions for the development, implementation, execution, validation, monitoring, and improvement of data science solutionsDevelop intelligent data management and pipelines solutions for reusabilityEstablish business domain knowledge for existing State Farm data sources and investigates, recommends, and initiates acquisition of new data resources from internal and external open and vendor data sources for model building, training, and deploymentConform with State Farm strategic analytic data related policies, environments, and directionUnderstand the basics of machine learning models in order to optimize data science solutionsIdentify critical and emerging technologies, techniques, tools, data sources, and platforms in the data engineering field, including cloud-based solutions, that will support and extend quantitative analytic deployment solutionsDevelop and maintain an effective network of both scientific and business contacts/knowledge obtaining relevant information and intelligence around the market and emergent opportunities', 'Technical/Functional ExpertiseCritical ThinkingTeamworkAdaptability ', 'An understanding of critical and emerging technologies, techniques, tools, data sources, and platforms in the data engineering field, including cloud-based solutions, that will support and extend quantitative analytic deployment solutions', ""Applicants are required to be eligible to lawfully work in the U.S. immediately; employer will not sponsor applicants for U.S. work authorization (e.g. H-1B visa) for this opportunity**QualificationsWe Are Looking for Candidates WithExperience and strong knowledge in the Property & Casualty (P&C) data concepts, usage and design, ability to work collaboratively and adapt to change quickly.Expertise in data engineering practices including the identification, acquisition, cleansing, profiling, and ETL (extracting, transformation, and loading) of data used in data science discovery and deployment solutionsExpertise in providing pipeline solutions for the development, implementation, execution, validation, monitoring, and improvement of data science solutionsExpertise in providing reusable data pipeline solutionsUnderstanding of the basics of machine learning models in order to optimize data science solutionsAn understanding of critical and emerging technologies, techniques, tools, data sources, and platforms in the data engineering field, including cloud-based solutions, that will support and extend quantitative analytic deployment solutionsAn ability to develop and maintain an effective network of both scientific and business contacts and knowledge to obtain relevant information and intelligence around the market and emergent opportunitiesCompetenciesTechnical/Functional ExpertiseCritical ThinkingTeamworkAdaptability What You Can ExpectNext Steps: Competitive candidates may be invited to participate in pre-employment testing and/or the interview process. This is where the excitement begins!What’s In It For YouCompetitive Benefits, Pay and Bonus Potential: Who doesn’t want money, right? Volunteer opportunities: Get involved and give back to the community! Continuing Education Support: We support opportunities for you to learn and grow!A Learning Culture: Mentoring, Professional Designations, Employee Development, and more!We embrace Diversity and Inclusion: We are one team and it is simply the right thing to do!Incumbency PeriodThere is a 24-month incumbency period (beginning on the employee's effective date) for any lateral job movements and a 12-month incumbency period for any promotional opportunities, which must be met before the employee may post for other State Farm positions. The incumbency period does not affect the at-will relationship between State Farm and the employee and does not create an employment contract, nor contractual rights.Last Date to Apply: 3/31/2021Learn more about our benefits at State Farm Careers!We are not just offering a job but a meaningful career! We’re here to help life go right®Come join our passionate team!SFARM"", 'Last Date to Apply:', 'Come join our passionate team!', 'Competitive Benefits, Pay and Bonus Potential: Who doesn’t want money, right? ', 'Responsibilities', 'Develop and maintain an effective network of both scientific and business contacts/knowledge obtaining relevant information and intelligence around the market and emergent opportunities', 'Expertise in providing pipeline solutions for the development, implementation, execution, validation, monitoring, and improvement of data science solutions', 'Understanding of the basics of machine learning models in order to optimize data science solutions', 'Qualifications', 'Continuing Education Support: We support opportunities for you to learn and grow!', 'One Company…Many Careers!', 'Office Location', 'We embrace Diversity and Inclusion: We are one team and it is simply the right thing to do!', 'Develop intelligent data management and pipelines solutions for reusability', 'Experience and strong knowledge in the Property & Casualty (P&C) data concepts, usage and design, ability to work collaboratively and adapt to change quickly.', 'A Learning Culture: Mentoring, Professional Designations, Employee Development, and more!', 'Experience and strong knowledge in the Property & Casualty (P&C) data concepts, usage and design, ability to work collaboratively and adapt to change quickly.Expertise in data engineering practices including the identification, acquisition, cleansing, profiling, and ETL (extracting, transformation, and loading) of data used in data science discovery and deployment solutionsExpertise in providing pipeline solutions for the development, implementation, execution, validation, monitoring, and improvement of data science solutionsExpertise in providing reusable data pipeline solutionsUnderstanding of the basics of machine learning models in order to optimize data science solutionsAn understanding of critical and emerging technologies, techniques, tools, data sources, and platforms in the data engineering field, including cloud-based solutions, that will support and extend quantitative analytic deployment solutionsAn ability to develop and maintain an effective network of both scientific and business contacts and knowledge to obtain relevant information and intelligence around the market and emergent opportunities', 'Competitive Benefits, Pay and Bonus Potential: Who doesn’t want money, right? Volunteer opportunities: Get involved and give back to the community! Continuing Education Support: We support opportunities for you to learn and grow!A Learning Culture: Mentoring, Professional Designations, Employee Development, and more!We embrace Diversity and Inclusion: We are one team and it is simply the right thing to do!', 'Establish business domain knowledge for existing State Farm data sources and investigates, recommends, and initiates acquisition of new data resources from internal and external open and vendor data sources for model building, training, and deployment', 'We are not just offering a job but a meaningful career! We’re here to help life go right®', 'Incumbency Period', 'What’s In It For You', 'Volunteer opportunities: Get involved and give back to the community! ', 'Competencies']",Not Applicable,Contract,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,BuildZoom,"Scottsdale, AZ",2 weeks ago,Be among the first 25 applicants,"['', 'Exceptional written/verbal communication and emotional intelligence.', 'Nice to have experience: Go, Airflow, Elasticsearch, Redis, AWS Lambda, AWS Aurora, Snowflake', 'Values', 'Collaboration. Our team is stronger than the sum of its parts.', 'Scrappy and Tenacious. Win in the face of seemingly insurmountable obstacles.', 'Drive. A passionate desire to improve the construction and remodeling industry.Self Improvement. A dedication to personal growth, achievement, and self-actualization.Collaboration. Our team is stronger than the sum of its parts.Quantitative. Our business decisions are made with data.Scrappy and Tenacious. Win in the face of seemingly insurmountable obstacles.', 'Broad interests to influence data and business strategy.', 'By submitting your interest in this job, you agree to receive text notifications with additional steps to complete your job application. You will receive up to 6 messages from the number ""63879"". Message & data rates may apply. Please refer to our privacy policy for more information.', 'Drive. A passionate desire to improve the construction and remodeling industry.', '3+ years of hands-on software engineering experience in a professional team-based environment.', 'This position can be located in Phoenix, AZ or San Francisco, CA. ', 'Must have professional experience: Python, SQL, Data Modeling', 'Requirements', 'Exceptional written/verbal communication and emotional intelligence.3+ years of hands-on software engineering experience in a professional team-based environment.Broad interests to influence data and business strategy.Must have professional experience: Python, SQL, Data ModelingNice to have experience: Go, Airflow, Elasticsearch, Redis, AWS Lambda, AWS Aurora, Snowflake', 'Self Improvement. A dedication to personal growth, achievement, and self-actualization.', 'Quantitative. Our business decisions are made with data.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,KDR Recruitment USA,United States,,N/A,"['', '● Experience designing solutions in Azure specializing in any of the following technologies: Azure Data Factory, Azure Data Lake, Azure SQL & Azure SQL Data Warehouse, Azure Functions\xa0', 'My client is an exciting Data Consultancy who is looking for contract Data Engineering Consultants.  They are a global enterprise that focus on cloud, data, and machine learning to help clients navigate human-centered digital transformation.', 'Do you thrive in a fast-paced environment?', 'Unfortanley my client can not offer a VISA transfer for this position.\xa0', 'Are you looking for a 100% remote position?', 'Key skills required -\xa0', '● Strong SQL and Python experience\xa0', 'You would be working with some really great clients which include the likes of Netflix, Cannon, Facebook, PayPal, and Starbucks to name a few!\xa0', '\xa0']",Mid-Senior level,Contract,Information Technology,Management Consulting,2021-03-24 13:05:10
Data Engineer,"McCarthy Building Companies, Inc.","Dallas, TX",7 days ago,Be among the first 25 applicants,"['', 'Promote ADF and database objects through environments using Azure DevOps and Visual Studio', 'Key Responsibilities', 'Skills & Qualifications', 'Design, implement, and support ETL and ELT pipelines using Azure Data Factory pipelines and dataflows (both mapping and wrangling)', 'Experience building data pipelines to ingest unstructured and streaming data', ""McCarthy Building Companies, Inc. is one of America's premier commercial construction companies. Our reputation for tackling the toughest building challenges starts with our focus on building high-performing teams that collaborate with clients and industry partners starting in the earliest stages of design, throughout construction and beyond project completion. With offices and employees nationwide, we specialize in a wide range of project types including healthcare, education, renewable energy, marine, water/wastewater, commercial office and retail, hospitality/entertainment and airports. Originally founded as a family business in 1864, today we are proud to be 100 percent employee owned."", 'Azure Data Engineer', 'Azure DevOps', 'Create and run testing protocols for data solutions', 'Mature understanding of data warehouse and data lake concepts and design (including star schemas)', 'Experience building data pipelines to ingest unstructured and streaming dataExperience preparing data for Data Science and Machine Learning use casesExperience with master data managementExperience designing reports using Power BI, Power Query, and DAX', 'Assure development work accords with best practices including security and data quality', 'Experience with master data management', 'Participate in code and design review to ensure alignment to standards and best practices', ' 5+ years of data solution delivery experience with 1+ years on the Microsoft Azure platform:Azure Data Factory (including pipelines and mapping & wrangling data flows)', 'Experience developing and promoting work through devops pipelines and utilizing source control, preferably GIT, and Visual Studio’s database tools', 'McCarthy is seeking a full-time Azure Data Engineer who, as part of the Business Intelligence team, will be responsible to deliver end-to-end data pipelines and repositories leveraging both traditional tools and practices as well as modern consumption and delivery models to support a growing Business Intelligence practice.', 'Entrepreneurial attitude with a passion to deliver value for the organization and a desire to foster and develop team members\xa0', 'Research, analyze, recommend and select technical approaches for solving challenging and complex development and integration problems', 'Azure Data Lake Gen2', 'Extensive experience querying API endpoints', 'Excellent analytical, conceptual, and problem-solving abilities', 'Preferred:\xa0', 'Design, implement, and support modern data solutions with Azure Data Factory, Azure Data Lake, and Azure SQL', 'Experience designing reports using Power BI, Power Query, and DAX', 'Design, implement, and support modern data solutions with Azure Data Factory, Azure Data Lake, and Azure SQLDesign, implement, and support ETL and ELT pipelines using Azure Data Factory pipelines and dataflows (both mapping and wrangling)Create and run testing protocols for data solutionsPromote ADF and database objects through environments using Azure DevOps and Visual StudioAssure development work accords with best practices including security and data qualityParticipate in code and design review to ensure alignment to standards and best practicesResearch, analyze, recommend and select technical approaches for solving challenging and complex development and integration problems', 'Required:\xa0', 'Extensive experience with T-SQL (queries and DDL)', 'McCarthy is proud to be an equal opportunity and affirmative action employer regardless of race, color, gender, age, sexual orientation, gender identity, religious beliefs, marital status, genetic information, national origin, disability or protected veteran status.', 'Azure SQL (nice to have: Azure Data Warehouse/Synapse)', 'Experience preparing data for Data Science and Machine Learning use cases', 'Mature understanding of data warehouse and data lake concepts and design (including star schemas) 5+ years of data solution delivery experience with 1+ years on the Microsoft Azure platform:Azure Data Factory (including pipelines and mapping & wrangling data flows)Azure SQL (nice to have: Azure Data Warehouse/Synapse)Azure Data Lake Gen2Azure DevOpsExtensive experience with T-SQL (queries and DDL)Extensive experience querying API endpointsExperience M and Power Query data wranglingExperience developing and promoting work through devops pipelines and utilizing source control, preferably GIT, and Visual Studio’s database toolsExperience with Agile/Scrum methodology preferredExcellent analytical, conceptual, and problem-solving abilitiesEntrepreneurial attitude with a passion to deliver value for the organization and a desire to foster and develop team members\xa0', 'Experience M and Power Query data wrangling', 'Experience with Agile/Scrum methodology preferred']",Associate,Full-time,Information Technology,Construction,2021-03-24 13:05:10
Data Engineer,Accenture Federal Services,"San Antonio, Texas Metropolitan Area",2 weeks ago,43 applicants,"['', 'Minimum of 3 Years’\xa0experience with the following:', ""Here's what you need:\xa0"", 'The work:', 'Equal Employment Opportunity:\xa0All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.\xa0', ""Accenture\xa0Federal Services, helping our federal clients tackle their toughest challenges while unleashing their fullest potential…and then some. What makes our approach so unique? Operating from the nation’s capital, we bring together commercial innovation and leading-edge technologies to deliver an integrated and interactive experience that far exceeds expectations. How? Our passion meets purpose! Through our diverse culture and inclusive thinking, we embrace our employees' ideas taking them from concept to practical solutions. Not to mention, we sleep well at night knowing our work directly impacts and improves the way the world works. We keep our tech smarts sharp by providing abundant training and certification opportunities. Are you ready to learn and grow in a career, while making a difference?"", ""Bachelor's Degree"", 'Must be a U.S. Citizen; no Dual Citizenship', 'Accenture Federal Services is committed to providing veteran employment opportunities to our service men and women.\xa0', 'Location: San Antonio, TX area', 'Experience working within a hybrid environment.', '\xa0\xa0', 'Experience Leading small teams of engineers, data analysts. Collaborating with data scientists, and data visualizers to bring powerful analytical solutions and insights to our clients', 'Experience with 2 of 3 - Java, Scala, and Python programming language.\xa0For example, utilizing Java to build data sorting algorithms and machine learning sequences.', 'Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.\xa0', 'viewers', 'Bonus Points:', 'You are:', 'stay tuned', 'Experience in the full software development lifecycle, utilizing emerging technologies and big data design principles in developing data pipelines, interfaces, and architecture to support big data and analytics initiativesExperience Leading small teams of engineers, data analysts. Collaborating with data scientists, and data visualizers to bring powerful analytical solutions and insights to our clients', 'Data Engineering or Big Data Technologies, or Data Transformation, and modelingCloud Technologies (Data Lake, Azure, Google, AWS etc.) or with open-source technologies (Spark, Kafka, Presto, Hive, Cassandra etc.) Will ensure that the data they extract is valid, assisting in documenting any requirements, and resolving ambiguities in the data.SQL and/or NOSQL databases to organize the collection, processing, and storing of data from different sources.Experience in architecting and building scalable data platformsMust be a U.S. Citizen; no Dual Citizenship', 'We are:', 'Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.\xa0', 'ratings', '\xa0', 'Professional Cloud Data Engineer certification in AWS, Azure or GCP', 'The Chip and Joanna\xa0Gaines\xa0at developing data pipelines. Like turning a\xa0fixer-upper\xa0house into the\xa0best on the block, you turn complicated datasets into useful,\xa0showstopping\xa0formats. You live in the cloud and open-source world and enjoy working with your team to bring all the pieces of the data story together, keeping\xa0ratings\xa0high and\xa0viewers\xa0wowed. What does a good time look like to you? Hanging out on the reddit big data feeds and chatting about the latest advances in machine learning and AI. And of course, you always\xa0stay tuned\xa0for what is coming next.', 'Experience in the full software development lifecycle, utilizing emerging technologies and big data design principles in developing data pipelines, interfaces, and architecture to support big data and analytics initiatives', 'Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture Federal Services.\xa0', 'Organization: Accenture Federal Services', 'Data Engineering or Big Data Technologies, or Data Transformation, and modeling', 'Must be conversant with cloud native CLIs for various data engineering related activities such as setting up compute and storage resources, IAM and ETL.', 'Cloud Technologies (Data Lake, Azure, Google, AWS etc.) or with open-source technologies (Spark, Kafka, Presto, Hive, Cassandra etc.) Will ensure that the data they extract is valid, assisting in documenting any requirements, and resolving ambiguities in the data.', 'Federal Data Engineer', 'showstopping\xa0', ""Professional Cloud Data Engineer certification in AWS, Azure or GCPExperience with 2 of 3 - Java, Scala, and Python programming language.\xa0For example, utilizing Java to build data sorting algorithms and machine learning sequences.Must be conversant with cloud native CLIs for various data engineering related activities such as setting up compute and storage resources, IAM and ETL.Experience working within a hybrid environment.Bachelor's Degree"", 'An active security clearance or the ability to obtain one may be required for this role.', 'Accenture Federal Services is an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.\xa0', 'SQL and/or NOSQL databases to organize the collection, processing, and storing of data from different sources.', 'We are:\xa0\xa0', 'Important Information', 'Experience in architecting and building scalable data platforms', 'fixer-upper', 'best on the block']",Associate,Full-time,Consulting,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Jerry.ai,"Boston, MA",3 weeks ago,110 applicants,"['', 'Develop tools supporting self-service data pipeline management (ETL)', 'Boston', ' Start-up energy working with a brilliant and passionate team Exponential growth (5 straight quarters of 50-100%+ quarter over quarter growth) Flat structure and access to senior leadership for continuous mentorship Meritocracy - we promote based on performance, not tenure Rockstar teammates. You will be working with a strong team with prior work experience at Amazon, Microsoft, NVIDIA, Alibaba, etc. ', 'Expertise in Python for developing and maintaining data pipeline code.', 'Rockstar teammates. You will be working with a strong team with prior work experience at Amazon, Microsoft, NVIDIA, Alibaba, etc.', 'Locations ', ' Owner of the core company data pipeline, responsible for scaling up data processing flow to meet the rapid data growth Consistently evolve data model & data schema based on business and engineering needs Implement systems tracking data quality and consistency Develop tools supporting self-service data pipeline management (ETL) SQL and MapReduce job tuning to improve data processing performance ', 'Flat structure and access to senior leadership for continuous mentorship', 'Meritocracy - we promote based on performance, not tenure', 'personal concierge for your car and home', 'About The Role', 'Owner of the core company data pipeline, responsible for scaling up data processing flow to meet the rapid data growth', 'Proficient in SQL, specially with Postgres dialect.', 'We’d love to hear from you if you like ', ' 2+ years of data engineering experience within a rigorous engineering environment Proficient in SQL, specially with Postgres dialect. Expertise in Python for developing and maintaining data pipeline code. Experience with Apache Spark and PySpark library (experience with AWS extension of PySpark is a plus). Experience with BI software (preferably Metabase or Tableau). Experience with Hadoop (or similar) Ecosystem. Experience with deploying and maintaining data infrastructure in the cloud (experience with AWS preferred). Comfortable working directly with data analytics to bridge business requirements with data engineering ', '2+ years of data engineering experience within a rigorous engineering environment', 'Implement systems tracking data quality and consistency', 'Comfortable working directly with data analytics to bridge business requirements with data engineering', 'Requirements', 'SQL and MapReduce job tuning to improve data processing performance', 'Experience with Hadoop (or similar) Ecosystem.', 'Toronto', 'Responsibilities', 'Palo Alto', 'About Jerry.ai', 'Experience with Apache Spark and PySpark library (experience with AWS extension of PySpark is a plus).', 'Experience with deploying and maintaining data infrastructure in the cloud (experience with AWS preferred).', 'Start-up energy working with a brilliant and passionate team', 'Consistently evolve data model & data schema based on business and engineering needs', 'Exponential growth (5 straight quarters of 50-100%+ quarter over quarter growth)', 'Experience with BI software (preferably Metabase or Tableau).', ' Toronto Boston Palo Alto']",Associate,Full-time,Engineering,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer,Teachable,"New York, NY",1 week ago,83 applicants,"['', 'What You Might Work On', 'Maintaining and improving event collection, queueing, and processing', 'Experience with kafka or other pub/sub messaging queues is a plus', 'Who cares about code quality and strives to balance efficiency with readability and to help teammates achieve the same', 'Experience with Airflow is a plus', 'Who upholds Teachable values, including working as part of a diverse team', 'Building an in-house link tracker or integrating with an attribution vendor', 'Supporting customer-facing reportingBuilding an in-house link tracker or integrating with an attribution vendorConverting event processing from batch to streamHelping the engineering org transition from ruby sidekiq to kafka consumers for webhooks', 'Helping the engineering org transition from ruby sidekiq to kafka consumers for webhooks', ""Teachable encourages individuals from a broad diversity of backgrounds to apply for positions. We are an equal opportunity employer, meaning we're committed to a fair and consistent interview process. Please tell us in your application if you require an accommodation to apply for a job or to perform your job."", 'What You Will Be Doing', 'Maintaining and improving the ETL platform', 'Supporting customer-facing reporting', 'Improving data warehouse performance by building out federated data sources', 'Assisting the the next phase of data lake development', '2-3+ years of hands-on experience writing python and SQL ETL jobs and working with open-source ETL technologies', 'Maintaining and improving event collection, queueing, and processingAssisting the the next phase of data lake developmentMaintaining and improving the ETL platformImproving data warehouse performance by building out federated data sourcesResponding to issues and alerts as they arise', '2-3+ years of hands-on experience writing python and SQL ETL jobs and working with open-source ETL technologiesExperience with Airflow is a plusExperience with Docker or Kubernetes is a plusExperience with kafka or other pub/sub messaging queues is a plusWho cares about code quality and strives to balance efficiency with readability and to help teammates achieve the sameWho upholds Teachable values, including working as part of a diverse team', 'Experience with Docker or Kubernetes is a plus', 'Benefits', 'Responding to issues and alerts as they arise', 'Converting event processing from batch to stream', 'We Are Looking For Someone With']",Mid-Senior level,Full-time,Information Technology,Internet,2021-03-24 13:05:10
Data Engineer,Northwestern Mutual,"Milwaukee, WI",4 weeks ago,Be among the first 25 applicants,"['', ' Exceptionally self-motivated and directed', ' Financial Services industry background', "" What's the Role? "", ' Assures data consistency and reliability. Performs quality checks, contributes to metadata and data dictionaries, documents repeatable extract, transform and load processes.', ' Understand the nature of distributed systems design', ' Cloud experience a strong plus', ' Analyzes data needs and identifies available internal and external sources.', 'At Northwestern Mutual, we are strong, innovative and growing. We invest in our people. We care and make a positive difference. ', ' BI Data Engineer ', "" Designs and performs data extraction, assessment, translation, transformation, and load (ETL) processes with minimal guidance or from prescribed specifications I n charge of building data equity, it's underlying code, tools, databases, and related infrastructure Administers and maintains existing data engineering pipelines to their full extent of engineering needs Analyzes data needs and identifies available internal and external sources. U tilizes business and analytical data modeling skills to design data integration and structure approaches. Utilizes programming skills to access and extract data from diverse sources residing on multiple platforms and implement data models by combining, synthesizing and structuring data from databases, files and spreadsheets. Assures data consistency and reliability. Performs quality checks, contributes to metadata and data dictionaries, documents repeatable extract, transform and load processes."", 'Nice To Have', ' 2+ years of work experience with SSIS/other ETL tool, data modeling and business intelligence big data architecture.', ' Proficient in SQL', ' 1 to 2+ years of experience in BI tools, particularly on Power BI , DAX, SSRS', ' Experience with data marts/warehouses', 'At Northwestern Mutual, we are strong, innovative and growing. We invest in our people. We care and make a positive difference.', 'This job is not covered by the existing Collective Bargaining Agreement.', ' Financial Services industry background Cloud experience a strong plus Experience working with AWS S3, Databricks, python, Bash, scala, java is a strong plus Experience with Apache Airflow Experience in automated deployment, Git, CI/CD Understand the nature of distributed systems design', ' Utilizes programming skills to access and extract data from diverse sources residing on multiple platforms and implement data models by combining, synthesizing and structuring data from databases, files and spreadsheets.', ' Experience with Apache Airflow', 'A Great Candidate Will Meet The Following Requirements', ' Familiar with database structures, theories, principles, and practices', 'Required Certifications', "" I n charge of building data equity, it's underlying code, tools, databases, and related infrastructure"", ' We are an equal opportunity/affirmative action employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, gender identity or expression, sexual orientation, national origin, disability, age or status as a protected veteran, or any other characteristic protected by law.', ' Experience in automated deployment, Git, CI/CD', ' U tilizes business and analytical data modeling skills to design data integration and structure approaches.', 'Grow your career with a best-in-class company that puts our client’s interests at the center of all we do. Get started now! ', ' Designs and performs data extraction, assessment, translation, transformation, and load (ETL) processes with minimal guidance or from prescribed specifications', ' Bachelor’s degree in Computer Science, MIS, or related field 2+ years of work experience with SSIS/other ETL tool, data modeling and business intelligence big data architecture. 1 to 2+ years of experience in BI tools, particularly on Power BI , DAX, SSRS Proficient in SQL Exceptionally self-motivated and directed Proven Analytical, evaluative and problem-solving capabilities. Familiar with database structures, theories, principles, and practices Experience with data marts/warehouses', ' Proven Analytical, evaluative and problem-solving capabilities.', ' Administers and maintains existing data engineering pipelines to their full extent of engineering needs', ' Experience working with AWS S3, Databricks, python, Bash, scala, java is a strong plus', ' Bachelor’s degree in Computer Science, MIS, or related field']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Orchard,New York City Metropolitan Area,2 weeks ago,124 applicants,"['', 'Orchard is radically simplifying the way people buy and sell their homes. For the average American, the home purchase and sale process takes months, creates anxiety, and is filled with uncertainty and hassle. Orchard has reimagined the end-to-end experience of buying and selling, from innovative home search tools to find the perfect home to the ability to buy a new home before selling your current one. Orchard customers manage the entire experience through a personalized online dashboard, while also getting the support of best-in-class Orchard real estate agents.\xa0', 'Design, create, and continuously improve upon reports, dashboards, etc. to help company stakeholders measure performance and make informed decisions; closely collaborate with functional group leads throughout the reporting lifecycle, from initial scoping to deployment to maintenance', 'Dive into and deeply understand new data sources and their underlying data libraries to transform, integrate, and make them accessible for self-directed analysis by stakeholders across the business, including team leads from sales, marketing, product, and operations', 'Equity participation', 'What You’ll Do Here:', 'We’d Love to Hear From You if You Have:', 'About Orchard', 'Spearhead business intelligence infrastructure efforts, including owning the design and implementation of a modular, parallelized codebase of SQL scripts necessary for long-term business intelligence scalability', 'Spearhead business intelligence infrastructure efforts, including owning the design and implementation of a modular, parallelized codebase of SQL scripts necessary for long-term business intelligence scalabilityDive into and deeply understand new data sources and their underlying data libraries to transform, integrate, and make them accessible for self-directed analysis by stakeholders across the business, including team leads from sales, marketing, product, and operationsDesign, create, and continuously improve upon reports, dashboards, etc. to help company stakeholders measure performance and make informed decisions; closely collaborate with functional group leads throughout the reporting lifecycle, from initial scoping to deployment to maintenance', 'Experience prioritizing, building, and deploying code using business intelligence reporting tools; Looker proficiency a plus.', ""We're proud to be recognized by Inc. Magazine, Fast Company and Forbes as one of the best workplaces of 2020. We also have a 4.9 Glassdoor rating! Orchard is building the first one-stop-shop in real estate and we’re bringing together the most innovative professionals across real estate, business, marketing, technology and design. We also have some pretty great perks:"", 'Flexible PTO', 'Equity participationFlexible PTOUp to 18 weeks of paid family leaveEmployee discount on Orchard’s services', 'Why Orchard', 'Experience driving fast-paced projects from scratch to completion (e.g. building a new code base to tackle a complex problem) in a highly organized manner', 'Headquartered in New York City and with offices in Austin, Denver and Atlanta, Orchard has over 200 employees and has grown 10x year over year. We have raised over $130 million in equity financing from top-tier investors including Revolution, Firstmark, Accomplice, Navitas and Juxtapose. Our investors have also backed the likes of Pinterest, AirBnb, Shopify and Sweetgreen.', 'About the Role', 'As a Data Engineer, you will be involved in creating, updating and maintaining the full business intelligence stack. Focusing on building scalable back-end data architecture but also producing high impact front-end dashboards. Working closely with the Product and Technology teams, you will extract and transform data from new products and build & own the analytics layer of the company’s data environment to support our business intelligence tooling. You will help lead the charge to surface critical data to end users and enable leadership to make fast, data-informed decisions.\xa0', 'Up to 18 weeks of paid family leave', 'This is a full-time role reporting to our Head of Engineering, and will be based out of our New York City office.', 'Employee discount on Orchard’s services', '4+ years of experience in a business intelligence, analytics, data science or engineering role. Experience working at a high-growth technology company a plus.', 'Orchard is proud to be an equal opportunity employer. We provide employment opportunities without regard to age, race, color, ancestry, national origin, religion, disability, sex, gender identity or expression, sexual orientation, veteran status, or any other protected status in accordance with applicable law.', 'BA/BS degree in a quantitative discipline (Computer Science, Math, Statistics, Physics or Engineering)', 'Exceptional dimensional data modeling skills. Knowledgeable about data warehouse technical architecture, ETL frameworks, and OLAP principles.', '4+ years of experience in a business intelligence, analytics, data science or engineering role. Experience working at a high-growth technology company a plus.BA/BS degree in a quantitative discipline (Computer Science, Math, Statistics, Physics or Engineering)Experience working in SQL and Python. Familiarity with Postgres, Redshift, Airflow or Looker is a plus.Exceptional dimensional data modeling skills. Knowledgeable about data warehouse technical architecture, ETL frameworks, and OLAP principles.Experience prioritizing, building, and deploying code using business intelligence reporting tools; Looker proficiency a plus.Experience driving fast-paced projects from scratch to completion (e.g. building a new code base to tackle a complex problem) in a highly organized manner', 'Experience working in SQL and Python. Familiarity with Postgres, Redshift, Airflow or Looker is a plus.']",Associate,Full-time,Engineering,Real Estate,2021-03-24 13:05:10
Data Engineer,VMware,"Austin, TX",1 week ago,93 applicants,"['', 'Scripting languages like Perl/Python/Shell;SQL;Unix/Linux/Windows operating systems.', 'Experience: ', 'Category : ', 'Subcategory: ', 'SQL;', 'Unix/Linux/Windows operating systems.', 'Full Time/ Part Time: ', 'Business Summary: ', 'Scripting languages like Perl/Python/Shell;', 'Job Role And Responsibility', 'Preferred Skills: N/A', 'Posted Date:', 'Required Skills']",Not Applicable,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Nextuple Inc,United States,2 weeks ago,Over 200 applicants,"['', 'Excellent understanding of common data science toolkits', 'At least 2-4 years relevant working experience', 'Work with multiple stakeholders to facilitate preprocessing, analysis of data.', 'Experience with data visualisation tools', 'Excellent applied statistics skills, such as distributions, statistical testing, regression, etc.', 'Passionate about technology, demonstrates ability to generate new ideas and innovations; excellent in self-learning, problem analysis and solving; works independently and is proactive', 'Ability to consume research papers and convert them into quick proof of concepts\xa0', 'Developing testing strategies', 'Data mining using state-of-the-art methodsWork with multiple stakeholders to facilitate preprocessing, analysis of data.Apply state of the art research techniques to derive insights and spot trendsUsing machine learning techniques to correlate customer behavior to historical dataDeveloping testing strategiesAd-hoc analysis and presenting results', 'Experience with SQL and NoSQL databases, such as MongoDB, Cassandra, HBase.', 'At least 2-4 years relevant working experienceMasters or PhD degree in computer science, operations research, statistics, mathematics, or equivalent fieldsSolid theoretical foundations and industry experience in large scale data analysis/platforms, machine learning, sales forecasting.Experience in machine learning libraries such as Tensorflow, Keras, PyTorch and Scikit-learnSolid programming skills in SQL, Java, Python and ScalaAbility to consume research papers and convert them into quick proof of concepts\xa0Passionate about technology, demonstrates ability to generate new ideas and innovations; excellent in self-learning, problem analysis and solving; works independently and is proactiveGood interpersonal and communication skills, including the ability to describe the logic and complex models to stakeholders.', 'Solid programming skills in SQL, Java, Python and Scala', 'Experience in machine learning libraries such as Tensorflow, Keras, PyTorch and Scikit-learn', 'Masters or PhD degree in computer science, operations research, statistics, mathematics, or equivalent fields', 'Solid theoretical foundations and industry experience in large scale data analysis/platforms, machine learning, sales forecasting.', 'Skills and Qualifications', 'Responsibilities', 'Apply state of the art research techniques to derive insights and spot trends', 'Using machine learning techniques to correlate customer behavior to historical data', 'Experience in large scale data processing, preferably with SparkKnowledge of multi processing and cluster programming techniques.Excellent understanding of common data science toolkitsExperience with data visualisation toolsExperience with SQL and NoSQL databases, such as MongoDB, Cassandra, HBase.Excellent applied statistics skills, such as distributions, statistical testing, regression, etc.', 'Data mining using state-of-the-art methods', 'Experience in large scale data processing, preferably with Spark', 'Nextuple is helping retailers deliver perfect post purchase experiences to customers using AI/ML tools that are integrated with our products. We are looking for a Data-ML Engineer who can help us with building data ingestion pipelines, constructing\xa0 ML infrastructure, building research models and developing A/B testing procedures for ML models. This role is a full time opportunity based in the US or Canada.\xa0', 'Knowledge of multi processing and cluster programming techniques.', 'Good interpersonal and communication skills, including the ability to describe the logic and complex models to stakeholders.', 'If you possess the skills and are passionate about joining a growing team with phenomenal potential, we would love to chat with you.\xa0\xa0Please send us your resume via careers@nextuple.com', 'Job Requirements', 'Ad-hoc analysis and presenting results']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Atlantic Broadband,"Quincy, MA",2 days ago,Be among the first 25 applicants,"['', 'Assembling large, complex sets of data that meet non-functional and functional business requirementsIdentifying, designing and implementing internal process improvements optimizing data delivery, and automating manual processesBuilding required infrastructure for optimal extraction, transformation and loading of data from various data sources including MySQL, Oracle, flat files, CSV into GCP.The process involves:Data modelling: working with the data analyst and GCP engineers to define the data requirements, the source of the data and the formatsData Architecture: defining, together with the GCP engineers, the architecture of the GCP databasesData pipeline development: this includes extraction, formatting, and uploading of the data.', 'Data modelling: working with the data analyst and GCP engineers to define the data requirements, the source of the data and the formats', 'Experience with UNIX/Linux environments', 'Ability to perform root cause analysis on external and internal processes and data to identify opportunities for improvements', 'Responsibilities:', 'Job Summary', 'Ability to analyze existing systems and software to understand current processes and designs', 'Excellent analytic skills associated with working on unstructured datasets', '5 years of experience', ""Ability to build and optimize data sets, 'big data' data pipelines and architecturesAbility to perform root cause analysis on external and internal processes and data to identify opportunities for improvementsExcellent analytic skills associated with working on unstructured datasetsAbility to build automated processes that support data transformation, workload management, data structures, dependency and metadata using tools and scripting languages readily available on a variety of operating systems"", ""Ability to build and optimize data sets, 'big data' data pipelines and architectures"", 'Data Architecture: defining, together with the GCP engineers, the architecture of the GCP databases', 'Knowledge of relational databases: MySQL/MariaDB, Oracle, MS SQL, Big Query', ""Bachelor's degree in Information Technology, Computer Science or equivalent combination of training and/or experience."", ""Bachelor's degree in Information Technology, Computer Science or equivalent combination of training and/or experience.Ability to analyze existing systems and software to understand current processes and designsExperience with UNIX/Linux environmentsFamiliarity accessing APIs and consumer Interfaces that utilize XML, JSON, REST, SOAP, etc.Scripting/programming: Powershell, PERL, Bash etcFunctional knowledge of encryption technologies: SSL, TLS, SSH etcKnowledge of relational databases: MySQL/MariaDB, Oracle, MS SQL, Big QueryStrong initiative to find ways to improve solutions, systems, and processes. 5 years of experience"", 'This role is responsible for the extraction, transformation and load of data from our legacy systems into our new Google Cloud environment.\xa0This position will be part of the Data & Analytics team.', 'Responsibilities', 'Education & Knowledge:', 'Identifying, designing and implementing internal process improvements optimizing data delivery, and automating manual processes', 'Skills:', 'The process involves:', 'Assembling large, complex sets of data that meet non-functional and functional business requirements', 'Ability to build automated processes that support data transformation, workload management, data structures, dependency and metadata using tools and scripting languages readily available on a variety of operating systems', 'Building required infrastructure for optimal extraction, transformation and loading of data from various data sources including MySQL, Oracle, flat files, CSV into GCP.', 'Scripting/programming: Powershell, PERL, Bash etc', 'Data pipeline development: this includes extraction, formatting, and uploading of the data.', 'Strong initiative to find ways to improve solutions, systems, and processes. ', 'Functional knowledge of encryption technologies: SSL, TLS, SSH etc', 'Familiarity accessing APIs and consumer Interfaces that utilize XML, JSON, REST, SOAP, etc.']",Associate,Full-time,Information Technology,Telecommunications,2021-03-24 13:05:10
Data Science Engineer,WorkBoard Inc.,"Denver, CO",7 days ago,Be among the first 25 applicants,"['', 'Leverage your mastery of business, communication, and technical skills to consult with experts in most successful enterprises in the business world. ', 'COMING IN', ' Flexible PTO & sick days Paid holidays Health insurance  401K with employer matching Quarterly All-Hands Meetings And much more! ', 'And much more!', ' Bring at least 5 years of combined experience in Business Intelligence, Data Science and Data Warehousing Software development expertise in Python. Expertise in SQL and experience with large relational database systems. Experience creating an Events framework to enable better data analytics Experience with Data Visualization standard methodologies Experience working with the Product team to define and create Product Success metrics and engagement drivers. BS in Computer Science, Engineering or related technical or equivalent experience Have worked at a fast growing SaaS organization where you’ve demonstrated personal accountability and willingness to go above and beyond the job description. You are genuine, warm, positive, empathetic, and engaging with a passion for technology, and customer success. ', ' Rich, deep data set to draw insights from and influence internal and product and engineering leadership Be part of the foundational team with great people, who have an entrepreneurial mindset and bring their absolute best every day. Leverage your mastery of business, communication, and technical skills to consult with experts in most successful enterprises in the business world.  Provide customer feedback and work with the engineering team to translate the feedback into product features.  ', 'Paid holidays', 'Understand and be a point of contact for our event logging pipeline and the business logic associated with the events', ""Within Three Months, You'll"", 'Become a certified OKR Coach and WorkBoard Expert!', 'Understand our user engagement and product analytics for 3 areas better than anyone', 'honest ~ ', 'Ability to have client-facing conversations on our published metrics and help them understand the product logic', 'Experience with Data Visualization standard methodologies', 'BS in Computer Science, Engineering or related technical or equivalent experience', 'Expertise in SQL and experience with large relational database systems.', 'THE TEAM', 'Understand our current Analytics events structure and framework', 'Be part of the foundational team with great people, who have an entrepreneurial mindset and bring their absolute best every day.', 'Experience creating an Events framework to enable better data analytics', 'Have action plans in place to achieve your Key Results!', 'WorkBoard', 'Rich, deep data set to draw insights from and influence internal and product and engineering leadership', 'Provide customer feedback and work with the engineering team to translate the feedback into product features. ', 'Software development expertise in Python.', 'Work with App Engineering, Product and Customer Success teams to roll out in-product data focused features', 'Maintain / Enhance our internal Dashboard in Python and Sigma', ' Fully understand the Schema for our entire Relational Database Understand and be a point of contact for our event logging pipeline and the business logic associated with the events Ability to have client-facing conversations on our published metrics and help them understand the product logic Maintain / Enhance our internal Dashboard in Python and Sigma Work with App Engineering, Product and Customer Success teams to roll out in-product data focused features Dig deep into data to find levers to increase adoption and success and work cross functionally to drive awareness of these levers ', 'Experience working with the Product team to define and create Product Success metrics and engagement drivers.', 'OUR VALUES - WE LIVE BY THE 4 Hs', 'happy ', '401K with employer matching', 'Fully understand the Schema of our core product tables', 'Fully understand the Schema for our entire Relational Database', 'Have a deep understanding of the problem space we are building the platform for, and how are various offerings are interconnected to provide a comprehensive business solution', 'THE OPPORTUNITY', 'Humble ', 'a Few Of Our Awesome Benefits', ' Have a deep understanding of the problem space we are building the platform for, and how are various offerings are interconnected to provide a comprehensive business solution Fully understand the Schema of our core product tables Understand our current Analytics events structure and framework Understand our user engagement and product analytics for 3 areas better than anyone ', 'Hungry ', 'You are genuine, warm, positive, empathetic, and engaging with a passion for technology, and customer success.', 'Demonstrate 100% understanding of WorkBoard’s underlying data structure and it’s mapping to our UI Objects', 'Dig deep into data to find levers to increase adoption and success and work cross functionally to drive awareness of these levers', 'Bring at least 5 years of combined experience in Business Intelligence, Data Science and Data Warehousing', 'THE WORKBOARD STORY', 'Flexible PTO & sick days', ""Within One Month, You'll"", 'Health insurance ', ' Become a certified OKR Coach and WorkBoard Expert! Demonstrate 100% understanding of WorkBoard’s underlying data structure and it’s mapping to our UI Objects Have action plans in place to achieve your Key Results! ', 'Have worked at a fast growing SaaS organization where you’ve demonstrated personal accountability and willingness to go above and beyond the job description.', ""Within Six Months, You'll"", 'Quarterly All-Hands Meetings']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,HAN Staffing,"Sunnyvale, CA",19 hours ago,Be among the first 25 applicants,"['', ' Proven experience creating and maintaining fault-tolerant data pipelines using relational, non-relational, and cloud-based data warehouse systems Data modelling and data architecture experience Initiate and drive projects to completion with minimal guidance in a fast-paced, dynamic environment Detail-oriented, inquisitive by nature, with a can-do attitude and a passion for quality results and an interest in learning new technologies Strong coding skills and experience with SQL, Python, and Java Proven experience securing DB services for SQL and noSQL environments (MySQL, Kibana)', ' Strong coding skills and experience with SQL, Python, and Java', 'Requirements', ' Aggregate and store quality data in an efficient and transparent manner for reporting, analytics, and data science uses', ' Aggregate and store quality data in an efficient and transparent manner for reporting, analytics, and data science uses Implement tools for monitoring and ensuring data quality and consistency Build, support, and improve custom tools necessary for data and analytics self-serve initiatives Work closely with Security and Operations teams to develop and enforce proper data security and privacy practices Work cross-functionally and communicate in an effective manner Investigate, advocate for, and proactively obtain new data sets', ' Investigate, advocate for, and proactively obtain new data sets', ' Detail-oriented, inquisitive by nature, with a can-do attitude and a passion for quality results and an interest in learning new technologies', ' Build, support, and improve custom tools necessary for data and analytics self-serve initiatives', ' Data modelling and data architecture experience', ' Initiate and drive projects to completion with minimal guidance in a fast-paced, dynamic environment', ' Work closely with Security and Operations teams to develop and enforce proper data security and privacy practices', ' Proven experience securing DB services for SQL and noSQL environments (MySQL, Kibana)', 'Job Description', ' Proven experience creating and maintaining fault-tolerant data pipelines using relational, non-relational, and cloud-based data warehouse systems', ' Work cross-functionally and communicate in an effective manner', ' Implement tools for monitoring and ensuring data quality and consistency']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Parsons Corporation,"Centreville, VA",2 days ago,Be among the first 25 applicants,"['', 'You have a minimum of 5 years of database development experience at an enterprise level.', 'Knowledge of and experience with AWS (RDS, Dynamo, EC2, SQS, SNS, Kinesis)', 'You can exercise discretion and work independently.', 'You have a minimum of 5 years of database development experience at an enterprise level.You’ve worked with SQL/NoSQL Databases, and you have proven, solid experience with data warehousing, data lakes, and data pipelining.You’re comfortable with both Linux and Windows systems.', 'You enjoy working with technical teams to create a solid data ingestion strategy, and be responsible for its maintenance.', 'You can confidently contribute to overall function and system design, as well as define db standards, policies, and procedures for best practice data management. You’ve facilitated many data requirement sessions, and you can create data models from disparate systems to achieve effective visualizations. You know how to build enterprise grade data solutions, and you can design databases to support business applications that are scalable, secure, performant, and reliable.You can develop database architectural strategies at the modeling, design and implementation stages, and collaborate with system architects, software architects, design analysts, and others to address specific business or industry requirements.You’re experienced in the development of data models for applications, metadata tables, views or related database structures, and setting up database clusters, backup and recovery processes.You can design database applications, such as interfaces, data transfer mechanisms, global temporary tables, data partitions, and function-based indexes to enable efficient access of the generic database structure.You enjoy working with technical teams to create a solid data ingestion strategy, and be responsible for its maintenance.You can document and communicate database schemas, using accepted notations and you can clearly articulate and explain your work and your vision to non-technical personnel.You’re comfortable working in SCRUM and/or Kanban environments.You can exercise discretion and work independently.You have excellent written and oral communication skills.', 'Minimum Clearance Required To Start', 'T4 Program Summary: The Federal Aviation Administration (FAA) awarded the Technical Support Services Contract 4 (T4) to Parsons as the prime contractor. This $1.3 billion+ contract has a 4-year base period with two 3-year options. The statement of work includes a variety of activities that support the FAA’s Capital Investment Plan (CIP) modernization efforts: Site selection and engineering, construction, environmental and fire/life safety, equipment installation and testing, CAD, and other technical services as required. Work is performed across the nation in each of the FAA’s nine regions as well as the Aeronautical Center in Oklahoma City, Oklahoma, and the Technical Center in Atlantic City, New Jersey and in several U.S. territories. An average staff of 400-500 with a peak staff over 650 supports this contract, in addition to those subcontractors who perform construction work.', 'Required', 'You’re comfortable working in SCRUM and/or Kanban environments.', 'You’re experienced in the development of data models for applications, metadata tables, views or related database structures, and setting up database clusters, backup and recovery processes.', 'It would be great if you also have the following:', 'Your Career So Far', 'EXPERIENCE & SKILLS:', 'You’ve facilitated many data requirement sessions, and you can create data models from disparate systems to achieve effective visualizations. ', 'We are looking for a proactive, enthusiastic, and adaptable Data Engineer to join our growing team on a fast-moving custom-built enterprise application ecosystem. ', 'Your Business Skills And Dev Competencies Look Like This', 'You have excellent written and oral communication skills.', 'You know how to build enterprise grade data solutions, and you can design databases to support business applications that are scalable, secure, performant, and reliable.', 'You’re comfortable with both Linux and Windows systems.', 'Database development experience within the defense, security, and infrastructure government contracting sector(s)', 'Database development experience within the defense, security, and infrastructure government contracting sector(s)Knowledge of and experience with AWS (RDS, Dynamo, EC2, SQS, SNS, Kinesis)Redis experience', 'You’ve worked with SQL/NoSQL Databases, and you have proven, solid experience with data warehousing, data lakes, and data pipelining.', 'You can document and communicate database schemas, using accepted notations and you can clearly articulate and explain your work and your vision to non-technical personnel.', 'You can develop database architectural strategies at the modeling, design and implementation stages, and collaborate with system architects, software architects, design analysts, and others to address specific business or industry requirements.', 'You can confidently contribute to overall function and system design, as well as define db standards, policies, and procedures for best practice data management. ', 'Redis experience', 'EDUCATION:', 'Job Description', 'Undergraduate degree or higher in a relevant field, such as Computer Engineering or Computer Science, OR an equivalent combination of educations and experience ', 'You can design database applications, such as interfaces, data transfer mechanisms, global temporary tables, data partitions, and function-based indexes to enable efficient access of the generic database structure.']",Not Applicable,Full-time,Information Technology,Construction,2021-03-24 13:05:10
Data Engineer,Ergo,"New York, NY",4 weeks ago,Be among the first 25 applicants,"['', ' Technical: Strong coding skills and experience developing data pipelines for warehouses/lakes (academic project or work setting)', ' Exposure of working with Big data tools and technologies like Data Bricks, HIVE and SPARK.', ' Experience/knowledge of insurance preferably commercial specialty', ' Knowledgeable with agile development methodologies and tools (i.e. Jira, AzureDevOps)', ' Maintain a close working relationship with other group data teams within Munich RE to understand existing solutions and resources available for use within MRSI.', ' Innovation: Forward thinking and passionate about solving problems and has a strong drive to constantly learn and keep up to speed with the new technologies', ' Coordinate closely with delivery lead and data team lead to ensure work is scoped, planned and delivered on-time and budget.', ' Adhere and enhance ETL and architecture standards and procedures to ensure consistency and accuracy of development, execution, knowledge sharing and documentation.', ' Collaborate with technical and business experts to improve our existing data landscape and drive better data driven decisions within the business.', ' 3 + years hands on experience in SQL, HSQL, Python or R, and Azure Web services (Azure SQL, Azure Data Factory, Logic Apps etc.) and other web development application', ' Organization: Ability to deal with ambiguity and operate in a fast-paced and rapidly changing environment with competing project priorities', ' Communication: Ability to communicate clearly, effectively, and concisely, both verbally and in written form.', ' In depth knowledge of database structure principles, data modelling concepts, ETL procedures, Cloud data practices etc.', ' Act as the owner and developer of a predefined subset of processes and data pipelines, ensuring highly accurate and reliable delivery of outputs', ' Experience working on data projects involving data engineers, architects and analysts.', ' Experience with writing advanced SQL queries and performing query optimization.', 'Qualifications', ' Act as the owner and developer of a predefined subset of processes and data pipelines, ensuring highly accurate and reliable delivery of outputs  Collaborate with technical and business experts to improve our existing data landscape and drive better data driven decisions within the business.  Adhere and enhance ETL and architecture standards and procedures to ensure consistency and accuracy of development, execution, knowledge sharing and documentation.  Work closely with business architecture, IT and analytic teams help implement an end to end data strategy from data ingest to output into business tools.  Coordinate closely with delivery lead and data team lead to ensure work is scoped, planned and delivered on-time and budget.  Adhere and enhance the existing data governance framework and personally ensure compliance with regulatory and privacy requirements.  Maintain a close working relationship with other group data teams within Munich RE to understand existing solutions and resources available for use within MRSI.', ' Adhere and enhance the existing data governance framework and personally ensure compliance with regulatory and privacy requirements.', ' Work closely with business architecture, IT and analytic teams help implement an end to end data strategy from data ingest to output into business tools.', ' 3+ years’ experience in a data engineering role with exposure to system design, data modelling, data-curation and validation procedures', ' Bachelor’s Degree in Computer Science, Statistics, Math or equivalent combination of education and experience', ' Experience in creating scalable and robust Data Pipelines and working with data lakes.', ' Commercial : Ability to understand complex business priorities and translate them into clearly defined technical/data specifications for implementation', ' Knowledge of data provisions requirements for web based business intelligence tools including Cognos, PowerBI and Tableau.', 'In This Position You Will']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,GoHealth,"Charlotte, NC",3 weeks ago,146 applicants,"['', 'Hands-on experience with troubleshooting performance issues and fine tuning queries.', 'Experience extracting data from relational and document databases.', 'Knowledge of and experience with a version control system (such as Git, Mercurial, SVN, etc).', 'Create and maintain documentation of the technical detail design, operational support and maintenance procedures for all data pipelines.', 'Monitor and ensure operational stability of data pipelines.', 'Familiarity with data warehousing platforms, such as Redshift, Snowflake, SQL Server, etc.', 'Happy hours, ping-pong tournaments, and more company-sponsored events', 'Design, develop and deploy batch and streaming data pipelines.', 'Collaborate with the rest of the Engineering Team, subject matter experts and department leaders to understand, analyze, build and deliver new data-related processes and/or reports.', 'Casual dress code', ' Design, develop and deploy batch and streaming data pipelines. Monitor and ensure operational stability of data pipelines. Create and maintain documentation of the technical detail design, operational support and maintenance procedures for all data pipelines. Ensure data quality and compliance with development, architecture, reporting, and regulatory standards throughout entire data pipeline. Collaborate with the rest of the Engineering Team, subject matter experts and department leaders to understand, analyze, build and deliver new data-related processes and/or reports. ', 'Experience consuming data over HTTP and in formats such as HTML, XML, and JSON.', 'Open vacation policy', 'Medical, dental, vision, and life insurance benefits', '4+ years of experience in the design and development of data pipelines and tasks.', ' Open vacation policy 401k program with company match Medical, dental, vision, and life insurance benefits Flexible spending accounts Commuter and transit benefits Professional growth opportunities Casual dress code Generous employee referral bonuses Happy hours, ping-pong tournaments, and more company-sponsored events Subsidized gym memberships GoHealth is an Equal Opportunity Employer', 'GoHealth is an Equal Opportunity Employer', 'Benefits And Perks', '401k program with company match', 'Responsibilities', 'Bachelor’s Degree in computer science or equivalent experience required.', 'Proficiency in Java or Python programming languages.', 'Skills And Experience', 'Commuter and transit benefits', 'Strong analytical and problem solving ability with strong attention to detail and accuracy.', 'Professional growth opportunities', 'Understanding of data warehousing concepts and dimensional data modeling.', ""Due to the unprecedented situation of COVID-19, GoHealth has decided to protect our current and future employees by managing our business remotely. This is inclusive of interviewing, onboarding and each role day-to-day. Please consider that our roles will not be remote long-term and will return to an office setting once we're safe to do so following the guidance of local health authorities’ and the CDC."", 'Flexible spending accounts', 'Subsidized gym memberships', 'Generous employee referral bonuses', ' Bachelor’s Degree in computer science or equivalent experience required. 4+ years of experience in the design and development of data pipelines and tasks. Strong analytical and problem solving ability with strong attention to detail and accuracy. Understanding of data warehousing concepts and dimensional data modeling. Hands-on experience with troubleshooting performance issues and fine tuning queries. Experience extracting data from relational and document databases. Experience consuming data over HTTP and in formats such as HTML, XML, and JSON. Knowledge of and experience with a version control system (such as Git, Mercurial, SVN, etc). Proficiency in Java or Python programming languages. Familiarity with data warehousing platforms, such as Redshift, Snowflake, SQL Server, etc. ', 'Ensure data quality and compliance with development, architecture, reporting, and regulatory standards throughout entire data pipeline.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Gusto,"Denver, CO",4 weeks ago,146 applicants,"['', 'Be an owner as you architect, build, and refine our data infrastructure technologies, using our development workflow — the outcome of your work will drive decisions that affect billions of dollars of transactions', 'At least 2 years of software engineering experience (Python, Ruby or Java).', 'Here’s What You’ll Do Day-to-day', 'Ability to turn vague requirements into clear deliverables with minimal guidance', 'About Gusto', 'Our Engineering Culture and Values', 'Experience building out data pipelines, efficient ETL design, implementation, and maintenance', 'Experience with AWS tools', 'Here’s What We’re Looking For', ' At least 2 years of software engineering experience (Python, Ruby or Java). Passion for creating data infrastructure technologies from scratch using the right tools for the job Experience with AWS tools Experience building out data pipelines, efficient ETL design, implementation, and maintenance Experience building and maintaining a data warehouse in production environments Experience with systems for transforming large datasets such as Spark and Hadoop Ability to turn vague requirements into clear deliverables with minimal guidance ', 'Experience building and maintaining a data warehouse in production environments', 'Be surrounded by people who are doing the best work of their lives and loving every moment of it', ' Be an owner as you architect, build, and refine our data infrastructure technologies, using our development workflow — the outcome of your work will drive decisions that affect billions of dollars of transactions Build and refine a fault-tolerant data ingestion pipeline into our data warehouse, while helping guide the decisions about the future of our data infrastructure Work with engineers and product managers to analyze edge cases, clear ambiguities, and plan for architectural scalability Write complex and efficient queries to transform raw data sources into easily accessible models for our teams (e.g., Product, Growth, Finance, Risk) Care deeply about helping millions of small business owners focus on what they love Be surrounded by people who are doing the best work of their lives and loving every moment of it ', 'Build and refine a fault-tolerant data ingestion pipeline into our data warehouse, while helping guide the decisions about the future of our data infrastructure', ' Our Engineering Culture and Values How We Built a Service-Driven Team Our Diversity Goals and Efforts ', 'Work with engineers and product managers to analyze edge cases, clear ambiguities, and plan for architectural scalability', 'Our Diversity Goals and Efforts', 'Learn More About The Team', 'Write complex and efficient queries to transform raw data sources into easily accessible models for our teams (e.g., Product, Growth, Finance, Risk)', 'Passion for creating data infrastructure technologies from scratch using the right tools for the job', 'Experience with systems for transforming large datasets such as Spark and Hadoop', 'How We Built a Service-Driven Team', 'Care deeply about helping millions of small business owners focus on what they love']",Entry level,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Junior Data Engineer,Arthur Financial ,"London, KY",4 days ago,Be among the first 25 applicants,"['', ' Highly numerate – expecting a good (2.1 or above) degree in mathematics/computer science or related disciplines. Programming skills in Python (C#, R, MATLAB will be considered) Knowledge of SQL Server and ETL techniques (especially SSIS) Advanced knowledge of Microsoft Excel and VBA', ' Support ETL processes', ' You will assist in the provision of technical support within the Data & MI team and wider business. Develop/enhance new or existing solutions that support analytics capabilities, database usage and dashboards using new or existing technologies Develop and maintain data warehouses, structures and loads. Analyse new datasets/warehouses. Automate business processes using SQL, SSIS, Python, C# or other relevant technologies. Support ETL processes Liaise with other business units to help them identify and build the right solutions they require.', ' Advanced knowledge of Microsoft Excel and VBA', ' Develop and maintain data warehouses, structures and loads. Analyse new datasets/warehouses.', ' Programming skills in Python (C#, R, MATLAB will be considered)', ' Knowledge of SQL Server and ETL techniques (especially SSIS)', 'Main Responsibilities', ' Highly numerate – expecting a good (2.1 or above) degree in mathematics/computer science or related disciplines.', 'Requirements', 'DATA ENGINEER', ' Automate business processes using SQL, SSIS, Python, C# or other relevant technologies.', ' You will assist in the provision of technical support within the Data & MI team and wider business.', ' Liaise with other business units to help them identify and build the right solutions they require.', ' Develop/enhance new or existing solutions that support analytics capabilities, database usage and dashboards using new or existing technologies']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,"Integrated Resources, Inc ( IRI )","Richmond, VA",1 day ago,Be among the first 25 applicants,"['', 'To ensure data quality, integrity, and security throughout the data processing system;', 'The demonstrated ability to translate data architecture and stakeholder requirements into technology solutions for data acquisition, handling, and storage;To ensure data quality, integrity, and security throughout the data processing system;To prototype data and analytics solutions that establish concept viability;To collaborate successfully with technical and business stakeholders;To provide effective oral and written communications on related issues.A considerable knowledge of database management systems, query tools, data schemas, and distributed systems, relational databases; of tools to extract, clean, transform, and load/warehouse data; and of cloud computing environments.Considerable skill in the use of a personal computer with data engineering and standard office applications.Must have a strong background in SQL Server and SQL development.Must have a strong understanding in SSIS, Talend and other ETL solutions.', 'Location: Chesterfield, VA ', 'Developing data analytics tools to provide business insights.', 'The demonstrated ability to translate data architecture and stakeholder requirements into technology solutions for data acquisition, handling, and storage;', 'A considerable knowledge of database management systems, query tools, data schemas, and distributed systems, relational databases; of tools to extract, clean, transform, and load/warehouse data; and of cloud computing environments.', 'Title: Data Engineer', 'Competencies Required:', 'Duration: 3+ months with possible extension ', 'To collaborate successfully with technical and business stakeholders;', 'Building infrastructures that overcome data delivery and handling problems for applications that consume, process, and store large volumes, varieties, and velocities of data;', 'Warehousing large, complex datasets that meet business requirements;', 'To provide effective oral and written communications on related issues.', 'Must have a strong understanding in SSIS, Talend and other ETL solutions.', 'Building infrastructures that overcome data delivery and handling problems for applications that consume, process, and store large volumes, varieties, and velocities of data;Warehousing large, complex datasets that meet business requirements;Developing data analytics tools to provide business insights.', 'Duties include: ', 'Considerable skill in the use of a personal computer with data engineering and standard office applications.', 'To prototype data and analytics solutions that establish concept viability;', 'Must have a strong background in SQL Server and SQL development.']",Entry level,Contract,Information Technology,Staffing and Recruiting,2021-03-24 13:05:10
Data Engineer,Electronic Arts (EA),"Seattle, WA",3 weeks ago,126 applicants,"['', ' Experience with AWS cloud services: EC2, EMR, RDS, Redshift. Or similar experience with GCP or Azure. ', ' 1+ years experience in data technologies as data engineer or related roles', ' EA Security  ', ' Keep our data separated and secure across national boundaries through multiple data centers and AWS regions. ', ' Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.  Experience with big data tools: Hadoop, Spark, Kafka, AWS etc.  Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.  Experience with data pipeline and workflow management tools: Airflow, Azkaban, Luigi, etc.  Experience with AWS cloud services: EC2, EMR, RDS, Redshift. Or similar experience with GCP or Azure.  Experience with stream-processing systems: Storm, Spark-Streaming, etc.  Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.  Master degree required in a engineering discipline (i.e. Computer Science, Industrial Engineering or equivalent)  1+ years experience in data technologies as data engineer or related roles', ' Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases. ', ' Experiment with and recommend new technologies that simplify or improve the tech stack ', ' Master degree required in a engineering discipline (i.e. Computer Science, Industrial Engineering or equivalent) ', ' Use tools, debugging logs, performance metrics, and other data sources to make code- and application-level improvements ', ' Strong interpersonal and communication skills ', ' Experience with big data tools: Hadoop, Spark, Kafka, AWS etc. ', ' Results-driven and highly quantitative ', ' Able to multi-task and thrive in a dynamic, fast-paced environment ', 'Requirements', ' Experience with stream-processing systems: Storm, Spark-Streaming, etc. ', 'Responsibilities', 'Data Engineer ', ' Work collaboratively and communicate effectively with stakeholders ', ' Design, develop, and maintain data pipelines and backend services for real-time decisioning, reporting, optimization, data collection, and related functions  Design efficient data structures and database schemas  Incorporate data processing and workflow management tools into pipeline design  Onboard new data to support business needs  Use tools, debugging logs, performance metrics, and other data sources to make code- and application-level improvements  Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.  Create and maintain SLAs on data reporting through reports and dashboards  Work collaboratively and communicate effectively with stakeholders  Experiment with and recommend new technologies that simplify or improve the tech stack ', 'Skills And Experience', ' Experience with data pipeline and workflow management tools: Airflow, Azkaban, Luigi, etc. ', ' Strong problem solving skills and ability to collaborate within a team environment ', ' Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc. ', ' Experience with relational SQL and NoSQL databases, including Postgres and Cassandra. ', ' Incorporate data processing and workflow management tools into pipeline design ', ' True passion for understanding customer behavior on-platform and in-game ', 'Data Engineer', ' Able to multi-task and thrive in a dynamic, fast-paced environment  Strong problem solving skills and ability to collaborate within a team environment  Results-driven and highly quantitative  True passion for understanding customer behavior on-platform and in-game  Strong interpersonal and communication skills ', 'EA Security ', ' Create and maintain SLAs on data reporting through reports and dashboards ', ' Design efficient data structures and database schemas ', ' Onboard new data to support business needs ', ' The Challenge Ahead ', ' Design, develop, and maintain data pipelines and backend services for real-time decisioning, reporting, optimization, data collection, and related functions ']",Not Applicable,Full-time,Strategy/Planning,Computer Games,2021-03-24 13:05:10
Data Engineer (Remote Eligible),ID.me,"McLean, VA",19 hours ago,Be among the first 25 applicants,"['', 'Ability to work independently and autonomously, as well as part of a team', 'Develop, prototype, and build frameworks based on open source and commercially available tools', 'ID.me Core Values:', '3+ years of hands-on experience in data engineering for a SaaS company or a mature startup', 'Demonstrate a passion for serving the needs of internal and external customers by enabling them with self-service reporting tools and analytics capabilities', 'Develop technical solutions using proven techniques in data and analytics processes', 'Must be located in the continental U.S.', 'Some level of experience working in the cloud - AWS, Azure, or GCP', 'Data-driven, detail-oriented individual with excellent storytelling and problem-solving abilities', 'Orchestrate and maintain data pipelines that meet security standards and ensure the integrity and quality of data', 'Evangelize data driven culture by breaking down silos and encouraging data sharing', 'Understand the data related challenges, nuances, and requirements to identify and recommend the optimal technical approach', ' Develop technical solutions using proven techniques in data and analytics processes Develop, prototype, and build frameworks based on open source and commercially available tools Orchestrate and maintain data pipelines that meet security standards and ensure the integrity and quality of data Demonstrate a passion for serving the needs of internal and external customers by enabling them with self-service reporting tools and analytics capabilities Drive the execution of data initiatives that provide key performance metrics Understand the data related challenges, nuances, and requirements to identify and recommend the optimal technical approach Train and educate team members as well as stakeholders about best practices in data engineering and governance Collaborate closely with the engineering and devops team to implement DataOps, thus reducing our analytics development cycle Research and improve our data platform to ingest, process, transform, and distribute insightful data to our audience ranging from executives, analysts, and engineers to customers, vendors, and partners Evangelize data driven culture by breaking down silos and encouraging data sharing ', 'Overview', 'Experience in developing, managing, and manipulating large, complex datasets', ' 3+ years of hands-on experience in data engineering for a SaaS company or a mature startup Proven experience working with various tools but more importantly, familiarity with how to best assemble and deploy production ready data stack to any cloud environment BS in a quantitative or scientific field such as computer science, computer engineering or equivalent experience Experience in applying agile software development approach - Git, CI/CD, Jira, etc - to data engineering Familiarity with popular programming languages (such as Ruby, Python, .NET, etc) Exceptional fluency with SQL; you conquered the join venn diagram long ago and have moved on to explaining cost based optimization to your peers on the engineering team Some level of experience working in the cloud - AWS, Azure, or GCP Experience with ingesting, processing, and visualizing data sources of varying types - structured/relational and unstructured Experience in developing, managing, and manipulating large, complex datasets Data-driven, detail-oriented individual with excellent storytelling and problem-solving abilities Ability to work independently and autonomously, as well as part of a team Superb time management, prioritization of tasks and ability to meet deadlines with little supervision Must be located in the continental U.S. ', 'Experience with ingesting, processing, and visualizing data sources of varying types - structured/relational and unstructured', 'Superb time management, prioritization of tasks and ability to meet deadlines with little supervision', 'Exceptional fluency with SQL; you conquered the join venn diagram long ago and have moved on to explaining cost based optimization to your peers on the engineering team', 'Familiarity with popular programming languages (such as Ruby, Python, .NET, etc)', 'Experience in applying agile software development approach - Git, CI/CD, Jira, etc - to data engineering', 'Vision', 'Mission', 'Responsibilities', 'People', 'Train and educate team members as well as stakeholders about best practices in data engineering and governance', 'Qualifications', 'Drive the execution of data initiatives that provide key performance metrics', 'BS in a quantitative or scientific field such as computer science, computer engineering or equivalent experience', 'Collaborate closely with the engineering and devops team to implement DataOps, thus reducing our analytics development cycle', 'Proven experience working with various tools but more importantly, familiarity with how to best assemble and deploy production ready data stack to any cloud environment', 'Research and improve our data platform to ingest, process, transform, and distribute insightful data to our audience ranging from executives, analysts, and engineers to customers, vendors, and partners']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
IT Lead - Data and Analytics Engineer ,Johnson & Johnson,"New Brunswick, NJ",6 days ago,33 applicants,"['', 'Supervise day-to-day execution and delivery from start to completion involving deadlines, achievement and processes; engaging necessary and appropriate resources across product lines and shared services per solution blueprint and align IT capacity with top business priorities.', 'Solid understanding of change management principles and performance evaluation processes is preferred.', ' Technical scope of knowledge includes a broad understanding of solutions including machine learning, artificial intelligence & data science.  Technical skills range from experience with core analytics tools like Qlik, Tableau, Informatica & Alteryx to cloud computing platforms like Teradata, Cloudera, AWS, Azure, and data platforms such as Informatica, WebMethods, Apigee. Demonstrated strength in examining issues, crafting strategy & influencing effectively across the organization, driving people, process & technology change.  Ability to succeed in a matrixed environment & influence adoption of new thinking- PREF Solid understanding of change management principles and performance evaluation processes is preferred. ', 'Demonstrated strength in examining issues, crafting strategy & influencing effectively across the organization, driving people, process & technology change. ', 'Technical skills range from experience with core analytics tools like Qlik, Tableau, Informatica & Alteryx to cloud computing platforms like Teradata, Cloudera, AWS, Azure, and data platforms such as Informatica, WebMethods, Apigee.', ' A minimum of a bachelor’s degree is required; a field in Computer Science, Math, or other quantitative field. A minimum of six (6) years of Information Technology experience  Experience implementing data solutions using Informatica, Alteryx, Cloudera, Teradata, AWS, Azure, Python, Tableau, WebMethods, informatica ETL, Streamsets and similar technologies. A minimum of (3) three years of hands on experience working with large scale data and advance analytics initiatives, including data discovery, business intelligence, data quality management, metadata management, master data management and related disciplines  Hands on experience with relational databases, data warehouses, operational data stores, in-memory databases, SQL and noSQL technologies and concepts. Experience in all phases of an established SDLC and/or AGILE methodologies. Experience applying concepts of Agile solutioning to rapidly deliver value through user design, prototyping, Minimal Viable Product & continuous improvement.  Experience leading a technical team across business and technical areas. Must have strong written and oral communication skills. ', 'Executing the plan and determining priorities for building the data and analytics capabilities and scaling the experiments to production implementation', 'A minimum of a bachelor’s degree is required; a field in Computer Science, Math, or other quantitative field.', 'Preferred', 'Must have strong written and oral communication skills.', 'Experience leading a technical team across business and technical areas.', 'A minimum of six (6) years of Information Technology experience ', 'Technical scope of knowledge includes a broad understanding of solutions including machine learning, artificial intelligence & data science. ', 'Working with business and technology partners to identify business priorities and product needs.', 'Conducting experiments with limited sized audiences or data samples to understand potential positive impact and value', 'Required', 'Evolving experiments to full production implementations using data lakes and data warehousing platforms, Master & Reference data management tools, Data Quality and Metadata Management tools, analytical tools and artificial intelligence', 'A minimum of (3) three years of hands on experience working with large scale data and advance analytics initiatives, including data discovery, business intelligence, data quality management, metadata management, master data management and related disciplines ', 'In This Role, Your Responsibilities Would Focus On', 'IT Lead - Data and Analytics Engineer, t', 'Qualifications', 'Hands on experience with relational databases, data warehouses, operational data stores, in-memory databases, SQL and noSQL technologies and concepts.', 'Ability to succeed in a matrixed environment & influence adoption of new thinking- PREF', 'Experience implementing data solutions using Informatica, Alteryx, Cloudera, Teradata, AWS, Azure, Python, Tableau, WebMethods, informatica ETL, Streamsets and similar technologies.', 'Leading teams to support Data Analytics and integration that enable many J&J applications to subscribe data. ', 'Experience in all phases of an established SDLC and/or AGILE methodologies.', ' Working with business and technology partners to identify business priorities and product needs. Conducting experiments with limited sized audiences or data samples to understand potential positive impact and value Evolving experiments to full production implementations using data lakes and data warehousing platforms, Master & Reference data management tools, Data Quality and Metadata Management tools, analytical tools and artificial intelligence Leading teams to support Data Analytics and integration that enable many J&J applications to subscribe data.  Ensuring IT Operations support to HR Data Solutions are adequate and monitor performance metrics for optimal support. Supervise day-to-day execution and delivery from start to completion involving deadlines, achievement and processes; engaging necessary and appropriate resources across product lines and shared services per solution blueprint and align IT capacity with top business priorities. Executing the plan and determining priorities for building the data and analytics capabilities and scaling the experiments to production implementation ', 'Ensuring IT Operations support to HR Data Solutions are adequate and monitor performance metrics for optimal support.', 'Job Description', 'Experience applying concepts of Agile solutioning to rapidly deliver value through user design, prototyping, Minimal Viable Product & continuous improvement. ']",Not Applicable,Full-time,Information Technology,Hospital & Health Care,2021-03-24 13:05:10
Data Engineer,Vivid Resourcing Ltd,"Austin, Texas Metropolitan Area",4 weeks ago,128 applicants,"['', '3+ years in SQL', 'Competitive salary + bonus structure', 'Fully paid medical benefits', 'Send me your resume at carlos.rueda@vividresourcing.com', 'Unlimited PTO', 'Remote flexibility', 'Currently recruiting for an excellent Data Engineer in the Austin area for an exciting tech company! They are looking to grow their Data platform team extensively this year and are looking to bring on two Data Engineers.', 'Please reach out if you are interested.', 'With a growing team, you will make a big impact on the direction they are looking to move forward and ultimately have the opportunity to grow with the company!', 'Experienced in AWS but okay with GCP or Azure', 'Competitive salary + bonus structureRemote flexibilityFully paid medical benefitsUnlimited PTOAmazing office culture; hosted lunches, happy hours, and special events!', 'Amazing office culture; hosted lunches, happy hours, and special events!', 'Skills:', 'What they offer:', '**Unfortunately this position can not sponsor at this time. If you are authorized to work in the US you are encouraged to apply (US citizen or GC holder)**\xa0', 'Fulltime Direct Hire\xa0', 'Data Engineer', 'Location: Austin, TX (Remote)', 'Experience building ETL pipelines', '3+ years in Python, Scala, or Spark.', 'Bonus if you have experience with Snowflake or Kafka', '2-5 years in Data Engineering or Analytics3+ years in SQL3+ years in Python, Scala, or Spark.Experienced in AWS but okay with GCP or AzureBonus if you have experience with Snowflake or KafkaExperience building ETL pipelines', '2-5 years in Data Engineering or Analytics']",Mid-Senior level,Full-time,Information Technology,Staffing and Recruiting,2021-03-24 13:05:10
Data Engineer/Analyst,System One,"Windsor Mill, MD",1 week ago,Be among the first 25 applicants,"['', 'Expert in designing complex and semantically rich data structures.', 'Location', 'Worked with OLTP/data warehouse modeling, and play key role in multiple projects and act as knowledge expert (Technical, Functional).', 'Extensively used ETL methodologies for supporting data extraction, transformations and loading processing, in a corporate-wide-ETL Solution.', 'Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.', 'Position Description:', 'Ability to understand complex business processes to derive conceptual and logical data models.', 'Education', 'Good data analysis, problem solving and SQL skills.', 'Expertise in Conceptual Data Modeling, Logical Data Modeling, Physical Data Modeling, Enterprise Data Warehouse Design, DataMart Design, Metadata, Data Quality, Master Data Management and Master Data/Data Governance using Erwin', 'Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements', 'Lead complex discussions and engagements that may involve multiple project teams from client.', 'Ability to optimize and performance tune SQL queries', 'Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirementsAble to work independently with BA and PM to gather requirements, perform data discovery, come up with data model, work with data engineers to build data pipeline, and work with developers to answer their questions.Extensively used ETL methodologies for supporting data extraction, transformations and loading processing, in a corporate-wide-ETL Solution.Expertise in Conceptual Data Modeling, Logical Data Modeling, Physical Data Modeling, Enterprise Data Warehouse Design, DataMart Design, Metadata, Data Quality, Master Data Management and Master Data/Data Governance using ErwinTechnical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.Ability to understand complex business processes to derive conceptual and logical data models.Lead complex discussions and engagements that may involve multiple project teams from client.Experience designing relational data models, analytical data models and non-relational data models for OLTP and advanced analytic applications.Expert in designing complex and semantically rich data structures.Worked with OLTP/data warehouse modeling, and play key role in multiple projects and act as knowledge expert (Technical, Functional).Ability to optimize and performance tune SQL queriesGood data analysis, problem solving and SQL skills.', 'Skills Requirements:', 'Able to work independently with BA and PM to gather requirements, perform data discovery, come up with data model, work with data engineers to build data pipeline, and work with developers to answer their questions.', 'Experience designing relational data models, analytical data models and non-relational data models for OLTP and advanced analytic applications.', 'Clearance']",Entry level,Other,Information Technology,Information Technology and Services,2021-03-24 13:05:10
"Data Engineer, Analytics",Facebook,"Washington, DC",1 week ago,25 applicants,"['', 'Solve our most challenging data integration problems, utilizing optimal ETL patterns, frameworks, query techniques, sourcing from structured and unstructured data sources.', 'Optimize pipelines, dashboards, frameworks, and systems to facilitate easier development of data artifacts.', 'Knowledge and practical application of Python.', 'Create and contribute to frameworks that improve the efficacy of logging data, while working with data infrastructure to triage issues and resolve.', 'Conceptualize and own the data architecture for multiple large-scale projects, while evaluating design and operational cost-benefit tradeoffs within systems.', 'Conceptualize and own the data architecture for multiple large-scale projects, while evaluating design and operational cost-benefit tradeoffs within systems.Create and contribute to frameworks that improve the efficacy of logging data, while working with data infrastructure to triage issues and resolve.Collaborate with engineers, product managers, and data scientists to understand data needs, representing key data insights visually in a meaningful way.Define and manage SLA for all data sets in allocated areas of ownership.Determine and implement the security model based on privacy requirements, confirm safeguards are followed, address data quality issues, and evolve governance processes within allocated areas of ownership.Design, build, and launch collections of sophisticated data models and visualizations that support multiple use cases across different products or domains.Solve our most challenging data integration problems, utilizing optimal ETL patterns, frameworks, query techniques, sourcing from structured and unstructured data sources.Assist in owning existing processes running in production, optimizing complex code through advanced algorithmic concepts.Optimize pipelines, dashboards, frameworks, and systems to facilitate easier development of data artifacts.Influence product and cross-functional teams to identify data opportunities to drive impact.Mentor team members by giving/receiving actionable feedback.', 'Preferred Qualification', '5+ years experience in the data warehouse space.5+ years experience in custom ETL design, implementation and maintenance.5+ years experience with object-oriented programming languages.5+ years experience with schema design and dimensional data modeling.5+ years experience in writing SQL statements.Experience analyzing data to identify gaps and inconsistencies.Experience managing and communicating data warehouse plans to internal clients.', 'BS/BA in Technical Field, Computer Science or Mathematics.', 'Collaborate with engineers, product managers, and data scientists to understand data needs, representing key data insights visually in a meaningful way.', 'Determine and implement the security model based on privacy requirements, confirm safeguards are followed, address data quality issues, and evolve governance processes within allocated areas of ownership.', 'Design, build, and launch collections of sophisticated data models and visualizations that support multiple use cases across different products or domains.', 'Influence product and cross-functional teams to identify data opportunities to drive impact.', 'Minimum Qualification', '5+ years experience with schema design and dimensional data modeling.', 'BS/BA in Technical Field, Computer Science or Mathematics.Experience working with either a MapReduce or an MPP system.Knowledge and practical application of Python.Experience working autonomously in global teams.Experience influencing product decisions with data.', '5+ years experience with object-oriented programming languages.', 'Experience influencing product decisions with data.', '5+ years experience in the data warehouse space.', 'Mentor team members by giving/receiving actionable feedback.', 'Responsibilities', '5+ years experience in writing SQL statements.', 'Experience analyzing data to identify gaps and inconsistencies.', 'Experience working autonomously in global teams.', 'Define and manage SLA for all data sets in allocated areas of ownership.', 'Assist in owning existing processes running in production, optimizing complex code through advanced algorithmic concepts.', '5+ years experience in custom ETL design, implementation and maintenance.', 'Experience managing and communicating data warehouse plans to internal clients.', 'Experience working with either a MapReduce or an MPP system.']",Not Applicable,Full-time,Information Technology,Internet,2021-03-24 13:05:10
Data Engineer (Python & Cloud Data),WarnerMedia,"Atlanta, GA",5 days ago,84 applicants,"['', 'Experience with ""big data"" platforms such as Snowflake, Hadoop, Hive, Presto or cloud-based tools such as Amazon Redshift, Google BigQuery', 'Minimum 2-3 years solid experience writing SQL', 'Minimum 3-4 years solid experience writing Object-oriented programs in Python', 'Company Overview', 'Experience with Apache Airflow or similar job scheduling application.Knowledge and working experience with AWS such as S3, Lambda, EC2, Kinesis, etc.Knowledge of Luigi, AWS Glue, AWS Athena, and Apache Spark.Experience with ""big data"" platforms such as Snowflake, Hadoop, Hive, Presto or cloud-based tools such as Amazon Redshift, Google BigQueryExperience working with REST and SOAP APIsSpecific application experience with media, Web Analytics and consumer data systems a plusExperience with BI tools such as Looker and Tableau', 'WarnerMedia is a leading media and entertainment company that creates and distributes premium and popular content from a diverse array of talented storytellers and journalists to global audiences through its consumer brands including: HBO, HBO Max, Warner Bros., TNT, TBS, truTV, CNN, DC Entertainment, New Line, Cartoon Network, Adult Swim, Turner Classic Movies and others.', 'Working under direction of Data Architects and Technical Managers, this position is responsible for building data pipelines into and out of a data lake using state of the art technologies. This includes the ingestion and transformation of data that provides critical information to the enterprise and business partners.', ' ', 'BS or higher in Computer Science, Mathematics, MIS, business, or equivalent education / training / experience', 'Basic understanding of statistics', 'The Team', 'Experience with BI tools such as Looker and Tableau', 'The Essentials', 'Create solutions to transform data from various sources and load it into Snowflake to create a data lake', 'Knowledge of Luigi, AWS Glue, AWS Athena, and Apache Spark.', 'Experience in working with Linux environments and Unix Shell Scripting', 'As a technologist at WarnerMedia, you will work at the intersection of art and science. You’ll work for brands that inform and entertain the world including [adult swim], Bleacher Report, Boomerang, Cartoon Network, CNN, ELEAGUE, Great Big Story, HLN, iStreamPlanet, TBS, Turner Classic Movies (TCM), TNT, truTV and Turner Sports- which includes the NBA, NCAA March Madness, Major League Baseball and the UEFA Champions League. You’ll be part of a company that enables community and belonging by creating content that connects with fans when, how and where they want it.', 'Understanding of ETL processes and design', 'The Daily', 'Specific application experience with media, Web Analytics and consumer data systems a plus', 'Experience working with REST and SOAP APIs', 'Create and maintain transformations to summarize/aggregate data and load it so users can consume this data using various BI/Analytics tools', 'Be at the cutting edge of utilizing data about consumers in the media industry to improve audience experience', 'Experience with Apache Airflow or similar job scheduling application.', '45 years ago we changed the face of television, and we continue that today by building and delivering next-generation entertainment and technology solutions across the globe. Our innovations impact advertising, data management, information security, content creation and delivery, business operations, broadcasting and ultimately, the fan experience.', '\ufeff', 'Knowledge and working experience with AWS such as S3, Lambda, EC2, Kinesis, etc.', '*** If you are interested in the position above, please apply to the following link on our career site!', ' *** If you are interested in the position above, please apply to the following link on our career site! https://warnermediacareers.com/global/en/job/181005BR/Data-Engineer-Cloud-Big-Data', 'The Job', 'Create solutions to transform data from various sources and load it into Snowflake to create a data lakeCreate and maintain transformations to summarize/aggregate data and load it so users can consume this data using various BI/Analytics toolsDevelop and maintain standards for administration and operation including the scheduling, running, monitoring, logging, management of errors, recovery from failures, and validation of outputsContribute to the project planning process by estimating tasks and deliverablesWork closely with Revenue Analytics team members to understand user requirementsBe at the cutting edge of utilizing data about consumers in the media industry to improve audience experience', 'Contribute to the project planning process by estimating tasks and deliverables', 'Minimum 3-4 years solid experience writing Object-oriented programs in PythonMinimum 2-3 years solid experience writing SQLExperience in working with Linux environments and Unix Shell ScriptingUnderstanding of ETL processes and designBasic understanding of statisticsBS or higher in Computer Science, Mathematics, MIS, business, or equivalent education / training / experience', ' The Extra Credit', 'Work closely with Revenue Analytics team members to understand user requirements', 'Develop and maintain standards for administration and operation including the scheduling, running, monitoring, logging, management of errors, recovery from failures, and validation of outputs']",Mid-Senior level,Full-time,Information Technology,Entertainment,2021-03-24 13:05:10
Data Engineer,Navitas Business Consulting Inc,"Kansas City, MO",3 days ago,Be among the first 25 applicants,[''],Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Bounteous,"Chicago, IL",2 days ago,33 applicants,"['', 'College degree in Computer Science, Data Science, Analytics or related field', 'Strong understanding of customer data platforms', 'Be a platform expert in leading CDP solutions like AEP, TreasureData, RedPoint or similar', 'Must be legally eligible to work in Canada.', 'Proficient in at least one programming language such as Python, Java, Scala', 'Work with Consultants and Data Scientists to identify and write the necessary queries needed for segmentation, reporting, analysis, and ML models', 'Preferred Qualifications', 'Develop a deep expertise in our client’s data infrastructure and partner with the respective teams ', 'Build the unified customer profile ', '5+ years of experience building data pipelines and extracting, transforming, and loading marketing and customer data ', 'Proficient in code version control systems like Git', 'Be a platform expert in leading CDP solutions like AEP, TreasureData, RedPoint or similarDevelop a deep expertise in our client’s data infrastructure and partner with the respective teams Work with the Solution Architect and extract, transform and load marketing and customer data into the platform in an automated and scalable mannerBuild the unified customer profile Work with Consultants and Data Scientists to identify and write the necessary queries needed for segmentation, reporting, analysis, and ML models', 'Experience working with data warehouse solutions like Amazon Redshift, Google BigQuery, Snowflake, or similar', 'Experience working with cloud technologies such as AWS, Google Cloud, Azure, or similar ', 'Data Engineer ', 'Strong SQL skills ', 'For Employment Opportunities Based In Canada', 'Role And Responsibilities', 'College degree in Computer Science, Data Science, Analytics or related field5+ years of experience building data pipelines and extracting, transforming, and loading marketing and customer data Strong SQL skills Proficient in at least one programming language such as Python, Java, ScalaExperience working with data warehouse solutions like Amazon Redshift, Google BigQuery, Snowflake, or similarExperience working with cloud technologies such as AWS, Google Cloud, Azure, or similar Proficient in code version control systems like GitStrong understanding of customer data platformsExposure to Spark, Hadoop, and other big data technologies is a plus', 'Exposure to Spark, Hadoop, and other big data technologies is a plus', 'Bounteous is an equal opportunity employer. We embrace diversity and are committed to creating an inclusive workplace. In accordance with the Ontario Human Rights Code and Accessibility for Ontarians with Disabilities Act, 2005, accommodation will be provided at any point throughout the hiring process, provided the candidate makes their accommodation needs known to Bounteous. We welcome applications from all qualified candidates. ', 'Work with the Solution Architect and extract, transform and load marketing and customer data into the platform in an automated and scalable manner']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer,ClearObject,"Indianapolis, IN",5 days ago,72 applicants,"['', '●\xa0\xa0\xa0\xa0\xa0Desire and ability to learn new skills while improving upon current skills', '●\xa0\xa0\xa0\xa0\xa0Build data systems and pipelines', '●\xa0\xa0\xa0\xa0\xa0Familiarity with managing CI/CD pipelines (e.g., Jenkins, CircleCI, Spinnaker, Gitlab)', '●\xa0\xa0\xa0\xa0\xa0Conduct complex data analysis and report on results', 'Core Responsibilities', '●\xa0\xa0\xa0\xa0\xa0Technical expertise with data models, data mining, and segmentation techniques', '●\xa0\xa0\xa0\xa0\xa0Build algorithm and prototypes', '●\xa0\xa0\xa0\xa0\xa0Familiarity with either the Spark or Dask ecosystem', 'At ClearObject we transform data into valuable outcomes, and we are looking for a Data Engineer to join our development team. You will work with our Digital Products team to employ various methods to transform data into useful data systems. For example, you’ll create algorithms for processing real-time IoT data and conduct statistical analysis. As a member of the Digital Products team, you’ll strive for efficiency by aligning data systems with product outcomes.', '●\xa0\xa0\xa0\xa0\xa0Knowledge of programming languages (e.g., Python)', ""The level of this position will be determined based on the applicant's education, skills and experience."", 'To qualify, applicants must be legally authorized to work in the United States, and should not require, now or in the future, sponsorship for employment visa status.', '●\xa0\xa0\xa0\xa0\xa0Prepare data for modeling', 'To perform this job successfully, an individual must be able to perform the following satisfactorily; other roles or responsibilities may be assigned.', '●\xa0\xa0\xa0\xa0\xa0Building and maintaining ETL Pipelines (Bash, Airflow)', '●\xa0\xa0\xa0\xa0\xa0Collaborate with data scientists, full-stack and back end developers, and the other product team members on several projects', '●\xa0\xa0\xa0\xa0\xa0Proficient with SQL & noSQL database design', '●\xa0\xa0\xa0\xa0\xa0BA/BS degree in Computer Science, IT, ML, Statistics or related technical field or equivalent practical experience; a master’s degree is a plus', '●\xa0\xa0\xa0\xa0\xa0Experience with DevOps', '●\xa0\xa0\xa0\xa0\xa0Experience with Pandas', '●\xa0\xa0\xa0\xa0\xa0Experience with data visualization tools such as Google Data Studio, Tableau, PowerBI, or similar', '●\xa0\xa0\xa0\xa0\xa0Parallelizing Operations (Spark / Dask Jobs)', '●\xa0\xa0\xa0\xa0\xa0Developing and maintaining datasets', 'ClearObject employees performing certain job functions may require access to technology or software subject to export or import regulations. To comply with these regulations, ClearObject may obtain nationality or citizenship information from applicants for employment. ClearObject collects this information solely for trade law compliance purposes and does not use it to discriminate unfairly in the hiring process.', 'Additional Information', 'Desired Skills & Experience', '●\xa0\xa0\xa0\xa0\xa0Familiarity with Infrastructure as Code (e.g., Terraform)', '\xa0', 'If you are detail-oriented with excellent communication, organizational skills and experience in this field, we’d like to hear from you!', '●\xa0\xa0\xa0\xa0\xa0Perform exploratory data analysis and report on results', 'To succeed in this data engineering role, you should have strong analytical skills and the ability to combine data from various sources. Data engineer skills also include familiarity with several programming languages and knowledge of machine learning methodologies.', 'Nice to haves:', '●\xa0\xa0\xa0\xa0\xa0Combine data from multiple sources', 'ClearObject is an equal opportunity employer.\xa0All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status or any other characteristic protected by law.', '●\xa0\xa0\xa0\xa0\xa0Data engineering certification (e.g., Google Cloud Platform Certified Professional Data Engineer)', '●\xa0\xa0\xa0\xa0\xa0Analyze and Organize raw data', '●\xa0\xa0\xa0\xa0\xa0Improving data quality, efficiency and reliability', 'The requirements listed below are representative of the knowledge, skill, and/or ability required.', '●\xa0\xa0\xa0\xa0\xa0Interpret trends and patterns', '●\xa0\xa0\xa0\xa0\xa0CI/CD Development', '●\xa0\xa0\xa0\xa0\xa0Familiarity with Kubernetes and Docker']",Mid-Senior level,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Data Engineer,Mindtree,"St Louis, MO",4 weeks ago,111 applicants,"['', 'Build and manage CI/CD pipelines using Jenkins.Automate mundane tasks using scripting languages.Handle large volumes of proprietary data, introspect and provide insights - Identify, gather, transform and analyze data within and across database platforms.Influence and design cloud infrastructure, architecture, standards and methods for large-scale systemsSupport services prior to production via infrastructure design, software platform development, load testing, capacity planning and launch reviewsMaintain services during deployment and in production by measuring and monitoring key performance and service level indicators including availability, latency, and overall system healthAutomate system scalability and continually work to improve system resiliency, performance and efficiencyPractice sustainable incident response as part of an on-call rotation and through blameless postmortemsRemediate tasks within corrective action plan via sustainable, preventative, and automated measures whenever possibleProvision and manage GCP infrastructure including Deploying and implementing Google Kubernetes Engine resourcesAutomating infrastructure builds/configurationsDefine, Implement and assign ownership for Stability/Reliability(SLIs, SLOs, Error Budgets)Collaboration with tribes/dev teams on Reliability development (Fixes, Logging, Delivery Metrics)', 'Provision and manage GCP infrastructure including Deploying and implementing Google Kubernetes Engine resources', 'Mindtree [NSE: MINDTREE] is a global technology consulting and services company, helping enterprises marry scale with agility to achieve competitive advantage. “Born digital,” in 1999 and now a Larsen & Toubro Group Company, Mindtree applies its deep domain knowledge to 350+ enterprise client engagements to break down silos, make sense of digital complexity and bring new initiatives to market faster. We enable IT to move at the speed of business, leveraging emerging technologies and the efficiencies of Continuous Delivery to spur business innovation. Operating in more than 15 countries across the world, we’re consistently regarded as one of the best places to work, embodied every day by our winning culture made up of 21,000 entrepreneurial, collaborative and dedicated “Mindtree Minds”', '3+\xa0years of experience developing and/or administering software in public\xa0cloud. Hands-on 6+ months in GCP.', '3+\xa0years of experience developing and/or administering software in public\xa0cloud. Hands-on 6+ months in GCP.Experience in monitoring infrastructure and application uptime and availability to ensure functional and performance objectives.Experience in languages such as Python, Ruby, Bash, Java, Go, Perl, JavaScript and/or node.jsDemonstrable cross-functional knowledge with systems, storage, networking, security and databasesSystem administration skills, including automation and orchestration of Linux/Windows using Chef, Puppet, Ansible, Salt Stack and/or containers (Docker, Kubernetes, etc.)Proficiency with continuous integration and continuous delivery tooling and practicesExperience managing Infrastructure as code via tools such as Terraform or CloudFormationExperience in setting up and managing/modifying CI/CD pipelines using Jenkins.Significant experience in configuring industry leading infrastructure/application monitoring tools (Stackdriver, Kibana, Grafana, Datadog, Splunk, Dynatrace, AppDynamics etc)', 'Handle large volumes of proprietary data, introspect and provide insights - Identify, gather, transform and analyze data within and across database platforms.', 'scripting languages', 'public cloud', 'Proficiency with continuous integration and continuous delivery tooling and practices', '2+ Hands-on experience with\xa0scripting languages\xa0such as Python and Bash.', 'Ability to handle large volumes of proprietary data, introspect and provide insights', '3+ years of hands-on experience in setting up and managing/modifying\xa0CI/CD\xa0pipelines using Jenkins preferably.2+ Hands-on experience with\xa0scripting languages\xa0such as Python and Bash.1+ years Hands-on experience with Hadoop infrastructure and Data Ingestion\xa0Ability to handle large volumes of proprietary data, introspect and provide insights1+ years of developing and/or administering software in\xa0public cloud\xa0(AWS, Azure or GCP)', 'Automate system scalability and continually work to improve system resiliency, performance and efficiency', 'Company Name: Mindtree', 'About Mindtree', 'Collaboration with tribes/dev teams on Reliability development (Fixes, Logging, Delivery Metrics)', 'Mindtree Equal Employment Opportunity Policy', 'Experience managing Infrastructure as code via tools such as Terraform or CloudFormation', 'Influence and design cloud infrastructure, architecture, standards and methods for large-scale systems', '• Recognition is the cornerstone of our culture', 'Demonstrable cross-functional knowledge with systems, storage, networking, security and databases', 'Experience in languages such as Python, Ruby, Bash, Java, Go, Perl, JavaScript and/or node.js', 'Support services prior to production via infrastructure design, software platform development, load testing, capacity planning and launch reviews', 'Company Location: Atlanta, GA', 'Roles & Responsibilities', 'Experience in monitoring infrastructure and application uptime and availability to ensure functional and performance objectives.', 'CI/CD', 'Role: Data Engineer (DevOps)', 'Build and manage CI/CD pipelines using Jenkins.', '3+ years of hands-on experience in setting up and managing/modifying\xa0CI/CD\xa0pipelines using Jenkins preferably.', 'Practice sustainable incident response as part of an on-call rotation and through blameless postmortems', 'Automating infrastructure builds/configurations', 'Automate mundane tasks using scripting languages.', 'Maintain services during deployment and in production by measuring and monitoring key performance and service level indicators including availability, latency, and overall system health', '• Good work-life balance encouraged', 'Why Work with Us:', 'Roles and Responsibilities:', '1+ years of developing and/or administering software in\xa0public cloud\xa0(AWS, Azure or GCP)', '• Values-driven and engaged leadership', 'Define, Implement and assign ownership for Stability/Reliability(SLIs, SLOs, Error Budgets)', 'Key Skillsets:', 'Welcome to possible', 'Experience in setting up and managing/modifying CI/CD pipelines using Jenkins.', 'We are hiring for one of our strategic projects and looking for Data Engineer (DevOps) who are willing to join us as Fulltime Employees and interested in the below job description.', '• Recognized for our employee learning and talent development practices', 'System administration skills, including automation and orchestration of Linux/Windows using Chef, Puppet, Ansible, Salt Stack and/or containers (Docker, Kubernetes, etc.)', 'Screening Highlights:', 'Remediate tasks within corrective action plan via sustainable, preventative, and automated measures whenever possible', '1+ years Hands-on experience with Hadoop infrastructure and Data Ingestion\xa0', 'Mindtree provides equal employment opportunities to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics.', 'Significant experience in configuring industry leading infrastructure/application monitoring tools (Stackdriver, Kibana, Grafana, Datadog, Splunk, Dynatrace, AppDynamics etc)']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Uplight,"Boston, MA",1 week ago,163 applicants,"['', 'Strong troubleshooting skills that span the full-stack (front-end clients, APIs, networking, DNS, Linux, containers, databases, distributed systems, etc.)', 'What You Bring To Uplight', 'Experience working cross-functionally with design, product, customer success, sales, etc.', 'Are focused on career growth by following defined career ladders', 'Solid knowledge of ETL and data integration', 'Tackle complex problems that span a wide range of technical abilities, including:Developing data pipelines to transform and process data between systemsProductionize machine learning pipelines leveraging billions of rows of dataScaling our software to handle the ever-growing customer data', 'Experience in the utility industryExperience working cross-functionally with design, product, customer success, sales, etc.Deep technical knowledge of Python, AWS/GCP, Docker, and/or PostgreSQL.', 'Developing data pipelines to transform and process data between systemsProductionize machine learning pipelines leveraging billions of rows of dataScaling our software to handle the ever-growing customer data', 'Are proud to be over 400+ rebels with an important cause by helping to create a more sustainable planet.Are committed to the environment, our employees, and our communities.Are focused on career growth by following defined career laddersTake our work and mission seriously and….we love to laugh!', 'Work effectively on an Agile team and collaborate well with your other team members.', 'Developing data pipelines to transform and process data between systems', 'Are committed to the environment, our employees, and our communities.', 'Take our work and mission seriously and….we love to laugh!', 'A value for testing and developing quality software', 'Skills and experience are necessary, but we hire on value alignment first, so if you feel you would be a good fit with us, still consider applying.', 'What Makes Working At Uplight Amazing', 'Are proud to be over 400+ rebels with an important cause by helping to create a more sustainable planet.', 'Experience working in an Agile environment and a strong understanding of the full SDLC', 'Description', 'A minimum of 3 years of professional experience developing in a modern programming language (Python preferred)', 'Productionize machine learning pipelines leveraging billions of rows of data', 'The Position:', 'What You Get To Do', 'A minimum of 3 years of professional experience developing in a modern programming language (Python preferred)Solid knowledge of ETL and data integrationA value for testing and developing quality softwareStrong critical thinking skills and a desire to work with ambiguous challengesExperience working in an Agile environment and a strong understanding of the full SDLCStrong troubleshooting skills that span the full-stack (front-end clients, APIs, networking, DNS, Linux, containers, databases, distributed systems, etc.)Experience deploying production applications on at least one major cloud provider (AWS, GCP, Azure)Experience writing and maintaining data pipelines and ETLs leveraging Spark or similar tooling', 'Uplight provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.', 'Work as an Engineer on our analytics engineering team, primarily developing in Python and leveraging a wide range of technologies, notably: AWS and GCP, Docker, Apache Airflow, Apache Spark, and PostgreSQLTake problems from inception all the way to completion - own the building, testing, deployment, and maintenance of the code that you work onTackle complex problems that span a wide range of technical abilities, including:Developing data pipelines to transform and process data between systemsProductionize machine learning pipelines leveraging billions of rows of dataScaling our software to handle the ever-growing customer dataWork effectively on an Agile team and collaborate well with your other team members.Build robust and well documented processes to facilitate data triage and associated fixesSave the planet', 'Work as an Engineer on our analytics engineering team, primarily developing in Python and leveraging a wide range of technologies, notably: AWS and GCP, Docker, Apache Airflow, Apache Spark, and PostgreSQL', 'Experience deploying production applications on at least one major cloud provider (AWS, GCP, Azure)', 'Experience writing and maintaining data pipelines and ETLs leveraging Spark or similar tooling', 'We Also', 'Have an innovative flexible time off policy', 'Strong critical thinking skills and a desire to work with ambiguous challenges', 'Keep you energized with plenty of food and drink', 'Bonus Points', 'Provide a 401k MatchHave an innovative flexible time off policyKeep you energized with plenty of food and drink', 'Provide a 401k Match', 'Deep technical knowledge of Python, AWS/GCP, Docker, and/or PostgreSQL.', 'Save the planet', 'Experience in the utility industry', 'Take problems from inception all the way to completion - own the building, testing, deployment, and maintenance of the code that you work on', 'Scaling our software to handle the ever-growing customer data', 'Build robust and well documented processes to facilitate data triage and associated fixes']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Conduent,"Morrisville, NC",3 weeks ago,126 applicants,"['', 'Excellent SQL coding experience with performance optimization for data queries.', 'We are looking for a Data Engineer who will be part of our Analytics Practice and will be expected to actively work in a multi-disciplinary fast paced environment. This role requires a broad range of skills and the ability to step into different roles depending on the size and scope of the project; its primary responsibility is the development and mainteneance of data pipelines including acquisition, transformation, loading and processing of data.\xa0', 'ETL experience with data integration to support data marts, extracts and reporting', 'Engineer and maintain a modern Cloud data pipeline to collect, organize, and process data from disparate sources.Performs data management tasks, such as conduct data profiling, assess data quality, and write SQL queries to extract and integrate dataDevelop / maintain efficient data collection systems and sound strategies for getting quality data from different sourcesConsume and analyze data from the data pool to support inference, prediction and recommendation of actionable insights to support business growth.Design, develop and maintain ETL processes using tools and scripting. Troubleshoot and debug ETL processes. Performance tuning and opitimization of the ETL processes.Provide support to new or existing applications while recommending best practices and leading projects to implement new functionality.Collaborate in design reviews and code reviews to ensure standards are met. Recommend new standards for visualizations.Learn and develop new ETL techniques as required to keep up with the contemporary technologies.Reviews the solution requirements and architecture to ensure selection of appropriate technology, efficient use of resources and integration of multiple systems and technology.Support presentations to Customers and PartnersAdvising on new technology trends and possible adoption to maintain competitive advantage', 'Demonstrates high standards of professional behavior in dealings with clients, colleagues and staff.', 'Develop / maintain efficient data collection systems and sound strategies for getting quality data from different sources', '\xa0\xa0', 'Experience in cloud-based ETL development processes.', 'Provide support to new or existing applications while recommending best practices and leading projects to implement new functionality.', 'Performs data management tasks, such as conduct data profiling, assess data quality, and write SQL queries to extract and integrate data', 'Responsibilities:', 'Location: Morrisville, North Carolina / REMOTE option available', 'Engineer and maintain a modern Cloud data pipeline to collect, organize, and process data from disparate sources.', 'Is familiar with the principles and practices involved in development and maintenance of software solutions and architectures and in service delivery.', '5+ years of related experience is required.A BS or Masters degree in Computer Science or related technical discipline is requiredETL experience with data integration to support data marts, extracts and reportingExperience connecting to varied data sourcesExcellent SQL coding experience with performance optimization for data queries.Understands different data models like normalized, de-normalied, stars, and snowflake models. Worked with transactional, temporal, time series, and structured and unstructured data.Worked in big data environments, cloud data stores, different RDBMS and OLAP solutions.Experience in cloud-based ETL development processes.Experience in deployment and maintenance of ETL Jobs.Is familiar with the principles and practices involved in development and maintenance of software solutions and architectures and in service delivery.Has strong technical background and remains evergreen with technology and industry developments.', 'Is able to make sound and far reaching decisions alone on major issues and to take full responsibility for them on a technical basis.', 'Advising on new technology trends and possible adoption to maintain competitive advantage', 'A BS or Masters degree in Computer Science or related technical discipline is required', 'Worked in big data environments, cloud data stores, different RDBMS and OLAP solutions.', 'Learn and develop new ETL techniques as required to keep up with the contemporary technologies.', 'Requirements', 'Job Overview', 'Has strong technical background and remains evergreen with technology and industry developments.', 'Consume and analyze data from the data pool to support inference, prediction and recommendation of actionable insights to support business growth.', '\xa0', 'Title: Data Engineer', 'Experience in deployment and maintenance of ETL Jobs.', 'Time management and multitasking skills to effectively meet deadlines under time-to-market pressure', 'Demonstrates a rational and organized approach to the tasks undertaken and an awareness of the need to achieve quality.', ' Data Engineer ', 'Reviews the solution requirements and architecture to ensure selection of appropriate technology, efficient use of resources and integration of multiple systems and technology.', '5+ years of related experience is required.', '\ufeff', 'Collaborate in design reviews and code reviews to ensure standards are met. Recommend new standards for visualizations.', 'Prior experience with application delivery using an Onshore/Offshore model', 'Support presentations to Customers and Partners', 'Additional', 'Experience with gathering end user requirements and writing technical documentation', 'Experience with business processes across multiple Master data domains in a services based company', 'Design, develop and maintain ETL processes using tools and scripting. Troubleshoot and debug ETL processes. Performance tuning and opitimization of the ETL processes.', 'Requires some travel (on average 10%-20%)', 'Strong written communication skills. Is effective and persuasive in both written and oral communication.', 'Demonstrated ability to have successfully completed multiple, complex technical projectsPrior experience with application delivery using an Onshore/Offshore modelExperience with business processes across multiple Master data domains in a services based companyDemonstrates a rational and organized approach to the tasks undertaken and an awareness of the need to achieve quality.Demonstrates high standards of professional behavior in dealings with clients, colleagues and staff.Is able to make sound and far reaching decisions alone on major issues and to take full responsibility for them on a technical basis.Strong written communication skills. Is effective and persuasive in both written and oral communication.Experience with gathering end user requirements and writing technical documentationTime management and multitasking skills to effectively meet deadlines under time-to-market pressureRequires some travel (on average 10%-20%)', 'Additional Requirements', 'Experience connecting to varied data sources', 'Demonstrated ability to have successfully completed multiple, complex technical projects', 'Experience Needed:', 'Understands different data models like normalized, de-normalied, stars, and snowflake models. Worked with transactional, temporal, time series, and structured and unstructured data.']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Meridian Technology Group,United States,3 weeks ago,147 applicants,"['', 'Meridian Technology Group is committed to equal employment opportunity (EEO) and non-discrimination for all employees in all job classifications and for prospective employees without regard to race, color, religion, sex, age, sexual orientation, veteran status, physical or mental disability, national origin, or any other characteristic protected by applicable federal or state law. All hiring is contingent on eligibility to work in the United States. We are unable to sponsor applicants for work visas therefore, please do not apply if you are not\xa0eligible to work without sponsorship, as sponsorship is not available at this time.  No 3rd party companies/candidates.', 'This position will be responsible for establishing data quality best practices, and for building, testing and maintaining our data ingestion pipelines. Powered by an analytics and workflow engine, data is the centerpiece of our platform. We use Artificial Intelligence (AI) and Natural Language Processing (NLP) to transform structured and unstructured data into accurate, actionable insights. Establishing robust data quality assurance is a critical aspect of this role.', ""What you'll be doing"", 'What we’re looking for', 'Conducting database design and architecture', 'Excellent knowledge of SQL', '100% remote (forever) anywhere in the United States', 'Experience working with healthcare dataUnderstanding of PHI and HIPAA complianceExperience with HL7, FHIR or any Electronic Health Record (EHR) platformExperience with any of the following: Postgres, Docker, Kubernetes, AWS', 'Nice to have', 'Experience with HL7, FHIR or any Electronic Health Record (EHR) platform', 'Constructing, testing and maintaining data ingestion pipelines and architecture', 'No 3rd party companies/candidates.', 'Making a difference in millions of lives', 'Other smart, humble engineers to work with', '\ufeffWhat’s in it for you', 'Understanding of PHI and HIPAA compliance', 'Experience with any of the following: Postgres, Docker, Kubernetes, AWS', 'Meridian Technology Group is committed to equal employment opportunity (EEO) and non-discrimination for all employees in all job classifications and for prospective employees without regard to race, color, religion, sex, age, sexual orientation, veteran status, physical or mental disability, national origin, or any other characteristic protected by applicable federal or state law. All hiring is contingent on eligibility to work in the United States. We are unable to sponsor applicants for work visas therefore, please do not apply if you are not\xa0eligible to work without sponsorship, as sponsorship is not available at this time. ', 'Any offer of employment will be conditional, based on successfully passing a Criminal Background Check.', 'Interesting, real-world problems to solve', 'Good experience with ETL / data ingestion', 'Experience working with healthcare data', 'Proficiency with at least one programming languages (Scala, Python, Java, etc.)', 'Developing and maintaining a data dictionary for persistent database', 'Interesting, real-world problems to solveMaking a difference in millions of livesFull salary and benefit package including unlimited PTOOther smart, humble engineers to work with', 'Strong, professional experience in data engineering', 'Data Engineer', 'Constructing, testing and maintaining data ingestion pipelines and architectureConducting database design and architectureIdentifying and implementing ways to improve data reliability, efficiency and qualityDeveloping and maintaining a data dictionary for persistent database', 'Other tech includes Scala, Python, AWS, Docker, Kubernetes, Postgres', 'If you’re looking for something meaningful, working with other smart engineers, let’s talk.', 'Identifying and implementing ways to improve data reliability, efficiency and quality', 'Meridian Technology Group is seeking a Data\xa0Engineer', 'This position is Remote.\xa0', 'Full salary and benefit package including unlimited PTO', 'Meridian Technology Group is seeking a Data\xa0Engineer. ', 'Strong, professional experience in data engineeringExcellent knowledge of SQLProficiency with at least one programming languages (Scala, Python, Java, etc.)Good experience with ETL / data ingestion']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
AWS Data Engineer,Kani Solutions Inc,"Los Angeles, CA",1 day ago,Be among the first 25 applicants,"['', 'Required Skills:', '(Min. 8 yrs of experience)', 'Title: AWS Data Engineer', 'should have working experience in the below stack,', 'Spark + Python or PySpark is Must', 'Duration: Full time', 'Informatica, BDM, MongoDB etc. are good to have.', 'Locations: Phoenix, AZ or Los Angeles, CA', 'AWS Knowledge on Lambda, Step Functions, EMR is Must']",Not Applicable,Full-time,Staffing and Recruiting,N/A,2021-03-24 13:05:10
Data Engineer,StaffQuest LLC,"Santa Clara, CA",2 weeks ago,123 applicants,"['StaffQuest is searching for a Data Engineer with Python and cloud experience that is passionate about machine learning and AI. This is a Full Time (FTE) opportunity with a small, high growth SaaS team supporting the healthcare industry. You will be responsible for collaborating with a cross functional team comprised of highly skilled designers / front end developers, product managers and data scientists. The back end data engineer will take accountability and ownership of the code base and produce design documents as well as automation friendly code. You will also perform design and code reviews, resolve production issues and ensure service is running smoothly at all times. We are seeking candidates with a passion for technology that will have a positive impact on the team and are natural leaders willing to share their knowledge base for the betterment of the team.', '', 'Strong professional experience creating production RESTful web servicesProfessional Amazon Web Services (AWS) experiencePython expertiseStrong SQL experience and database performanceStrong critical thinking and analytical skillsOutgoing personality and the willingness to take initiative to complete challenging workExcellent communication (verbal and written) skills', 'Excellent communication (verbal and written) skills', 'Strong professional experience creating production RESTful web services', 'Python expertise', 'Professional Amazon Web Services (AWS) experience', 'Requirements', 'Strong critical thinking and analytical skills', 'Outgoing personality and the willingness to take initiative to complete challenging work', 'Strong SQL experience and database performance']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Thomson Reuters,"Eagan, MN",2 weeks ago,31 applicants,"['', 'Degree in a technical field such as Computer Science/Engineering, Physics, Mathematics or Statistics Familiarity with Docker and Elastic ContainersFamiliarity with Serverless design, architecture and frameworks such as LambdaFamiliarity with IaC frameworks such as CloudFormation or TerraformExperience with CI/CD pipelines and processesExperience with Shell scripting and Linux OSExperience with Database/Data Warehouse technologies such as PostgreSQL or SnowflakeExperience with enterprise security Experience with Agile software development practices', 'Accessibility ', 'Hands-on knowledge in using SQL queries (analytical functions) and writing and optimizing SQL queries', 'Experience working with data visualization tools (Tableau, Power BI...)', 'Learning & Development:', ""What's in it for you!"", 'Be Customer Driven: Continuously improve software engineering practices and contribute new ideas to the team ', '2+ years development experience in building ETL/ELT data flows', ' Data Engineer, ', 'Be a subject Matter Expert: Regularly self-improve by monitoring changes in tech landscape', 'Preferred Qualifications', 'Bachelor’s Degree or Equivalent Work Experience', 'Experience with CI/CD pipelines and processes', 'Be Agile', 'About You', 'In this role as a', 'Degree in a technical field such as Computer Science/Engineering, Physics, Mathematics or Statistics ', 'Be Responsible:', 'Experience with Agile software development practices', 'About The Role', 'Benefits:', 'Experience with Shell scripting and Linux OS', 'Strong problem-solving and interpersonal skills', 'Experience with Database/Data Warehouse technologies such as PostgreSQL or Snowflake', 'Be Collaborative', 'Be Responsible: For the end-to-end delivery of features across our product suite ', 'Bachelor’s Degree or Equivalent Work Experience2+ years development experience in building ETL/ELT data flowsExperience with Python or Java developmentHands-on knowledge in using SQL queries (analytical functions) and writing and optimizing SQL queriesExperience working with data visualization tools (Tableau, Power BI...)Experience with version control systems such as Git Experience with cloud platforms and services such as AWS/AzureStrong problem-solving and interpersonal skillsAbility to perform in a changing environment', 'Learning & Development: You are will be joining a growing sales team that has the commitment of the company to prioritize organic growth and has made investments to expand the sales team capability.', 'Experience with enterprise security ', 'Experience with Python or Java development', 'Compensation: Base salary and a variable compensation that is directly related to your success', 'Compensation:', 'Perks: You will have the opportunity to work for a company that has a market dominant position for both content and technology and is passionate about giving back to the community.Learning & Development: You are will be joining a growing sales team that has the commitment of the company to prioritize organic growth and has made investments to expand the sales team capability.Compensation: Base salary and a variable compensation that is directly related to your successBenefits: Extraordinary benefits package.', 'Ability to perform in a changing environment', 'Familiarity with IaC frameworks such as CloudFormation or Terraform', 'Perks: You will have the opportunity to work for a company that has a market dominant position for both content and technology and is passionate about giving back to the community.', 'Thomson Reuters', 'Familiarity with Serverless design, architecture and frameworks such as Lambda', 'Required Qualifications:', 'Experience with version control systems such as Git ', 'Locations', 'Be Collaborative: Work with product owner(s) and/or team members to understand product vision and requirements ', 'Benefits: Extraordinary benefits package.', 'Be Agile: Working with a team to design, build and deploy high quality software products for ISRM ', 'Benefits', 'Be Customer Driven: ', 'Data Engineer', 'Experience with cloud platforms and services such as AWS/Azure', 'Be a subject Matter Expert: ', 'Familiarity with Docker and Elastic Containers', 'Be Agile: Working with a team to design, build and deploy high quality software products for ISRM Be Responsible: For the end-to-end delivery of features across our product suite Be Customer Driven: Continuously improve software engineering practices and contribute new ideas to the team Be Collaborative: Work with product owner(s) and/or team members to understand product vision and requirements Be a subject Matter Expert: Regularly self-improve by monitoring changes in tech landscape', 'you will…', 'Job Description', 'Perks:']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,vHire Inc,"San Francisco, CA",6 days ago,Be among the first 25 applicants,"['', 'Required Skills']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer (Remote),CUNA Mutual Group,"Remote, OR",7 days ago,Be among the first 25 applicants,"['', 'Applied knowledge of cloud computing', 'Work on cross-functional teams to design and test data-driven applications and products', 'Job Responsibilities', 'Thought partner for development teams in applying data modeling techniques and the usage of data modeling and repository tools in consumer data domain', 'Experience with software engineering tools and workflows (e.g. Jenkins, CI/CD, git)', 'Expertise in big data batch computing tools, with demonstrated experience developing distributed data processing solutions', 'Strong understanding of database internals, such as indexes, binary logging, and transactions', 'Perform exploratory data analyses and provide guidance for use (both for marketing analysis and data science)Partner with data scientists on both model rebuilds and new model developmentAnalyze new (possibly unstructured) data sources to determine what additional value they may bring and how to effectively make use of themReview developed solutions to solve specific business problemsInvestigate, analyze and recommend data formats and structureKnowledge of how data entities and elements should be structured to assure accuracy, performance, understanding, operational, analytical, reporting, and data science efficienciesThought partner in prototyping emerging self-service technologies involving data ingestion and transformation, distributed file systems, databases and frameworksWork on cross-functional teams to design and test data-driven applications and products', 'Analyze new (possibly unstructured) data sources to determine what additional value they may bring and how to effectively make use of them', 'Solid data understanding and business acumen in the data rich industries like insurance or financial', 'Actively solicits and listens to feedback to determine need for change or improvement', 'Investigate, analyze and recommend data formats and structure', 'Perform exploratory data analyses and provide guidance for use (both for marketing analysis and data science)', 'Applied knowledge of data modeling principles (e.g. dimensional modeling and star schemas)', 'Communicates complex ideas, anticipates potential objections and persuades others, often at senior levels, to adopt a different point of view', 'Practical experience consuming web services', 'In-depth knowledge of SQL, SAS, R, and Python programming languages, and extensive hands-on data engineering experience. Experience with SparkR, Hadoop, Tableau, and graph databases is helpful.', 'Keeps up to date on industry, competitor and market trends', 'Review developed solutions to solve specific business problems', 'Partner with data scientists on both model rebuilds and new model development', 'Knowledge of how data entities and elements should be structured to assure accuracy, performance, understanding, operational, analytical, reporting, and data science efficiencies', 'Communicates complex ideas, anticipates potential objections and persuades others, often at senior levels, to adopt a different point of viewKeeps up to date on industry, competitor and market trendsCommunicates difficult concepts and negotiates with others to persuade a different point of viewActively solicits and listens to feedback to determine need for change or improvementThought partner for development teams in applying data modeling techniques and the usage of data modeling and repository tools in consumer data domain', ""Bachelor's degree in computer science or related field"", ""Bachelor's degree in computer science or related field6-8 years of experience technology related workIn-depth knowledge of SQL, SAS, R, and Python programming languages, and extensive hands-on data engineering experience. Experience with SparkR, Hadoop, Tableau, and graph databases is helpful.Experience with Databricks and SnowflakeCapable of authoring robust, high quality, reusable code and contributing to the division's inventory of librariesExpertise in big data batch computing tools, with demonstrated experience developing distributed data processing solutionsApplied knowledge of cloud computingApplied knowledge of data modeling principles (e.g. dimensional modeling and star schemas)Strong understanding of database internals, such as indexes, binary logging, and transactionsExperience with software engineering tools and workflows (e.g. Jenkins, CI/CD, git)Practical experience consuming web servicesSolid data understanding and business acumen in the data rich industries like insurance or financial"", 'Please provide your Work Experience and Education or attach a copy of your resume. Applications received without this information may be removed from consideration.', 'Job Requirements', 'Experience with Databricks and Snowflake', ""Capable of authoring robust, high quality, reusable code and contributing to the division's inventory of libraries"", 'Thought partner in prototyping emerging self-service technologies involving data ingestion and transformation, distributed file systems, databases and frameworks', '6-8 years of experience technology related work', 'Job Purpose', 'Communicates difficult concepts and negotiates with others to persuade a different point of view']",Not Applicable,Full-time,Information Technology,Financial Services,2021-03-24 13:05:10
Data Engineer Data Operations,Cotiviti,"United, LA",1 day ago,Be among the first 25 applicants,"['', 'Work as a team member to develop tools and techniques for improving process efficiencies and query performance.', 'Bachelor’s degree in relevant field such as Computer Science, Computer Engineering, a related field, or equivalent experience.', 'Ability to work well independently or in a team environment.', 'Review & test the data to ensure accuracy & validity of the data prior to uploading the data to the warehouse. ', 'Work with Technical Operations to troubleshoot complex database issues related to the entire environment including OS, storage and servers. Provide off hours On-Call support to resolve production issues when necessary.', 'Excellent written and verbal communication skills, with the ability to multitask and prioritize projects to meet scheduled deadlines.', 'Proficient in Microsoft Office Suite, including PowerPoint, Word, Excel, Outlook and Project.', '2 years’ experience within the Healthcare or Insurance space in a Data Warehouse operations organization. Strong knowledge of healthcare enrollment, medical claims, and drug claims data required. Additional knowledge of non-traditional data types (health & wellness, workforce productivity, and EMR) preferred.', 'Experience with ticketing and documentation tools such as Jira.', 'Requirements', 'Principal Responsibilities And Essential Duties', 'Work as a team member in creation and maintenance of ETL scripts, tools, queries and applications used for healthcare data management, data validation, statistical report generation, and program validation.', 'Perform data analysis, data mining and investigations and identify root cause of issues with SQL queries.', ' 2 years’ experience within the Healthcare or Insurance space in a Data Warehouse operations organization. Strong knowledge of healthcare enrollment, medical claims, and drug claims data required. Additional knowledge of non-traditional data types (health & wellness, workforce productivity, and EMR) preferred. ', 'Must be able to perform duties with or without reasonable accommodation.', ' Perform data analysis, data mining and investigations and identify root cause of issues with SQL queries. Work with Technical Operations to troubleshoot complex database issues related to the entire environment including OS, storage and servers. Provide off hours On-Call support to resolve production issues when necessary. Completes all responsibilities as outlined on annual Performance Plan.  Completes all special projects and other duties as assigned. Must be able to perform duties with or without reasonable accommodation. ', 'Strong knowledge of coding and ability to follow best practices while developing and deploying codes.', ' Data Mapping and Programming Work as a team member in creation and maintenance of ETL scripts, tools, queries and applications used for healthcare data management, data validation, statistical report generation, and program validation. Work as a team member to develop tools and techniques for improving process efficiencies and query performance. Program per data transformation specifications to convert source data to be loaded into target data warehouse tables using T-SQL and other Data Integration/ETL tools. Review & test the data to ensure accuracy & validity of the data prior to uploading the data to the warehouse.  ', 'Ability to analyze data with a high level of detailed accuracy and identify root causes of issues using queries.', ' Bachelor’s degree in relevant field such as Computer Science, Computer Engineering, a related field, or equivalent experience. 2-4 years’ experience with data aggregation, standardization, linking, quality check mechanisms, and reporting.', 'Experience with RDBMS (SQL, Oracle, SQL, Vertica, etc.) and using T-SQL or other data integration/ETL tools. Understanding of new technologies like HDFS and NOSQL a plus.', 'Data Mapping and Programming', 'Program per data transformation specifications to convert source data to be loaded into target data warehouse tables using T-SQL and other Data Integration/ETL tools.', '2-4 years’ experience with data aggregation, standardization, linking, quality check mechanisms, and reporting.', 'Completes all responsibilities as outlined on annual Performance Plan. ', 'Experience in the Analysis, design and development of solutions and strategies for creating extraction, transformation and loading (ETL) and real-time applications.', 'Completes all special projects and other duties as assigned.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,"PennyMac Loan Services, LLC","Raleigh, NC",3 weeks ago,Be among the first 25 applicants,"['', ' Moderate experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.  Moderate knowledge of AWS cloud services: EC2, EMR, RDS, Redshift  Experience with object-oriented/object function scripting languages: Python, Java, C++  Experience building and optimizing AWS data pipelines, architectures and data sets.  Strong project management and organizational skills.  Moderate skill in business intelligence tools such as Tableau or Qlik  Moderate skills with MS Office, including Excel & PowerPoint  Must be a team player with strong attention to detail and able to work independently  Proven track record at delivering timely and accurate information in a fast-paced environment  Excellent critical thinking, problem solving, and mathematical skills, and sound judgment  Strong business acumen and ability to interface with executive management', ' Must be a team player with strong attention to detail and able to work independently ', ' Build analytics tools that utilize the data pipeline to provide actionable insights into operational efficiency, financial reports and other key business performance metrics ', ' Work with stakeholders including the Engineering and Analytic teams to assist with data-related technical issues and support their data infrastructure needs ', ' Experience with object-oriented/object function scripting languages: Python, Java, C++ ', ' Experience building and optimizing AWS data pipelines, architectures and data sets. ', ' Create data tools to support business informational technology process ', ' Strong project management and organizational skills. ', ' Create and maintain optimal data pipeline architecture ', ' Moderate skills with MS Office, including Excel & PowerPoint ', ' Moderate skill in business intelligence tools such as Tableau or Qlik ', ' Moderate experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases. ', ' Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources ', ' Demonstrate behaviors which are aligned with the organization’s desired culture and values ', ' Work with the data analytics team to strive for greater functionality in our data systems ', ' Strong business acumen and ability to interface with executive management', ' Perform other related duties as required and assigned ', ' Moderate knowledge of AWS cloud services: EC2, EMR, RDS, Redshift ', ' Excellent critical thinking, problem solving, and mathematical skills, and sound judgment ', ' Proven track record at delivering timely and accurate information in a fast-paced environment ', ' Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources  Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability  Create and maintain optimal data pipeline architecture  Assemble large, complex data sets that meet business requirements  Build analytics tools that utilize the data pipeline to provide actionable insights into operational efficiency, financial reports and other key business performance metrics  Work with stakeholders including the Engineering and Analytic teams to assist with data-related technical issues and support their data infrastructure needs  Create data tools to support business informational technology process  Work with the data analytics team to strive for greater functionality in our data systems  Perform other related duties as required and assigned  Demonstrate behaviors which are aligned with the organization’s desired culture and values ', 'Ideal Candidate Will Have The Following', ' Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability ', ' Assemble large, complex data sets that meet business requirements ', 'Job Description']",Not Applicable,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Arcadia,"Washington, DC",3 weeks ago,79 applicants,"['', 'Experience with Amazon Web Services (AWS) or other cloud infrastructure platforms', 'Extensive experience in managing data pipelines, schemas, and storage for multiple systems for multiple teams', 'Strong communication skills', 'Manage data pipelines from disparate sources, standardizing and feeding them into our centralized data warehouse', 'Experience in one or more of the following languages: Python, Java, Ruby, Javascript', 'Experience in predictive modeling and statistical analysis', ' Manage data pipelines from disparate sources, standardizing and feeding them into our centralized data warehouse Work with both the Engineering and Analytics & Data Science teams to optimize data flow and queries for large data sets to improve scalability Sync data across internal and external systems, such as marketing and sales automation tools, to enable key stakeholders to build best-in-class experiences Support ongoing efforts to establish and enforce best practices on data quality, use, and security across the company ', 'Undergraduate and/or graduate degree in math, statistics, engineering, computer science, or related technical field', 'Support ongoing efforts to establish and enforce best practices on data quality, use, and security across the company', 'Sync data across internal and external systems, such as marketing and sales automation tools, to enable key stakeholders to build best-in-class experiences', 'Advanced knowledge of algorithms, data structures, and relational algebra', 'Join a mashup of energy enthusiasts and creative tech wizards who are taking the fight to climate change. Disrupt and reimagine the energy experience using modern technologies.', 'Work with both the Engineering and Analytics & Data Science teams to optimize data flow and queries for large data sets to improve scalability', 'Paid Time Off (holidays, vacation, professional development, volunteer, parental leave)', 'Professional development opportunities', 'What You’ll Do', 'A supportive engineering culture that values diversity, empathy, teamwork, trust, and efficiency', 'A chance to decarbonize and disrupt the energy sector', 'Market-based compensation (salary + equity)', 'Experience in the energy sector', 'What We’re Looking For', 'Healthcare, dental, vision, 401(k) and commuter benefits', ' 3+ years combined programming and/or DevOps experience Significant experience with and a strong understanding of languages/tools relevant to engineering & data teams’ work Experience in one or more of the following languages: Python, Java, Ruby, Javascript Advanced knowledge of algorithms, data structures, and relational algebra Database management experience with PostgreSQL, RDS, or Redshift Data extraction experience with a strong understanding of thread-based and event-based paradigms Extensive experience in managing data pipelines, schemas, and storage for multiple systems for multiple teams Strong communication skills ', ' Market-based compensation (salary + equity) Healthcare, dental, vision, 401(k) and commuter benefits Paid Time Off (holidays, vacation, professional development, volunteer, parental leave) A supportive engineering culture that values diversity, empathy, teamwork, trust, and efficiency Professional development opportunities All-company lunches Free clean energy A chance to decarbonize and disrupt the energy sector ', 'Eliminating carbon footprints, eliminating carbon copies.', 'Data extraction experience with a strong understanding of thread-based and event-based paradigms', 'Significant experience with and a strong understanding of languages/tools relevant to engineering & data teams’ work', 'Benefits', 'Free clean energy', 'Experience with enterprise database interfaces and messaging APIs', 'a 100% clean energy future.', 'Experience with entity resolution at scale', 'Must-haves', 'What will help you succeed:', '3+ years combined programming and/or DevOps experience', 'Database management experience with PostgreSQL, RDS, or Redshift', 'All-company lunches', 'Nice-to-haves', ' Undergraduate and/or graduate degree in math, statistics, engineering, computer science, or related technical field Experience in predictive modeling and statistical analysis Experience with enterprise database interfaces and messaging APIs Experience with Amazon Web Services (AWS) or other cloud infrastructure platforms Experience with entity resolution at scale Experience in the energy sector ']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer,DataKitchen,Greater Boston,5 days ago,Over 200 applicants,"['', 'Solid experience with at least one object-oriented programming language, preferably Python', '5+ years of hands-on data engineering experience', 'Knowledge of Commercial Pharmaceutical / Life Sciences data sets (NPP, DDD, Xponent, Plantrak, SPP, Symphony, IQVIA)', 'DataKitchen has an excellent opportunity for a US based remote Data Engineer or Customer Success professional to be part of an exciting, growing company delivering a cloud-based DataOps Solution! We have several roles available for all levels of experience and leadership.  We are leading the DataOps movement, making it possible for enterprise data teams to turn data into true business value. DataKitchen was honored as 2019 Gartner Cool Vendor and a CRN Big Data “Start-Up to Watch” in 2020. Our company is profitable, rapidly growing and stock will be part of the package.', 'Write Amazon Redshift SQL and leverage other AWS products (S3, Lambda, Glacier) and technologies to transform raw data into analytic assets\xa0', 'Day to Day:', 'Develop and maintain data pipelines utilizing our DataOps softwareWrite Amazon Redshift SQL and leverage other AWS products (S3, Lambda, Glacier) and technologies to transform raw data into analytic assets\xa0Self-manage and lead client projectsRun with open-ended requirements to mockup features for clients and then iterate and improve them over timeCommunicate directly with customers; maintain transparency through Jira ticketing and Confluence documentationWork in an agile working environment: weekly sprints, daily scrums', 'Work in an agile working environment: weekly sprints, daily scrums', 'DataKitchen has a culture of trust and transparency. As a Data Engineer at DataKitchen, you will own and take responsibility for the work you do. We pride ourselves on being collaborative amongst the team and with our customers. We are always asking “Is there a better way we can do this?,” refactoring and building on what we have done as a team.', 'Desired Tools and Experience:', '5+ years of hands-on data engineering experienceMS in a quantitative field (Computer Science, preferred)Advanced knowledge of cloud database design, warehousing (AWS, Snowflake, Google Cloud, etc.)Solid experience with at least one object-oriented programming language, preferably PythonStrength communicating technical details, at both a high level and very detailedKnowledge of Commercial Pharmaceutical / Life Sciences data sets (NPP, DDD, Xponent, Plantrak, SPP, Symphony, IQVIA)', 'Run with open-ended requirements to mockup features for clients and then iterate and improve them over time', 'Advanced knowledge of cloud database design, warehousing (AWS, Snowflake, Google Cloud, etc.)', 'Communicate directly with customers; maintain transparency through Jira ticketing and Confluence documentation', 'MS in a quantitative field (Computer Science, preferred)', 'Why You Should Work at DataKitchen', 'Self-manage and lead client projects', 'Develop and maintain data pipelines utilizing our DataOps software', 'Strength communicating technical details, at both a high level and very detailed', 'We follow agile development practices while embracing our errors and falling forward. We hire the best and brightest and give everyone the opportunity to contribute to the growth of the company.']",Mid-Senior level,Full-time,Engineering,Computer Software,2021-03-24 13:05:10
Data Engineer,Engage3,"Sacramento, CA",2 days ago,Be among the first 25 applicants,"['', ' You are comfortable with setting and meeting SLAs for data availability and quality', 'As a Qualified Applicant', ' You have strong experience with working with tools & platforms within the AWS ecosystem (EC2, S3, Aurora, Lambda, API Gateway, etc)', ' ML production system (AWS, Python) Data Warehouse (Snowflake) ETL system & data pipelines BI system (Tableau Online)', ' You have familiarity with big data technologies like Hadoop, Spark, Hive', ' Data Warehouse (Snowflake)', ' You are proficient in at least one programming language like Python, Scala and Java', ' You have demonstrated experience of ETL developments', ' You are a mentor to your team & colleagues and have passion in sharing your knowledge', "" You have planned, built & managed data infrastructures in a public cloud You have strong experience with working with tools & platforms within the AWS ecosystem (EC2, S3, Aurora, Lambda, API Gateway, etc) You have in-depth experience with MySQL databases and Snowflake's data warehouse You have managed a business intelligence system You have demonstrated experience of ETL developments You are proficient in at least one programming language like Python, Scala and Java You have familiarity with big data technologies like Hadoop, Spark, Hive You are comfortable with setting and meeting SLAs for data availability and quality You have an understanding of Machine Learning / AI principles in data engineering You are a mentor to your team & colleagues and have passion in sharing your knowledge You've worked in an Agile environment. You thrive on iteration. You make opportunities to bring value sooner rather than later. You value data-driven decisions. You are always looking for opportunities to quickly produce the right data to make decisions quickly. You keep cool under pressure. You are a self-driven, highly motivated technologist who can work with a high degree of autonomy, is able to prioritize effectively and drive the data architecture vision."", ' BI system (Tableau Online)', ' ETL system & data pipelines', ' You have planned, built & managed data infrastructures in a public cloud', ' You value data-driven decisions. You are always looking for opportunities to quickly produce the right data to make decisions quickly. You keep cool under pressure.', "" You have in-depth experience with MySQL databases and Snowflake's data warehouse"", "" You've worked in an Agile environment. You thrive on iteration. You make opportunities to bring value sooner rather than later."", ' You are a self-driven, highly motivated technologist who can work with a high degree of autonomy, is able to prioritize effectively and drive the data architecture vision.', ' You have an understanding of Machine Learning / AI principles in data engineering', ' ML production system (AWS, Python)', ' You have managed a business intelligence system']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,"Velocity Works, LLC","Pittsburgh, PA",1 week ago,Be among the first 25 applicants,"['', 'Implement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it.', 'Collaborate with others to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.Work closely with product and engineering to develop strategy for long term data platform architecture.Align data pipeline, warehouse and analytics architecture with business requirementsDevelop, construct, test and maintain data management systems using appropriate tools to perform data extraction, modeling, transformation, cleansing, loading, and interpretationIntegrate up-and-coming data management and software engineering technologies into existing data structures.Implement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it.Define and catalog company data assets Automate read, extract, transform, stage and load data processes using cloud-based toolsAnalyze and troubleshoot data related issues Collected data from commercial databases, client databases, EMR systems, or other structured and unstructured sources.Filter and ""clean"" the data to remove errors and ensure consistency and completeness of data. Use ETL tools and programming skills to write cleansing scripts to correct errors through automation.Organize and store data in a structurally relevant way in data warehouse or data lakes for easy access and analysis by team members.Works with the project team to define tasks and create team work plans with moderate supervision. Identify issues affecting work progress and recommends solutions. Control project costs, communicating any project-related expenses and recommend ways to control costs. ', 'Works with the project team to define tasks and create team work plans with moderate supervision. ', '6+ years of SQL or No-SQL ', 'Filter and ""clean"" the data to remove errors and ensure consistency and completeness of data. ', 'Control project costs, communicating any project-related expenses and recommend ways to control costs. ', '5+ years working with data models, database warehouse design and data ETL automation ', 'Collected data from commercial databases, client databases, EMR systems, or other structured and unstructured sources.', '5 or more years’ experience as a data engineer6+ years of with schema design and dimensional data modeling6+ years of SQL or No-SQL 5+ years of programming/scripting in Python or javascriptFamiliar with Informatica, Pentaho, or similar5+ years designing, building, and maintaining data processing systems in AWS5+ years working with data models, database warehouse design and data ETL automation HL7 experience ', '5+ years of programming/scripting in Python or javascript', 'Organize and store data in a structurally relevant way in data warehouse or data lakes for easy access and analysis by team members.', 'Familiar with Informatica, Pentaho, or similar', '6+ years of with schema design and dimensional data modeling', '5+ years designing, building, and maintaining data processing systems in AWS', 'Align data pipeline, warehouse and analytics architecture with business requirements', 'Collaborate with others to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.', 'Integrate up-and-coming data management and software engineering technologies into existing data structures.', 'Develop, construct, test and maintain data management systems using appropriate tools to perform data extraction, modeling, transformation, cleansing, loading, and interpretation', 'Define and catalog company data assets ', 'HL7 experience ', 'Work closely with product and engineering to develop strategy for long term data platform architecture.', 'Use ETL tools and programming skills to write cleansing scripts to correct errors through automation.', 'Analyze and troubleshoot data related issues ', 'Identify issues affecting work progress and recommends solutions. ', 'Automate read, extract, transform, stage and load data processes using cloud-based tools', '5 or more years’ experience as a data engineer']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,RemyCorporation,"Massachusetts, United States",4 weeks ago,145 applicants,"['', 'I have both Sr. and Jr. level positions. Must have experience with Azure Data Factory, Snowflake, Python scripting, building pipelines from scratch, Best Practices around building pipelines and strong documentation skills.', 'Really looking for someone who is curious and always learning new technologies, not just a doer but a thinker! If this is you please reach out.', 'This is a perm/ remote opportunity, the client will provide sponsorship if needed. Please no 3rd party calls.', 'Must (requirement) have experience in either the Consumer Goods or Retail Industry.', 'Please call or email me 303-990-2320 rachel@remycorp.com']",Mid-Senior level,Full-time,Information Technology,Consumer Goods,2021-03-24 13:05:10
Data Engineer,Pear Therapeutics,"Boston, MA",1 week ago,86 applicants,"['', 'Pear Therapeutics is the leader in prescription digital therapeutics. We aim to redefine medicine by discovering, developing, and delivering clinically validated software-based therapeutics to provide better outcomes for patients, smarter engagement and tracking tools for clinicians, and cost-effective solutions for payers. Pear has a pipeline of products and product candidates across therapeutic areas, including severe psychiatric and neurological conditions. Our first product, reSET®, treats Substance Use Disorder and was the first prescription digital therapeutic to receive marketing authorization from the FDA to treat disease. Pear’s second product, reSET-O®, for the treatment of Opioid Use Disorder, received marketing clearance from the FDA in December 2018. Pear’s third PDT, Somryst®, is the first FDA-authorized prescription digital therapeutic (PDT) for patients with chronic insomnia and the first product submitted through FDA’s traditional 510(k) pathway while simultaneously reviewed through FDA’s Software Precertification Pilot Program.', 'Build and maintain scalable and efficient data pipelines and infrastructure', 'Wrangling data model modifications to accommodate source data changes', 'Identify, design, and implement data team process improvements and automation', 'The Pear Data Program’s mission is to deliver infrastructure, tools, and processes to provide stakeholders with accurate, reliable, integrated data and actionable insights. You will be focused on designing and developing the infrastructure, tools, and Data Warehouse to advance this mission. Your role will span all aspects of the ETL process. In addition, you’ll work with external partners to extract data from the Data Warehouse that is minimally scoped to the external entity.\xa0\xa0', 'Pear Therapeutics is committed to equal opportunity in the terms and conditions of employment for all employees and job applicants without regard to race, color, religion or religious creed, ancestry, age, sex (including pregnancy, childbirth, breastfeeding and related medical conditions), sexual orientation, gender identity or gender expression, national origin, genetic information, qualified physical or mental disability or handicap, medical condition, qualified military or veteran status, or any other basis protected by applicable law. Pear Therapeutics also follows all applicable national, state and local laws governing nondiscrimination in employment as well as employment eligibility verification requirements of the Immigration and Nationality Act. This policy applies to all terms and conditions of employment, including hiring, placement, promotion, termination, layoff, recall, transfers, leave of absences, compensation and training.', 'Experience working as a Data Engineer at multiple companies; you know what works', 'Deep experience with SQL and no-SQL data sourcesExperience working as a Data Engineer at multiple companies; you know what worksA strong sense of ownership\xa0Experience writing ETL scripts using python, Java, SQL, and/or other languagesExperience manipulating, processing and extracting value from large disconnected datasets', 'About Pear Therapeutics', 'Description', 'Equal Employment Opportunity', 'Understand and integrate large, complex, evolving data sets\xa0', 'Responsibilities', 'Deep experience with SQL and no-SQL data sources', 'Qualifications', 'Experience manipulating, processing and extracting value from large disconnected datasets', 'A strong sense of ownership\xa0', 'Create data tools for analytics and data science teams', 'Effectively and efficiently transform data from source systems\xa0', 'Experience writing ETL scripts using python, Java, SQL, and/or other languages', 'Build and maintain scalable and efficient data pipelines and infrastructureUnderstand and integrate large, complex, evolving data sets\xa0Effectively and efficiently transform data from source systems\xa0Create data tools for analytics and data science teamsWrangling data model modifications to accommodate source data changesIdentify, design, and implement data team process improvements and automation']",Mid-Senior level,Full-time,Engineering,Biotechnology,2021-03-24 13:05:10
Data Engineer,Storable,"Texas, United States",4 weeks ago,166 applicants,"['', 'We leverage our platform in combination with our over 25 years of storage industry expertise to help our thousands of storage customers achieve their tenant experience and operational efficiency objectives every single day.', 'Computer science degree or equivalent experience3+\xa0 years experience in software development, data engineering, BI development, and / or data architectureExperience with Python, SQL, Airflow, AWS, RESTful APIs, and Tableau or other data visualization toolsConsistent track record of leading successful delivery for a large-scale project or being a key contributor on multiple projectsConsistent track record of positively influencing project direction and contributions to cross-functional and/or cross-organizational collaborations', 'Generous health coverage for you and your family, including short- and long-term disability coverage, 401(k) and HSA matching, company-provided life insurance, and more.', 'What you’ll do everyday:', 'Work with data and analytics experts to strive for greater functionality in our data systems', '3+\xa0 years experience in software development, data engineering, BI development, and / or data architecture', 'Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using various SQL and ‘big data’ technologies', 'All applicants must be currently authorized to work in the United States on a full-time basis.', 'The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.', 'Do More, Be More – Company rewards and recognition that add up and can be redeemed for once-in-a-lifetime Bucketlist experiences!', 'Storable is looking for a Data Engineer to join our Data Architecture team in Austin. This team member will be responsible for expanding and optimizing our cloud native big data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing distributed systems as well as building them from the ground up.\xa0', 'Storable is committed to providing equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, Storable will provide reasonable accommodations for qualified individuals with disabilities.', 'At Storable, we believe storage operators should have one partner they can trust to help you get the results they need for their business. That’s why we’ve built the industry’s first fully integrated platform that offers facility management software, facility websites, marketing programs and services, payments, and deeply integrated tenant insurance capabilities all in one solution.\xa0', 'Must\xa0be located on:\xa0TX, KS, NC, MO, CO, PA, IL, IN', 'Experience with Python, SQL, Airflow, AWS, RESTful APIs, and Tableau or other data visualization tools', 'About Us:', 'Location:\xa0Remote', 'Benefits and Perks:', 'Fun company events, including Halloween costume contests, ugly sweater competitions, baseball game outings, ice cream socials, food trucks, and more.', 'Generous health coverage for you and your family, including short- and long-term disability coverage, 401(k) and HSA matching, company-provided life insurance, and more.Engage remotely! Since mid-March we transitioned to a fully remote environment and we run regular contests, trivia games and remote happy hours!Instant-Access & Flexible vacation. We trust you, so we have a ‘take what you need’ vacation policy. No waiting to use it or need for accruals.Do More, Be More – Company rewards and recognition that add up and can be redeemed for once-in-a-lifetime Bucketlist experiences!Get active in the community by joining one of our many quarterly offsite volunteer and community service events.Fun company events, including Halloween costume contests, ugly sweater competitions, baseball game outings, ice cream socials, food trucks, and more.', 'Consistent track record of leading successful delivery for a large-scale project or being a key contributor on multiple projects', 'Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics', 'Instant-Access & Flexible vacation. We trust you, so we have a ‘take what you need’ vacation policy. No waiting to use it or need for accruals.', 'Create and maintain optimal data pipeline architecture', 'Computer science degree or equivalent experience', 'Consistent track record of positively influencing project direction and contributions to cross-functional and/or cross-organizational collaborations', 'Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.', 'Must', 'Data Engineer', 'Work closely with stakeholders including the Executive, Product, Data and Design teams to design and deliver products and functionality to address analytical and functional data needs', 'Get active in the community by joining one of our many quarterly offsite volunteer and community service events.', 'What you need to bring to the table:', 'The Data Engineer will support our software developers, database architects, data analysts and data scientists on stakeholder initiatives and ensure delivered architecture is consistent and supportable throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products.\xa0', 'Create data tools for analytics and data scientist and business operations team members that assist them in building and optimizing our product', 'Assemble large, complex data sets that meet both functional and non-functional requirements', 'Create and maintain optimal data pipeline architectureAssemble large, complex data sets that meet both functional and non-functional requirementsIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using various SQL and ‘big data’ technologiesBuild analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metricsWork closely with stakeholders including the Executive, Product, Data and Design teams to design and deliver products and functionality to address analytical and functional data needsCreate data tools for analytics and data scientist and business operations team members that assist them in building and optimizing our productWork with data and analytics experts to strive for greater functionality in our data systems', 'Engage remotely! Since mid-March we transitioned to a fully remote environment and we run regular contests, trivia games and remote happy hours!']",Mid-Senior level,Full-time,Information Technology,Internet,2021-03-24 13:05:10
Senior Data Engineer,Austin Fraser,"Austin, Texas Metropolitan Area",2 days ago,29 applicants,"['', '30 person Engineering Department planning to grow to 50 by EOY', 'Austin Fraser Inc is acting as an employment business in relation to this advert. As a professional company, we gladly welcome applications from persons of any age and background and do not intend to discriminate with advert text and terminology.', 'Skills', 'Salary: $150k - $180k', 'Skills: Java and GCP', 'CEO and CTO moved to Austin to build a local Engineering team', '40-person Data Engineering Department', 'The leader in their space with plans to IPO', 'Salary: $130k - $150k', ""Here are some basic details on each position and I'd be more than happy to share the full job description if you are interested."", '20 person startup and you would be the first data hire!', 'For further details please submit a resume.', 'Senior Data Engineer - Cyber Security', 'CEO and CTO moved to Austin to build a local Engineering teamHave a greenfield opportunity to become an expert in GCPSkills: Java and GCPSalary: $135k - $160k', 'Salary', 'Skills:\xa0Python/AWS & strong in Data Architecture', '40-person Data Engineering DepartmentThe leader in their space with plans to IPOSkills: Python, AWS, SparkSalary: $150k - $180k', 'Senior Data Engineer - Construction Software', 'Salary: $135k - $160k', '30 person Engineering Department planning to grow to 50 by EOYA leader in their space with plans to IPOSkills: Python, AWS, SQLSalary: $130k - $150k', 'Senior Data Engineer - Healthcare Marketing', '20 person startup and you would be the first data hire!Skills:\xa0Python/AWS & strong in Data ArchitectureSalary: $130k - $150k', 'Every week a new company is asking me to find them a Senior Data Engineer. The positions below are all for companies that are thriving during COVID and in no way will be forcing you back to an office anytime soon.', 'Skills: Python, AWS, SQL', 'Skills: Python, AWS, Spark', 'A leader in their space with plans to IPO', 'Senior Data Engineer - eCommerce', 'Have a greenfield opportunity to become an expert in GCP']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
SQL Data Engineer,Jettison,"Hoffman Estates, IL",2 weeks ago,145 applicants,"['', 'Required:\xa0', 'Machine learning experience\xa0', 'Hands on work with Azure and Python\xa0', 'Jettison is representing a client who is searching for a\xa0SQL Data Engineer\xa0in the Hoffman Estates, Illinois area.\xa0\xa0This position is a contract to hire role and will be onsite in the near future.', 'Great verbal and written communication skills\xa0', 'Work experience using PowerBI\xa0', 'Must have VERY strong SQL knowledge\xa0', 'Bachelors Degree', '3+ years of SQL Data Engineer experience', 'Demonstrated ability to walk thru raw SQL', 'Jettison is an Equal Opportunity/Affirmative Action employer', 'Ability analyze data using T-SQL', 'Knowledge of SSIS and SSRS']",Mid-Senior level,Full-time,Marketing,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Averity,New York City Metropolitan Area,,N/A,"['', '10 - 15 percent bonus', 'A interest or experience working on Machine Learning Models', '$110,000 - $125,000\xa0', 'Python', 'Technical Must Haves?', 'PythonSQLHadoopA interest or experience working on Machine Learning ModelsSpark or Databricks (Nice to have)', 'This is an opportunity where your work truly helps change the lives of millions of people. If you are mission driven and want your work to mean something then this is the role for you. In addition, we also offer stability and ample room for growth within our organization.', 'SQL', 'Compensation:', 'We are a fortune 500 healthcare company that strives to help people obtain a healthier lifestyle. We sell traditional and consumer directed health care insurance and related services and products. We work to help transform healthcare through new innovations that make healthcare and a healthier lifestyle easier to use, more accessible, and less expensive. If you are mission driven and want to help better the lives of people then we want to speak with you!', 'Spark or Databricks (Nice to have)', 'Whats in it for you?', 'Hadoop', '$110,000 - $125,000\xa010 - 15 percent bonusBenefits', 'As a Data Engineer you will work along side the Data Scientist to maximize our ability to engage with members to help them make better healthcare decisions. You will build large scale\xa0ETL pipelines, data warehouses, and utilize Machine Learning/Predictive models in an effort to implement scalable, configurable and self-learning marketing platform. We use petabytes of data so you will be using big data tools such as Spark and Databricks on this team.', 'Benefits', 'What is the Job?', 'Who Are We?']",Mid-Senior level,Full-time,Engineering,Computer Software,2021-03-24 13:05:10
"Data Engineer, Research & Development",Houston Astros,"Houston, TX",1 week ago,37 applicants,"['Travel', 'Strong interpersonal and communication skills (written and verbal)', 'Work Environment', 'EOE/M/F/Vet/Disability', 'Experience with non-SQL data management solutions is preferred', 'Classification:\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0 Full-time (Exempt)', 'Design, implement, and maintain data mapping procedures', 'Experience building data solutions using Python, C#, or other languages is preferred, including using REST and other APIs to load data from external sources', 'Bachelor’s degree in computer science or related field is preferredExperience with SQL Server and T-SQLExperience with database maintenance and DevOps is preferredExperience building data solutions using Python, C#, or other languages is preferred, including using REST and other APIs to load data from external sourcesExperience with Microsoft Azure/Amazon Web Services/Google Cloud platform is preferredExperience with non-SQL data management solutions is preferredExperience working with baseball data (e.g., TrackMan, Statcast) is a plusStrong analytical and problem-solving skillsStrong interpersonal and communication skills (written and verbal)', 'The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. This is a largely sedentary role; however, some filing is required. This would require the ability to lift files, open filing cabinets and bend or stand on a stool as necessary.\xa0', 'Maintain and support internal database solutions', 'Actively participate with software developers and architects in design reviews, code reviews, and other best practices', 'Experience with Microsoft Azure/Amazon Web Services/Google Cloud platform is preferred', 'Education and/or Experience:', 'Respond to and resolve technical problems and issues in a timely manner', 'Rare travel may be expected in this role.', 'Supervisor: \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0 Director, Research and Development', '\xa0', 'Experience working with baseball data (e.g., TrackMan, Statcast) is a plus', 'The Houston Astros are seeking a Baseball Systems Data Engineer for the team’s Baseball Research and Development group. The Data Engineer will join a cross-functional, agile team of analysts and developers, and will build and maintain systems that promote the use and understanding of analytical baseball data and information throughout Baseball Operations.\xa0The Data Engineer will be responsible for collecting, processing, storing, and integrating many sources of baseball data, as well as designing and building new data solutions, both on-premises and in the cloud.', 'This job operates in an office setting. This role routinely uses standard office equipment such as computers, phones, photocopiers, and filing cabinets.\xa0 The noise level is usually moderate but can be loud within the stadium environment.\xa0', 'Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities and activities may change at any time with or without notice.', 'Bachelor’s degree in computer science or related field is preferred', 'Ability to work a flexible schedule, including evenings, weekends, and holidays.', 'Develop data quality assurance tools to ensure data integrity and system performance', 'Other Duties', 'Work closely with baseball analysts to design and implement data solutions', 'Essential Duties & Responsibilities:', 'We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law.\xa0', 'Strong analytical and problem-solving skills', 'Department:\xa0 \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0 Baseball Operations ', 'Create, maintain, and optimize ETL jobs for incoming data feeds', 'Experience with SQL Server and T-SQL', 'Position Type and Expected Hours of Work', 'Experience with database maintenance and DevOps is preferred', 'Create, maintain, and optimize ETL jobs for incoming data feedsDevelop data quality assurance tools to ensure data integrity and system performanceDesign, implement, and maintain data mapping proceduresMaintain and support internal database solutionsActively participate with software developers and architects in design reviews, code reviews, and other best practicesWork closely with baseball analysts to design and implement data solutionsRespond to and resolve technical problems and issues in a timely manner', 'Physical Demands']",Not Applicable,Full-time,Analyst,Sports,2021-03-24 13:05:10
Data Engineer,E-Solutions,"Washington, DC",1 day ago,Be among the first 25 applicants,"['', '&sect;\xa0 ', 'Snowflake Warehouse ', 'Experience in handling semi-structured data (JSON, XML) using the VARIANT attribute in Snowflake', 'Must Have', 'Nice to Have', 'Own, monitor, and improve automated solutions to ensure quality and performance SLAs are met', 'Data EngineerLocation: Seattle, WA', 'Design and develop both physical and logical data models along with define metadata standards for the DW', 'Prepare technical documentation including metadata and diagrams of entity relationships, process flows and others', 'data migration', 'Matillion', 'Perform data mapping data between source systems and DW', 'Hands-on experience in Cloud technologies such as', 'At least 10+ years of hands-on experience in Development/Data Integration/Data Modeling/Data migration', 'Python', 'In-depth understanding of database management systems and advantages & disadvantages of different types of systems (OLAP, OLTP, etc.).\xa0', 'Strong experience with ELT concepts & tools such as Talend, Matillion', ' - Blob Storage, Cool Blob Storage, Virtual Machine, Functions, SQL Datawarehouse', 'Sound experience in Python scripting to read & load data from ', 'Advanced data visualization skills using any technology is a plus', 'Experience in design or development of enterprise data solutions, applications, and integrations in Snowflake Warehouse environment', 'Experience in in Re-clustering of the data in Snowflake with good understanding on how Micro-Partition works inside Snowflake is a plus', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0 ', 'Experience in Hadoop, Hive, HBASE, Spark is a plus', 'Snowflake Enterprise Warehouse ', 'Data Integration', 'Develop complex SQL queries, scripts, user defined functions, views, and triggers for business logic implementation in\xa0Snowflake Enterprise Data Warehouse', 'Develop and own ETL Data Pipeline and Data Model solutions for integrating new data sets into the\xa0Snowflake Enterprise Warehouse from different sources.', 'Very good understanding of the Snowflake architecture', 'Prior experience in Oracle Financial data is a plus ', 'Snowflake Enterprise Data Warehouse', 'Strong experience with Data warehousing methodologies and modelling techniques', 'Sound experience in working with Massively Parallel Processing (MPP) Analytical Datastores such as Netezza, Teradata', 'Create scripts for data warehouse testing - ranging from unit to integration testing', 'Oracle Financial', 'Primary Skill : AWS, Snowflake', 'Disclaimer:\xa0E-Solutions provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, disability, genetic information, marital status, amnesty, or status as a covered veteran in accordance with applicable federal, state and local laws. We especially invite women, minorities, veterans, and individuals with disabilities to apply. EEO/AA/M/F/Vet/Disability.', 'Develop & execute ETL procedures/pipelines as well monitor and tune data loads and queries', 'Duration: 6-12 months', 'AWS - S3', 'Azure', 'Good experience in data migration from various systems to Snowflake warehouse', 'Data migration', 'Experience in working with Batch and Stream data', 'Experience in Netsuite/Zuora is a huge plus', 'Experience working directly with business users and solving business questions with data', 'Netsuite/Zuora', ', Glacier, EC2, Lambda, SQS, Redshift']",Entry level,Contract,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Advisor360°,"Weston, MA",3 weeks ago,126 applicants,"['', '\u200b', 'Data modeling experience', 'Responsible for the design, development, and support of data management, quality and privacy processes, frameworks, and tools. ', 'Adhere to best practices in engineering and data management', 'Responsible for the design, development, and support of data management, quality and privacy processes, frameworks, and tools. Responsible for data and process improvement, identifying methods to enhance existing methods.Responsible for the design, development and implementation of data quality assessment, verification, and improvement practices.Adhere to best practices in engineering and data managementCreate processes to aggregate and auto categorize data categories.', 'Experience with .NET, Python, or PowerShell', '2+ years’ experience with data sharing through APIs in any scripting language - .Net and Python are a plus.', 'Knowledge of laws and regulations that can impact data environments and use, such as GDPR, CCPA, HIPAA, etc.', 'Bachelor’s degree in computer science or equivalent work experience of 3+ years', '2+ years of experience with enterprise data management tool sets such as Collibra, Informatica, Purview or Alation.', 'Responsible for data and process improvement, identifying methods to enhance existing methods.', 'Bachelor’s degree in computer science or equivalent work experience of 3+ years3+ years of hands-on SQL and T-SQL programming skills2+ years of experience in a highly regulated data environment such as financial services or healthcare.2+ years of experience with enterprise data management tool sets such as Collibra, Informatica, Purview or Alation.2+ years’ experience with data sharing through APIs in any scripting language - .Net and Python are a plus.Strong exploratory, problem-solving, and analytical skills as related to data quality, categorization, and transformation tasksExperience working against datasets of varying size, type, and volumeDemonstrated ability to create and deliver high quality code using software engineering best practices including coding standard, code review, source control management, build process, testing, and operations.Must be self-motivated, detail-oriented, and have excellent communication and interpersonal skillsMust demonstrate flexibility, self-direction, and a growth mindset that is open to change.', 'Responsible for the design, development and implementation of data quality assessment, verification, and improvement practices.', 'Requirements', 'Additional skills and knowledge', 'Data modeling experienceExperience with .NET, Python, or PowerShellExposure to machine learning concepts for automated data categorizationExposure to cloud ETL products such as Azure Data Factory or DatabricksKnowledge of laws and regulations that can impact data environments and use, such as GDPR, CCPA, HIPAA, etc.', 'Demonstrated ability to create and deliver high quality code using software engineering best practices including coding standard, code review, source control management, build process, testing, and operations.', 'Exposure to machine learning concepts for automated data categorization', 'Must be self-motivated, detail-oriented, and have excellent communication and interpersonal skills', 'Exposure to cloud ETL products such as Azure Data Factory or Databricks', '3+ years of hands-on SQL and T-SQL programming skills', 'The Data Engineer role will be part of Advisor360°’s Engineering\xa0organization, as a member of our Data Management team. The primary responsibilities of this role will be creating and technically supporting data management, quality and privacy frameworks and processes. Working across the organization, the data engineer will assist in the selection, implementation and adoption of enterprise data management tooling and integration.\xa0They will also be responsible for creating processes improve overall data quality, ensure that data management and quality requirements are represented in the overall data pipeline, and implement our data privacy framework.', 'Create processes to aggregate and auto categorize data categories.', '2+ years of experience in a highly regulated data environment such as financial services or healthcare.', 'Strong exploratory, problem-solving, and analytical skills as related to data quality, categorization, and transformation tasks', 'Must demonstrate flexibility, self-direction, and a growth mindset that is open to change.', 'Experience working against datasets of varying size, type, and volume', 'Key responsibilities']",Mid-Senior level,Full-time,Information Technology,Financial Services,2021-03-24 13:05:10
Data Engineer,Vivint,"Lehi, UT",3 weeks ago,60 applicants,"['', 'Who You Will Work With', 'Working Conditions', 'Paid holidays and flexible paid time away', 'Ability to initiate, drive, and manage projects with competing priorities', '1+ years of ETL & data pipeline development experience ', 'Job Summary', 'Why Vivint', ' https://www.fastcompany.com/3067476/why-vivint-smart-home-is-one-of-the-most-innovative-companies-of-2 ', ' http://archive.sltrib.com/article.php?id=5360131&itype=CMSID ', 'Casual dress code', ' Medical/dental/vision/life coverage ', 'Experience with Tableau or similar data visualization tool', 'Must have a passion for data and helping the business turn data into information and action1+ years of data engineering experience1+ years of ETL & data pipeline development experience Ability to initiate, drive, and manage projects with competing prioritiesAbility to communicate effectively with business leaders, IT leadership, and engineers Expert in SQL, databases, and ETL development processes & tools (Cloud MPP like Snowflake or Redshift)', 'Proficiency in the python scripting language. Preference given to knowledge of more (Perl, .net, etc.)', 'Bonus Skills', 'Employee pricing on smart home products', '1+ years of data engineering experience', 'Experience with AWS or Azure data product offerings and platform', 'Ability to communicate effectively with business leaders, IT leadership, and engineers ', 'Experience with big data technologies (HDFS, Hadoop, Spark, Elastic Search, Redshift, Snowflake, etc...)', 'What We Stand For', "" Find out more about what it's like to work here: "", ' https://www.vivint.com/company/careers/culture ', 'Expert in SQL, databases, and ETL development processes & tools (Cloud MPP like Snowflake or Redshift)', 'Onsite gym, gaming tables across our campus', 'Paid holidays and flexible paid time awayYour choice between Mac or PCEmployee pricing on smart home productsCasual dress codeOnsite gym, gaming tables across our campusOnsite health clinic Medical/dental/vision/life coverage ', 'Your choice between Mac or PC', 'Experience with machine learning technologies (R, SparkML, AzureML, etc.)', 'What We’re Looking For', 'Proficiency in the python scripting language. Preference given to knowledge of more (Perl, .net, etc.)Familiarity with one or more web technologies (Angular, React, PHP, ASP.net, etc.)', 'Experience with big data technologies (HDFS, Hadoop, Spark, Elastic Search, Redshift, Snowflake, etc...)Experience with Tableau or similar data visualization toolExperience with AWS or Azure data product offerings and platformExperience with machine learning technologies (R, SparkML, AzureML, etc.)', 'Who Are We', 'Familiarity with one or more web technologies (Angular, React, PHP, ASP.net, etc.)', 'Must have a passion for data and helping the business turn data into information and action', 'Onsite health clinic', 'Job Description', 'If you are an active Vivint employee, please apply through Workday by searching ""Find Jobs"".', 'Safety']",Entry level,Full-time,Information Technology,Consumer Electronics,2021-03-24 13:05:10
Data Support Engineer,Homesnap,"Chevy Chase, MD",4 weeks ago,94 applicants,"['', 'Roles and Responsibilities ', 'Knowledge of .NET/C# and database architecting/design a plus ', 'Write complex SQL queries to generate custom reports ', 'Basic Qualifications ', ' ', '*This position is based out of our Bethesda, MD office.*   ', ""Bachelor's Degree 2+ years of experience with Microsoft SQL Server and Transact-SQL 1+ years of experience working as a support representative "", ""Homesnap, a member of the CoStar Family, seeks a Data Support Engineer to join the Homesnap development team based in Bethesda, MD. As an integral member of our world-class development team, you'll help us support our customers by resolving issues related to big data, act as a liaison to our support team, and providing other technical support as needed. "", 'Preferred Qualifications ', ' We welcome all qualified candidates who are currently eligible to work full-time in the United States to apply.  However, please note that CoStar is not able to provide visa sponsorship for this position.', 'Handle all tickets escalated by the Customer Support team Identify and report cosmetic and functional bugs Diagnosis and fix data related issues Build or modify in-house admin tools for recurring issues Write complex SQL queries to generate custom reports ', '1+ years of experience working as a support representative ', 'Build or modify in-house admin tools for recurring issues ', ""Bachelor's Degree "", 'Handle all tickets escalated by the Customer Support team ', 'Identify and report cosmetic and functional bugs ', 'Detail-oriented and able to self-manage several concurrent tasks Ability to work both independently and collaboratively with team members You love big data Knowledge of .NET/C# and database architecting/design a plus ', 'You love big data ', 'Diagnosis and fix data related issues ', 'Detail-oriented and able to self-manage several concurrent tasks ', '2+ years of experience with Microsoft SQL Server and Transact-SQL ', 'Ability to work both independently and collaboratively with team members ']",Associate,Full-time,Engineering,Real Estate,2021-03-24 13:05:10
Data Engineer,Suffolk Construction,"Boston, MA",2 weeks ago,99 applicants,"['', 'Develop and maintain a scalable data pipeline and build out new API integrations to support continuing increases in data volume and complexity.', 'A bachelor’s or master’s degree in computer science, statistics, applied mathematics, data management, information systems, or a related quantitative field [or equivalent work experience] is required.At least 5 years or more work experience in data management disciplines including data integration, data modeling, data management, and data quality.At least 3 years of experience working in a cross-functional team and collaborating with business stakeholders in support of departmental and/or multi-departmental data management and analytics initiatives.At least 3 years of experience with cloud data warehouses (i.e., Snowflake, AWS Redshift, Azure Synapse), with an aptitude to learn new tools.Strong SQL skills with a background for building data models.Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources.Proven ability to perform data cleaning, wrangling, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks.Proven ability to design, build, and manage data pipelines for data structures encompassing data transformation, data models, schemas, metadata and workload management.Strong experience with advanced analytics programming languages such a R, Python, C++, Scala, etc.Well-versed in BI tools to include Microsoft PowerBI, Tableau, etc.Experienced in working with IT to build a data-driven infrastructure.Strong verbal and written communication skills.Construction experience preferred but not required.', 'Work closely with all business units and engineering teams to develop a long-term data platform architecture strategy.', 'Proven ability to perform data cleaning, wrangling, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks.', 'Research new uses for existing data.', 'Collaborate with the data and analytics team and business teams to improve data models that feed business intelligence tools, increasing data accessibility, and fostering data-driven decision-making across the organization.', 'Proven ability to design, build, and manage data pipelines for data structures encompassing data transformation, data models, schemas, metadata and workload management.', 'Excellent analytical and problem-solving skills', 'Implement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders.', 'Design, construct, install, test, and maintain data management systems.', 'Sound business acumen', 'About our people', 'Analyze and organize raw data; prepare data for descriptive modeling.', 'Develop data models that can be used to make predictions and answer questions for the overall business.', 'About Suffolk', 'Qualifications:', 'Recommend opportunities for reuse of data models in new environments.', 'At least 5 years or more work experience in data management disciplines including data integration, data modeling, data management, and data quality.', 'Develop best practices for standard naming conventions and coding practices to ensure consistency of data models.', 'Demonstrates Suffolk’s Core Values of hard work, professionalism, integrity, passion, and caring', 'Demonstrates Suffolk’s Core Values of hard work, professionalism, integrity, passion, and caringExcellent analytical and problem-solving skillsSound business acumenPositive attitude with a strong willingness to learnCurious and tenaciousStrong drive to insight', 'Understand and translate business needs into data models supporting long-term solutions.', '\xa0', 'Work and collaborate with IT to understand the current data architecture, data pipes, data security, and IT data strategy. ', 'Well-versed in BI tools to include Microsoft PowerBI, Tableau, etc.', 'We build buildings. Iconic, complex buildings that are changing our cities and transforming our skylines. We do it by gathering the people, innovations, and partnerships that are redefining what it means to do our jobs. We see every day as an opportunity to challenge the status quo and go beyond what we thought was possible.', 'Suffolk is a national enterprise that invests, innovates, and builds. We provide value throughout the entire project lifecycle by leveraging our core construction management services with vertical service lines that include real estate capital investment, design, self-perform construction services, technology start-up investment and innovation research and development. We have $4.5 billion in annual revenue, 2,400 employees, and main offices in Boston (headquarters), New York, Miami, West Palm Beach, Tampa, Estero, Dallas, Los Angeles, San Francisco, and San Diego. We serve clients in every major industry sector, including health care, science and technology, education, gaming, transportation, and aviation, and commercial. Suffolk is privately held and is led by founder, chairman and CEO John Fish. We’re ranked #23 on the Engineering News Record list of “Top 400 Contractors.” And we’re proud to be a certified 2020 “Great Place to Work”.', 'We want people who are bold. Curious. Innovative. Caring. Looking for the career opportunity of a lifetime. We’ll challenge and inspire you to be your very best. We’ll embrace what makes you unique and lift you up as you take chances. Here, you’ll find a place where you can act with purpose and integrity, bringing intelligence and grit to every aspect of your job.', 'Strong SQL skills with a background for building data models.', 'But when we say we build buildings, that’s not the whole truth—it’s our people who build buildings. And it is our mission to empower our people to prove impossible wrong.', 'The Data Engineer will be responsible for creating and optimizing our data models and data pipeline architecture. The ideal candidate is an experienced data pipeline builder, data wrangler, and data modeler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our data analysts and data scientists to ensure optimal delivery of data throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products. The right candidate will be excited by the prospect of designing our company’s data architecture to support our next generation of products and data initiatives.', 'Responsibilities', 'Create logical and physical data models using best practices to ensure high data quality and reduced redundancy.', 'A bachelor’s or master’s degree in computer science, statistics, applied mathematics, data management, information systems, or a related quantitative field [or equivalent work experience] is required.', 'Suffolk Construction is seeking an experienced Data Engineer to support Suffolk’s enterprise data management program and systems. The Data Engineer, an emerging role in Suffolk’s data and analytics team, will be pivotal in operationalizing the most-urgent data and analytics initiatives for Suffolk’s digital business initiatives. This role will require both creative and collaborative work with IT and Suffolk’s wider business. It will involve evangelizing effective data management practices and promoting a better understanding of data and analytics.', 'Evaluate data models and physical databases for variances and discrepancies.', 'Curious and tenacious', 'At least 3 years of experience with cloud data warehouses (i.e., Snowflake, AWS Redshift, Azure Synapse), with an aptitude to learn new tools.', 'Support the data visualization team and data science team with formatted data and provides data model QA/QC.', 'Necessary Attributes', 'At least 3 years of experience working in a cross-functional team and collaborating with business stakeholders in support of departmental and/or multi-departmental data management and analytics initiatives.', 'Analyze and organize raw data; prepare data for descriptive modeling.Combine raw data from different sources and explore ways to enhance data quality and reliability.Ability to prepare curated datasets for self-service consumption (data democratization) Develop and maintain a scalable data pipeline and build out new API integrations to support continuing increases in data volume and complexity.Understand and translate business needs into data models supporting long-term solutions.Work and collaborate with IT to understand the current data architecture, data pipes, data security, and IT data strategy. Prepare data for predictive and prescriptive modeling.Create logical and physical data models using best practices to ensure high data quality and reduced redundancy.Develop best practices for standard naming conventions and coding practices to ensure consistency of data models.Recommend opportunities for reuse of data models in new environments.Design, construct, install, test, and maintain data management systems.Collaborate with the data and analytics team and business teams to improve data models that feed business intelligence tools, increasing data accessibility, and fostering data-driven decision-making across the organization.Develop data models that can be used to make predictions and answer questions for the overall business.Implement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders.Work closely with all business units and engineering teams to develop a long-term data platform architecture strategy.Research new uses for existing data.Evaluate data models and physical databases for variances and discrepancies.Support the data visualization team and data science team with formatted data and provides data model QA/QC.', 'Strong experience with advanced analytics programming languages such a R, Python, C++, Scala, etc.', 'Combine raw data from different sources and explore ways to enhance data quality and reliability.', 'The Role:', 'Ability to prepare curated datasets for self-service consumption (data democratization) ', 'Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources.', 'Prepare data for predictive and prescriptive modeling.', 'Experienced in working with IT to build a data-driven infrastructure.', 'Strong verbal and written communication skills.', 'Construction experience preferred but not required.', 'Strong drive to insight', 'Positive attitude with a strong willingness to learn']",Mid-Senior level,Full-time,Management,Construction,2021-03-24 13:05:10
Data Engineer,Knock,United States,7 days ago,62 applicants,"['', ' Build data pipelines and aggregate data. Design data schemas and optimize internal data warehouses, augmenting data from multiple sources. Design, build, and maintain REST APIs to serve data to customers Cross-functionally collaborate with our Data Science and Machine Learning teams.  Understand the data that powers our applications, and be able to propose appropriate data models for new features. Build new ETL jobs from scratch, as well as maintain existing jobs. Be committed to good engineering practice of testing, logging, alerting and deployment processes. Monitor and troubleshoot operational or data issues in the data pipelines. Drive architectural plans and implementation for future data storage, reporting, and analytic solutions. ', 'Your experience in SQL, MySQL, or Postgres will be valuable here to identify slow queries and debugging', 'Drive architectural plans and implementation for future data storage, reporting, and analytic solutions.', ' Knock is a 100% remote, work from home culture and has been since our inception in 2015  100% employee covered medical, dental, & vision premiums  Unlimited PTO (2 weeks mandatory) + flexible work schedules  Paid parental leave  $1,000 each year for education, training, and professional development  Salary + Equity. We offer equitable compensation based market data, accomplishments, experience, & your location ', 'All of our teams have a strong customer-first mindset and data-driven approach to our work, and that should be your approach as well', 'Knock is a 100% remote, work from home culture and has been since our inception in 2015 ', 'Salary + Equity. We offer equitable compensation based market data, accomplishments, experience, & your location', '100% employee covered medical, dental, & vision premiums ', 'Design data schemas and optimize internal data warehouses, augmenting data from multiple sources.', 'Please no recruitment firm or agency inquiries, you will not receive a reply from us.', 'Have proven success working 100% remote in prior positions & are experienced working with a distributed, national team. ', 'PDF is our preferred format for resumes and any other attachments. Thank you!', 'Understand the data that powers our applications, and be able to propose appropriate data models for new features.', 'Have experience building data pipelines and utilizing programming tools to do so. Here Knock we use Apache Spark, Scala, Rust, Go, Python, and Rest APIs - but you can learn these technologies on the job', ' Have experience building data pipelines and utilizing programming tools to do so. Here Knock we use Apache Spark, Scala, Rust, Go, Python, and Rest APIs - but you can learn these technologies on the job You should be versed in developing APIs to serve data produced by ETL jobs You have a desire to work at a rapidly growing startup and make it a success, and are comfortable learning new technologies and tools Your experience in SQL, MySQL, or Postgres will be valuable here to identify slow queries and debugging All of our teams have a strong customer-first mindset and data-driven approach to our work, and that should be your approach as well Believe in creating diverse, equitable, and inclusive practices and programs that will further Knock’s commitment to making an impact, learning, putting people first, being open, and courageous. Have proven success working 100% remote in prior positions & are experienced working with a distributed, national team.  We encourage you to apply even if you don’t have every listed requirement.  ', 'Monitor and troubleshoot operational or data issues in the data pipelines.', 'We encourage you to apply even if you don’t have every listed requirement. ', '$1,000 each year for education, training, and professional development ', 'Cross-functionally collaborate with our Data Science and Machine Learning teams. ', 'Build data pipelines and aggregate data.', 'This position is in the continental United States.', 'Build new ETL jobs from scratch, as well as maintain existing jobs.', 'Paid parental leave ', 'Be committed to good engineering practice of testing, logging, alerting and deployment processes.', 'Benefits, Perks, & Enjoying Life', 'You should be versed in developing APIs to serve data produced by ETL jobs', 'Design, build, and maintain REST APIs to serve data to customers', 'Believe in creating diverse, equitable, and inclusive practices and programs that will further Knock’s commitment to making an impact, learning, putting people first, being open, and courageous.', 'Unlimited PTO (2 weeks mandatory) + flexible work schedules ', 'You have a desire to work at a rapidly growing startup and make it a success, and are comfortable learning new technologies and tools']",Mid-Senior level,Full-time,Engineering,Internet,2021-03-24 13:05:10
Data Analytics Engineer (Python) - Remote Contract,Cella,"Reading, PA",22 hours ago,60 applicants,"['', 'Ability to make models operational and monitor for excessive levels of degradation', 'Primary role involves developing models to help solve business problems', 'Familiarity with requirement gathering', 'Coding/statistical analysis capabilities include at least one of the following: SAS, Python, SPSS', 'Background', 'Clearly communicate outcomes of models and or analysis', 'Strong communication skills', ' 2 to 3 years of data/business analytics heavily focused on data exploration, statistical programming and modeling using SAS/Python/SPSS Ability to communicate complex results in a comprehensible way is a critical skill ', 'Ability to communicate complex results in a comprehensible way is a critical skill', 'contract to hire ', 'A mathematical, statistical, or econometric background with practical business experience', 'Practical modeling/coding experience', 'SAS, Python, SPSS', '2 to 3 years of data/business analytics heavily focused on data exploration, statistical programming and modeling using SAS/Python/SPSS', 'Summary', ' A mathematical, statistical, or econometric background with practical business experience Strong communication skills Familiarity with requirement gathering An understanding of business analytics and model building methodology Practical modeling/coding experience Coding/statistical analysis capabilities include at least one of the following: SAS, Python, SPSS ', ' Primary role involves developing models to help solve business problems Clearly communicate outcomes of models and or analysis Ability to make models operational and monitor for excessive levels of degradation ', 'Core Capabilities', 'remote Data Analytics Engineer', 'specialization in math, statistics', 'An understanding of business analytics and model building methodology']",Entry level,Temporary,Advertising,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer,io173,"Boston, MA",1 week ago,140 applicants,"['', 'The ideal candidate should have experience using the following technologies:', 'On offer is the opportunity to work with a Global Top Tier Consulting organisation who are at the fore-front of the Artificial Intelligence and Data Science Markets.  Excellent compensation package is on offer for the right individual.', ""Partner with our clients, from data owners and users to C-level executives, to understand their needs and build impactful analytics solutionsDesign and build data pipelines to support data science projects following software engineering best practicesUse state of the art technologies to acquire, ingest and transform big datasetsMap data fields to hypothesis, curate, wrangle and prepare data to be used in advanced analytics modelsCreate and manage data environments in the cloud or on premiseEnsure information security standards are maintained at all timeContribute to cross-functional problem-solving sessions with your team and deliver presentations to colleagues and clientsBe flexible to travel to our clients' offices to deliver presentations, gather information or share knowledgeHave the opportunity to contribute to R&D and internal asset development projects"", 'Use state of the art technologies to acquire, ingest and transform big datasets', 'Duties and Responsibilities :', 'Create and manage data environments in the cloud or on premise', 'Ensure information security standards are maintained at all time', ' Python, PySpark, SQL, Airflow, Databricks, container technologies such as Docker and Kubernetes, cloud solutions such as AWS, GCP or Azure and more', 'You will be required to work with clients, senior stakeholder and data owners and enjoy solving problems in a collaborative way.', 'Contribute to cross-functional problem-solving sessions with your team and deliver presentations to colleagues and clients', ""Be flexible to travel to our clients' offices to deliver presentations, gather information or share knowledge"", 'io173 are currently looking to recruit an experienced Data Engineer on behalf of our Global Consultancy Client.', 'Have the opportunity to contribute to R&D and internal asset development projects', 'Partner with our clients, from data owners and users to C-level executives, to understand their needs and build impactful analytics solutions', 'This role can be based in Boston, New York, Chicago or Montreal.', 'Map data fields to hypothesis, curate, wrangle and prepare data to be used in advanced analytics models', 'You will be part of a Global Data Engineering community and you will work in cross-functional and Agile project teams alongside Project Managers, Data Scientists, other Data Engineers, Machine Learning Engineers and industry experts.', 'Design and build data pipelines to support data science projects following software engineering best practices']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,ektello,"Florham Park, NJ",3 days ago,Be among the first 25 applicants,"['', ' Demonstrated industry efficiency in the fields of database, data warehousing or data sciences.', 'Direct Hire Opportunity! ', ' Ability to collaborate effectively across organizations.', ' Bachelor’ s degree is required while a Masters or PhD in Computer Science, Physics, Engineering or Math, with Advanced certifications are highly desirable.', ' Implementing AWS services in a variety of distributed computing, enterprise environments', ' Ability to think strategically about business, product, and technical challenges in an enterprise environment.', ' Customer facing skills with the ability to drive discussions with senior leadership regarding trade-offs, best practices, and', 'Job Description', ' Understanding of database and analytical technologies in the industry including MPP and NoSQL databases, Data Warehouse design, PostgreSQL, BI reporting, and Dashboard development', ' Bachelor’ s degree is required while a Masters or PhD in Computer Science, Physics, Engineering or Math, with Advanced certifications are highly desirable. Hands on experience leading enterprise-wide data engineering, warehousing, and analytics projects Ability to think strategically about business, product, and technical challenges in an enterprise environment. Ability to collaborate effectively across organizations. Understanding of database and analytical technologies in the industry including MPP and NoSQL databases, Data Warehouse design, PostgreSQL, BI reporting, and Dashboard development Demonstrated industry efficiency in the fields of database, data warehousing or data sciences. Implementing AWS services in a variety of distributed computing, enterprise environments Customer facing skills with the ability to drive discussions with senior leadership regarding trade-offs, best practices, and', ' Hands on experience leading enterprise-wide data engineering, warehousing, and analytics projects']",Entry level,Full-time,Engineering,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,G2 Solutions ,Raleigh-Durham-Chapel Hill Area,,N/A,"['', '• Ability to write advanced SQL queries. ', '• Develop new analysis to improve metrics around in stocks, turns, and trans cost. ', '• Prepare data for modelling and make best/creative use of applicable and available internal or external data ', '• Ability to communicate complex technical information in common language to foster teaching and analytics guidance to internal customers. • Advanced experience in analytics, data cleaning, and predictive modeling. ', '• 3+ years of experience with predictive modeling (classification, regression, parameter tuning, optimization criteria, feature selection), preferably with multiple techniques ', 'Qualifications ', '•Strong knowledge in SAS, R, Python or another platform to develop and implement predictive models. ', '• Operate with minimal supervision and once given general assignments, prioritizes and executes tasks. ', '• Detail-oriented and ability to work collaboratively', '• Research new statistical and mathematical techniques that are suitable and helpful for solving business related problems ', 'Would you like to join a growing Predictive Modeling team? You’ve landed on the right page! ', '• Utilizes industry-leading standards for working with very large datasets to extract meaningful business information using statistics, machine learning, and predictive analytics. ', '• 2+ years of Data Modeling or similar experience. ', 'Responsibilities ', '• Bachelor’s degree in statistics, applied mathematics, or related discipline', '• Demonstrated experience working with large relational data sets. ', '• Experience in model validation techniques, model testing and continuous monitoring of model performance ', '• Develop predictive systems and algorithms for identifying trends and driving business solutions. ', 'G2 Solutions is searching for a Data Scientist for a direct hire position located in Raleigh, NC. In this role you will explore large data sets to build model ready data products and leverage analytics tools like SAS to build machine learning models for predictive strategies. You must be very familiar with modeling tools such as Python, R, & SAS with the ability to write advanced SQL queries. We are looking for detail-oriented individuals who are self-starting and have the ability to work collaboratively with a team. Please review the requirements and responsibilities below and apply today to speak with one of our experienced recruiters. ', '• Ability and willingness to quickly gain knowledge of SAS enterprise guide and enterprise miner. ']",Mid-Senior level,Full-time,Information Technology,Staffing and Recruiting,2021-03-24 13:05:10
Data Engineer,Nesco Resource,United States,3 weeks ago,89 applicants,"['', 'Advanced knowledge with SQL Server Database - including writing advanced SQL scripts, profiling and optimization.', 'Minimum of 5 years of experience with NoSQL database, including Postgres', 'Prior experience working with other Big Data tools such as Spark, Snowflake, and Kafka', 'Work with and support\xa0an Application Engineering team, DBA, Infrastructure, and Project Management Office.', 'Design/develop data pipelines to extract data from multiple data sources using Azure, Snowflake Cloud, and other cloud-native technologies', 'This is a direct hire/perm role that includes a great benefits package and a bonus plan. ', 'Minimum 3 years of experience\xa0working with Azure Cloud Services: Data Factory, SQL database, Functions, Data Lake, Databricks, Logic Apps, and Azure Automation.', ' Duties/Responsibilities: ', 'Fluent with the following scripting languages: Python, Scala, and C#', 'Nesco Resource is hiring a Data Engineer - Azure for a growing company in Austin. We are looking for a passionate Data Engineer to join a growing Data and Analytics team whose mission is to modernize their platform to the next-generation cloud platform. You will be responsible for expanding, optimizing, and improving overall data quality and set up next-generation data orchestration using modern cloud tools and technologies. \xa0This will be a remote role but at some point this year you will\xa0be going back into the Austin office.This is a direct hire/perm role that includes a great benefits package and a bonus plan. Would you be available? ', 'Working knowledge of BI Tools: MS Integration Services, Reporting Services, and Analysis Services, and Power BI', 'Design/develop data pipelines to extract data from multiple data sources using Azure, Snowflake Cloud, and other cloud-native technologiesBuild out a data model to gain actionable insights from data, operational efficiency, and other key business performance metrics.Design, manage, & monitor inbound and outbound data processesAutomate the data testing processes and integrate them with monitoring systemsWork with and support\xa0an Application Engineering team, DBA, Infrastructure, and Project Management Office.Analyze existing legacy systems and data sets to help Business Analysts define the functional/non-functional requirements.', ' Data Engineer', 'Design, manage, & monitor inbound and outbound data processes', 'Knowledge/Skills: ', 'Data Engineer - Azure', 'Duties/Responsibilities:', 'Knowledge/Skills:', 'Data Engineer', 'Automate the data testing processes and integrate them with monitoring systems', 'Build out a data model to gain actionable insights from data, operational efficiency, and other key business performance metrics.', 'Minimum of 5 years of experience with NoSQL database, including PostgresMUST HAVE prior experience migrating to Azure from a legacy system (SSIS to Azure is fine)Minimum 3 years of experience\xa0working with Azure Cloud Services: Data Factory, SQL database, Functions, Data Lake, Databricks, Logic Apps, and Azure Automation.Fluent with the following scripting languages: Python, Scala, and C#Advanced knowledge with SQL Server Database - including writing advanced SQL scripts, profiling and optimization.Working knowledge of BI Tools: MS Integration Services, Reporting Services, and Analysis Services, and Power BIPrior experience working with other Big Data tools such as Spark, Snowflake, and Kafka', 'Thank you for your time and consideration!', 'MUST HAVE prior experience migrating to Azure from a legacy system (SSIS to Azure is fine)', 'Analyze existing legacy systems and data sets to help Business Analysts define the functional/non-functional requirements.']",Mid-Senior level,Full-time,Information Technology,Insurance,2021-03-24 13:05:10
Data Engineer,ComResource,"Columbus, Ohio Metropolitan Area",23 hours ago,49 applicants,"['', 'Design, implement, write, and deploy applications', 'Familiarity with Hadoop, HDFS, HBase, Hive, Impala, Ooozie, Sqoop', 'Responsibilities:', 'Working knowledge of AWS, Google Cloud, Cloud Foundry, Azure, or OpenStack Cloud', 'Familiarity with Hadoop, HDFS, HBase, Hive, Impala, Ooozie, SqoopWorking knowledge of AWS, Google Cloud, Cloud Foundry, Azure, or OpenStack CloudAbility to implement Best Practices surrounding Big Data initiatives, Test-Driven Development, and/or Secure Code', 'Provide vendor communication, knowledge transfer, requirements eliciting, and build relationships with Cloud providers (AWS / GCP/ Azure)', 'Respond to alerts, review errors, debug, and deploy in a timely manner', 'Use of open source technologies4+ years of professional experience in a software engineering role using Java, Scala, Python, and/or similar OOP languagesWorking knowledge of Source Control, preferred Github', '4+ years of professional experience in a software engineering role using Java, Scala, Python, and/or similar OOP languages', 'Working knowledge of Source Control, preferred Github', 'Requirements:', 'ComResource is looking for a a Mid Level Developer proficient in the latest technologies that enable large-scale processing of data. This Developer will be part of an Agile team performing independent and collaborative deliverables as needed.', 'Work with Cloudera technology to enable support and plan/build of products', 'Use of open source technologies', 'Ability to implement Best Practices surrounding Big Data initiatives, Test-Driven Development, and/or Secure Code', 'Design, implement, write, and deploy applicationsWork with Cloudera technology to enable support and plan/build of productsProvide vendor communication, knowledge transfer, requirements eliciting, and build relationships with Cloud providers (AWS / GCP/ Azure)Respond to alerts, review errors, debug, and deploy in a timely manner', 'Plus:']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
"Data Engineer, Data Warehouse",SoFi,"Murray, UT",3 days ago,Be among the first 25 applicants,"['', 'Strong business communication skills that can break down technical problems into business language for non-technical personnel', 'Competitive salary packages and bonusesComprehensive medical, dental, vision and life insurance benefitsGenerous vacation and holidaysPaid parental leave for eligible employees401(k) and education on retirement planningTuition reimbursement on approved programsMonthly contribution up to $200 to help you pay off your student loansGreat health & well-being benefits including: telehealth parental support, subsidized gym programEmployer paid lunch program (except for remote employees)Fully stocked kitchen with snacks and drinks (When we’re back in the office of course)', 'Comprehensive medical, dental, vision and life insurance benefits', 'Generous vacation and holidays', 'Data modelingBuild and maintain data structures and ETL/ELT data pipelinesProvision, optimize and maintain data feeds to external systemsWrite code to validate data quality and clean existing dataHelp analytics team, upstream engineering teams, as well as non-technical business users in understanding the Data WarehouseBe part of an on call support rotation to support the Data Warehouse and it’s automated processesCreating technical documentation ', 'Fully stocked kitchen with snacks and drinks (When we’re back in the office of course)', 'Experience using kafka', 'Monthly contribution up to $200 to help you pay off your student loans', 'What You’ll Need', 'Understands database architecture', 'Tuition reimbursement on approved programs', 'Great health & well-being benefits including: telehealth parental support, subsidized gym program', 'Nice To Have', 'Data modeling', 'Experience using cloud data technologies such as Redshift, Snowflake, or GCP', 'Experience writing SQL against several different database platforms', 'Experience in building data feeds and business reports', 'Experience using AWS data technologies such as (S3, Glue, Kinesis, Lambda, etc.)', 'Write code to validate data quality and clean existing data', 'Creating technical documentation ', 'Ability to work in a fast-paced environment, meet deadlines, and prioritize a workload', 'Paid parental leave for eligible employees', 'Position at SoFi', 'Description', 'Provision, optimize and maintain data feeds to external systems', 'Working knowledge of some AWS data technologies', 'Experience using business intelligence reporting tools (Tableau, Looker, etc.)', 'Responsibilities', 'Why You’ll Love Working Here', 'Competitive salary packages and bonuses', 'Understanding of the software development lifecycle process', '401(k) and education on retirement planning', 'Experience writing SQL against several different database platformsExperience creating data pipelines using Python scriptingExperience using cloud data technologies such as Redshift, Snowflake, or GCPExperience using AWS data technologies such as (S3, Glue, Kinesis, Lambda, etc.)Experience in dockerExperience using kafkaExperience in building data feeds and business reports', 'Employer paid lunch program (except for remote employees)', '3+ years working experience working with automated scripting, data modeling, and data architectureProficient in writing and optimizing SQL scripts Understands database architectureWorking experience in the Python language with an emphasis on dataWorking knowledge of some AWS data technologiesUnderstanding of the software development lifecycle processSkills and experience in finding, investigating, and resolving data quality issuesAbility to work in a fast-paced environment, meet deadlines, and prioritize a workloadAbility to bring new ideas and promote process improvementStrong business communication skills that can break down technical problems into business language for non-technical personnel', 'Build and maintain data structures and ETL/ELT data pipelines', 'Be part of an on call support rotation to support the Data Warehouse and it’s automated processes', 'Working experience in the Python language with an emphasis on data', 'Help analytics team, upstream engineering teams, as well as non-technical business users in understanding the Data Warehouse', '3+ years working experience working with automated scripting, data modeling, and data architecture', 'Experience in docker', 'The Role', 'Skills and experience in finding, investigating, and resolving data quality issues', 'Ability to bring new ideas and promote process improvement', 'Proficient in writing and optimizing SQL scripts ', 'Experience creating data pipelines using Python scripting']",Not Applicable,Full-time,Information Technology,Financial Services,2021-03-24 13:05:10
Data Engineer ,Bayside Solutions,"Seattle, WA",1 week ago,64 applicants,"['', '5+ years of experience in big data ecosystem. Example technologies include batch and stream processing (e.g. Spark, Hive, Flink, Beam), analytical engines (e.g. Presto, Druid), search platform (e.g. Solr/Lucene), tooling (e.g. Airflow, Jupyter, Superset, Tableau), and storage format (e.g. Iceberg)Excellent verbal and written communication skills, able to collaborate cross-functionally with data science, machine learning, data platform and analytics teamsCustomer-focused mindset, with emphasis on user experience and satisfactionSuperb problem-solving skills, and able to thrive in a fast-paced and dynamic environmentHands-on in designing, building, scaling, and troubleshooting solutions to big data problemsMust be self-driven, and able to provide advice and support to users to properly integrate with our data platformProgramming experience in Java, Python, Scala, or similar languagesPassionate about latest big data technologies, open source community presence is a big plusExperience with AWS, Kubernetes, Infrastructure-as-code, and data privacy & compliance is a big plus', 'Hands-on in designing, building, scaling, and troubleshooting solutions to big data problems', 'You will be a key member in the client engineering team that drives user engagement with our new generation of data platform, collaborates cross-functionally with data, product management, and infrastructure teams, and provides data tools to enable ingesting, storing, processing and interacting of data with a focus in user experience. RESPONSIBILITIES INCLUDE: - Collaborate with our infrastructure users and product management to ensure success in customer engagement and onboarding of new users - Be an advocate of our tech stack, stay on top of technology advancement and explore innovation opportunities - Prototype, build, diagnose, fix, improve and automate complex issues across the entire stack to power ETL, analytics and privacy efforts across AI/ML - Advise and support other teams on proper integration of our platform, including holding regular brown bag, office hour and training sessions - Build relationships with Data Scientists, Product Managers and Software Engineers to understand data needs - Establish and fulfill SLAs for supported data tools', 'BS, MS, or PhD degree in Computer Science or equivalent', 'Excellent verbal and written communication skills, able to collaborate cross-functionally with data science, machine learning, data platform and analytics teams', 'Experience with AWS, Kubernetes, Infrastructure-as-code, and data privacy & compliance is a big plus', 'Preferred - Only W2 candidates and Candidate needs to be willing to re-lo to Seattle if and when the client goes back onsite.', 'Customer-focused mindset, with emphasis on user experience and satisfaction', '5+ years of experience in big data ecosystem. Example technologies include batch and stream processing (e.g. Spark, Hive, Flink, Beam), analytical engines (e.g. Presto, Druid), search platform (e.g. Solr/Lucene), tooling (e.g. Airflow, Jupyter, Superset, Tableau), and storage format (e.g. Iceberg)', 'Description', 'Passionate about latest big data technologies, open source community presence is a big plus', 'Programming experience in Java, Python, Scala, or similar languages', 'Summary', 'Education & Experience', 'Only W2 candidates and Candidate needs to be willing to re-lo to Seattle if and when the client goes back onsite.', 'Superb problem-solving skills, and able to thrive in a fast-paced and dynamic environment', 'Must be self-driven, and able to provide advice and support to users to properly integrate with our data platform', '\xa0Data Infrastructure team within an organization that powers analytics, experimentation and ML feature engineering. The mission of the Data Infrastructure org is to provide our engineers and data scientists a cutting edge, reliable and easy to use infrastructure for ingesting, storing, processing and interacting with data and ultimately help the teams that build data intensive applications be successful. You will work with many cross functional teams and lead the planning, execution and success of technical projects with the ultimate purpose of improving experience for customers. We are looking for engineers who want to bring their passion for infrastructure to build world class infrastructure products. Are you a passionate about building scalable, reliable, maintainable infrastructure and solving data problems at scale? Come join us and be part of the Data Infrastructure journey.', 'Key Qualifications', 'Preferred ']",Associate,Contract,Engineering,Consumer Electronics,2021-03-24 13:05:10
Data Engineer,Node.Digital,"McLean, VA",6 days ago,Be among the first 25 applicants,"['', 'Experience with programming Web Services technologies - REST, SOAP, Java, and Costpoint Integration Console', '571 360 4341', 'Location US-VA-Tysons', 'Company Overview', 'Assist (as necessary) in gathering and documenting user and process requirements.', 'Must be a U.S. citizen with the ability to obtain and maintain a Federal Government clearance.', 'Strong time management skills and ability to coordinate and prioritize tasks with little supervision.', 'Provide technical support to system users and functional subject matter experts; contribute to developing and leading subject matter expert teams across various business units and matrixed service lines', 'OUR CORE VALUES', 'Good communication skills, specifically listening to customer needs, restating those needs, and documenting your work', 'Support the execution of training scripts and training plans.', 'Bachelor’s degree required. Degree in Information Technology preferred. Degree can be substituted with equivalent professional experience.', 'Requirements', 'Thanks & Regards,', '5 years programming experience in Java, C#, or similar language', 'Experience with the following systems; Streamsets, Databasics, iCIMS, ServiceNow, Tableau, and Jira', 'Develop and manage system integrations across all corporate systems', 'Experience with database programming - SQL Server scripting preferred and developing triggers, stored procedures, indexes, functions, tables, views, linked servers, scripts, and queries.', 'Familiar with standard accounting concepts and methods', 'Security Requirements', 'Experience with working on project teams to implement software upgrades, process improvements, and software implementations', '5 - 8 years using and administering Deltek products such as Costpoint 7.1, T&E 9/10', 'Responsibilities', 'Experience with systems/data integration software and products – Streamsets preferred', 'Qualifications', 'Able to adapt to changing work efforts, manage impact of shifting priorities, and resolve technical emergencies as they may occur.', ' Bachelor’s degree required. Degree in Information Technology preferred. Degree can be substituted with equivalent professional experience. 5 - 8 years creating and supporting data integrations with ERP and other corporate business systems 5 - 8 years working with Microsoft SQL, writing custom code to support business processes, data management, and reporting. 5 years programming experience in Java, C#, or similar language ', 'Conduct technical demonstrations for employees to help influence adoption and use of Costpoint and other Deltek products.', 'Experience with data warehousing - Kimball method preferred', ' Experience with programming Web Services technologies - REST, SOAP, Java, and Costpoint Integration Console Experience with database programming - SQL Server scripting preferred and developing triggers, stored procedures, indexes, functions, tables, views, linked servers, scripts, and queries. Experience with database design– ERD, working/temp tables, tablespaces, partitions, indexes, dynamic object sizing, statistics. Experience with systems/data integration software and products – Streamsets preferred Experience with working on project teams to implement software upgrades, process improvements, and software implementations Understanding of the underlying Costpoint and T&E databases Experience with data warehousing - Kimball method preferred Ability to analyze problems to provide the best solution. Good communication skills, specifically listening to customer needs, restating those needs, and documenting your work Strong time management skills and ability to coordinate and prioritize tasks with little supervision. Able to adapt to changing work efforts, manage impact of shifting priorities, and resolve technical emergencies as they may occur. ', 'Provide backup administration support for Deltek Costpoint 7, Time & Expense 9, and Cognos 10.2 Business Intelligence to include application updates and backups', 'Develop extensibility programming within the Costpoint software platform', 'Ability to analyze problems to provide the best solution.', 'Sue', '5 - 8 years working with Microsoft SQL, writing custom code to support business processes, data management, and reporting.', ' Creating and supporting data integrations with Deltek systems 5 - 8 years using and administering Deltek products such as Costpoint 7.1, T&E 9/10 Experience with the following systems; Streamsets, Databasics, iCIMS, ServiceNow, Tableau, and Jira Familiar with standard accounting concepts and methods ', '5 - 8 years creating and supporting data integrations with ERP and other corporate business systems', ' Manage the relationships between financial systems and the dependencies within each application as well as the integrations with LMI systems outside of the Deltek suite of products. Develop and manage system integrations across all corporate systems Develop extensibility programming within the Costpoint software platform Provide technical support to system users and functional subject matter experts; contribute to developing and leading subject matter expert teams across various business units and matrixed service lines Provide backup administration support for Deltek Costpoint 7, Time & Expense 9, and Cognos 10.2 Business Intelligence to include application updates and backups Provide guidance to IT project managers on levels of effort, as well as recommendations for adherence to requirements, project plans, timelines, and deliverables. Conduct technical demonstrations for employees to help influence adoption and use of Costpoint and other Deltek products. Assist (as necessary) in gathering and documenting user and process requirements. Support the execution of training scripts and training plans. ', 'Manage the relationships between financial systems and the dependencies within each application as well as the integrations with LMI systems outside of the Deltek suite of products.', 'Experience with database design– ERD, working/temp tables, tablespaces, partitions, indexes, dynamic object sizing, statistics.', 'Understanding of the underlying Costpoint and T&E databases', 'Data Engineer (Deltek)', 'Preferred Experience', 'REQUIRED EXPERIENCE ', 'Creating and supporting data integrations with Deltek systems', 'Provide guidance to IT project managers on levels of effort, as well as recommendations for adherence to requirements, project plans, timelines, and deliverables.']",Mid-Senior level,Full-time,Engineering,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Pandera Systems,"Illinois, United States",3 weeks ago,101 applicants,"['', 'Designing pipelines and architectures for data processing', 'Implement solutions for structured, semi-structured, and unstructured data sources, relational and non-relational databases.', 'Certified Google Cloud Data Engineer / ArchitectExperience working in an AGILE environmentAdvanced Java/Python coding skillsExperience with CI-CD pipelines for promoting big data release deployments and designing log monitoring features.GIT expertiseMDM expertiseInfrastructure as Code experience (Terraform, Ansible, etc.)Experience with Deploying a Data Governance Program', 'Experience with Deploying a Data Governance Program', 'GIT expertise', 'Experience working in an AGILE environment', 'Do you like data? Do you know how to take data and transform it to get meaningful data into end users’ hands? Do you enjoy taking complex problems and turning them into technical solutions that power Reporting Solutions, Advanced Analytics, and Data Sciences? We are looking for a Cloud Data Engineer to join our team of engineers, analysts, and architects focused on designing, building, and delivering data solutions that drive business decisions. As a\xa0Pandera\xa0Employee, you will have the opportunity to build world-class solutions to help our clients and partners solve challenging problems through data.', 'Education:', 'Partner with multiple client stakeholders including partners, business users, BI and Analytics teams.', 'Be Rewarded: A competitive salary and instant vesting on 401k are only a few of the rewards for a job well done.', 'Infrastructure as Code experience (Terraform, Ansible, etc.)', '*Local Hire or candidates relocating to Ohio, Indiana, Illinois, or Michigan', 'Primary Responsibilities:Design and build data engineering solutions using Google Cloud Platform (GCP) services: BigQuery, DataFlow, Pub/Sub, BigTable, Data Fusion, DataProcData modeling and schema design that will range across multiple business domains and industries within the cloud for large enterprise data warehouse and data lakes solutions.Extracting, Loading, Transforming, cleaning, and validating data using cloud ETL/ELT toolsDesigning pipelines and architectures for data processingPartner with multiple client stakeholders including partners, business users, BI and Analytics teams.Work with teams to conduct workshops to identify data sources, flows, and requirements.Conduct client workshops to help shape their future data strategy by providing future state architectures, roadmaps, and implementation plans.', 'MDM expertise', 'Certified Google Cloud Data Engineer / Architect', 'Location*:', 'Type:', 'Experience developing and deploying ETL / ELT processes and documentation including physical data model, source to target mappings, ETL / ELT packages (Matillion, Fivetran, Spark, Google Data Fusion, etc.)', 'Be Inspired: Collaborative workspace and unlimited vacation to keep your mind fresh and ready to take on the next new idea.', 'Be Healthy: Health, dental, and vision offered through top tier providers and unlimited sick leave to keep you feeling at the top of your game.', 'Be Rewarded: A competitive salary and instant vesting on 401k are only a few of the rewards for a job well done.Be Healthy: Health, dental, and vision offered through top tier providers and unlimited sick leave to keep you feeling at the top of your game.Be Inspired: Collaborative workspace and unlimited vacation to keep your mind fresh and ready to take on the next new idea.Be Supported: A large network of industry experts, internal training platform, and external learning opportunities to grow your skills and experience.Be a Team: Team outings, happy hours, passion presentations, volunteer opportunities, meetups, etc. we are creating a community to continuously share and grow as a team.', 'Primary Responsibilities:', 'Extracting, Loading, Transforming, cleaning, and validating data using cloud ETL/ELT tools', 'Basic Qualifications:', 'Primary Responsibilities:Design and build data engineering solutions using Google Cloud Platform (GCP) services: BigQuery, DataFlow, Pub/Sub, BigTable, Data Fusion, DataProc', 'Data modeling and schema design that will range across multiple business domains and industries within the cloud for large enterprise data warehouse and data lakes solutions.', 'Experience working in GCP based Big Data deployments (Batch/Real-Time) leveraging Big Query, Big Table, Google Cloud Storage, PubSub, Data Fusion, Dataflow, Dataproc, etc.', 'Excellent analytical and troubleshooting skills', 'Experience with CI-CD pipelines for promoting big data release deployments and designing log monitoring features.', 'Be Supported: A large network of industry experts, internal training platform, and external learning opportunities to grow your skills and experience.', 'Be a Team: Team outings, happy hours, passion presentations, volunteer opportunities, meetups, etc. we are creating a community to continuously share and grow as a team.', 'Type:\xa0Full time ', 'Location*:\xa0This GCP Cloud Data Engineer position\xa0will be based in the Midwest', 'Salary: Competitive rate based on qualifications', '3-5+ years of experience in Data Engineering/BI areas with at least 2 years data engineering on GCP·', 'Travel:\xa0Onsite client location(s) - overnight is based on the needs of the client.', 'Advanced Java/Python coding skills', 'Strong written and oral communication skills', 'Proven ability to work with users to define requirements and business issues', ""If working with the world's leading Cloud and Data Platforms is something that you are interested in, and you like to be able to have the ability to work on building next generation smart data platforms, then Pandera is the right place for you. From migration and modernization of legacy data systems to the Cloud, along with building Cloud Native solutions for our customers, the adventure will never be dull, and you will always be learning and growing with Pandera."", 'Salary', '*Authorized to work in the USA', 'Experience developing logical data models within cloud data warehouses', '*Fully remote option available ', 'Demonstrated mastery in cloud database concepts and large-scale cloud data warehouse and lake implementations', 'Demonstrated mastery in Google BigQuery', ""Education:\xa0Bachelor's or Master’s degree in technical discipline; Master's preferred, or equivalent years’ of experience in the field."", 'Conduct client workshops to help shape their future data strategy by providing future state architectures, roadmaps, and implementation plans.', 'Work with teams to conduct workshops to identify data sources, flows, and requirements.', '3-5+ years of experience in Data Engineering/BI areas with at least 2 years data engineering on GCP·Experience working in GCP based Big Data deployments (Batch/Real-Time) leveraging Big Query, Big Table, Google Cloud Storage, PubSub, Data Fusion, Dataflow, Dataproc, etc.Experience developing logical data models within cloud data warehousesExperience developing and deploying ETL / ELT processes and documentation including physical data model, source to target mappings, ETL / ELT packages (Matillion, Fivetran, Spark, Google Data Fusion, etc.)Demonstrated mastery in Google BigQueryDemonstrated mastery in cloud database concepts and large-scale cloud data warehouse and lake implementationsImplement solutions for structured, semi-structured, and unstructured data sources, relational and non-relational databases.Proven ability to work with users to define requirements and business issuesExcellent analytical and troubleshooting skillsStrong written and oral communication skills', 'Ideal Qualifications:', 'You are a hot commodity and having you on the team would be an honor to us! Here are some of the ways we pay it forward to recognize your contribution to our vision!', 'Travel:', 'What do you get out of this?', 'Pandera Systems is a highly specialized analytics and technology consulting firm with a core focus in developing data-driven solutions in the Cloud. We have strategic partnerships with some of the largest Cloud Platforms, including Premier Partner with Google Cloud Platform and Premier Partner of Snowflake. We have been providing innovative data driven solutions to some of the world’s largest companies for over 11 years.']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Compliant Pharmacy Alliance Cooperative,"Stoughton, WI",2 weeks ago,61 applicants,"['', 'Basic understanding of Power BI', 'Azure', 'Fundamental knowledge of T-SQL syntax and database design', 'Basic understanding of SSMS (SQL Server Management Studio)', 'What we need:', 'Data Engineer', '2-3 years of Azure data experience (work as a Data Engineer with emphasis in Azure, Database Administrator in cloud-based environment, and/or ETL Developer with an emphasis in Azure)Basic understanding of Power BIFundamental knowledge of T-SQL syntax and database designBasic understanding of SSMS (SQL Server Management Studio)Knowledge of how to extract flat files (Delimited, JSON, Parquet, etc.) and transform the data into usable structuresAbility to engineer, create, and maintain Azure data pipelines', 'This role can be a remote role, but will need to be within driving distance of Stoughton as there will be periodic needs to be onsite.\xa0', 'Azure Data Engineer', 'Knowledge of how to extract flat files (Delimited, JSON, Parquet, etc.) and transform the data into usable structures', 'What you’ll do:', 'Compliant Pharmacy Alliance Cooperative in Stoughton, WI, is currently seeking an Azure Data Engineer to join their team.\xa0Working for a small, dynamic, and growing company this role is responsible for taking in raw data from multiple sources, transforming the data to a usable format and storing the data securely for company use.\xa0Transforming the data is complex, but is key to this role as we are refactoring (not doing a lift and shift) to ensure success in how we’re storing the data to ensure we’re not pulling forward unnecessary legacy methodologies.\xa0', ""The Azure Data Engineer develops, constructs, and maintains database systems and data pipeline processing systems.\xa0They are responsible for building data acquisition processes, managing data flows through distributed systems, and storing data utilizing relational databases and flat files.\xa0They also engineer and maintain accurate reporting, data, and analytical tools relative to the organization's needs."", '2-3 years of Azure data experience (work as a Data Engineer with emphasis in Azure, Database Administrator in cloud-based environment, and/or ETL Developer with an emphasis in Azure)', 'Ability to engineer, create, and maintain Azure data pipelines']",Associate,Full-time,Information Technology,Pharmaceuticals,2021-03-24 13:05:10
Data Engineer,HealthLevel,"Mountain View, CA",1 week ago,102 applicants,[''],Entry level,Full-time,Information Technology,Hospital & Health Care,2021-03-24 13:05:10
Data Engineer,Talution Group,"Chicago, IL",2 weeks ago,73 applicants,"['Job Description', '', 'Talution Group is hiring a Data Engineer to place on contract with our client in Chicago. This is currently a remote position, but our consultant will need to be on-site in the future.']",Entry level,Contract,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,ReviveMed,"Cambridge, MA",3 weeks ago,115 applicants,"['', 'Experience building new solutions using Docker and the Kubernetes container orchestration', '·\xa0\xa0\xa0\xa0\xa0\xa0Opportunities for personal and professional growth as our company expands', 'Why you should join ReviveMed', 'The ability to think strategically about data architecture and build data pipelines', 'We are looking for an exceptional data science software engineer to join our team. We have a fast-paced, high-performance, and result-oriented culture. The preferred candidate will bring relevant experience, entrepreneurial drive, and a passion for advancing our vision.', 'We are an award-winning enthusiastic team, passionate about innovative, data-driven approaches to bring the right therapeutics for the right patients, and ultimately save millions of lives.', 'Required Qualifications', '\xa0', 'ReviveMed is an MIT spinout, artificial intelligence (AI)-driven drug discovery platform by uniquely leveraging data from small molecules or metabolites in the human body. Located in the heart of biotech innovation in Cambridge, ReviveMed uniquely overcomes the difficulties of identifying a large set of metabolites for each patient, based on technology that our team developed at MIT and published in Nature Methods. Our AI platform further combines the data from small molecules with other molecular data and translates these data into novel therapeutic insights for drug discovery. Currently, ReviveMed is collaborating with tier-one pharmaceutical companies and pursuing internal drug discovery, initially focused on metabolic diseases, including non-alcoholic fatty liver.', 'BS, MS, or Ph.D. degree in Computer Science or related fields; 2-5 years of work experience preferred, but we will consider strong candidates with less experience.Proven programming skills: We use Python along with shell scripts, R, JavaScript, HTML, and other programming languages as needed. Links to your publicly accessible programming work is a big plus (like GitHub, Bitbucket, or Gitlab).Extensive experience working with large relational and non-relational databases (SQL and NoSQL).The ability to think strategically about data architecture and build data pipelinesExperience with data handling, ETL, and schema designExperience building new solutions using Docker and the Kubernetes container orchestrationExtensive experiences with cloud computing platforms including AWS and GCPstrong analytical, oral, and writing skills', 'strong analytical, oral, and writing skills', '\xa0The Role', '\xa0Required Qualifications', '·\xa0\xa0\xa0\xa0\xa0\xa0Making an impact on the world with every line of code', 'Proven programming skills: We use Python along with shell scripts, R, JavaScript, HTML, and other programming languages as needed. Links to your publicly accessible programming work is a big plus (like GitHub, Bitbucket, or Gitlab).', '·\xa0\xa0\xa0\xa0\xa0\xa0An opportunity to learn about the applications of advanced artificial intelligence algorithms to drug discovery and development from the beginning', 'The company', 'Extensive experiences with cloud computing platforms including AWS and GCP', 'BS, MS, or Ph.D. degree in Computer Science or related fields; 2-5 years of work experience preferred, but we will consider strong candidates with less experience.', '·\xa0\xa0\xa0\xa0\xa0\xa0Involvement in critical partnerships projects with pharmaceutical clients', 'Experience with data handling, ETL, and schema design', '·\xa0\xa0\xa0\xa0\xa0\xa0Competitive compensation, health insurance, as well as share options', 'Extensive experience working with large relational and non-relational databases (SQL and NoSQL).']",Mid-Senior level,Full-time,Information Technology,Biotechnology,2021-03-24 13:05:10
Data Engineer,"PF1 Professional Services, Inc.","Tampa, FL",5 days ago,48 applicants,"['', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Highly motivated, comfortable working independently ', 'Personal Characteristics:', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Strong work ethic with a demonstrated history of outworking peers', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Works collaboratively and energetically', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Positive attitude, team player ', 'o\xa0\xa0Work with Business Intelligence and Data Scientists to strive for greater functionality in our data systems.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Excellent time management and organizational skills', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0An undying commitment to personal growth and development', '  o\xa0\xa0Create artifacts to document Data Pipeline and Data Lake models, solutions, and implementations.', 'Responsibilities:', 'o\xa0\xa0Establish validation checks to ensure data quality and accuracy as information is moved through the data pipeline.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience ingesting data from APIs', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Persistent, pro-active, self-motivated', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa05+ years of experience in a role primarily focused on Data Engineering', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Stellar time management and organizational skills', 'Experience:', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience working with a variety of data formats (Flat Files, XML, JSON, etc.)', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Master’s degree in computer science, information technology, mathematics, or a related field with a track record of strong academic and extracurricular performance', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Strong ability to communicate technical findings into business language', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Interested in a long-term career at a company that is building something to last for decades', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Enterprise Data Expert', 'Job Description – Data Engineer', '\xa0', 'o\xa0\xa0Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Resourceful – figures out how to achieve results creatively with finite resources', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Extreme formatting, organization and priority setting skills', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Enthusiastic, personable, positive attitude, and high integrity', 'o\xa0\xa0Continuously strive to identify pipeline performance optimization opportunities.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Advanced SQL Knowledge', 'o\xa0\xa0Design and develop scalable data processes and architecture to ingest business application data into Enterprise Data Lake.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Self-aware, accepting, honest, open, and respectful of others', 'The day-to-day responsibilities of this Business Intelligence position will consist of:', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Thrives in a nimble, lean, unstructured, fast-paced entrepreneurial environment', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience with Snowflake a plus', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Excellent interpersonal skills, attention to detail and teamwork', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Impeccable integrity and ethical standards', ""Company's success is predicated on the ability to generate and communicate actionable operational insights that help leaders of our family of companies to make the right decisions to grow their business. Therefore, company is looking for a candidate that wants to be in the role long-term and will offer a significant amount of upward mobility as well as attractive compensation for a successful professional. The position will offer a long-term career path at company and have direct access to world-class training and mentorship from company's senior management."", ""The Data Engineer will be a critical and high-profile early employee, participating in building a leading, highly scalable, best-in-class data architecture. Company is seeking a successful data engineering professional with an entrepreneurial mindset to help grow the analytic insight operations at the family of company's partner companies.\xa0As a high-profile member of the team, this position will expose you to being part of an organization building an industry leading insights operation from ground up. Successful candidates will have a meaningful impact on the value creation of a growth-minded company grounded in honesty, caring, and hard-work."", 'Mission of the Role:', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Strong attention to detail', 'o\xa0\xa0Provide support for technical issues related to Data Pipeline and Data Lake infrastructure', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience building and optimizing data pipelines, architectures, and data sets', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience with Microsoft Azure Services and Solutions', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Excellent communicator – written and oral ', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Data Pipeline & Data Lake', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0High energy, ownership of work product and dedication to driving results ', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Analytical – finds meaning and action in numbers, and is a data-driven decision-maker', 'Candidates should be excited about a role where they have an opportunity to build out an enterprise analytics platform. Candidates must also be professional and strong in communication as they will be working closely with non-technical operational partners.\xa0', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Strong interest in analytics and technology']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Westinghouse Electric Company,"Cranberry, PA",2 weeks ago,Be among the first 25 applicants,"['', ""  Bachelor's or Master's degree in Computer Science or related technical field or equivalent professional experience   Greater than 3 years of experience   Experience with Azure products; Data Factory or Databricks is required  "", 'Job Responsibilities', ' Assist with creation of data schemas, stored procedures, data pipelines, and views ', ' Teamwork and Accountability ', "" Bachelor's or Master's degree in Computer Science or related technical field or equivalent professional experience "", ' Collaborate with data analysts, data scientists, and stakeholders during design discussions to uncover more detailed business requirements related to data engineering ', ' Comprehensive Health, Wellness and Income Protection Benefits ', ' Collaborate across roles to embrace best practices in reporting and analysis, including data integrity, test design, validation, and documentation ', '  Safety and Quality   Integrity and Trust   Customer Focus and Innovatoin   Speed and Passion to Win   Teamwork and Accountability  ', 'We Enable Our Delivery Of This Vision By Living Our Value System', 'This role is remote eligible ', ' Ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy ', ' Experience with Azure products; Data Factory or Databricks is required ', ' Speed and Passion to Win ', ' Greater than 3 years of experience ', 'Why Westinghouse?', 'Posting Title', ' Educational Reimbursement Program ', ' Build and automate actionable reports ', ' Safety and Quality ', 'The Following Are Representative Of What We Offer', ' Employee Referral Program ', ' Develop strong hypotheses, independently solve problems, and share actionable insights with engineering ', ' Competitive Salary ', 'Data Engineer ', ' Integrity and Trust ', 'Your Day-to-Day', ' Customer Focus and Innovatoin ', 'Get Connected With Westinghouse On Social Media', '  Competitive Salary   Comprehensive Health, Wellness and Income Protection Benefits   401(k) Savings Plan with Company Match   Paid Vacations and Holidays   Opportunities for Flexible Work Arrangements   Educational Reimbursement Program   Employee Referral Program  ', ' Partner and develop strong relationships with cross-functional teams ', ' Paid Vacations and Holidays ', ' Help build and maintain technical solutions required for optimal ingestion, transformation, and loading of data from a wide variety of data sources and large, complex data sets ', ' 401(k) Savings Plan with Company Match ', ' Opportunities for Flexible Work Arrangements ', 'Who You Are', '  Assist with creation of data schemas, stored procedures, data pipelines, and views   Help build and maintain technical solutions required for optimal ingestion, transformation, and loading of data from a wide variety of data sources and large, complex data sets   Collaborate across roles to embrace best practices in reporting and analysis, including data integrity, test design, validation, and documentation   Ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy   Build and automate actionable reports   Collaborate with data analysts, data scientists, and stakeholders during design discussions to uncover more detailed business requirements related to data engineering   Develop strong hypotheses, independently solve problems, and share actionable insights with engineering   Partner and develop strong relationships with cross-functional teams  ']",Entry level,Full-time,Information Technology,Construction,2021-03-24 13:05:10
Senior Data Engineer,Smarsh,"Portland, Oregon Metropolitan Area",20 hours ago,Be among the first 25 applicants,"['', 'Design and code data pipeline features and data processing jobs for collecting automotive data from disparate systems for the purpose of storing in a data warehouse', 'Write ETL pipelines to implement pre-defined business rules and metrics along with ensuring data quality in data warehouse processes', 'Skills / Qualifications', 'Has strong interpersonal communication skills; effectively communicates in verbal and written form', 'Design robust data warehouse management functions covering resource management, security, backup and recovery, performance monitoring and tuning and high availability', 'Ensure smooth ongoing operations of data warehouse platform with high availability while making continuous improvements.', 'Proven ability to work within set timelines and update management on deviation from these estimates', 'A degree in math, statistics, computer science, or equivalent technical field5+ years of experience with data engineering with emphasis on data analytics and reportingExpert experience with SQL and Relational database engineering (Oracle, SQL Server, Teradata)—expert-level SQL abilitiesDemonstrated experience working in large-scale data environments which included real-time and batch processing requirementsExperience working with source-code management tools such as GitHub and JenkinsAbility to partner with business and technology team members, to understand business requirements and translate those into value-add technology solutionsExperience designing and delivering large scale, 24-7, mission-critical data pipelines and features using modern big data architecturesA sound understanding of BI best practices, relational structures, dimensional data modeling, structured query language (SQL) skills, data warehouse and reporting techniquesProven ability to work within set timelines and update management on deviation from these estimatesHas strong interpersonal communication skills; effectively communicates in verbal and written formYou appreciate agile software processes, data-driven development, reliability, and responsible experimentation.', 'Upkeep standard development methodologies of best practices for collaborative coding, modular efficiency, version control, QA, and release management', '5+ years of experience with data engineering with emphasis on data analytics and reporting', 'Experience developing solutions in SnowflakeKnowledge of building solutions with data visualization and reporting tools such as Looker, PowerBI, Tableau, etc.Experience integrating data from multiple data sources and file types such as JSON and AVRO formats.', 'Design and standup up a robust data warehouse to support to support reporting & analytics for internal stakeholders and clients.', 'Work on integrated teams of product management, architects, and data engineers to ensure we’re building the best products possible', 'Experience developing solutions in Snowflake', 'Knowledge of building solutions with data visualization and reporting tools such as Looker, PowerBI, Tableau, etc.', 'Experience designing and delivering large scale, 24-7, mission-critical data pipelines and features using modern big data architectures', 'Expert experience with SQL and Relational database engineering (Oracle, SQL Server, Teradata)—expert-level SQL abilities', 'Collaborate with business units to automate reporting needs', 'Experience integrating data from multiple data sources and file types such as JSON and AVRO formats.', 'Ability to partner with business and technology team members, to understand business requirements and translate those into value-add technology solutions', '\xa0', 'Smarsh is seeking a Senior data engineer to join our Professional Archive team to lead the design on building a data warehouse to grow our reporting & analytics capabilities. You will partner closely with teams across the company to gather business requirements. You are a self-starter, possess a strong commitment to detail and can work in a rapidly changing environment. You will build working relationships to communicate within and across teams to solve problems. The ability to proactively suggest and implement report development and enhancements from understanding business processes, goals, and benchmarks will be invaluable to the success of this role.', 'Responsibilities', 'Work with real-time data streams from multiple internal/external sources', 'A sound understanding of BI best practices, relational structures, dimensional data modeling, structured query language (SQL) skills, data warehouse and reporting techniques', 'Participate in peer code reviews and produce high quality documentation', 'Design and standup up a robust data warehouse to support to support reporting & analytics for internal stakeholders and clients.Design and code data pipeline features and data processing jobs for collecting automotive data from disparate systems for the purpose of storing in a data warehouseCreate and maintain optimal data pipeline architecture.Work with real-time data streams from multiple internal/external sourcesWrite ETL pipelines to implement pre-defined business rules and metrics along with ensuring data quality in data warehouse processesEnsure smooth ongoing operations of data warehouse platform with high availability while making continuous improvements.Provide scalable solutions to manage large imports and implement operational procedures as necessaryDesign robust data warehouse management functions covering resource management, security, backup and recovery, performance monitoring and tuning and high availabilityParticipate in peer code reviews and produce high quality documentationWork on integrated teams of product management, architects, and data engineers to ensure we’re building the best products possibleCollaborate with business units to automate reporting needsUpkeep standard development methodologies of best practices for collaborative coding, modular efficiency, version control, QA, and release management', 'Provide scalable solutions to manage large imports and implement operational procedures as necessary', 'Demonstrated experience working in large-scale data environments which included real-time and batch processing requirements', 'A degree in math, statistics, computer science, or equivalent technical field', 'Create and maintain optimal data pipeline architecture.', 'Experience working with source-code management tools such as GitHub and Jenkins', 'Job Description', 'Additional Preferences Are', 'You appreciate agile software processes, data-driven development, reliability, and responsible experimentation.']",Mid-Senior level,Full-time,Engineering,Computer Software,2021-03-24 13:05:10
Data Engineer,Borrego,"San Diego, CA",1 week ago,26 applicants,"['Competitive base salary', 'Potential for bonuses', 'Develop, enforce, and recommend enhancements to Applications in the area of standards, methodologies, compliance, and quality assurance practices; participate in design and code walkthroughs', 'A successful history of manipulating, processing and extracting value from large disconnected datasets', 'EEO M/F/D/V', 'Working with a system at scale and with Docker/Kubernetes/Jenkins CI/CD pipeline', '401(k) plan with company match and immediate vesting', 'Continuing education and professional development assistance', 'Borrego is a national leader in solar & energy storage, with strong and steady leadership and an outstanding culture of teamwork, integrity, and a commitment to continuous improvement. Unique in our industry, Borrego has steadily grown and remained profitable every year over the last decade &mdash; and we’re continuing to grow across all areas of our company.', 'Strong analytic skills related to working with unstructured datasets and big query expertise', 'OUR MISSION', 'Comprehensive benefits package including dental, vision, health, life, and disability insurance', 'Work experience with Tableau', 'Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field', 'Keep up to date on relevant technologies and frameworks', 'Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement', 'Work with various Google Cloud Platform (GCP) technologies, Metadata Management tool, Data Quality (DQ) tool, Cloud-Native Microservice architecture, CI/CD, Dev/Ops', 'REQUIRED SKILLS AND EXPERIENCE:', 'Borrego provides equal employment opportunities to all employees and applicants without regard to race, color, religion, gender, age, sexual orientation, national origin, ancestry, disability, genetics, veteran status or any other characteristic protected by state, federal and local laws. In addition to federal law requirements, Borrego complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities.', 'Utilize technical and domain knowledge to develop and implement effective solutions; provide hands on mentoring to team members through all phases of the Systems Development Life Cycle (SDLC) using Agile practices', 'POSITION RESPONSIBILITIES WILL INCLUDE, BUT ARE NOT LIMITED TO:', 'Work with systems that handle sensitive data with strict SOX controls and change management processes', 'COMPENSATION', 'Build ad-hoc applications as needed, to support more curious data users and to provide automation as possible', 'Build processes supporting data transformation, data structures, metadata, dependency and workload management', 'ABOUT BORREGO', '\xa0', '5+ years of experience building and optimizing google BigQuery data pipelines, architectures and data sets', 'Work experience with TableauExperience with data science tools such as Pandas, Numpy, RWorking with a system at scale and with Docker/Kubernetes/Jenkins CI/CD pipeline', 'Be a Part of Our Mission. Join the GigaWatt Revolution', 'DESIRED SKILLS:', 'Borrego is seeking a\xa0Data Engineer to join our growing Data & Analytics team!\xa0 We are building an analytics platform for business and operation intelligence. At Borrego, we want to help business users to make data-driven decisions. Our platform will allow businesses to answer ""what"", ""when"" and ""how"" questions, as well as allow them to ask ""what if"". In this role, you will design, develop, maintain and support our Enterprise Data Warehouse & BI platform within Borrego, using various data & BI tools.\xa0 This position offers a unique opportunity to make a significant impact on the entire organization in developing data tools and driving a data-driven culture.\xa0\xa0The Data Engineer will report to the Director of Applications Development.', ""Borrego's approach to compensating our employees is unique and progressive. We offer a casual community-based workplace that is upbeat and hard working. We strive for quality workmanship and place a large emphasis on customer satisfaction. We promote from within and are looking for career minded individuals, looking to advance in the growing photovoltaic industry. We offer pay commensurate with experience along with excellent benefits including:"", 'Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases', 'Experience with Firestore with focus on performance', 'Experience with object-oriented/object function scripting languages: Python, Java, Scala, etc.', 'Develop collaborative relationships and work with key business owners, IT resources to gather requirements and for the efficient resolution of requests to help them observe patterns and spot anomalies', 'Experience with data science tools such as Pandas, Numpy, R', 'Communicate technical and business topics, as appropriate, in a 360 degree fashion, when required; communicate using written, verbal and/or presentation materials as necessary', 'This is a full-time, permanent position.', 'Work in a time-constrained environment to analyze, design, develop, and deliver Enterprise Data Warehouse solutionsCreate ETL/ELT pipelines using Python, AirflowWork with various Google Cloud Platform (GCP) technologies, Metadata Management tool, Data Quality (DQ) tool, Cloud-Native Microservice architecture, CI/CD, Dev/OpsBuild ad-hoc applications as needed, to support more curious data users and to provide automation as possibleWork with systems that handle sensitive data with strict SOX controls and change management processesDevelop collaborative relationships and work with key business owners, IT resources to gather requirements and for the efficient resolution of requests to help them observe patterns and spot anomaliesCommunicate technical and business topics, as appropriate, in a 360 degree fashion, when required; communicate using written, verbal and/or presentation materials as necessaryDevelop, enforce, and recommend enhancements to Applications in the area of standards, methodologies, compliance, and quality assurance practices; participate in design and code walkthroughsUtilize technical and domain knowledge to develop and implement effective solutions; provide hands on mentoring to team members through all phases of the Systems Development Life Cycle (SDLC) using Agile practicesTake ownership of deployment and release processKeep up to date on relevant technologies and frameworks', 'Experience supporting and working with cross-functional teams in a dynamic environment', '5+ years building data catalog, data dictionary, and make data searchable for our stakeholders and users', 'Work in a time-constrained environment to analyze, design, develop, and deliver Enterprise Data Warehouse solutions', 'Data Engineer', 'Our mission is “to solve the world’s energy problems by accelerating the adoption of renewable energy.” To date we have developed and built more than 640 Megawatts of solar while maintaining a reputation for quality, deep staff expertise, and a focus on building long-term relationships based on trust and transparency. But being the largest private commercial solar and energy storage company in the United States is not enough. Solving the world’s energy problems means moving from Megawatts to Gigawatts, and that is what we are going to do. Our 5-year goal is to be the leading solar and energy storage provider in the United States.\xa0Be a Part of Our Mission. Join the GigaWatt Revolution!', 'Take ownership of deployment and release process', 'Self-managed flexible work schedules and time-off policies', 'Competitive base salaryPotential for bonusesComprehensive benefits package including dental, vision, health, life, and disability insuranceSelf-managed flexible work schedules and time-off policies401(k) plan with company match and immediate vestingContinuing education and professional development assistance', 'Create ETL/ELT pipelines using Python, Airflow', 'Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databasesExperience with object-oriented/object function scripting languages: Python, Java, Scala, etc.5+ years building data catalog, data dictionary, and make data searchable for our stakeholders and users5+ years of experience building and optimizing google BigQuery data pipelines, architectures and data setsExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvementStrong analytic skills related to working with unstructured datasets and big query expertiseExperience with Firestore with focus on performanceBuild processes supporting data transformation, data structures, metadata, dependency and workload managementA successful history of manipulating, processing and extracting value from large disconnected datasetsExperience supporting and working with cross-functional teams in a dynamic environmentGraduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field']",Mid-Senior level,Full-time,Information Technology,Renewables & Environment,2021-03-24 13:05:10
Data/Analytics Engineer,SmartAsset,"New York, NY",4 weeks ago,Be among the first 25 applicants,"['', '2+ years of development experience with exposure to Data Science', 'PhD/MSc', 'Experience with AWS services/EC2 and cloud storage', 'Bachelor’s Degree in a hard science, mathematics, or engineering', ' Bachelor’s Degree in a hard science, mathematics, or engineering 2+ years of development experience with exposure to Data Science Solid Python and SQL skills Good knowledge of pandas with hands on data wrangling experience Comfortable in the Linux environment Solid math skills - Probability/statistics/calculus ', 'Preferred Skills And Experience', ' PhD/MSc ETL/data pipelining experience Experience with AWS services/EC2 and cloud storage Data Science experience', 'Good knowledge of pandas with hands on data wrangling experience', 'Data Science experience', 'Required Skills And Experience', 'Data/Analytics Engineer', 'Solid Python and SQL skills', 'Comfortable in the Linux environment', 'ETL/data pipelining experience', 'Solid math skills - Probability/statistics/calculus', 'About You']",Associate,Full-time,Analyst,Internet,2021-03-24 13:05:10
"Data Engineer - Remote, SF, or DEN",Quizlet,"San Francisco, CA",4 weeks ago,Over 200 applicants,"['', 'Do all of this in a way that internal customers find approachable and easy to use', 'Meet with Potential Colleagues and Leadership', ""Create and maintain libraries, APIs, and other shared abstractions used by Quizlet's Data Science and Analytics teams or delivered as microservices."", 'We provide a monthly in home office stipend to employees while our teams are working remote for the COVID pandemic', 'We strive to make everyone feel comfortable and welcome! ', 'Experience with any / all of:Horizontal sharding solutions for relational data storesMySQL (Cloud SQL), Cloud Spanner, Memcache, RedisContainers, Kubernetes, Terraform, Helm, and supporting technologies running on Google Cloud PlatformVisualization tools like DataDog that connect metrics and eventsPython, TypeScript, GoLang, Kotlin/JavaAVRO, DBTApache Airflow, RabbitMQ, Apache Kafka', 'Quizlet will not pay fees to any third-party agency or firm nor will it be responsible for any agency fees associated with unsolicited resumes.', 'Preferred Qualifications', 'MySQL (Cloud SQL), Cloud Spanner, Memcache, Redis', ""Help scale our application data layer so it can handle > 100k transactions per secondBuild new streaming data infrastructure to feed machine learning and analyticsEnsure reliability of the systems that move and transform Quizlet’s data for use in analytics and the productCreate and maintain libraries, APIs, and other shared abstractions used by Quizlet's Data Science and Analytics teams or delivered as microservices.Do all of this in a way that internal customers find approachable and easy to use"", 'Meet with Hiring Manager', 'Containers, Kubernetes, Terraform, Helm, and supporting technologies running on Google Cloud Platform', 'Required Qualifications', 'A reliable teammate with a strong sense of ownership.', 'Empathy for the wide array of people who rely on Quizlet data platforms (analytics, product, data science… everybody, really)', 'Next Steps', 'Meet with Recruiter', 'Benefits And Perks', 'AVRO, DBT', 'Python, TypeScript, GoLang, Kotlin/Java', 'We’ll provide you with a laptop, top-notch benefits available to all fulltime Quizleters', 'To All Recruiters And Placement Agencies', 'Experience with any / all of:', 'Visualization tools like DataDog that connect metrics and events', 'Systematic problem-solving approach, coupled with good communication skills and a strong sense of ownership.', 'In Closing', 'Ensure reliability of the systems that move and transform Quizlet’s data for use in analytics and the product', 'Collaborate with your manager and team to create a schedule that ensures a high level of productivity (creating that ideal work/life balance)', 'Apache Airflow, RabbitMQ, Apache Kafka', 'Systematic problem-solving approach, coupled with good communication skills and a strong sense of ownership.Empathy for the wide array of people who rely on Quizlet data platforms (analytics, product, data science… everybody, really)A reliable teammate with a strong sense of ownership.', 'The Role', 'Help scale our application data layer so it can handle > 100k transactions per second', 'About Quizlet', 'Horizontal sharding solutions for relational data stores', 'Build new streaming data infrastructure to feed machine learning and analytics', 'We offer 20 days of paid vacation (and we expect you to take them)']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data/ETL Engineer,OutworX Corporation,"Parsippany, NJ",2 days ago,Be among the first 25 applicants,"['', 'Required Experience And Qualifications']",Entry level,Full-time,Business Development,Information Technology and Services,2021-03-24 13:05:10
Data Analytics Engineer - Tableau,"ITCO Solutions, Inc.","Menlo Park, CA",2 days ago,83 applicants,"['', ' Develop data products and analytics to empower operational and exploratory analyses Develop deep understanding of people data model – write queries and manage data transformation to enable analytics and reporting Partner with Data Engineering to develop core data sets that empower operational and exploratory analyses Help analyze, visualize, and provide analytics on data to build reporting solutions to support various company initiatives Build rich and dynamic dashboards using out-of-box features, customizations, and visualizations using d3, Angular.js or equivalent ', 'Build rich and dynamic dashboards using out-of-box features, customizations, and visualizations using d3, Angular.js or equivalent', 'At least 3 years of business intelligence and data warehouse experience', 'Demonstrated experience creating dashboards in d3.js, Angular.js or a similar visualization tool', 'Demonstrated experience in utilizing R, Python, SPSS or comparable to develop analyses', 'Develop data products and analytics to empower operational and exploratory analyses', 'Help analyze, visualize, and provide analytics on data to build reporting solutions to support various company initiatives', 'Minimum Qualifications', 'Partner with Data Engineering to develop core data sets that empower operational and exploratory analyses', 'Develop deep understanding of people data model – write queries and manage data transformation to enable analytics and reporting', 'Must have Tableau and SQL', ""Bachelor's in Computer Science, Statistics, Math, Engineering, Business, or other quantitative field"", 'Job Description', 'Minimum of 2 years experience wrangling production datasets in relational databases (SQL/PLSQL/Hive)', 'Ability to clearly and effectively communicate the results of complex analyses', "" Must have Tableau and SQL Ability to clearly and effectively communicate the results of complex analyses At least 3 years of business intelligence and data warehouse experience Demonstrated experience creating dashboards in d3.js, Angular.js or a similar visualization tool Demonstrated experience in utilizing R, Python, SPSS or comparable to develop analyses Bachelor's in Computer Science, Statistics, Math, Engineering, Business, or other quantitative field Minimum of 2 years experience wrangling production datasets in relational databases (SQL/PLSQL/Hive)""]",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Sheetz,"Pittsburgh, PA",1 week ago,Be among the first 25 applicants,"['', 'Master’s degree in Computer Science, MIS or Computer Engineering preferred.', ""All team members are working remote due to the\xa0pandemic. Keeping\xa0our employees safe is most important to us. After we transition back to office work, when it's safe, we do have the option for partial remote work arrangements for candidates living within our 6 state footprint."", 'PERKZ', 'Minimum 5 years development experience in at least one object-oriented language (Python, Perl, Java, etc.) required', 'Building, integrating and supporting the cutting-edge applications that power 600+ stores, 22,000 employees and millions of customers isn’t an easy job, but that’s why we do it!\xa0And we do it in an award-winning culture that’s casual and fun. If you’re looking for a Made to Order career, hit that APPLY button and let’s chat!', 'Minimum 5 years working in a Data Engineer role requiredMinimum 5 years working with large databases and data warehouses utilizing both relational and non-relational data models requiredEnd to end experience in the analytic lifecycle from structured/unstructured raw data, data wrangling, creating data pipelines, to self-service dashboards leveraged by the business requiredAdvanced knowledge of SQL requiredMinimum 5 years development experience in at least one object-oriented language (Python, Perl, Java, etc.) requiredProficiency in data visualization tool (Tableau) preferredStrong understanding of cloud computing database technologies (Azure, AWS, GCP) requiredExperience building and optimizing “Big Data” pipelines, architectures and data sets requiredExperienced with data wrangling and preparation for use within data science, business intelligence or similar analytical functions require', 'Establish standardization and educate data users on query best practices allowing for re-use and analytic efficiency.', 'Employee Profit Share', '\xa0Co-lead in the selection and build of our data and analytic tools including Enterprise Data Warehouse, Data Lake, Analytics platform and Data Catalogue.', ""HOW YOU'LL GROW"", 'We have all the perks you would expect from a leading employer, such as medical, dental & vision, in addition to that we have:', 'College Tuition Reimbursement', ""EXPERIENCE YOU'LL BRING"", 'Employee Stock OwnershipCollege Tuition Reimbursement401(K) Retirement Savings Plan – a 4% match!Employee Profit ShareVIP Sheetz Membership (Discounts!)', 'Education', 'Sheetz, Inc. is a fast-growing, family-owned, food/convenience company that has been in business since 1952. Sheetz has over 600 locations in Pennsylvania, Ohio, Virginia, West Virginia, Maryland and North Carolina.', 'End to end experience in the analytic lifecycle from structured/unstructured raw data, data wrangling, creating data pipelines, to self-service dashboards leveraged by the business required', 'Experienced with data wrangling and preparation for use within data science, business intelligence or similar analytical functions require', 'Responsible for developing complex, large scale data models and pipelines that organize and standardize the data to make it readily accessible and consumable by the business for reporting and data science needs. Collaborate with various areas of the Business in order to determine and source the appropriate data through internal and external means. Investigate new and existing technologies and data sources and assess their viability within the Sheetz environment.', 'Experience', '\xa0', 'Minimum 5 years working with large databases and data warehouses utilizing both relational and non-relational data models required', 'Manage the democratization of data knowledge across the organization through the communication and maintenance of master data, metadata, data management repositories, data models, and data standards', '\xa0Co-lead in the selection and build of our data and analytic tools including Enterprise Data Warehouse, Data Lake, Analytics platform and Data Catalogue.Design, implement, manage data architecture and data pipeline across multiple data sources.Collaborate closely with Business and Technical Owners in identifying data sources and aggregating structured or unstructured data into dimensional data models able to be consumed by the business.Identify potential process improvements and designs and implement automated solutions.Manage the democratization of data knowledge across the organization through the communication and maintenance of master data, metadata, data management repositories, data models, and data standardsEstablish standardization and educate data users on query best practices allowing for re-use and analytic efficiency.', 'Advanced knowledge of SQL required', 'Strong understanding of cloud computing database technologies (Azure, AWS, GCP) required', ""Our mission at Sheetz has been to meet the needs of customers on the go. Of course, things have changed over those nearly 70 years. Life is faster and busier, and customers expect us to be there when they need us most. One thing that hasn't changed is our commitment to our customers, our employees and the communities in which we operate. Sheetz donates millions of dollars every year to the charities it holds dear."", 'Bachelor degree in Computer Science, Management Information Systems, Computer Engineering or a related field required.Master’s degree in Computer Science, MIS or Computer Engineering preferred.', 'Identify potential process improvements and designs and implement automated solutions.', 'Design, implement, manage data architecture and data pipeline across multiple data sources.', ""WHAT YOU'LL DO"", 'Proficiency in data visualization tool (Tableau) preferred', 'Experience building and optimizing “Big Data” pipelines, architectures and data sets required', 'VIP Sheetz Membership (Discounts!)', 'We are an innovator in our space and we want to make sure you can keep doing that. We are happy to send you to conferences and continuing education to make sure you continue to be the expert in your field.', 'always', 'Collaborate closely with Business and Technical Owners in identifying data sources and aggregating structured or unstructured data into dimensional data models able to be consumed by the business.', '401(K) Retirement Savings Plan – a 4% match!', 'We also provide employee training programs to continue your growth in your field while here. We believe there’s always room to learn and we offer opportunities to gain hands on experience. Our employees are\xa0always\xa0encouraged to explore new opportunities, especially within our organization.', 'Bachelor degree in Computer Science, Management Information Systems, Computer Engineering or a related field required.', 'Minimum 5 years working in a Data Engineer role required', 'We sell hot dogs, beer, gas, and a bunch of other things, but we’re really in the business of “convenience” and that should resonate with you.\xa0At Sheetz, we have a history of making our customers’ lives easier. We pioneered self-service touch screens for ordering food. We rolled out our industry leading Sheetz mobile app long before curbside pickup was around. Although we’re not a tech company, technology has been at the forefront of our success. And who makes that happen? You do!\xa0', 'Employee Stock Ownership', 'ABOUT SHEETZ']",Mid-Senior level,Full-time,Information Technology,Retail,2021-03-24 13:05:10
Data Engineer,HTC Global Services,"Bloomington, IL",3 days ago,52 applicants,"['', 'Candidates should e-mail their resume to nirmal.raj@htcinc.com and call me at 248-308-1527', 'HTC*s competitive package includes besides compensation Health, Dental, Vision, Disability Cover, both Short and Long term, Life Insurance, Flexible Spending, 401k and Paid Vacation', 'Technical background can include but not be limited to AWS Services, Python, Bigdata, Spark.Experience in creating Lambda Functions, knowledge in Limitations of Lambda Functions and BOTO3 library for Lambda.Experience in Coding for serverless applications, Implementing application design to application code.Specific experiences on the following AWS tools are beneficial: Kinesis, Athena, EMR, Glue, Redshift, Dynamo DB, and QuickSight.Experience in S3, EC2, and serverless computing on AWS.Good background in Python and Pyspark is required.Knowledge in AWS QuickSight is a great plus.Knowledge in Data Pipeline.', 'Good background in Python and Pyspark is required.', 'Benefits:', 'Project Details:\xa0', 'Knowledge in Data Pipeline.', 'We are currently looking for a Senior Data Engineer with PySpark.\xa0We will consider relocation for candidates with exceptional skills.', 'Knowledge in AWS QuickSight is a great plus.', 'Project Details:', 'Experience in Coding for serverless applications, Implementing application design to application code.', 'EEO/M/F/V/H', 'Senior Data Engineer with PySpark', 'Technical background can include but not be limited to AWS Services, Python, Bigdata, Spark.', 'About the Job', 'Be sure to reference the job number and title in the subject line. A relevant degree or its foreign equivalent is required.', 'Benefits', 'Experience in creating Lambda Functions, knowledge in Limitations of Lambda Functions and BOTO3 library for Lambda.', 'Experience in S3, EC2, and serverless computing on AWS.', 'Specific experiences on the following AWS tools are beneficial: Kinesis, Athena, EMR, Glue, Redshift, Dynamo DB, and QuickSight.']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer Tech Lead (open to remote),Mission Lane,"Richmond, VA",4 weeks ago,75 applicants,"['', '5+ years experience working with ETL tools such as Matillion, Informatica, Talend, Alooma, or comparable experience with Python, Java, Scala, Hadoop or Spark', 'Provide in architecting data solutions that will capture, manage, process and serve small to large data at scale', 'Demonstrated track record of scaling data warehouse solution, meeting SLAs, and expertise in performance analysis', 'Author, review and approve data requirements and designs of the data warehousing and data engineering domain including ETL/data movement and pipelines, business intelligence, and analytics', 'fair and clear ', 'Assist in defining and implementing standards and best practices within the data engineering domain that are communicated and leveraged by all data team members and other users of the platform and tools. Standards and best practices include datasets, tables, columns, scripts, variables, etc.', '5+ years of experience with SQL and databases such as Snowflake, Redshift, Postgres', '5+ years of experience as a Data Engineer including building ETL/data pipelines as well as data visualization and BI solutions', 'Demonstrated strategic thinking and ability to anticipate the downstream costs of critical engineering decisions', '5+ years experience with database architecture, design and modeling and working with a variety of data warehousing systems', ' Author, review and approve data requirements and designs of the data warehousing and data engineering domain including ETL/data movement and pipelines, business intelligence, and analytics Build data pipelines leveraging internal and external data sources to meet business objectives Provide in architecting data solutions that will capture, manage, process and serve small to large data at scale Identify and develop governance controls for systems within the data warehousing and data engineering domains and encourage the team to develop controls as well Assist in defining and implementing standards and best practices within the data engineering domain that are communicated and leveraged by all data team members and other users of the platform and tools. Standards and best practices include datasets, tables, columns, scripts, variables, etc. Assist in the definition of data management processes related to data governance, stewardship, data quality, training, data retention, etc. Define how systems should be maintained, including tools to be used for monitoring, thresholds, and runbooks and actively work on refining on-call practices Ensure that testing is performed and documented to ensure the quality of the work being delivered Work directly with Product Owners and end-users to develop solutions in a highly collaborative and agile environment Work with business units to understand business objectives and how to leverage our data platform and tools to achieve those objectives ', '5+ years of experience with Business Intelligence and Visualization tools such as Chartio, Tableau, etc.', '5+ years of experience working in a fast-paced environment; continuous deployment, test-driven development, agile methodologies', '. ', ""Mission Laners are learners and here's what we know for certain:"", '5+ years experience building consumer-facing products', ':', 'Are you purpose-driven, performance-oriented, and principles-led?', 'As The Principal Data Engineer (Tech Lead), You Will', 'Identify and develop governance controls for systems within the data warehousing and data engineering domains and encourage the team to develop controls as well', '5+ years experience building robust, highly available, and scalable services', 'Bachelor’s degree in Computer Science, Computer Engineering or related experience', 'Quickly', 'Work directly with Product Owners and end-users to develop solutions in a highly collaborative and agile environment', 'What does this mean exactly?', 'To set you up for success in this role from day one, Mission Lane is looking for candidates who have', 'Build data pipelines leveraging internal and external data sources to meet business objectives', 'fee harvesters ', 'Mission Lane is not currently sponsoring new applicant employment authorization for this position. And please, no third party recruiters.', '5 + years of experience working with automated build and continuous integration systems', ' 5+ years experience building consumer-facing products 5+ years of experience with SQL and databases such as Snowflake, Redshift, Postgres 5+ years experience working with ETL tools such as Matillion, Informatica, Talend, Alooma, or comparable experience with Python, Java, Scala, Hadoop or Spark 5+ years of experience with Business Intelligence and Visualization tools such as Chartio, Tableau, etc. 5 + years of experience working with automated build and continuous integration systems 5+ years experience with database architecture, design and modeling and working with a variety of data warehousing systems ', 'Define how systems should be maintained, including tools to be used for monitoring, thresholds, and runbooks and actively work on refining on-call practices', 'Enter Mission Lane', 'Mission Lane is an Equal Opportunity Employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex, race, color, age, national origin, religion, physical and mental disability, genetic information, marital status, sexual orientation, gender identity/assignment, citizenship, pregnancy or maternity, protected veteran status, or any other status prohibited by applicable national, federal, state or local law.Mission Lane is not currently sponsoring new applicant employment authorization for this position. And please, no third party recruiters.', '5+ years experience using SQL and advanced SQL techniques', '5+ years experience building and deploying services in the cloud', 'candidates don’t need good credit to apply.', ' Bachelor’s degree in Computer Science, Computer Engineering or related experience 5+ years of experience as a Data Engineer including building ETL/data pipelines as well as data visualization and BI solutions 5+ years of experience working in a fast-paced environment; continuous deployment, test-driven development, agile methodologies 5+ years experience using SQL and advanced SQL techniques 5+ years experience building robust, highly available, and scalable services 5+ years experience building and deploying services in the cloud Demonstrated track record of scaling data warehouse solution, meeting SLAs, and expertise in performance analysis Demonstrated strategic thinking and ability to anticipate the downstream costs of critical engineering decisions ', 'Assist in the definition of data management processes related to data governance, stewardship, data quality, training, data retention, etc.', ' And we have only just begun.', 'Ensure that testing is performed and documented to ensure the quality of the work being delivered', 'Work with business units to understand business objectives and how to leverage our data platform and tools to achieve those objectives', 'You Get Bonus Points For', 'Mission Lane is an Equal Opportunity Employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex, race, color, age, national origin, religion, physical and mental disability, genetic information, marital status, sexual orientation, gender identity/assignment, citizenship, pregnancy or maternity, protected veteran status, or any other status prohibited by applicable national, federal, state or local law.', 'Are you passionate about making a meaningful impact?']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Cervello,"Addison, TX",3 weeks ago,99 applicants,"['', 'Plan and execute secure, good practice data integration strategies and approachesAcquire, ingest, and process data from multiple sources and systems into Big Data platformsCreate and manage data environments in the CloudCollaborate with our business analysts and data scientists to map data fields to hypotheses and curate, wrangle, and prepare data for use in their advanced analytical modelsHave a strong understanding of Information Security principles to ensure compliant handling and management of client dataThis is a fantastic opportunity to be involved in end-to-end data management for cutting edge Advanced Analytics and Data Science', 'Collaborate with our business analysts and data scientists to map data fields to hypotheses and curate, wrangle, and prepare data for use in their advanced analytical models', 'Experience with NLP, Machine Learning, etc. is a plus', 'Experience or familiarity with real-time ingestion and streaming frameworks is a plus', 'Experience working on lively projects and a consulting setting, often working on different and multiple projects at the same time', 'Experience and desire to work with open source and branded open source frameworks', 'Have a strong understanding of Information Security principles to ensure compliant handling and management of client data', 'Excel in team collaboration and working with others from diverse skill-sets and backgrounds', 'Experience on client-facing projects, including working in close-knit teamsExperience and interest in Big Data technologies (Hadoop / Spark / NoSQL DBs)Experience or familiarity with real-time ingestion and streaming frameworks is a plusExperience and desire to work with open source and branded open source frameworksExperience working on projects within the cloud ideally AWS or AzureExperience with NLP, Machine Learning, etc. is a plusExperience working on lively projects and a consulting setting, often working on different and multiple projects at the same timeStrong development background with experience in at least two scripting, object oriented or functional programming language, etc. SQL, Python, Java, Scala, C#, RData Warehousing experience, building operational ETL data pipelines across a number of sources, and constructing relational and dimensional data modelsExcellent interpersonal skills when interacting with clients in a clear, timely, and professional manner.A deep personal motivation to always produce outstanding work for your clients and colleaguesExcel in team collaboration and working with others from diverse skill-sets and backgrounds', 'Acquire, ingest, and process data from multiple sources and systems into Big Data platforms', 'Plan and execute secure, good practice data integration strategies and approaches', 'ABOUT US: OUR WORKPLACE IS FUN AND FAST-PACED:', 'Summary', 'Qualifications', 'Excellent interpersonal skills when interacting with clients in a clear, timely, and professional manner.', 'This is a fantastic opportunity to be involved in end-to-end data management for cutting edge Advanced Analytics and Data Science', 'Experience and interest in Big Data technologies (Hadoop / Spark / NoSQL DBs)', 'A deep personal motivation to always produce outstanding work for your clients and colleagues', 'Experience working on projects within the cloud ideally AWS or Azure', 'Create and manage data environments in the Cloud', 'Data Warehousing experience, building operational ETL data pipelines across a number of sources, and constructing relational and dimensional data models', 'Equal Employment Opportunity and Nondiscrimination', 'Experience on client-facing projects, including working in close-knit teams', 'Strong development background with experience in at least two scripting, object oriented or functional programming language, etc. SQL, Python, Java, Scala, C#, R']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer - Tableau,Rackspace Technology,"Austin, TX",22 hours ago,Be among the first 25 applicants,"['', 'Excellent technical architecture skills, design and deliver innovative Proof of Concepts for customers.', '8+ Years of experience in Tableau development and Tableau Administration 2+ years of experience in delivering AWS Data Solutions.Experienced in DW/BI, ETL and Data related technologies and toolsExperienced working with any public cloud platform like AWS, GCP or AzureExperience with configuring Tableau with different Identity stores and have knowledge of Integrating with AD for SAML.A solid understanding of SQL, rational databases, and normalization.Proficiency in use of query and reporting analysis tools.DW/BI, ETL and Data related technologies and toolsGood conceptual knowledge and working experience on report/dashboard creation, creating report specifications, integration test planning & testing, unit test planning & testing, UAT & implementation support', 'Tableau certification along with any Cloud certifications such as AWS Solutions Architect, Developer, GCP Professional Data Engineer or Microsoft Data / AI certifications', '2+ years of experience in delivering AWS Data Solutions.', 'Provide support and expertise to the business community to assist with better utilization of Tableau.Understand business requirements, conduct analysis, and recommend solution optionsShould be good with writing SQL queries and Stored proceduresWorking closely with the business, this role will be responsible for designing, building and maintaining Tableau WorkbooksDevelop reports, dashboards, scorecardsGather business requirements, elicit technical requirements, prepare report specificationsManaging Tableau ServerTwisting SQL queries for improving performances', 'Expert in Tableau administration/architecture', '8+ Years of experience in Tableau development and Tableau Administration ', 'Skilled Tableau Developer with experience in implementing best practice in analytics and visualization, promote agile delivery, and help expose data', 'Mentor and train other architects/Engineer within the wider Rackspace community on Tableau and other tools like Looker', 'Drive the engagements with customers from the architectural pillar, from design to delivery, create runbooks etc.', 'About Rackspace Technology', 'More on Rackspace Technology', 'Development and deployment of Tableau, implement and maintain test strategies and plans', 'Job Description Summary', 'A solid understanding of SQL, rational databases, and normalization.', 'Extensive experience in developing, maintaining and managing Tableau driven dashboards & analytics', 'Experience with Data level security, maintaining users, groups and role in Tableau', 'Should be able to install and configure Multi Node Tableau Architecture in Linux and Window Environments', 'Gather business requirements, elicit technical requirements, prepare report specifications', 'Understand business requirements, conduct analysis, and recommend solution options', 'Managing Tableau Server', 'Twisting SQL queries for improving performances', 'DW/BI, ETL and Data related technologies and tools', 'Experienced in DW/BI, ETL and Data related technologies and tools', 'Extensive experience in developing, maintaining and managing Tableau driven dashboards & analyticsExpert in Tableau administration/architectureSkilled Tableau Developer with experience in implementing best practice in analytics and visualization, promote agile delivery, and help expose dataExperience with Data level security, maintaining users, groups and role in TableauDevelopment and deployment of Tableau, implement and maintain test strategies and plansDrive the engagements with customers from the architectural pillar, from design to delivery, create runbooks etc.Should be able to install and configure Multi Node Tableau Architecture in Linux and Window EnvironmentsPerforming and documenting data analysis, data validation, and data mapping/design.Work with customers and the wider Rackspace organizations on re-thinking and re-designing IT Data landscapes using cloud-native technologiesExcellent technical architecture skills, design and deliver innovative Proof of Concepts for customers.Mentor and train other architects/Engineer within the wider Rackspace community on Tableau and other tools like Looker', 'Qualifications & Experience', 'Work with customers and the wider Rackspace organizations on re-thinking and re-designing IT Data landscapes using cloud-native technologies', 'Should be good with writing SQL queries and Stored procedures', 'Experienced working with any public cloud platform like AWS, GCP or Azure', 'Cloud Certifications', 'Experience with configuring Tableau with different Identity stores and have knowledge of Integrating with AD for SAML.', 'Performing and documenting data analysis, data validation, and data mapping/design.', 'Develop reports, dashboards, scorecards', 'Good conceptual knowledge and working experience on report/dashboard creation, creating report specifications, integration test planning & testing, unit test planning & testing, UAT & implementation support', 'What You’ll Be Doing', 'Proficiency in use of query and reporting analysis tools.', 'Working closely with the business, this role will be responsible for designing, building and maintaining Tableau Workbooks', 'Provide support and expertise to the business community to assist with better utilization of Tableau.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Sogeti,"Bellevue, WA",1 week ago,71 applicants,"['', '7+ of SQL Server development', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Personal Time Off (PTO) and Paid Holidays', 'Must have strong experience with Azure – Azure Data Lake / Azure Data Factory.\xa0', 'Create Technical Design Specification documentation that clearly articulates the design and code being implemented.', 'Expertise in T-SQL, DW Concepts, Tabular Cube.', 'Excellent coding and debugging skills.', 'Ability to communicate with Business and developers accordingly.', 'FTE w/ Benefits', 'Provide support as needed throughout Test and User Acceptance Testing phases.', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0100% Company-paid mobile phone plan', 'Be able to translate technical specifications into finished programs and systems and also have a thorough understanding of developing solutions to handle large volume data sets that are typical with BI solutions.', 'Responsibilities:', 'Ensure development and testing follow industry standards', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0401(k) Savings Plan- Matched 150% up to 6%. (Our 401k is in the top 1% of 401(k) plans offered in the US!)', 'Must have strong experience with Azure – Azure Data Lake / Azure Data Factory.', 'Work on complex BI ecosystems and design / develop solutions based on SQL SSIS, Azure Databricks, SQL SSAS', '7+ of SQL Server development experience writing complex stored procedures, triggers, views.', 'Experience with Power BI', '7+ of SQL Server development experience writing complex stored procedures, triggers, views.Strong understanding of BI areas.\xa0Ability to work in large, complex development BI projects including the proactive identification of issues and coordination of resolutions.Expertise in T-SQL, DW Concepts, Tabular Cube.Experience with Power BIMust have strong experience with Azure – Azure Data Lake / Azure Data Factory.\xa0Strong Analytical and troubleshooting skillsExcellent coding and debugging skills.Able to work independently to implement a solution with minimal guidance.Ability to communicate with Business and developers accordingly.Strong communication skills in both written and spoken English.Working knowledge on Cosmos / Big Data Platforms (Azure, Databricks) is recommended.Excellent communication skills and ability to work under continual deadline constraints are necessary', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Medical and Dental Coverage ', '\xa0', 'Manage errors gracefully. Document code and work completed.', 'Proficiency in creating visualizations / reports / dashboards based on business requirements', 'Conduct thorough unit testing of code and document the unit test cases.', 'Please share your resume and work authorization status for US via email to me at harneet.sapra@sogeti.com', 'Strong communication skills in both written and spoken English.', 'Design and code high quality database solutions within a fast-paced sprint release cycle', 'Assist in architectural design reviews and ensure technology stack selection is relevant', 'Provide client communication as appropriate to project.', 'The benefits our employees enjoy:', 'Able to work independently to implement a solution with minimal guidance.', 'Location: Bellevue, WA', 'Work Experience:', 'Strong understanding of BI areas.\xa0Ability to work in large, complex development BI projects including the proactive identification of issues and coordination of resolutions.', 'Design and develop PL / SQL procedures / ETL jobs with optimized processing time', 'Communicate design, requirements, feature set, functionality, usability, and limitations of subsystem to team and/or development lead or manager.', 'Data Engineer', 'Participate in Business Requirements and Functional Requirements meetings, identify gaps in requirements and drive discussion around appropriate solutions.', 'Strong Analytical and troubleshooting skills', 'Working knowledge on Cosmos / Big Data Platforms (Azure, Databricks) is recommended.', 'Work on complex BI ecosystems and design / develop solutions based on SQL SSIS, Azure Databricks, SQL SSASDesign and develop PL / SQL procedures / ETL jobs with optimized processing timeCommunicate design, requirements, feature set, functionality, usability, and limitations of subsystem to team and/or development lead or manager.Participate in Business Requirements and Functional Requirements meetings, identify gaps in requirements and drive discussion around appropriate solutions.Design and code high quality database solutions within a fast-paced sprint release cycleProficiency in creating visualizations / reports / dashboards based on business requirementsManage errors gracefully. Document code and work completed.Conduct thorough unit testing of code and document the unit test cases.Conduct appropriate performance testing to ensure all solutions will meet SLAs and performance criteria.Provide support as needed throughout Test and User Acceptance Testing phases.Create Technical Design Specification documentation that clearly articulates the design and code being implemented.Provide client communication as appropriate to project.Be able to translate technical specifications into finished programs and systems and also have a thorough understanding of developing solutions to handle large volume data sets that are typical with BI solutions.Assist in architectural design reviews and ensure technology stack selection is relevantEnsure development and testing follow industry standards', 'Conduct appropriate performance testing to ensure all solutions will meet SLAs and performance criteria.', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Competitive Base Salary', 'Education Requirements: Bachelor’s Degree', 'Excellent communication skills and ability to work under continual deadline constraints are necessary']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Trex Company,"Winchester, VA",1 week ago,Be among the first 25 applicants,"['', 'Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.', 'Experience supporting and working with cross-functional teams in a dynamic environment.', 'Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.', 'Build processes supporting data transformation, data structures, metadata, dependency and workload management.', 'Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.', 'Create and maintain optimal data pipeline architecture,', ' Create and maintain optimal data pipeline architecture, Assemble large, complex data sets that meet functional business requirements. Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies. Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics. Work with stakeholders including the production, engineering and quality teams to assist with data-related technical issues and support their data infrastructure needs. Keep our data separated and secure across boundaries through multiple data centers and regions. Create data tools for analytics and engineering team members that assist them in building and optimizing our product into an innovative industry leader. Work with data and analytics experts to strive for greater functionality in our data systems. ', 'Create data tools for analytics and engineering team members that assist them in building and optimizing our product into an innovative industry leader.', 'Strong analytic skills related to working with unstructured datasets.', 'Keep our data separated and secure across boundaries through multiple data centers and regions.', 'Work with stakeholders including the production, engineering and quality teams to assist with data-related technical issues and support their data infrastructure needs.', 'Strong project management and organizational skills.', 'Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.', 'A successful history of manipulating, processing and extracting value from large disconnected datasets.', 'Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.', 'Work with data and analytics experts to strive for greater functionality in our data systems.', 'Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.', ' Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases. Experience building and optimizing ‘big data’ data pipelines, architectures and data sets. Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. Strong analytic skills related to working with unstructured datasets. Build processes supporting data transformation, data structures, metadata, dependency and workload management. A successful history of manipulating, processing and extracting value from large disconnected datasets. Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores. Strong project management and organizational skills. Experience supporting and working with cross-functional teams in a dynamic environment.', 'Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.', 'Assemble large, complex data sets that meet functional business requirements.']",Entry level,Full-time,Information Technology,Construction,2021-03-24 13:05:10
Data Engineer,Visual Concepts,"Agoura Hills, CA",7 days ago,143 applicants,"['', 'What You Will Do', 'Video games industry experience and/or gaming familiarity.', 'Establish and distribute regular performance reports based on telemetry and advanced models.', 'Experience with varied data modeling approaches.', 'Expert communication, facilitation, and collaboration skills to effectively present, explain, influence, and advise within cross-functional teams.', 'Preferred Qualifications', ' Minimum of 2 years of experience in data mining & analytics, building models with very large, complex and multi-dimensional data sets. Minimum of 1 year of basic programming experience with C++, and able to demonstrate C++ coding proficiency. Expert in relational databases and SQL data carpentry. Experience with varied data modeling approaches. Expert communication, facilitation, and collaboration skills to effectively present, explain, influence, and advise within cross-functional teams. ', ' Create statistical models that can predict and analyze player behavior, optimize engagement, and inform post-launch feature creation. Using created statistical models and analyses, interpret the underlying story to answer business and game design questions. Develop code to facilitate statistical queries and implement hooks to improve data gathering techniques. Work closely with development teams to provide insights into game quality, difficulty, and fun. Establish and distribute regular performance reports based on telemetry and advanced models. Collaborate with other departments to determine project needs and analytics requirements. Maintain data integrity and resolve issues found with incorrect or incomplete datasets. ', 'Minimum of 1 year of basic programming experience with C++, and able to demonstrate C++ coding proficiency.', 'Using created statistical models and analyses, interpret the underlying story to answer business and game design questions.', 'Requirements', 'Create statistical models that can predict and analyze player behavior, optimize engagement, and inform post-launch feature creation.', 'WWE knowledge and familiarity a plus.', 'Expert in relational databases and SQL data carpentry.', 'Maintain data integrity and resolve issues found with incorrect or incomplete datasets.', 'Work closely with development teams to provide insights into game quality, difficulty, and fun.', ' Video games industry experience and/or gaming familiarity. WWE knowledge and familiarity a plus. Experience with JIRA, DeltaDNA, Perforce, and Confluence. Experience with predictive modeling techniques and accomplished in use of R or Python machine learning and related modules.', 'Experience with JIRA, DeltaDNA, Perforce, and Confluence.', 'Collaborate with other departments to determine project needs and analytics requirements.', 'Develop code to facilitate statistical queries and implement hooks to improve data gathering techniques.', 'Experience with predictive modeling techniques and accomplished in use of R or Python machine learning and related modules.', 'Minimum of 2 years of experience in data mining & analytics, building models with very large, complex and multi-dimensional data sets.']",Entry level,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Data Engineer,Travelers,"Hartford, CT",4 weeks ago,Be among the first 25 applicants,"['', 'Having awareness of web development technologies and frameworks like Node Js, React Js, HTML, Java script, CSS is a plus', 'Provides guidance and mentorship to lower level technical employees.', 'Hands on experience using Spark/Python. Utilizing DataBricks a plus!Understanding of AWS Native tools like Lambda, Glue, Kinesis, Terraform scripting, EC2, IAM etc.Hands-on experience in writing complex, optimized SQL queries across large datasets. Understanding of tools/technologies like Athena, Redshift and snowflake a plusUnderstanding of Visualization tools: Tableau, Qliksense or MicrostrategyNice to have experience with Airflow DAG for orchestration.Having awareness of web development technologies and frameworks like Node Js, React Js, HTML, Java script, CSS is a plus', 'Minimum Qualifications', 'Examples (but not limited to): Big data and Hadoop platforms and languages - SAS, SQL, Spark, Python, Pig, Hive, QlikView, Tableau.', 'Able to clearly and concisely communicate with the business. Demonstrates willingness, cooperation, and concern for business issues. Able to understand assigned business unit and business priorities.', 'Effectively contributes and communicates with the immediate team.', 'Education, Work Experience, & Knowledge', 'Develops moderate and applies complex data derivations, business transformation rules, and data requirements.', 'Hands-on experience in writing complex, optimized SQL queries across large datasets. ', 'Creates moderate (technology and features) data visualization techniques to help support data exploration.', 'Applies knowledge of current industry trends and techniques to formulate solutions within the context of assigned projects and/or enhancements.', 'Understanding of AWS Native tools like Lambda, Glue, Kinesis, Terraform scripting, EC2, IAM etc.', 'Understanding of Visualization tools: Tableau, Qliksense or Microstrategy', 'Develops process to acquire and integrate data.', 'Job Description Summary', 'Able to recognize, analyze, and diagnose business and data issues of advanced complexity.', 'Understanding of tools/technologies like Athena, Redshift and snowflake a plus', 'Reviews unfamiliar data sources. Connects to value for business. Able to link to systems.', 'Company Summary', 'Works within Travelers standards, processes, and protocols.', 'Consultation:Shares knowledge with users on data or analytic products.', 'Exhibits active and effective communication skills with team members - including active listening and effective written and verbal communication skills.', 'Ability to communicate thoughts/designs/ideas in an unambiguous manner and adjusts communication based on audience.', 'Builds effective relationships with stakeholders.', 'Employment Practices', 'Performs analysis of complex (type, quality, volume) sources to determine value and use. Determines and recommends data to include in analytical projects.', 'Leads medium scale projects and coordinates aspects of larger projects with limited supervision.', 'Ensures customer satisfaction through professional communication, follow-up, and responsiveness to issues.', 'Able to coordinate with other technical areas to achieve project/department or division goals.', 'Reviews unfamiliar data sources. Connects to value for business. Able to link to systems.Develops process to acquire and integrate data.Independently reviews, prepares, designs and integrates complex (type, quality, volume) data correcting problems and recommends data cleansing/quality solutions to major issues.Works within Travelers standards, processes, and protocols.Develops moderate and applies complex data derivations, business transformation rules, and data requirements.Leads medium scale projects and coordinates aspects of larger projects with limited supervision.Performs analysis of complex (type, quality, volume) sources to determine value and use. Determines and recommends data to include in analytical projects.Creates moderate (technology and features) data visualization techniques to help support data exploration.Utilizes business knowledge to explain technical activities in business terms.Actively seeks opportunities to expand technical knowledge and capabilities.Develops and maintains relationships across the enterprise.Applies knowledge of current industry trends and techniques to formulate solutions within the context of assigned projects and/or enhancements.Ensures customer satisfaction through professional communication, follow-up, and responsiveness to issues.Consultation:Shares knowledge with users on data or analytic products.Builds effective relationships with stakeholders.Provides guidance and mentorship to lower level technical employees.', 'Advanced knowledge of data tools, techniques, and manipulation preferred.Examples (but not limited to): Big data and Hadoop platforms and languages - SAS, SQL, Spark, Python, Pig, Hive, QlikView, Tableau.Ability to communicate thoughts/designs/ideas in an unambiguous manner and adjusts communication based on audience.Exhibits active and effective communication skills with team members - including active listening and effective written and verbal communication skills.Effectively contributes and communicates with the immediate team.Able to clearly and concisely communicate with the business. Demonstrates willingness, cooperation, and concern for business issues. Able to understand assigned business unit and business priorities.Able to coordinate with other technical areas to achieve project/department or division goals.Able to recognize, analyze, and diagnose business and data issues of advanced complexity.', 'Utilizes business knowledge to explain technical activities in business terms.', 'Develops and maintains relationships across the enterprise.', 'Target Openings', 'Independently reviews, prepares, designs and integrates complex (type, quality, volume) data correcting problems and recommends data cleansing/quality solutions to major issues.', 'Hands on experience using Spark/Python. Utilizing DataBricks a plus!', 'Job Specific Technical Skills & Competencies', '4 years of relevant experience with data tools, techniques, and manipulation required.', 'Nice to have experience with Airflow DAG for orchestration.', 'Advanced knowledge of data tools, techniques, and manipulation preferred.', 'Primary Job Duties & Responsibilities', 'Actively seeks opportunities to expand technical knowledge and capabilities.']",Entry level,Full-time,Information Technology,Law Practice,2021-03-24 13:05:10
Data Engineer,Peptilogics,"Pittsburgh, PA",4 weeks ago,101 applicants,"['', 'At Peptilogics, diversity, inclusion and equality are embedded in our DNA.\xa0Together, regardless of gender, race, ethnicity, national origin, age, sexual orientation or identity, education or disability, the Peptilogics team envisions a workplace where all employees feel valued and respected. We continue to build an inclusive culture that encourages, supports, and celebrates the diverse voices of our team members.\xa0', 'Identify and implement tools for data visualization.', 'About the Position', 'Excellent communication and interpersonal skills.', 'This Data Engineer position contributes to research and development to advance discovery and development of peptide therapeutics. The position operates in close collaboration with experts in machine learning, computational biology, peptide science, and software platform development.', ""Acquire relevant data from public and proprietary sources and integrate it with Peptilogics' data systems."", 'Experience with Linux, Python, and API design and implementation.', 'Recommend and implement ways to improve data quality and efficiency.', 'Identify, implement, maintain, and test data architecture to maximize the value of large, heterogeneous data collections and to maximize their performance.', 'M.S. in computer science, bioinformatics, or related discipline.Experience with, and strong working knowledge of, diverse database technologies including graph (e.g., Dgraph, Neo4j), document (e.g., MongoDB), and relational (e.g., MariaDB MySQL).Experience working with bioinformatic data.Experience with high-performance computing (HPC) and public clouds (e.g., AWS, GCP, Azure).Experience with Linux, Python, and API design and implementation.Excellent communication and interpersonal skills.', 'Experience working with bioinformatic data.', 'Contribute requirements specifications and feedback to the software platform team.', '\xa0', 'Diversity & Inclusion: Our Foundation for Innovation', 'Peptilogics is a clinical-stage biotechnology company that designs and develops novel peptide therapeutics. The company leverages machine learning, automation, and peptide synthesis to build platforms for science at scale. With its foundations in drug development and engineering, the company’s platforms vertically integrate proprietary hardware, software, bioinformatics, chemistry, and molecular biology to advance basic research, target validation, and clinical trials.', 'Peptilogics: Leaders in Scalable, Rational Design', '\ufeff', 'Communicate methods and results to stakeholders representing diverse expertise and backgrounds.', 'Experience with high-performance computing (HPC) and public clouds (e.g., AWS, GCP, Azure).', 'Identify and implement data access tools and APIs.', 'Job duties and responsibilities:', 'Experience with, and strong working knowledge of, diverse database technologies including graph (e.g., Dgraph, Neo4j), document (e.g., MongoDB), and relational (e.g., MariaDB MySQL).', 'M.S. in computer science, bioinformatics, or related discipline.', ""Acquire relevant data from public and proprietary sources and integrate it with Peptilogics' data systems.Recommend and implement ways to improve data quality and efficiency.Identify, implement, maintain, and test data architecture to maximize the value of large, heterogeneous data collections and to maximize their performance.Identify and implement data access tools and APIs.Identify and implement tools for data visualization.Contribute requirements specifications and feedback to the software platform team.Communicate methods and results to stakeholders representing diverse expertise and backgrounds."", 'Peptilogics is committed to serving as a model of diversity and inclusion for the entire biotech and biopharmaceutical industry and to maintaining an inclusive environment with equitable treatment for all. It fuels our innovation, reinforces our vision, and more closely connects us to the communities we serve.', 'Education and experience:']",Not Applicable,Full-time,Engineering,Biotechnology,2021-03-24 13:05:10
Data Analytics Engineer,GavinHeath,"Englewood, CO",1 day ago,Be among the first 25 applicants,"['', '3+ years of exerience working / developing Tableau OR PowerBI DashboardsAbility to work independenlty and within a large data engineering teamAbility to improve dashboards with Tableau OR PowerBI', 'Ability to improve dashboards with Tableau OR PowerBI', 'Working with SQL Database and new data to develop dashboards based on ongoing requirements ', '3+ years of exerience working / developing Tableau OR PowerBI Dashboards', 'Ability to work independenlty and within a large data engineering team', 'Work with datasrouces to bring out improved business knowledge based on new data daily', 'Work with Data Analytics lead and Data Engineering team to develop, build and maintatin Dashboards in Tableau and PowerBIWorking with SQL Database and new data to develop dashboards based on ongoing requirements Work with datasrouces to bring out improved business knowledge based on new data daily', 'Work with Data Analytics lead and Data Engineering team to develop, build and maintatin Dashboards in Tableau and PowerBI']",Entry level,Other,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer - Data Platform,Rover.com,"Seattle, WA",4 weeks ago,56 applicants,"['', 'Supporting our partner teams by solving complex problems in development and production in a timely manner. ', 'Competitive compensation', 'Commuter benefits', 'Familiarity with large-scale distributed real-time tools such as Kafka and Spark. ', 'Competitive benefits package, including medical, dental, and vision insurance', 'Building self-serve tools that make it easy to get key business insights.', 'Experience designing and deploying systems with effective monitoring and logging practices. ', 'Pet benefits, including $1000 toward adopting your first dog or cat', 'Stock options', ' Your Qualifications ', 'Strong Python skills in both data and software engineering contexts.', ' Data Engineering At Rover ', ' Your Responsibilities ', '401k Match', ""Designing, developing, testing, and automating high-performance batch and stream data processing systems. Monitoring, maintaining and improving Rover's data pipelines and its associated infrastructure. Supporting our partner teams by solving complex problems in development and production in a timely manner. Building self-serve tools that make it easy to get key business insights."", 'At least 3 years of relevant industry experience.Strong Python skills in both data and software engineering contexts.Experience designing and deploying systems with effective monitoring and logging practices. Solid understanding of relational databases and schema design. Passionate about automated testing and delivering good quality code.Familiarity with large-scale distributed real-time tools such as Kafka and Spark. Plus: Experience working with AWS, Docker, Luigi, Jupyter, Kubernetes, Redshift, RDS, Terraform. ', 'Flexible PTO', ' Benefits Of Working For Rover ', 'Passionate about automated testing and delivering good quality code.', 'Competitive compensation401k MatchStock optionsFlexible PTOCompetitive benefits package, including medical, dental, and vision insuranceCommuter benefitsBring your dog to work (and unlimited puppy time)Pet benefits, including $1000 toward adopting your first dog or catStocked fridges, coffee, soda, and lots of treats (for humans and dogs) and free catered lunches semi-monthly (currently on hold due to Covid19)Regular team activities, including happy hours, snow tubing, game nights, and more (currently performed virtually due to Covid19)Due to COVID-19, Rover Employees are not required to be in office until July 2021 at the earliest. All new hires will be expected to work from Seattle/ Spokane/ Barcelona once Rover Employees return to office.', 'Designing, developing, testing, and automating high-performance batch and stream data processing systems. ', 'Plus: Experience working with AWS, Docker, Luigi, Jupyter, Kubernetes, Redshift, RDS, Terraform. ', 'Stocked fridges, coffee, soda, and lots of treats (for humans and dogs) and free catered lunches semi-monthly (currently on hold due to Covid19)', 'Bring your dog to work (and unlimited puppy time)', 'At least 3 years of relevant industry experience.', ""Monitoring, maintaining and improving Rover's data pipelines and its associated infrastructure. "", 'Due to COVID-19, Rover Employees are not required to be in office until July 2021 at the earliest. All new hires will be expected to work from Seattle/ Spokane/ Barcelona once Rover Employees return to office.', 'Who We Are ', 'Solid understanding of relational databases and schema design. ', 'Regular team activities, including happy hours, snow tubing, game nights, and more (currently performed virtually due to Covid19)']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,InfoVision Inc.,"Plano, TX",3 weeks ago,87 applicants,"['At least 2 years of experience with Linux, server automation and scripting.', 'At least 3 years of experience with Python programmingAt least 2 years of experience in technology delivery in a Cloud Engineering environment.At least 2 years of experience with Linux, server automation and scripting.At least 1 year of experience working with Agile Development Practices.AWS Certified', 'At least 2 years of experience in technology delivery in a Cloud Engineering environment.', 'At least 1 year of experience working with Agile Development Practices.', ""Bachelor's Degree"", 'Basic Qualifications:', 'Preferred Qualifications:', 'At least 3 years’ experience with SQL', 'AWS Certified', 'At least 3 years of experience with Python programming', 'At least 3 years of experience with AWS', ""Bachelor's DegreeAt least 3 years of experience with AWSAt least 3 years of experience with Python programmingAt least 3 years’ experience with SQLAt least 2 years’ experience with Spark"", 'At least 2 years’ experience with Spark', '\xa0']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Ledgent Technology,Miami-Fort Lauderdale Area,,N/A,"['', '•\tRegularly interpret, summarize and present data outcomes to senior leadership.\xa0', '**The ideal candidate will reside in Florida but not required**\xa0', '•\tFamiliarity with Access (preferred)', '•\tDevelop instrumentation to track progress against key business objectives.', '•\tAdvanced proficiency in Excel that includes pivot tables, macros, v/h lookups (required).', '•\tAn understanding of relational database structures, theories, principles, and practices (required).', 'This position requires more than just analysis. Candidates will have to take a SQL Assessment to ensure their expertise in SQL. Degree is required.\xa0', '•\tDevelop complex analysis requiring the integration of multiple data sources.', '•\tWork with individuals from various departments to ensure the requirements of their requests are understood and the outcome presented meets their expectations.', '•\tIdentify, design, and implement new internal process improvements.\xa0', '•\tData mining large data files and analyzing, forecasting, and trending data to support various teams.\xa0', '•\tSearch for opportunities to automate manual processes and develop on-demand tools for           descriptive data.', '•\tCollaborate with management team to implement process and system changes.', '•\tStandardize data to consistent format/structure.', '•\tPrepare feasibility analysis, business process assessments, gap analysis, cost-benefit analysis, and industry best practice research. Write reports to identify areas of improvement or growth.', '•\tMonitor, analyze, and report variances from planned and forecasted figures.', '•\tAssemble large, complex data sets and evaluate data integrity.\xa0', 'What you should bring to the table:', '•\tBachelor’s degree in Mathematics, Statistics, Information Technology, Finance, Economics, or a related field of study (required)', '•\tExtract large data records from internal systems through data mining, queries, data analysis methods, building and implementing models, using/creating algorithms, and creating/running simulations.', '•\t5+ years of data analysis and business analysis experience (required).\xa0', '•\tAmple experience analyzing data and establishing business rules and requirements (required).', 'What you will be doing:', '•\tVerify the relevance and accuracy of data.', '•\tProficiency with statistical and analytical software (required).', '•\tExperience writing SQL queries (required)']",Mid-Senior level,Full-time,Information Technology,Staffing and Recruiting,2021-03-24 13:05:10
Data Engineer,Springer Nature Technology and Publishing Solutions,"Springer, NM",1 week ago,Be among the first 25 applicants,"['', 'Work with architects and other data engineers in the organisation to align the data processing architecture ', ' You understand the benefits of Test Driven Development and automation. You are comfortable pair programming and practising continuous integration and continuous delivery. You see the value in developers owning production software and view failure as a chance to learn. ', 'Promote and advocate the use of data across Springer Nature.', ' Have an understanding of the team’s context within the wider organisation. Be a supportive member of the team, developing the platform by using the appropriate technology solutions to solve the problem at hand. Triage support queries and diagnose issues in our live applications. Identify new sources of data across the organisation and build relationships with data providers to gain access. Understand the processes by which data is acquired and any resulting limitations or bias and communicate this to the team. Develop and maintain data pipelines to load data into systems like BigQuery, to analyse, clean and join datasets, in an automated, repeatable way. Ensure that data is stored securely and in compliance with GDPR. Work with data owners to understand how we can allow them to self-serve their data using tools we develop. ', 'Get familiar with our emerging technology stack and data landscape. ', 'Day To Day Responsibilities', 'To apply please upload a copy of your CV and covering letter, stating your salary expectations. ', 'You enjoy getting involved with every stage of the engineering lifecycle.', ' Take part in our daily stand-ups. Contribute to ceremonies like steering, story writing, collaborative design and retrospectives. Develop new features and improve code quality by pair programming with other team members. Take part in the support and monitoring of our services. ', 'About You', 'Triage support queries and diagnose issues in our live applications.', 'At Springer Nature, we value and celebrate the diversity of our people. We recognise the many benefits of a diverse workforce and strive for an inclusive workplace that empowers all our colleagues to thrive. Our search for the best talent fully encompasses and embraces these values and principles.', 'Develop new features and improve code quality by pair programming with other team members.', 'For Examples', 'By 6-12 Months You Will', 'We are committed to growing and nurturing our people for the long-term. We spend 10% of our time working on our own projects to promote learning and innovation; as well as regular lunch n’ learn sessions to share knowledge.', 'If you have an interest in data science there may be opportunities to apply machine learning techniques to these datasets to assist in the work of domain teams.', 'Across the programme, our teams are cross-functional, diverse and made up of different experience levels. All team members collaborate to deliver the best solutions that satisfy our customers’ needs.', 'Have an understanding of data and distributed systems concepts.', 'Visit', 'The job is based London (UK) or in Lisbon (Portugal) and you will work remotely at times with colleagues in many of our global offices including, Berlin (Germany), or in Heidelberg (Germany). Some travel will be required. This role is on a small, autonomous team and you will be expected to impact what we do and how we work. We like to keep our processes light, and bureaucracy slim.', 'You understand the benefits of Test Driven Development and automation.', 'Align yourself with the work of the data platform team and understand the data requirements and issues facing our users.', 'Be a supportive member of the team, developing the platform by using the appropriate technology solutions to solve the problem at hand.', 'Identify new sources of data across the organisation and build relationships with data providers to gain access.', 'Work with data owners to understand how we can allow them to self-serve their data using tools we develop.', 'Able to gauge the complexity or scope of a piece of work, breaking it into smaller pieces when appropriate.', 'We’re looking for a Data Engineer to join Data.SN within Springer Nature Operations. Springer Nature is a leading publisher of scientific books, journals and magazines with over 3000 journal titles and one of the world’s largest corpora of peer-reviewed scientific text data. You would be joining a new programme of work to transform how Springer Nature uses its data: building up data capabilities, creating a data platform and engineering capability (technology, people and process) to create a foundation for the future, adding value to cross-organisation Initiatives and kick-starting data-driven Innovation.', 'Understand the processes by which data is acquired and any resulting limitations or bias and communicate this to the team.', 'Have an understanding of the team’s context within the wider organisation.', 'What You Will Be Doing', 'Collaborate effectively with each discipline on the team.', 'Mentor other members of the team in the principles of data engineering and promote best practice.', 'You have several years of experience in Data/Software Engineering on a cloud platform.', 'About Us', ' Get familiar with our emerging technology stack and data landscape.  Align yourself with the work of the data platform team and understand the data requirements and issues facing our users. Collaborate effectively with each discipline on the team. Actively participate in technical discussions and share ideas. Work with architects and other data engineers in the organisation to align the data processing architecture  ', 'As part of an Agile product team, day-to-day you will:', 'You like working in a collaborative team where there is collective ownership.', 'You are comfortable pair programming and practising continuous integration and continuous delivery.', 'Within 3 Months you will:', 'Why Not Follow Our Early Careers Page On Hive To Find Out More', 'Contribute to ceremonies like steering, story writing, collaborative design and retrospectives.', 'You understand the benefits agile software engineering practises can bring to data engineering.', 'Develop processes and tools to monitor feeds and test data integrity and completeness and to alert users when a problem occurs.', 'Give and receive constructive feedback within your team.', 'Springer Nature is one of the world’s leading global research, educational and professional publishers. It is home to an array of respected and trusted brands and imprints, with more than 170 years of combined history behind them, providing quality content through a range of innovative products and services. Every day, around the globe, our imprints, books, journals and resources reach millions of people, helping researchers and scientists to discover, students to learn and professionals to achieve their goals and ambitions. The company has almost 13,000 staff in over 50 countries.', 'Take part in our daily stand-ups.', 'By 3-6 Months You Will', ' Develop processes and tools to monitor feeds and test data integrity and completeness and to alert users when a problem occurs. Understand our customers’ needs, both internal and external, and how your work affects their experience. Able to gauge the complexity or scope of a piece of work, breaking it into smaller pieces when appropriate. Give and receive constructive feedback within your team. Mentor other members of the team in the principles of data engineering and promote best practice. Promote and advocate the use of data across Springer Nature. If you have an interest in data science there may be opportunities to apply machine learning techniques to these datasets to assist in the work of domain teams. ', 'You see the value in developers owning production software and view failure as a chance to learn.', 'Develop and maintain data pipelines to load data into systems like BigQuery, to analyse, clean and join datasets, in an automated, repeatable way.', 'Take part in the support and monitoring of our services.', 'Ensure that data is stored securely and in compliance with GDPR.', 'Actively participate in technical discussions and share ideas.', 'Understand our customers’ needs, both internal and external, and how your work affects their experience.']",Entry level,Full-time,Information Technology,Online Media,2021-03-24 13:05:10
Data Engineer (Remote),KAR Global,United States,5 days ago,Be among the first 25 applicants,"['', 'Bachelor’s degree in Business, Computer Science, Management Information Systems or equvalent (required).5+ years experience Postgres or Oracle SQL development including functions, stored procedures, and indexing or equivalent (required).Experience in production data management in high availability product delivery ODS / RDBMS or equivalent (required).Experience planning and designing maintainable data schemas (required).Experience with Python, Docker, and data warehouse environments (preferred).Experience using Github / Jenkins (CI/CD) / Artifactory / PyPy or comparable delivery stacks (preferred).Experience with Postgres, Elastic Search, AWS EMR, and AWS ECS (preferred).Experience with AWS Redshift, MPP, or Dynamo DB (preferred).Experience with Informatica (preferred)Experience with Kinesis/Kafka (preferred).Experience working with large enterprise data lakes / Snowflake (preferred).Experience with Informatica MDM Hub configurations - Data modeling & Data Mappings (Landing, staging and Base Objects), Data validation, Match and Merge rules, Active VOS, SIF Framework, and MDM User Exits (preferred)Experience working with master data management in automotive or business to business customer domains (preferred)', 'Experience in production data management in high availability product delivery ODS / RDBMS or equivalent (required).', 'About Our Candidate: ', 'Location', 'Location - Remote, North America', 'Experience with AWS Redshift, MPP, or Dynamo DB (preferred).', 'Design, build, and delivery of enterprise data services solutions for MDM on the DaaS Platform.', 'Experience planning and designing maintainable data schemas (required).', 'We’re an analytics company\xa0leveraging data to inform and empower our customers with clear, actionable insights.', 'This candidate should be a self-starter who is interested in learning new systems/environments and passionate about developing quality supportable data service solutions for internal and external customers. We highly value natural curiosity about data and technology that drives results through quality, repeatable, and long-term sustainable database and code development. The candidate should be highly dynamic and excited by opportunities to learn many different products and data domains and how they drive business outcomes and value for our customers.', 'Experience with Informatica (preferred)', 'Experience working with master data management in automotive or business to business customer domains (preferred)', 'KAR Global powers the world’s most trusted automotive marketplaces through innovation, technology and people. Our end-to-end platform serves the remarketing needs of the world’s largest OEMs, dealers, fleet operators, rental companies and financial institutions.', 'Experience working with large enterprise data lakes / Snowflake (preferred).', 'About Our Team: ', 'Contribute to the Data Engineering and MDM team delivery framework including building of re-usable code, implementing industry best practices, and maintain a common delivery framework.', 'And we’re an auction company\xa0powering the world’s most advanced and integrated mobile, digital and physical auction marketplaces.', 'Experience with Kinesis/Kafka (preferred).', 'Experience with Informatica MDM Hub configurations - Data modeling & Data Mappings (Landing, staging and Base Objects), Data validation, Match and Merge rules, Active VOS, SIF Framework, and MDM User Exits (preferred)', 'Bachelor’s degree in Business, Computer Science, Management Information Systems or equvalent (required).', 'We’re a technology company\xa0delivering next generation tools to accelerate and simplify remarketing.We’re an analytics company\xa0leveraging data to inform and empower our customers with clear, actionable insights.And we’re an auction company\xa0powering the world’s most advanced and integrated mobile, digital and physical auction marketplaces.', '5+ years experience Postgres or Oracle SQL development including functions, stored procedures, and indexing or equivalent (required).', 'Members of the data engineering team participate daily in ceremonies of Agile sprint to help design, plan, build, test, develop, and support KAR Global MDM data products and platforms consisting of Python ETL pipelines, Informatica applications, Postgres, Redshift, Dynamo DB, Oracle, and Snowflake databases. Our team works in a shared services delivery model supporting seven lines of business including front-end customer-facing products, B2B portals, mobile applications, business analytics, and data science initiatives.', 'Collaborates with source systems data stewards, Data owners and technical personnel for data governance and resolves any data quality or technical issues related to data ingestion.', 'Responsibilities include:', 'Work with product, data science, analytics, and engineering teams to learn project data needs and define project scope.Design, build, and delivery of enterprise data services solutions for MDM on the DaaS Platform.Building and delivery of Python/Docker feed framework data pipeline jobs and services.Contribute to the Data Engineering and MDM team delivery framework including building of re-usable code, implementing industry best practices, and maintain a common delivery framework.Monitoring, maintenance, documentation, and incident resolution of scheduled production data jobs supporting internal and external customers data needs.Collaborates with source systems data stewards, Data owners and technical personnel for data governance and resolves any data quality or technical issues related to data ingestion.', 'What You Need to Be Successful: ', 'KAR Global Data-as-a-Service team (DAAS) is looking to expand our data team as we continue to grow our data platform in support of a mission of digital transformation in automotive wholesale markets.\xa0 The data engineering team is responsible for the ingestion and persistence of data supporting an array of data products supporting KAR Global’s automotive wholesale business.', 'We’re a technology company\xa0delivering next generation tools to accelerate and simplify remarketing.', 'Work with product, data science, analytics, and engineering teams to learn project data needs and define project scope.', 'Building and delivery of Python/Docker feed framework data pipeline jobs and services.', 'Monitoring, maintenance, documentation, and incident resolution of scheduled production data jobs supporting internal and external customers data needs.', 'Experience using Github / Jenkins (CI/CD) / Artifactory / PyPy or comparable delivery stacks (preferred).', 'As part of a small, passionate and accomplished team of experts, you will work the full spectrum of Master Data Management (MDM). This is a high-impact, high-visibility team and is responsible for ensuring all master data is accurate, complete, and consistent across the entire enterprise. This team supports current new product development projects and performs ongoing guidance throughout the master data lifecycle.', 'The DRIVIN, a KAR Global brand, is comprised of a team that is passionate about the intersection of data, technology and cars. As the Chicago-based innovation and data science hub for KAR, this team of data scientists and analysts creates new products to help power the company’s physical, online, and digital/mobile automotive auction marketplaces. DRIVIN was founded in 2015 and joined the KAR family of companies in April 2017. Since then, DRIVIN has expanded its capabilities across the organization, shifting from a transactional marketplace to a full-service data and analytics platform. The DRIVIN data engine is unmatched, fueling powerful insights and recommendations that help KAR’s customers optimize risk, price and automotive inventory.', 'Who We Are:', 'Experience with Python, Docker, and data warehouse environments (preferred).', 'Experience with Postgres, Elastic Search, AWS EMR, and AWS ECS (preferred).', 'What You Will Be Doing: ']",Mid-Senior level,Full-time,Information Technology,Information Services,2021-03-24 13:05:10
 Data Engineer,Dunhill Professional Search & Government Solutions,"San Antonio, TX",4 weeks ago,80 applicants,"['', '1-2 years hands-on experience with python as a scripting language\xa0', 'Support development and operations of AIP (a\xa0data analytics and AI/ML platform as a service) focusing on\xa0data analytics applications,\xa0engineering of data pipelines and database administration.\xa0', 'Experience with DoD and Federal Government is strongly desired.\xa0', 'Develop or provide input to engineering artifacts associated with the data repositories\xa0', '1-2 years hands-on experience designing and implementing data integration, Big Data, and/or business intelligence solutions\xa0', 'Data Engineer\xa0', 'Ability to\xa0be a strong\xa0team\xa0player.\xa0', 'Advanced knowledge of security network and infrastructure tools, including access control and/or encryption.\xa0', 'Advanced written and verbal communication skills.\xa0', 'Ability to multitask and work well under pressure.\xa0', 'Advanced knowledge of database backup and recovery strategies.\xa0', 'In depth knowledge of troubleshooting skills and out of the box thinking to overcome data obstacles.\xa0', 'Support design and development of data access APIs that allow enterprise access to data;\xa0', 'Preferred Skills & Qualifications:\xa0', 'Experience with DBA productivity and performance tools.\xa0', 'Perform database engineering and management activities associated with designing, maintaining, and enhancing data analytics\xa0systems/applications\xa0using an Agile DevSecOps model;\xa0', 'Perform research and development and provide technical support to identify and integrate new and emerging technologies into the data environment;\xa0', 'Perform\xa0database management functions across one or more environments, including designing, implementing and maintaining databases, backup/recovery and configuration management.\xa0\xa0', 'Design and implement storage and indexing strategies to provide efficient storage and retrieval to support visualizations such as graph-based and geospatial;\xa0', '1-2 years hands-on experience in ETL design, and development in large scale data environment\xa0', '2-3 years hands-on experience designing and implementing data integration, Big Data, and/or business intelligence solutions\xa0', '1-2 years hands-on experience with AWS or Azure\xa0', '1-2 years hands-on experience with SQL relational databases: PostgreSQL, Oracle, MySQL etc...\xa0', 'US Citizenship required, dual citizens not eligible\xa0', 'Architect, design, and implement updates and enhancements to data repositories and indices to support data ingest, enrichment, analysis, visualization and dissemination;\xa0', 'Bachelor’s Degree in a related field of study.\xa0', 'Minimum Qualifications\xa0', 'Job Responsibilities\xa0', '5-10 years of related work experience.\xa0', '2-3 years of related experience and master’s degree in computer science, information engineering, information systems, or other related discipline.\xa0', 'Deploy data analytics applications and COTS products for\xa0client environments;\xa0', 'Excellent attention to detail and analytical skills.\xa0', 'Review, design, and develop data quality management processes and automated procedures intended to produce high levels of data integrity;\xa0', 'Other Job Specific Skills\xa0']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Princess Polly,United States,3 weeks ago,56 applicants,"['', 'We recognize that asking you to give 100% of yourself on a daily basis, requires us to show you the love and offer a package that can only be described as best in class within the retail space today!', 'Interest in conducting reproducible analysis that is peer-reviewed, version-controlled, and well-documented', 'Partner with our analysts to improve the quality of data that we load to our warehouse via custom pipelines (e.g. via Fivetran)', '·\xa0\xa0401(k) Program (100% Match Up to 5% of Pay)', '·\xa0\xa0Positive Company Culture that Celebrates both Personal & Company Milestones', 'Take ownership over our data architecture, managing, selecting, and improving the tools we use to store, load, transform, and visualize data', 'What are you waiting for? Come experience something different and amazing in the retail space!', '2+ years of experience in a data/analytics engineering role', 'Responsibilities:', 'Expert SQL skills (required)', ""We are always on the hunt for incredibly talented individuals like YOU to join the Polly team. We’re customer-obsessed, it’s what drives us to always stay relevant. Our ideal you, is driven, a diva for on-trend fashion, is not just all talk about how quickly you can go but someone who can walk/run/sprint when needed, as we are a fast-paced business! If you can identify with our brand and vision don't waste another moment, submit your application today!"", 'Implement testing, validation, and documentation to flag and resolve issues with poor-quality data', 'Qualifications:', '·\xa0\xa015 Vacation Days + 10 Sick Days + 10 Holidays', 'While already setting the pace in the industry and maintaining our market-leading position, in late 2018 we set up our Los Angeles office and tripled sales for the region within the first 12 months. Now a team of over 230 people globally, we are looking for the best and brightest to grow the Princess Polly brand in our US office located in West Hollywood!\xa0', '\xa0', '·\xa0\xa0Amazing Employee Discount Program (40%)', '·\xa0\xa0Company Paid Life, Short Term Disability, Long Term Disability, & Employee Assistance Plans', '·\xa0\xa0Company Sponsored Medical (HMO & PPO Options), Dental, & Vision Plans', 'Experience using dbt (strongly preferred)', 'Experience in ecommerce/direct to consumer businesses', 'Proactively seek out and explore new technologies to advance our data capabilities', 'Enthusiasm for writing clean code', 'Princess Polly is looking for a talented Data Engineer to join our Data team. He/She will be responsible for ensuring high-quality, accurate data modeling (we use dbt), ensuring that our raw data is complete and accurate, and helping data analysts and others on the Data team transform raw data into clean, modeled data that is ready for analysis by end users. This role will have a high level of autonomy with the ability to alter the processes we maintain, and make other decisions related to data/analytics engineering that improve the capabilities and outputs of our team. This role will report to the Head of of Data & Analytics.', '#PrincessPolly #PursueYourPassion #PrincessPollyCareers #SomethingDifferent\xa0', 'Design, build and maintain scalable data models (in dbt) to power self-service business intelligence tools and support fast analysis by our analysts', '·\xa0\xa0Individual & Team Based Leadership Development Programs', 'If you need assistance or accommodation during the hiring process due to a disability, please contact us at accomodation@exceleratebrands.com. Please note that we do not respond to application inquiries or resume submissions via this email address.', 'Established as an online force in the Australian retail scene, Princess Polly has delivered fashion-forward, trend-driven apparel and accessories for over 15 years. We are obsessed with creating the best customer experience available online and are committed to ensuring a speedy delivery so our customers can wear their picks this weekend!\xa0', 'Be a champion for efficient, effective data modeling, reviewing pull requests, suggesting improvements, and helping/coaching others on the team to write better code', ""Princess Polly is an Equal Opportunity Employer (EOE) . We're committed to a diverse and inclusive workplace and encourage applicants from all walks of life. Come join us, different makes us better"", 'Optimize our data modeling layer (dbt), reducing runtime and decreasing unnecessary complexity', 'Proficiency with a scripting language like Python (strongly preferred)', 'Aside from the amazing array of tangible benefits and perks, Princess Polly offers you the chance to make a daily impact on a global business. You have the opportunity to pursue your passion and plan your own future as part of our team!', 'Experience working with cloud data warehouses (required)']",Mid-Senior level,Full-time,Information Technology,Retail,2021-03-24 13:05:10
Data Engineer,Panasonic North America,"Palo Alto, CA",6 days ago,102 applicants,"['', 'Competitive compensation packageComprehensive benefitsPet InsurancePaid Parental Care LeaveEmployee Referral ProgramEducational AssistanceFlexible Work ProgramVolunteer time OffCasual Dress CodeTotal Well Being Program', 'Flexible Work Program', 'What You’ll Get To Do', 'Total Well Being Program', 'Preferred Qualifications', 'Experience with code development tools (version control, ticket systems, IDE integration)', 'Collaborate with cross-functional teams to help create and improve end-to-end features', 'BA/BS degree in Computer Science, Computer Engineering, Math/Statistics, Electrical Engineering or a related field or equivalent practical experience1 year of experience building data pipelines', '1 year of experience building data pipelines', 'Design, prototype, test and deploy data pipelinesBuild dashboards for visualizing dataCollaborate with cross-functional teams to help create and improve end-to-end features', 'Experience with visualizing data (e.g. data dashboards)', 'What You’ll Bring', 'Educational Assistance', 'Pet Insurance', ' Watch this video', 'Design, prototype, test and deploy data pipelines', 'Casual Dress Code', 'Experience with cloud computing platforms', 'Volunteer time Off', 'Click here to learn more', 'Competitive compensation package', 'What We Offer', 'Experience & Education:', 'Employee Referral Program', 'Comprehensive benefits', 'Build dashboards for visualizing data', '1+ years’ experience in one or more of: Python, C/C++, Java, SQL', 'Experience with code development tools (version control, ticket systems, IDE integration)Experience with cloud computing platformsExperience with visualizing data (e.g. data dashboards)1+ years’ experience in one or more of: Python, C/C++, Java, SQL', 'BA/BS degree in Computer Science, Computer Engineering, Math/Statistics, Electrical Engineering or a related field or equivalent practical experience', 'Paid Parental Care Leave']",Entry level,Contract,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Azure Data Engineer,"Adroitco, Inc","Houston, TX",1 day ago,76 applicants,"['', '• Synapse/SQL Datawarehouse ( Synapse not required for P66)- (Mandatory) ', 'Common Skills ', 'Location: CST time zone, 100% remote – Candidate can work from anywhere in USA', '• Scala or Python or Spark, #C/.Net- working knowledge of one of the languages below ( should know 1) ( Mandatory)', '• Azure Data Lake (Mandatory) ', 'Start Date: 6 months contract', '• Data bricks (optional if they have Spark or another language) ', 'Job Title: Azure Data Engineer', '• Data Factory (Mandatory) ']",Mid-Senior level,Contract,Product Management,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Epiq,"Kansas, United States",7 days ago,Be among the first 25 applicants,"['', 'Demonstrated experience designing and developing medium to large-scale, mission-critical database solutions, to include the following strengths: Thorough understanding of key performance indicators (KPI) Reports, and data mining structuresExpert data modeler and proficient with modeling toolsExtensive knowledge of data normalization concepts, Relational Database Management Systems, data warehouse systems, and star schemasExpertise with Index management: creation, maintenance, and tuning', 'Demonstrated experience designing and developing medium to large-scale, mission-critical database solutions, to include the following strengths: Thorough understanding of key performance indicators (KPI) Reports, and data mining structures', 'Competencies ', 'Stored Procedures', 'Position requires no significant manual labor.', 'Self-directed professional who seeks out opportunities and areas where improvements can be realized', 'Identify, design, and implement internal process improvements: automation of manual processes, optimization of data delivery methods, re-design of infrastructure for greater scalability, etc.', 'Reporting Services (SSRS)Creating simple and complex reports in SSRS, including matrix / pivot outputs, charts and graphs, Drill Through and Drill Down', 'Energy', 'Essential Job Responsibilities', 'Create and maintain optimal data pipeline architectureAssemble large, complex data sets that meet functional / non-functional business requirementsIdentify, design, and implement internal process improvements: automation of manual processes, optimization of data delivery methods, re-design of infrastructure for greater scalability, etc.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQLBuild analytics tools that utilize the data pipeline to provide actionable insights into customer business usage and needs, operational efficiency and other key business performance metricsProvide documentation of solution approaches and processesWork with key stakeholders to strive for greater functionality in our data systemsServe as a Subject Matter Expert (SME) regarding data architecture, as well as proper data collection and storage methods', 'Work Environment', 'Flexibility', 'Troubleshooting', 'Views', 'Azure experience is preferred but not required.', 'Experience developing dashboards and other macro-level outputs using Power BI required.', 'Excellent communication skills; ability to express complex ideas in writing at appropriate level of detail for intended audience; fluent in English', 'Learning Ability', 'Team player, willing to engage within and across the organization', 'Passionate about technology, automation, and eliminating human toil', 'Integrity', 'Results-Driven', 'Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL', 'Listening', 'Attention to Detail', 'Job Description:', 'Extracting, Transforming, and Loading (ETL) data from MS Excel, flat files, or other sources to SQL', 'Position requires close vision.', 'Analysis', 'Performance tuning', 'Integration Services (SSIS)Extracting, Transforming, and Loading (ETL) data from MS Excel, flat files, or other sources to SQLReporting Services (SSRS)Creating simple and complex reports in SSRS, including matrix / pivot outputs, charts and graphs, Drill Through and Drill Down', 'Functions', 'Position requires talking and listening to communications with clients and employees.', 'BS Computer Science or related discipline, or equivalent industry experience', 'Decisiveness', 'Ability to support data needs of multiple initiatives, systems, and products concurrently in a fast-paced environment', 'It is Epiq’s policy to comply with all applicable equal employment opportunity laws by making all employment decisions without unlawful regard or consideration of any individual’s race, religion, ethnicity, color, sex, sexual orientation, gender identity or expressions, transgender status, sexual and other reproductive health decisions, marital status, age, national origin, genetic information, ancestry, citizenship, physical or mental disability, veteran or family status or any other basis protected by applicable national, federal, state, provincial or local law. Epiq’s policy prohibits unlawful discrimination based on any of these impermissible bases, as well as any bases or grounds protected by applicable law in each jurisdiction. In addition Epiq will take affirmative action for minorities, women, covered veterans and individuals with disabilities. If you need assistance or an accommodation during the application process because of a disability, it is available upon request. Epiq is pleased to provide such assistance and no applicant will be penalized as a result of such a request. Pursuant to relevant law, where applicable, Epiq will consider for employment qualified applicants with arrest and conviction records.', 'Work with key stakeholders to strive for greater functionality in our data systems', 'Assemble large, complex data sets that meet functional / non-functional business requirements', 'Extensive knowledge of data normalization concepts, Relational Database Management Systems, data warehouse systems, and star schemas', 'Integrity – ', 'Adaptability', 'Extensive experience with T-SQL, including but not limited to:TroubleshootingTriggersStored ProceduresFunctionsViewsPerformance tuning', 'Build analytics tools that utilize the data pipeline to provide actionable insights into customer business usage and needs, operational efficiency and other key business performance metrics', 'Summary', 'Create and maintain optimal data pipeline architecture', 'Qualifications', 'Position does require moderate standing or sitting.', 'Position requires no significant manual labor.Position does require moderate standing or sitting.Position requires close vision.Position requires talking and listening to communications with clients and employees.', 'Team Player', 'Development of Others', 'Provide documentation of solution approaches and processes', 'Client Focus – ', 'Integration Services (SSIS)Extracting, Transforming, and Loading (ETL) data from MS Excel, flat files, or other sources to SQL', 'TroubleshootingTriggersStored ProceduresFunctionsViewsPerformance tuning', 'Verbal & Written Communication', 'Entrepreneurial Orientation', 'Creating simple and complex reports in SSRS, including matrix / pivot outputs, charts and graphs, Drill Through and Drill Down', 'Triggers', 'Serve as a Subject Matter Expert (SME) regarding data architecture, as well as proper data collection and storage methods', 'Expert data modeler and proficient with modeling tools', 'BS Computer Science or related discipline, or equivalent industry experience5+ years as a software developer and/or SQL developer designing and building software and database solutions using SQL Server 2008R2 or newerDemonstrated experience designing and developing medium to large-scale, mission-critical database solutions, to include the following strengths: Thorough understanding of key performance indicators (KPI) Reports, and data mining structuresExpert data modeler and proficient with modeling toolsExtensive knowledge of data normalization concepts, Relational Database Management Systems, data warehouse systems, and star schemasExpertise with Index management: creation, maintenance, and tuningExtensive experience in SQL Server Business Intelligence Development Studio(BIDs):Integration Services (SSIS)Extracting, Transforming, and Loading (ETL) data from MS Excel, flat files, or other sources to SQLReporting Services (SSRS)Creating simple and complex reports in SSRS, including matrix / pivot outputs, charts and graphs, Drill Through and Drill DownExtensive experience with T-SQL, including but not limited to:TroubleshootingTriggersStored ProceduresFunctionsViewsPerformance tuningAzure experience is preferred but not required.Experience developing dashboards and other macro-level outputs using Power BI required.Excellent communication skills; ability to express complex ideas in writing at appropriate level of detail for intended audience; fluent in EnglishTeam player, willing to engage within and across the organizationAbility to support data needs of multiple initiatives, systems, and products concurrently in a fast-paced environmentSelf-directed professional who seeks out opportunities and areas where improvements can be realizedPassionate about technology, automation, and eliminating human toil', '5+ years as a software developer and/or SQL developer designing and building software and database solutions using SQL Server 2008R2 or newerDemonstrated experience designing and developing medium to large-scale, mission-critical database solutions, to include the following strengths: Thorough understanding of key performance indicators (KPI) Reports, and data mining structuresExpert data modeler and proficient with modeling toolsExtensive knowledge of data normalization concepts, Relational Database Management Systems, data warehouse systems, and star schemasExpertise with Index management: creation, maintenance, and tuning', 'Extensive experience in SQL Server Business Intelligence Development Studio(BIDs):Integration Services (SSIS)Extracting, Transforming, and Loading (ETL) data from MS Excel, flat files, or other sources to SQLReporting Services (SSRS)Creating simple and complex reports in SSRS, including matrix / pivot outputs, charts and graphs, Drill Through and Drill Down', 'Expertise with Index management: creation, maintenance, and tuning', 'Creativity', 'Physical Demands']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer ,IRIS Consulting Corporation,Greater Minneapolis-St. Paul Area,5 days ago,49 applicants,"['', 'Web Application security, SSL, Active Directory, Auth0 Familiarity Desirable', 'Expertise in languages like SQL, Python, R 2+ Years', 'RESPONSIBILITIES:', 'Analyze, design, build, query, troubleshoot and maintain data pipelines (Azure Data Factory, Snowflake, Azure SQL)', '3+ years overall Data Engineering experience', 'Investigate new technologies to improve data pipelines', 'Technical skills outlined in resume Data warehousing or data engineering experience preferred', 'Ability to gather requirements, design and execute a solution end to end', 'Translate business requirements into technical requirements to ensure solutions meet business needs', 'Bachelors degree in computer science, management information systems, or related, or equivalent experience', 'Work in agile environment', '\xa0Data & Analytics team works with business leaders across the enterprise to drive optimized strategy using data. This position contributes to clients success by building cloud-based data services for analytic solutions. You will build data pipelines that are scalable, repeatable, secure and adhere to data quality standards. Responsibilities also include evangelizing data on cloud solutions with customers, leading business and IT stakeholders through designing a robust, secure and optimized Azure architecture, and delivering the target solution. This role will involve working with customers and collaborating with internal engineering teams, development of new application functionality, as well as maintenance and support of source code for existing Azure and Snowflake data platforms using the Azure data services.', 'Preferred experience with Streaming Analytics, No SQL database (Cosmos preferred), Data Warehousing & Reporting', 'Experience using Microsoft Azure for Data and Analytics solutions (Data Lake Store, Blob, SQL, Functions, Data Factory) 3+ years', 'TOP SKILLS: Ability to build relationships with team and business partners', 'Design and develop cloud data stores (Snowflake, Azure SQL)', 'Integrate API (Services) to the data management platform', 'email resume to:\xa0jwilliams@iriscc.com', 'POSITION SUMMARY:', 'QUALIFICATIONS:']",Mid-Senior level,Contract,Marketing,Information Technology and Services,2021-03-24 13:05:10
Data Engineer Lead,Synechron,"Piscataway, NJ",1 day ago,75 applicants,"['', ' Responsibilities for Engineers for data platform:', 'Familiarity with DevOps methodologies and Continuous Integration/Continuous Delivery within a large scale data delivery environment', ' Architect, build and support the operation of our Cloud infrastructure and enterprise data platform \xa0', 'Design robust, reusable and scalable data driven solutions and data pipeline frameworks to automate the ingestion, processing and delivery of both structured and unstructured batch and real-time streaming data', 'Kind regards,', ' ', 'Sr. Associate-Recruitment', ' \xa0Collective experience in data engineering, data analysis, data warehousing, data integration or business intelligence, in a similarly sized organization. ', 'Synechron,\xa0one of the fastest-growing digital, business consulting & technology services providers, is a $500 million firm based in New York. Since inception in 2001, Synechron has been on a steep growth trajectory. With 8000+ professionals operating in 18 countries across the world with presence across the USA, Canada, UK, Europe, Asia, and the Middle East.', 'Job Description:', 'Qualifications:', 'Synechron Inc. is seeking Data Engineer Lead to join our team in Harrison, NY/Piscataway, NJ.', 'Please do visit our website:\xa0http://www.synechron.com', 'Knowledge of Linux operations including basic commands and shell scripting experience', 'Responsibilities for Engineers for data platform:', '\xa0', 'Experience building and administering big data and real-time streaming analytics architectures in cloud environments (Preferably in AWS) leveraging technologies such as Hadoop, Spark, S3, EMR, Postgres, Redshift, Airflow, and HudiExperience architecting, building and administering large-scale distributed applicationsKnowledge of Linux operations including basic commands and shell scripting experienceFamiliarity with DevOps methodologies and Continuous Integration/Continuous Delivery within a large scale data delivery environmentSoftware development experience in least three or more of following languages: Python, Scala, Node.js (Preferably Python 3)Expertise in usage of SQL for data profiling, analysis and extraction', 'Expertise in usage of SQL for data profiling, analysis and extraction', ' \xa0', 'Email: Tanu.Sardana@synechron.com', 'Tanu Sardana', 'Experience building and administering big data and real-time streaming analytics architectures in cloud environments (Preferably in AWS) leveraging technologies such as Hadoop, Spark, S3, EMR, Postgres, Redshift, Airflow, and Hudi', 'Understand and implement best practices in management of enterprise data, including master data, reference data, metadata, data quality and lineage', 'Assist in selection and integration of data related tools, frameworks and applications required to expand our platform capabilities \xa0', 'Build data APIs and data delivery services to support critical operational processes, analytical models and machine learning applications \xa0', ' Working collaboratively with other engineers, data scientists, analytics teams, and business product owners in an agile environment', '\xa0one of the fastest-growing digital, business consulting & technology services providers, is a $500 million firm based in New York. Since inception in 2001, Synechron has been on a steep growth trajectory. With 8000+ professionals operating in 18 countries across the world with presence across the USA, Canada, UK, Europe, Asia, and the Middle East.', ' Qualifications:', 'Synechron,', ' Working collaboratively with other engineers, data scientists, analytics teams, and business product owners in an agile environment Architect, build and support the operation of our Cloud infrastructure and enterprise data platform \xa0Design robust, reusable and scalable data driven solutions and data pipeline frameworks to automate the ingestion, processing and delivery of both structured and unstructured batch and real-time streaming dataBuild data APIs and data delivery services to support critical operational processes, analytical models and machine learning applications \xa0Assist in selection and integration of data related tools, frameworks and applications required to expand our platform capabilities \xa0Understand and implement best practices in management of enterprise data, including master data, reference data, metadata, data quality and lineage', 'Phone\xa0+1 848 215-7045', ' Bachelor’s degree in a technical field (e.g. Comp Science, Math, Engineering) or related experience', 'Experience architecting, building and administering large-scale distributed applications', 'Software development experience in least three or more of following languages: Python, Scala, Node.js (Preferably Python 3)']",Mid-Senior level,Contract,Information Technology,Financial Services,2021-03-24 13:05:10
Data Engineer (Remote),Latitude Inc,United States,5 days ago,Be among the first 25 applicants,"['', 'Build algorithms and prototypes.', 'Prepare data for prescriptive and predictive modeling.', 'Explore ways to enhance data quality and reliability.', 'Extensive experience with NoSQL databases.', 'Improving data quality and efficiency.', 'Essential Duties And Responsibilities', 'Conduct complex data analysis and report on results.', 'Knowledge of programming languages (e.g. Go, Java, Python)', 'Model events, notification and alerts.', 'Degree in Computer Science, IT, or similar field; a master’s is a plus', '5 years of previous experience as a data engineer or in a similar role', 'Desired Skills And Experience', 'Analyze and interpret trends and patterns related to the data.', 'Strong numerical and analytical skills', 'Transform raw data to actional data from various sources.', 'Develop analytical tools and programs.', 'Data engineering certification is a plus', 'Identify opportunities for data acquisition.', 'Analyzing raw data.', 'Collaborate with data scientists and architects on several projects.', 'Developing and maintaining datasets.', '5 years of previous experience as a data engineer or in a similar roleTechnical expertise with data models, data mining, and segmentation techniquesKnowledge of programming languages (e.g. Go, Java, Python)Extensive experience with NoSQL databases.Strong numerical and analytical skillsDegree in Computer Science, IT, or similar field; a master’s is a plusData engineering certification is a plus', 'Model queries and build templates for querying large data sets.', 'Technical expertise with data models, data mining, and segmentation techniques', 'Job Description', 'Build optimal data systems and pipelines.', 'Analyzing raw data.Developing and maintaining datasets.Improving data quality and efficiency.Model queries and build templates for querying large data sets.Model events, notification and alerts.Build optimal data systems and pipelines.Transform raw data to actional data from various sources.Analyze and interpret trends and patterns related to the data.Conduct complex data analysis and report on results.Prepare data for prescriptive and predictive modeling.Build algorithms and prototypes.Explore ways to enhance data quality and reliability.Identify opportunities for data acquisition.Develop analytical tools and programs.Collaborate with data scientists and architects on several projects.']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer/Analyst,Leidos,"Baltimore, MD",1 week ago,Be among the first 25 applicants,"['', 'Desired Skills', 'Pay Range', 'Travel', ' Expert in designing complex and semantically rich data structures.', ' Ability to create dashboards in Tableau or Webfocus using various sources like database, CSV files, XL spreadsheets etc.', ' Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.', ' Excellent communication skills and ability to convey complex topics through effective documentation as well as presentation.', ' Experience designing relational data models, analytical data models and non-relational data models for OLTP and advanced analytic applications.', 'Job Description:', ' Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements', ' Experience in PostGreSQL / Greenplum database is good to have', 'Required Education/Experience', ' Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements Able to work independently with BA and PM to gather requirements, perform data discovery, come up with data model, work with data engineers to build data pipeline, and work with developers to answer their questions. Extensively used ETL methodologies for supporting data extraction, transformations and loading processing, in a corporate-wide-ETL Solution. Expertise in Conceptual Data Modeling, Logical Data Modeling, Physical Data Modeling, Enterprise Data Warehouse Design, DataMart Design, Metadata, Data Quality, Master Data Management and Master Data/Data Governance using Erwin Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics. Ability to understand complex business processes to derive conceptual and logical data models. Lead complex discussions and engagements that may involve multiple project teams from client. Experience designing relational data models, analytical data models and non-relational data models for OLTP and advanced analytic applications. Expert in designing complex and semantically rich data structures. Worked with OLTP/data warehouse modeling, and play key role in multiple projects and act as knowledge expert (Technical, Functional). Ability to optimize and performance tune SQL queries Good data analysis, problem solving and SQL skills.', ' Ability to understand complex business processes to derive conceptual and logical data models.', ' Expertise on NoSQL data modeling using Hackolade tool for databases like MongoDB, HBASE, Hive and JSON Schema. Familiar with machine learning and advanced analytic application development. Experience working in large-scale cloud database environments is a plus. Ability to create dashboards in Tableau or Webfocus using various sources like database, CSV files, XL spreadsheets etc. Experience in PostGreSQL / Greenplum database is good to have Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards Excellent communication skills and ability to convey complex topics through effective documentation as well as presentation.', 'Clearance Level Required', ' Familiar with machine learning and advanced analytic application development.', 'Description', 'Potential For Telework', ' Ability to optimize and performance tune SQL queries', ' Extensively used ETL methodologies for supporting data extraction, transformations and loading processing, in a corporate-wide-ETL Solution.', ' Good data analysis, problem solving and SQL skills.', ' Expertise on NoSQL data modeling using Hackolade tool for databases like MongoDB, HBASE, Hive and JSON Schema.', 'External Referral Bonus', ' Experience working in large-scale cloud database environments is a plus.', ' Expertise in Conceptual Data Modeling, Logical Data Modeling, Physical Data Modeling, Enterprise Data Warehouse Design, DataMart Design, Metadata, Data Quality, Master Data Management and Master Data/Data Governance using Erwin', ' Lead complex discussions and engagements that may involve multiple project teams from client.', ' Worked with OLTP/data warehouse modeling, and play key role in multiple projects and act as knowledge expert (Technical, Functional).', ' Able to work independently with BA and PM to gather requirements, perform data discovery, come up with data model, work with data engineers to build data pipeline, and work with developers to answer their questions.', ' Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Science Engineer,BetterCloud,United States,2 days ago,Be among the first 25 applicants,"['', 'Competitive base salary', 'Google Cloud Platform Services - GCS, DataProc, Data Prep, Composer, AI Platform, BigQuery and Security features like IAM, Secret Manager', '1+ year of experience with data transformation/preparation using apache beam, wrangler or any other open source technologies. ', ' Google Cloud Platform Services - GCS, DataProc, Data Prep, Composer, AI Platform, BigQuery and Security features like IAM, Secret Manager Hadoop/Presto/Delta Lake/Hive Setup and Maintenance Familiarity with Tableau or Looker or DataStudio  Interest/awareness with Jenkins or any CI/CD pipelines to maintain Data and ML pipelines. Familiarity with container orchestration Kubernates ', ' Deep experience with Spark, Hadoop on Cloud Storage and other ETL tools ', 'Ownership of setting up process and frameworks to codify best practices -data extraction, transformations, load and data lake modeling and governance.', 'Deep experience with Spark, Hadoop on Cloud Storage and other ETL tools', 'Familiarity with Tableau or Looker or DataStudio ', 'Preferred Qualifications', 'Responsible to develop and Maintain ML operations in production.', 'Create a Machine Learning Model to solve business operations problems like pattern or anomaly detection. And also develop many other models for in app features.', 'Provide and maintain portal for data analytics, metrics and query big data to help development teams', '4+ years of experience with managing a high performance and highly scalable data pipelines', ' Data Science Engineer', '4+ years experience with data lake design and operations', ' 4+ years experience with data lake design and operations 4+ years of experience with managing a high performance and highly scalable data pipelines', 'Altitude', 'Career growth with an industry innovator', '3+ years experience Scala, Java, Python, R programming ', 'Requirements', '3+ years of experience with Scikit, TensorFlow Machine Learning frameworks', 'Experience with enterprise cloud offering in Big Data Services, Data Store options and ML/AI offering', ' Competitive base salary Full benefits package Stock Options Career growth with an industry innovator ', 'Compensation | Benefits', 'Full benefits package', 'Colleagues describe you as self-driven, fast-learning, and hardworking', 'Familiarity with container orchestration Kubernates', '1+ years of Query experience with technologies like Kafka, MySQL, ElasticSearch, BigTable and BigQuery ', 'Interest/awareness with Jenkins or any CI/CD pipelines to maintain Data and ML pipelines.', 'Responsibilities', 'State of SaaSOps', 'Work with multiple teams like development, product, security and BI to understand data sources and channel them into Data Lake.', 'Primary responsibility for soundness of database pipeline design against standards for development, security and performance', 'Hadoop/Presto/Delta Lake/Hive Setup and Maintenance', ' Primary responsibility for soundness of database pipeline design against standards for development, security and performance Ownership of setting up process and frameworks to codify best practices -data extraction, transformations, load and data lake modeling and governance. Provide and maintain portal for data analytics, metrics and query big data to help development teams Create a Machine Learning Model to solve business operations problems like pattern or anomaly detection. And also develop many other models for in app features. Responsible to develop and Maintain ML operations in production. Work with multiple teams like development, product, security and BI to understand data sources and channel them into Data Lake. Maintain Data Lake, Delta Lake, troubleshoot data source pipelines, handle scaling issues and be compliant with GDPR and Security.  Responsible for anonymizing the data and creating test and training data for Machine Learning.  Create and Maintain visual dashboards for data driven decisions. ', 'BetterCloud has offices in New York, Atlanta, and San Francisco. We are currently seeking new talent in Salt Lake City, Denver, and Austin.', 'BetterCloud is an Equal Opportunity Employer, including disabled and vets.', 'Create and Maintain visual dashboards for data driven decisions.', 'Responsible for anonymizing the data and creating test and training data for Machine Learning. ', 'Maintain Data Lake, Delta Lake, troubleshoot data source pipelines, handle scaling issues and be compliant with GDPR and Security. ', 'Stock Options']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Inspire Brands,"Atlanta, GA",4 weeks ago,31 applicants,"['', 'Skills', 'Versatile, creative temperament, ability to think out-of-the box while defining sound and practical solutions. Ability to master new skills.', '#LIIB', 'Key Responsibilities', 'BS in either Information Systems, Finance/Mathematics, or Computer Science or similar', 'Apache Airflow, Azure Data Factory experience', 'Proactive approach to problem solving with effective influencing skills.', 'Developing new and enhancing existing data processing (Data Ingest, Data Transformation, Data Store, Data Management, Data Quality) components', 'Design, develop and maintain reliable automated data solutions based on the identification, collection and evaluation of business requirements. Including but not limited to data models, database objects, stored procedures and views. Developing new and enhancing existing data processing (Data Ingest, Data Transformation, Data Store, Data Management, Data Quality) componentsSupport and troubleshoot the data environment (including periodically on call)Document technical artifacts for developed solutions', ""Inspire Brands is a multi-brand restaurant company whose portfolio includes more than 11,200 Arby’s, Buffalo Wild Wings, SONIC Drive-In, Rusty Taco and Jimmy John's locations worldwide."", 'Cloud Data Warehouse solutions experience (Snowflake, Azure DW, or Redshift); data modeling, analysis, programming', 'BS in either Information Systems, Finance/Mathematics, or Computer Science or similar5 - 8 years experience', ' Soft Skills', '4+ years professional data engineering experience focused on batch and real time data pipelines development using Spark, Python or Java; Data processing / data transformation using ETL tools, Azure Databricks platform (preferred)', 'Technical Skills', 'Hands-on Talend work experience (anyone with this skill will have an advantage over other candidates)', '5 - 8 years experience', 'Experienced in Azure Cloud Platform. (ADLS, Blob)', 'Familiar with Agile practices and methodologies', 'Document technical artifacts for developed solutions', 'Experience with a DevOps model utilizing a CI/CD tool', 'Hands-on Talend work experience (anyone with this skill will have an advantage over other candidates)Apache Airflow, Azure Data Factory experience', 'We are creating a family of brands with maverick qualities, each with their own distinct positioning, guest experience, and product offering. Our brands are diverse, distinctive, and fan favorites. In a sense, you could say we seek those who provide something different than the norm.', 'Support and troubleshoot the data environment (including periodically on call)', '4+ years professional data engineering experience focused on batch and real time data pipelines development using Spark, Python or Java; Data processing / data transformation using ETL tools, Azure Databricks platform (preferred)Cloud Data Warehouse solutions experience (Snowflake, Azure DW, or Redshift); data modeling, analysis, programmingExperience with a DevOps model utilizing a CI/CD toolExperienced in Azure Cloud Platform. (ADLS, Blob)', 'Design, develop and maintain reliable automated data solutions based on the identification, collection and evaluation of business requirements. Including but not limited to data models, database objects, stored procedures and views. ', 'Education & Experience Requirements', 'Good interpersonal skills; comfort and competence in dealing with different teams within the organization. Requires an ability to interface with multiple constituent groups and build sustainable relationships.', 'Good interpersonal skills; comfort and competence in dealing with different teams within the organization. Requires an ability to interface with multiple constituent groups and build sustainable relationships.Versatile, creative temperament, ability to think out-of-the box while defining sound and practical solutions. Ability to master new skills.Proactive approach to problem solving with effective influencing skills.Familiar with Agile practices and methodologies']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer,Amply Media,"Kansas City, KS",3 weeks ago,Be among the first 25 applicants,"['', 'Apply for this Position', ' This is an opportunity to be with an industry leading company that continues to experience tremendous growth ', ' Experience using Snowflake, Postgres, or equivalent ', ' Prioritization: Able to arrange projects in order of importance relative to each other. ', ' Support existing reporting infrastructure and troubleshoot issues related to data  Create new reports based on client requirements using metric and dimension based reporting data models.  Proactively improve reporting workflows and automate manual processes throughout the business for higher efficiency, robustness, and speed.  Support the development of new products and services via ingestion, processing, and formatting data for reporting and analytics.  Interact face-to-face with business stakeholders, develop cooperative relationships, and acquire domain knowledge of the business.  Design and implement effective database models to store and retrieve company data ', ' Team Player: Demonstrates a strong ability to support department staff members and managers; able to establish collaborative relationships with peers. ', 'Company Overview', ' Create new reports based on client requirements using metric and dimension based reporting data models. ', ' Design and implement effective database models to store and retrieve company data ', ' Experience with developing, executing and maintaining complex SQL queries against multiple databases is required. ', ' Communication (written and oral): Able to successfully communicate product recommendations based on design knowledge ', ' Thank You ', ' Located on the Country Club Plaza- next to world class shopping and restaurants ', ' Desire to work with a team of people solving complex problems and sharing knowledge freely ', ' Adaptability: Capable of adjusting to changing priorities in a fast-paced environment.  Prioritization: Able to arrange projects in order of importance relative to each other.  Passion for Technology: Excitement for new technology, bleeding edge applications, and a positive attitude towards solving real world challenges.  Creativity: Research the latest trends to design competitive ad units.  Resourceful: Looks for ways to achieve goals with available resources.  Team Player: Demonstrates a strong ability to support department staff members and managers; able to establish collaborative relationships with peers.  Communication (written and oral): Able to successfully communicate product recommendations based on design knowledge ', ' We offer a fun, work hard – play hard culture  We have shuffleboard, virtual reality, a retro gaming console and a beer fridge  No dress code policy! Wear your flip flops and shorts in the summer  Complimentary snacks and beverages as well as catered lunches  Located on the Country Club Plaza- next to world class shopping and restaurants  Fun social events and perks! - check out our Instagram to see more  This is an opportunity to be with an industry leading company that continues to experience tremendous growth  Great Place to Work Certified and named Ingram’s Best Companies To Work For in Kansas City ', ' Passion for Technology: Excitement for new technology, bleeding edge applications, and a positive attitude towards solving real world challenges. ', ' Experience with visualization and reporting tools, such as Tableau, PowerBI and Looker. ', ' Support existing reporting infrastructure and troubleshoot issues related to data ', 'Responsibilities', 'The Successful Candidate Will Also Demonstrate The Following Abilities', 'Qualifications', ' Complimentary snacks and beverages as well as catered lunches ', ' Experience with developing, executing and maintaining complex SQL queries against multiple databases is required.  Experience using Snowflake, Postgres, or equivalent  Experience with visualization and reporting tools, such as Tableau, PowerBI and Looker.  Desire to work with a team of people solving complex problems and sharing knowledge freely  Passion for data management and data technologies ', ' Support the development of new products and services via ingestion, processing, and formatting data for reporting and analytics. ', ' Creativity: Research the latest trends to design competitive ad units. ', ' No dress code policy! Wear your flip flops and shorts in the summer ', ' Interact face-to-face with business stakeholders, develop cooperative relationships, and acquire domain knowledge of the business. ', ' Great Place to Work Certified and named Ingram’s Best Companies To Work For in Kansas City ', 'By submitting your interest in this job, you agree to receive text notifications with additional steps to complete your job application. You will receive up to 6 messages from the number ""63879"". Message & data rates may apply. Please refer to our privacy policy for more information.', ' We offer a fun, work hard – play hard culture ', ' Adaptability: Capable of adjusting to changing priorities in a fast-paced environment. ', ' Fun social events and perks! - check out our Instagram to see more ', ' Proactively improve reporting workflows and automate manual processes throughout the business for higher efficiency, robustness, and speed. ', ' Resourceful: Looks for ways to achieve goals with available resources. ', ' Passion for data management and data technologies ', ' We have shuffleboard, virtual reality, a retro gaming console and a beer fridge ']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer/Wrangler,Kaiser Permanente,"Greenwood Village, CO",1 month ago,Be among the first 25 applicants,"['', ""Master's degree in Computer Science, CIS, or related field."", 'Ensures quality metrics are tracked across testing phases (e.g., SIT, Performance, UAT, Automation, Production, Validation).', 'Three (3) years experience working with IT vendors.', 'Defines and ensures adherence with entry and exit criteria according to Kaiser testing standards.', 'Minimum Qualifications', 'Three (3) years experience writing technical documentation in a software testing or quality assurance environment.', 'Oversees and addresses critical issues, dependencies, and risks related to testing.', 'Quality Assurance Institute (QAI) or American Society for Quality (ASQ) or similar certification.', 'Preferred Qualifications', 'Leads the development of quality assurance (QA) test project strategies, methodologies, and standard processes for large-scale, complex IT initiatives spanning all QA domains by analyzing business and technology requirements to ensure testability and traceability, and determining testing scope and approach.', 'Initiates and evaluates required business process improvements in order to achieve business results and appropriate solutions for customers.', 'Minimum two (2) years in a leadership role working with technical teams.', 'Two (2) years of work experience in a role requiring interaction with senior leadership (e.g., Director level and above)', 'Reviews and signs off on testing scope and approach, and partners with cross-functional IT and business stakeholders to review and approve the overall testing approach.', ""Minimum two (2) years SQA experience working across multiple IT domains or business units in a corporate setting.Minimum two (2) years in a leadership role working with technical teams.Bachelor's degree in Computer Science, CIS, or related field and Minimum eight (8) years experience in SQA, software testing or related field. Additional equivalent work experience may be substituted for the degree requirement."", ""Two (2) years of work experience in a role requiring interaction with senior leadership (e.g., Director level and above)Three (3) years experience in a leadership role of a large matrixed organization.Three (3) years experience working with IT vendors.Four (4) years experience working on project(s) involving the implementation of software development life cycle(s) (SDLC).Three (3) years experience writing technical documentation in a software testing or quality assurance environment.Four (4) years experience working with integration technologies (e.g., web services, MQ).Two (2) years experience working in and defining the requirements for virtual testing environments.Two (2) years experience working in and defining the requirements for healthcare testing environments.Two (2) years experience testing and defining the requirements testing business system applications.Two (2) years experience working with and defining the requirements for data mart/data warehousing.Master's degree in Computer Science, CIS, or related field.Quality Assurance Institute (QAI) or American Society for Quality (ASQ) or similar certification."", 'Conducts or oversees business-specific projects by applying deep expertise in subject area; promoting adherence to all procedures and policies; developing work plans to meet business priorities and deadlines; determining and carrying out processes and methodologies; coordinating and delegating resources to accomplish organizational goals; partnering internally and externally to make effective business decisions; solving complex problems; escalating issues or risks, as appropriate; monitoring progress and results; recognizing and capitalizing on improvement opportunities; evaluating recommendations made; and influencing the completion of project tasks by others.', 'Essential Responsibilities', 'Two (2) years experience testing and defining the requirements testing business system applications.', 'Two (2) years experience working with and defining the requirements for data mart/data warehousing.', 'Four (4) years experience working on project(s) involving the implementation of software development life cycle(s) (SDLC).', 'Develops guidelines and best practices to ensure test plans and timelines are aligned with project/program milestones.', 'Description', 'Develops quality assurance (QA) project plans for moderately to highly complex projects by identifying project scope, work plans, schedules, milestones, and critical paths, and ensuring proper staffing.', 'Manages the development test scenarios and execution of test cases across all testing phases (e.g., SIT, Performance, UAT, Automation, Production, Validation).', 'Minimum two (2) years SQA experience working across multiple IT domains or business units in a corporate setting.', 'Reviews project status and milestones reports, provides justification for and first-level authorization for exceptions and waivers, and meets with IT and business stakeholders to address contingency plans, as appropriate.', 'Generates scheduled reports (e.g., test execution, defects, ad hoc reports) and provides daily test execution metrics to IT teams and management, as appropriate.', 'Four (4) years experience working with integration technologies (e.g., web services, MQ).', 'Three (3) years experience in a leadership role of a large matrixed organization.', 'Two (2) years experience working in and defining the requirements for healthcare testing environments.', ""Bachelor's degree in Computer Science, CIS, or related field and Minimum eight (8) years experience in SQA, software testing or related field. Additional equivalent work experience may be substituted for the degree requirement."", 'Two (2) years experience working in and defining the requirements for virtual testing environments.', 'Practices self-leadership and promotes learning in others by building relationships with cross-functional stakeholders; communicating information and providing advice to drive projects forward; influencing team members within assigned unit; listening and responding to, seeking, and addressing performance feedback; adapting to competing demands and new responsibilities; providing feedback to others, including upward feedback to leadership and mentoring junior team members; creating and executing plans to capitalize on strengths and improve opportunity areas; and adapting to and learning from change, difficulties, and feedback.', 'Conducts or oversees business-specific projects by applying deep expertise in subject area; promoting adherence to all procedures and policies; developing work plans to meet business priorities and deadlines; determining and carrying out processes and methodologies; coordinating and delegating resources to accomplish organizational goals; partnering internally and externally to make effective business decisions; solving complex problems; escalating issues or risks, as appropriate; monitoring progress and results; recognizing and capitalizing on improvement opportunities; evaluating recommendations made; and influencing the completion of project tasks by others.Practices self-leadership and promotes learning in others by building relationships with cross-functional stakeholders; communicating information and providing advice to drive projects forward; influencing team members within assigned unit; listening and responding to, seeking, and addressing performance feedback; adapting to competing demands and new responsibilities; providing feedback to others, including upward feedback to leadership and mentoring junior team members; creating and executing plans to capitalize on strengths and improve opportunity areas; and adapting to and learning from change, difficulties, and feedback.Leads the development of quality assurance (QA) test project strategies, methodologies, and standard processes for large-scale, complex IT initiatives spanning all QA domains by analyzing business and technology requirements to ensure testability and traceability, and determining testing scope and approach.Oversees and addresses critical issues, dependencies, and risks related to testing.Ensures quality assurance projects are appropriately staffed, work plans are followed, and milestones are met.Reviews and signs off on testing scope and approach, and partners with cross-functional IT and business stakeholders to review and approve the overall testing approach.Manages the development test scenarios and execution of test cases across all testing phases (e.g., SIT, Performance, UAT, Automation, Production, Validation).Ensures quality metrics are tracked across testing phases (e.g., SIT, Performance, UAT, Automation, Production, Validation).Manages the review and validation of testable business processes, test data, and test environment requirements.Develops quality assurance (QA) project plans for moderately to highly complex projects by identifying project scope, work plans, schedules, milestones, and critical paths, and ensuring proper staffing.Develops guidelines and best practices to ensure test plans and timelines are aligned with project/program milestones.Defines and ensures adherence with entry and exit criteria according to Kaiser testing standards.Reviews project status and milestones reports, provides justification for and first-level authorization for exceptions and waivers, and meets with IT and business stakeholders to address contingency plans, as appropriate.Generates scheduled reports (e.g., test execution, defects, ad hoc reports) and provides daily test execution metrics to IT teams and management, as appropriate.Initiates and evaluates required business process improvements in order to achieve business results and appropriate solutions for customers.', 'Manages the review and validation of testable business processes, test data, and test environment requirements.', 'Ensures quality assurance projects are appropriately staffed, work plans are followed, and milestones are met.']",Entry level,Full-time,Information Technology,Financial Services,2021-03-24 13:05:10
Data Engineer,Deloitte,"Austin, TX",4 weeks ago,44 applicants,"['', 'The team', ' Strong data & logical analysis skills', ' Drive operational efficiency by maintaining their data ecosystems, sourcing analytics expertise and providing As-a-Service offerings for continuous insights and improvements', ' 5+ years of experience in Core JAVA and SQL', ' Must be willing to live and work in the Greater Austin, TX area (preferred) or San Jose, California. Relocation assistance provided to qualifying candidates', 'Data Engineer with MapReduce- Project Delivery Specialist', ' 3+ years of experience in building scalable and high-performance data pipelines using Apache Hadoop, Apache Spark, Pig or Hive', 'Recruiter tips', 'Core JAVA', ' Spark', ' Implement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to generate insights leveraging cloud-based platforms', ' Support the implementation of data integration requirements and develop the pipeline of data from raw to curation layers including the cleansing, transformation, derivation and aggregation of data.', 'Required', ' Support in the development of technical solutions to business problems', 'Deloitte’s culture', 'SQL', 'Corporate citizenship', 'Optional / Nice to Have ', ' Hands on big data/ Hadoop performance tuning and optimization experience', ' Leverage automation, cognitive and science-based techniques to manage data, predict scenarios and prescribe actions', "" Bachelor's degree, preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience"", ' Communicate effectively (written and spoken) and work with the multi-location development teams and self-manage own work', 'Pig ', ' 3+ years or experience in Python / Unix Shell Scripting', 'Qualifications', 'Hive', 'Benefits', ' Limited immigration sponsorship may be available', 'Additional Requirements', ' Strong SQL knowledge with ability to work with the latest database technologies.', ' Travel up to 25% (While 25% of travel is a requirement of the role, due to COVID-19, non-essential travel has been suspended until further notice.)', ' Experience in Map Reduce is a plus', ' 5+ years of hands-on experience as a Data Engineer or Big Data developer', ' Experience with bigdata cross platform compatible file formats like Apache Avro & Apache Parquet', 'Work you’ll do', 'How You’ll Grow']",Not Applicable,Full-time,Management,Accounting,2021-03-24 13:05:10
Data Engineer,Wimmer Solutions,"Sunnyvale, CA",2 weeks ago,133 applicants,"['', 'Expertise in data modeling for time series, spatial, and image data for analytic and operational use cases', 'Experience crafting data systems to support machine learning and robotics applications', ' Experience working with high dimensional data: images, videos, point clouds, etc. Experience crafting data systems to support machine learning and robotics applications Experienced in data mining and visualization of large data sets Experience developing on Kubernetes based systems ', 'Required Professional Skills & Experience', 'Experienced in data mining and visualization of large data sets', 'Help enable our users to find their data! Develop best practices for data access and queries.', 'Strong Python programmer', 'Experience working with high dimensional data: images, videos, point clouds, etc.', 'Work with product, ML scientists, roboticists, and software engineers to build a data platform that supports development of Intelligent Machines', 'Data lifecycle management experience', ' Design and build updates to our data solutions supporting robotics and machine learning development cycle Develop and architect enhanced systems to enable rapid retrieval of imagery and time series data Promote standard methodologies in data modeling, storage, and processing Assess, benchmark and select new technologies to be added to the digital product portfolio. Help enable our users to find their data! Develop best practices for data access and queries. Work with product, ML scientists, roboticists, and software engineers to build a data platform that supports development of Intelligent Machines ', 'Role Responsibilities', 'Experience developing on Kubernetes based systems', 'Self-motivated, ability to work both independently and in team environments', 'Bachelor’s Degree in Computer Science or related technical subject area', 'Design and build updates to our data solutions supporting robotics and machine learning development cycle', 'Preferred Skills & Experience', 'Experience with infrastructure as code, such as Terraform', 'Assess, benchmark and select new technologies to be added to the digital product portfolio.', 'Experience with a diversity of datastores such as Redshift, Dynamo, Athena, Mongo, Postgres', 'Develop and architect enhanced systems to enable rapid retrieval of imagery and time series data', 'More About Wimmer Solutions', 'Data EngineerJob ID: 18920Position Description', 'Experience developing ETL in a microservice architecture', 'Excellent communicator', 'Promote standard methodologies in data modeling, storage, and processing', ' Expertise in data modeling for time series, spatial, and image data for analytic and operational use cases Strong Python programmer Experience with a diversity of datastores such as Redshift, Dynamo, Athena, Mongo, Postgres Experience developing ETL in a microservice architecture Data lifecycle management experience Experience with infrastructure as code, such as Terraform Self-motivated, ability to work both independently and in team environments Excellent communicator Bachelor’s Degree in Computer Science or related technical subject area ']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,ROSEN,"Columbus, OH",2 weeks ago,Be among the first 25 applicants,"['', ' Development or application of suitable tools for data access ', 'University degree in computer science, commercial information technology, physics or equivalent', ' Definition of data requirements together with Data Scientists ', 'University degree in computer science, commercial information technology, physics or equivalent5+ years relevant work experience in the field of data engineering', ' Successfully completed a degree in computer science or other area with strong programming background ', ' Knowledge of industry wide technology trends and best practices ', ' Maintenance and continuous improvement of machine learning models ', ' Passion for data and for the development of tools to master them ', 'OUR OFFER', ' Mastering of a programming language like python and database systems ', ' Provision of data sources for consumers in the form of interfaces (APIs) ', ' Visit of national and international conferences on a regular basis ', 'Qualification Or Skills', 'Responsibilties', 'Requirements', ' Design of a big data infrastructure and big data platform (storage, formats, interfaces, etc.) ', 'New Team, New Office! Protect Our Environment By Your Algorithms', ' Good teamwork skills in interdisciplinary teams ', 'Description', 'ROSEN USA offers an exceptional working environment, salary commensurate with experience and incredible benefits package.', 'Education and Experience:', 'New Challenge:', ' Design of a big data infrastructure and big data platform (storage, formats, interfaces, etc.)  Creation of ETL pipelines to validate, enrich and persist data  Provision of data sources for consumers in the form of interfaces (APIs)  Fusion of extremely structured, semi-structured and unstructured data sources(e.g. historical data, metadata, simulation data, etc.)  Development or application of suitable tools for data access  Maintenance and continuous improvement of machine learning models  Definition of data requirements together with Data Scientists  Visit of national and international conferences on a regular basis ', ' Creation of ETL pipelines to validate, enrich and persist data ', ' Fusion of extremely structured, semi-structured and unstructured data sources(e.g. historical data, metadata, simulation data, etc.) ', 'To become part of the ROSEN family', ' Good grasp of containerization technology and Linux ', '5+ years relevant work experience in the field of data engineering', ' Successfully completed a degree in computer science or other area with strong programming background  Mastering of a programming language like python and database systems  Good grasp of big data infrastructures and relevant big data tools  Passion for data and for the development of tools to master them  Good grasp of containerization technology and Linux  Knowledge of industry wide technology trends and best practices  Good teamwork skills in interdisciplinary teams ', 'Be part of a new team, shape the team and your future and enjoy working in a multi-cultural and multi-national team. ', ' Good grasp of big data infrastructures and relevant big data tools ']",Entry level,Full-time,Information Technology,Electrical/Electronic Manufacturing,2021-03-24 13:05:10
Associate Data Engineer,SalesPage Technologies,"Kalamazoo, MI",1 week ago,109 applicants,"['', 'Familiarity with relational databases, including SQL and use of database objects such as views, stored procedures, and functions.', 'For over 30 years, SalesPage has been working with some of the largest and most respected asset management firms in the nation to solve business challenges with industry-leading software solutions.\xa0We\xa0are headquartered in\xa0downtown Kalamazoo, Michigan,\xa0in the Foundry Building, a state-of-the-art facility for tech companies. We’re looking for intelligent and passionate individuals to join our team and help continue our advancement towards being the most respected and in-demand distribution management partner in the industry.\xa0\xa0\xa0', 'We are interested in every qualified candidate who is eligible to work in the United States. However, we are not able to sponsor visas.', 'Full SDLC for data processing jobs, including ETL, on-demand volume processing, and analytics.Using and supporting technology used for implementing data processing jobs.Optimizing performance of existing batch processing jobs.Innovating on new data processing techniques to improve accuracy, including data science techniques.', 'Associate Data Engineers at SalesPage are responsible for:', 'Innovating on new data processing techniques to improve accuracy, including data science techniques.', '0-5 years of experience in work related to the position.', 'The ideal Data Engineer candidate will have the following qualifications:', 'Associate Data Engineer', '\xa0', 'Experience working and processing large datasets, and techniques for working with data in volume effectively.', 'Familiarity using a source control tool such as Subversion or Git.', 'SalesPage provides solutions for asset managers focusing on data collection, management, aggregation, and analysis as well as industry-specific CRM capability.\xa0SalesPage functions as a hub within our client’s data architecture, serving as a center point where data is collected and distributed to other systems, such as BI platforms and Salesforce.\xa0We help asset managers enter the world of “big data” to find the best way to connect their investment products to those who will benefit from them most.', 'Using and supporting technology used for implementing data processing jobs.', 'Optimizing performance of existing batch processing jobs.', 'SalesPage provides opportunity for developers to expand and explore other skillsets as part of a client-focused team.\xa0Developers at SalesPage work as part of that team and are given a lot of independence and responsibility to fulfill their role in the team.\xa0SalesPage is an expanding company that is constantly innovating with new techniques and technologies to provide the best capability for our clients, and developers at SalesPage are responsible for that innovation.\xa0New employees will contribute to that growth and innovation.', 'We are interested in every qualified candidate who is eligible to work in the United States. However, we are not able to sponsor visas.\xa0', 'Full SDLC for data processing jobs, including ETL, on-demand volume processing, and analytics.', 'Experience with Java or Groovy.', 'Bachelor’s degree in computer science, data science, data analytics, or a related field.', 'Bachelor’s degree in computer science, data science, data analytics, or a related field.0-5 years of experience in work related to the position.Familiarity with relational databases, including SQL and use of database objects such as views, stored procedures, and functions.Experience working and processing large datasets, and techniques for working with data in volume effectively.Experience with Java or Groovy.Familiarity using a source control tool such as Subversion or Git.', 'About SalesPage', 'We use a Java EE web-based solution to provide an application for users to manage and view data, and many Java-based data processing, data analysis, and ETL jobs. We provide both software for our clients and a SaaS solution in the AWS cloud.\xa0All developers at SalesPage are full-stack developers, familiar with web application capabilities and development, implementing business logic around client data, using a relational database, and mass data processing techniques.\xa0Data Engineers are focused on the mass data processing and relational database usage components of the stack. This is a junior-level position.']",Associate,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Data Engineer - Cloud,Infoblox,"Santa Clara, CA",1 day ago,Be among the first 25 applicants,"['', ' MS or BS in Computer Science or a related field. ', ' Collaborate on design, implementation, and deployment of applications with the rest of data engineering.', ' Software development experience with Python 2/3, Java, GoLang or Scala.', 'Data Engineer Intern - Cloud Engineering ', ' Design and implement mechanisms to monitor the functionality of a data abstraction layer and report out performance metrics as well as outages. Convert prototypes into production data engineering solutions through disciplined software engineering practices, Spark optimizations, and modern deployment pipelines. Collaborate on design, implementation, and deployment of applications with the rest of data engineering. Create test plans and run manual, functional and unit tests for developed code. Develop technical documentation as needed.', 'Education', ' High tolerance for ambiguity – able to convert rough ideas into functioning solutions.', 'Requirements', ' Software development experience with Python 2/3, Java, GoLang or Scala. Basic familiarity with cloud service providers such as AWS or Azure. Basic understanding of object-oriented design and S.O.L.I.D. principles. Solid understanding of big data concepts and data-lakes. Experience with containers; Docker & Kubernetes. High tolerance for ambiguity – able to convert rough ideas into functioning solutions.', 'Description', ' Design and implement mechanisms to monitor the functionality of a data abstraction layer and report out performance metrics as well as outages.', 'https://www.youtube.com/watch?v=6qCY9tU8wL0&feature=youtu.be', 'Responsibilities', ' Solid understanding of big data concepts and data-lakes.', 'Infoblox, where I belong', ' Basic understanding of object-oriented design and S.O.L.I.D. principles.', ' Convert prototypes into production data engineering solutions through disciplined software engineering practices, Spark optimizations, and modern deployment pipelines.', ' Basic familiarity with cloud service providers such as AWS or Azure.', ' Experience with containers; Docker & Kubernetes.', ' Develop technical documentation as needed.', ' Create test plans and run manual, functional and unit tests for developed code.']",Not Applicable,Full-time,Project Management,Computer Software,2021-03-24 13:05:10
Data Engineer (remote),Solugenix,"Irvine, CA",2 weeks ago,134 applicants,"['', 'Drive internal process improvements and automating manual processes for data quality and SLA management.', 'Strong analytic skills related to working with structured and unstructured datasets.', 'Certification as an AWS, Azure, or Google Solutions Architect. Cloud Security Administrator a big plus.', 'Partner with the Data Scientist, Web Developer, and Data Warehouse team to render structure and non-structured data through the web for clinicians to evaluate.', 'Design and manage data pipelines between on-prem and cloud repositories.', 'Solid understanding of data flows through web APIs and web services.', 'Data Engineer - direct hire opportunity within healthcare insurance industryIrvine, CAJob ID 2021-5137', 'Control cloud environments in accordance with company security guidelines (Ex. HIPAA/HITECH).', 'Experience with Python a huge plus.', 'Bachelor’s degree in Computer Science or Engineering; Master’s degree a plus.', 'Recommend improvements and modifications to exist data and ETL pipelines.', 'Knowledge of web-services, API, REST, and RPC.', 'Experience with data pipeline and workflow management tools.', 'Manage cloud data pipelines, ELT/ETL cloud processes for AI/ML, and Power BI reporting', 'Develop cloud-based data repositories to store structured and non-structured data (x-rays).', 'Bachelor’s degree in Computer Science or Engineering; Master’s degree a plus.Certification as an AWS, Azure, or Google Solutions Architect. Cloud Security Administrator a big plus.3-5 years of experience in a Cloud Data Engineer role or related position.Experience with SOA applications and cloud-based services.Experience with open-source technology such as Python and R.Knowledge of web-services, API, REST, and RPC.Database experience, including knowledge of SQL and NoSQL, and related data stores such as Postgres.Strong awareness of networking and internet protocols, including TCP/IP, DNS, SMTP, HTTP, and distributed networks.Solid understanding of data flows through web APIs and web services.Experience with Source Control and DevOps is highly desired.', 'Advanced working SQL knowledge and experience working with relational databases.', 'Experience with Source Control and DevOps is highly desired.', 'Experience with open-source technology such as Python and R.', 'Strong awareness of networking and internet protocols, including TCP/IP, DNS, SMTP, HTTP, and distributed networks.', 'Responsibilities', 'Build product-focused datasets and scalable, fault-tolerant pipelines.', 'Qualifications', 'Experience with SOA applications and cloud-based services.', '3-5 years of experience in a Cloud Data Engineer role or related position.', 'Able to immerse yourself in all aspects of the product, understand the problems, and tie them back to data engineering solutions.', 'Define and own the data engineering roadmap for our Artificial Intelligence and Data Science initiatives.', 'Capture clinician’s feedback and store it on a cloud repository.', 'Database experience, including knowledge of SQL and NoSQL, and related data stores such as Postgres.', 'Communicate and influence strategies and processes around data modeling and architecture to multi-functional groups and leadership.', 'Define and own the data engineering roadmap for our Artificial Intelligence and Data Science initiatives.Partner with the Data Scientist, Web Developer, and Data Warehouse team to render structure and non-structured data through the web for clinicians to evaluate.Capture clinician’s feedback and store it on a cloud repository.Design and manage data pipelines between on-prem and cloud repositories.Advanced working SQL knowledge and experience working with relational databases.Build product-focused datasets and scalable, fault-tolerant pipelines.Able to immerse yourself in all aspects of the product, understand the problems, and tie them back to data engineering solutions.Recommend improvements and modifications to exist data and ETL pipelines.Communicate and influence strategies and processes around data modeling and architecture to multi-functional groups and leadership.Drive internal process improvements and automating manual processes for data quality and SLA management.Strong analytic skills related to working with structured and unstructured datasets.Experience with data pipeline and workflow management tools.Manage cloud data pipelines, ELT/ETL cloud processes for AI/ML, and Power BI reportingExperience with Python a huge plus.Develop cloud-based data repositories to store structured and non-structured data (x-rays).Control cloud environments in accordance with company security guidelines (Ex. HIPAA/HITECH).']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,ICE,"Atlanta, GA",1 week ago,Be among the first 25 applicants,"['', '  Bachelor’s degree in computer science, engineering, or related field   2+ years of relevant work experience   Master’s degree in computer science, engineering, or related field is preferred   Experience in data analysis, data cleansing, and data transformation (parsing, mapping, and serialization processes)   Scripting language like python or similar language   Proficiency in SQL and Basic Linux system processing with shell scripting   Prior experience working with large data volumes and performance tuning is a preferred   Experience with data gathering, cleaning and transforming techniques   Experience and understanding of database architecture, and transformation tools   Understanding of Data Warehousing & ETL/ELT techniques   Experience working with development teams, business teams, and database administrators, within agile methodology   Strong communication and problem-solving skills   Self-starter with proven ability and initiative to learn and research new concepts, ideas, and technologies  ', ' Job Type Standard ', 'Knowledge And Experience', ' Proficiency in SQL and Basic Linux system processing with shell scripting ', '  Job Type Standard   Schedule Full-time', ' Scripting language like python or similar language ', ' Strong communication and problem-solving skills ', ' Document data collection, storage and reporting. ', ' Understanding of Data Warehousing & ETL/ELT techniques ', ' Troubleshoot production issues quickly and efficiently. ', ' Create and maintain ETL jobs to populate storage farm and database loads. ', ' Develop various internal and external reports from a Data Warehouse. ', ' Learn and develop understanding of source data and transformed datasets. ', ' Prior experience working with large data volumes and performance tuning is a preferred ', ' Experience working with development teams, business teams, and database administrators, within agile methodology ', ' Self-starter with proven ability and initiative to learn and research new concepts, ideas, and technologies ', ' Experience in data analysis, data cleansing, and data transformation (parsing, mapping, and serialization processes) ', ' Bachelor’s degree in computer science, engineering, or related field ', 'Responsibilities', ' Experience with data gathering, cleaning and transforming techniques ', ' Performance tune ETL processes as well as queries to extract information from the databases ', ' Support our team in the daily and weekend operational work and ad-hoc analysis and reports for business users. ', ' Master’s degree in computer science, engineering, or related field is preferred ', ' Integrate new data management technologies and software engineering tools into existing structures and processes. ', ' Analyze large datasets, including csv files and tables in database. ', ' Schedule Full-time', ' Experience and understanding of database architecture, and transformation tools ', 'Job Purpose ', '  Create and maintain ETL jobs to populate storage farm and database loads.   Develop various internal and external reports from a Data Warehouse.   Integrate new data management technologies and software engineering tools into existing structures and processes.   Learn and develop understanding of source data and transformed datasets.   Analyze large datasets, including csv files and tables in database.   Performance tune ETL processes as well as queries to extract information from the databases   Troubleshoot production issues quickly and efficiently.   Support our team in the daily and weekend operational work and ad-hoc analysis and reports for business users.   Document data collection, storage and reporting.  ', ' 2+ years of relevant work experience ']",Not Applicable,Full-time,Quality Assurance,Financial Services,2021-03-24 13:05:10
Data Engineer,Believe,"Brooklyn, NY",2 weeks ago,Be among the first 25 applicants,"['', 'Matillion / Fivetran / MDM', 'Be comfortable working in a complex data environment involving in-depth data discovery, exploration and troubleshooting.', 'Qualifications: ', 'Comprehensive knowledge of data-modeling principles specifically with a focus on data analytics', 'Strong ETL proficiency using GUI-based tools or code-based patterns.', 'Have a thorough understanding of building data pipelines for RDBMS systems and various cloud-based data sources.', 'AWS', 'Experience with software engineering practicesMatillion / Fivetran / MDMAWSSnowflakeTableau', 'Be highly proficient in SQL & Python with an ability to work with large and highly complex datasets', 'Experience with software engineering practices', 'Excellent communication and interpersonal skills with the ability to liaise with multiple teams.', 'Candidates Should', 'TuneCore is seeking a seasoned Data Engineer who thrives in a dynamic and fast-paced environment and is able to work alongside our Software Engineering, Dev-Ops and Data Analytics teams to provide impactful solutions for all our data-driven business needs. ', 'Ensure adequate and thorough documentation of all existing and new processes.', 'Must have at least 5 years of demonstrable work experience as a Data Engineer or Data Architect in an enterprise environment.', 'Must have at least 5 years of demonstrable work experience as a Data Engineer or Data Architect in an enterprise environment.In-depth knowledge of RDBMS systems such as MySQL, MariaDB, PostgreSQL as well as cloud-based data warehousing solutions such as RedShift & Snowflake.Comprehensive knowledge of data-modeling principles specifically with a focus on data analyticsStrong ETL proficiency using GUI-based tools or code-based patterns.Excellent communication and interpersonal skills with the ability to liaise with multiple teams.', 'Be able to apply their skills in ingenious ways to solve new and existing problems.', 'Highly Desired Skills', 'Tableau', 'Be open to exploring and learning new technologies on the go.', 'Snowflake', 'Qualifications', 'Be highly proficient in SQL & Python with an ability to work with large and highly complex datasetsHave a thorough understanding of building data pipelines for RDBMS systems and various cloud-based data sources.Be comfortable working in a complex data environment involving in-depth data discovery, exploration and troubleshooting.Be able to apply their skills in ingenious ways to solve new and existing problems.Be open to exploring and learning new technologies on the go.Have the desire to lead and own the data engineering component of our new enterprise cloud-based data architecture.Be comfortable working in a cross-functional responsibility.Ensure adequate and thorough documentation of all existing and new processes.', 'In-depth knowledge of RDBMS systems such as MySQL, MariaDB, PostgreSQL as well as cloud-based data warehousing solutions such as RedShift & Snowflake.', 'Job Description', 'Be comfortable working in a cross-functional responsibility.', 'Have the desire to lead and own the data engineering component of our new enterprise cloud-based data architecture.']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer,Kinesso,"New York, United States",1 week ago,174 applicants,"['', 'Build and manage the infrastructure required for optimal automated extraction, transformation, and loading of data from a wide variety of data sources using SQL, Snowflake, and AWS ‘big data’ technologies, for use in speculative machine learning and AI applications.', 'Create and maintain optimal data pipeline architecture, assemble large, complex data sets that meet functional / non-functional business requirements.', 'Experience designing, programming, testing, and maintaining reliable connections to RESTful APIs\xa0', 'The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. They must be self-directed, comfortable working in the presence of ambiguity, and confident enough to respond to ambiguity by making a reasoned judgment, trying an optimal approach, evaluating the success or failure of that approach, and trying again if necessary.', 'In pursuing its mission, the Research & Development Team constantly identifies new data sources from both inside and outside the existing business and incorporates them into local platforms for use in its projects.\xa0The R&D Data Engineer will be responsible for efficiently architecting, building, and maintaining (1) the data infrastructure to enable the R&D Team’s machine learning and data science specialists to build applications that use this data, and (2) the application infrastructure to optimize the R&D process and eventual handoff of R&D tools for full productization elsewhere in the company.\xa0', 'Integrate R&D Team tools with internal and external platforms via API, FTP, and other means.', 'Experience with reinforcement learning environments a bonus', '\xa0', 'Desired Skills & Experience ', 'R&D Data Engineer', 'Experience with cloud and container systems including AWS cloud services: EC2, EMR, RDS, S3, Redshift; Docker and Kubernetes', 'Experience with cloud and container systems including AWS cloud services: EC2, EMR, RDS, S3, Redshift; Docker and KubernetesExperience with object-oriented/object function scripting languages: Python required; C++ a bonus, etc.Experience designing, programming, testing, and maintaining reliable connections to RESTful APIs\xa0Experience creating database stored procedures and functionsExperience architecting fast-cycle development for speculative applicationsExperience with reinforcement learning environments a bonusAbility to communicate, collaborate and work in ambiguous and unmapped contexts a necessity.', 'Work with data science and machine learning/artificial intelligence experts to strive for greater functionality in our data systems.', 'Experience with object-oriented/object function scripting languages: Python required; C++ a bonus, etc.', 'Architect application infrastructure to contain and support complex AI tools connecting reinforcement learning agents, deep neural networks, proprietary algorithms, sensitive data sources, and various external partners.', 'Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.', 'Key Responsibilities ', 'Create and maintain optimal data pipeline architecture, assemble large, complex data sets that meet functional / non-functional business requirements.Architect application infrastructure to contain and support complex AI tools connecting reinforcement learning agents, deep neural networks, proprietary algorithms, sensitive data sources, and various external partners.Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Build and manage the infrastructure required for optimal automated extraction, transformation, and loading of data from a wide variety of data sources using SQL, Snowflake, and AWS ‘big data’ technologies, for use in speculative machine learning and AI applications.Collaborate with data science and machine learning specialists to produce analytics tools that utilize the data pipeline and application infrastructure to provide actionable insights into customer acquisition and implement closed-loop optimization of media buying and customer acquisition systems.Integrate R&D Team tools with internal and external platforms via API, FTP, and other means.Work with data science and machine learning/artificial intelligence experts to strive for greater functionality in our data systems.', 'Collaborate with data science and machine learning specialists to produce analytics tools that utilize the data pipeline and application infrastructure to provide actionable insights into customer acquisition and implement closed-loop optimization of media buying and customer acquisition systems.', 'Ability to communicate, collaborate and work in ambiguous and unmapped contexts a necessity.', 'Research & Development Team’s', 'Experience creating database stored procedures and functions', 'Experience architecting fast-cycle development for speculative applications', 'The Kinesso Research & Development Team’s mission is to produce novel and disruptive solutions in the media, advertising, and marketing technology space.\xa0We pursue this mission by identifying key industry opportunities and trends and designing new algorithmic tools using concepts from game theory, information theory, probability & statistics, reinforcement learning, nonlinear dynamics, and various other areas of mathematics, data science, and technology, to extract commercial value.\xa0At any given time, we will be developing two to four major projects with one- to two-year time horizons and a small number of faster-cycle projects with a more narrow focus.\xa0We are a small team whose existence consists mainly of pushing at high-speed down a path while simultaneously drawing the map for that path.\xa0']",Mid-Senior level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Analytics Engineer,Vouch Insurance,"San Francisco, CA",1 day ago,42 applicants,"['', 'You Will Be Responsible For', 'Working with our sales, marketing, product, and insurance teams to design and build data sets to drive Vouch’s business', ' 3+ years experience developing ETL workflows as a data analytics engineer, data engineer, or data analyst Expert in SQL, capable in Python, and experience with Business Intelligence tools such as Looker, Mode Analytics or Tableau Experience building business reporting processes (e.g. for finance, sales, or business operations) Uses software development best practices with a focus on testing, reliability and maintainability Experience working with cloud-based data warehouses like Snowflake, Redshift, or BigQuery Experience with data transformation tooling (dbt is preferred), data pipeline services like Stitch and Fivetran, and orchestration tools like Airflow You can effectively talk (and listen) to engineers, designers, executives, and other stakeholders. ', ' Working with our sales, marketing, product, and insurance teams to design and build data sets to drive Vouch’s business Setting up and maintaining timely and reliable ingestion of external data sources via our data loading platforms. Providing clean, transformed data that is ready to be piped to our product, CRM, Finance, Underwriting, and other relevant systems Clearly documenting data models with source, description and field definitions for better collaboration, maintainability and usability. Establishing engineering best practices and methodologies to ensure data transformations and computations are accurate, efficient, and tested. Working with dbt, Snowflake, Airflow, Python, Stitch, and git, and we welcome new ideas.  ', 'Vouch believes in putting our people first and building a diverse team is at the front of everything that we do. We welcome people from different backgrounds, experiences, and perspectives. We are an equal opportunity employer and celebrate the diversity of our growing team.', 'About You', 'Providing clean, transformed data that is ready to be piped to our product, CRM, Finance, Underwriting, and other relevant systems', 'Nice To Have', 'A background in insurance or other regulated categories ', 'Expert in SQL, capable in Python, and experience with Business Intelligence tools such as Looker, Mode Analytics or Tableau', 'Uses software development best practices with a focus on testing, reliability and maintainability', '3+ years experience developing ETL workflows as a data analytics engineer, data engineer, or data analyst', 'Experience with data transformation tooling (dbt is preferred), data pipeline services like Stitch and Fivetran, and orchestration tools like Airflow', 'Establishing engineering best practices and methodologies to ensure data transformations and computations are accurate, efficient, and tested.', 'Experience building business reporting processes (e.g. for finance, sales, or business operations)', 'About Vouch', 'Working with dbt, Snowflake, Airflow, Python, Stitch, and git, and we welcome new ideas. ', ' Experience building executive level slide decks  Exposure to and passion for early-stage startups and/or high growth environments A background in insurance or other regulated categories  ', 'Experience building executive level slide decks ', 'The Job', 'Exposure to and passion for early-stage startups and/or high growth environments', 'Experience working with cloud-based data warehouses like Snowflake, Redshift, or BigQuery', 'Setting up and maintaining timely and reliable ingestion of external data sources via our data loading platforms.', 'Clearly documenting data models with source, description and field definitions for better collaboration, maintainability and usability.', 'You can effectively talk (and listen) to engineers, designers, executives, and other stakeholders.']",Entry level,Full-time,Information Technology,Insurance,2021-03-24 13:05:10
Data Engineer,DPR Construction,"Edison, NJ",1 week ago,33 applicants,"['', 'Secure the movement of sensitive information in a manner consistent with company policy and management expectations', ' to success in this role', 'Excellent knowledge of EL and ELT, Datawarehousing, and cloud-based tools', 'Identify, design, and implement internal process improvements, automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.', 'Growth and Development – Know or learn what is needed to deliver results and successfully compete', 'Define and lead API integration strategies and for the enterprise', 'Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.', 'Experience in scripting languages like Batch, Shell in Unix environment', 'Ability to understand, consume and use API’s, JSON, Webservices for Data pipelines.', 'Ability to work effectively with others who are in remote locations and varying time zones', 'Assemble large, complex data sets that meet functional / non-functional business requirements.', 'Ability to adapt quickly to change & deep curiosity to learn new tools and technologies and apply themAbility to work with and collaborate across the team and work effectively with others to identify the impact on the company’s business processes, other applications, network, etc.Strong analytical and problem-solving abilities.Seek and Embrace Change – Continuously improve work processes rather than accepting the status quoGrowth and Development – Know or learn what is needed to deliver results and successfully competeAbility to work effectively with others who are in remote locations and varying time zonesResourceful creative approach to problem-solving is expectedStrong communication skills, with the ability to work both independently and in project teamsMotivation to continually learn and take on added responsibilities while maintaining a positive attitude', ' DPR has been nationally recognized for its strong company culture, based on a well-defined purpose “We Exist to Build Great Things,” and four core values: integrity, enjoyment, uniqueness and ever forward. A flat, title-less organization that empowers people at all levels to make decisions, DPR ranked on FORTUNE’s “100 Best Companies to Work For” list for five consecutive years. For more information, visit http://www.dpr.com .', 'Control integration quality and develop ways to detect and correct anomalies with data exchange', 'Ability to work with and collaborate across the team and work effectively with others to identify the impact on the company’s business processes, other applications, network, etc.', 'Motivation to continually learn and take on added responsibilities while maintaining a positive attitude', 'Create and maintain optimal data pipeline architectureAssemble large, complex data sets that meet functional / non-functional business requirements.Enable data access, data processing, and data products by architecting, maintaining, scaling, monitoring and securing Data Warehouse, EL & ETL system, and data pipelines and BI systemsIdentify, design, and implement internal process improvements, automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.Keep our data separated and secureCreate data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.Work with data and analytics experts to strive for greater functionality in our data systems.Define and lead API integration strategies and for the enterpriseImplement enterprise integrations that result in a scalable, flexible, and highly available solutions that perform well under high trafficSecure the movement of sensitive information in a manner consistent with company policy and management expectationsControl integration quality and develop ways to detect and correct anomalies with data exchange', 'Seek and Embrace Change – Continuously improve work processes rather than accepting the status quo', 'Strong with SQL development knowledge for Relational Databases', 'Experience in Data Mapping, XML/JSON, and web service', 'Strong communication skills, with the ability to work both independently and in project teams', 'Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.', 'Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.', 'Business and Technical Analysis skills', 'Experience in Software development.', 'Solid understanding of database engineering and design (Relational, De-normalized, Data Lakes, etc.)Knowledge of AWS and Azure platformsExperience in Software development.Ability to understand, consume and use API’s, JSON, Webservices for Data pipelines.Excellent knowledge of EL and ELT, Datawarehousing, and cloud-based toolsStrong with SQL development knowledge for Relational DatabasesBusiness and Technical Analysis skillsExperience in scripting languages like Batch, Shell in Unix environmentExperience with integration of data from multiple data sources like API’s, JSON and any other databases, Flat-files, Spreadsheets.Experience in Data Mapping, XML/JSON, and web service', 'Responsibilities', 'Ability to adapt quickly to change & deep curiosity to learn new tools and technologies and apply them', 'Implement enterprise integrations that result in a scalable, flexible, and highly available solutions that perform well under high traffic', 'Enable data access, data processing, and data products by architecting, maintaining, scaling, monitoring and securing Data Warehouse, EL & ETL system, and data pipelines and BI systems', 'Create and maintain optimal data pipeline architecture', 'Qualifications', 'Key', 'Solid understanding of database engineering and design (Relational, De-normalized, Data Lakes, etc.)', 'Work with data and analytics experts to strive for greater functionality in our data systems.', 'Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.', 'Keep our data separated and secure', 'Resourceful creative approach to problem-solving is expected', 'Experience with integration of data from multiple data sources like API’s, JSON and any other databases, Flat-files, Spreadsheets.', 'Job Description', 'Knowledge of AWS and Azure platforms', 'Position Summary', 'Strong analytical and problem-solving abilities.']",Entry level,Full-time,Information Technology,Construction,2021-03-24 13:05:10
Data Engineer (648150),Technamo,"Richmond, VA",2 days ago,Be among the first 25 applicants,"['', 'Contact us:', 'Competencies Required', ' The demonstrated ability to translate data architecture and stakeholder requirements into technology solutions for data acquisition, handling, and storage; to ensure data quality, integrity, and security throughout the data processing system; to prototype data and analytics solutions that establish concept viability; to collaborate successfully with technical and business stakeholders, and to provide effective oral and written communications on related issues. A considerable knowledge of database management systems, query tools, data schemas, and distributed systems, relational databases; of tools to extract, clean, transform, and load/warehouse data; and of cloud computing environments. Considerable skill in the use of a personal computer with data engineering and standard office applications. Must have a strong background in SQL Server and SQL development. Must have a strong understanding of SSIS, Talend, and other ETL solutions.', '– Required 7 years.', ' Must have a strong background in SQL Server and SQL development.', 'Company Description', ' The demonstrated ability to translate data architecture and stakeholder requirements into technology solutions for data acquisition, handling, and storage; to ensure data quality, integrity, and security throughout the data processing system; to prototype data and analytics solutions that establish concept viability; to collaborate successfully with technical and business stakeholders, and to provide effective oral and written communications on related issues.', 'Duties include', 'Job Description', ' Must have a strong understanding of SSIS, Talend, and other ETL solutions.', ' A considerable knowledge of database management systems, query tools, data schemas, and distributed systems, relational databases; of tools to extract, clean, transform, and load/warehouse data; and of cloud computing environments. Considerable skill in the use of a personal computer with data engineering and standard office applications.', 'Required Skills']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-03-24 13:05:10
"Data Support Engineer, Homesnap",CoStar Group,"Bethesda, MD",1 day ago,Be among the first 25 applicants,"['', 'You love big data', ""Bachelor's Degree"", 'Identify and report cosmetic and functional bugs', 'Preferred Qualifications', '1+ years of experience working as a support representative', 'Detail-oriented and able to self-manage several concurrent tasksAbility to work both independently and collaboratively with team membersYou love big dataKnowledge of .NET/C# and database architecting/design a plus', '2+ years of experience with Microsoft SQL Server and Transact-SQL', 'Knowledge of .NET/C# and database architecting/design a plus', 'Handle all tickets escalated by the Customer Support teamIdentify and report cosmetic and functional bugsDiagnosis and fix data related issuesBuild or modify in-house admin tools for recurring issuesWrite complex SQL queries to generate custom reports', 'Handle all tickets escalated by the Customer Support team', 'Diagnosis and fix data related issues', 'Detail-oriented and able to self-manage several concurrent tasks', 'Write complex SQL queries to generate custom reports', ""Bachelor's Degree2+ years of experience with Microsoft SQL Server and Transact-SQL1+ years of experience working as a support representative"", 'Build or modify in-house admin tools for recurring issues', 'Basic Qualifications', 'Job Description', 'Ability to work both independently and collaboratively with team members']",Entry level,Full-time,Information Technology,Commercial Real Estate,2021-03-24 13:05:10
Data Engineer,TargetCW,San Francisco Bay Area,,N/A,"['', 'Build ETL pipelines between our Salesforce instance and our data warehouse', 'Strong proficiency in SQL\xa0', 'Experience with building data warehouses and dimensional modeling', 'Experience with Salesforce backends', 'Experience using cloud services to automate the data pipelines', 'Experience with best practices for development including query optimization, version control, code reviews, and documentation', 'For this role, we’re looking for a data engineer to join the Creator Analytics team: Responsibilities', 'PLEASE SUBMIT YOUR RESUME TO BE CONSIDERED!', 'Maintain a high level of trust in these data sets: keep existing data sources fresh against changing requirements, definitions, and data quality issues.\xa0', 'Good to have: experience with Tableau', 'Fluency with a Git/GitHub version control workflow\xa0', 'Data Engineer (W2 ONLY)', 'Build ETL pipelines between our Salesforce instance and our data warehouseMaintain a high level of trust in these data sets: keep existing data sources fresh against changing requirements, definitions, and data quality issues.\xa0Improve reliability and performance: Conduct unit, integration, and system tests on our data sources in order to validate data against source systems, and continuously optimize performance in order to improve query speed and reduce cost.Improve data discovery and literacy: Create documentation on data relationships and dependencies', 'Requirements', 'Familiarity with AWS services such as Redshift and S3', '\xa0', 'Remote', '**MUST BE US CITIZEN, NO C2C**', '$70-75+/hr DOE', '3+ years of experience in data engineeringStrong proficiency in using one of the script languages, such as PythonStrong proficiency in SQL\xa0Familiarity with AWS services such as Redshift and S3Experience using cloud services to automate the data pipelinesExperience with building data warehouses and dimensional modelingExperience with best practices for development including query optimization, version control, code reviews, and documentationExperience with Salesforce backendsFluency with a Git/GitHub version control workflow\xa0Good to have: experience with Tableau', 'Estimated Duration:\xa0', 'Strong proficiency in using one of the script languages, such as Python', 'Improve data discovery and literacy: Create documentation on data relationships and dependencies', '3+ years of experience in data engineering', 'Full Time', 'Estimated Duration:\xa09 months with a possibility of extending up to 18+ months ', ""We are the\xa0world's leading live streaming platform for gamers and the things we love. We make it possible to watch, play and chat with millions of other fans from around the world. We are looking for a Data Engineer to join our team ASAP! If you have 3+ years of experience in Data Engineering along with Python and SQL, we want to speak to you!"", 'Improve reliability and performance: Conduct unit, integration, and system tests on our data sources in order to validate data against source systems, and continuously optimize performance in order to improve query speed and reduce cost.']",Mid-Senior level,Full-time,Engineering,Computer Games,2021-03-24 13:05:10
 Data Engineer/Architect ,Steven Cardwell Recruitment & Engineering Search Firm Inc.,"Georgia, United States",,N/A,"['A competitive salary and comprehensive benefits package, including medical, dental, vision, maternity support program, discounted virtual physician visits, voluntary medical benefits (Critical Illness, Hospital Care, and Accidental Injury), FSA, HSA,\xa0life insurance, short term and long term disability, cash matching 401(k) plan, employee assistance program (EAP), pet insurance, employee discount program, generous vacation and 11 paid holidays.', '3+ years of experience as a data engineer within a modeling environment including parsing JSON data', 'Job Details', 'Identify, design and implement internal process improvements such as automating manual data processes, optimizing data delivery and scalability', 'Create data tools for analytics and data science team members that will assist them in building and optimizing the product portfolio', 'Designed, built, and configured the architecture for an IoT program; successfully completed multiple iterations of changes and/or upgrades to the system', 'As the Data Engineer/Architect, you will be part of the team that delivers customer solutions and outcomes in the industry.\xa0The successful candidate must have experience with successfully designing, building, and configuring the architecture for an IoT program.\xa0The Data Engineer/Architect will spend a significant amount of time working with cloud-based components and replacing and/or upgrading components like the database, data conditioning, visualization, analytics, and math engine’s in the IoT system architecture.', 'Build and maintain the infrastructure that is required to transfer and hold data from a wide variety of data sources', 'Education & Experience:', 'Experience building and optimizing big data pipelines architectures and data sets.', 'Work closely with the Product Manager and internal team members to ensure all the data is optimized for next step or process.', 'Development with Big Data, IoT data, SQL, AWS and built productionized solutions', 'Work with stakeholders on data-related technical issues and their data needs, and maintain a data dictionary of all the data elements and all of the different variations based on equipment data tags', 'Assemble large and complex data sets that meet both functional and non-functional business needs', 'Ensure the long-term data storage, processing, infrastructure, speed and quality of the data-based solutions in the marketplace', 'Create and maintain optimal data pipeline (AWS construct) architectures in the AWS cloud environment', 'Data Engineer/Architect ', 'Key Responsibilities:', '2+ years of experience in working with at least one NoSQL system like MongoDB.', '3+ years of data & analytics experience inside industrial industry is preferred', 'Experience in Machine learning model deployment in live environment', ""Bachelor's degree in Computer Science, Information Systems, Applied Mathematics or equivalent work experienceExperience in Machine learning model deployment in live environmentDesigned, built, and configured the architecture for an IoT program; successfully completed multiple iterations of changes and/or upgrades to the systemDevelopment with Big Data, IoT data, SQL, AWS and built productionized solutionsExperience building and optimizing big data pipelines architectures and data sets.3+ years of experience as a data engineer within a modeling environment including parsing JSON data3+ years of data & analytics experience inside industrial industry is preferred2+ years of experience in working with at least one NoSQL system like MongoDB."", 'Location:\xa0Georgia & Wisconsin (This is not a remote role)', 'Create and maintain optimal data pipeline (AWS construct) architectures in the AWS cloud environmentWork closely with the Product Manager and internal team members to ensure all the data is optimized for next step or process.Assemble large and complex data sets that meet both functional and non-functional business needsIdentify, design and implement internal process improvements such as automating manual data processes, optimizing data delivery and scalabilityEnsure the long-term data storage, processing, infrastructure, speed and quality of the data-based solutions in the marketplaceBuild and maintain the infrastructure that is required to transfer and hold data from a wide variety of data sourcesCreate analytics tools that utilize the data pipeline to provide actionable insights, operational efficiencies and other key performance metrics/indicators (KPM/KPI)Create data tools for analytics and data science team members that will assist them in building and optimizing the product portfolioWork with stakeholders on data-related technical issues and their data needs, and maintain a data dictionary of all the data elements and all of the different variations based on equipment data tags', ""Bachelor's degree in Computer Science, Information Systems, Applied Mathematics or equivalent work experience"", 'Create analytics tools that utilize the data pipeline to provide actionable insights, operational efficiencies and other key performance metrics/indicators (KPM/KPI)']",Mid-Senior level,Full-time,Engineering,Staffing and Recruiting,2021-03-24 13:05:10
Data Engineer (REMOTE),Converge Resources,"Houston, TX",4 weeks ago,Be among the first 25 applicants,"['Build ETL pipeline to support data marts/data warehouse', 'Employer Size: 500-2000', '2+ years of experience modeling logical and physical designs for data models', '2+ years of SSAS tabular models and queries for self-service analytics and PowerBI2+ years of ETL for data warehousing (SSIS, Informatica, Talend)2+ years of experience modeling logical and physical designs for data modelsExperience with star schema and Snowflake design', 'Build tabular models', 'Data warehouse and data marts design, model and develop table', 'Responsibilities', '2+ years of ETL for data warehousing (SSIS, Informatica, Talend)', 'Retrieve and combine data from various data sources', 'Client Type: Oil and Gas', 'Manage end-to-end availability, performance monitoring, and capacity planning using a variety of open source and developed toolsets.', 'Experience with star schema and Snowflake design', 'Qualifications', 'Implement various automation solutions across all aspects of the environments including installation, configuration, monitoring, and utilization.', '2+ years of SSAS tabular models and queries for self-service analytics and PowerBI', 'Manage end-to-end availability, performance monitoring, and capacity planning using a variety of open source and developed toolsets.Implement various automation solutions across all aspects of the environments including installation, configuration, monitoring, and utilization.Data warehouse and data marts design, model and develop tableBuild ETL pipeline to support data marts/data warehouseBuild tabular modelsRetrieve and combine data from various data sources', 'Title: Data Engineer']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Moser Consulting,"Indianapolis, IN",3 weeks ago,56 applicants,"['', 'Certifications are beneficial.', 'Ability to create functioning ETL prototypes to address quickly changing business needs. ', 'Ability to develop automation solutions with a goal to reduce manual work.', ' Bachelor’s degree from an accredited four-year college or university with major coursework in computer information systems, computer science, data management, information technology, or any other related discipline.  Experience in cloud-based architectures involving Data Lakes, Ingestion Frameworks, Transformation and Data Set generation. Strong understanding of ETL/ELT fundamentals and solutions involving tools such as Azure Data Factory, SSIS, or Informatica. Should be proficient in writing advanced / complex SQL statements. Expertise in performance tuning and optimization of SQL queries required. Previous projects should display technical leadership with an emphasis on data warehouse solutions and/or business intelligence solutions. Proven ability to design and develop tailored data structures.  Develop, implement, and optimize stored procedures and functions using T-SQL. Ability to create functioning ETL prototypes to address quickly changing business needs.  Ability to develop automation solutions with a goal to reduce manual work. Clearly articulate pros and cons of proposed technologies/solutions.  Certifications are beneficial. Preferred Locations: Indianapolis, IN and Baltimore, MD. United States required. ', 'Clearly articulate pros and cons of proposed technologies/solutions. ', 'Bachelor’s degree from an accredited four-year college or university with major coursework in computer information systems, computer science, data management, information technology, or any other related discipline. ', 'Expertise in performance tuning and optimization of SQL queries required.', 'Strong understanding of ETL/ELT fundamentals and solutions involving tools such as Azure Data Factory, SSIS, or Informatica.', 'Experience in cloud-based architectures involving Data Lakes, Ingestion Frameworks, Transformation and Data Set generation.', 'Requirements', 'Proven ability to design and develop tailored data structures. ', 'Description', 'Design, Develop, Deploy, and Monitor complex data pipelines which may contain various required transformations of data using various languages and techniques.', ' Design, Develop, Deploy, and Monitor complex data pipelines which may contain various required transformations of data using various languages and techniques. Identifying, designing and implementing internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes. Working with stakeholders on data-related technical issues. ', 'Working with stakeholders on data-related technical issues.', 'Preferred Locations: Indianapolis, IN and Baltimore, MD. United States required.', 'Benefits', 'Identifying, designing and implementing internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes.', 'Should be proficient in writing advanced / complex SQL statements.', 'Previous projects should display technical leadership with an emphasis on data warehouse solutions and/or business intelligence solutions.', 'Develop, implement, and optimize stored procedures and functions using T-SQL.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Senior Data Engineer,The Zebra,"Austin, TX",3 days ago,Be among the first 25 applicants,"['', 'A passion for problem solving and performance tuning with strong analytical capabilities', ""WHAT YOU'LL DO:"", 'A desire to follow and advocate for exceptional software engineering processes', 'BENEFITS + PERKS:', 'About The Zebra', ' 2+ years experience building large-scale, distributed, high-volume systems Experience building ML pipelines and deploying data science models to production A passion for problem solving and performance tuning with strong analytical capabilities A desire to follow and advocate for exceptional software engineering processes Significant experience with AWS (Amazon Web Services) Advanced degree in Computer Science A Github profile or public code portfolio ', 'Have a meaningful role in the design and development of a reliable, effective and scalable data platform', 'Experience with Hadoop, Spark, Flink, and/or related technologies', 'HSA offering + employer contribution', 'Learning & Development Stipends', 'Unlimited PTO + flexibility to enjoy it', 'Assist and guide the company in finding data-driven solutions to business problems ', '401k with match', 'REQUIREMENTS / QUALIFICATIONS:', 'A Github profile or public code portfolio', 'Familiarity with event streams and stream processing', 'Experience with microservice architecture', 'Comfort with the Linux/Unix command line and shell scripting', 'Competitive Compensation & Stock Option Offering', 'Bi-Monthly Wednesday Catered Lunch', ' Have a meaningful role in the design and development of a reliable, effective and scalable data platform Design and implement fast and scalable data pipelines Create datasets that allow for more advanced reporting and analytics capabilities Mentor other engineers and analysts on data processing, data access and manipulation techniques Assist and guide the company in finding data-driven solutions to business problems  ', ' 5+ years of professional programming experience Experience with scalable data warehouses (Snowflake, Redshift, BigQuery, etc) Exceptional experience with relational and NoSQL databases Experience with Hadoop, Spark, Flink, and/or related technologies Familiarity with event streams and stream processing Experience with microservice architecture Comfort with the Linux/Unix command line and shell scripting Deep understanding of one or more of the following languages: Python, Java, Scala ', 'Paid Parental Leave Program', 'Zeal Care - Monthly wellness subscriptions ($35/month)', 'Experience with scalable data warehouses (Snowflake, Redshift, BigQuery, etc)', 'Deep understanding of one or more of the following languages: Python, Java, Scala', 'Experience building ML pipelines and deploying data science models to production', 'Opportunity to join Employee Resource Groups (ERGs) or drive our diversity & inclusion stance by creating your own', ' Competitive Compensation & Stock Option Offering Health, Dental, Vision & Disability Coverages Health, Dental, Vision & Disability Coverages HSA offering + employer contribution 401k with match Unlimited PTO + flexibility to enjoy it Paid Parental Leave Program Wellness perk ($100/month) Pet Adoption Reimbursement ($300/year) Learning & Development Stipends Bi-Monthly Wednesday Catered Lunch Zeal Care - Monthly wellness subscriptions ($35/month) Curated monthly snack box - sent to your house Opportunity to join Employee Resource Groups (ERGs) or drive our diversity & inclusion stance by creating your own Join a team that truly lives their values, and values their lives (outside of the office. Cliche, we know… but we really mean it) ', 'Health, Dental, Vision & Disability Coverages', 'Cliche, we know… but we really mean it', 'Create datasets that allow for more advanced reporting and analytics capabilities', '2+ years experience building large-scale, distributed, high-volume systems', 'No external recruiters or agents, please.**', 'Design and implement fast and scalable data pipelines', 'Advanced degree in Computer Science', 'Wellness perk ($100/month)', 'Pet Adoption Reimbursement ($300/year)', 'Mentor other engineers and analysts on data processing, data access and manipulation techniques', 'Curated monthly snack box - sent to your house', 'EXPERIENCE THAT WILL IMPRESS THE HECK OUT OF US:', 'Join a team that truly lives their values, and values their lives (outside of the office. Cliche, we know… but we really mean it)', 'Exceptional experience with relational and NoSQL databases', '5+ years of professional programming experience', 'Significant experience with AWS (Amazon Web Services)', 'INTRODUCTION ']",Associate,Full-time,Information Technology,Insurance,2021-03-24 13:05:10
Data Engineer,Koddi,United States,4 weeks ago,75 applicants,"['', ""You have strong coding fundamentals with experience in C, C++, GoLang, or Java\xa0You have experience working with a large multi-contributor Git repositoryYou have experience working with SQL and No-SQL databases (SQL Server, Postgres, Redis, Aerospike)You’ve run large scale applications in AWSYou adapt well working with emerging technologiesYou have a strong technical base and innovative mindsetYou have experience mentoring teammatesYou thrive in ever-changing environmentsYou communicate well with coworkers of all levelsYou have a Bachelor's or Master’s degree in Computer Science with at least five years working experience on an enterprise-level application"", 'Recommend and implement improvements to data processes and warehouses that improve supportability, usability, and scalability', 'You communicate well with coworkers of all levels', 'Improve the efficiency, scalability, and reliability of applications', 'As a Senior Data Engineer at Koddi, you will work with our technology team to build and maintain our suite of data pipelines, stores, and databases that power sophisticated adtech products used by many of the world’s largest advertisers. We are looking for smart and hardworking individuals who love to build world-class software. The right candidates will be creative thinkers who can design and deploy professional applications using the newest technologies to solve real business problems.', 'Based in Fort Worth, Texas, we’ve grown to become a diverse global team. Ranked by Forbes, Deloitte, and Inc. magazine as one of the fastest-growing companies in the nation, we’re growing rapidly and looking for innovative problem solvers to join our team. ', 'WHAT YOU’LL DO', 'WHO WE ARE', 'Design, deliver and implement data engineering solutions for optimized downstream impact, with end-to-end input and oversight of data engineering projects', 'OUR MISSION AND PROMISE', 'Passionate about development in leading technologies? Looking to become a major player on a diverse team in your next career adventure? Koddi Data Engineers drive innovation by embracing challenges and leveraging decision science to solve complex problems in adtech. ', 'You’ve run large scale applications in AWS', 'WHO YOU ARE', 'Work within robust data systems and develop custom solutions while consulting with external customersDesign, deliver and implement data engineering solutions for optimized downstream impact, with end-to-end input and oversight of data engineering projectsRecommend and implement improvements to data processes and warehouses that improve supportability, usability, and scalabilityOptimize and refactor existing codeImprove the efficiency, scalability, and reliability of applications', 'You have experience working with SQL and No-SQL databases (SQL Server, Postgres, Redis, Aerospike)', 'You have a strong technical base and innovative mindset', 'You have strong coding fundamentals with experience in C, C++, GoLang, or Java\xa0', 'Work within robust data systems and develop custom solutions while consulting with external customers', 'Koddi is a technology company that was born in 2013 from an opportunity to innovate in the\xa0 adtech space. Our award-winning SaaS platform provides a robust network for brands to connect with consumers and drive revenue through native sponsored placements, metasearch, and programmatic media campaigns.', 'Getting the experience right, together.\xa0', 'You have experience working with a large multi-contributor Git repository', 'You have experience mentoring teammates', 'Optimize and refactor existing code', ""You have a Bachelor's or Master’s degree in Computer Science with at least five years working experience on an enterprise-level application"", 'You thrive in ever-changing environments', 'You adapt well working with emerging technologies']",Mid-Senior level,Full-time,Information Technology,Internet,2021-03-24 13:05:10
Data Engineer,Salesforce,"Indianapolis, IN",3 weeks ago,118 applicants,"['', 'Salesforce.com', 'Must be comfortable with changing requirements and priorities', 'Closely work with Metric owners and users to build the next generation data integration capabilities which in turn will support a variety of finance processes', 'Collaboratively work and act as liaison with IT and Product teams in fast paced environment, to meet deadlines', 'Experience executing projects with cross-functionally with various stakeholders from start to finish', 'Have a bachelors or Masters in Computer Science or related field with at least 5 years experience in a highly technical data organization2+ years experience in writing complicated database queries in SQL language ( Oracle, Snowflake, Hive, etc.)2+ years of experience in ETL design and implementation using Informatica/Alteryx/JitterbitExperience working with very large data sets, knows how to build programs that leverage the parallel capabilities of Python and database platformsStrong knowledge of database performance concepts like indices, segmentation, projections, and partitions.Experience working in a data warehouse environment with diverse data sources and visualization tools like Tableau, Einstein Analytics and Business objectsProficient in Python and shell scriptingProficient in designing, documenting and developing scalable data architecture and process flowsCommunicates in a clear, concise and timely manner and operates effectively as a team playerExperience presenting to stakeholders or other decision makers to present and sell ideas to various audiences (technical and non-technical)Experience executing projects with cross-functionally with various stakeholders from start to finishSelf-directed/motivated with excellent organizational skillsMust be comfortable with changing requirements and prioritiesResourceful with a strong work ethic and are willing to go the extra mile to get work done. Must be results oriented and ability to move forward without complete informationStrong desire to explore and learn Data science related technologies. Proactive in identifying uses-case and solutionsSalesforce experience/ certification is a plus but not requiredFamiliarity with big data, Hadoop Ecosystem of Tools', 'Use your expertise across a number of tools and work with IT team to build data analytics solution/ integrations', 'Build data feed prototypes from enterprise data warehouse and other data sources into the FI Data MartTake complete ownership of the data quality, data mapping, business logic, transformation rule for the data feeds built and have a passion for high quality dataAs we expand into other platforms, build and implement data prototypes across diverse technology platforms i.e. Snowflake, Oracle databases, Python and cloud platforms like HerokuClosely work with Metric owners and users to build the next generation data integration capabilities which in turn will support a variety of finance processesUse your expertise across a number of tools and work with IT team to build data analytics solution/ integrationsHave a high sense of urgency to deliver projects as well as troubleshoot and fix data queries/ issuesAlways be on the lookout to automate and improve existing data processes for quicker turnaround and high productivityCollaboratively work and act as liaison with IT and Product teams in fast paced environment, to meet deadlinesDesired Skills and Experience:', 'Job Details', 'Desired Skills and Experience:', 'Proficient in Python and shell scripting', 'Salesfore.com', 'Experience presenting to stakeholders or other decision makers to present and sell ideas to various audiences (technical and non-technical)', 'Proficient in designing, documenting and developing scalable data architecture and process flows', 'Desired Skills And Experience', 'Build data feed prototypes from enterprise data warehouse and other data sources into the FI Data Mart', 'Communicates in a clear, concise and timely manner and operates effectively as a team player', '2+ years experience in writing complicated database queries in SQL language ( Oracle, Snowflake, Hive, etc.)', 'Accommodations ', '2+ years of experience in ETL design and implementation using Informatica/Alteryx/Jitterbit', 'As we expand into other platforms, build and implement data prototypes across diverse technology platforms i.e. Snowflake, Oracle databases, Python and cloud platforms like Heroku', 'Accommodations  - ', 'Resourceful with a strong work ethic and are willing to go the extra mile to get work done. Must be results oriented and ability to move forward without complete information', 'Take complete ownership of the data quality, data mapping, business logic, transformation rule for the data feeds built and have a passion for high quality data', 'Experience working with very large data sets, knows how to build programs that leverage the parallel capabilities of Python and database platforms', 'Self-directed/motivated with excellent organizational skills', 'Salesforce.org', 'Strong desire to explore and learn Data science related technologies. Proactive in identifying uses-case and solutions', 'To get the best candidate experience, please consider applying for a maximum of 3 roles within 12 months to ensure you are not duplicating efforts.', 'Responsibilities', 'Familiarity with big data, Hadoop Ecosystem of Tools', 'Salesforce experience/ certification is a plus but not required', 'Posting Statement', 'Experience working in a data warehouse environment with diverse data sources and visualization tools like Tableau, Einstein Analytics and Business objects', 'Strong knowledge of database performance concepts like indices, segmentation, projections, and partitions.', 'Have a bachelors or Masters in Computer Science or related field with at least 5 years experience in a highly technical data organization', 'Job Category', 'Always be on the lookout to automate and improve existing data processes for quicker turnaround and high productivity', 'The Role', 'Have a high sense of urgency to deliver projects as well as troubleshoot and fix data queries/ issues']",Not Applicable,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Azure Data analyst/Engineer,Saransh Inc,"Winston-Salem, NC",2 days ago,69 applicants,"['2+ year experience in Azure using Azure Data Lake Store, Azure Event Hubs or Azure Synapse', '1+ year of experience in Kubernetes', 'Excellent verbal/written communication skills, including communicating technical issues to non-technical audiences', 'Experience with mentoring software developers', 'Client is looking for profiles with the below skills which are mandatory.', '\xa0', 'Experience with manipulating and transforming data', 'Exposure to NoSQL databases, Spark and the Hadoop Ecosystem (MapReduce, Oozie, Hive, Pig)', '1+ years of experience with ETL Development', '1+ year of Production Support and operationalization of Applications', 'Strong critical thinking, decision making, and problem-solving skills', 'Experience with Change Data Capture (CDC) technologies and relational databases such as MS SQL, Oracle and DB2', 'some one with strong Datawarehouse experience .', '1+ year of experience with RabbitMQ, Kafka or other messaging technologies', '2+ years of experience working with Java based applications, open source technologies and SQL.']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,"NBCUniversal Media, LLC","New York, NY",2 weeks ago,Be among the first 25 applicants,"['', ' Experience with Snowflake, Amazon Web Services, or related cloud platforms a plus  Understanding of big data technology stacks (Hive / Spark etc) is a plus ', ' Direct experience with data modeling, ETL/ELT development principles, and data warehousing concepts ', ' Direct experience working with sources like Nielsen, Adobe Analytics, comScore, and other media/entertainment industry datasets a plus ', 'Notices', "" Bachelor's degree in Computer Science, Data Science, Statistics, Informatics, Information Systems or related field. "", ' Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using cloud-native data engineering principles ', ' Implement the appropriate design patterns while optimizing performance, cost, security, and scale and end user experience ', 'Multiple Locations', "" Action-oriented – You're constantly figuring out new problems and are regularly showing results with a positive attitude, always displaying ethical behavior, integrity, and building trust Strong understanding of Agile principles and best practices  You’ve dealt with ambiguity and can make quality decisions in a dynamic, fast-paced environment "", ' Strong understanding of Agile principles and best practices ', 'Country', ' Knowledge of data management fundamentals and data storage principles ', ' Experience with Snowflake, Amazon Web Services, or related cloud platforms a plus ', ' Media-focused – Strong knowledge/passion for media including broadcast TV, digital, and film Direct experience working with sources like Nielsen, Adobe Analytics, comScore, and other media/entertainment industry datasets a plus ', ' 5+ years of experience in a data engineering role ', ' Analytical – You have experience in delivering data analytics solutions that promote data discovery Experience with Snowflake, Amazon Web Services, or related cloud platforms a plus  Understanding of big data technology stacks (Hive / Spark etc) is a plus ', ' Understanding of big data technology stacks (Hive / Spark etc) is a plus ', ' Build and manage relationships with supporting IT teams in order to effectively deliver work products to production ', "" Analytical – You have experience in delivering data analytics solutions that promote data discovery Experience with Snowflake, Amazon Web Services, or related cloud platforms a plus  Understanding of big data technology stacks (Hive / Spark etc) is a plus  Media-focused – Strong knowledge/passion for media including broadcast TV, digital, and film Direct experience working with sources like Nielsen, Adobe Analytics, comScore, and other media/entertainment industry datasets a plus  Communicator – You have excellent verbal and written skills with the ability to communicate ideas effectively across all levels of the organization, both technical and non-technical  Action-oriented – You're constantly figuring out new problems and are regularly showing results with a positive attitude, always displaying ethical behavior, integrity, and building trust Strong understanding of Agile principles and best practices  You’ve dealt with ambiguity and can make quality decisions in a dynamic, fast-paced environment "", 'City', ' You’ve dealt with ambiguity and can make quality decisions in a dynamic, fast-paced environment ', 'About Us', ' Strong understanding of Agile principles and best practices  You’ve dealt with ambiguity and can make quality decisions in a dynamic, fast-paced environment ', 'Responsibilities', ' Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. ', ' Communicator – You have excellent verbal and written skills with the ability to communicate ideas effectively across all levels of the organization, both technical and non-technical ', 'Qualifications/Requirements', 'Qualifications', 'State/Province', ' General understanding of cloud data engineering design patterns and use cases ', ' Collaborate with business leaders, engineers, and product managers to understand data needs.  Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using cloud-native data engineering principles  Design, build, and scale data pipelines across a variety of source systems and streams (internal, third-party, as well as cloud-based), distributed/elastic environments, and downstream applications and/or self-service solutions  Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.  Implement the appropriate design patterns while optimizing performance, cost, security, and scale and end user experience  Participate in development sprints, demos, and retrospectives, as well as release and deployment  Build and manage relationships with supporting IT teams in order to effectively deliver work products to production ', ' Collaborate with business leaders, engineers, and product managers to understand data needs. ', ' Desired Characteristics: ', ' Demonstratable experience in Airflow, Luigi or similar orchestration engines ', ' Participate in development sprints, demos, and retrospectives, as well as release and deployment ', 'Sub-Business', "" 5+ years of experience in a data engineering role  Direct experience with data modeling, ETL/ELT development principles, and data warehousing concepts  Knowledge of data management fundamentals and data storage principles  Experience in building data pipelines using Python/SQL or similar programming languages  Demonstratable experience in Airflow, Luigi or similar orchestration engines  General understanding of cloud data engineering design patterns and use cases  Bachelor's degree in Computer Science, Data Science, Statistics, Informatics, Information Systems or related field. "", ' Experience in building data pipelines using Python/SQL or similar programming languages ', ' Design, build, and scale data pipelines across a variety of source systems and streams (internal, third-party, as well as cloud-based), distributed/elastic environments, and downstream applications and/or self-service solutions ', 'Career Level']",Not Applicable,Full-time,Information Technology,Broadcast Media,2021-03-24 13:05:10
Data Engineer,Sense,"Cambridge, MA",1 month ago,103 applicants,"['', 'Excited about high-volume, real-time data and solving the challenges it poses.', 'Solid knowledge of computer science.', 'Work with a small team of experienced entrepreneurs creating revolutionary technology.', 'Passionate about the energy sector and climate change.', '50 on Fire - BostInno', 'Great opportunity to gain experience at a consumer smart home startup.', '5+ years professional experience as a data or backend engineer.', 'Top 100 - Red Herring', 'Experience programming in Python.', 'Knowledge of C/C++ or the willingness to learn is a plus.', 'Experience with AWS services is a plus.', 'Work with the data science team to develop active learning tools and UIs that will facilitate the development of statistical models.', 'Design and implement new data pipelines and databases as needed.', 'Requirements', 'Have a big impact at a VC-backed consumer startup that\'s doing big things Best Startups in Cambridge - Tech Tribune ""One of the world\'s top 100 AI companies"" - VentureBeat Clean Tech Company of the Year - New England Venture Capital Association 50 on Fire - BostInno Top 100 - Red Herring Best Consumer AI Technology - AI Dev World Global Cleantech 100  ', 'Why Sense', ' Best Startups in Cambridge - Tech Tribune ""One of the world\'s top 100 AI companies"" - VentureBeat Clean Tech Company of the Year - New England Venture Capital Association 50 on Fire - BostInno Top 100 - Red Herring Best Consumer AI Technology - AI Dev World Global Cleantech 100 ', 'Be a part of building something that will make a difference in the world.', 'Clean Tech Company of the Year - New England Venture Capital Association', 'Best Consumer AI Technology - AI Dev World', ' As the first Data Engineer on our Data Science team, collaborate in building the infrastructure and architecture for data generation. Work with the data science team to develop active learning tools and UIs that will facilitate the development of statistical models. Analyze, maintain, and improve our existing data systems and infrastructure. Design and implement new data pipelines and databases as needed. ', 'Competitive compensation and generous healthcare benefits.', 'Flexible hours and WFH policy.', 'Though our company is fully remote due to the COVID-19 pandemic, we have a great office in Central Square in Cambridge, MA right by the Red Line. (re-opening July 2021 at the earliest due to COVID-19)', 'Global Cleantech 100', ' Be a part of building something that will make a difference in the world. Have a big impact at a VC-backed consumer startup that\'s doing big things Best Startups in Cambridge - Tech Tribune ""One of the world\'s top 100 AI companies"" - VentureBeat Clean Tech Company of the Year - New England Venture Capital Association 50 on Fire - BostInno Top 100 - Red Herring Best Consumer AI Technology - AI Dev World Global Cleantech 100   Work with a small team of experienced entrepreneurs creating revolutionary technology. A wide range of difficult and interesting problems to be solved. Great opportunity to gain experience at a consumer smart home startup. Competitive compensation and generous healthcare benefits. Flexible hours and WFH policy. Paid parental leave. Though our company is fully remote due to the COVID-19 pandemic, we have a great office in Central Square in Cambridge, MA right by the Red Line. (re-opening July 2021 at the earliest due to COVID-19)', 'Paid parental leave.', '""One of the world\'s top 100 AI companies"" - VentureBeat', 'Must be authorized to work in the U.S.', 'Benefits', 'Data Engineer', 'Experience with relational databases such as MySQL.', 'Motivated to learn new tools and software to develop performant and scalable systems.', 'As the first Data Engineer on our Data Science team, collaborate in building the infrastructure and architecture for data generation.', 'Analyze, maintain, and improve our existing data systems and infrastructure.', ' Passionate about the energy sector and climate change. Excited about high-volume, real-time data and solving the challenges it poses. Motivated to learn new tools and software to develop performant and scalable systems. ', ' 5+ years professional experience as a data or backend engineer. Solid knowledge of computer science. Experience programming in Python. Knowledge of C/C++ or the willingness to learn is a plus. Experience with relational databases such as MySQL. Experience with AWS services is a plus. Must be authorized to work in the U.S. ', 'Job Description', 'Best Startups in Cambridge - Tech Tribune', 'A wide range of difficult and interesting problems to be solved.']",Not Applicable,Full-time,Engineering,Information Technology and Services,2021-03-24 13:05:10
Remote Data Engineer,"WaveStrong, Inc.","Pleasanton, CA",2 weeks ago,118 applicants,"['', 'Demonstrated expertise with at least one RDBMS', 'Improving data recovery processes', 'Improve reliability, recovery, and integrity of our data ingestion pipeline', 'Experience developing automated ETL pipelines with high-reliability requirements', 'Develop reporting tools to help our in-house experts and utility clients understand the impact of our services', 'Improving the reliability of ETL pipelines', 'Dealing with streaming ingestion', 'Demonstrated expertise with Mongo DB, Cosmos DB, DynamoDB, or similar document-oriented datastore', 'Improving tooting for data correctness', ' Dealing with streaming ingestion Improving the reliability of ETL pipelines Improving data recovery processes Improving tooting for data correctness .net / Azure MSFT stack experience / Python or Java experience', '.net / Azure MSFT stack experience / Python or Java experience', 'Requirements', ' Improve reliability, recovery, and integrity of our data ingestion pipeline Develop reporting tools to help our in-house experts and utility clients understand the impact of our services Experience developing automated ETL pipelines with high-reliability requirements At least 2 years of experience working on a professional web development team Demonstrated expertise with at least one RDBMS Demonstrated expertise with Mongo DB, Cosmos DB, DynamoDB, or similar document-oriented datastore ', 'At least 2 years of experience working on a professional web development team']",Associate,Contract,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,"U.S. Xpress, Inc.","Chattanooga, TN",3 weeks ago,50 applicants,"['', 'On-site work out facility', 'Why U.S. Xpress?', 'Tuition Reimbursement', 'Unlimited Vacation', 'Bachelors or Master’s Degree in Computer Science, Information Systems, or a related field.', 'Experience with ETL tools (such as Informatica)', 'Who We Are', 'Data Visualization with PowerBI / Tableau / QlikView or equivalent is a plus', ' Medical, Dental and Vision Unlimited Vacation Paid Parental Leave Tuition Reimbursement On-site work out facility ', 'Experience with Git and/or bitbucket is a plus. ', 'Medical, Dental and Vision', 'Some experience programming in Java & Python. Big data knowledge is a plus', 'Strong collaborative mindset, good judgment with great interpersonal skills required to help solve complex business problems.', 'Design logical data models and their physical schema design. ', 'Work Environment / Physical Requirements – Normal Office Settings.', 'Experience', 'Partner with Data Owners to design solutions that align to data governance and data management principles best practices.', 'High level expertise with SQL. Graph database experience is highly desired.', 'Develop robust, scalable solutions for collecting & analyzing large data sets. Must be proficient in creating & maintaining data pipelines.Proficiency in developing packages, stored procedures, functions, triggers, and complex SQL statements. Experience with ETL tools (such as Informatica)Design logical data models and their physical schema design. Some experience programming in Java & Python. Big data knowledge is a plusExperience with Git and/or bitbucket is a plus. Ability to work with multiple data sources and types (structured/semi-structured/unstructured)Cloud experience (AWS/Azure/Google)', 'What You’ll Do', 'POWER OF U.S.', 'Develop robust, scalable solutions for collecting & analyzing large data sets. Must be proficient in creating & maintaining data pipelines.', 'High level expertise with SQL. Graph database experience is highly desired.Partner with Data Owners to design solutions that align to data governance and data management principles best practices.Data Visualization with PowerBI / Tableau / QlikView or equivalent is a plusAbility to work in a fast-paced, agile and dynamic environment with both virtual and face-to-face interactions.Strong collaborative mindset, good judgment with great interpersonal skills required to help solve complex business problems.', 'What We’re Looking For', 'Ability to work with multiple data sources and types (structured/semi-structured/unstructured)', 'Cloud experience (AWS/Azure/Google)', 'Data Engineer', 'Proficiency in developing packages, stored procedures, functions, triggers, and complex SQL statements. ', 'Ability to work in a fast-paced, agile and dynamic environment with both virtual and face-to-face interactions.', 'Paid Parental Leave', 'Relentlessly Delivering Big Ideas.']",Entry level,Full-time,Information Technology,Construction,2021-03-24 13:05:10
Data Engineer (Remote),Advantage Resourcing,"San Francisco, CA",5 days ago,Be among the first 25 applicants,"['', 'About The Role', 'About Advantage Resourcing', 'Building and maintaining data pipelines in Python', 'Attention to detail that comes with dealing with extremely messy and', 'Strong documentation and communication so we can work best as a teamExperience with AWS EC2, RDS, and Lambda desired', 'Designing and implementing relational databases', 'Building and maintaining data pipelines in PythonProficiency in PostgresDesigning and implementing relational databasesResourcefulness and tenacity to source data and be on top of our vendorsAttention to detail that comes with dealing with extremely messy and', 'Resourcefulness and tenacity to source data and be on top of our vendors', 'Skills And Experience', 'Strong documentation and communication so we can work best as a team', 'Proficiency in Postgres', 'Experience with AWS EC2, RDS, and Lambda desired']",Entry level,Full-time,Information Technology,Construction,2021-03-24 13:05:10
Data Engineer,Big Cloud,San Francisco Bay Area,,N/A,"['', 'This exciting technology company with over 30 million users, is currently seeking to recruit a Data Engineer.', 'Are you an experienced Data Engineer? Do you have experience supporting data science/machine learning projects?', 'You will have…', 'Experience in building and deploying ETL data pipelines (Spark, ElasticSearch)', 'You’ll design, develop and deploy data processing and streaming platforms and data pipelines, capable of training algorithms with multi-millions of daily data points.', 'The company are about to launch their product globally and you will contribute by developing the new data products to help during this exciting period of growth.', 'As the Data Engineer, you’ll develop new backend systems for natural language, search, recommendation, computer vision and user behaviour applications.', 'Production level coding skills in Scala, Clojure or Python', 'Architectural mindset and the ability to look at problems holistically', 'Good communication skills', 'A minimum of 3-5 years experience in building and deploying production software', 'Experience with cloud platforms (AWS, GCP)', 'A minimum of 3-5 years experience in building and deploying production softwareProduction level coding skills in Scala, Clojure or PythonExperience in building and deploying ETL data pipelines (Spark, ElasticSearch)Experience with cloud platforms (AWS, GCP)Good communication skillsArchitectural mindset and the ability to look at problems holistically']",Mid-Senior level,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Data Engineer,Coppei,Greater Seattle Area,2 weeks ago,84 applicants,"['', 'Consulting', 'Develop and apply exceptional knowledge of common and emerging data platforms (ex: MuleSoft, SQL Server Integration Services SSIS, Talend, Alteryx, AWS Glue, Matillion, Fivetran or Azure Data Factory)', 'Legal Disclaimers', 'Job Responsibilities', 'Coppei\xa0Partners is an Equal Opportunity Employer – Minority / Women / Disability / Veteran / Gender Identity / Sexual Orientation / Age\xa0Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.\xa0Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities and activities may change at any time with or without notice.\xa0', 'Design and lead implementation of data and infrastructure solutions for client teams', 'Combine expertise in data pipelines and data engineering with innovative thinking to deliver excellent client workDesign and lead implementation of data and infrastructure solutions for client teamsCollaborate with and guide client partners on their technology stack of servicesParticipate in solution design discussions with a Coppei project teamConduct assessments of a company technology stack, identifying challenges and gaps, business and technical pain points, current state and future state scenariosPlan and manage delivery of complex solutionsDevelop and apply exceptional knowledge of common and emerging data platforms (ex: MuleSoft, SQL Server Integration Services SSIS, Talend, Alteryx, AWS Glue, Matillion, Fivetran or Azure Data Factory)', 'Plan and manage delivery of complex solutions', 'Preferred Qualifications', 'Consulting experience or having operated within a departmental or enterprise analytics shared service7+ years of experience as a data engineer, delivering end-to-end data technologies and solutions using scripting or coding4+ years working on cloud-based data solutions (ex: Azure, Google Cloud Platform GCP, or Amazon Web Services AWS)2+ years working with a cloud data warehouse or enterprise data warehouse EDW (ex: Redshift, Snowflake, Oracle, Teradata, Netezza, Vertica, Greenplum, SAP hana or Azure DW)Experience with relational database management systems RDBMS, Massively Parallel Processing MPP, or distributed data platformsExperience with solution delivery using agile or pod-based delivery methodologiesExperience with business Intelligence and visualization tools (ex: Looker, Tableau, Domo or Power\xa0BI)', 'Conduct assessments of a company technology stack, identifying challenges and gaps, business and technical pain points, current state and future state scenarios', 'The name\u202fCoppei\u202fis pronounced “copy-eye”. Our people are what make us exceptional. We are a career destination for those who love the consulting world but were ready to join a more personal firm where they would have a bigger impact and belong to a team that holds transparency, ethics, and consultant care as its highest priorities. We are seeking experienced, motivated individuals excited to join a fast-growing practice.\xa0', 'Legal Disclaimers\xa0', 'Experience with relational database management systems RDBMS, Massively Parallel Processing MPP, or distributed data platforms', 'Utilize best practices and methodologies and apply prior experiences to deliver high-quality client workOperate across business and technical organizations\xa0\xa0Collaborate with your Coppei team, while personally owning and driving key deliverablesStay on top of emerging technologies, and partner with market-leading technology organizations to understand and leverage their products', 'Combine expertise in data pipelines and data engineering with innovative thinking to deliver excellent client work', 'Consulting experience or having operated within a departmental or enterprise analytics shared service', 'Bachelor’s Degree\xa0or equivalent work experience', '2+ years working with a cloud data warehouse or enterprise data warehouse EDW (ex: Redshift, Snowflake, Oracle, Teradata, Netezza, Vertica, Greenplum, SAP hana or Azure DW)', 'Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities and activities may change at any time with or without notice.\xa0', 'Operate across business and technical organizations\xa0\xa0', 'Degree\xa0', 'Collaborate with and guide client partners on their technology stack of services', '\xa0', 'As a Data Engineering Senior Consultant at Coppei you will partner with clients to implement industry-leading technical solutions necessary to drive vital business decisions. You will be nimble yet methodical in the face of changing business priorities to deliver high-quality, scalable, and secure data solutions. Your expertise in developing data pipelines and cloud solutions will enable transformational outcomes for our clients. No project is the same – you will have the opportunity to work across technologies and learn while developing unique solutions to a breadth of challenges. If you are a data engineer with a consulting mindset, who thrives on shaping data solutions from vision through deployment, read on!', '7+ years of experience as a data engineer, delivering end-to-end data technologies and solutions using scripting or coding', 'Participate in solution design discussions with a Coppei project team', 'Data Engineering', 'Collaborate with your Coppei team, while personally owning and driving key deliverables', '4+ years of experience with scripting or coding (ex: SQL, Python, JavaScript, PHP, VBScript, Ruby or Java)', 'Coppei\xa0Partners is an Equal Opportunity Employer – Minority / Women / Disability / Veteran / Gender Identity / Sexual Orientation / Age\xa0', 'Experience with solution delivery using agile or pod-based delivery methodologies', '2+ years working on cloud-based data solutions (ex: Azure, AWS or Google Cloud) or cloud data warehouses (ex: Snowflake, Azure DW or Redshift)', 'Experience with business Intelligence and visualization tools (ex: Looker, Tableau, Domo or Power\xa0BI)', '4+ years of experience in data engineering', '4+ years working on cloud-based data solutions (ex: Azure, Google Cloud Platform GCP, or Amazon Web Services AWS)', 'Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.\xa0', '4+ years of experience in data engineering4+ years of experience with scripting or coding (ex: SQL, Python, JavaScript, PHP, VBScript, Ruby or Java)2+ years working on cloud-based data solutions (ex: Azure, AWS or Google Cloud) or cloud data warehouses (ex: Snowflake, Azure DW or Redshift)Bachelor’s Degree\xa0or equivalent work experience', 'Basic Qualifications', 'Stay on top of emerging technologies, and partner with market-leading technology organizations to understand and leverage their products', 'Utilize best practices and methodologies and apply prior experiences to deliver high-quality client work']",Mid-Senior level,Full-time,Information Technology,Management Consulting,2021-03-24 13:05:10
Data Support Engineer - Remote,Beacon Hill Staffing Group,"Milwaukee, WI",2 weeks ago,96 applicants,"['', 'This Engineer will be on a team that is touching both hardware and software but the main focus will be on data and cloud. This team is highly critical because all of the work they are doing now is going to impact the entire organization for the next several years. As an Engineer on this team, you will be working on:', 'Informatica (on prem and cloud solutions)Business ObjectsSASPower BIER Studio', 'SAS', 'Configurations', 'Business Objects', 'Tuning', 'Learn more about Beacon Hill Staffing Group and our specialty divisions, Beacon Hill Associates, Beacon Hill Financial, Beacon Hill HR, Beacon Hill Legal, Beacon Hill Life Sciences and Beacon Hill Technologies by visiting www.beaconhillstaffing.com. We look forward to working with you.Beacon Hill. Employing the Future™', 'Patches', 'Software installsPatchesUpgradesConfigurationsIntegrations to other internal systemsTuning', 'Integrations to other internal systems', 'Upgrades', 'Software installs', ""If you're interested in this role or know someone who is, please reach out to Rachael Siegfried at rsiegfried@beaconhillstaffing.com "", 'ER Studio', 'Power BI', 'Our client does not sponsor and is only considering W2 candidates at this time.', 'Informatica (on prem and cloud solutions)', ""Find Us on Facebook!Follow Us on Twitter!Beacon Hill is an Equal Opportunity Employer that values the strength diversity brings to the workplace. Individuals with Disabilities and Protected Veterans are encouraged to apply.Company Profile:Beacon Hill Technologies, a premier National Information Technology Staffing Group, provides world class technology talent across all industries utilizing a complete suite of staffing services.  Beacon Hill Technologies' dedicated team of recruiting and staffing experts consistently delivers quality IT professionals to solve our customers' technical and business needs. Beacon Hill Technologies covers a broad spectrum of IT positions, including Project Management and Business Analysis, Programming/Development, Database, Infrastructure, Quality Assurance, Production/Support and ERP roles."", 'Some of the day to day work will include:', 'One of our largest clients in the Milwaukee area is looking for a Data Support Engineer to join their team in a contract to hire role! This company is currently 100% remote and this Engineer can sit remote through Covid and beyond with travel expectations to the Midwest offices quarterly. This role is open due to growth and will be the first of many hires this year! ']",Entry level,Contract,Information Technology,Computer Hardware,2021-03-24 13:05:10
Data Engineer,Cortex Consultants LLC,"San Francisco, CA",5 days ago,Be among the first 25 applicants,"['', 'Required Skills']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,"ConsumerTrack, Inc.",United States,2 weeks ago,132 applicants,"['', 'Experience with Tableau or other reporting tools is a plus.', 'Competitive salary with excellent growth opportunity; we pride ourselves in having a team that exudes leadership, high initiative, creativity, and passion.Awesome medical, dental, and vision plans with heavy employer contributionPaid maternity leave and paternity leave programsPaid vacation, sick days, and holidaysCompany funding for outside classes and conferences to help you improve your skillsContribution to student loan debt payments after the first year of employment401(k) -- employees can start contributing immediately. After the first year, CTI matches your contribution up to 4% of your salary', 'Experience with AWS infrastructure.', 'Work with our data science and product management teams to design, rapid prototype, and productize new data product ideas and capabilities.', 'Functions/Responsibilities', 'Work with the data engineering team to migrate and enhance our existing Pentaho-based ETL pipeline to a new ELT-based/SaaS Integration system.', 'Conquer complex problems by finding new ways to solve with simple, efficient approaches with a focus on reliability, scalability, quality, and cost of our platforms.', 'Build and maintain multiple data pipelines to ingest new data sources (API and file-based) and support products used by both external users and internal teams.Optimize by building tools to evaluate and automatically monitor data quality, develop automated scheduling, testing, and distribution of feeds.Work with our data science and product management teams to design, rapid prototype, and productize new data product ideas and capabilities.Work with the data engineering team to migrate and enhance our existing Pentaho-based ETL pipeline to a new ELT-based/SaaS Integration system.Conquer complex problems by finding new ways to solve with simple, efficient approaches with a focus on reliability, scalability, quality, and cost of our platforms.Build processes supporting data transformation, data structures metadata, and workload management.Collaborate with the team to perform root cause analysis and audit internal and external data and processes to help answer specific business questions.', 'Knowledge for ETL/ELT tools like Informatica, IBM DataStage, or SaaS ETL tools is a plus.', 'Must have excellent troubleshooting and problem-solving skills.', 'Excellent communication and teamwork, and a passion for learning.', 'Our teams are working remotely 100% for the foreseeable future and have flex time. We’re in the digital media space so we’re mobile and flexible!*Option to work from an office (if you need to get away!)Tools & resources are available to keep our team connected across North America. (JIRA, Trello, Airtable, Slack, Zoom and so much more!)To keep our community of ConsumerTrackers engaged and connected, virtual team building events are held weekly and monthly.For wellness and balance, weekly virtual fitness classes such as yoga are available.To care for the local communities that we’re a part of across the U.S our team members host socially distanced philanthropic events every quarter.And most importantly, we’ve committed to consistent and transparent communication to help us all stay informed, engaged and to keep us on our path to success and #greatness.', 'Ability to operate in an agile, entrepreneurial start-up environment, and prioritize.', 'The ConsumerTrack has big growth plans ahead and is looking for a rockstar Data Engineer experienced in Data warehousing and Python to join our Data Engineering team. The CTI Data Engineering team is responsible for designing and developing the Data lake, enterprise database, data warehouse, reporting solutions, and pipelines for data processing. If you are a critical thinker with a solid track record of developing data solutions and solving complex problems with SQL and Python, we want you to join our team! You will play a vital role in designing and developing our next generation data pipelines and data platform. Join the team and prototype new data product ideas and concepts!', 'Basic Qualifications:', ""A note about our response to COVID -19 and our new norm: The world has changed and we know it’s important to adapt and to do our part to take care of our teams in this global pandemic. Our number one priority is to have our ConsumerTrackers feel safe, balanced and connected. We’re committed to providing our teams with the best resources and tools to navigate this new virtual world that we’re living in. We've also reinvented the ways in which we recognize, celebrate, and engage with each other to keep our culture strong!"", '*Option to work from an office (if you need to get away!)', 'To care for the local communities that we’re a part of across the U.S our team members host socially distanced philanthropic events every quarter.', 'Company funding for outside classes and conferences to help you improve your skills', 'We are an equal-opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law.', 'Collaborate with the team to perform root cause analysis and audit internal and external data and processes to help answer specific business questions.', 'Build processes supporting data transformation, data structures metadata, and workload management.', 'To keep our community of ConsumerTrackers engaged and connected, virtual team building events are held weekly and monthly.', 'Here’s a peek into our world at ConsumerTrack -', 'Strong skills to write complex, highly-optimized SQL queries across large volumes of data.', 'Our teams are working remotely 100% for the foreseeable future and have flex time. We’re in the digital media space so we’re mobile and flexible!', 'Master’s Degree (or a B.S. degree with relevant industry experience) in math, statistics, computer science, or equivalent technical field.3+ years of professional Dimensional Data Warehousing/Data Modeling and ‘Big Data’ Experience.Strong skills to write complex, highly-optimized SQL queries across large volumes of data.Comfortable working directly with data analytics to bridge business requirements with data engineering.Experience with AWS infrastructure.Must have excellent troubleshooting and problem-solving skills.Ability to operate in an agile, entrepreneurial start-up environment, and prioritize.Excellent communication and teamwork, and a passion for learning.Curiosity and passion for data, visualization, and solving problems.Willingness to question the validity, accuracy of data and assumptions.', 'Build and maintain multiple data pipelines to ingest new data sources (API and file-based) and support products used by both external users and internal teams.', 'Optimize by building tools to evaluate and automatically monitor data quality, develop automated scheduling, testing, and distribution of feeds.', 'Curiosity and passion for data, visualization, and solving problems.', 'Competitive salary with excellent growth opportunity; we pride ourselves in having a team that exudes leadership, high initiative, creativity, and passion.', 'Experience with Redshift, Snowflake, or other MPP databases is a plus.', 'Paid maternity leave and paternity leave programs', 'Experience with Redshift, Snowflake, or other MPP databases is a plus.Knowledge for ETL/ELT tools like Informatica, IBM DataStage, or SaaS ETL tools is a plus.Experience with Tableau or other reporting tools is a plus.', '3+ years of professional Dimensional Data Warehousing/Data Modeling and ‘Big Data’ Experience.', 'Benefits', '401(k) -- employees can start contributing immediately. After the first year, CTI matches your contribution up to 4% of your salary', 'Tools & resources are available to keep our team connected across North America. (JIRA, Trello, Airtable, Slack, Zoom and so much more!)', 'And most importantly, we’ve committed to consistent and transparent communication to help us all stay informed, engaged and to keep us on our path to success and #greatness.', 'Paid vacation, sick days, and holidays', 'Contribution to student loan debt payments after the first year of employment', 'Preferred Qualifications:', 'Master’s Degree (or a B.S. degree with relevant industry experience) in math, statistics, computer science, or equivalent technical field.', 'For wellness and balance, weekly virtual fitness classes such as yoga are available.', 'ConsumerTrack™ is unique in the digital marketing and media industry - we combine marketing, digital, content and fintech. Our performance based approach increases brand awareness and generates targeted audience engagement on our internal web properties and partner sites.', 'Willingness to question the validity, accuracy of data and assumptions.', 'Awesome medical, dental, and vision plans with heavy employer contribution', 'Comfortable working directly with data analytics to bridge business requirements with data engineering.']",Associate,Full-time,Advertising,Internet,2021-03-24 13:05:10
Data Engineer,"Adroit Resources, Inc.","San Jose, CA",3 weeks ago,145 applicants,"['', 'Skills', 'Collaborate with analytics, data scientists and product teams to assist in building and optimizing innovative products and services to market', 'Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.', 'Keep data separated and secure across architectural boundaries through multiple data centres and AWS regions.', '\ufeffHadoop, Spark, Kafka, SQL, NoSQL, Cassandra/MongoDB, AWS/Azure/GCP, Python/Java/C++/Scala', 'Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.', ""What You'll Do"", '8+ years of experience in a Data Engineer role', 'Develop analytics tools that utilize data pipeline to provide actionable insights into customer acquisition, operational efficiency and key business performance metrics.', 'Experience with big data tools: Hadoop, Spark, Kafka, etc.', 'What You Know', 'Optimize data delivery, automate manual processes, re-design infrastructure for improved scalability and identify, design and implement data ingestion, data delivery process improvements.', '8+ years of experience in a Data Engineer roleGraduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.Experience with big data tools: Hadoop, Spark, Kafka, etc.Experience with relational SQL and NoSQL databases such as Cassandra.Experience with AWS cloud services: EC2, EMR, AthenaExperience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.', 'Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.', 'Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.', 'Optimize data delivery, automate manual processes, re-design infrastructure for improved scalability and identify, design and implement data ingestion, data delivery process improvements.Collaborate with analytics, data scientists and product teams to assist in building and optimizing innovative products and services to marketDevelop analytics tools that utilize data pipeline to provide actionable insights into customer acquisition, operational efficiency and key business performance metrics.Build big data pipelines, data sets and conduct performance tuningWorking knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.Experience supporting and working with cross-functional teams in a dynamic environmentKeep data separated and secure across architectural boundaries through multiple data centres and AWS regions.', 'Build big data pipelines, data sets and conduct performance tuning', 'Experience supporting and working with cross-functional teams in a dynamic environment', 'Experience with AWS cloud services: EC2, EMR, Athena', 'Experience with relational SQL and NoSQL databases such as Cassandra.']",Mid-Senior level,Contract,Information Technology,Staffing and Recruiting,2021-03-24 13:05:10
Data Engineer,North Highland,"Portland, OR",7 days ago,38 applicants,"['', '  Ability to understand and translate the need of business units into effective data management solutions.   Experience in data validation and ability to analyze large datasets.   Excellent interpersonal skills, including collaboration, facilitation, and negotiation.   5+ years of experience as a Data Engineer or in a similar role   Informatica development and administration experience.   Experience with data modeling, data warehousing, and building ETL pipelines.   Experience in SQL.   Experience with data modeling, data warehouse design, and building data marts and data pipelines.   Design and implement system modifications.   Proficiency with RDBMS technology fundamentals, SQL and Query optimization.   Strong troubleshooting and diagnostic skills to determine and resolve issues.   Experience supporting, architecting, and designing requirements with respect to the flow of data between SAP and non-SAP systems preferred.   Ability to optimize the performance of enterprise business intelligence solutions by means redesigning or re-architecting data structures or data flows.   Experience working in the Microsoft Azure cloud environment.   Background in Gas Utilities industry preferred.   Understanding and knowledge of IT standards and controls, IT Service Management [ITSM, ITIL] methods  ', ' Experience supporting, architecting, and designing requirements with respect to the flow of data between SAP and non-SAP systems preferred. ', ' Design and implement system modifications. ', ' Experience with data modeling, data warehouse design, and building data marts and data pipelines. ', ' Ability to understand and translate the need of business units into effective data management solutions. ', ' Ability to optimize the performance of enterprise business intelligence solutions by means redesigning or re-architecting data structures or data flows. ', ' Applicants must be authorized to work in the United States without the need for visa sponsorship by North Highland. Work visa sponsorship will not be provided, now or in the future, for this position. North Highland makes change happen for organizations who dare to be different. By melding workforce, customer and operational transformation, they are one of the world’s leading consulting groups, with 65+ offices around the globe. They break new ground today so tomorrow is easier to explore.For more information, visit northhighland.com and connect with us on LinkedIn , Twitter and Facebook .North Highland is an Equal Employment Opportunity (EEO)/Affirmative Action employer. All qualified applicants will receive fair and impartial consideration without regard to race, color, sex, gender identity, religion, national origin, age, sexual orientation, disability, veteran status, or any other characteristic protected by law.', ' Click ', ' Experience with data modeling, data warehousing, and building ETL pipelines. ', ' 5+ years of experience as a Data Engineer or in a similar role ', ' MAKE CHANGE HAPPEN. ', ' Background in Gas Utilities industry preferred. ', 'LEAVE YOUR MARK ON A BETTER WORLD. ', '  Applicants must be authorized to work in the United States without the need for visa sponsorship by North Highland. Work visa sponsorship will not be provided, now or in the future, for this position. North Highland makes change happen for organizations who dare to be different. By melding workforce, customer and operational transformation, they are one of the world’s leading consulting groups, with 65+ offices around the globe. They break new ground today so tomorrow is easier to explore.For more information, visit northhighland.com and connect with us on LinkedIn , Twitter and Facebook .North Highland is an Equal Employment Opportunity (EEO)/Affirmative Action employer. All qualified applicants will receive fair and impartial consideration without regard to race, color, sex, gender identity, religion, national origin, age, sexual orientation, disability, veteran status, or any other characteristic protected by law.', ' Understanding and knowledge of IT standards and controls, IT Service Management [ITSM, ITIL] methods ', ' Experience in data validation and ability to analyze large datasets. ', ' Experience in SQL. ', ' to apply ', ' HERE ', ' Proficiency with RDBMS technology fundamentals, SQL and Query optimization. ', ' Informatica development and administration experience. ', ' Excellent interpersonal skills, including collaboration, facilitation, and negotiation. ', ' Strong troubleshooting and diagnostic skills to determine and resolve issues. ', 'Skills Needed', ' Experience working in the Microsoft Azure cloud environment. ', ' COLLABORATE WITH AMAZING PEOPLE. ']",Entry level,Contract,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,StockX,"Seattle, WA",2 weeks ago,124 applicants,"['', 'Qualifications', 'Nice To Have', 'Responsibilities']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
"Data Engineer, Operations Decision Science",Delta Air Lines,"Atlanta, GA",2 weeks ago,49 applicants,"['', ""Provide technical leadership to the Delta's business units"", 'Define and execute the data engineering roadmap (e.g. establish an operational data lake and a real-time reporting environment for the operations)', 'Practices safety-conscious environment resulting in employee safety and well-being', 'Strong project management, organizational, and prioritizations skills', 'Mentor junior members in technical proficiency and business acumen', 'Working knowledge of statistical/machine learning tools (e.g. SAS,R, TensorFlow) preferredWorking knowledge of ""Big Data"" solutions such as Hadoop, NoSQL, MapReduce, etc preferredWorking knowledge of API Consumption preferredStrong written, oral communication, and interpersonal skillsStrong project management, organizational, and prioritizations skillsMust be able to interact and collaborate at all levels within Operations Analysis & Performance, OCC, cross-divisional working groups, and outside entitiesMust be performing satisfactorily in present position', 'Proficiency in Python', ""Bachelor's degree in Data Science, Statistics, Mathematics, Operations Research, Computer Science, or equivalent combination of education and experience (Master's degree preferred)Embraces diverse people, thinking and stylesConsistently makes safety and security, of self and others, the priorityProficiency in SQL (CTE, window functions, temporal data)Proficiency in Python"", 'Lead complex process improvement and project management engagements for both individual business units and cross-divisional initiatives.', 'Working knowledge of statistical/machine learning tools (e.g. SAS,R, TensorFlow) preferred', ""Bachelor's degree in Data Science, Statistics, Mathematics, Operations Research, Computer Science, or equivalent combination of education and experience (Master's degree preferred)"", 'Must be able to interact and collaborate at all levels within Operations Analysis & Performance, OCC, cross-divisional working groups, and outside entities', 'Partner with Information Technology to optimize and enhance the database environment for optimal efficiency and best practices', 'Working knowledge of ""Big Data"" solutions such as Hadoop, NoSQL, MapReduce, etc preferred', 'Primary Functions', 'Strong written, oral communication, and interpersonal skills', 'Embraces diverse people, thinking and styles', 'Leverage emerging technologies and identify efficient and meaningful ways to disseminate data and analysis in order to satisfy the business’ needs', 'Working knowledge of API Consumption preferred', 'Proficiency in SQL (CTE, window functions, temporal data)', 'Must be performing satisfactorily in present position', 'Design, implement data engineering solutions (e.g. locate and extract data from a variety of sources for use in reporting, analysis, and statistical modeling to drive continuous improvement)', ""Design, implement data engineering solutions (e.g. locate and extract data from a variety of sources for use in reporting, analysis, and statistical modeling to drive continuous improvement)Define and execute the data engineering roadmap (e.g. establish an operational data lake and a real-time reporting environment for the operations)Partner with Information Technology to optimize and enhance the database environment for optimal efficiency and best practicesProvide technical leadership to the Delta's business unitsLeverage emerging technologies and identify efficient and meaningful ways to disseminate data and analysis in order to satisfy the business’ needsMentor junior members in technical proficiency and business acumenLead complex process improvement and project management engagements for both individual business units and cross-divisional initiatives.Interface with business unit leaders to develop and maintain internal customer relationshipsPractices safety-conscious environment resulting in employee safety and well-being"", 'Consistently makes safety and security, of self and others, the priority', 'Interface with business unit leaders to develop and maintain internal customer relationships']",Entry level,Full-time,Information Technology,Airlines/Aviation,2021-03-24 13:05:10
Data Engineer,Capital Group,"San Antonio, TX",4 weeks ago,36 applicants,"['', '  Excellent technical knowledge of performance tuning and query optimization across large data sets, and exposure to bottlenecks at the storage, network or compute layers   Working experience with data cataloging and enablement through APIs   Excellent understanding of traditional RDBMS   You have strengths in leadership, interpersonal, and problem solving skills with the ability to continually learn new concepts and technologies and effectively apply them   ', ' Excellent understanding of traditional RDBMS ', ' Working experience with data cataloging and enablement through APIs ', 'Location', ' Receive 2-for-1 matching gifts for your charitable contributions and the opportunity to secure annual grants for the organizations you love ', ' You write and optimize advanced SQL queries with large-scale, complex datasets ', "" You have a bachelor's degree in Computer Science, Engineering or a related technical field "", '""I can succeed as a Data Engineer at Capital Group.""', 'I am the person Capital Group is looking for.""', ' Access on-demand professional development resources that allow you to hone existing skills and learn new ones ', ' You have 3-5 years of experience delivering data solutions, including cloud technologies like AWS ', 'Req ID', ' You have experience with integration of data from multiple sources and a knowledge of ETL/ELT technologies and tools like Informatica and AWS solutions ', ""COVID-19 HIRING Our recruiting and onboarding activities are virtual during the pandemic and we've transitioned to a work-from-home environment until further notice. We are offering generous work-from-home benefits to improve our associate's ability to work remotely. "", '""I can be myself at work.""', 'Travel required', 'Complimentary Experience', '""I can learn more about Capital Group."" ', ' You have a solid background with data analysis and modelling ', '""I can influence my income.""', '""I can apply in less than 4 minutes.""', 'Other location(s)', 'Nearest Major Market ', "" You're well-versed in Machine Learning and data mining "", ' You have coding proficiency in at least one modern language such as Python, R and Spark ', 'Job Segment ', '   Excellent technical knowledge of performance tuning and query optimization across large data sets, and exposure to bottlenecks at the storage, network or compute layers   Working experience with data cataloging and enablement through APIs   Excellent understanding of traditional RDBMS   You have strengths in leadership, interpersonal, and problem solving skills with the ability to continually learn new concepts and technologies and effectively apply them    ', 'Relocation benefits offered', ' Enjoy generous time-away and health benefits from day one, with the opportunity for flexible work options ', ' You have experience in cloud-first design, preferably AWS or Azure ', ""  You have a bachelor's degree in Computer Science, Engineering or a related technical field   You have 3-5 years of experience delivering data solutions, including cloud technologies like AWS   You write and optimize advanced SQL queries with large-scale, complex datasets   You have experience with integration of data from multiple sources and a knowledge of ETL/ELT technologies and tools like Informatica and AWS solutions   You have a solid background with data analysis and modelling   You have coding proficiency in at least one modern language such as Python, R and Spark   You have experience in cloud-first design, preferably AWS or Azure   You're well-versed in Machine Learning and data mining   "", ' You have strengths in leadership, interpersonal, and problem solving skills with the ability to continually learn new concepts and technologies and effectively apply them ', ""   You have a bachelor's degree in Computer Science, Engineering or a related technical field   You have 3-5 years of experience delivering data solutions, including cloud technologies like AWS   You write and optimize advanced SQL queries with large-scale, complex datasets   You have experience with integration of data from multiple sources and a knowledge of ETL/ELT technologies and tools like Informatica and AWS solutions   You have a solid background with data analysis and modelling   You have coding proficiency in at least one modern language such as Python, R and Spark   You have experience in cloud-first design, preferably AWS or Azure   You're well-versed in Machine Learning and data mining    "", ' Excellent technical knowledge of performance tuning and query optimization across large data sets, and exposure to bottlenecks at the storage, network or compute layers ', '  Excellent technical knowledge of performance tuning and query optimization across large data sets, and exposure to bottlenecks at the storage, network or compute layers   Working experience with data cataloging and enablement through APIs   Excellent understanding of traditional RDBMS   You have strengths in leadership, interpersonal, and problem solving skills with the ability to continually learn new concepts and technologies and effectively apply them  ', ""  You have a bachelor's degree in Computer Science, Engineering or a related technical field   You have 3-5 years of experience delivering data solutions, including cloud technologies like AWS   You write and optimize advanced SQL queries with large-scale, complex datasets   You have experience with integration of data from multiple sources and a knowledge of ETL/ELT technologies and tools like Informatica and AWS solutions   You have a solid background with data analysis and modelling   You have coding proficiency in at least one modern language such as Python, R and Spark   You have experience in cloud-first design, preferably AWS or Azure   You're well-versed in Machine Learning and data mining  "", '""I can lead a full life.""', '  Enjoy generous time-away and health benefits from day one, with the opportunity for flexible work options   Receive 2-for-1 matching gifts for your charitable contributions and the opportunity to secure annual grants for the organizations you love   Access on-demand professional development resources that allow you to hone existing skills and learn new ones  ']",Not Applicable,Full-time,Information Technology,Financial Services,2021-03-24 13:05:10
Data Engineer,W. R. Berkley Corporation,"Manassas, VA",4 weeks ago,37 applicants,"['', ' 3+ years of experience in a Data Engineer role ', ' Knowledge of cloud based data warehousing products such as Snowflake', ' Assemble large, complex data sets that meet functional/non-functional business requirements. ', ' A budget for continual improvement ', 'Skills You’ll Need', ' Bachelor’s Degree ', ' Bachelor’s Degree  3+ years of experience in a Data Engineer role  3+ years of experience with relational SQL databases.  1+ year of experience with object-oriented/object function scripting languages like Python.  1+ year of experience working with or understanding formal ETL tools  Knowledge of cloud based data warehousing products such as Snowflake', ' An engaged and supportive leadership team that will invest in you ', 'What We’ll Bring', ' Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery/ dataOps. ', ' A broad group of industry experts who work closely with us on everything we do ', ' Talented engineering teams to build products with ', ' 1+ year of experience working with or understanding formal ETL tools ', ' An engaged and supportive leadership team that will invest in you  Talented engineering teams to build products with  A broad group of industry experts who work closely with us on everything we do  A budget for continual improvement  Generous retirement plan  Excellent medical and dental insurance (and other health benefits) ', ' Create and maintain optimal data pipeline architecture. ', ' Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics. ', 'Responsibilities', ' 3+ years of experience with relational SQL databases. ', 'Company Details', 'Qualifications', ' Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL. ', ' Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. ', 'What We’re Looking For', ' 1+ year of experience with object-oriented/object function scripting languages like Python. ', ' Generous retirement plan ', ' Create and maintain optimal data pipeline architecture.  Assemble large, complex data sets that meet functional/non-functional business requirements.  Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery/ dataOps.  Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL.  Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.  Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. ', ' Excellent medical and dental insurance (and other health benefits) ']",Associate,Full-time,Information Technology,Financial Services,2021-03-24 13:05:10
Data Engineer,Fortive,"Pittsburgh, PA",2 weeks ago,Be among the first 25 applicants,"['', ' MS or PhD in Computer Science or other technical field Knowledge of machine learning and data science processes Experience supporting data science and analytical efforts is preferred Hands-on experience in Snowflake ', 'Passion for Innovation: A demonstrated interest and desire to participate in innovation through personal study, on-the-job initiative, or other endeavors.', 'Good unordaining of Data Warehouse and Business Intelligence environment', 'Excellent problem solving, critical thinking, and communication skills', ' Design and implement real-time ETL and batch processing of data to support modeling and reporting needs Connect data to users in multi/mixed cloud and on-prem environments Define and share best practices and design for the management of data with The Fort and across Fortive OpCos Partner with data scientists to support modeling and reporting needs and to build and maintain internal data processing and visualization  Translate requests into replicable analytic reports using varying applications Create tools to serve data such as APIs and packages as needed ', ' FOR000662', 'Preferred Qualifications', 'Experience supporting data science and analytical efforts is preferred', 'Comfort with Ambiguity: A willingness and aptitude for spending time in and thriving with deep uncertainty and environments where there is no clear “right answer”.', 'Design and implement real-time ETL and batch processing of data to support modeling and reporting needs', 'Translate requests into replicable analytic reports using varying applications', 'These Are The Traits We Value', 'Required Qualifications', 'Alongside a team of entrepreneurial, high-performing, curious people, you’ll deliver breakthrough solutions to drive sustainable growth for Fortive. As a Lead Data Scientist, you’ll drive execution and play a vital role in building a culture of innovation at Fortive. You will build, manage, and inspire alongside a multi-functional team, that solves for our toughest growth challenges.', 'Empathy & a Teacher’s Mindset: The ability to teach and mentor effectively, successfully facilitate teams and different personalities in training sessions, and to care deeply about his/her colleague’s growth, understanding, and experience.', 'Some experience with Azure environment such as Synapse, Azure SQL DB, Azure ML Pipeline, Azure Data Factory', 'Knowledge of machine learning and data science processes', 'Some experience with MLOps either in AWS or Azure', 'Proficiency with Python, databases and SQL expertise', 'Ability to Deliver Results: A track record of setting vision and strategy, organize and lead a team, and implement to meet and exceed expectations.', 'Positive Outlook: A desire to look past the challenged in search of the opportunity.', 'Bias for Action: A need for speed; demonstrated ability to make decisions quickly and to act upon them.', 'Connect data to users in multi/mixed cloud and on-prem environments', 'Responsibilities', 'Define and share best practices and design for the management of data with The Fort and across Fortive OpCos', 'Must have worked with AWS compute infrastructure such as Lambda, ECS, EC2, Docker, Cloudformation, Step function, Glue, SageMaker', 'Create tools to serve data such as APIs and packages as needed', ' Ability to Deliver Results: A track record of setting vision and strategy, organize and lead a team, and implement to meet and exceed expectations. Entrepreneurial Attitude: A proactive outlook that provides the chutzpah needed to overcome barriers, creatively problem solve, and challenge conventional thinking. Comfort with Ambiguity: A willingness and aptitude for spending time in and thriving with deep uncertainty and environments where there is no clear “right answer”. Passion for Innovation: A demonstrated interest and desire to participate in innovation through personal study, on-the-job initiative, or other endeavors. Positive Outlook: A desire to look past the challenged in search of the opportunity. Empathy & a Teacher’s Mindset: The ability to teach and mentor effectively, successfully facilitate teams and different personalities in training sessions, and to care deeply about his/her colleague’s growth, understanding, and experience. Bias for Action: A need for speed; demonstrated ability to make decisions quickly and to act upon them. Alongside a team of entrepreneurial, high-performing, curious people, you’ll deliver breakthrough solutions to drive sustainable growth for Fortive. As a Lead Data Scientist, you’ll drive execution and play a vital role in building a culture of innovation at Fortive. You will build, manage, and inspire alongside a multi-functional team, that solves for our toughest growth challenges. ', 'Proficiency with multi cloud environment such as AWS and Azure', 'Experience with distributed version control system (e.g. git)', 'MS or PhD in Computer Science or other technical field', "" Bachelor's degree in Computer Science or related technical field or equivalent practical experience  Good unordaining of Data Warehouse and Business Intelligence environment Proficiency with Python, databases and SQL expertise Proficiency with multi cloud environment such as AWS and Azure Must have worked with AWS compute infrastructure such as Lambda, ECS, EC2, Docker, Cloudformation, Step function, Glue, SageMaker Some experience with Azure environment such as Synapse, Azure SQL DB, Azure ML Pipeline, Azure Data Factory Snowflake experience is a plus Some experience with MLOps either in AWS or Azure Excellent problem solving, critical thinking, and communication skills Experience with distributed version control system (e.g. git) "", 'Partner with data scientists to support modeling and reporting needs and to build and maintain internal data processing and visualization ', 'Snowflake experience is a plus', 'Entrepreneurial Attitude: A proactive outlook that provides the chutzpah needed to overcome barriers, creatively problem solve, and challenge conventional thinking.', ""Bachelor's degree in Computer Science or related technical field or equivalent practical experience "", 'Hands-on experience in Snowflake']",Not Applicable,Full-time,Information Technology,Medical Devices,2021-03-24 13:05:10
Sr Data Engineer - Core Data team,Disney Streaming Services,"New York, NY",1 week ago,27 applicants,"['', 'New York City preferredRemote possible for the right candidate who is willing to work EST hours from within the USA', 'Traditional relational databases and/or distributed systems such as Hadoop/Hive, BigQuery, Redshift, Snowflake', 'Technology Skills & Experience', 'Drive and maintain a culture of quality, innovation and experimentation', 'Experience with terabyte or petabyte scale data systems', 'Workflow management tools such as Airflow', 'Job Summary', 'Coach data engineers best practices and technical concepts of building large scale data platforms', '5+ years of relevant professional experienceExperience with terabyte or petabyte scale data systemsProficiency in Scala (or willingness to learn)', 'Work in an Agile environment that focuses on collaboration and teamwork', 'Hadoop (or similar) Ecosystem (MapReduce, Yarn, HDFS, Hive, Spark, Presto, Pig, HBase)', 'Additional Information', 'New York City preferred', 'Develop high performance Spark ETLs in Scala and PythonDevelop unit/integration tests and data validations to ensure the quality of code and dataDrive and maintain a culture of quality, innovation and experimentationWork in an Agile environment that focuses on collaboration and teamworkCoach data engineers best practices and technical concepts of building large scale data platforms', 'Proficiency in Scala (or willingness to learn)', 'Proficiency with Scala or Python (preferably both)', 'Proficiency with Scala or Python (preferably both)Traditional relational databases and/or distributed systems such as Hadoop/Hive, BigQuery, Redshift, SnowflakeStreaming platforms such as Flink, SparkWorkflow management tools such as AirflowHadoop (or similar) Ecosystem (MapReduce, Yarn, HDFS, Hive, Spark, Presto, Pig, HBase)Data exploration and data visualization tools such as Looker, Tableau, Chartio, Apache Superset, Plotly / Dash ', 'Preferred Education', 'Responsibilities', 'Remote possible for the right candidate who is willing to work EST hours from within the USA', 'Data exploration and data visualization tools such as Looker, Tableau, Chartio, Apache Superset, Plotly / Dash ', '5+ years of relevant professional experience', 'Streaming platforms such as Flink, Spark', 'Develop high performance Spark ETLs in Scala and Python', 'Basic Qualifications', 'Develop unit/integration tests and data validations to ensure the quality of code and data']",Mid-Senior level,Full-time,Quality Assurance,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer II,Blue Cross and Blue Shield of Kansas City,"Kansas City, MO",1 day ago,Be among the first 25 applicants,"['', 'Bachelor’s degree in Computer Science, Computer Engineering, Information Systems. or related field, or an equivalent combination of education, training, and experience.', 'Experience with Apache Airflow and Celery . ', 'Experience with DLM and Enterprise Platforms for managing data movement.', ' Experience implementing, managing, and developing in Apache Airflow or other orchestration tools. ', ' Experience implementing data solutions in a m ajor cloud provider ( Azure , AWS or GCP) . ', 'Minimum Qualifications', 'Preferred Qualifications', 'Works closely with application developers, architects, and engineers to ensure that we produce high-quality code and infrastructure.', 'Experience implementing Kafka, Schema Registry and Kafka Streaming and/or Spark Streaming.', '3+ years of SQL development.', ' Experience with Snowflake or Synapse. ', 'Maintains the high quality of our data pipelines in production, ensuring data quality and performance.', 'Job Description Summary', 'Designs and implements the data pipelines and transformations necessary for our new development.Ensures quality and accuracy of data used by performing data quality measurement and analysis.Works closely with application developers, architects, and engineers to ensure that we produce high-quality code and infrastructure.Maintains the high quality of our data pipelines in production, ensuring data quality and performance.Stays current with rapidly developing data technologies and tools. Shares knowledge with other Blue KC resources.Develops strategies, standards, and best practices for data ingestion and integration', ' Basic understanding of insurance industry (including claims processes, how attribution works, and the core business concepts of insurance) ', 'Ensures quality and accuracy of data used by performing data quality measurement and analysis.', ' Experience with NodeJS and React. ', ' Highly technical with hands-on experience using Python, or JVM language ( Java , Scala, Clojure, or Kotlin ) . ', 'Experience with Microsoft Azure. ', ' Experience with Scala, Python and Java. ', ' Experience with semi-structured formats ( Avro, Parquet, ORC, YAML , JSON, or XML). ', ' Experience with Scrum and Agile development practices ', ' Experience with Scrum and Agile development practices Experience implementing Kafka, Schema Registry and Kafka Streaming and/or Spark Streaming.Experience with Microsoft Azure. Experience with Apache Airflow and Celery .  Experience with Docker, Kubernetes and Terraform.  Experience with Snowflake or Synapse.  Experience with Scala, Python and Java.  Experience with NodeJS and React.  Experience with Azure Data Lake Storage Gen2.', 'Develops strategies, standards, and best practices for data ingestion and integration', ' Experience with Docker, Kubernetes and Terraform. ', ' Experience with Azure Data Lake Storage Gen2.', 'Stays current with rapidly developing data technologies and tools. Shares knowledge with other Blue KC resources.', 'Bachelor’s degree in Computer Science, Computer Engineering, Information Systems. or related field, or an equivalent combination of education, training, and experience.3+ years’ experience in ETL or ELT utilizing enterprise tools (DataStage, Informatica IICS, Matillion, others).3+ years of SQL development.Experience with DLM and Enterprise Platforms for managing data movement. Basic understanding of insurance industry (including claims processes, how attribution works, and the core business concepts of insurance)  Highly technical with hands-on experience using Python, or JVM language ( Java , Scala, Clojure, or Kotlin ) .  Experience implementing, managing, and developing in Apache Airflow or other orchestration tools.  Experience implementing data solutions in a m ajor cloud provider ( Azure , AWS or GCP) .  Experience with semi-structured formats ( Avro, Parquet, ORC, YAML , JSON, or XML). ', '3+ years’ experience in ETL or ELT utilizing enterprise tools (DataStage, Informatica IICS, Matillion, others).', 'Job Description', 'Designs and implements the data pipelines and transformations necessary for our new development.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Databricks Data Engineer,JCW,United States,3 weeks ago,85 applicants,"['', 'Experience with the following tools: SQL or relational databases, Python, data pipeline management such as SSIS, Azure Data Factory or Apache Spark for batch and streaming.', 'Corey Rague recruits Risk and Analytics professionals across the financial services sector and would be happy to provide further details about this or other positions. You can reach him at (646) 517-8533 or at corey.rague@jcwresourcing.com', 'A minimum of 2 years of experience in a Data Engineer role.', 'Create and maintain optimal data pipeline architecture, including optimal extraction, transformation, and loading of data, leveraging databricks note books, Apache Spark (SQL, Python, Scala), and other Microsoft Azure or open-source tools as required.', 'Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.', 'Create and manage large, complex data sets that meet business requirements and serve as the foundation behind analytics and management reporting, informing critical decision-making across the organization.', 'Responsibilities include:', 'Position requirements:', 'Create and manage large, complex data sets that meet business requirements and serve as the foundation behind analytics and management reporting, informing critical decision-making across the organization.Create and maintain optimal data pipeline architecture, including optimal extraction, transformation, and loading of data, leveraging databricks note books, Apache Spark (SQL, Python, Scala), and other Microsoft Azure or open-source tools as required.Maintain high-quality documentation about data structures in shared data governance tools for the business to access.Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.', 'A minimum of 2 years of experience in a Data Engineer role.Proficiency in databricks and apache sparkExperience with the following tools: SQL or relational databases, Python, data pipeline management such as SSIS, Azure Data Factory or Apache Spark for batch and streaming.Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.', 'Maintain high-quality documentation about data structures in shared data governance tools for the business to access.', 'Proficiency in databricks and apache spark', 'JCW\xa0is currently working on a search for a Risk Data Engineer with a leading consumer lending organization for their Portland, Oregon location.\xa0The qualified candidate\xa0will build and optimize foundational data marts, which will support reporting and analysis across the Risk Department, and other areas in the organization. This will include building data pipeline architecture, data flows, data transformations, and clear documentation. The incumbent will be joining the Risk analytics team and partner with data owners and users to build, problem-solve, and optimize key data structures. As a firm, they’re transitioning to Azure cloud and databricks analytics platform.', 'Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.', 'Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.', '\xa0']",Mid-Senior level,Full-time,Finance,Banking,2021-03-24 13:05:10
Data Engineer,"KYM ADVISORS, INC.","Camp Springs, KY",1 week ago,Be among the first 25 applicants,"['', 'Forward-thinking approach with interest in highlighting internal process improvement with respect to automating manual processes, optimizing data delivery, improving infrastructure for greater scalability, etc.', 'Experience with Graph databases like Neo4j or GraphFrames', 'Experience working within an Agile and/or Scrum environment preferred', 'Life and AD&D insurance', 'Advanced working SQL knowledge with experience working with relational and NoSQL databases, Apache Spark, and Spark SQL', 'Experience working within enterprise data security management policies and procedures', 'Implements real time and batch processing requirements as well as graph databases in large-scale data environments.', 'Current DHS clearance or prior government agency security clearance strongly preferred', ' Current DHS clearance or prior government agency security clearance strongly preferred Experience with Databricks as a platform strongly preferred 2+ years’ experience in machine learning (ML), artificial intelligence (AI), and/or natural language processing (NLP) preferred Experience working within an Agile and/or Scrum environment preferred Experience with Graph databases like Neo4j or GraphFrames Experience with advanced analytics solutions (data science, machine learning, data modeling, etc.) including infrastructure (e.g., data engineering) preferred Experience in an environment using leading data science and machine learning platforms (Databricks, SAS, Anaconda, Alteryx, etc.) preferred Consulting experience at large consulting firm (e.g. Booz Allen Hamilton, Deloitte, Accenture, Cap Gemini) nice-to-have Experience with Tableau and SAS is nice-to-have ', ' Medical Dental Vision Short-term Disability Long-term Disability Life and AD&D insurance 401K Plan with employer match', 'Prior experience utilizing APIs to interface with Apache Spark; experience utilizing PySpark', 'Experience with advanced analytics solutions (data science, machine learning, data modeling, etc.) including infrastructure (e.g., data engineering) preferred', 'Consulting experience at large consulting firm (e.g. Booz Allen Hamilton, Deloitte, Accenture, Cap Gemini) nice-to-have', '2+ years’ experience in machine learning (ML), artificial intelligence (AI), and/or natural language processing (NLP) preferred', 'Short-term Disability', 'Dental', 'Requirements', ' Bachelor’s Degree in Information Technology, Engineering, Computer Science, Machine Learning, Statistics or related fields 3+ years’ experience in IT, data analytics, data engineering, or related discipline Advanced working SQL knowledge with experience working with relational and NoSQL databases, Apache Spark, and Spark SQL Previous hands-on coding and scripting experience in languages such as Python and Scala Prior experience utilizing APIs to interface with Apache Spark; experience utilizing PySpark Experience working within enterprise data security management policies and procedures Due to COVID-19, certain on-site client work has been suspended until further notice, however, on-site work may be required on a case-by-case basis for this position Must be a U.S. Citizen with the ability to obtain and maintain a government suitability clearance. ', 'Assembles large complex data sets and creates and maintains data pipelines required for optimal ETL/ELT for internal and external data sources; focuses on scalability and high performance', 'Medical', 'Due to COVID-19, certain on-site client work has been suspended until further notice, however, on-site work may be required on a case-by-case basis for this position', 'Vision', 'Experience with Databricks as a platform strongly preferred', 'Experience with Tableau and SAS is nice-to-have', 'Data Engineer ', 'Must be a U.S. Citizen with the ability to obtain and maintain a government suitability clearance.', 'Bachelor’s Degree in Information Technology, Engineering, Computer Science, Machine Learning, Statistics or related fields', 'Long-term Disability', 'Previous hands-on coding and scripting experience in languages such as Python and Scala', '3+ years’ experience in IT, data analytics, data engineering, or related discipline', 'Desired', ' Independently and as part of a project team, works to collect, build, cleanse, assemble, and refine datasets to support a variety of data analytics needs put forward by business and technical stakeholders.  Assembles large complex data sets and creates and maintains data pipelines required for optimal ETL/ELT for internal and external data sources; focuses on scalability and high performance Implements real time and batch processing requirements as well as graph databases in large-scale data environments. Works with a variety of business and technical stakeholders including executive, product, data, and information technology teams to intake requirements, provide progress updates, troubleshoot, and generally supports their needs. Forward-thinking approach with interest in highlighting internal process improvement with respect to automating manual processes, optimizing data delivery, improving infrastructure for greater scalability, etc. ', '401K Plan with employer match', 'Works with a variety of business and technical stakeholders including executive, product, data, and information technology teams to intake requirements, provide progress updates, troubleshoot, and generally supports their needs.', 'Benefits', 'Independently and as part of a project team, works to collect, build, cleanse, assemble, and refine datasets to support a variety of data analytics needs put forward by business and technical stakeholders. ', 'Experience in an environment using leading data science and machine learning platforms (Databricks, SAS, Anaconda, Alteryx, etc.) preferred']",Mid-Senior level,Full-time,Engineering,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Versique Search & Consulting,"Minneapolis, MN",2 weeks ago,34 applicants,"['', ' Advanced working SQL knowledge and experience working with relational databases, query authoring as well as working familiarity with a variety of databases. ', ' Build processes supporting data transformation, data structures, metadata, dependency and workload management. ', ' Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. ', 'Role & Responsibilities', ' Strong project management and organizational skills. ', ' A successful history of manipulating, processing and extracting value from large disconnected datasets. ', ' Create and maintain optimal data pipeline architecture  Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.  Assemble large, complex data sets that meet functional / non-functional business requirements.  Build the infrastructure required for optimal ETL of data from a wide variety of data sources using SQL and AWS technologies.  Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.  Implements processes and systems to monitor data quality, ensuring production data is accurate and available for key stakeholders and business processes that depend on it.  Work with stakeholders including the Executive, Product, and Data Science teams to assist with data-related technical issues and support their data infrastructure needs.  Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.  Work with data and analytics experts to strive for greater functionality in our data systems. ', ' Work with data and analytics experts to strive for greater functionality in our data systems. ', ' Implements processes and systems to monitor data quality, ensuring production data is accurate and available for key stakeholders and business processes that depend on it. ', ' Experience supporting and working with cross-functional teams in a dynamic environment. ', ' Create and maintain optimal data pipeline architecture ', ' Experience building and optimizing data pipelines, architectures and data sets. ', ' 5+ years of experience in a Data Engineer role. ', ' Strong analytic skills related to working with unstructured datasets. ', 'Required Skills & Experience', ' Bachelor’s Degree, an undergraduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field ', 'Data Engineer ', ' Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. ', ' Working knowledge of message queuing, stream processing, and highly scalable data stores. ', 'About Versique', ' Work with stakeholders including the Executive, Product, and Data Science teams to assist with data-related technical issues and support their data infrastructure needs. ', ' Bachelor’s Degree, an undergraduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field  5+ years of experience in a Data Engineer role.  Advanced working SQL knowledge and experience working with relational databases, query authoring as well as working familiarity with a variety of databases.  Experience building and optimizing data pipelines, architectures and data sets.  Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.  Strong analytic skills related to working with unstructured datasets.  Build processes supporting data transformation, data structures, metadata, dependency and workload management.  A successful history of manipulating, processing and extracting value from large disconnected datasets.  Working knowledge of message queuing, stream processing, and highly scalable data stores.  Strong project management and organizational skills.  Experience supporting and working with cross-functional teams in a dynamic environment. ', ' Assemble large, complex data sets that meet functional / non-functional business requirements. ', ' Build the infrastructure required for optimal ETL of data from a wide variety of data sources using SQL and AWS technologies. ', ' Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics. ', ' Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader. ', 'Job Description']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Kite,"San Francisco, CA",3 weeks ago,197 applicants,"['', 'Minimum of 3+ years of engineering experience', 'Experience building data pipelines, stream processing, and/or big-data messaging systems', 'Built ETL processes for large, disconnected, and/or un-structured datasets', 'Bachelor’s Degree in Computer Science or related fieldMinimum of 3+ years of engineering experienceExperience building reliable and scalable systemsExcited to work on a small, growing team in a startup environmentAbility to work daily from our San Francisco office (wonderful office in the Union Square area) when it is safe to do so', ""Programmers spend too much time doing repetitive work — copying and pasting from StackOverflow, fixing simple errors, and writing boilerplate code. We're building an AI code engine that does this work for you. Programming using Kite is faster and more fun."", 'Excited to work on a small, growing team in a startup environment', ""What You've Done"", 'Maintain and iteratively improve the data pipeline infrastructure', 'Maintain and iteratively improve the data pipeline infrastructureWrite data transformation logic to scrub and join large datasets, and handle changes in source dataTurn technical requirements into a plan to execute, test, and deliver on timeBecome an engineering leader as the team grows, if desiredWrite clean, maintainable codeDesign and code reviews', 'Experience building reliable and scalable systems', ""Kite is well-funded by top investors in Silicon Valley, including the founders of PayPal, Stripe, Palantir, and Dropbox to name a few. We are looking to expand our 16-person startup with talented individuals who are interested in joining an early stage startup. The ideal candidate is excited to help guide the direction of our product and company. They will have a significant amount of ownership of critical technical components. Our team is growing rapidly and we hope you'll grow with us too!"", 'Track record of working in small teams to build new software from scratch', ""What You'll Do"", 'Bachelor’s Degree in Computer Science or related field', 'Write data transformation logic to scrub and join large datasets, and handle changes in source data', 'Strong ownership instinct and ability to deliver results', 'Design and code reviews', 'Write clean, maintainable code', 'Ability to learn and evaluate new technologies quickly', 'Experience building data pipelines, stream processing, and/or big-data messaging systemsBuilt ETL processes for large, disconnected, and/or un-structured datasetsTrack record of working in small teams to build new software from scratchAbility to learn and evaluate new technologies quicklyStrong ownership instinct and ability to deliver resultsPublic cloud (AWS, Azure, etc.) automation', 'Public cloud (AWS, Azure, etc.) automation', 'Become an engineering leader as the team grows, if desired', 'As a Data Engineer you will work on the data pipeline infrastructure and ETL logic at the core of our product. You will be a key part of our machine learning team by providing easy, fast, flexible and reliable access to training data.', 'Ability to work daily from our San Francisco office (wonderful office in the Union Square area) when it is safe to do so', 'Turn technical requirements into a plan to execute, test, and deliver on time', 'Who You Are']",Associate,Full-time,Engineering,Computer Software,2021-03-24 13:05:10
Data Engineer,Rally Health,"Portland, OR",1 week ago,Be among the first 25 applicants,"['', 'we will', 'You Will', 'Write and understand complex SQL queries', ' consider for employment qualified applicants with arrest and conviction records.', '3+ years experience in ETL, Data Engineering, or Business Intelligence fields with experience in data transformationsWrite and understand complex SQL queriesExperience working in object-oriented or functional languages like Python, Scala or JavaExperience working with databases such as Postgres, MongoDB, and MySQLExposure to data modeling methodologies such as Kimball and InmanExperience with Airflow or another workflow orchestration framework (ex. Prefect, DAGster)Experience with data visualization or BI tools is a plusExperience with Spark is a plusHelp with team goals including the support of live 24/7 production systemsBS in Computer Science, Engineering or a related technical role or equivalent experiencePlease note that hiring for this position will only be considered in the following states: AZ, CA, DC, FL, IL, MD, MN, NC, NJ, NV, NY, OR, PA, TN, TX, VA, or WA ', 'Experience with Airflow or another workflow orchestration framework (ex. Prefect, DAGster)', 'Participate in business analysis and data discovery to gather data requirements', 'Experience with Spark is a plus', 'Flexible paid time off for full-time employees & paid leave for new parents', 'Exposure to data modeling methodologies such as Kimball and Inman', 'Our Benefits', 'Experience working in object-oriented or functional languages like Python, Scala or Java', 'Collaborate on designs for data pipelines and data tooling that meet performance and data quality standards', 'Help with team goals including the support of live 24/7 production systems', 'Optimize data pipelines to handle growing data volumes', 'Experience working with databases such as Postgres, MongoDB, and MySQL', 'BS in Computer Science, Engineering or a related technical role or equivalent experience', 'Please note that hiring for this position will only be considered in the following states: AZ, CA, DC, FL, IL, MD, MN, NC, NJ, NV, NY, OR, PA, TN, TX, VA, or WA ', ').', 'AZ, CA, DC, FL, IL, MD, MN, NC, NJ, NV, NY, OR, PA, TN, TX, VA, or WA', 'Comprehensive benefits package for full-time employees, including medical, dental, vision coverage, stock purchase plan, and 401(k)', '3+ years experience in ETL, Data Engineering, or Business Intelligence fields with experience in data transformations', 'Great compensation package', 'Rally Health believes in a policy of equal employment and opportunity for all people. It is our policy to train and promote individuals in all job titles, and administer all programs, without regard to race, color, religion, national origin or ancestry, citizenship, sex, age, marital status, pregnancy, childbirth or related medical conditions, personal appearance, sexual orientation, gender identity or expression, family responsibilities, genetic information, disability, matriculation, political affiliation, veteran status, union affiliation, or any other category protected by applicable federal, state or local laws.', 'You Have', 'recruiting@rallyhealth.com', 'Build pipelines for large-scale healthcare data in Spark', 'Play an important role in the discovery, design, development, and validation of big data pipelines', 'Great compensation packageComprehensive benefits package for full-time employees, including medical, dental, vision coverage, stock purchase plan, and 401(k)Wellness programs, including physical and mental health servicesFlexible paid time off for full-time employees & paid leave for new parentsEmployees in this position may also earn performance-based bonuses', 'Wellness programs, including physical and mental health services', 'Employees in this position may also earn performance-based bonuses', 'Play an important role in the discovery, design, development, and validation of big data pipelinesBuild pipelines for large-scale healthcare data in SparkBuild datasets and data products that support population health management and positive recommendations that allow individuals to improve their health and manage their costsOptimize data pipelines to handle growing data volumesCollaborate on designs for data pipelines and data tooling that meet performance and data quality standardsParticipate in business analysis and data discovery to gather data requirements', 'Individuals with disabilities and veterans are encouraged to apply. Applicants who require an accommodation related to the application or review process should notify Talent Acquisition (', 'Pursuant to the San Francisco Fair Chance Ordinance, ', 'Build datasets and data products that support population health management and positive recommendations that allow individuals to improve their health and manage their costs', 'Experience with data visualization or BI tools is a plus']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Hearst Autos,"New York, NY",1 week ago,Be among the first 25 applicants,"['', 'Work with GCP Big Data products including BigQuery, Pub/Sub, Cloud Composer, Dataflow, Dataproc, Dataprep, Data Studio, Bigtable, Cloud Storage, etc.', 'You apply software engineering best practices to the data engineering process.', 'Implement new and support existing data pipelines (ETL, ELT) and data wrangling procedures using Python and SQL.', 'You are passionate about working with diverse and large datasets.', 'You have experience with batch and stream data processing.', 'You have at least 3-5 years experience as a data engineer and are experienced in the Google data services stack.', 'Potentially work in AWS cloud services.', 'Research, analyze, and recommend technical approaches for solving difficult and challenging development and integration problems.', 'Work with GCP Big Data products including BigQuery, Pub/Sub, Cloud Composer, Dataflow, Dataproc, Dataprep, Data Studio, Bigtable, Cloud Storage, etc.Implement different types of storage (filesystem, relational, NoSQL) and work with various kinds of data (structured, unstructured, metrics, log files, etc.)Implement new and support existing data pipelines (ETL, ELT) and data wrangling procedures using Python and SQL.Tune application and query performance using performance profiling tools and SQLFamiliarity with agile software development practices and drive to ship quickly.Research, analyze, and recommend technical approaches for solving difficult and challenging development and integration problems.Potentially work in AWS cloud services.', 'Tune application and query performance using performance profiling tools and SQL', 'What You’ll Do', 'Familiarity with agile software development practices and drive to ship quickly.', 'Your Impact', 'You have at least 3-5 years experience as a data engineer and are experienced in the Google data services stack.You are passionate about working with diverse and large datasets.You apply software engineering best practices to the data engineering process.You have a desire to learn and continuously improve.You have experience with batch and stream data processing.', 'You have a desire to learn and continuously improve.', 'Why Hearst Magazines?', 'Job Description', 'Implement different types of storage (filesystem, relational, NoSQL) and work with various kinds of data (structured, unstructured, metrics, log files, etc.)', 'Who You Are']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer/Lead,Michael Page,"New York, NY",6 days ago,Be among the first 25 applicants,"['', 'Familiarity with Python', 'Utilize data integration pipe-lining tools in everyday work', 'Strong ability to design and implement data engineering solutions ', 'A plus to have both marketing data and treasure data experience', 'Ability to work in a fast paced environment and not have hand held', 'The Successful Applicant', 'Quote job ref: 1517007', 'Contact: Ryan Gutstein', 'Fluency in SQL', ""MPI does not discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity or expression, national origin, age, disability, veteran status, marital status, or based on an individual's status in any group or class protected by applicable federal, state or local law. MPI encourages applications from minorities, women, the disabled, protected veterans and all other qualified applicants."", 'Fill data needs through creating queries and data modeling tools', 'About Our Client', 'Data Engineer/Lead', 'Description', 'Prestigious and reputable Global Beverage Company ', ""What's On Offer"", 'Fluency in SQLFamiliarity with PythonStrong ability to design and implement data engineering solutions A plus to have both marketing data and treasure data experienceAbility to work in a fast paced environment and not have hand held', 'Put forth abilities in modeling data and SQL', 'Work mainly as an individual contributor, at times manage a team of data engineers', 'Job Description', 'Good Culture ', 'Fill data needs through creating queries and data modeling toolsUtilize data integration pipe-lining tools in everyday workPut forth abilities in modeling data and SQLWork mainly as an individual contributor, at times manage a team of data engineers']",Associate,Full-time,Information Technology,Consumer Goods,2021-03-24 13:05:10
Data Engineer,U.S. Bank,"Richfield, MN",5 days ago,Be among the first 25 applicants,"['', 'Ability to perform periodic on-call and production support for the ElasticStack/Splunk environment', 'Modeling: data, process, events, objects -IT standards, procedures, policy -Change control -System development life cycle -Application testing', ""Bachelor's degree or equivalent work experience"", 'Job Description', ' DataDog', 'Required Qualifications', ' Kafka, Kubernetes and Public cloud (AWS, GCP)', 'Working experience:', 'Skills/Experience', ""Bachelor's degree or equivalent work experienceAt least 6 years experience with developing and implementing applications"", 'Experience with system software and hardware infrastructure concepts', ' Experience architecting /engineering solutions involving large amount of log volumes and associated Monitoring/observability', 'EEO is the Law', ' Experience with ETL processes (retrieving data from sources, transforming the data, storing data)', ' Understanding of data formats such as XML and JSON', 'Benefits', 'E-Verify', 'Basic Qualifications', ' Scripting experience such as Perl, Python, Powershell - Advanced Microsoft Excel skills', ' Experience with data structures, data types, indexes, databases', 'Preferred Experience', ' Usage of dashboard tools such as Tableau, Elastic Kibana, Splunk, Grafana, Microsoft Power BI', 'At least 6 years experience with developing and implementing applications', 'At least two years of experience with Business analysis, Requirements gathering and Project Management involving IT infrastructure']",Entry level,Full-time,Information Technology,Banking,2021-03-24 13:05:10
Data Engineer,UNIQUE System Skills LLC,"Jersey City, NJ",3 days ago,Be among the first 25 applicants,[''],Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer ,Civitas Learning,"Austin, TX",4 weeks ago,Over 200 applicants,"['', 'For more information, visit:', 'What we are looking for in a Data Engineer:', 'About Civitas Learning:', 'Experience using Git and working with code collaboratively', 'At Civitas Learning, the Data Engineer acts as the technical lead on projects to integrate customers with our platform and applications. In this role, you will develop SQL logic using our internally developed Python tool to map customer data into our platform. You’ll work closely with customer-facing teams, such as the Project Management or Training & Enablement teams, to guide users through the integration process. You will also collaborate closely with product and engineering teams. In short, you will work side by side with a diverse range of people dedicated to our customers.', 'An office full of talented, mission-driven people who are committed to inclusion and strengthened by diversity', 'Civitas Learning helps colleges and universities harness the power of their student data to improve student success outcomes. We embed actionable intelligence in workflow tools so higher education can focus their student success strategies, deliver proactive care, inspire holistic advising, and quickly measure what’s working for whom. With our platform, software and services, our customers empower leaders, advisors, faculty, & students to measurably improve enrollment, persistence, and graduation outcomes. Today, we work with 375 colleges and universities, serving nearly 8 million students. Together with our growing community of customers, we are making the most of the world’s learning data to help graduate a million more students per year by 2025.', 'Expertise in some of the following is valued but not required:', 'Comfortable on the command line and write or understand basic scripts', 'While we are an open source shop, some of our customers use Microsoft solutions: SSIS, SSRS, Azure etc...', 'Write SQL queries to aggregate and transform data into our canonical data model', 'Python', 'Experience withmany SQL techniques including all types of joins, aggregate and window functions, CTE etc.', 'JIRA or other ticketing systems', 'Amazon Web Services', 'The chance to use your skills to do something that truly matters.\xa0', 'Amazon Web ServicesHigher Education Student Information Systems or Learning Management Systems (Ellucian Banner, PeopleSoft, Blackboard, Canvas, etc.)JIRA or other ticketing systemsPythonWhile we are an open source shop, some of our customers use Microsoft solutions: SSIS, SSRS, Azure etc...', '\ufeffWhat you are looking for in your career:', '2-4 years of experience - Data Engineer4-6 years of experience - Sr. Data Engineer6+ years of experience - Principal Data EngineerSkills using SQL to aggregate and transform data. Any flavor but Postgres / Redshift preferred.\xa0Experience withmany SQL techniques including all types of joins, aggregate and window functions, CTE etc.Comfortable on the command line and write or understand basic scriptsExperience using Git and working with code collaborativelyEffective communicator with different levels of technical audiencesWork, life, educational, or other experience with diverse groups of people', '\xa0', '6+ years of experience - Principal Data Engineer', 'Skills using SQL to aggregate and transform data. Any flavor but Postgres / Redshift preferred.\xa0', 'Civitas Learning is helping colleges and universities all over the world to make the most of their data by building custom data pipelines, patented data science, and SaaS applications to improve student outcomes. If you love working in fast-paced environments that require learning, critical thinking, and strong communication skills, then we’d love to talk.', 'An office full of talented, mission-driven people who are committed to inclusion and strengthened by diversityThe chance to use your skills to do something that truly matters.\xa0', 'Work, life, educational, or other experience with diverse groups of people', 'Higher Education Student Information Systems or Learning Management Systems (Ellucian Banner, PeopleSoft, Blackboard, Canvas, etc.)', 'www.civitaslearning.com. Interested in learning more about our Culture & Mission? Check out \u200b#civmission\u200b on \u200bLinkedIn\u200b. Civitas offers a comprehensive benefits package including medical, dental, vision, disability insurance, onsite paid parking, 401-K Program and a flexible paid time off policy. Civitas Learning is an equal opportunity employer, and we know diversity makes us stronger. We do not discriminate on the basis of race, religion, national origin, gender, sexual orientation, age, marital status, veteran status, or disability.', 'Continuously improve our data platform mappings', 'What you will do as a Data Engineer:', '2-4 years of experience - Data Engineer', 'Write SQL queries to aggregate and transform data into our canonical data modelGuide customers through platform requirements and code validation exercisesTrain and operationalize predictive modelsContinuously improve our data platform mappings', '4-6 years of experience - Sr. Data Engineer', 'Data Engineer', 'Train and operationalize predictive models', 'Effective communicator with different levels of technical audiences', 'Guide customers through platform requirements and code validation exercises']",Mid-Senior level,Full-time,Engineering,Computer Software,2021-03-24 13:05:10
Data Engineer,Launch Consulting Group,"Bellevue, WA",2 weeks ago,113 applicants,"['', 'Participate in data governance; ensure all existing data is created in the right way, and that new data is created according to appropriate standards and with proper documentation ', 'Required Experience/Qualifications', 'Hands-on experience working with CICD technologies in the data engineering space ', ' Participate in implementation, and support of a data warehouse/data base and analytics platform utilizing Azure cloud technology Design and implement data load processes from Cloud or On Premises data sources into Azure & Snowflake Cloud solutions.  Migrate existing processes and data from On Premises SQL Server and other environments to Azure and/or Snowflake storage solutions coupled with Azure or AWS compute.  Collaborate with Data Scientists on Data Provenance and Lineage. Design & implement processes/tasks with a focus on data quality, master data management, and data lineage as components of the overall data ecosystem.  Explore and learn the latest Cloud technologies to provide new capabilities and increase efficiency with a focus on Snowflake, Azure, and AWS. Work with top-notch technical professionals developing complex systems at scale and with a focus on sustained operational excellence  Collaborate with Data Analysts & Data Scientists; for products that require reporting data or AI/ML models to ensure that datasets are in place and are used consistently internally/externally  Participate in data governance; ensure all existing data is created in the right way, and that new data is created according to appropriate standards and with proper documentation  Read, write, and configure code for end-to-end service telemetry, alerting and self-healing capabilities  Strive for continuous improvement of code quality and development practices  Help continually improve ongoing data platform, analysis and reporting processes, automating or simplifying self-service support for customers  ', 'Help continually improve ongoing data platform, analysis and reporting processes, automating or simplifying self-service support for customers ', '2+ years of Spark, Python and PowerShell development or equivalent ', 'Experience with Teradata', 'Collaborate with Data Scientists on Data Provenance and Lineage. Design & implement processes/tasks with a focus on data quality, master data management, and data lineage as components of the overall data ecosystem. ', ' 3+ years of experience in working as an data engineering or analytics team member working with cross functional teams  3+ years of SQL DW/DB development or equivalent  3+ years of experience designing solutions in Azure specializing in any of the following technologies: Azure Data Factory, Azure Data Lake, Azure SQL & Azure SQL Data Warehouse, Azure Functions  Experience with Teradata Experience with Snowflake and Matillion orchestration  2+ years of Spark, Python and PowerShell development or equivalent  Bachelor’s Degree in Computer Sciences, Analytics, Systems Eng., Statistics or related field  Strong attention to detail and sense of urgency  Excellent communication skills both written and oral  Hands-on experience working with CICD technologies in the data engineering space  ', 'Explore and learn the latest Cloud technologies to provide new capabilities and increase efficiency with a focus on Snowflake, Azure, and AWS.', 'Read, write, and configure code for end-to-end service telemetry, alerting and self-healing capabilities ', 'Strong attention to detail and sense of urgency ', 'Excellent communication skills both written and oral ', 'DATA ENGINEER', '3+ years of experience in working as an data engineering or analytics team member working with cross functional teams ', 'Experience with Snowflake and Matillion orchestration ', 'Responsibilities', 'Bachelor’s Degree in Computer Sciences, Analytics, Systems Eng., Statistics or related field ', 'Work with top-notch technical professionals developing complex systems at scale and with a focus on sustained operational excellence ', 'Participate in implementation, and support of a data warehouse/data base and analytics platform utilizing Azure cloud technology', 'Migrate existing processes and data from On Premises SQL Server and other environments to Azure and/or Snowflake storage solutions coupled with Azure or AWS compute. ', 'Please refer to job #4356', 'Collaborate with Data Analysts & Data Scientists; for products that require reporting data or AI/ML models to ensure that datasets are in place and are used consistently internally/externally ', '3+ years of experience designing solutions in Azure specializing in any of the following technologies: Azure Data Factory, Azure Data Lake, Azure SQL & Azure SQL Data Warehouse, Azure Functions ', 'Strive for continuous improvement of code quality and development practices ', '3+ years of SQL DW/DB development or equivalent ', 'Design and implement data load processes from Cloud or On Premises data sources into Azure & Snowflake Cloud solutions. ', 'We are Navigators in the Age of Transformation.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Senior Big Data Engineer,Punchh,"Austin, Texas Metropolitan Area",1 day ago,Be among the first 25 applicants,"['', 'Demonstrated strength in data modelling, data warehousing and SQL. ', 'Excellent software engineering background. High familiarity with software development life cycle. Familiarity with GitHub/Airflow. ', '401K ', 'Maternity and Paternity (Bonding) Leave ', 'Unlimited PTO ', 'Punchh is the leader in customer loyalty, offer management, and AI solutions for offline and omni-channel merchants including restaurants, convenience stores, and retailers. Punchh brings the power of online to physical brands by delivering omni-channel experiences and personalization across the entire customer journey--from acquisition through loyalty and growth--to drive same store sales and customer lifetime value. Punchh uses best-in-class integrations to POS and other in-store systems such as WiFi, to deliver real-time SKU-level transaction visibility and offer provisioning for physical stores. ', 'Collaborate with stakeholders to design scalable solutions. ', ""What You'll Do "", ""What You'll Need "", ' ', 'This role requires close collaborations with data, engineering, and product organizations. His/her job functions include ', '5+ years of experience as a Big Data engineering professional, developing scalable big data solutions. Advanced degree in computer science, engineering or other related fields. Demonstrated strength in data modelling, data warehousing and SQL. Extensive knowledge with cloud technologies, e.g. AWS and Azure. Excellent software engineering background. High familiarity with software development life cycle. Familiarity with GitHub/Airflow. Advanced knowledge of big data technologies, such as programming language (Python, Java), relational (Postgres, mysql), NoSQL (Mongodb), Hadoop (EMR) and streaming (Kafka, Spark). Strong problem solving skills with demonstrated rigor in building and maintaining a complex data pipeline. Exceptional communication skills and ability to articulate a complex concept with thoughtful, actionable recommendations. ', 'Work with infra and operations team to monitor and optimize existing infrastructure ', 'Title - Big Data Engineer', 'Advanced knowledge of big data technologies, such as programming language (Python, Java), relational (Postgres, mysql), NoSQL (Mongodb), Hadoop (EMR) and streaming (Kafka, Spark). ', ""What You'll Do"", 'Life and AD&D insurance ', 'Strong problem solving skills with demonstrated rigor in building and maintaining a complex data pipeline. ', 'Competitive salaries, bonus and stock options ', 'EAP ', 'Advanced degree in computer science, engineering or other related fields. ', 'A technical leader of Punchh’s big data platform that supports AI and BI products. ', 'Title - Big Data Engineer ', 'Punchh is seeking to hire Big Data Engineer at either a senior or tech lead level. Reporting to the Director of Big Data, he/she will play a critical role in leading Punchh’s big data innovations. By leveraging prior industrial experience in big data, he/she will help create cutting-edge data and analytics products for Punchh’s business partners. ', ""Punchh is growing exponentially, serves 200+ brands that encompass 91K+ stores globally.  Punchh’s customers include the top convenience stores such as Casey’s General Stores, 25+ of the top 100 restaurant brands such as Papa John's, Little Caesars, Denny’s, Focus Brands (5 of 7 brands), and Yum! Brands (KFC, Pizza Hut, and Taco Bell), and retailers.  For a multi-billion $ brand with 6K+ stores, Punchh drove a 3% lift in same-store sales within the first year.  Punchh is powering loyalty programs for 135+ million consumers. "", 'Healthcare coverage, FSA, HSA Life and AD&D insurance 401K Competitive salaries, bonus and stock options Professional development EAP Maternity and Paternity (Bonding) Leave Unlimited PTO Paid holidays Free lunch every single day, social events, plus a well stocked refrigerator ', ""What You'll Need"", '5+ years of experience as a Big Data engineering professional, developing scalable big data solutions. ', 'Extensive knowledge with cloud technologies, e.g. AWS and Azure. ', 'Work with large data sets and implement sophisticated data pipelines with both structured and structured data. Collaborate with stakeholders to design scalable solutions. Manage and optimize our internal data pipeline that supports marketing, customer success and data science to name a few. A technical leader of Punchh’s big data platform that supports AI and BI products. Work with infra and operations team to monitor and optimize existing infrastructure Occasional business travels are required. ', 'Free lunch every single day, social events, plus a well stocked refrigerator ', 'Benefits', 'Professional development ', 'Punchh has raised $70 million from premier Silicon Valley investors including Sapphire Ventures and Adam Street Partners, has a seasoned leadership team with extensive experience in digital, marketing, CRM, and AI technologies as well as deep restaurant and retail industry expertise. ', 'Manage and optimize our internal data pipeline that supports marketing, customer success and data science to name a few. ', 'Exceptional communication skills and ability to articulate a complex concept with thoughtful, actionable recommendations. ', 'Paid holidays ', 'Work with large data sets and implement sophisticated data pipelines with both structured and structured data. ', 'Punchh is proud to provide equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics. If you’d like more information about your EEO rights as an applicant, please click here.', 'Healthcare coverage, FSA, HSA ', 'Occasional business travels are required. ']",Mid-Senior level,Full-time,Engineering,Computer Software,2021-03-24 13:05:10
Data Engineer,UST,United States,5 days ago,25 applicants,"['', '\xa0• Great understanding with builds, software development and GIT.', '\xa0• Mentor peers on coding standards, patterns and strategy.', '\xa0• Collaborate with peers, reviewing complex change and enhancement requests.', '\xa0• 1+ years combined of hands on Google Cloud Platform (GCP) development experience', '\xa0\xa0Requirements:', '\xa0\xa0Responsibilities:', '\xa0\xa0', '\xa0• Experience with one or more software version control systems (e.g. Git, Subversion)', '\xa0• Experience with Sprint Boot, Maven, Bamboo and great debugging skills.', '\xa0• Experience with Big Data processing frameworks (Spark, Hadoop) is required.', '\xa0• Define and enforce application coding standards and best practices.', '\xa0• Atlassian products – BitBucket, JIRA, Confluence etc.', '\xa0• Be involved and supportive of agile sprint model of development, helping to enforce the practice and the discipline.', '\xa0• Coach and mentor team members on Teradata, Python, Spark and Hadoop development best practices.', '\xa0• Experience in agile environment', '\xa0Description:', '\xa0• Partner with other IT teams during integration activities to facilitate successful implementations.', '\xa0• Strong effective communication skills, both written and verbal', '\xa0• Gitflow', '\xa0• Have a good understanding of where their project fits into the larger goals for engineering and adapts their work so that the priorities of the systems they are creating match those of the organization', '\xa0• Experience with DevOps tools and techniques (Continuous Integration, Jenkins, Puppet) is required.', '\xa0', '\xa0• Continuous Integration tools such as Bamboo, Jenkins, or TFS', '\xa0• Diagnose and troubleshoot performance and other issues.', '\xa0• Guide the team on best practices in Teradata, Python, Spark and Hadoop as well as perform code reviews.', '\xa0• Be able to help others break down large team goals into specific and manageable tasks.', '\xa0• Identify and resolve technical and process impediments preventing delivery teams from meeting delivery commitments.', '\xa0• Provide expertise in the development of estimates for EPICs and User Stories for planning and execution.', '\xa0• Document functional/technical requirements and design based on requirements or objectives.', '\xa0• Participate in on-call application support and respond to application issues when identified.', 'Data Engineer ', '\xa0UST Global® is looking for a highly energetic and collaborative Data Engineer with experience building enterprise data intelligence on Cloud platforms. The Data Engineer will be responsible for delivering quality reports and data intelligence solutions to the organization and assist client teams with drawing insights to make informed, data driven decisions for a leading Retailer in the United States. The ideal candidate is expected to be experienced in all phases of the data management lifecycle, gather and analyze requirements, collect, process, store and secure, use, share and communicate, archive, reuse and repurpose data. Identify and manage existing and emerging risks that stem from business activities and ensure these risks are effectively identified and escalated to be measured, monitored and controlled. The candidate should be a proven self-starter with demonstrated ability to make decisions and accept responsibility and risk. Excellent written and verbal communication skills with the ability to collaborate effectively with domain experts and IT leadership team is key to be successful in this role.', '\xa0• Expertise working in GCS Connector, DataProc, Bigquery', '\xa0• Evaluate potential changes and enhancements for objectives, scope and impact.', '\xa0• Build and maintain active relationships with customers to determine business requirements.', '\xa0• Align and collaborate with architects, other team leads, and IT leadership to develop technical architectural runways supporting upcoming features and capabilities.', '\xa0• Excepted to provide hands on software development for a large data project, hosted in a cloud environment.', '\xa0• BA/BS degree or technical institute training or equivalent work experience', '\xa0• Develop and refine the technical architecture used with Teradata, Python, Spark and Hadoop development teams.', '\xa0• Experience working in ADF Python will be an added advantage', '\xa0• Take a proactive approach to development work, leading peers and partners to strategic technical solutions in a complex IT environment.', '\xa0• 4+ years of hands on Teradata, Python, Spark and Hadoop development experience', '\xa0\xa0As a Data Engineer, you will', '\xa0\xa0\xa0\xa0The most successful candidates will also have experience in the following:', '\xa0• Excellent communication and presentation skills.', '\xa0• Communicate effectively with technical peers in in a clear manner, while also being able to articulate complex solutions in ways nontechnical business partners can understand.', '\xa0• Experience overseeing team members.']",Mid-Senior level,Temporary,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Kalderos,"Chicago, IL",3 days ago,37 applicants,"['', 'Excellent project managements skills, and familiarity working in an agile environment Some ways you may demonstrate this are: Describing a time in which you had a large project you needed to manage Relevant work experience on a Agile team    ', 'Work with product teams to understand and develop data models that can meet requirements and operationalize well', 'Some ways you may demonstrate this are: Professional project descriptions on your resume SQL Certifications you have earned Answering SQL questions in a phone interview Advanced SQL queries you wrote in your Github repository  ', ' Work with product teams to understand and develop data models that can meet requirements and operationalize well Build out automated ETL jobs that reliably process large amounts of data, and ensure these jobs runs consistently and well Build tools that enable other data engineers to work more efficiently Try out new data storage and processing technologies in proof of concepts and make recommendations to the broader team Tune existing implementations to run more efficiently as they become bottlenecks, or migrate existing implementations to new paradigms as needed Learn and apply knowledge about the drug discount space, and become a subject matter expert for internal teams to draw upon ', 'Proven data engineering experience that involved creating, and maintaining a database and implementing data processing pipelines  Some ways you may demonstrate this are: Professional experience described on your resume Open source implementations of data intensive applications A personal side project that involved maintaining a database like a website Online course completion in relevant areas Vendor certification in a relevant field    ', 'What We Are Looking For', 'Professional experience described on your resume', 'What Data Engineers Do', 'Experience with streaming technologies such as Kafka or Event Hubs', 'data engineer', 'Learn and apply knowledge about the drug discount space, and become a subject matter expert for internal teams to draw upon', 'Company Perks', 'Vendor certification in a relevant field', 'SQL Certifications you have earned', 'Answering SQL questions in a phone interview', 'Candidates must be authorized to work in the US and be able to work in Chicago/Milwaukee', 'Build tools that enable other data engineers to work more efficiently', 'Sending a code snippet that exemplifies your work', ' Some ways you may demonstrate this are: Describing a time in which you had a large project you needed to manage Relevant work experience on a Agile team   ', '4+ years work experience as a Data Engineer in a full-time role', ' Some ways you may demonstrate this are: Professional project descriptions on your resume SQL Certifications you have earned Answering SQL questions in a phone interview Advanced SQL queries you wrote in your Github repository   ', 'Open source implementations of data intensive applications', 'Some ways you may demonstrate this are: Professional project descriptions on your resume GitHub repositories that have working code that you created Sending a code snippet that exemplifies your work  ', 'Professional project descriptions on your resume', 'Healthcare benefits', '  Some ways you may demonstrate this are: Professional experience described on your resume Open source implementations of data intensive applications A personal side project that involved maintaining a database like a website Online course completion in relevant areas Vendor certification in a relevant field   ', ' Professional project descriptions on your resume SQL Certifications you have earned Answering SQL questions in a phone interview Advanced SQL queries you wrote in your Github repository ', 'Some ways you may demonstrate this are: Describing a time in which you had a large project you needed to manage Relevant work experience on a Agile team  ', 'Relevant work experience on a Agile team', 'Advanced SQL skills and understanding performance trade-offs of various SQL implementations Some ways you may demonstrate this are: Professional project descriptions on your resume SQL Certifications you have earned Answering SQL questions in a phone interview Advanced SQL queries you wrote in your Github repository    ', ' 4+ years work experience as a Data Engineer in a full-time role Candidates must be authorized to work in the US and be able to work in Chicago/Milwaukee Excellent project managements skills, and familiarity working in an agile environment Some ways you may demonstrate this are: Describing a time in which you had a large project you needed to manage Relevant work experience on a Agile team     Proven data engineering experience that involved creating, and maintaining a database and implementing data processing pipelines  Some ways you may demonstrate this are: Professional experience described on your resume Open source implementations of data intensive applications A personal side project that involved maintaining a database like a website Online course completion in relevant areas Vendor certification in a relevant field     Programming experience with a modern computing language that supports data engineering work (Python, C#, JVM-languages like Scala) Some ways you may demonstrate this are: Professional project descriptions on your resume GitHub repositories that have working code that you created Sending a code snippet that exemplifies your work     Advanced SQL skills and understanding performance trade-offs of various SQL implementations Some ways you may demonstrate this are: Professional project descriptions on your resume SQL Certifications you have earned Answering SQL questions in a phone interview Advanced SQL queries you wrote in your Github repository     ', '401K plan with matching', 'Experience with SQL databases like MS SQL Server', 'What We Do', 'What May Set You Apart', ' Experience with SQL databases like MS SQL Server Experience with Azure cloud computing such as hosted databases Experience with streaming technologies such as Kafka or Event Hubs Experience with orchestration frameworks like Azure Data Factory or Airflow Experience with NoSQL data stores such as MongoDB and CosmosDB Experience in the healthcare or pharmaceutical industries Experience with large scale migrations of databases Experience creating RESTful APIs to service data needs for web applications Experience with implementing Software Development Lifecycle approaches to database work, such as Testing, CI/CD and other automation ', 'Celebration, and education stipends', ' Some ways you may demonstrate this are: Professional experience described on your resume Open source implementations of data intensive applications A personal side project that involved maintaining a database like a website Online course completion in relevant areas Vendor certification in a relevant field  ', 'Online course completion in relevant areas', 'Try out new data storage and processing technologies in proof of concepts and make recommendations to the broader team', 'Advanced SQL queries you wrote in your Github repository', 'Programming experience with a modern computing language that supports data engineering work (Python, C#, JVM-languages like Scala) Some ways you may demonstrate this are: Professional project descriptions on your resume GitHub repositories that have working code that you created Sending a code snippet that exemplifies your work    ', 'Experience creating RESTful APIs to service data needs for web applications', 'Experience in the healthcare or pharmaceutical industries', 'Experience with Azure cloud computing such as hosted databases', ' 401K plan with matching Healthcare benefits Flexible schedule and a fair PTO system that allows for a healthy work-life balance Opportunity to work on new technologies and learn new skills Celebration, and education stipends ', 'Experience with orchestration frameworks like Azure Data Factory or Airflow', 'Experience with NoSQL data stores such as MongoDB and CosmosDB', ' Describing a time in which you had a large project you needed to manage Relevant work experience on a Agile team ', 'Flexible schedule and a fair PTO system that allows for a healthy work-life balance', 'Experience with large scale migrations of databases', ""Due to the circumstances of the COVID-19 pandemic, Kalderos has decided to protect our current and future employees by shifting to an entirely remote workforce. We will continue to operate, interview, onboard, and work remotely. Please be aware that some of our roles will not be remote long-term and will return to an office setting once we're safe to do so following the guidance of local health authorities and the CDC."", ' Some ways you may demonstrate this are: Professional project descriptions on your resume GitHub repositories that have working code that you created Sending a code snippet that exemplifies your work   ', 'A personal side project that involved maintaining a database like a website', ' Professional project descriptions on your resume GitHub repositories that have working code that you created Sending a code snippet that exemplifies your work ', 'GitHub repositories that have working code that you created', 'Describing a time in which you had a large project you needed to manage', 'Opportunity to work on new technologies and learn new skills', 'Build out automated ETL jobs that reliably process large amounts of data, and ensure these jobs runs consistently and well', 'Experience with implementing Software Development Lifecycle approaches to database work, such as Testing, CI/CD and other automation', 'Tune existing implementations to run more efficiently as they become bottlenecks, or migrate existing implementations to new paradigms as needed', ' Professional experience described on your resume Open source implementations of data intensive applications A personal side project that involved maintaining a database like a website Online course completion in relevant areas Vendor certification in a relevant field ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Real Chemistry,"New York, United States",2 weeks ago,61 applicants,"['', 'Experience in building real time streaming data ingestion and processing pipeline using Apache Beam (running on either Google Dataflow or Apache (Apex Flink, or Spark) or Kafka in an analytics or data science use case', 'Extensive, demonstrated expertise with Python. should be generally comfortable in different categories (machine learning, development, scripting, etc.)', 'Delivery of cloud architecture to support new distributed computing solutions that often span the full array of cloud services. This will include migration of existing applications and development of new applications using cloud services.', 'Demonstrated experience with distributed computing', 'Build processes supporting data transformation, data structures, metadata, dependency and workload management.', 'Understanding of database and analytical technologies in the industry including MPP and NoSQL databases, Data Warehouse design, PostgreSQL, BI reporting and Dashboard development', 'Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.', 'Own the Insights gleaned by the creation of advanced technology roadmaps. Share real world implementations and recommend new capabilities that would simplify adoption and drive greater value.', 'Desire and ability to interact with all levels of the organization', 'Strong analytic skills related to working with unstructured datasets.', '5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative fieldAdvanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.Extensive, demonstrated expertise with Python. should be generally comfortable in different categories (machine learning, development, scripting, etc.)Experience in building real time streaming data ingestion and processing pipeline using Apache Beam (running on either Google Dataflow or Apache (Apex Flink, or Spark) or Kafka in an analytics or data science use caseBuild processes supporting data transformation, data structures, metadata, dependency and workload management.Strong analytic skills related to working with unstructured datasets.Hands on experience leading enterprise-wide data engineering, warehousing and analytics projectsExperience with data pipeline and workflow management tools: Airflow, Oozie etc.Ability to think strategically about business, product, and technical challenges in an enterprise environmentUnderstanding of database and analytical technologies in the industry including MPP and NoSQL databases, Data Warehouse design, PostgreSQL, BI reporting and Dashboard developmentDemonstrated industry efficiency in the fields of database, data warehousing or data sciencesCustomer facing skills with the ability to drive discussions with senior leadership regarding trade-offs, best practices and risk mitigationDesire and ability to interact with all levels of the organization', '5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field', 'Ability to think strategically about business, product, and technical challenges in an enterprise environment', 'Requirements', 'Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.', '\xa0', 'Design and implement data ingestion solutions on GCP using GCP native services', 'This role will be that of an over-arching Data Engineering guru. The ideal candidate will connect and work closely with Data Warehousing, Data Analytics, SW Engineering and Data Sciences as well as Product and Project Management. They will be responsible for solving some of the most complex and high scale data Engineering challenges in our industry while impacting the lives of healthcare professionals and their patients. The ideal candidate must be technologically curious, driven to work with some of newest data engineering technologies on the marketplace today. They will be a visionary and a key driver in the rapid adoption and scalability of crucial data engineering, warehousing and cloud-based technologies.', 'Recently named Best Place to Work by MM&M, The Holmes Report, PR News, PRWeek, and AdAge, Real Chemistry is an integrated marketing and communications firm powered by analytics and specializing in healthcare.\xa0We are currently seeking a talented Data Engineer to join our growing team of pharmaceutical advertising, technology and data professionals. This is a great opportunity to join a dynamic, fast-growing global agency.', 'Responsibilities', 'Innovate and engage with key technology stakeholders to create a compelling vision of a data-driven enterprise environment and the impact it will have on their teams, their projects and their outcomes.', 'Hands on experience leading enterprise-wide data engineering, warehousing and analytics projects', 'This Data Engineer will collaborate with executive level internal and external stakeholders. They will lead the development, delivery and implement of AI, IOT, data engineering and data analytics projects which will leverage data to develop industry leading business insights. This person will focus on solutions such as machine learning, IoT, batch/real-time data processing, data and business intelligence while ensuring enterprise wide data security and integrity.', 'Customer facing skills with the ability to drive discussions with senior leadership regarding trade-offs, best practices and risk mitigation', 'As a Data Engineer on the technology team, you’ll be responsible for building and maintaining our data warehouse environment, maintaining our internal data processes, and implementing and supporting business intelligence capabilities across multiple business divisions. You will play a key role in helping design our enterprise data architecture to support overarching data governance and related initiatives such as data architecture management, data security management, and data quality management.', 'Design and implement data ingestion solutions on GCP using GCP native servicesDemonstrated experience with distributed computingDesign and optimize data models on GCP cloud using GCP data stores such as BigQueryIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.Delivery of cloud architecture to support new distributed computing solutions that often span the full array of cloud services. This will include migration of existing applications and development of new applications using cloud services.Own the Insights gleaned by the creation of advanced technology roadmaps. Share real world implementations and recommend new capabilities that would simplify adoption and drive greater value.Innovate and engage with key technology stakeholders to create a compelling vision of a data-driven enterprise environment and the impact it will have on their teams, their projects and their outcomes.', 'Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.', 'Design and optimize data models on GCP cloud using GCP data stores such as BigQuery', 'Data Engineer', 'Demonstrated industry efficiency in the fields of database, data warehousing or data sciences', 'Experience with data pipeline and workflow management tools: Airflow, Oozie etc.', 'Role:']",Associate,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer,DC Public Charter School Board,"Washington, DC",2 weeks ago,Be among the first 25 applicants,"['', 'Compensation', 'About The Role', 'DC PCSB is an equal opportunity employer committed to building a culturally diverse staff. We strive to foster an environment where everyone feels included. We believe that when people bring their unique identities, backgrounds, perspectives, and experiences to our community, we are able to truly achieve excellence in our work.', 'About The Dc Public Charter School Board (dc Pcsb)', 'TO APPLY', 'Competencies and Qualifications:']",Mid-Senior level,Full-time,Information Technology,Nonprofit Organization Management,2021-03-24 13:05:10
Engineer - Data Engineer III,AmerisourceBergen,"Conshohocken, PA",1 month ago,Be among the first 25 applicants,[''],Associate,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Tata Consultancy Services,"Charlotte, NC",6 days ago,Be among the first 25 applicants,[''],Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,HelloFresh,"Boulder, CO",3 weeks ago,95 applicants,"['', 'Company sponsored outings & Employee Resource Groups', 'Implement ETLs monitoring automation', 'Snacks, cold brew on tap & monthly catered lunches', 'Able to develop an in-depth understanding of HelloFresh’s core product and architecture, and act as an ambassador for state of the art software solutions and industry best practices', 'Come see what’s cookin’ at HelloFresh!', 'Design and implement end-to-end data products and marketing automation flows: from data ingestions for data science modeling to creation of automated pipelines to external software (Salesforce, etc.)', 'Proficient in Python (with knowledge of OOP) and SQL (DDL, DML, CTEs, query optimization, ...). ', 'Past experience working with Apache Spark required', 'Data cleaning/enrichment: keeping data clean and consistent with production systems (e.g. bug fixes, backfills …)', ' Competitive Salary & 401k company match that vests immediately upon participation Generous parental leave of 16 weeks & PTO policy $0 monthly premium and other flexible health plans effective first day of employment 75% discount on your subscription to HelloFresh (as well as other product initiatives) Snacks, cold brew on tap & monthly catered lunches Company sponsored outings & Employee Resource Groups Collaborative, dynamic work environment within a fast-paced, mission-driven company ', 'Competitive Salary & 401k company match that vests immediately upon participation', 'BSc in a STEM discipline', 'You are ...', 'You will do ...', 'Salary: $95,000-$105,000', '$0 monthly premium and other flexible health plans effective first day of employment', 'Generous parental leave of 16 weeks & PTO policy', 'At a minimum, you have ...', 'The ability to design, implement and deliver maintainable and high-quality code using best practices (e.g. git/github, secrets, configurations, yaml/json)', "" Design and deploy cloud-based Data infrastructure (AWS, Databricks) Implement ETLs monitoring automation Help design, update and extend HelloFresh's data model (create new schemas, fact tables, mat views, joins, etc.) Data cleaning/enrichment: keeping data clean and consistent with production systems (e.g. bug fixes, backfills …) Design and implement end-to-end data products and marketing automation flows: from data ingestions for data science modeling to creation of automated pipelines to external software (Salesforce, etc.) Data Transformations: implement the logic of the data pipeline (aggregations, projections, selections, etc …) "", ' BSc in a STEM discipline 2+ years’ data engineering experience is required Proficient in Python (with knowledge of OOP) and SQL (DDL, DML, CTEs, query optimization, ...).  Past experience working with Apache Spark required Experience with end-to-end testing and general DevOps practices for data pipelines The ability to design, implement and deliver maintainable and high-quality code using best practices (e.g. git/github, secrets, configurations, yaml/json) Knowledge of data structures (DataFrames, RDDs, Dataclasses) and data formats (CSV, JSON, Parquet, Avro, ORC) Experience with software design patterns, and building highly scalable solutions preferred Experience with job orchestration tools like Airflow, Luigi or similar preferred ', 'An active, solution-oriented member of autonomous, cross-functional agile teams collaborating with Product Owners, Data Scientists, and Business Intelligence teams', '2+ years’ data engineering experience is required', 'Experience with software design patterns, and building highly scalable solutions preferred', ""Help design, update and extend HelloFresh's data model (create new schemas, fact tables, mat views, joins, etc.)"", '75% discount on your subscription to HelloFresh (as well as other product initiatives)', 'Knowledge of data structures (DataFrames, RDDs, Dataclasses) and data formats (CSV, JSON, Parquet, Avro, ORC)', ' An active, solution-oriented member of autonomous, cross-functional agile teams collaborating with Product Owners, Data Scientists, and Business Intelligence teams Able to develop an in-depth understanding of HelloFresh’s core product and architecture, and act as an ambassador for state of the art software solutions and industry best practices ', 'Experience with end-to-end testing and general DevOps practices for data pipelines', 'Collaborative, dynamic work environment within a fast-paced, mission-driven company', 'Design and deploy cloud-based Data infrastructure (AWS, Databricks)', 'Experience with job orchestration tools like Airflow, Luigi or similar preferred', 'Data Transformations: implement the logic of the data pipeline (aggregations, projections, selections, etc …)', 'You’ll get …', 'Job Description']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer,Viasat Inc.,"Carlsbad, CA",1 week ago,Be among the first 25 applicants,"['', 'Experienced designing highly scaled systems', 'A BS in Computer Science, Computer Engineering, Software Engineering, Electrical Engineering, Math, Physics or related field', '4+ years of development experience', 'Understanding of distributed and parallel computing architectures', 'Experience with Redshift, Google Big Query, and/or Presto', 'In This Position, You Will', 'Travel up to 10%', 'Design and build data engineering and integration solutionsDesign and build analytic tools and data models', 'Requirements', 'Design and build data engineering and integration solutions', 'Familiarity with RDBMS technologies (PostgreSQL, MySQL, etc.)', 'Familiar with several languages, including Python, Java, Scala', 'Experience building scalable infrastructure using AWS products', 'Experience with Snowplow', 'Design and build analytic tools and data models', 'Experience with data transformation technologies like DBT', 'Experience with SnowplowExperience with data transformation technologies like DBTExperience with Redshift, Google Big Query, and/or PrestoFamiliarity with RDBMS technologies (PostgreSQL, MySQL, etc.)Understanding of distributed and parallel computing architecturesExperience with modern deployment orchestration toolschains including CloudformationExperience maintaining SLAs4+ years of development experience', 'Familiar with Linux environment and common tools', 'Experience maintaining SLAs', 'Experienced designing highly scaled systemsExperience building scalable infrastructure using AWS productsFamiliar with several languages, including Python, Java, ScalaFamiliar with Linux environment and common toolsA BS in Computer Science, Computer Engineering, Software Engineering, Electrical Engineering, Math, Physics or related fieldTravel up to 10%', 'Experience with modern deployment orchestration toolschains including Cloudformation', 'Preferences']",Not Applicable,Full-time,Information Technology,Defense & Space,2021-03-24 13:05:10
Data Engineer,Tencent America,"Los Angeles, CA",2 days ago,34 applicants,"['', ""2+ years' experience in custom ETL design, implementation and maintenance."", 'Build data expertise and own data quality for your areas.', 'Experience querying massive datasets using Spark, Hadoop, Presto, Hive, Impala, etc.', 'Communicate, at scale, through multiple mediums: Presentations, dashboards, company-wide datasets, bots and more.', '2+ years of Python development experience.', 'Data Engineer ', 'Leverage data and business principles to solve large scale web, mobile and data infrastructure problems.', 'Requirements:', "" 2+ years of Python development experience. 2+ years of SQL experience. 2+ years of experience with workflow management engines (i.e. Airflow, Luigi, Prefect, Dagster, digdag.io, Google Cloud Composer, AWS Step Functions, Azure Data Factory, UC4, Control-M). 2+ years' experience in custom ETL design, implementation and maintenance. Experience querying massive datasets using Spark, Hadoop, Presto, Hive, Impala, etc."", '2+ years of experience with workflow management engines (i.e. Airflow, Luigi, Prefect, Dagster, digdag.io, Google Cloud Composer, AWS Step Functions, Azure Data Factory, UC4, Control-M).', 'Responsibility:', 'Design, build and launch extremely efficient and reliable data pipelines to move data across a number of platforms including Data Warehouse, online caches and real-time systems.', '2+ years of SQL experience.', ""Educate your partners: Use your data and analytics experience to 'see what's missing', identifying and addressing gaps in their existing logging and processes."", "" Design, build and launch extremely efficient and reliable data pipelines to move data across a number of platforms including Data Warehouse, online caches and real-time systems. Communicate, at scale, through multiple mediums: Presentations, dashboards, company-wide datasets, bots and more. Educate your partners: Use your data and analytics experience to 'see what's missing', identifying and addressing gaps in their existing logging and processes. Leverage data and business principles to solve large scale web, mobile and data infrastructure problems. Build data expertise and own data quality for your areas. ""]",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Senior Data Engineer,Beyondsoft,"Redmond, WA",20 hours ago,40 applicants,"['', '2+ years’ experience building cloud hosted data systems. Azure preferred. ', '2+ years’ experience with OOP language (C#, Python)', 'Required:', 'Preferred:', 'Qualifications:', '5+ years’ experience with SQL Server. Expert level TSQL knowledge required. 5+ years’ experience designing and implementing scalable ETL processes including data movement (SSIS, replication, etc.) and quality tools. 2+ years’ experience building cloud hosted data systems. Azure preferred. 2+ years’ experience with OOP language (C#, Python)Strong communication and collaboration skills', '5+ years’ experience with SQL Server. Expert level TSQL knowledge required. ', 'Beyondsoft is seeking a Senior Data Engineer for a contract position with one of our leading clients in Redmond, WA.\xa0', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0While no degree is required, there is a preference for someone with a Computer Science, Data Science or Machine Learning degree is a plus.', '\ufeffResponsibilities:', '\xa0', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Building pipelines in Azure Data Factory.', 'You will help build state of the art pipelines and data models that are at the heart of the decision-making process.', '5+ years’ experience designing and implementing scalable ETL processes including data movement (SSIS, replication, etc.) and quality tools. ', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Modern Big Data Analytics using Data Lake, Spark and formats like Parquet', 'Strong communication and collaboration skills', 'As a member of the team, you will get to live on the front edge of modern technology by building on things like Azure Data Explorer and Azure Data Lake while building ETLs and process in Databricks and Data Factory. You will help build state of the art pipelines and data models that are at the heart of the studio decision-making process. Your large-scale warehouses will be multi-source environments that present data feeds to real-time production systems. Your customers will be the business, design, test and development teams and they will look to you to help shape how we capture data to improve our test-driven methodologies and build a culture around data driven development.', 'Senior Data Engineer', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Working with data in and from Azure Data Explorer/Kusto.']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Maritz,"Frederick, MD",4 weeks ago,Be among the first 25 applicants,"['', 'Maritz will only employ applicants who have authorization to work permanently in the U.S. This is not a position for which sponsorship will be provided. Those who need sponsorship for work authorization now or in the future are not eligible for hire. No calls or agencies please.', 'Maritz is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to sex, race, color, religion, national origin, age, marital status, political affiliation, sexual orientation, gender identity, genetic information, disability or protected veteran status. We are committed to providing a workplace free of any discrimination or harassment. If you have a disability and are having difficulty accessing or using this website to apply for a position, you can request help by calling 1-636-827-1650 or by sending an email to idisability.administrator@maritz.com .', 'EXCITED TO GROW YOUR CAREER? WE’RE GLAD YOU’RE HERE! ', 'Performing tuning experience preferred.', 'Experience with Microsoft SQL server or equivalent in development and production environment. Experience with creating database object and performing data analysis.', 'AWS Experience preferred.', 'C# experience preferred. ', 'EXCITED TO GROW YOUR CAREER? WE’RE GLAD YOU’RE HERE!  ', '3-5 years’ experience working with Microsoft SQL Server or equivalent.', 'Qualifications', 'Experience implementing 3rd party tools for monitoring, comparing, or auditing.', 'Bachelor’s degree in Computer Science, equivalent field of study or equivalent job experience.3-5 years’ experience working with Microsoft SQL Server or equivalent.C# experience preferred. AWS Experience preferred.Experience with Microsoft SQL server or equivalent in development and production environment. Experience with creating database object and performing data analysis.Experience implementing 3rd party tools for monitoring, comparing, or auditing.Performing tuning experience preferred.', 'Bachelor’s degree in Computer Science, equivalent field of study or equivalent job experience.', 'Primary Responsibilities', ' DISCLAIMER: ']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Talend Data Engineer,World Wide Technology,"New Jersey, United States",21 hours ago,Be among the first 25 applicants,"['', '5+ years of infrastructure-as-code or software-development experience; Python and Java experience are preferred', 'Company Overview', 'Further developing subject matter expertise in data and cloud architecture and technologies to support initiatives across the company', 'World Wide Technology (WWT) is a global technology integrator and supply chain solutions provider. Through our culture of innovation, we inspire, build, and deliver business results, from idea to outcome. ', 'Talend Data Engineer', 'Talend Data Engineer ', 'Experience as a developer (Backend Development, Front End Development, Splunk Development)', 'Duration/Type of Job: 6+ Months', 'Data warehouse experience: data sourcing, analyzing and modeling experience.', '\xa0', ""Facilitating and leading cross-functional teams to define, expand, document, and implement the company's data lake resources"", 'Title: Talend Data Engineer', 'Based in St. Louis, WWT works closely with industry leaders such as Cisco, HPE, Dell EMC, NetApp, VMware, Intel, AWS, Microsoft, and F5, focusing on three market segments: Fortune 500 companies, service providers and the public sector.\xa0WWT is a $13 billion dollar privately held organization that employs more than 6,000 people and operates in more than 2.5+ million square feet of state-of-the-art warehousing, distribution and integration space strategically located throughout the world.\xa0WWT is proud to announce that it has been named on the FORTUNE ""100 Best Places to Work For®"" list for the ninth consecutive year and was awarded for multiple categories on Glassdoor’s 2019 Employees\' Choice Awards, honoring the Best Places to Work.', 'New Jersey (Remote). ', 'Designing cloud-infrastructure architecture options for the company’s data pipelines and stores', 'Researching and implementing best practices derived from industry experiences and standards', 'Location: New Jersey (Remote)', ""Experience as a developer (Backend Development, Front End Development, Splunk Development)Designing cloud-infrastructure architecture options for the company’s data pipelines and storesResearching and implementing best practices derived from industry experiences and standardsFacilitating and leading cross-functional teams to define, expand, document, and implement the company's data lake resourcesFurther developing subject matter expertise in data and cloud architecture and technologies to support initiatives across the company5+ years of progressive systems-infrastructure and cloud experience5+ years of infrastructure-as-code or software-development experience; Python and Java experience are preferred5+ years of ETL and data-pipeline experience using tools, specifically Talend3+ years of database experience, including database administration, database queries, and data modellingData warehouse experience: data sourcing, analyzing and modeling experience."", 'World Wide Technology Holding Co, LLC. (WWT) has an opportunity available Talend Data Engineer to support our client located in the New Jersey (Remote). This position will be focused on the migration and preparation of a large health systems data utilizing Talend.', '5+ years of progressive systems-infrastructure and cloud experience', 'Equal Opportunity Employer Minorities/Women/Veterans/Disabled', '5+ years of ETL and data-pipeline experience using tools, specifically Talend', '3+ years of database experience, including database administration, database queries, and data modelling', 'Responsibilities/Job Duties/Job Description/Qualifications:']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,E. & J. Gallo Winery,"Modesto, CA",5 days ago,Be among the first 25 applicants,"['', 'What You Will Do', 'Experience building and optimizing data sets.', 'Creates ETLs (Extract Transform Load), validates, and supports data movement.', 'What You Will Need', 'Must Have experience in 2 of the following: Database technologies Oracle, Hana, MsSQL, Postgres, Neo4J, RedShift.', 'Identifies and builds large data sets to address business needs.', 'Builds processes that support data transformation, workload management, data structures, dependency and metadata', 'Deploys sophisticated analytics programs, machine learning and statistical methods.', 'We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. Gallo is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process.If you need any assistance or accommodations due to a disability, please let us know at 209.341.7000.', ""Creates ETLs (Extract Transform Load), validates, and supports data movement.Assembles large, complex sets of data that meet non-functional and functional business requirements.Identifies, designs and implements internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes.Builds required pipeline for optimal extraction, transformation and loading of data from various data sources using ANSI SQL technologies.Builds analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisitionWorks with stakeholders including data, design, product and executive teams and assists with data-related technical issues.Works with stakeholders including the Executive, Product, Data and Design teams to support data infrastructure needs while assisting with data-related technical issues.Identifies and builds large data sets to address business needs.Deploys sophisticated analytics programs, machine learning and statistical methods.Prepares data for predictive and prescriptive modeling.Containerizes workloads by processing to be used by a receiving system allowing for more isolated workloads, to increase quality and decrease conflicts between job requirements.Builds and optimizes data sets, 'big data' data pipelines and architectures.Performs root cause analysis on external and internal processes and data to identify opportunities for improvement and answers questions.Builds processes that support data transformation, workload management, data structures, dependency and metadataMaintains satisfactory attendance, to include timeliness.Responsible for understanding and complying with applicable quality, environmental and safety regulatory considerations. If accountable for the work of others, responsible for ensuring their understanding and compliance."", ""Builds and optimizes data sets, 'big data' data pipelines and architectures."", 'AWS Certified in Big Data.', 'Experience with Big data tools such as Hadoop, Spark, Kafka, etc.', 'Maintains satisfactory attendance, to include timeliness.', 'Experience creating Containerized workloads.', 'Our Brands. Your Future.', 'Experience Processing Data Streams.', 'Experience performing root cause analysis on internal and external processes and data to identify opportunities for improvement and to answer questions.', 'Containerizes workloads by processing to be used by a receiving system allowing for more isolated workloads, to increase quality and decrease conflicts between job requirements.', 'Bachelor’s Degree in Computer Science, MIS, Mathematics, Engineering, Data Sciences, Economics, or Physics plus 4 years of working in a product or platform development environment reflecting increasing levels of responsibility.', 'AWS Certified Solution Architect.', 'Works with stakeholders including data, design, product and executive teams and assists with data-related technical issues.', 'Master’s Degree in Computer Science, MIS, Mathematics, Engineering, or Data Sciences plus experience working in a product or platform development environment; OR Bachelor’s Degree in Computer Science, MIS, Mathematics, Engineering, Data Sciences, Economics, or Physics plus 2 years of experience working in a product or platform development environment reflecting increasing levels of responsibility; OR Bachelor’s Degree plus 3 years of experience working in a product or platform development environment reflecting increasing levels of responsibility; OR Highschool Diploma plus 7 years of experience working in a product or platform development environment reflecting increasing levels of responsibility.', 'At least 2 years programming experience within Python.', 'Works with stakeholders including the Executive, Product, Data and Design teams to support data infrastructure needs while assisting with data-related technical issues.', 'Performs root cause analysis on external and internal processes and data to identify opportunities for improvement and answers questions.', 'Ability to write ANSI SQL queries.', 'Identifies, designs and implements internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes.', 'Master’s Degree in Computer Science, MIS, Mathematics, Engineering, or Data Sciences plus experience working in a product or platform development environment; OR Bachelor’s Degree in Computer Science, MIS, Mathematics, Engineering, Data Sciences, Economics, or Physics plus 2 years of experience working in a product or platform development environment reflecting increasing levels of responsibility; OR Bachelor’s Degree plus 3 years of experience working in a product or platform development environment reflecting increasing levels of responsibility; OR Highschool Diploma plus 7 years of experience working in a product or platform development environment reflecting increasing levels of responsibility.Must Have experience in 2 of the following: Database technologies Oracle, Hana, MsSQL, Postgres, Neo4J, RedShift.Experience Processing Data Streams.At least 2 years programming experience within Python.Ability to write ANSI SQL queries.Knowledge of basic programming against Spark.Experience creating Containerized workloads.Experience building and optimizing data sets.Experience performing root cause analysis on internal and external processes and data to identify opportunities for improvement and to answer questions.', 'Builds analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition', 'Assembles large, complex sets of data that meet non-functional and functional business requirements.', 'Prepares data for predictive and prescriptive modeling.', 'Gallo is an Equal Employment opportunity and Affirmative Action Employer. We do not discriminate on the basis of race, traits historically associated with race, including but not limited to, hair texture and protective hairstyles (such as braids, locks, and twists), color, national origin, ancestry, creed, religion, physical disability, mental disability, medical condition as defined by applicable state law (including cancer and predisposing genetic characteristics in California), genetic information, marital status, familial status, sex, gender, gender identity, gender expression, pregnancy, childbirth or related medical conditions, sexual orientation (actual or perceived), transgender status, sex stereotyping, age, military or veteran status, domestic violence or sexual assault victim status, or any other basis protected by applicable law. ', 'Experience with AWS cloud services: Ec2, EMR, RDS, Redshift, etc.', 'What Will Set You Apart', 'Master’s Degree in Computer Science, MIS, Mathematics, Engineering, Data Sciences plus 2 years of experience working in a product or platform development environment reflecting increasing levels of responsibility; ORBachelor’s Degree in Computer Science, MIS, Mathematics, Engineering, Data Sciences, Economics, or Physics plus 4 years of working in a product or platform development environment reflecting increasing levels of responsibility.AWS Certified in Big Data.AWS Certified Solution Architect.Experience with Big data tools such as Hadoop, Spark, Kafka, etc.Experience with AWS cloud services: Ec2, EMR, RDS, Redshift, etc.Programming experience against Pyspark, Scala, etc.Experience building and optimizing ‘big data’ data pipelines and architectures.Excellent analytical skills associated with working on datasets.', 'Programming experience against Pyspark, Scala, etc.', 'Master’s Degree in Computer Science, MIS, Mathematics, Engineering, Data Sciences plus 2 years of experience working in a product or platform development environment reflecting increasing levels of responsibility; OR', 'Builds required pipeline for optimal extraction, transformation and loading of data from various data sources using ANSI SQL technologies.', 'Experience building and optimizing ‘big data’ data pipelines and architectures.', 'Excellent analytical skills associated with working on datasets.', 'Knowledge of basic programming against Spark.', 'To View a Full Job Description, Please Click Here.', 'Why Gallo?', 'Responsible for understanding and complying with applicable quality, environmental and safety regulatory considerations. If accountable for the work of others, responsible for ensuring their understanding and compliance.']",Not Applicable,Full-time,Information Technology,Consumer Goods,2021-03-24 13:05:10
Senior Data QA Engineer,Amwell,"Boston, MA",24 hours ago,29 applicants,"['', '4 years of experience in test execution analysis & reporting.', 'The equipment and resources needed to be successful working in a remote setting', 'Working at Amwell', 'Knowledge on Test Automation, Python and Cloud Technology (AWS etc)', 'Ability to communicate complex technology solutions to diverse teams namely, technical, business and management teams', 'Help track quality assurance metrics and KPIs such as defect densities and defect counts', 'Your Team:', 'Understand the test cases / scenarios, design and develop simple scripts using specialized testing tool(s)', 'Core Responsibilities:', 'Perform regression testing when defects are resolved', 'Knowledge of Oracle, Python and Unix command and Data profiling', 'Experience managing team size of 2-3 would be a plus', ' 4 years of experience in test strategy and test Design 4 years of experience in test execution analysis & reporting. At least 4 years of experience in ETL Testing & Report Testing At least 2 years of experience in Informatica and Netezza Knowledge of ETL tools to execute data load jobs and analyze job failure & log issues Knowledge of Oracle, Python and Unix command and Data profiling Hands on knowledge of SQL writing and troubleshooting data issues Use RDBMS system to execute the test scripts, modify the scripts as required, identify, classify and raise defects, and participate in defect triages Knowledge on Test Automation, Python and Cloud Technology (AWS etc) Understand the test cases / scenarios, design and develop simple scripts using specialized testing tool(s) Ability to communicate complex technology solutions to diverse teams namely, technical, business and management teams Experience managing team size of 2-3 would be a plus Excellent verbal and written communication skills ', 'Qualifications:', 'Excellent verbal and written communication skills', 'Use RDBMS system to execute the test scripts, modify the scripts as required, identify, classify and raise defects, and participate in defect triages', 'Additional Information', 'Competitive health, dental and vision insurance plans', 'Knowledge of ETL tools to execute data load jobs and analyze job failure & log issues', ' Create Test Strategy, Test scenarios and Test cases with hands on technical knowledge Identify, record, and thoroughly document test cases Perform regression testing when defects are resolved Perform functional and nonfunctional validation of the algorithms, services, and components that make up our cloud ETL services. Help track quality assurance metrics and KPIs such as defect densities and defect counts Stay up to date with new software testing tools and testing methodologies Develop subject matter expertise on the Amwell platform ', 'Develop subject matter expertise on the Amwell platform', 'Identify, record, and thoroughly document test cases', 'Create Test Strategy, Test scenarios and Test cases with hands on technical knowledge', 'At least 2 years of experience in Informatica and Netezza', 'At least 4 years of experience in ETL Testing & Report Testing', ' Unlimited Paid Time Off 401K match Competitive health, dental and vision insurance plans The equipment and resources needed to be successful working in a remote setting', 'Brief Overview:', 'Unlimited Paid Time Off', 'Company Description', '401K match', 'Perform functional and nonfunctional validation of the algorithms, services, and components that make up our cloud ETL services.', 'Hands on knowledge of SQL writing and troubleshooting data issues', 'Stay up to date with new software testing tools and testing methodologies', '4 years of experience in test strategy and test Design']",Associate,Full-time,Quality Assurance,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,AirDNA,"Denver, CO",6 days ago,27 applicants,"['', 'Competitive cash compensation and benefits,\xa0the salary range for this position is $110,000 - $140,000Meaningful equity upside—we are a small team with no outside investors and want every hire to share in our long term financial successWe have a great LoDo office space, a short walk from the Union Station light-rail stop; enjoy coffee, tea, beer, snacks, bike parking and great food and drink nearby', 'Competitive cash compensation and benefits,\xa0the salary range for this position is $110,000 - $140,000', 'Your Role', 'Passionate about providing well documented and user friendly data sources', 'Job Perks', 'Cluster management', '3+ years of data engineering experience', 'Meaningful equity upside—we are a small team with no outside investors and want every hire to share in our long term financial success', 'Familiarity with Scala (or similar JVM language)', 'Build, maintain, and monitor data pipelinesCluster managementHelp design and build out the next generation of our data platformCraft and optimize data pipelines using SparkImplement data and application integrations with key industry partnersWork closely with our Data Science team to develop new products and scale out algorithmsMaintain and migrate legacy data systems', 'Help design and build out the next generation of our data platform', 'Strong experience with Spark', 'Experience building composable and stable ETL systems', ""Here's what you'll get to do:"", 'About AirDNA', 'We have a great LoDo office space, a short walk from the Union Station light-rail stop; enjoy coffee, tea, beer, snacks, bike parking and great food and drink nearby', ""We're a tight-knit group who care about empowering the next generation of shared economy entrepreneurs. We like to work hard. We believe in work/life balance because we make work a fun environment you’ll want to be a part of. We all wear multiple hats and are working on at least 5 projects at a time. Life is never boring here. We're risk takers. We care about each other and lend helping hands all day, every day."", 'Our customer roster includes a who’s who of investment banks, hotels, hedge funds, real estate investors, and vacation rental pros. We are bootstrapped, profitable, and looking to take the world by storm in 2021 and beyond.', 'Craft and optimize data pipelines using Spark', '3+ years of data engineering experienceStrong experience with SparkFamiliarity with Scala (or similar JVM language)Experience building composable and stable ETL systemsPassionate about providing well documented and user friendly data sourcesDemonstrated successful experience working in a hands-on, fast-paced, creative, entrepreneurial environmentBS or MS in Computer Science/Engineering preferred', ""Here's what you'll need to be successful:"", 'Implement data and application integrations with key industry partners', 'Demonstrated successful experience working in a hands-on, fast-paced, creative, entrepreneurial environment', 'BS or MS in Computer Science/Engineering preferred', '\ufeffThe AirDNA Team', 'Maintain and migrate legacy data systems', 'Work closely with our Data Science team to develop new products and scale out algorithms', 'AirDNA is looking for a Data Engineer to help us improve and build a solid and scalable data infrastructure and analytical environment. The ideal candidate enjoys data wrangling, optimizing data pipelines, and building solutions for data collection and analysis. The AirDNA infrastructure currently processes billions of data points and hosts databases well into terabyte-size\xa0ranges, making this an interesting and challenging opportunity.\xa0', ""As the world's leading provider of short-term rental data and analytics, AirDNA tracks the daily performance of over 10 million properties on Airbnb and Vrbo in 120,000 global markets. Through a combination of machine learning algorithms and source data on over 750,000 vacation rental properties, AirDNA has revolutionized pricing, benchmarking, and investing in alternative accommodations."", 'Build, maintain, and monitor data pipelines']",Mid-Senior level,Full-time,Engineering,"Leisure, Travel & Tourism",2021-03-24 13:05:10
Data Engineer,Calibrate,"New York, NY",4 weeks ago,57 applicants,"['', 'About Our Team', 'Products you will build and work on:', 'You will be working with our member facing, coaches and operation, medical, growth team ', '5+ years of Data engineering experience shipping production level code. Should be able to hit the ground running and be comfortable deploying a change in the first week. ', 'Generous paid time off', 'As a member of our fast growing product and engineering team, you have the opportunity to work on a wide range of products. ', 'Calibrate ', 'Robust internal framework that connects to external healthcare and non-healthcare data sources.', 'AIl telehealth communication platform (chat, video, phone) between patients and providers.', 'Real results matter', 'Who We Are', 'OUR MISSION', 'Calibrate is committed to bringing together people from different backgrounds and perspectives to deliver real results for our members. ', 'About The Role', ""You're in control"", 'Responsibilities:', ' Consumer facing application (mobile and web) to engage and change behaviors to improve weight health. AIl telehealth communication platform (chat, video, phone) between patients and providers. Doctor and coaching internal platform that helps our team efficiently deliver the right care to the right members at the right time. Growth product and infrastructure to drive sales from initial ad campaigns through conversion. Robust internal framework that connects to external healthcare and non-healthcare data sources. ', 'Qualifications:', 'Small wins create big wins', 'Doctor and coaching internal platform that helps our team efficiently deliver the right care to the right members at the right time.', 'Competitive salary, with opportunity for equity in an early stage, high growth business', 'OUR VALUES', 'NYC-based with opportunities to be fully remote', 'Senior Data Engineers ', 'Competitive Paid Parental Leave for parents', 'One Medical membership and Teladoc virtual care', ' Competitive salary, with opportunity for equity in an early stage, high growth business Generous paid time off Calibrate-funded health benefits (medical, dental, vision) - starting at zero cost to you One Medical membership and Teladoc virtual care Therapy on your time with three free months of Talkspace membership Health advocacy and assistance services through Health Advocate Commuter benefits, ClassPass memberships, and gym memberships for post-pandemic life NYC-based with opportunities to be fully remote Competitive Paid Parental Leave for parents ', ""At Calibrate, we're committed to our vision of putting our members and our teammates in control of their health. Some of our benefits include:"", 'Commuter benefits, ClassPass memberships, and gym memberships for post-pandemic life', 'Solution-oriented and motivated by complex challenges that have a deep impact.', 'Growth product and infrastructure to drive sales from initial ad campaigns through conversion.', ' 5+ years of Data engineering experience shipping production level code. Should be able to hit the ground running and be comfortable deploying a change in the first week.  Experience with relational databases, like PostgreSQL Experience with data warehouses, like Big Query, Redshift Experience with data transformation system, like dbt ', 'Experience with data transformation system, like dbt', 'Our teams are organized based on the Spotify squad model. ', 'Consumer facing application (mobile and web) to engage and change behaviors to improve weight health.', ' As a member of our fast growing product and engineering team, you have the opportunity to work on a wide range of products.  Our teams are organized based on the Spotify squad model.  You will be working with our member facing, coaches and operation, medical, growth team  ', 'Benefits', 'Experience with relational databases, like PostgreSQL', 'Experience with data warehouses, like Big Query, Redshift', ' A passionate, people-minded and mission-driven group of product, technical and medical professionals from companies like One Medical, Oscar Health, Capsule and more. Solution-oriented and motivated by complex challenges that have a deep impact. ', 'A passionate, people-minded and mission-driven group of product, technical and medical professionals from companies like One Medical, Oscar Health, Capsule and more.', 'recruits, employs, compensates, and promotes regardless of race, religion, color, national origin, gender identity, disability, age, veteran status, and other protected status as required by applicable law and as a matter of our company values.', 'Health advocacy and assistance services through Health Advocate', 'Calibrate-funded health benefits (medical, dental, vision) - starting at zero cost to you', ""We're in this together"", 'Therapy on your time with three free months of Talkspace membership']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer ,"IDR, Inc.","Atlanta, GA",6 days ago,60 applicants,"['', 'Hands on programming experience in either Scala, Python, or Spark\xa0', ""IDR's largest client is looking to hire a Fully Remote Data Engineer to their team. This individual will be responsible for\xa0migrating existing on premise data warehouse to Azure platform."", 'Qualifications of the data engineer: ', 'Join a flexible friendly laid-back environment', 'What’s in it for you?', '3+ years of professional experience as a Data Engineer\xa0\xa0', '3+ years of professional experience as a Data Engineer\xa0\xa0Hands on programming experience in either Scala, Python, or Spark\xa0Experience with Azure Data Lake and Azure Data FactoryExperience with C#/.NET (preferred)Working knowledge of one of the following: PowerBI, Tableau or Mendix', 'Responsibilities of the Data Engineer include: ', '\xa0', 'Developing hands on pipelines based on requirementsDesigning, developing and maintaining reliable data solutions based on the identification, collection and evaluation of business requirementsWorking on a technical team with Agile practices and methodologiesProactively identify and implement needed system changes, updates, inefficiencies and/or inaccuracies that impact the efficiency and reliability of the system.', 'Join an extremely secure organization that offers job stability', 'Enjoy true work/life balance', 'Designing, developing and maintaining reliable data solutions based on the identification, collection and evaluation of business requirements', 'Enjoy extremely competitive compensation and benefits package', 'Developing hands on pipelines based on requirements', 'Proactively identify and implement needed system changes, updates, inefficiencies and/or inaccuracies that impact the efficiency and reliability of the system.', 'Data Engineer', 'Join a flexible friendly laid-back environmentEnjoy extremely competitive compensation and benefits packageEnjoy true work/life balanceJoin an extremely secure organization that offers job stability', 'Experience with Azure Data Lake and Azure Data Factory', 'Experience with C#/.NET (preferred)', 'Working on a technical team with Agile practices and methodologies', 'Working knowledge of one of the following: PowerBI, Tableau or Mendix']",Mid-Senior level,Full-time,Information Technology,Staffing and Recruiting,2021-03-24 13:05:10
Data Engineer,RevolutionParts,"Phoenix, AZ",3 weeks ago,52 applicants,"['', '3+ years experience as a data engineerAbility to own data problems and help to shape the solution for business challengesAbility to work with others and know when to support and when to pushGood communication and collaboration skills; comfortable discussing projects with anyone from end users up to the executive company leadershipFluency with a scripting language - we use Python and PHP heavily Ability to write and optimize complex SQL statementsFamiliarity with ETL pipeline tools such as Airflow or AWS GlueFamiliarity with data visualization and reporting tools, especially TableauExperience working in a cloud-based software development environment, preferably with AWSFamiliarity with no-SQL databases such as ElasticSearch, DynamoDB or MongoDB', ' Responsibilities ', 'Ability to write and optimize complex SQL statements', 'Support technical and business stakeholders by providing key reports and supporting the BI team to become fully self-service', 'Understand our data sources, ETL logic, and data schemas and help craft tools for managing the full data lifecyclePlay a key role in building the next generation of our data ingestion pipeline and data warehouseRun ad hoc analysis of our data to answer questions and help prototype solutionsSupport and optimize existing ETL pipelinesSupport technical and business stakeholders by providing key reports and supporting the BI team to become fully self-serviceOwn problems through to completion both individually and as part of a data teamSupport product engineering teams by performing query analysis and optimization', '3+ years experience as a data engineer', 'Familiarity with data visualization and reporting tools, especially Tableau', 'Experience working in a cloud-based software development environment, preferably with AWS', 'Play a key role in building the next generation of our data ingestion pipeline and data warehouse', 'Support and optimize existing ETL pipelines', 'Ability to work with others and know when to support and when to push', 'Ability to own data problems and help to shape the solution for business challenges', 'Own problems through to completion both individually and as part of a data teamSupport product engineering teams by performing query analysis and optimization', 'Familiarity with ETL pipeline tools such as Airflow or AWS Glue', 'Understand our data sources, ETL logic, and data schemas and help craft tools for managing the full data lifecycle', 'Good communication and collaboration skills; comfortable discussing projects with anyone from end users up to the executive company leadership', 'Run ad hoc analysis of our data to answer questions and help prototype solutions', 'Fluency with a scripting language - we use Python and PHP heavily ', 'Familiarity with no-SQL databases such as ElasticSearch, DynamoDB or MongoDB']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Associate Data Engineer,KPMG US,"New York, NY",4 days ago,50 applicants,"['', 'Rapidly architect, design, prototype, and implement architectures to tackle the Big Data needs for a variety of Fortune 1000 corporations and other major organizations', 'Ability to quickly learn, adapt, and implement Open Source technologies and desire to learn new skills and techniques', 'Requisition Number:', ""Rapidly architect, design, prototype, and implement architectures to tackle the Big Data needs for a variety of Fortune 1000 corporations and other major organizationsWork in cross-disciplinary teams with KPMG industry experts to understand client needs and ingest data sources such as documents, financial data, and operational dataResearch, experiment, and utilize leading Big Data methodologies, such as Hadoop, Spark, Redshift, Netezza, SAP HANA, and Microsoft AzureImplement and test data processing pipelines, and data mining / data science algorithms on a variety of hosted settings, such as AWS, Azure, client technology stacks, and KPMG's own clustersTranslate advanced business analytics problems into technical approaches that yield actionable recommendationsDevelop skills in business requirement capture and translation, hypothesis-driven consulting, work stream and project management, and client relationship development"", 'A minimum of one year of data analytics experience with a an internal strategy/analytics group, or similar environment', 'Research, experiment, and utilize leading Big Data methodologies, such as Hadoop, Spark, Redshift, Netezza, SAP HANA, and Microsoft Azure', ""Bachelor's degree in a technical field from an accredited college or university (master's or MBA degree preferred) with expertise in programming languages and a working knowledge of topics such as Data Exploration, Profiling, Quality, Transformation and Mining"", ""Implement and test data processing pipelines, and data mining / data science algorithms on a variety of hosted settings, such as AWS, Azure, client technology stacks, and KPMG's own clusters"", 'Proficient in designing efficient and robust ETL/ELT workflows, schedulers, and event-based triggers', ""A minimum of one year of data analytics experience with a an internal strategy/analytics group, or similar environmentBachelor's degree in a technical field from an accredited college or university (master's or MBA degree preferred) with expertise in programming languages and a working knowledge of topics such as Data Exploration, Profiling, Quality, Transformation and MiningProficient in designing efficient and robust ETL/ELT workflows, schedulers, and event-based triggersAbility to quickly learn, adapt, and implement Open Source technologies and desire to learn new skills and techniquesAbility to work independently with limited supervision as well as contribute to team efforts and ability to support multiple projects simultaneously and work in a fast-paced environmentAbility to travel up to eighty percent of the time; Applicants must be currently authorized to work in the United States without the need for visa sponsorship now or in the future"", 'Ability to work independently with limited supervision as well as contribute to team efforts and ability to support multiple projects simultaneously and work in a fast-paced environment', 'Develop skills in business requirement capture and translation, hypothesis-driven consulting, work stream and project management, and client relationship development', 'Description', 'Translate advanced business analytics problems into technical approaches that yield actionable recommendations', 'Responsibilities', 'Qualifications', 'Ability to travel up to eighty percent of the time; Applicants must be currently authorized to work in the United States without the need for visa sponsorship now or in the future', 'Work in cross-disciplinary teams with KPMG industry experts to understand client needs and ingest data sources such as documents, financial data, and operational data']",Mid-Senior level,Full-time,General Business,Management Consulting,2021-03-24 13:05:10
Data Engineer - Washington,Mobomo,"Washington, DC",4 weeks ago,Be among the first 25 applicants,"['', ' Must be able to communicate technical requirements across multiple organizations clearly and concisely', ' Must have at least three years of experience in the following: Data Warehouse Tools Data Audit and Profiling Data Warehouse ETL Testing Informatica Axon Informatica Data Quality Informatica Enterprise Data Catalog   Active Secret clearance required  Prior experience with federal government agencies', 'About Mobomo', ' Support the creation of a cloud based data management Infrastructure through the development of data lakes and a data warehouse. You will be involved in all aspects of the modeling process, including data discovery, data quality, and logical/physical modeling. The analyst should have experience with particular Informatica tools (EDC, IDQ, and Axon) in an Azure Cloud environment, as well as other Azure based tools such as Polybase and Synapse.', 'Responsibilities', ' Active Secret clearance required', ' Must have at least three years of experience in the following: Data Warehouse Tools Data Audit and Profiling Data Warehouse ETL Testing Informatica Axon Informatica Data Quality Informatica Enterprise Data Catalog ', 'Requirements', ' Prior experience with federal government agencies', 'Soft Skills', 'Data Engineer – Washington, D.C.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Infosys,"Phoenix, AZ",2 days ago,Be among the first 25 applicants,"['', 'Phoenix, AZ', 'Skillset', ' Ability to communicate complex technology solutions to diverse teams namely, technical, business and management teams ', 'Company', ' Ability to work in team in diverse/ multiple stakeholder environment ', ' Experience and desire to work in a Global delivery environment ', ' Experience in preparing test scripts and test cases to validate data and maintaining data quality ', 'Preferred Qualifications', ' Hands-on development, with a willingness to troubleshoot and solve complex problems ', ' Candidate must be located within commuting distance of Phoenix, AZ or be willing to relocate to the area. This position may require travel in the US and Canada. ', ' 1+ years of experience with Information Technology ', ' Excellent verbal and written communication skills ', ' CI / CD exposure ', 'Required Qualifications', 'Country', ' U.S. Citizenship or Permanent Residency required, we are not able to sponsor at this time ', ' EOE/Minority/Female/Veteran/Disabled/Sexual Orientation/Gender Identity/National Origin ', ' Knowledge and experience with full SDLC lifecycle ', ' 1+ year of experience in software development life cycle ', 'Interest Group', ' Bachelor’s Degree or foreign equivalent, will consider work experience in lieu of a degree ', ' Good expertise in impact analysis due to changes or issues ', 'Work Location', ' Strong knowledge and hands-on experience in SQL, Unix shell scripting ', ' 1+ years of experience in Big data and cloud technologies, solution design and documentation ', ' Data Engineer ', 'About Us', ' Strong understanding and hands-on programming/scripting experience skills – UNIX shell, Perl, and JavaScript ', ' 1+ years of experience in Project life cycle activities on development and maintenance projects ', ' Experience to Big data technologies is preferred. ', ' Candidate must be located within commuting distance of Phoenix, AZ or be willing to relocate to the area. This position may require travel in the US and Canada.  Bachelor’s Degree or foreign equivalent, will consider work experience in lieu of a degree  1+ years of experience with Information Technology  1+ years of experience in Big data and cloud technologies, solution design and documentation  Good experience in end-to-end implementation of DW BI projects, especially in data warehouse and mart developments  Python is a huge plus  Strong knowledge and hands-on experience in SQL, Unix shell scripting  Knowledge and experience with full SDLC lifecycle  Experience with Lean / Agile development methodologies  U.S. Citizenship or Permanent Residency required, we are not able to sponsor at this time ', 'State / Region / Province', ' Python is a huge plus ', ' Experience with Lean / Agile development methodologies ', ' The job may entail extensive travel. The job may also entail sitting as well as working at a computer for extended periods of time. Candidates should be able to effectively communicate by telephone, email, and face to face. ', ' 1+ year of experience in software development life cycle  1+ years of experience in Project life cycle activities on development and maintenance projects  Good experience in end-to-end implementation of DW BI projects, especially in data warehouse and mart developments  Good understanding of Data integration and Data Quality  Experience to Big data technologies is preferred.  Good expertise in impact analysis due to changes or issues  Experience in preparing test scripts and test cases to validate data and maintaining data quality  Strong understanding and hands-on programming/scripting experience skills – UNIX shell, Perl, and JavaScript  Hands-on development, with a willingness to troubleshoot and solve complex problems  CI / CD exposure  Ability to work in team in diverse/ multiple stakeholder environment  Ability to communicate complex technology solutions to diverse teams namely, technical, business and management teams  Excellent verbal and written communication skills  Experience and desire to work in a Global delivery environment ', ' Good understanding of Data integration and Data Quality ', 'This position may require travel in the US and Canada.', ' Good experience in end-to-end implementation of DW BI projects, especially in data warehouse and mart developments ', 'Domain', 'Job Description']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,TrueData,"Los Angeles, CA",2 weeks ago,157 applicants,"['', 'Speak the language - Be willing to create prose with at least three of Java, Kotlin, Scala, SQL, Python. Know which to use for the time & purpose.', ' Health Care Plan (Medical, Dental & Vision) Retirement Plan (401k, IRA) Paid Time Off (Vacation, Skick & Public Holidays) Family Leave (Maternity, Paternity, etc...) Work From Home', 'Prosper with us - Work with devops, analysts, data scientists, programmers, marketers, filmmakers, finance, project management, product management and leaders to amaze, surprise and delight.', '<3 data - Hypothesize, discover, collect, iterate and prove .', 'XP - 2+ years of programming experience and 3+ years of data pipeline experience. Academic and hobby experience counts.', '#algorithm and #datastructures fanatic - Have gorged on distributed, parallel and probabilistic algorithms and data structures.', ' XP - 2+ years of programming experience and 3+ years of data pipeline experience. Academic and hobby experience counts. Speak the language - Be willing to create prose with at least three of Java, Kotlin, Scala, SQL, Python. Know which to use for the time & purpose. <3 data - Hypothesize, discover, collect, iterate and prove . Achievement Unlocked Inquisitive, Determined, Scholar - Be up for the challenge. #algorithm and #datastructures fanatic - Have gorged on distributed, parallel and probabilistic algorithms and data structures. Build > buy - Know the power of seeking out and putting together the right open source into a highly flexible and cohesive system instead of purchasing a solution that delivers a portion of the need, stops you from delivering the rest and ties you to your pricey investment. ', 'Family Leave (Maternity, Paternity, etc...)', 'Achievement Unlocked Inquisitive, Determined, Scholar - Be up for the challenge.', 'Requirements', 'Build > buy - Know the power of seeking out and putting together the right open source into a highly flexible and cohesive system instead of purchasing a solution that delivers a portion of the need, stops you from delivering the rest and ties you to your pricey investment.', 'Create awesome software - Use agile techniques to build software and platforms like recommendation engines, analytics engines, advertising engines.', 'Empower the business - Support the business to unearth data, transform data, access data, understand data and exploit data', 'Command data - Leverage petabytes of data, data mining, statistics, machine learning, deep learning, open source, orchestration tools, NoSQL, Apache Spark, and Apache Kafka', 'Substantial equivalent research oriented analytical work experience may be acceptable. B.S. or M.S. in Computer Science, Mathematics, Engineering or a Physical Science from an accredited or notable institution preferred. ', 'Work From Home', 'Health Care Plan (Medical, Dental & Vision)', 'Paid Time Off (Vacation, Skick & Public Holidays)', 'Retirement Plan (401k, IRA)', 'Benefits', ' Empower the business - Support the business to unearth data, transform data, access data, understand data and exploit data Create awesome software - Use agile techniques to build software and platforms like recommendation engines, analytics engines, advertising engines. Command data - Leverage petabytes of data, data mining, statistics, machine learning, deep learning, open source, orchestration tools, NoSQL, Apache Spark, and Apache Kafka Prosper with us - Work with devops, analysts, data scientists, programmers, marketers, filmmakers, finance, project management, product management and leaders to amaze, surprise and delight. ']",Mid-Senior level,Full-time,Engineering,Marketing and Advertising,2021-03-24 13:05:10
Junior Data Engineer,Invesco Ltd.,"Atlanta, GA",2 weeks ago,50 applicants,[''],Associate,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Lead Data Engineer,General Motors,"Austin, TX",3 weeks ago,30 applicants,"['', 'In recent years, GM Information Technology has successfully executed the largest IT transformation in the history of the automotive industry, fully insourcing what once was a nearly completely outsourced IT function. Today GM IT is a dynamic and fast paced organization that designs, develops and maintains all IT infrastructure, applications and solutions enabling GM’s global operations. From designing and building the next generation of electric and other vehicles to developing a world-class GM experience for our dealers and customers, GM IT is driving real change in the most iconic automaker on the planet. Our team delivers unique enterprise-wide IT solutions in cutting-edge technologies such as mobility, telematics, mission-critical business systems, supercomputing, cloud, vehicle engineering and real-time computing. We offer challenging positions for passionate professionals looking to advance their careers and be a part of an IT organization focused on innovation, speed and business value.', 'Communicate and maintains Master Data, Metadata, Data Management Repositories, Logical Data Models, Data StandardsCreate and maintain optimal data pipeline architectureYou will assemble large, complex data sets that meet functional / non-functional business requirementsYou will identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Build industrialized analytic datasets and delivery mechanisms that utilize the data pipeline to deliver actionable insights into vehicle quality, operational efficiency and other key business performance metricsWork with business partners on data-related technical issues and develop requirements to support their data infrastructure needsCreate highly consistent and accurate analytic datasets suitable for business intelligence and data scientist team members', 'We are not able to accommodate international relocation.', 'Communicate and maintains Master Data, Metadata, Data Management Repositories, Logical Data Models, Data Standards', 'Ability to tackle software engineering and data problems quickly and completely', ""developing data architecture and ETL solutions using sound, repeatable, industry standard methodologies. You will have the opportunity to work hands-on defining ETL solutions based on business requirements and system specifications.Development activities will include enhancing existing data systems and optimizing ETL systems.Additionally you will lead development activities to migrate out of legacy technologies to GM's Big Data Platform utilizing new technologies such as, Kafka, Hadoop, PySpark, Greenplum and GM's internally developed Big Data tools."", ""Development activities will include enhancing existing data systems and optimizing ETL systems.Additionally you will lead development activities to migrate out of legacy technologies to GM's Big Data Platform utilizing new technologies such as, Kafka, Hadoop, PySpark, Greenplum and GM's internally developed Big Data tools."", ' Healthcare (including a triple tax advantaged health savings account and wellness incentive), dental, vision and life insurance plans to cover you and your family;', 'Understand and maintain compliance with GM standards and industry standard methodology', 'For This Role You Will Be Responsible For', 'You will assemble large, complex data sets that meet functional / non-functional business requirements', 'Preferred', ' Company and matching contributions to 401K savings plan to help you save for retirement;', 'At least 3 years of hands on experience with Big Data Tools: Hadoop, Spark, Kafka, etc.', 'Ability to identify tasks which require automation and automate them', 'In this role you will also lead junior developers, mentor, coach and help them to develop their software development skills in Data Engineering technologies.Demonstrate mastery of many programming languages, tools and/or technologies with emphasis on ETL & Database developmentUnderstand and maintain compliance with GM standards and industry standard methodology', 'Requirements', 'About GM', 'In this role you will also lead junior developers, mentor, coach and help them to develop their software development skills in Data Engineering technologies.', 'Work with business partners on data-related technical issues and develop requirements to support their data infrastructure needs', ' Discount on GM vehicles for you, your family and friends.', '7 or more years with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.', 'Responsibilities', 'Build industrialized analytic datasets and delivery mechanisms that utilize the data pipeline to deliver actionable insights into vehicle quality, operational efficiency and other key business performance metrics', 'developing data architecture and ETL solutions using sound, repeatable, industry standard methodologies. You will have the opportunity to work hands-on defining ETL solutions based on business requirements and system specifications.', 'Create and maintain optimal data pipeline architecture', ' Paid time off including vacation days, holidays, and parental leave for mothers, fathers and adoptive parents;', ""At least 3 years of hands on experience with Big Data Tools: Hadoop, Spark, Kafka, etc.Master databases - Advanced SQL and NoSQL databases, including Postgres and Cassandra, Oracle and GreenplumData Wrangling and Preparation using GM's approved tools: PySpark, Jupyter etc.Stream-processing systems: Storm, Spark-Streaming, etc.Ability to tackle software engineering and data problems quickly and completelyAbility to identify tasks which require automation and automate themAbility to multi-task and stay organized in a dynamic work environment and work collaboratively with other IT organizations (GDAAS, Platform Engineering etc)"", 'Demonstrate mastery of many programming languages, tools and/or technologies with emphasis on ETL & Database development', ""Data Wrangling and Preparation using GM's approved tools: PySpark, Jupyter etc.Stream-processing systems: Storm, Spark-Streaming, etc."", 'As a Data Engineer, you will build industrialized data assets and optimize data pipelines in support of Vehicle Quality Business Intelligence and Advance Analytics objectives. You will work closely with our Quality Business teams, forward-thinking Data Scientists, BI Developers, System Architects and Data Architects to deliver value to our vision for the future.', 'Master databases - Advanced SQL and NoSQL databases, including Postgres and Cassandra, Oracle and Greenplum', ' Tuition assistance and student loan refinancing;', 'Ability to multi-task and stay organized in a dynamic work environment and work collaboratively with other IT organizations (GDAAS, Platform Engineering etc)', ' Global recognition program for peers and leaders to recognize and be recognized for results and behaviors that reflect our company values;', 'Benefits Overview', 'Job Description', 'Create highly consistent and accurate analytic datasets suitable for business intelligence and data scientist team members', 'You will identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.']",Not Applicable,Full-time,Information Technology,Automotive,2021-03-24 13:05:10
Data Engineer,Qwinix,"Denver, CO",3 weeks ago,31 applicants,"['', 'What You Will Do', 'Location', 'Who We Are']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer - Remote US,Seamless.AI,"Columbus, OH",2 days ago,Be among the first 25 applicants,"['', ' Working experience in an AGILE environment with modern SDLC', ' 5+ years of hands-on professional (not academic) experience in pioneering data engineering and/or big data. Working experience in an AGILE environment with modern SDLC Expert knowledge of data engineering, predictive modeling, statistics and machine learning in general. Deep experience with algorithms, statistics, and data analytics. Experience/knowledge in our more major technical & programming languages/cloud computing including but not limited to: Aurora PostgreSQL, AWS Redshift, AWS Glue, AWS EMR, AWS Spark (Scala/Python/PySpark), AWS QuickSight, Presto, DynamoDB, Redis, ELK stack, Node.JS, Pandas (Python Library)', ' Apply Today If You Have ', ' Expert knowledge of data engineering, predictive modeling, statistics and machine learning in general. Deep experience with algorithms, statistics, and data analytics.', ' Experience/knowledge in our more major technical & programming languages/cloud computing including but not limited to: Aurora PostgreSQL, AWS Redshift, AWS Glue, AWS EMR, AWS Spark (Scala/Python/PySpark), AWS QuickSight, Presto, DynamoDB, Redis, ELK stack, Node.JS, Pandas (Python Library)', ' 5+ years of hands-on professional (not academic) experience in pioneering data engineering and/or big data.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Jr. Data Engineer,ServiceTitan,"Atlanta, GA",1 week ago,37 applicants,"['', 'Establish', 'Map data from various legacy databases into the ServiceTitan platform, subsequently developing SQL scripts that will extract the information efficiently and accuratelyDevelop automated scripts to validate legacy database values and identify previously unmapped fields prior to loading them into the ServiceTitan platformApply feedback from customers and internal stakeholders on data import quality into previously developed extraction scriptsDiscover opportunities to leverage information from legacy databases into the implementation process to avoid inquiring for additional information from customersEstablish quality working relationships with internal stakeholdersContribute material input to go/no-go/continue decisions upon test completion', 'Given the experimental nature of this job, we will require very tight compliance when it comes to data - we need to focus on learning', 'Apply ', 'Strong analytical thinking skills', 'Equal Opportunity Employer', 'Family-Friendly Benefits:', 'Advanced knowledge and experience in T-SQL, complex ETL tools and operations, and SSIS', 'As Our Jr. Data Engineer, You Will', 'Contribute material input to go/no-go/continue decisions upon test completion', 'Enrichment: ', 'Develop', 'Discover ', ' Family-Friendly Benefits: extended parental leave, pregnancy support, 20k in adoption reimbursement, Snoo Smart Sleeper, back-up childcare credits, legal benefit, discounted pet insurance', 'Vertical SaaS experience is highly desirable', 'Develop automated scripts to validate legacy database values and identify previously unmapped fields prior to loading them into the ServiceTitan platform', ' Work/Life Balance: flexible work schedule, flexible PTO', ' Enrichment: ongoing learning culture with access to Linkedin Learning and professional development workshops, diversity charter groups, orientation program, career pathing opportunities, mentorship programs', 'Discover opportunities to leverage information from legacy databases into the implementation process to avoid inquiring for additional information from customers', 'Work/Life Balance:', 'Health & Wellness:', 'Apply feedback from customers and internal stakeholders on data import quality into previously developed extraction scripts', 'About ServiceTitan', ' Health & Wellness: company-paid medical/vision/dental/life insurance/disability, employer HSA contribution, free One Medical membership, care coordination support, 401(k) with company match, stipend for home office equipment/supplies, gym discounts, monthly cell phone stipend', '2-5 years of experience with SQL Server 2008/2012/2014/2016Advanced knowledge and experience in T-SQL, complex ETL tools and operations, and SSISGiven the experimental nature of this job, we will require very tight compliance when it comes to data - we need to focus on learningStrong analytical thinking skillsExpert level understanding of database and data model conceptsVertical SaaS experience is highly desirableResults and solution oriented - we want to know how we can win, not why we can’tAbility to work independently and cross functionally', 'Establish quality working relationships with internal stakeholders', 'Expert level understanding of database and data model concepts', 'Ability to work independently and cross functionally', 'Results and solution oriented - we want to know how we can win, not why we can’t', 'Contribute', 'Perks & Benefits', 'Map data', 'Map data from various legacy databases into the ServiceTitan platform, subsequently developing SQL scripts that will extract the information efficiently and accurately', ""To Be Successful In This Role, You'll Need"", '2-5 years of experience with SQL Server 2008/2012/2014/2016', 'Life at ServiceTitan ']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,CNA Insurance,"Chicago, IL",1 week ago,Be among the first 25 applicants,"['', 'Adheres to coding and documentation best practices', 'May perform additional duties as assigned.', 'Uses knowledge and expertise to influence the organization at every level towards best in class data science tools and techniques. ', 'Aggregates disparate structured and unstructured data to create meaningful datasets that drive analytical insights.', 'Unposting Date', 'May actively partner with other analytical teams across the organization and/or participate in special projects.', 'EEO Statement', 'Practical experience with version control, preferably Git.', 'Organization', 'Other Locations', 'Performs a combination of duties in accordance with departmental guidelines:', 'Job Posting', 'Job Summary', 'Solid project management, organization and planning skills with the ability to manage multiple projects effectively and lead teams.', ' Uses knowledge and expertise to influence the organization at every level towards best in class data science tools and techniques.  Supports projects as Data Science SME; Collaborates with stakeholders to help define data requirements. Effectively communicates with project stakeholders by presenting complex or technical concepts in business terms. Builds data pipelines and develop model features for Machine Learning projects.  Supports process automation by leading smaller scale efforts to convert existing processes to Google Cloud Platform. Aggregates disparate structured and unstructured data to create meaningful datasets that drive analytical insights. Adheres to data governance best practices by leveraging Alation to build data dictionaries for new and existing data sources. Maintains and expands capabilities for data testing, validation and quality control. Supports data modeling in Looker, our cloud-based business intelligence platform. Works independently, receiving minimal guidance; acts as a resource for colleagues with less experience by providing instruction, guidance, and advice. May actively partner with other analytical teams across the organization and/or participate in special projects. Adheres to coding and documentation best practices Responds to and fulfills ad hoc data requests. ', "" Bachelor's Degree in Business, Economics, Mathematics, Finance, Statistics, or related field. Typically up to two years of related work experience. "", 'Reporting Relationship', ""Bachelor's Degree in Business, Economics, Mathematics, Finance, Statistics, or related field."", 'Typically up to two years of related work experience.', 'Responds to and fulfills ad hoc data requests.', 'Maintains and expands capabilities for data testing, validation and quality control.', 'Solid analytical, critical thinking and problem solving skills to effectively resolve complex situations and issues.', ""Solid interpersonal, communication and presentation skills. Effectively interacts with all levels of CNA's internal and external business partners."", 'Experience with Linux servers on Google Cloud Platform or other cloud provider. ', "" Ability to recommend analytical solutions to business problems and execute. Strong SQL knowledge and proven experience working with relational databases. Experience using R, Python, SQL, and other business−related software. Experience with Linux servers on Google Cloud Platform or other cloud provider.  Practical experience with version control, preferably Git. Experience using Looker or other data visualization tools a plus. Solid knowledge of core functions of insurance companies and general insurance acumen Solid interpersonal, communication and presentation skills. Effectively interacts with all levels of CNA's internal and external business partners. Solid analytical, critical thinking and problem solving skills to effectively resolve complex situations and issues. Solid project management, organization and planning skills with the ability to manage multiple projects effectively and lead teams.  Ability to solve issues with a sense of urgency; utilizes and manages the available resources to make informed decisions and achieve superior results. "", 'Experience using R, Python, SQL, and other business−related software.', 'Primary Location', ' Ability to solve issues with a sense of urgency; utilizes and manages the available resources to make informed decisions and achieve superior results.', 'Adheres to data governance best practices by leveraging Alation to build data dictionaries for new and existing data sources.', 'Strong SQL knowledge and proven experience working with relational databases.', 'Experience using Looker or other data visualization tools a plus.', 'Education & Experience', 'Supervisory Position', 'Builds data pipelines and develop model features for Machine Learning projects. ', 'Supports projects as Data Science SME; Collaborates with stakeholders to help define data requirements.', 'Supports data modeling in Looker, our cloud-based business intelligence platform.', 'Effectively communicates with project stakeholders by presenting complex or technical concepts in business terms.', 'Job', 'Supports process automation by leading smaller scale efforts to convert existing processes to Google Cloud Platform.', 'Ability to recommend analytical solutions to business problems and execute.', 'Essential Duties & Responsibilities', 'Works independently, receiving minimal guidance; acts as a resource for colleagues with less experience by providing instruction, guidance, and advice.', 'Skills, Knowledge & Abilities', 'Solid knowledge of core functions of insurance companies and general insurance acumen']",Not Applicable,Full-time,Information Technology,Financial Services,2021-03-24 13:05:10
Data Engineer,Pozent Corporation,"Raritan, NJ",1 week ago,Be among the first 25 applicants,"['', ""A minimum of a Bachelor's Degree in EE/CS or related field from an accredited institution is required "", ' Design SQL Server database schema  Write Azure Data Factory pipelines Build CI/CD pipelines  Follow test-driven development  Perform database administration  Write stored procedures Automate data backup and recovery ', 'Experience with data modeling, design patterns, building highly scalable and secure applications is required ', 'Write stored procedures Automate data backup and recovery', 'Experience with Azure DevOps (pipelines) highly preferred ', 'Write Azure Data Factory pipelines', 'Perform database administration ', 'Can-do attitude and strong sense of ownership ', 'Build CI/CD pipelines ', 'Design SQL Server database schema ', 'Strong communication skills and attention to details', 'Candidates are strongly encouraged to send a link to their StackOverflow profile with their application.', 'Team player ', 'Follow test-driven development ', 'A minimum of five (5) years of industry experience in an established company is required ', "" A minimum of a Bachelor's Degree in EE/CS or related field from an accredited institution is required  A minimum of five (5) years of industry experience in an established company is required  Experience with data modeling, design patterns, building highly scalable and secure applications is required  Experience with MySQL or SQL Server, Docker, Unix, Java, Maven, Jenkins, Git, Azure Data Factory is required  Experience with Azure DevOps (pipelines) highly preferred  Can-do attitude and strong sense of ownership  Ability to work in an ambiguous environment and adapt to changing requirements  Strong communication skills and attention to details Team player  Candidates are strongly encouraged to send a link to their StackOverflow profile with their application."", 'Experience with MySQL or SQL Server, Docker, Unix, Java, Maven, Jenkins, Git, Azure Data Factory is required ', 'Ability to work in an ambiguous environment and adapt to changing requirements ']",Entry level,Contract,Information Technology,Information Technology and Services,2021-03-24 13:05:10
"Data Engineer, Business Intelligence",Essence,New York City Metropolitan Area,1 week ago,49 applicants,"['', 'Desirable', 'Support the development strong working relationships with third-party data providers that we rely on for access to necessary data', 'Analytically minded, enabling you to understand and overcome technically complex challenges, and to tell compelling stories with data', 'Experience using or building reports with business intelligence software, ideally Google DataStudio', 'As Data Engineer your primary responsibility will be to support a Senior Data Engineer to create and maintain underlying data infrastructure that provides the wider team with the data they need to provide timely, accurate and meaningful deliverables & reporting.\xa0 In doing so you will gain a foundational understanding of cloud technology and key data engineering skills and knowledge to help you build a career in this fast evolving, and in demand, industry.', 'Delivers best results when working in a team environment, and an ability to partner effectively with people of varying degrees of technical capability', 'Support other members of the team responsible for “last mile” transformation and visualization of data within Google Data Studio reports and dashboards', 'Provide hands-on support to users of reportion solutions, helping the wider team triage and respond to user queries in a timely manner', 'Assist the development of technical solutions, in line with specifications,\xa0 that collect, store and transform disparate data sourcesAssist the development and maintenance of automated jobs that ensure required data is made available in an efficient and scalable way as possibleAssist the development and maintenance of data quality checks and procedures, helping account teams overcome data issues before they impact the quality of the reporting solutionSupport the translation of user requirements and business needs into technical specificationsBecome a proficient user of Google Cloud Platform and associated technologies, ensuring your work incorporates industry best practiceMonitor automated jobs, troubleshooting data issues as-and-when they ariseSupport other members of the team responsible for “last mile” transformation and visualization of data within Google Data Studio reports and dashboardsProvide hands-on support to users of reportion solutions, helping the wider team triage and respond to user queries in a timely mannerAttend internal stakeholder meetings, presenting your solutions and providing updates on your work.Support the development strong working relationships with third-party data providers that we rely on for access to necessary data', 'Support the translation of user requirements and business needs into technical specifications', 'Required', 'Experience with programming and/or statistical languages (e.g. SQL, Python)', 'Monitor automated jobs, troubleshooting data issues as-and-when they arise', 'Experience with digital marketing platforms and the data they generate, in particular Google Marketing Platform, Facebook, Twitter etc.\xa0 Knowledge of their API’s a plus.', 'Assist the development and maintenance of data quality checks and procedures, helping account teams overcome data issues before they impact the quality of the reporting solution', 'An ability to demonstrate a passion for the digital marketing ecosystem, and an understanding of the role that data plays within it', 'Strong spoken and written communication skills, ensuring your thoughts and needs are heard and understood', 'related company (e.g. publisher, ad tech, client marketing org)', 'Attend internal stakeholder meetings, presenting your solutions and providing updates on your work.', 'Strong organizational skills and attention to detail, including the ability to manage multiple tasks in a fairly autonomous way', 'About Essence', 'A bit about yourself:', 'Previous experience working with data and technology', 'What you can expect from Essence', 'Assist the development of technical solutions, in line with specifications,\xa0 that collect, store and transform disparate data sources', 'This role forms part of a globally distributed business intelligence team, whose objective is to ensure that one of our most important global accounts have access to the right data and insights in order to inform their marketing decisions.\xa0\xa0', 'Work experience within a marketing organization, preferably at a media agency or\xa0', 'Become a proficient user of Google Cloud Platform and associated technologies, ensuring your work incorporates industry best practice', 'Experience building underlying data pipelines and ETL, particularly useful if done using Google Cloud Platform, Airflow, DBT etc.', 'An understanding of how data is tracked and exchanged in the process of digital advertising (e.g. role of ad servers and other third-party tech vendors)', 'Previous experience working with data and technologyExperience with programming and/or statistical languages (e.g. SQL, Python)Analytically minded, enabling you to understand and overcome technically complex challenges, and to tell compelling stories with dataStrong organizational skills and attention to detail, including the ability to manage multiple tasks in a fairly autonomous wayStrong spoken and written communication skills, ensuring your thoughts and needs are heard and understoodAn ability to demonstrate a passion for the digital marketing ecosystem, and an understanding of the role that data plays within itDelivers best results when working in a team environment, and an ability to partner effectively with people of varying degrees of technical capability', 'Experience building underlying data pipelines and ETL, particularly useful if done using Google Cloud Platform, Airflow, DBT etc.Experience with digital marketing platforms and the data they generate, in particular Google Marketing Platform, Facebook, Twitter etc.\xa0 Knowledge of their API’s a plus.An understanding of how data is tracked and exchanged in the process of digital advertising (e.g. role of ad servers and other third-party tech vendors)Experience using or building reports with business intelligence software, ideally Google DataStudioWork experience within a marketing organization, preferably at a media agency or\xa0', 'The team’s primary responsibility is to maintain a global Google Cloud based reporting solution, which automates the collection and transformation of disparate marketing data into a single source of truth.\xa0 Not only does that mean running and maintaining the solution that already exists, but also continually improving it to incorporate new data sources, and to derive new insights, to support ever-evolving business demands.', 'Assist the development and maintenance of automated jobs that ensure required data is made available in an efficient and scalable way as possible', 'The Role:', 'Some of the things we’d like you to do:', 'Visit essenceglobal.com for more information and follow us on Twitter at @essenceglobal.', 'Essence’s mission is to make advertising more valuable to the world.\xa0 We do this by employing the world’s very best talent to solve some of the toughest challenges of today’s digital marketing landscape.\xa0 It’s important that we hire people whose values reflect those of our own: genuine, results-focused, daring and insightful.\xa0 As an Essence employee, we promise you a workplace that invests in your career, cares for you and is fun and engaging.\xa0 We believe these factors create a workplace where you can be yourself and do amazing work.', ""Essence, part of GroupM, is a global data and measurement-driven media agency whose mission is to make brands more valuable to the world. Clients include Google, Flipkart, NBCUniversal, L'Oréal and the Financial Times. The agency is more than 2,000 people strong, manages $4.5B in annualized media spend, and deploys campaigns in 121 markets via 22 offices in APAC, EMEA and the Americas.\xa0""]",Mid-Senior level,Full-time,Engineering,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer,"M/I Homes, Inc.","Savage, MN",4 weeks ago,Be among the first 25 applicants,"['', 'Deploy and maintain system and database monitoring tools', 'Maintenance & Projects', 'Design, implement, and test data collection, storage, and mapping procedures', 'Excellent analytical and problem-solving skills', 'Continually look to improve and refine the IS data services delivered to the business, globally', 'Perform other related duties as assigned', 'Establish and maintain effective relationships with staff members, customers, and vendors', ' Design, architect, and document the database schema and data pipelines Design, implement, and test data collection, storage, and mapping procedures Research, design, and test computational environments to support analytical modeling (statistics, machine learning, optimization, and simulation) Develop statistical models for data quality assurance and data imputation Write, optimize, and automate data processing tasks Deploy and maintain system and database monitoring tools Perform other related duties as assigned ', ' Communicate effectively, verbally and in writing with people at all organizational levels Work in a team environment making positive contributions to the organization Establish and maintain effective relationships with staff members, customers, and vendors Other duties as assigned or required ', 'Develop statistical models for data quality assurance and data imputation', 'Research, design, and test computational environments to support analytical modeling (statistics, machine learning, optimization, and simulation)', 'Maintain central repository/code base for production code (e.g. GitHub)', 'Maintain technical documentation for solutions and standard operating procedures such that services are delivered in an efficient and effective manner', 'Help define and meet SLA requirements for analytics components', 'Familiarity with large-scale analytics systems', 'Design, architect, and document the database schema and data pipelines', 'Other duties as assigned or required', 'Contribute deep technical skills, and industry experience and best practices to the rest of the team, driving change', 'Advanced SQL development skills and an in-depth understanding of database technologies', 'Write, optimize, and automate data processing tasks', 'Minimum 5 years of experience in cloud-based ETL architecture/development, implementation and support', 'Systems Engineering & Administration', 'Minimum 5 years of experience with Python, Bash, and other scripting languages', 'Strong written and oral communication skills', 'Key Accountabilities Include', 'Communication & Collaboration', 'Maintain knowledge of emerging technology', 'Embrace agile methodology for iterative development', ' Advanced SQL development skills and an in-depth understanding of database technologies Minimum 5 years of experience with Python, Bash, and other scripting languages Experience with cloud infrastructure and distributed computing Familiarity with large-scale analytics systems Excellent analytical and problem-solving skills Strong written and oral communication skills ', 'Meet project and task deadlines', 'Service Development', 'Required Experience', ' B.S. or M.S. in computer science, computer engineering, or a related technical field Minimum 5 years of experience in cloud-based ETL architecture/development, implementation and support ', 'Creation and refinement of standards for the Data Practice, including data ingestion, data governance, request process, testing/QA, etc.', 'Manage multifaceted and technically sophisticated projects and on-going operations', 'Work in a team environment making positive contributions to the organization', 'Experience with cloud infrastructure and distributed computing', ' Creation and refinement of standards for the Data Practice, including data ingestion, data governance, request process, testing/QA, etc. Management and maintenance of the Data Analytics environment, Cloud integrated applications and application delivery infrastructure Maintain central repository/code base for production code (e.g. GitHub) Maintain technical documentation for solutions and standard operating procedures such that services are delivered in an efficient and effective manner Manage multifaceted and technically sophisticated projects and on-going operations Meet project and task deadlines ', 'Management and maintenance of the Data Analytics environment, Cloud integrated applications and application delivery infrastructure', 'Communicate effectively, verbally and in writing with people at all organizational levels', ' Continually look to improve and refine the IS data services delivered to the business, globally Maintain knowledge of emerging technology Help define and meet SLA requirements for analytics components Contribute deep technical skills, and industry experience and best practices to the rest of the team, driving change Embrace agile methodology for iterative development ', 'B.S. or M.S. in computer science, computer engineering, or a related technical field', 'Required Skills']",Entry level,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Data Engineer,Herman Miller,"Zeeland, MI",3 weeks ago,38 applicants,"['', ' Cleaning, normalizing, and aggregating IoT sensor telemetry data for Herman Miller Live Platform reporting and analytics. ', ' Architecting and developing in AWS Services like AWS Glue, Lambda, Step Functions, S3, EMR, Cloudwatch, CloudTrail, Lake Formation, Cloudformation, etc. ', 'Employee Status', 'Travel', ' Fully proficient data engineering abilities typically gained through 5+ years of working with Python/Java, SQL, and working schema design and dimensional data modeling. ', 'Who We Hire', '  Cleaning, normalizing, and aggregating IoT sensor telemetry data for Herman Miller Live Platform reporting and analytics.   Providing support to functional areas by acting as an expert in tools and methods for accessing, analyzing, and reporting corporate data.   Collaborating with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.   Designing data integrations and data quality framework.   Developing and maintains scalable data pipelines and builds out new API integrations to support continuing increases in data volume and complexity.   Implementing processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it.   Performing data analysis required to troubleshoot data related issues and assist in the resolution of data issues.  ', '  A Bachelor’s degree in Computer Science or a related technical field.   Fully proficient data engineering abilities typically gained through 5+ years of working with Python/Java, SQL, and working schema design and dimensional data modeling.   Experience designing, building and maintaining data processing systems.   Experience working with AWS cloud services and Redshift data warehouse.   Architecting and developing in AWS Services like AWS Glue, Lambda, Step Functions, S3, EMR, Cloudwatch, CloudTrail, Lake Formation, Cloudformation, etc.   Ability in managing and communicating date warehouse plans to internal clients.   Implementing and maintaining large date warehouse.   Experience in implementing ETL using one of the following languages: Python, Scala, or Java   Excellent oral and written communication skills with a keen of customer service.  ', 'Inside the Job', 'Work Schedule', ' Developing and maintains scalable data pipelines and builds out new API integrations to support continuing increases in data volume and complexity. ', ' Experience in implementing ETL using one of the following languages: Python, Scala, or Java ', ' Experience working with AWS cloud services and Redshift data warehouse. ', ' Performing data analysis required to troubleshoot data related issues and assist in the resolution of data issues. ', 'Your Day-to-day Work Will Involve', ' Excellent oral and written communication skills with a keen of customer service. ', 'Schedule', ' Providing support to functional areas by acting as an expert in tools and methods for accessing, analyzing, and reporting corporate data. ', 'Shift', ' Implementing processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it. ', 'What You Bring', ' Experience designing, building and maintaining data processing systems. ', 'Primary Location', 'Our Values', ' Ability in managing and communicating date warehouse plans to internal clients. ', ' A Bachelor’s degree in Computer Science or a related technical field. ', 'Why Join Us?', ' Designing data integrations and data quality framework. ', ' Collaborating with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization. ', ' Implementing and maintaining large date warehouse. ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Radancy,"Fayetteville, GA",1 week ago,Be among the first 25 applicants,"['', ' Build and maintain ETL pipelines utilizing Python that connect 1st and 3rd party data  Work with Cloud Computing Platforms (GCP/AWS), Luigi, Kafka and other open-source technologies  Conduct data modeling, schema design, and SQL development  Ingest and aggregate data from both internal and external data sources to build our world class datasets  Develop and lead the testing and fixing of new or enhanced solutions for data products and reports, including automating ETL testing  Collaborate with Product Owner and domain experts to recognize and help adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation  Assist with the development and review of technical and end user documentation including ETL workflows, research, and data analysis  Work with Product team to define data collection and engineering frameworks  Build monitoring dashboards and automate data quality testing  Responsible for daily integrity checks, performing deployments and releases  Own meaningful parts of our service, have an impact, grow with the company ', ' Build monitoring dashboards and automate data quality testing ', ' Exposure to front end development: HTML, JavaScript, jQuery, Angular or similar libraries ', ' Familiarity with C#, .Net, Kafka, Docker ', ' Exposure / familiarity with Google Cloud Platform / BigQuery / Amazon Redshift ', ' Collaborate with Product Owner and domain experts to recognize and help adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation ', ' Enthusiastic about working with and exploring new data sets ', ' The team has extensive experience in ETL development, works with large scale data in real time, and cross collaborates with other engineering teams across the organization. ', 'The Team', 'Overview', ' Work with Product team to define data collection and engineering frameworks ', 'Flexible Location (Remote Hubs): ', ' Build and maintain ETL pipelines utilizing Python that connect 1st and 3rd party data ', 'About The Job', 'Desired Technical Qualifications', ' Responsible for daily integrity checks, performing deployments and releases ', ' Work with Cloud Computing Platforms (GCP/AWS), Luigi, Kafka and other open-source technologies ', ' Product / reporting suite experience ', ' Assist with the development and review of technical and end user documentation including ETL workflows, research, and data analysis ', ' AdTech experience preferred ', ' Radancy Data Engineering works on data services across product organizations within Radancy, and supports building a customer facing data visualization product. The Data Engineering team supports an enterprise grade recruitment platform focusing on talent acquisition and job opportunity exploration.  The team has extensive experience in ETL development, works with large scale data in real time, and cross collaborates with other engineering teams across the organization. ', ' Bachelors or Masters degree in Computer Science or other related field ', ' Conduct data modeling, schema design, and SQL development ', ' Ingest and aggregate data from both internal and external data sources to build our world class datasets ', ' Detail oriented and strong communicator ', ' 2+ years of Python, SQL, and ETL development  Bachelors or Masters degree in Computer Science or other related field  Product / reporting suite experience  Familiarity with C#, .Net, Kafka, Docker  Exposure to front end development: HTML, JavaScript, jQuery, Angular or similar libraries  Exposure / familiarity with Google Cloud Platform / BigQuery / Amazon Redshift  AdTech experience preferred  Enthusiastic about working with and exploring new data sets  Detail oriented and strong communicator ', ' Radancy', ' Radancy Data Engineering works on data services across product organizations within Radancy, and supports building a customer facing data visualization product. The Data Engineering team supports an enterprise grade recruitment platform focusing on talent acquisition and job opportunity exploration. ', ' Own meaningful parts of our service, have an impact, grow with the company ', ' 2+ years of Python, SQL, and ETL development ', ' Develop and lead the testing and fixing of new or enhanced solutions for data products and reports, including automating ETL testing ']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer,"Conch Technologies, Inc","Salt Lake City, UT",3 days ago,Be among the first 25 applicants,"['', 'Qualifications For Data Engineer']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,TDECU,"Sugar Land, TX",3 weeks ago,Be among the first 25 applicants,"['', '\uf0b7 Ability to do data integrity and validation; and manage multiple priorities simultaneously.', 'Texas Dow Employees Credit Union is an equal opportunity employer, dedicated to a policy of non-discrimination in employment on any basis including race, color, age, protected veteran status, sex, religion, disability, genetic information, national origin or other status protected by federal, state or local law. Consistent with the American Disabilities Act, applicants may request accommodations needed to participate in the application process.', '\uf0b7 Experience in Team Foundation Server or other source control system.', '\uf0a7 A minimum of 5 years experience in integration services, reporting services, business documentation, and T-SQL systems.', '\uf0b7 In depth knowledge of SISS and its capabilities.', 'Specific vision abilities required by this position include close vision, distance vision, and the ability to adjust focus. The noise level in the work environment is usually moderate.', '\uf0a7 Experience in a financial institution preferred', 'Minimum Qualifications: ', '\uf0b7 Competent with MS Office (Word, Excel, PowerPoint, Access, Visio)', '\uf0b7 Ability to work both independently and within teams.', '\uf0a7 A minimum of 3 years experience in Analysis Services.\uf0a7 Experience in a financial institution preferredKnowledge, Skills, and Abilities \uf0b7 Competent with MS Office (Word, Excel, PowerPoint, Access, Visio)\uf0b7 Working knowledge of Microsoft Business Intelligence stack (Integration Services, Reporting Services, and Analysis Services)\uf0b7 C# knowledge for SSIS scripting\uf0b7 In depth knowledge of SISS and its capabilities.\uf0b7 Knowledge of T-SQL and ANSI SQL stored procedures with multiple parameters.\uf0b7 Ability to perform business analysis and documentation.\uf0b7 Experience in Team Foundation Server or other source control system.\uf0b7 Ability to do data integrity and validation; and manage multiple priorities simultaneously.\uf0b7 Excellent written and oral communication skills to Identifies, researches, and resolves technical problem.\uf0b7 Ability to work both independently and within teams.', 'Completes all mandatory compliance testing within designated time frame and other compliance assignments including Office of Foreign Assets Control (OFAC), Unusual Activity Reports (UAR), and Security processes.', '(The physical demands and work environment characteristics described herein are representative of those that must be met by an employee to successfully perform essential functions of this position and/or may be encountered while performing essential functions. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.) ', 'Position Summary: ', 'The Business Intelligence Specialist role will be responsible for identify business intelligence, reporting, and data analysis needs. They will work closely with end-users and business units to turn data into critical information and knowledge that can be used to make sound business decisions.', 'Analysis Services: Design and implement multidimensional mode database and Power BI mode data model including stand alone and through SharePoint.', 'Reporting Services:', '\uf0a7 A minimum of 3 years experience in Analysis Services.', '\ufeffEssential Duties and Responsibilities: ', '\uf0b7 Working knowledge of Microsoft Business Intelligence stack (Integration Services, Reporting Services, and Analysis Services)', '\uf0b7 C# knowledge for SSIS scripting', '(Education, Experience, Knowledge, Skills, and Abilities) ', '(Education, Experience, Knowledge, Skills, and Abilities) Essential education: \uf0b7 Bachelor’s degree required or equivalent years of experience.\uf0a7 A minimum of 5 years experience in integration services, reporting services, business documentation, and T-SQL systems.', 'Integration Services ETL (Extract, Transform, Load): Responsible for design, testing, and implementation of ETL processes for delivering data extracts and final data reports as well as loading fact and dimension tables in the data warehouse. Monitors integration services database to determine job status, troubleshoot, and resolve any issues that occur while running ETL jobs. Responsible for Data Warehouse Star Schema design.', '\uf0b7 Ability to perform business analysis and documentation.', 'Knowledge, Skills, and Abilities ', 'Essential education: ', 'Integration Services ETL (Extract, Transform, Load):', 'Disclaimer ', 'Reporting Services: Develop and deploy reports to the self-service portal. Maintain folder level security based on active directory groups. High level summary and self-service reporting and dashboards. Reporting against, multiple data warehouses, source systems (SQL Server and Oracle), and SSAS database in multidimensional mode and Power BI mode.', 'The above statements are intended to describe the general nature and level of work being performed by people assigned to this job. They are not intended to be an exhaustive list of all responsibilities, duties, and skills required of personnel so classified. ', 'Implements processes as required by the Bank Secrecy Act and TDECU policies and procedures including but not limited to completing and submitting Currency Transaction Reports, being knowledgeable of signs of unusual financial activities, and reporting signs of unusual activity and/or completing reporting for Suspicious Activity Report as appropriate per procedure.', 'Stays abreast of changes within the credit union relating to services and procedures.', '(The physical demands and work environment characteristics described herein are representative of those that must be met by an employee to successfully perform essential functions of this position and/or may be encountered while performing essential functions. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.) While performing the essential duties of this position, an employee would frequently be required to stand, walk, and sit. An employee must frequently lift and/or move up to 10 pounds and may occasionally lift and/or move up to 25 pounds to perform essential position functions.Specific vision abilities required by this position include close vision, distance vision, and the ability to adjust focus. The noise level in the work environment is usually moderate.', 'Physical Demands and Work Environment: ', 'Integration Services ETL (Extract, Transform, Load): Responsible for design, testing, and implementation of ETL processes for delivering data extracts and final data reports as well as loading fact and dimension tables in the data warehouse. Monitors integration services database to determine job status, troubleshoot, and resolve any issues that occur while running ETL jobs. Responsible for Data Warehouse Star Schema design.Reporting Services: Develop and deploy reports to the self-service portal. Maintain folder level security based on active directory groups. High level summary and self-service reporting and dashboards. Reporting against, multiple data warehouses, source systems (SQL Server and Oracle), and SSAS database in multidimensional mode and Power BI mode.Business Requirements Documentation: Responsible for ticketing system report requests with detail from all the various data sources, requirements, and time estimates. Maintain DBA Tools database with reports and extract information as well as Integration Services packages. Work with project management teams for documentation details. Technical report documentation of data extracts, automation processes, and data analytics.Analysis Services: Design and implement multidimensional mode database and Power BI mode data model including stand alone and through SharePoint.Completes all mandatory compliance testing within designated time frame and other compliance assignments including Office of Foreign Assets Control (OFAC), Unusual Activity Reports (UAR), and Security processes.Implements processes as required by the Bank Secrecy Act and TDECU policies and procedures including but not limited to completing and submitting Currency Transaction Reports, being knowledgeable of signs of unusual financial activities, and reporting signs of unusual activity and/or completing reporting for Suspicious Activity Report as appropriate per procedure.Stays abreast of changes within the credit union relating to services and procedures.', 'Analysis Services', '\uf0b7 Excellent written and oral communication skills to Identifies, researches, and resolves technical problem.', '\uf0b7 Bachelor’s degree required or equivalent years of experience.', 'Business Requirements Documentation', 'While performing the essential duties of this position, an employee would frequently be required to stand, walk, and sit. An employee must frequently lift and/or move up to 10 pounds and may occasionally lift and/or move up to 25 pounds to perform essential position functions.', 'Job Description ', '\uf0b7 Knowledge of T-SQL and ANSI SQL stored procedures with multiple parameters.', 'Business Requirements Documentation: Responsible for ticketing system report requests with detail from all the various data sources, requirements, and time estimates. Maintain DBA Tools database with reports and extract information as well as Integration Services packages. Work with project management teams for documentation details. Technical report documentation of data extracts, automation processes, and data analytics.', 'The above statements are intended to describe the general nature and level of work being performed by people assigned to this job. They are not intended to be an exhaustive list of all responsibilities, duties, and skills required of personnel so classified. Texas Dow Employees Credit Union is an equal opportunity employer, dedicated to a policy of non-discrimination in employment on any basis including race, color, age, protected veteran status, sex, religion, disability, genetic information, national origin or other status protected by federal, state or local law. Consistent with the American Disabilities Act, applicants may request accommodations needed to participate in the application process.']",Associate,Full-time,Information Technology,Financial Services,2021-03-24 13:05:10
Data Engineer,Charles River Laboratories,"Wilmington, MA",6 days ago,Be among the first 25 applicants,"['', ' Develops technical solutions to complex problems which require the regular use of ingenuity and creativity.', ' Experience with setting up and operating data pipelines using Python or SQL', ' Bachelor’s degree in Computer Engineering, Computer Science or related discipline, Master’s Degree preferred 7+ years of ETL design, development, and performance tuning on Microsoft SSIS in SQL Server 2012 and above (2016 preferable) in a multi-dimensional Data Warehousing environment 7+ years of SSAS design, development, maintenance and performance tuning on Microsoft SQL Server 2012 and above (2016 preferable), with expert MDX and DAX skills 7+ years of advanced SQL Programming: PL/SQL, T-SQL, U-SQL 5+ years of Enterprise Data & Analytics solution architecture 2+ years of Power BI experience including mobile solutions 2+ years of strong and extensive hands on experience in Azure, preferably data heavy / analytics applications leveraging relational and NoSQL databases, Data Warehouse and Big Data', 'About Charles River', ' Interacts with senior level customers to consult on scalable enterprise analytics solutions that provide the business with a competitive advantage. Develops technical solutions to complex problems which require the regular use of ingenuity and creativity. Design and build a modern data warehouse in the cloud Design and build a customer 360 Implement a flexible and audible data pipeline Enhance data collection procedures to build analytic systems. Process, cleanse, and verify the integrity of data used for analysis. Perform ad-hoc analysis and present results in a clear and user friendly manner Perform testing, resolve issues and automate unit tests. Develop proof-of-concept (POC) solutions to help business units better visualize their business needs and to clarify requirements for development.', ' Design and build a customer 360', ' Interacts with senior level customers to consult on scalable enterprise analytics solutions that provide the business with a competitive advantage.', ' 2+ years of strong and extensive hands on experience in Azure, preferably data heavy / analytics applications leveraging relational and NoSQL databases, Data Warehouse and Big Data', 'IMPORTANT: A resume is required to be considered for this position.', ' Perform testing, resolve issues and automate unit tests.', 'Job Summary', ' Experience with Azure Data Lake, Azure SQL Data Warehouse, Data Catalog, Azure Analysis Services, Data Bricks, Storage Account Gen2, Azure SQL Database, Azure DNS, Virtual Network, DocumentDB, Azure App Service, Data Factory', ' Bachelor’s degree in Computer Engineering, Computer Science or related discipline, Master’s Degree preferred', ' 7+ years of ETL design, development, and performance tuning on Microsoft SSIS in SQL Server 2012 and above (2016 preferable) in a multi-dimensional Data Warehousing environment', 'a resume/CV must be uploaded and submitted during the application process. ', ' 7+ years of advanced SQL Programming: PL/SQL, T-SQL, U-SQL', ' 2+ years of Power BI experience including mobile solutions', 'About Corporate Functions', ' Experience with Big Data Technologies such as: Hadoop, Sqoop, Hive, Kafka, Spark, Pyspark, Python, Scala or Pig', ' 5+ years of Enterprise Data & Analytics solution architecture', ' Enhance data collection procedures to build analytic systems.', 'Equal Employment Opportunity', ' 7+ years of SSAS design, development, maintenance and performance tuning on Microsoft SQL Server 2012 and above (2016 preferable), with expert MDX and DAX skills', ' Perform ad-hoc analysis and present results in a clear and user friendly manner', 'The Following Are Strongly Desired', ' Design and build a modern data warehouse in the cloud', ' Strong analytical abilities and a strong intellectual curiosity', ' Develop proof-of-concept (POC) solutions to help business units better visualize their business needs and to clarify requirements for development.', ' Experience with Azure Data Lake, Azure SQL Data Warehouse, Data Catalog, Azure Analysis Services, Data Bricks, Storage Account Gen2, Azure SQL Database, Azure DNS, Virtual Network, DocumentDB, Azure App Service, Data Factory Experience with Big Data Technologies such as: Hadoop, Sqoop, Hive, Kafka, Spark, Pyspark, Python, Scala or Pig Experience with Big Data Management (BDM) for relational and non-relational data (formats like json, xml, avro, parquet, copybook, etc.) Experience with setting up and operating data pipelines using Python or SQL Strong analytical abilities and a strong intellectual curiosity', ' Process, cleanse, and verify the integrity of data used for analysis.', 'Job Qualifications', ' Experience with Big Data Management (BDM) for relational and non-relational data (formats like json, xml, avro, parquet, copybook, etc.)', 'IMPORTANT:', ' Implement a flexible and audible data pipeline']",Entry level,Full-time,Information Technology,Biotechnology,2021-03-24 13:05:10
Data Engineer - Remote - 125K,Jefferson Frank,"Fairfax, VT",1 week ago,Be among the first 25 applicants,"['', 'Skills', ' 135 employees', ' HQ in Fairfax, VA. They are remote now but want the candidates in-office once they return', ' Up to 10% bonus for each role, to be paid out quarterly', ' 4+ yrs exp AWS experience required (Redshift required) Extensive Python exp Experience creating Data Pipelines & Data Lakes Data experience in Hadoop, Spark, HIVE Kubernetes/Kubernetes tools like Helm (Docker experience is OK) SQL, (Postgres, MySQL) APIs exp a plus', ' Never stopped hiring during COVID-19, very stable', ' Up to 10% bonus for each role, to be paid out quarterly 80% health, dental & vision coverage for individual AND families 401k - 100% match up to 6%. Fully vested after first year Unlimited sick leave, 3 weeks PTO', 'About My Client', ' SQL, (Postgres, MySQL)', ' 135 employees HQ in Fairfax, VA. They are remote now but want the candidates in-office once they return They are a health technology company that uses evidence-based data to solve health challenges. They come up with research-related products that can be utilized in the health care field employees', ' Data experience in Hadoop, Spark, HIVE', ' Unlimited sick leave, 3 weeks PTO', ' Kubernetes/Kubernetes tools like Helm (Docker experience is OK)', ' Solid mission, VERY strong funding Never stopped hiring during COVID-19, very stable Really strong benefits', ' 4+ yrs exp', ' APIs exp a plus', ' 401k - 100% match up to 6%. Fully vested after first year', ' AWS experience required (Redshift required)', ' They are a health technology company that uses evidence-based data to solve health challenges. They come up with research-related products that can be utilized in the health care field employees', ' Solid mission, VERY strong funding', ' Really strong benefits', ' Experience creating Data Pipelines & Data Lakes', ' 80% health, dental & vision coverage for individual AND families', 'Benefits', ' Extensive Python exp']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Raymond James,"St Petersburg, FL",7 days ago,Be among the first 25 applicants,"['', 'Performs Other Duties And Responsibilities As Assigned.', 'Travel', 'Requirements', ' Contribute to the continuous evolution of the firm', ' Work with and through others to achieve desired outcomes', 'Job', 'Primary Location', 'Schedule', 'Duties', 'Shift', 'We Expect Our Associates At All Levels To', ' Grow professionally and inspire others to do the same Work with and through others to achieve desired outcomes Make prompt, pragmatic choices and act with the client in mind Take ownership and hold themselves and others accountable for delivering results that matter Contribute to the continuous evolution of the firm', 'Organization', ' Take ownership and hold themselves and others accountable for delivering results that matter', ' Grow professionally and inspire others to do the same', ' Make prompt, pragmatic choices and act with the client in mind', 'Raymond James Guiding Behaviors']",Not Applicable,Full-time,Information Technology,Banking,2021-03-24 13:05:10
Data Engineer,Comtech LLC,"San Jose, CA",19 hours ago,Be among the first 25 applicants,"['', 'Identify opportunities to improve productivity via sophisticated statistical modeling', 'Type: Contract', 'Work closely with our engineering team to integrate and build algorithms', 'Extract data from a variety of relational databases, manipulate, explore data using quantitative, statistical and visualization tools', 'Perform statistical analysis to prioritize in order to maximize success', '8+ years exp in Java, Python and Scala', 'Big Data experience', 'Type: ContractFunction: Information Technology', 'Gather and process raw data at scale (including writing scripts, web scraping, calling APIs, write SQL queries, etc.).Work closely with our engineering team to integrate and build algorithmsProcess unstructured data into a form suitable for analysis – and then do the analysis.Support business decisions with ad hoc analysis as neededExtract data from a variety of relational databases, manipulate, explore data using quantitative, statistical and visualization toolsInform the selection of appropriate modeling techniques to ensure that predictive models are developed using rigorous statistical processesEstablish and maintain effective processes for validating and updating predictive modelsAnalyze, model, and forecast health service utilization patterns/ trends and create capability to model outcomes of what-if scenarios for novel health care delivery modelsCollaborate with internal business, analytics and data strategy partners to improve efficiency and increase applicability of predictive models into the core software productsPerform statistical analysis to prioritize in order to maximize successIdentify areas for improvement, communicating action plansPerform strategic data analysis and research to support business needsIdentify opportunities to improve productivity via sophisticated statistical modelingExplore data to identify opportunities to improve business resultsDevelop understanding of business processes, goals and strategy in order to provide analysis and interpretation to managementGain understanding of business needs and necessary analysis where appropriate through internal discussion', 'Roles And Responsibilities', 'With Spark and Machine Learning (3+)', 'Gather and process raw data at scale (including writing scripts, web scraping, calling APIs, write SQL queries, etc.).', 'Develop understanding of business processes, goals and strategy in order to provide analysis and interpretation to management', 'Information Technology', 'Analyze, model, and forecast health service utilization patterns/ trends and create capability to model outcomes of what-if scenarios for novel health care delivery models', 'Identify areas for improvement, communicating action plans', 'Big Data experience8+ years exp in Java, Python and ScalaWith Spark and Machine Learning (3+)Data mining, Data analysis', 'Collaborate with internal business, analytics and data strategy partners to improve efficiency and increase applicability of predictive models into the core software products', 'Establish and maintain effective processes for validating and updating predictive models', 'Process unstructured data into a form suitable for analysis – and then do the analysis.', 'Inform the selection of appropriate modeling techniques to ensure that predictive models are developed using rigorous statistical processes', 'Summary', 'Explore data to identify opportunities to improve business results', 'Qualifications', 'Perform strategic data analysis and research to support business needs', 'Function: Information Technology', 'Data mining, Data analysis', 'Support business decisions with ad hoc analysis as needed', 'Gain understanding of business needs and necessary analysis where appropriate through internal discussion', 'Company Description', 'Primary Skills', 'Contract']",Entry level,Contract,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer/Architect,The Select Group,"Raleigh, NC",1 week ago,Be among the first 25 applicants,"['', 'MySQL database experience doing database administration and engineering work Setting up ETL pipelines and defining ETL processes  ', 'Nice-to-have Skills', ' MySQL database experience doing database administration and engineering work Setting up ETL pipelines and defining ETL processes   Deep understanding of data orchestration, ideally with Apache Airflow Basic Python scripting experience   Understanding of at least one NoSQL database, such as MongoDB, DynamoDB, or AWS Redshift Working in a Cloud Environment, ideally AWS Experience working in the healthcare industry ', ' Setting up ETL pipelines and defining ETL processes ', 'Mentoring other associates in architecture, design and development best practices', 'Daily Responsibilities', 'Experience working on open-source technologies, ideally working on Java and JavaScript platforms', 'Provide technical direction to data engineering team while collaborating with client partners and business analysts to manage client relationships', 'The Principal Data Engineer will be responsible for developing the data warehouse solutions in SQL Server and Hadoop platform. The candidate will participate in all phases of the development lifecycle from initial requirements gathering and design through to coding and testing of our data warehousing solutions.', 'Experience working in the healthcare industry', 'Basic Python scripting experience', ' The Principal Data Engineer will be responsible for developing the data warehouse solutions in SQL Server and Hadoop platform. The candidate will participate in all phases of the development lifecycle from initial requirements gathering and design through to coding and testing of our data warehousing solutions. Provide technical direction to data engineering team while collaborating with client partners and business analysts to manage client relationships Coordinate software design needs between teams and management to meet development benchmarks Help to establish thought leadership in data warehousing space and lead in open source transition Design and develop scripts, code and ETL pipelines to leverage client data Participate in requirements and design Mentoring other associates in architecture, design and development best practices', 'Understanding of at least one NoSQL database, such as MongoDB, DynamoDB, or AWS Redshift', 'Setting up ETL pipelines and defining ETL processes', 'Deep understanding of data orchestration, ideally with Apache Airflow Basic Python scripting experience  ', 'Coordinate software design needs between teams and management to meet development benchmarks', 'Help to establish thought leadership in data warehousing space and lead in open source transition', 'Design and develop scripts, code and ETL pipelines to leverage client data', 'DATA ENGINEER / ARCHITECT – REMOTE ', ' Experience working on open-source technologies, ideally working on Java and JavaScript platforms ', ' L', 'Working in a Cloud Environment, ideally AWS', ' Basic Python scripting experience ', 'Must-have Skills', 'Participate in requirements and design']",Not Applicable,Full-time,Information Technology,Computer Networking,2021-03-24 13:05:10
Data Engineer - ETL,Sayari Labs,United States,4 weeks ago,106 applicants,"['', 'Strong experience with any two of Python, Java, Scala2+ years of experience designing and maintaining ETL pipelinesExperience using Apache SparkExperience with data orchestration frameworks like Apache AirflowSolid experience working with multiple databases, for example, Cassandra, Neo4J, or ElasticsearchExperience working on a cloud platform like GCP, AWS, or AzureExperience working collaboratively with git', 'Optional: Brief note to highlight relevant experience or skills.\xa0\xa0', 'Experience with, or interest in, graph databasesExperience with Docker and Kubernetes', 'Experience with Docker and Kubernetes', 'Strong process-oriented self-starter, with impeccable organizational skillsExperienced in supporting and working with cross-functional teams in a dynamic environmentInterested in learning from and mentoring team members\xa0Passionate about open source development and innovative technology', 'Sayari is a venture-backed and founder-led global corporate data provider and commercial intelligence platform, serving financial institutions, legal and advisory service providers, multinationals, journalists, and governments. Thousands of analysts and investigators in over 30 countries rely on our products to safely conduct cross-border trade, research front-page news stories, confidently enter new markets, and prevent financial crimes such as corruption and money laundering.', 'We are continually looking to expand our tech stack with new and innovative technologies.\xa0 Have an idea for something cutting edge?\xa0 Come talk to us about it.', 'A collaborative and positive culture - your team will be as smart and driven as you', 'Resume & any salary requirement', '2+ years of experience designing and maintaining ETL pipelines', 'How to Apply:\xa0', 'What You Will Need:', 'What We Would Like:', 'Limitless growth and learning opportunities\xa0A collaborative and positive culture - your team will be as smart and driven as youA strong commitment to diversity, equity & inclusion\xa0Outstanding competitive compensation & comprehensive benefits package, including full healthcare coverage plans, commuter benefits, 401K matching, generous vacation, and a variety of other benefits.\xa0', 'Position Description:', 'Experience with data orchestration frameworks like Apache Airflow', 'We at Sayari define our culture by a dedication to our mission of using open data to prevent illicit commercial and financial activity, a passion for finding novel approaches to complex problems, and an understanding that diverse perspectives create optimal outcomes. We embrace cross-team collaboration, open-source work, encourage training and learning opportunities, and reward initiative and innovation. If you like working with supportive, high-performing, and curious teams, Sayari is the place for you.\xa0', 'Who You Are:', 'Experience working on a cloud platform like GCP, AWS, or Azure', 'Experienced in supporting and working with cross-functional teams in a dynamic environment', 'Interested in learning from and mentoring team members\xa0', 'What We Offer:\xa0', 'Outstanding competitive compensation & comprehensive benefits package, including full healthcare coverage plans, commuter benefits, 401K matching, generous vacation, and a variety of other benefits.\xa0', 'Optional: Share links to any public repos of your previous work.\xa0', 'Sayari maintains a graph of global corporate information containing hundreds of millions of entities and relationships. The structures and techniques that we use to store the information in this graph are critical to allowing our clients to benefit from this unique dataset. The application that serves this graph to our clients is powered by several databases fed by ETL pipelines that run in Apache Spark.', 'What You Will Do:\xa0', 'Experience using Apache Spark', 'Strong experience with any two of Python, Java, Scala', 'Experience with, or interest in, graph databases', 'Strong process-oriented self-starter, with impeccable organizational skills', 'Limitless growth and learning opportunities\xa0', 'A strong commitment to diversity, equity & inclusion\xa0', 'Solid experience working with multiple databases, for example, Cassandra, Neo4J, or Elasticsearch', 'Resume & any salary requirementOptional: Brief note to highlight relevant experience or skills.\xa0\xa0Optional: Share links to any public repos of your previous work.\xa0', 'Sayari is looking for a mid-level to Senior Data Engineer to join our Infrastructure team located in Washington, DC. The Infrastructure team is an integral part of our Engineering division and works closely with our Software Engineering & Data Science teams, as well as other key stakeholders across the business.', ""As a member of Sayari's engineering team, you will work to maintain existing ETL pipelines and add additional pipelines to implement new features. These pipelines terminate in several different databases including Apache Cassandra, Elasticsearch, Postgres, and Tigergraph (a cutting edge in-memory graph database). In addition to developing pipelines, you'll contribute to the design of the database schemas to ensure that data can be efficiently retrieved by the application.\xa0"", 'About Sayari Labs:', 'What We Offer:', 'Sayari is an equal opportunity employer and strongly encourages diverse candidates to apply. We believe diversity and inclusion mean our team members should reflect the diversity of the United States. No employee or applicant will face discrimination or harassment based on race, color, ethnicity, religion, age, gender, gender identity or expression, sexual orientation, disability status, veteran status, genetics, or political affiliation. We strongly encourage applicants of all backgrounds to apply.', 'To apply, please email the documents listed below to HR@sayarianalytics.com\xa0', 'Passionate about open source development and innovative technology', 'Experience working collaboratively with git']",Associate,Full-time,Engineering,Information Services,2021-03-24 13:05:10
Data Engineer (Remote),funded.club,"Buffalo, NY",6 days ago,Be among the first 25 applicants,"['', 'Competencies:', 'Experience & Qualifications:', 'Experience with cloud environments, serverless architecture, SQL/NoSQL databases, containers (Docker)', ' Proficiency working with a modern programming language (e.g. Javascript or Python) Proven expertise in developing cloud-based, scalable software applications and services Experience managing large datasets and ETL pipelines. Experience deploying production applications to a public cloud (bonus points for AWS) Experience with cloud environments, serverless architecture, SQL/NoSQL databases, containers (Docker) Knowledgeable in developing and consuming APIs and micro-services to facilitate data access, business logic, and workflows Experience building and deploying end-to-end systems using either in-house infrastructure or cloud ML environments e.g. SageMaker Experience setting up and optimizing DBs for production usage for ML systems US citizen or permanent resident required ', 'Experience setting up and optimizing DBs for production usage for ML systems', 'Enhancing applications by identifying opportunities for improvement, making recommendations and designing and implementing systems', 'Short/Long Term Disability', 'Our commitment to inclusion across race, gender, age, religion, identity, and experience drives us forward every day', 'Identify ways to improve data reliability, efficiency and quality', ' Experience with Python, Javascript Experience with Postgres, DynamoDB, and ElasticSearch Experience with AWS and infrastructure as code Experience with Git and GitHub: Branching, merging, pull requests, etc… ', 'Solid understanding of computer science fundamentals (theory, data structures, algorithms).', 'Benefits:', 'Responsibilities:', "" Collaborating with different teams engineers to implement solutions that align with and heighten the DigitalClone Platform and solutions Identify ways to improve data reliability, efficiency and quality Prepare data for modeling Build and maintain Sentient Science's datasets and ETL processes Enhancing applications by identifying opportunities for improvement, making recommendations and designing and implementing systems Maintaining and improving existing codebases and peer review code changes Assisting with process optimization Develop automated build and deployment pipelines for your services Participating in post-implementation and system maintenance support Develop / integrate leading edge tools and frameworks to build and deploy scalable and efficient machine learning solutions "", 'Participating in post-implementation and system maintenance support', 'Experience building and deploying end-to-end systems using either in-house infrastructure or cloud ML environments e.g. SageMaker', 'Proven expertise in developing cloud-based, scalable software applications and services', 'US citizen or permanent resident required', 'Excellent relationship skills and ability to work cross-functionally Good professionalism / customer interaction skills', 'Experience managing large datasets and ETL pipelines.', 'Dental', 'Experience deploying production applications to a public cloud (bonus points for AWS)', 'Medical', 'Maintaining and improving existing codebases and peer review code changes', 'Experience with Git and GitHub: Branching, merging, pull requests, etc…', 'The role:', 'Develop / integrate leading edge tools and frameworks to build and deploy scalable and efficient machine learning solutions', 'Collaborating with different teams engineers to implement solutions that align with and heighten the DigitalClone Platform and solutions', 'Assisting with process optimization', 'Knowledgeable in developing and consuming APIs and micro-services to facilitate data access, business logic, and workflows', 'Proficiency working with a modern programming language (e.g. Javascript or Python)', 'Ability to work independently and as part of a team', ' Medical Dental 401K Short/Long Term Disability Life/AD&D insurance Unlimited Paid Time Off Flexible working hours and fully remote option available Our commitment to inclusion across race, gender, age, religion, identity, and experience drives us forward every day', 'Prepare data for modeling', 'Excellent written/oral communication and interpersonal skills are essential', '401K', 'Experience with Python, Javascript', 'Develop automated build and deployment pipelines for your services', 'Life/AD&D insurance', 'Preferred experience:', ' Solid understanding of computer science fundamentals (theory, data structures, algorithms). Solid understanding of data engineering fundamentals (big data, ETL pipelines) Ability to work in a fast-paced environment with a demonstrated ability to accomplish multiple task and meet deadlines Excellent written/oral communication and interpersonal skills are essential Excellent relationship skills and ability to work cross-functionally Good professionalism / customer interaction skills Strong attention to detail Ability to work independently and as part of a team ', 'Unlimited Paid Time Off', 'Strong attention to detail', 'Flexible working hours and fully remote option available', 'Experience with Postgres, DynamoDB, and ElasticSearch', ""Build and maintain Sentient Science's datasets and ETL processes"", 'Ability to work in a fast-paced environment with a demonstrated ability to accomplish multiple task and meet deadlines', 'Solid understanding of data engineering fundamentals (big data, ETL pipelines)', 'Experience with AWS and infrastructure as code']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,ANTENNA,"New York, NY",4 weeks ago,73 applicants,"['', ' Meaningful equity compensation Competitive base salary Full healthcare benefits Unlimited PTO 401K Wellness stipend Workstation stipend + work from anywhere And more!', 'Competitive base salary', 'Proficiency in Kubernetes, Spark, and Google Cloud Platform.', 'Proficiency in Python / Django / PostgreSQL.', 'About ANTENNA', 'Unlimited PTO', 'Quick to pick up new technologies.', 'You love data – and enjoy pushing the boundaries of what insights can be gleaned from it.', 'Create reliable data pipelines and build intuitive data products that allow our Data Analysts and Data Scientists to easily leverage data in a self-service manner.', 'Strong in written and verbal communication.', 'Basic front end knowledge (e.g. HTML, JavaScript)', 'Maintain an efficient data infrastructure.', 'Full healthcare benefits', 'Workstation stipend + work from anywhere', 'Integrate with a variety of data sources such as email receipt data, credit card data, and more.', 'Wellness stipend', 'Ensure data quality and build tools to detect and alert our team of any anomalies.', ' You love data – and enjoy pushing the boundaries of what insights can be gleaned from it. Proficiency in Python / Django / PostgreSQL. Proficiency in Kubernetes, Spark, and Google Cloud Platform. Basic front end knowledge (e.g. HTML, JavaScript) Strong background in distributed data processing, software engineering design, and data modeling concepts. :3+ years of experience building complex data pipelines and products. Quick to pick up new technologies. Strong in written and verbal communication. ', ""ANTENNA's Benefits"", ':3+ years of experience building complex data pipelines and products.', 'Strong background in distributed data processing, software engineering design, and data modeling concepts.', 'Meaningful equity compensation', '401K', 'And more!', ""At ANTENNA, you'll work hand-in-hand with the world's leading consumer subscription businesses to help build incredible, sustainable companies."", ' Create reliable data pipelines and build intuitive data products that allow our Data Analysts and Data Scientists to easily leverage data in a self-service manner. Design, build, and deploy streaming and batch data pipelines capable of processing and storing data quickly and reliably. Integrate with a variety of data sources such as email receipt data, credit card data, and more. Ensure data quality and build tools to detect and alert our team of any anomalies. Maintain an efficient data infrastructure. ', 'Design, build, and deploy streaming and batch data pipelines capable of processing and storing data quickly and reliably.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Haystax,"McLean, VA",1 week ago,Be among the first 25 applicants,"['', 'Manage customer expectations and keep project deliverables on schedule.', 'Desire understanding of AWS infrastructure – Command Line Interface (AWS CLI), S3, AWS Console.', 'A good level of experience and understanding of alerts and data sets from multiple security controls is a plus.Desire understanding of AWS infrastructure – Command Line Interface (AWS CLI), S3, AWS Console.Prior knowledge of Elasticsearch DSL language is a plus.Fundamental understanding of Bayesian statistics is a plus.Willing to dive deeper into development of customized solutions using programming language such as Python.A desire to dive deeper into full-stack development is a plus', ""Bachelor's degree in data science, data analytics or computer science"", 'Experience at working both independently and in a fast-paced, team-oriented environment.', 'Requirements:', 'Collaborate with the data analytics and customer success teams to prioritize business related tasks.', 'Perform data analytics on data sources specifically focused on Haystax Technology insider threat model.Perform the extraction, ingestion, augmentation and aggregation of data from multiple cyber and non-cyber data sources.Interpret customer data, analyze and tune the results using various statistical analysis techniques and machine learning algorithms.Analyze data sources and data flows working with both structured and unstructured data.Assist in the development of cloud and automation solutions to improve ongoing tasks.Work with the data analytics team to test and improve Haystax Technology models.Maintain day-to-day relationships with your technical counterparts at the existing projects.Monitor ongoing performance of platform integrations, identify issues, troubleshoot and escalate issues as needed.Manage customer expectations and keep project deliverables on schedule.Collaborate with the data analytics and customer success teams to prioritize business related tasks.Travel – as necessary, based on customer requests and requirements (once travel resumes).', 'Interpret customer data, analyze and tune the results using various statistical analysis techniques and machine learning algorithms.', 'In this role, the Haystax Data Analytics Engineer will engage with our customers to assist with data analytics related tasks. In addition, the engineer will engage with Haystax’s Product Development and Data Analytics teams to help with innovations related to the insider threat, Haystax risk models and related security risk management solutions. In this role, the engineer will also assist with the development of cloud solutions and automation algorithms. The ideal candidate must be comfortable with elementary logic and probability and demonstrate great communication and problem-solving skill sets. This role focuses on a balance between customer facing data analytics engagements, assisting the data analytics team in model improvements and the development of new customized automated solutions.', 'Preferred Experience:', 'Travel – as necessary, based on customer requests and requirements (once travel resumes).', ""Bachelor's degree in data science, data analytics or computer scienceMinimum 5 years of experience working to support business applications and/or application development.A good understanding of Unix/Linux (CentOS, RHEL, Ubuntu) command line interface including proficient with general system commands and text editors (i.e. ViM, Nano, Emacs)Experience with container technologies like Docker/KubernetesExperience with manipulation of data from high-volume and multiple data sets.Working knowledge or familiarity with NoSQL database technologies such as MongoDB, Elasticsearch.Fundamental skills in Python development and a knowledge and experience of manipulating CSV, JSON, HTML, and XML file formats using machine learning algorithms.Fundamental understanding and experience with regression, clustering, XGBoost, DBSCAN and linear machine learning algorithms.Excellent written and verbal communication skills.Ability to interact well with both internal teams and external customers.Experience at working both independently and in a fast-paced, team-oriented environment.High level of responsibility and commitment."", 'Assist in the development of cloud and automation solutions to improve ongoing tasks.', 'Experience with container technologies like Docker/Kubernetes', 'Perform data analytics on data sources specifically focused on Haystax Technology insider threat model.', 'Minimum 5 years of experience working to support business applications and/or application development.', 'A good level of experience and understanding of alerts and data sets from multiple security controls is a plus.', 'Maintain day-to-day relationships with your technical counterparts at the existing projects.', 'Monitor ongoing performance of platform integrations, identify issues, troubleshoot and escalate issues as needed.', 'A good understanding of Unix/Linux (CentOS, RHEL, Ubuntu) command line interface including proficient with general system commands and text editors (i.e. ViM, Nano, Emacs)', 'A desire to dive deeper into full-stack development is a plus', 'Fundamental skills in Python development and a knowledge and experience of manipulating CSV, JSON, HTML, and XML file formats using machine learning algorithms.', 'Perform the extraction, ingestion, augmentation and aggregation of data from multiple cyber and non-cyber data sources.', 'Experience with manipulation of data from high-volume and multiple data sets.', 'Fundamental understanding of Bayesian statistics is a plus.', 'Prior knowledge of Elasticsearch DSL language is a plus.', 'Work with the data analytics team to test and improve Haystax Technology models.', 'Analyze data sources and data flows working with both structured and unstructured data.', 'Haystax Technology (a subsidiary of the Fishtech Group) is a fast-moving startup working on the cutting edge of threat analytics, providing solutions to government and commercial customers interested in finding a better way to calculate, prioritize, and address their most challenging risks. From determining the likelihood that certain events are fraudulent attacks to predicting and preventing insider threats by deploying behavioral analytics in a truly unique way, Haystax is out in front of the most damaging and complex human, organizational, and economic risks.', 'Key Responsibilities:', 'Working knowledge or familiarity with NoSQL database technologies such as MongoDB, Elasticsearch.', 'High level of responsibility and commitment.', 'Willing to dive deeper into development of customized solutions using programming language such as Python.', 'Ability to interact well with both internal teams and external customers.', 'Fundamental understanding and experience with regression, clustering, XGBoost, DBSCAN and linear machine learning algorithms.', 'Excellent written and verbal communication skills.']",Associate,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Data Engineer,EnergyHub,"New York, NY",3 weeks ago,35 applicants,"['Gain well rounded experience', 'Be part of something important:', 'Gain well rounded experience: EnergyHub offers a diverse and dynamic environment where you will get the chance to work directly with executives and develop expertise across multiple areas of the business.', 'Improving data recovery processes', 'Improve reliability, recovery, and integrity of our data ingestion pipeline', ' Experience developing automated ETL pipelines with high reliability requirements At least 2 years of experience working on a professional web development team Demonstrated expertise with at least one RDBMS Demonstrated expertise with MongoDB, DynamoDB or similar document-oriented data store ', 'Be part of something important: Help create the future of how energy is produced and consumed. Make a positive impact on our climate.', 'Experience developing automated ETL pipelines with high reliability requirements', 'Improving tooling for data correctness', 'Make an immediate impact: New employees can expect to be given real responsibility for bringing new technologies to the marketplace. You are empowered to perform as soon as you join the team!', ""Focus on fun: EnergyHub places high value on our team culture. Happy hours and holiday parties are important to us, but what's also important is how our employees feel every single day. "", 'Software Engineer, Data EngineeringWe ingest thousands of data events per second from over a dozen different device companies (and growing!). This data helps us support the electric grid by providing optimized device control and precise reporting.Since device data is core to our business, we\'re looking for an engineer to focus on data engineering and make sure the device data we get is accurate, reliable, and timely. The team owns the entire pipeline from ingestion through to publishing data products, so you will also do some full-stack development work.Responsibilities Improve reliability, recovery, and integrity of our data ingestion pipeline Develop reporting tools to help our in-house experts and utility clients understand the impact of our services Work with our in-house Advanced Grid Services/Analytics team to productionize exciting new services for utilities Requirements Experience developing automated ETL pipelines with high reliability requirements At least 2 years of experience working on a professional web development team Demonstrated expertise with at least one RDBMS Demonstrated expertise with MongoDB, DynamoDB or similar document-oriented data store Nice-to-have experience Dealing with streaming ingestion Improving reliability of ETL pipelines Improving data recovery processes Improving tooling for data correctness Python or Java experience Why work for EnergyHub? Collaborate with outstanding people: Our employees work hard, do great work, and enjoy collaborating and learning from each other.  Make an immediate impact: New employees can expect to be given real responsibility for bringing new technologies to the marketplace. You are empowered to perform as soon as you join the team! Gain well rounded experience: EnergyHub offers a diverse and dynamic environment where you will get the chance to work directly with executives and develop expertise across multiple areas of the business. Work with the latest technologies: You\'ll gain exposure to a broad spectrum of IoT, SaaS and machine learning challenges, including distributed fault-tolerance, device control optimization, and process modeling to support scalable interaction with disparate downstream APIs.  Be part of something important: Help create the future of how energy is produced and consumed. Make a positive impact on our climate. Focus on fun: EnergyHub places high value on our team culture. Happy hours and holiday parties are important to us, but what\'s also important is how our employees feel every single day.  Company InformationEnergyHub is a growing enterprise software company that works with the most forward-thinking companies in smart energy. Our platform lets consumers turn their smart thermostats, electric cars, water heaters, and other products into virtual power plants that keep the grid stable and enable higher penetration of solar and wind power. We work on technology that already provides energy and cost savings to millions of people through partnerships with the leading companies in the Internet of Things.Company BenefitsEnergyHub offers a generous benefits package including 100% paid medical for employees and a 401(k) with employer match. We offer a casual environment, the flexibility to set your own schedule, a fully stocked fridge and pantry, free Citi Bike membership, secure bike rack, gym subsidy, paid parental leave, and an education assistance program.EnergyHub is an Equal Opportunity EmployerIn connection with your application, we collect information that identifies, reasonably relates to or describes you (""Personal Information""). The categories of Personal Information that we may collect include your name, government-issued identification number(s), email address, mailing address, other contact information, emergency contact information, employment history, educational history, and demographic information. We collect and use those categories of Personal Information about you for human resources and other business management purposes, including identifying and evaluating you as a candidate for potential or future employment or future positions, recordkeeping in relation to recruiting and hiring, conducting analytics, and ensuring compliance with applicable legal requirements and Company policies.', 'Make an immediate impact', "" Collaborate with outstanding people: Our employees work hard, do great work, and enjoy collaborating and learning from each other.  Make an immediate impact: New employees can expect to be given real responsibility for bringing new technologies to the marketplace. You are empowered to perform as soon as you join the team! Gain well rounded experience: EnergyHub offers a diverse and dynamic environment where you will get the chance to work directly with executives and develop expertise across multiple areas of the business. Work with the latest technologies: You'll gain exposure to a broad spectrum of IoT, SaaS and machine learning challenges, including distributed fault-tolerance, device control optimization, and process modeling to support scalable interaction with disparate downstream APIs.  Be part of something important: Help create the future of how energy is produced and consumed. Make a positive impact on our climate. Focus on fun: EnergyHub places high value on our team culture. Happy hours and holiday parties are important to us, but what's also important is how our employees feel every single day.  "", 'Collaborate with outstanding people: Our employees work hard, do great work, and enjoy collaborating and learning from each other. ', ""Work with the latest technologies: You'll gain exposure to a broad spectrum of IoT, SaaS and machine learning challenges, including distributed fault-tolerance, device control optimization, and process modeling to support scalable interaction with disparate downstream APIs. "", 'Requirements', 'Focus on fun', 'Collaborate with outstanding people', 'Why work for EnergyHub?', ' Dealing with streaming ingestion Improving reliability of ETL pipelines Improving data recovery processes Improving tooling for data correctness Python or Java experience ', 'Work with the latest technologies', 'Dealing with streaming ingestion', 'Responsibilities', 'Work with our in-house Advanced Grid Services/Analytics team to productionize exciting new services for utilities', 'At least 2 years of experience working on a professional web development team', 'Company Benefits', 'Demonstrated expertise with at least one RDBMS', 'Develop reporting tools to help our in-house experts and utility clients understand the impact of our services', 'Improving reliability of ETL pipelines', 'Python or Java experience', 'Demonstrated expertise with MongoDB, DynamoDB or similar document-oriented data store', ' Improve reliability, recovery, and integrity of our data ingestion pipeline Develop reporting tools to help our in-house experts and utility clients understand the impact of our services Work with our in-house Advanced Grid Services/Analytics team to productionize exciting new services for utilities ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Senior Data Engineer (Remote),Glassdoor,"Arlington, VA",4 weeks ago,Be among the first 25 applicants,"['', 'Background in Scrum/Agile development methodologies.', ' Work with purpose – join us in creating transparency for job seekers everywhere 100% company paid medical/dental/vision/life coverage, with 80% dependent coverage Long Term Incentive Plan  401(k) Plan with a Company Match to prepare for your future Generous paid holidays and open paid time off  ', 'Experience working with Tableau, Looker or other data visualization software.', 'Employee feedback transparency: ', 'Experience building customer facing products, machine learning pipelines or data products.', '5+ years of hands-on experience with developing data warehouse solutions and data products.', 'Capable of delivering on multiple competing priorities with little supervision.', 'Our Commitments', 'Nice To Have', 'Diversity & Inclusion transparency: We are committed to building a company that is more diverse and representative of society at large. Glassdoor externally publishes our Diversity & Inclusion report and information about our employee population to hold ourselves accountable to our commitment. We also provide programs and resources to create a greater sense of belonging for our employees.', 'Long Term Incentive Plan ', 'Pay transparency: We believe that with more salary transparency, you hold the information to ensure fair pay now and in the future as your career changes and grows. Pay bands and our compensation philosophy are shared publicly to ensure that everyone is paid fairly. Our annual Pay Gap Study found no gender or race/ethnicity pay gap at Glassdoor.', 'Experience with scripting languages: Perl, Shell, etc.', 'Participate in rotational on-call support.', 'Design and develop big data applications using a variety of different technologies.', ""Bachelor's Degree in computer science or equivalent experience."", '2+ years of hands-on experience developing a distributed data processing platform with Hadoop, Hive, Spark, Airflow, Kafka, etc.', 'Exposure to test driven development and automated testing frameworks.', 'Develop logical and physical data models for big data platforms.', 'Learn our business domain and technology infrastructure quickly.', 'Company performance transparency: ', 'Ability to program in multiple programming/scripting languages: Python, Java, JS, Scala, etc', ""Employee feedback transparency: We believe in providing you with greater insight into what it is really like to work with us. In addition to our Glassdoor Reviews, we publicly publish our employee feedback survey responses to ensure everyone has a complete picture of what it's like to work here."", 'Be passionate about or have contributed to open sourced engineering projects in the past.', 'Pay transparency:', 'Why Glassdoor?', 'Responsibilities', 'Document, share your knowledge freely and proactively with others in the team.', 'Generous paid holidays and open paid time off ', 'A values-based culture: Our values are Transparency, Innovation, Good People and Grit. We look for talented, passionate people who embody these values in how they show up to work every day.', 'Practice working with, processing, and managing large data sets (multi TB/PB scale).', ""Company performance transparency: We believe you should know how your work contributes to moving the company forward. That's why we share detailed business performance updates in our monthly All Hands and deeper financial results every quarter. For more insight into the performance of our parent company, you can explore the financial results of Recruit Holdings, and its HR Technology segment in particular, each quarter. Operating transparently at Glassdoor is fundamental as we advocate for transparency in the broader workforce. The ultimate goal is not just to change how we operate at Glassdoor, but for every employer to follow our lead!"", '2+ years of experience using Python as a programming language.', 'Excellent verbal and written communication skills.', 'Familiarity with AWS or GCS technologies.', ' Design and develop big data applications using a variety of different technologies. Develop logical and physical data models for big data platforms. Automate workflows using Apache Airflow. Write data pipelines using Apache Hive, Apache Spark. Create solutions on AWS using services such as Lambda, API Gateway, Kinesis etc. Participate in rotational on-call support. Provide ongoing maintenance and enhancements to existing systems. Learn our business domain and technology infrastructure quickly. Document, share your knowledge freely and proactively with others in the team. ', 'Key Qualifications', '2-3 years of hands-on experience in modeling and designing schema for data lakes or for RDBMS platforms.', '100% company paid medical/dental/vision/life coverage, with 80% dependent coverage', 'Create solutions on AWS using services such as Lambda, API Gateway, Kinesis etc.', 'Work with purpose – join us in creating transparency for job seekers everywhere', "" 5+ years of hands-on experience with developing data warehouse solutions and data products. 2+ years of hands-on experience developing a distributed data processing platform with Hadoop, Hive, Spark, Airflow, Kafka, etc. 2-3 years of hands-on experience in modeling and designing schema for data lakes or for RDBMS platforms. 2+ years of experience using Python as a programming language. Experience with scripting languages: Perl, Shell, etc. Practice working with, processing, and managing large data sets (multi TB/PB scale). Exposure to test driven development and automated testing frameworks. Background in Scrum/Agile development methodologies. Capable of delivering on multiple competing priorities with little supervision. Excellent verbal and written communication skills. Bachelor's Degree in computer science or equivalent experience. "", "" A values-based culture: Our values are Transparency, Innovation, Good People and Grit. We look for talented, passionate people who embody these values in how they show up to work every day. Pay transparency: We believe that with more salary transparency, you hold the information to ensure fair pay now and in the future as your career changes and grows. Pay bands and our compensation philosophy are shared publicly to ensure that everyone is paid fairly. Our annual Pay Gap Study found no gender or race/ethnicity pay gap at Glassdoor. Diversity & Inclusion transparency: We are committed to building a company that is more diverse and representative of society at large. Glassdoor externally publishes our Diversity & Inclusion report and information about our employee population to hold ourselves accountable to our commitment. We also provide programs and resources to create a greater sense of belonging for our employees. Employee feedback transparency: We believe in providing you with greater insight into what it is really like to work with us. In addition to our Glassdoor Reviews, we publicly publish our employee feedback survey responses to ensure everyone has a complete picture of what it's like to work here. Company performance transparency: We believe you should know how your work contributes to moving the company forward. That's why we share detailed business performance updates in our monthly All Hands and deeper financial results every quarter. For more insight into the performance of our parent company, you can explore the financial results of Recruit Holdings, and its HR Technology segment in particular, each quarter. Operating transparently at Glassdoor is fundamental as we advocate for transparency in the broader workforce. The ultimate goal is not just to change how we operate at Glassdoor, but for every employer to follow our lead! "", 'A values-based culture: ', 'Diversity & Inclusion transparency: ', ' Experience building customer facing products, machine learning pipelines or data products. Experience working with Tableau, Looker or other data visualization software. Familiarity with AWS or GCS technologies. Ability to program in multiple programming/scripting languages: Python, Java, JS, Scala, etc Be passionate about or have contributed to open sourced engineering projects in the past. ', 'Senior Data Engineer', '401(k) Plan with a Company Match to prepare for your future', 'Automate workflows using Apache Airflow.', 'Provide ongoing maintenance and enhancements to existing systems.', 'Write data pipelines using Apache Hive, Apache Spark.']",Associate,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Data Engineer,Rock Central,"Detroit, MI",6 days ago,Be among the first 25 applicants,"['', ' Design and support the new and evolving sources of data being brought into the data warehouse Work closely with data architects and follow best practices for data management consumption Work closely with business analysts to work through business requirements and develop processes to provide the needed data visibility via the data warehouse and reporting platform Model application layer and metadata design Design and create automated applications and reporting solutions Work closely with front-end developers to ensure data is being brought in and data integrity is being maintained Monitor and troubleshoot performance issues on the data warehouse servers', ' Work closely with business analysts to work through business requirements and develop processes to provide the needed data visibility via the data warehouse and reporting platform', 'Minimum Qualifications', ' Monitor and troubleshoot performance issues on the data warehouse servers', 'Preferred Qualifications', ' 3 years of experience working with SQL server integration services or ETL tools', 'Job Summary', ' 3 years of experience working with database tools', ' Software programming languages, such as Python and C#', ' Experience working with ETL tools', ' 3 years of programming experience using Python and C#', ' Proficiency in the Microsoft Office suite', "" Bachelor's degree in computer science, information technology, or a related field or equivalent experience"", 'Responsibilities', ' 3 years of experience working with data integration tools', ' 3 years of experience working with database tools 3 years of programming experience using Python and C# 3 years of experience working with SQL server integration services or ETL tools 3 years of experience working with data integration tools Proficiency in the Microsoft Office suite Experience working with ETL tools Knowledge of data integration tools Software programming languages, such as Python and C#', ' Knowledge of data integration tools', ' Model application layer and metadata design', ' Design and create automated applications and reporting solutions', ' Work closely with front-end developers to ensure data is being brought in and data integrity is being maintained', ' Design and support the new and evolving sources of data being brought into the data warehouse', ' Work closely with data architects and follow best practices for data management consumption']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Amitech Solutions,United States,3 weeks ago,162 applicants,"['', 'Description:', ""• Actively identify new technologies and practice within the domain of engineering and drive review for potential introduction to the team's infrastructure."", '• Work on the deployment, delivery and expansion of data pipelines;', 'Pet insurance', '• Provide consulting and feedback to partner teams on data architecture and strategy;', '• Collaborate with analytics and discovery teams to design and plan data engineering solutions;', '• Work on all aspects of the design, development, validation, scaling and delivery of analytical solutions;', '• Familiarity with ML workflows including validation and hyper parameter tuning approaches, and ML frameworks such as scikit-learn and tensor flow.', 'Location: Remote', '• Proven systems administration and operations experience;', '• Work on the development, deployment, and support of systems computing solutions;', '• Integrate proactive strategies and best practices to ensure security of stored data;', '• Experience with python, Java, R, or Scala.', '2018 - 2020 “Top Work Places” Winner', '• Implement, configure, and maintain critical third-party solutions related to engineering work, including compute environments, BI platforms, and cloud systems;', '• Collaborate and influence with cross-functional stakeholders to develop our strategic target state data infrastructure and organization model;', '\ufeffAbout Us', '• Experience with AWS cloud services (EMR, S3, RedShift, EC2, etc.);', 'Sr. Data Engineer', '• Two plus years of experience with GIS data and relevant tools (ArcGIS Pro, ArcGIS', '• Design and maintain data storage systems and access patterns;', '• Experience with distributed systems;', '• Network and Database administration;', 'Online, ArcMap)', 'Health, Dental, and Vision insuranceLong and Short-Term Disability401 (k)Life insurancePet insuranceReferral program2018 - 2020 “Top Work Places” Winner', '\xa0', '401 (k)', '• Technical knowledge with at least of seven years of experience in at least four of the following:', '• Design and maintain ETL workflows;', 'Long and Short-Term Disability', 'Health, Dental, and Vision insurance', '• Knowledge of algorithms and data structures;', 'Why Amitech:', '• Collaborate with interdisciplinary scientists to gather requirement for data pipelines;', '• Design, build, and maintain integrated data solutions such as “data lakes” and “data warehouses”;', 'Referral program', '• Bachelor’s degree in Computer Science, Electrical Engineering or a closely-related field with at least 6-8 years of industry experience OR Master’s Degree in Computer Science, Electrical Engineering or a closely-related field with at least eight years of industry experience OR Doctorate in Computer Science, Electrical Engineering, or a closely-related field with at least four years of industry experience;', '• Optimize algorithms and data workers to scale horizontally and contribute to the development of new algorithms and capabilities that will enable connected pipeline analytics for all pipelines;', '• Experience in running production cloud systems and diagnosing and fixing problems;', 'Required Qualifications:', '• Partner cross-functionally in the development of shared infrastructure where aligned with Breeding business needs;', '• Actively drive skill development of others in the space through proactive coaching, feedback, and training;', '• Primary focus on developing data pipelines with geospatial and imaging processing data pipelines to enable batch and real-time analytics.', '• Proven ability to plan, schedule and deliver quality software DevOps methodology;', 'o Experience with big data tools (Spark, Kafka, Flink, Hadoop, etc.);', '• Drive practices which help raise the success of the overall team, such as code reviews, integrated testing, and other practices;', 'Preferred Qualifications:', ' Amitech is a rapidly growing organization focused on our employees. Our diverse and innovative approach to everything we do means we’re looking for the groundbreakers and the pioneers—people who think differently and create the future.', 'Life insurance', ' We believe healthcare should and can be better.\xa0With a single-minded focus on value, we combine people, process, culture and technology to drive real and lasting change. We partner with our clients to deliver data analytics and digital transformation strategies and solutions to make healthcare more proactive, higher quality and less expensive for everyone.', 'o SQL and NoSQL databases (data warehousing, data modeling, etc.);', '• Experience with tools for authoring workflows and pipelines (Airflow, AWS Step Functions, KubeFlow, etc.);', '• Experience in Developing and supporting large scale geospatial and Imaging Processing data pipelines']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Senior Data Engineer,Imperfect Foods,United States,20 hours ago,Be among the first 25 applicants,"['', 'Design and maintain the architecture and organizational structure of our data warehouse, and keep it tidy by establishing and implementing a strategy for managing roles and permissions, monitoring usage, and cleaning up old objects', 'Passion for structure, organization, and efficiency, down to the details (e.g. maintaining consistent naming conventions and coding style)', 'Ensure quality of deliverables using development testing methods, documentation, code reviews, and user acceptance testing, and implement effective data tests and monitoring to ensure accuracy and reliability of data and ELT pipelines', 'Skills & Qualifications', 'About the Role:', 'You’re a self-starter with the ability to balance multiple projects at once and deliver end-to-end projects independently', 'About You:', 'Fluency in SQL (CTEs, window functions, query optimization) and Python (numpy, pandas, airflow, pyspark, PEP8)', 'Ability to communicate clearly about data to both technical and non-technical audiences', 'Support data analytics platforms and data science workflows, along with identifying ways to strengthen and scale our data infrastructure with Imperfect’s fast-paced growth, including exploring alternative data management and pipeline tools', 'Details of the Position:', 'Competitive benefits package including health care, paid vacation, 401K, paid parental leave, and recurring credit towards your Imperfect account', 'You’re excited to join a fast-paced startup environment', 'Experience working with sophisticated data problems, developing and implementing data strategy, and building high-quality end-to-end data solutions in an agile environment from requirements to production', '\xa0', 'You’re just as comfortable debugging and working deep in technical minutia as you are conveying technical concepts to non-technical business partners', 'Experience with data replication platforms (Stitch, Fivetran), cloud data warehouse technologies (Snowflake), job scheduling platforms (Dagster, Airflow), and deploying and maintaining data infrastructure in the cloud (Docker, AWS)', ""Imperfect is looking for an experienced Data Engineer to join our Business Intelligence team. Primary responsibilities will include the development, maintenance, and operational stability of scalable, resilient data pipelines and infrastructure. You’ll design and optimize data flows to be consumed by distributed systems, reporting, analytics, and machine learning. You'll architect solutions that enable robust and scalable data access and analysis, as our data volume and needs continue to grow at a rapid pace. You’ll develop sophisticated, accurate, and elegantly designed data models and support complex analyses. This role will involve taking business and technical requirements from concept and development through to the production environment. You’ll be enabling our fast-growing company to make better decisions, helping to deliver an ideal customer experience, and fighting food waste within our food system."", ""You're passionate about our mission to eliminate food waste, and create a better food system for all"", 'Responsibilities', 'Design, develop, and own ETL pipelines that deliver data with measurable quality and accuracy, allowing Imperfect to extract more insights from existing data and take advantage of new sourcesWrite production-quality (i.e., accurate, performant, and maintainable) data transformation code to meet the needs of analysts, data scientists, and other business partnersSupport data analytics platforms and data science workflows, along with identifying ways to strengthen and scale our data infrastructure with Imperfect’s fast-paced growth, including exploring alternative data management and pipeline toolsDesign and maintain the architecture and organizational structure of our data warehouse, and keep it tidy by establishing and implementing a strategy for managing roles and permissions, monitoring usage, and cleaning up old objectsEnsure quality of deliverables using development testing methods, documentation, code reviews, and user acceptance testing, and implement effective data tests and monitoring to ensure accuracy and reliability of data and ELT pipelinesWork with stakeholders throughout the organization to understand and identify data needs and opportunities for utilizing data to drive business solutions, translating business requirements into efficient and well-documented data models to empower data-informed development', 'Familiarity with data science acumen and common machine learning algorithms and an eagerness to continue to explore implementation', 'Write production-quality (i.e., accurate, performant, and maintainable) data transformation code to meet the needs of analysts, data scientists, and other business partners', 'Salary and stock options commensurate with experience', '4+ years of relevant experience in data analytics or engineering related fieldExperience working with sophisticated data problems, developing and implementing data strategy, and building high-quality end-to-end data solutions in an agile environment from requirements to productionFluency in SQL (CTEs, window functions, query optimization) and Python (numpy, pandas, airflow, pyspark, PEP8)Experience with and understanding of data modeling concepts and best practices, and writing production-quality ETL data transformation code with an eye towards performance, maintainability, and scalability (especially in dbt)Experience with data replication platforms (Stitch, Fivetran), cloud data warehouse technologies (Snowflake), job scheduling platforms (Dagster, Airflow), and deploying and maintaining data infrastructure in the cloud (Docker, AWS)Familiarity with data science acumen and common machine learning algorithms and an eagerness to continue to explore implementationAbility to communicate clearly about data to both technical and non-technical audiencesPassion for structure, organization, and efficiency, down to the details (e.g. maintaining consistent naming conventions and coding style)', ""You are a champion for data and data accuracyYou’re just as comfortable debugging and working deep in technical minutia as you are conveying technical concepts to non-technical business partnersYou’re a self-starter with the ability to balance multiple projects at once and deliver end-to-end projects independentlyYou’re excited to join a fast-paced startup environmentYou're passionate about our mission to eliminate food waste, and create a better food system for all"", '4+ years of relevant experience in data analytics or engineering related field', 'Experience with and understanding of data modeling concepts and best practices, and writing production-quality ETL data transformation code with an eye towards performance, maintainability, and scalability (especially in dbt)', 'You are a champion for data and data accuracy', 'Work with stakeholders throughout the organization to understand and identify data needs and opportunities for utilizing data to drive business solutions, translating business requirements into efficient and well-documented data models to empower data-informed development', 'Design, develop, and own ETL pipelines that deliver data with measurable quality and accuracy, allowing Imperfect to extract more insights from existing data and take advantage of new sources', 'Candidate can be remotely located within the US', 'Full-time, exempt position reports to the Director of Business Intelligence and is available immediately', 'Full-time, exempt position reports to the Director of Business Intelligence and is available immediatelyCandidate can be remotely located within the USSalary and stock options commensurate with experienceCompetitive benefits package including health care, paid vacation, 401K, paid parental leave, and recurring credit towards your Imperfect account']",Not Applicable,Full-time,Engineering,Food & Beverages,2021-03-24 13:05:10
Staff Data Engineer,Sam's Club,"Sunnyvale, CA",3 weeks ago,177 applicants,"['', 'Equal Opportunity Employer\xa0', 'Being human-led is our true disruption.', 'You have 3+ years of streaming expertise and leading teams building real time streaming pipeline, Spark streaming a plus', 'Walmart, Inc. is an Equal Opportunity Employer – By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing diversity- unique styles, experiences, identities, ideas and opinions – while being inclusive of all people.\xa0', 'Beyond competitive pay, you can receive incentive awards for your performance. Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO, multiple health plans, and much more.\xa0', 'Equal Opportunity Employer', 'We’re virtual', 'The above information has been designed to indicate the general nature and level of work performed in the role.', 'Deploying and monitoring products on Cloud platforms', 'Interacting with Sam’s Club engineering teams across geographies to leverage expertise and contribute to the tech community.', 'You are a collaborative connector who builds bridges between teams and functions\xa0', 'Job Description:', 'You’re\xa0experienced\xa0in\xa0computing platforms\xa0and\xa0companion\xa0tools (e.g., Azure/GCP,\xa0SQL and NoSQL)\xa0', ' It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications required of employees assigned to this job. The full Job Description can be made available as part of the hiring process.', ""Do you have boundless energy and passion for engineering data used to solve dynamic problems that will shape the future of retail?\xa0With the sheer scale of Sam’s\xa0Club Technologies environment\xa0comes the biggest of big data sets.\xa0As a\xa0Sam's Club\xa0Staff Data Engineer,\xa0you will dig\xa0into our\xa0mammoth\xa0scale of\xa0data to help\xa0unleash\xa0the power of retail\xa0data science by\xa0imagining, developing, and maintaining data pipelines\xa0that our Data Scientists and Analysts can rely on.\xa0\xa0You will be responsible for contributing to an orchestration layer of complex data transformations, refining raw data from source into targeted, valuable data assets for consumption in a governed way.\u202fYou will\xa0partner\xa0with\xa0Data Scientists, Analysts, other engineers\xa0and business stakeholders to\xa0solve complex\xa0and exciting\xa0challenges\xa0so that\xa0we can\xa0build out capabilities that\xa0evolve the\xa0retail business model\xa0while\xa0making\xa0a\xa0positive\xa0impact\xa0on\xa0our\xa0customers’\xa0lives.\xa0"", '(You must work in the Sunnyvale, Dallas OR Bentonville area)', 'Developing and/or making a Contribution to adding features that enable the adoption of data across Sam’s Club', 'You\xa0have a proven track record coding\xa0with at least one programming language (Java, Scala and/or Python) to write data pipeline and data processing layers\xa0', 'Benefits & Perks:', 'You have consistently high standards,\xa0your passion for quality is inherent in everything that you do\xa0', 'The above information has been designed to indicate the general nature and level of work performed in the role.\u202f It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications required of employees assigned to this job. The full Job Description can be made available as part of the hiring process.\xa0', 'Identifying the right open source tools to deliver product features by performing research, POC/Pilot and/or interacting with various open source forums', 'Benefits & Perks:\xa0', 'Engaging with Product Management and Business to drive the agenda, setting your priorities and delivering awesome product features to keep platform ahead of market scenarios.', 'You’re\xa0an\xa0inquisitive,\xa0out-of-the box thinker\xa0who’s\xa0continually on the lookout for\xa0opportunities\xa0to\xa0improve\xa0and innovate\xa0data\xa0systems and\xa0analytic\xa0solutions\xa0', 'About Global Tech', 'Developing and implementing the best-in-class monitoring processes to enable data applications to meet SLAs', 'Imagine working in an environment where one line of code can make life easier for hundreds of millions of people and put a smile on their face. That’s what we do at Walmart Global Tech. We’re a team of 15,000+ software engineers, data scientists and service professionals within Walmart, the world’s largest retailer, delivering innovations that improve how our customers shop and empower our 2.2 million associates. To others, innovation looks like an app, service or some code, but Walmart has always been about people. People are why we innovate, and people power our innovations. Being human-led is our true disruption.', 'You’ll sweep us off our feet if:\xa0', 'You are knowledgeable in Data Productization, Data Lineage and Metadata Management.', 'Sam’s Club is on a mission for Savings Made Simple. A division of Wal-Mart Stores Inc., we are the membership warehouse club solution for small business and everyday living. Since 1983, we’ve worked to provide our members quality products at incredible values. At Sam’s Club, everything we do is to serve our members. Whether it’s offering them quality products at an incredible value or remodeling our clubs to improve their shopping experience, we keep our members at the heart of everything we do.\xa0', 'You’ll sweep us off our feet if:', 'Working virtually this year has helped us make quicker decisions, remove location barriers across our global team, be more flexible in our personal lives and spend less time commuting.\xa0Today, we are reimagining the tech workplace of the future by making a permanent transition to virtual work for most of our team. Of course, being together in person is an important part of our culture and shared success. We’ll collaborate in person at a regular cadence and with purpose.\xa0', 'Guiding the team technically for end to end solution lifecycle', 'You’re\xa0an\xa0inquisitive,\xa0out-of-the box thinker\xa0who’s\xa0continually on the lookout for\xa0opportunities\xa0to\xa0improve\xa0and innovate\xa0data\xa0systems and\xa0analytic\xa0solutions\xa0You have consistently high standards,\xa0your passion for quality is inherent in everything that you do\xa0You are a collaborative connector who builds bridges between teams and functions\xa0You evangelize an extremely high standard of code quality, system reliability, and performance\xa0You\xa0have a proven track record coding\xa0with at least one programming language (Java, Scala and/or Python) to write data pipeline and data processing layers\xa0You’re\xa0experienced\xa0in\xa0computing platforms\xa0and\xa0companion\xa0tools (e.g., Azure/GCP,\xa0SQL and NoSQL)\xa0You’re\xa0skilled\xa0in data modeling &\xa0data migration\xa0protocols\xa0You have 3+ years of streaming expertise and leading teams building real time streaming pipeline, Spark streaming a plusYou are knowledgeable in Data Productization, Data Lineage and Metadata Management.', 'You’ll make an impact by:\xa0', 'You evangelize an extremely high standard of code quality, system reliability, and performance\xa0', 'Architecting, Designing, building, testing and deploying cutting edge solutions at scale, impacting millions of customers worldwide drive value from dataInteracting with Sam’s Club engineering teams across geographies to leverage expertise and contribute to the tech community.Engaging with Product Management and Business to drive the agenda, setting your priorities and delivering awesome product features to keep platform ahead of market scenarios.Identifying the right open source tools to deliver product features by performing research, POC/Pilot and/or interacting with various open source forumsDeveloping and/or making a Contribution to adding features that enable the adoption of data across Sam’s ClubDeploying and monitoring products on Cloud platformsDeveloping and implementing the best-in-class monitoring processes to enable data applications to meet SLAsGuiding the team technically for end to end solution lifecycle', 'You’ll make an impact by:', 'Architecting, Designing, building, testing and deploying cutting edge solutions at scale, impacting millions of customers worldwide drive value from data', 'You’re\xa0skilled\xa0in data modeling &\xa0data migration\xa0protocols\xa0']",Mid-Senior level,Full-time,Information Technology,Retail,2021-03-24 13:05:10
Data Engineer,Quadrant Inc.,"Aberdeen, MD",2 weeks ago,Be among the first 25 applicants,"['', 'Data Engineer', 'Washington DC (Remote during COVID)', 'Quadrant, Inc. is an equal opportunity and affirmative action employer. Quadrant is committed to administering all employment and personnel actions on the basis of merit and free of discrimination based on race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or status as an individual with a disability. Consistent with this commitment, we are dedicated to the employment and advancement of qualified minorities, women, individuals with disabilities, protected veterans, persons of all ethnic backgrounds and religions according to their abilities.', 'Duties', 'Musts', 'Experience Working With LogStash Is Required.']",Entry level,Contract,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Airvet,United States,6 days ago,109 applicants,"['', 'gather functional and non-functional requirements, and deliver the appropriate data model and high-quality datasets in a timely manner', 'Since its inception in 2019, Airvet has helped over 100,000 pets live healthier and happier lives and is rapidly growing its network of vets. In July 2020, we raised $14 million in Series A. As the company scales its products and operations to new markets and customers, we are beginning to tackle new challenges that include:', 'Basic understanding of data architecture and data warehousing', 'Analytics: We need to process at scale our sessions to gain insights and alert users about the health of the pets.', 'The company has just started assembling a team of top-notch engineers and product professionals. We are seeking a data engineer to help us build a data pipeline to support business intelligence and later advanced analytics, including ML. It is a greenfield development initiative with a lot of career and leadership opportunities.', 'Paid maternity/paternity leave', 'Good analytical and problem-solving skills', 'Databases: PostgreSQL, Redis', 'Build tooling and implement systems to overcome limitations of the data consumption portals when appropriateMaintain quality, integrity, and consistency of datasets you’ve producedDefine and enforce best practices and standards for the teamContinuous improvement to the data pipeline through new development or optimization', 'Thrives to create a company culture in a truly flat organization that values commitment, roles, and hustle over titles and rigid hierarchies', 'Scaling: We are seeing an increase in the number of connections and live streams. Our backend needs to handle the workload of today and the future.', 'Benefits/Perks', 'Enterprise application: We are building an enterprise-class application to help vets and hospitals to run a more productive practice.', 'Completely remote role\xa0', 'Bachelor’s degree in Computer Science, Math, Science or a related field; or equivalent years of experience in a relevant field', 'Backend: Go (transactions and network), Python (legacy and data pipeline)Databases: PostgreSQL, RedisInfrastructure: Terraform, Kubernetes on AWS/GCPMobile: iOS/Swift and Android/KotlinWeb: React.js, Next.js, JavaScript (legacy), TypeScript', 'Vision, Dental, and Medical coverage', 'Great sense of humor :-)', 'Healthy work-life balance encouraged!Completely remote role\xa0Vision, Dental, and Medical coverageThrives to create a company culture in a truly flat organization that values commitment, roles, and hustle over titles and rigid hierarchiesPTO - 14 days minimum to keep you freshPaid maternity/paternity leave', 'Founded in Los Angeles, Airvet is the #1 rated and reviewed consumer pet telemedicine app that offers face to face consultation and advice from a licensed veterinarian all from the comfort of your mobile phone. Airvet is the preferred platform of Veterinary industry leaders with its cloud-based technology. Airvet offers unique, attentive, and empathetic virtual care and workflow efficiency solutions that allow veterinarians to provide animal lovers the best care they can get. Through Airvet’s mobile app, pets and parents in the US and Canada have access to quality care from the most trusted veterinarians within seconds, anytime and anywhere. Airvet is a $30 flat fee with no time limit and follow-ups included.\xa0', 'Requirements', 'Own and develop data pipelines adhering with data governance principles', 'Own and develop data pipelines adhering with data governance principlesWork closely with stakeholders in Engineering, Finance, Marketing, and Product to\xa0', 'Self-driven with the ability to work in a fast-paced environment and dealing with ambiguity', 'AWS Redshift', 'Bachelor’s degree in Computer Science, Math, Science or a related field; or equivalent years of experience in a relevant field1 - 3+ years of experience in Python or/and an equivalent language1 - 3+ years of experience in SQL querying language1 - 3+ years of experience in ETLBasic understanding of data architecture and data warehousingBasic knowledge of software system, including Linux and Cloud infrastructure (AWS or Google Cloud)Self-driven with the ability to work in a fast-paced environment and dealing with ambiguityGood analytical and problem-solving skillsGood written and verbal communication skills', 'Mobile: iOS/Swift and Android/Kotlin', 'Build tooling and implement systems to overcome limitations of the data consumption portals when appropriate', 'User experience: We need to enhance our UI to provide an unparalleled user experience for our users.', 'Basic knowledge of software system, including Linux and Cloud infrastructure (AWS or Google Cloud)', 'Good written and verbal communication skills', 'Responsibilities', 'Extra Credit', 'Define and enforce best practices and standards for the team', '1 - 3+ years of experience in ETL', 'Healthy work-life balance encouraged!', 'Work closely with stakeholders in Engineering, Finance, Marketing, and Product to\xa0', '1 - 3+ years of experience in Python or/and an equivalent language', 'Big plus: Airflow', 'Continuous improvement to the data pipeline through new development or optimization', 'Maintain quality, integrity, and consistency of datasets you’ve produced', 'Web: React.js, Next.js, JavaScript (legacy), TypeScript', 'Data Engineer', 'Our Stack', 'User experience: We need to enhance our UI to provide an unparalleled user experience for our users.Scaling: We are seeing an increase in the number of connections and live streams. Our backend needs to handle the workload of today and the future.Enterprise application: We are building an enterprise-class application to help vets and hospitals to run a more productive practice.Analytics: We need to process at scale our sessions to gain insights and alert users about the health of the pets.Optimization: We need to optimize our video transcoding and streaming under high latency network conditions.', 'Big plus: AirflowAWS RedshiftQuick learnerGreat sense of humor :-)', 'Backend: Go (transactions and network), Python (legacy and data pipeline)', 'Optimization: We need to optimize our video transcoding and streaming under high latency network conditions.', 'Infrastructure: Terraform, Kubernetes on AWS/GCP', 'Quick learner', 'PTO - 14 days minimum to keep you fresh', '1 - 3+ years of experience in SQL querying language']",Entry level,Full-time,Information Technology,Veterinary,2021-03-24 13:05:10
Data Engineer Intermediate (work from home Mid-Atlantic US resident),Geisinger,"Harrisburg, PA",1 week ago,Be among the first 25 applicants,"['', 'Provides preliminary code review, testing, debugging, and general testing instructions.', 'Job Summary', 'Involves high-level participation in the design and management of a computational infrastructure for different purposes including applied medical research.', 'Builds data ingestion pipelines for the Big Data Hadoop environment.Programming for a Big Data distributed computing environment using Java, Scala or similar object oriented programming languages.Programming data processing and integration algorithms on the Apache Hadoop, HBase stack.Coordinates projects and responsible for timely and accurate execution.Collaborates and participates in the design and implementation of various projects.Involves high-level participation in the design and management of a computational infrastructure for different purposes including applied medical research.Collaborates with other technology team members, clinicians and researchers on projects requiring data and analytic services.Works closely with data architects to define and execute an enterprise data architecture for complex healthcare data flows.Writes code for parallel computing.Develops new programs and responsible for moving existing code to high performance distributed systems code.Responsible to document all changes completed on the system within designated timeframes.Responsible for following department coding/programming guidelines to produce efficient routines.Provides preliminary code review, testing, debugging, and general testing instructions.', 'Collaborates with other technology team members, clinicians and researchers on projects requiring data and analytic services.', 'Builds data ingestion pipelines for the Big Data Hadoop environment.', 'Responsible for following department coding/programming guidelines to produce efficient routines.', 'Coordinates projects and responsible for timely and accurate execution.', 'Programming for a Big Data distributed computing environment using Java, Scala or similar object oriented programming languages.', 'Job Duties', 'Programming data processing and integration algorithms on the Apache Hadoop, HBase stack.', 'Writes code for parallel computing.', 'Collaborates and participates in the design and implementation of various projects.', 'Works closely with data architects to define and execute an enterprise data architecture for complex healthcare data flows.', 'Responsible to document all changes completed on the system within designated timeframes.', 'Experience', 'Develops new programs and responsible for moving existing code to high performance distributed systems code.']",Entry level,Full-time,Information Technology,"Health, Wellness and Fitness",2021-03-24 13:05:10
Data Engineer – Data Acquisition,Balyasny Asset Management L.P.,"Chicago, IL",,N/A,"['', 'Our analysts and systematic trading teams rely on us to provide analytics-ready datasets. For each dataset we must consider the implications of point in time storage, optimize for our users’ access patterns, and create useful aggregations/slices. Our ideal candidate will have experience with storing, transforming, and modeling big data. In this role, you will:', 'Design and build services and plugins to enhance our Data Acquisition Platform', 'Role Overview', '3+ years of experience with at least one of Spark/Hive/Hadoop', 'Data Intelligence Group (DIG) ', 'Aptitude for designing infrastructure, data products, and tools for Data Scientists a plus', 'Strong oral and written communication skills, most importantly, must be a team player', 'Develop cloud-first data ingestion processes using Python, SQL, and Spark', 'We are looking for creative and enthusiastic Data Engineers to join our team in building the best Data Platform on the street. We’re responsible for managing the flow of data into the firm, maintaining the data lake, creating analytics-ready datasets, and building the APIs that make everything accessible to our clients. Our singular goal is to help our investment teams use data to make better investment decisions. ', 'Work directly with Analysts, Quants, and Portfolio Managers to understand requirements and provide end-to-end data solutions', 'Develop cloud-first data ingestion processes using Python, SQL, and SparkEngineer data models and infrastructure for a wide variety of market and alternative datasetsDesign and build services and plugins to enhance our Data Acquisition PlatformMaintain alerting systems to ensure smooth day-to-day operations for hundreds of datasetsAuthor tests to validate data quality and the stability of the platformInvestigate and defuse time-sensitive data incidentsCommunicate with data providers to onboard new datasets and troubleshoot technical issuesEvangelize best practices to our partners throughout the firmWork directly with Analysts, Quants, and Portfolio Managers to understand requirements and provide end-to-end data solutions', 'Engineer data models and infrastructure for a wide variety of market and alternative datasets', 'Financial industry experience is a plus', 'The Data Intelligence Group (DIG) is a key part of BAM’s continued growth. Year over year, the knowledge needed to leverage data plays an increasingly important role in the firm’s core business.\xa0The analysis, services, software, and operational expertise that DIG provides are part of BAM’s competitive advantage.', 'Ability to understand and contribute to our existing data system software', '1+ years of experience with cloud technologies ( AWS / Azure / Google Cloud )', '\xa0', 'WHAT YOU’LL BRING', 'Investigate and defuse time-sensitive data incidents', 'Experience with large data sets and techniques to architect them for performance', 'Maintain alerting systems to ensure smooth day-to-day operations for hundreds of datasets', 'Communicate with data providers to onboard new datasets and troubleshoot technical issues', 'Solid understanding of time series data and temporal queries', 'Bachelors/Masters degree in Computer Science or a related fieldStrong analytical, data, and programming skills (Python/SQL/NoSQL)3+ years of experience with at least one of Spark/Hive/Hadoop2+ years of experience orchestrating pipelines with a technology like Airflow/Luigi/Oozie/Nifi1+ years of experience with cloud technologies ( AWS / Azure / Google Cloud )Solid understanding of time series data and temporal queriesExperience with large data sets and techniques to architect them for performanceAbility to understand and contribute to our existing data system softwareAptitude for designing infrastructure, data products, and tools for Data Scientists a plusFinancial industry experience is a plusStrong oral and written communication skills, most importantly, must be a team player', 'Author tests to validate data quality and the stability of the platform', 'Bachelors/Masters degree in Computer Science or a related field', 'Evangelize best practices to our partners throughout the firm', 'Strong analytical, data, and programming skills (Python/SQL/NoSQL)', '2+ years of experience orchestrating pipelines with a technology like Airflow/Luigi/Oozie/Nifi']",Associate,Full-time,Information Technology,Investment Management,2021-03-24 13:05:10
Data Engineer,IDEXX,"Westbrook, ME",3 weeks ago,26 applicants,"['', 'Bachelor’s degree or equivalent combination of education and experience required.', 'Implement measures to ensure data accuracy and accessibility.', 'Adhere and contribute to naming conventions, data governance practices, and thorough testing principles.', 'Demonstrated initiative in resolving problems, balancing conflicting requirements in partnership with others.', 'Provide guidance on data design and requirements to other development and business teams.', 'Develop Structured Query Language (SQL), Data Definition Language (DDL) and Python, Scala or equivalent programming scripts to support data pipeline development, problem solving, data validation and performance tuning.', 'Initiative and self-motivation to work under minimal supervision with latitude for independent judgment.', 'Strong customer service and business relationship-building skills ', 'Ability to work independently and in teams. ', 'Design and implement scalable, reliable distributed data processing frameworks and analytical infrastructure using multiple technologies.', '3+ years of professional business experience with data related projects.', 'What You Will Need To Succeed', 'Strong communication skills, both verbal and written, including the ability to translate technical subject matter to non-technical audiences (both as a speaker and listener). ', 'Familiarity with cloud technology for big data initiatives such as Amazon Web Services (AWS), EMR, Spark, Hive, and Presto.', 'What You Can Expect In This Role', 'Propose or develop semantic layer features for the enterprise model.', 'Provide ongoing maintenance and process improvements for data initiatives.', 'Monitor performance and utilization.', ""Define, design, and implement data integration, management, storage, consumption, backup and recovery solutions that ensure high performance of the organization's enterprise data."", 'Proficiency in Structured Query Language (SQL), Scala, and Python or equivalent programming language.', 'Experience with Agile software development methodology.', 'Ability to problem solve and draw conclusions, in some cases with limited information.', 'Experience with databases: Oracle, MySQL, Snowflake', ' IDEXX values a diverse workforce and workplace and strongly encourages women, people of color, LGBTQ+ individuals, people with disabilities, members of ethnic minorities, foreign-born residents, and veterans to apply. ', ""Design and implement scalable, reliable distributed data processing frameworks and analytical infrastructure using multiple technologies.Define, design, and implement data integration, management, storage, consumption, backup and recovery solutions that ensure high performance of the organization's enterprise data.Develop Structured Query Language (SQL), Data Definition Language (DDL) and Python, Scala or equivalent programming scripts to support data pipeline development, problem solving, data validation and performance tuning.Adhere and contribute to naming conventions, data governance practices, and thorough testing principles.Document data design tasks or project requirementsImplement measures to ensure data accuracy and accessibility.Identify and resolve data-oriented problems, such as missing, duplicate or incorrect data.Monitor performance and utilization.Provide guidance on data design and requirements to other development and business teams.Propose or develop semantic layer features for the enterprise model.Provide ongoing maintenance and process improvements for data initiatives."", 'Data Engineer', 'Document data design tasks or project requirements', 'Experience with data integration/ETL tools such as Informatica PowerCenter or Sesame Relational Junction.', 'Planning and organizational skills, with ability to prioritize and be flexible with changing business needs.', 'Bachelor’s degree or equivalent combination of education and experience required.3+ years of professional business experience with data related projects.Experience with databases: Oracle, MySQL, SnowflakeProficiency in Structured Query Language (SQL), Scala, and Python or equivalent programming language.Experience with data integration/ETL tools such as Informatica PowerCenter or Sesame Relational Junction.Familiarity with cloud technology for big data initiatives such as Amazon Web Services (AWS), EMR, Spark, Hive, and Presto.Understanding of data warehousing solutions and relational database theory.Strong communication skills, both verbal and written, including the ability to translate technical subject matter to non-technical audiences (both as a speaker and listener). Demonstrated initiative in resolving problems, balancing conflicting requirements in partnership with others.Strong customer service and business relationship-building skills Planning and organizational skills, with ability to prioritize and be flexible with changing business needs.Initiative and self-motivation to work under minimal supervision with latitude for independent judgment.Ability to work independently and in teams. Ability to problem solve and draw conclusions, in some cases with limited information.Experience with Agile software development methodology.', 'Understanding of data warehousing solutions and relational database theory.', ' IDEXX is an equal opportunity employer. Applicants will not be discriminated against because of race, color, creed, sex, sexual orientation, gender identity or expression, age, religion, national origin, citizenship status, disability, ancestry, marital status, veteran status, medical condition, or any protected category prohibited by local, state, or federal laws.', 'Identify and resolve data-oriented problems, such as missing, duplicate or incorrect data.']",Not Applicable,Full-time,Information Technology,Biotechnology,2021-03-24 13:05:10
Lead Data Engineer,The Home Depot,"Atlanta, GA",5 days ago,Be among the first 25 applicants,"['', 'Environment', 'Designs and builds the data strategies and roadmaps necessary to serve marketing analytics and data science needs', 'POSITION PURPOSE', 'Travel', ' Communicates Effectively', 'Minimum Qualifications', 'BA/BS Degree in Computer Science, any Engineering discipline, Statistics, Information Systems or another quantitative field.', 'Preferred Qualifications', 'NATURE AND SCOPE', 'Education Required', 'Experience optimizing data pipelines/queries for performance and storage', 'Experience leading and presenting proof of values and business to leaders', 'BA/BS Degree in Computer Science, any Engineering discipline, Statistics, Information Systems or another quantitative field.6+ years of industry experience in data engineering, data science, or related field with a track record of manipulating, processing, and extracting value from large datasetsExperience leading a Data Engineering TeamExperience building and managing data pipelines and CI/CD to repositories in cloud environments such as Google Cloud, Microsoft Azure or AWSExperience optimizing data pipelines/queries for performance and storageExcellent knowledge of the industry standards and best practicesGood knowledge of coding standards and the ability to use versioning tools such as Github/subversion etc.,Experience in Airflow is a mustExperience extracting/cleansing data and generating insights from large transactional data sets using SQL, R, Python, Spark on the cloudExperience in Looker, Redshift, Apache Spark, Spark Structured Streaming, Spark SQL, Hive, HDFS, Kafka etc.Experience with cloud computing with Dataproc, Databricks or similar technologies. Strong verbal and written communications skills at all levels; ability to communicate complex customer behavior information to both functional partners and Executive LeadershipOpen to idea exploration with strong problem-solving/analytical abilitiesDemonstrated strength in creating partnerships and in building relationships with other functions and associates within the organizationExperience leading and presenting proof of values and business to leaders', 'Validate Data Engineering business data elements, organizational and business intelligence architecture designs for engineering functional areas from Dashboards, Data Lakes, Data Operations, ML - AI, and upstream/downstream intake and output processes', ' Tech Savvy', ' Strategic Mindset', 'Experience in Looker, Redshift, Apache Spark, Spark Structured Streaming, Spark SQL, Hive, HDFS, Kafka etc.', 'Physical Requirements', ' Manages Complexity', ' Decision Quality', 'Good knowledge of coding standards and the ability to use versioning tools such as Github/subversion etc.,', 'Excellent knowledge of the industry standards and best practices', 'Build analytical tools to increase productivity of Data Analysts, Data Scientists and Marketers', 'Design, build and launch extremely efficient and reliable data pipelines to move data across a number of platforms including Data Warehouse, online caches and real-time systems.Provide data architecture that is flexible, scalable, and consistent for cross-functional use, and aligned to stakeholder business requirements.Manage workflow orchestration and demonstrate strength in data modeling, ETL development, and data warehousingBuild analytical tools to increase productivity of Data Analysts, Data Scientists and MarketersPartner with BI, IT, Marketers and Data Scientists to understand data needs and assist them in accessing and leveraging key data setsDefine SLA and acceptable time lags by data source, define QA process, and socialize resolution process to ensure data is flowing accurately through data creation to our presentation layersHelp Marketing organization to become a 100% data driven organization by building a next generation data platform that brings accurate and timely data to the MarketersValidate Data Engineering business data elements, organizational and business intelligence architecture designs for engineering functional areas from Dashboards, Data Lakes, Data Operations, ML - AI, and upstream/downstream intake and output processesDesigns and builds the data strategies and roadmaps necessary to serve marketing analytics and data science needs', ' Action Oriented', 'Demonstrated strength in creating partnerships and in building relationships with other functions and associates within the organization', 'ENVIRONMENTAL JOB REQUIREMENTS', ' Manages Ambiguity', 'Manage workflow orchestration and demonstrate strength in data modeling, ETL development, and data warehousing', 'Experience with cloud computing with Dataproc, Databricks or similar technologies. ', 'Strong verbal and written communications skills at all levels; ability to communicate complex customer behavior information to both functional partners and Executive Leadership', 'Additional Minimum Qualifications:', 'Partner with BI, IT, Marketers and Data Scientists to understand data needs and assist them in accessing and leveraging key data sets', ' Business Insights', 'Additional', 'Knowledge, Skills, Abilities And Competencies', 'Major Tasks, Responsibilities And Key Accountabilities', ' Customer Focus', 'Design, build and launch extremely efficient and reliable data pipelines to move data across a number of platforms including Data Warehouse, online caches and real-time systems.', 'Experience leading a Data Engineering Team', 'Open to idea exploration with strong problem-solving/analytical abilities', 'Help Marketing organization to become a 100% data driven organization by building a next generation data platform that brings accurate and timely data to the Marketers', 'Define SLA and acceptable time lags by data source, define QA process, and socialize resolution process to ensure data is flowing accurately through data creation to our presentation layers', 'Years of Relevant Work Experience: ', 'Experience in Airflow is a must', 'Experience building and managing data pipelines and CI/CD to repositories in cloud environments such as Google Cloud, Microsoft Azure or AWS', 'Additional Environmental Job Requirements: ', '6+ years of industry experience in data engineering, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets', 'Provide data architecture that is flexible, scalable, and consistent for cross-functional use, and aligned to stakeholder business requirements.', 'Experience extracting/cleansing data and generating insights from large transactional data sets using SQL, R, Python, Spark on the cloud']",Associate,Full-time,Information Technology,Construction,2021-03-24 13:05:10
"Data Engineer (Remote, US)",Bombora,"Reno, NV",3 weeks ago,Be among the first 25 applicants,[],Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer,PlayVS,Los Angeles Metropolitan Area,2 weeks ago,Over 200 applicants,"['', ""Here's What You'll Get To Do"", 'Strong verbal and written communication skills', 'Be the data life force of our company, continually improving the stages of our data infrastructure and managing the quality and consistency of our data', 'Ability to execute on ETL design, implementation, and maintenance\xa0', ""Here's What We're Looking For"", 'Work with the Data, Product, Operations and Executive teams to extract value from data and support their data infrastructure needs.', 'Passionate about PlayVS and the problem we are solving', 'Familiarity with API integrations', '\xa0', 'Expertise with data warehousing (Redshift, BigQuery, Snowflake, Postgres), data pipelining (Airflow, AWS Data Pipeline, dbt), and data modeling.\xa0', 'Extensive experience with SQL and Python scripting', 'Possess a growth mindset: Live and breathe experimentation, data analysis, and ruthlessly prioritize for impact', 'Orchestrate the unity between our product data and external data sources, destinations and APIs', 'Ability to transform raw data into tangible insights through data visualization (Mode, Looker, Tableau)', '4+ years of experience in Data Engineering', 'Committed desire to understand our customers', 'Work with the Data, Product, Operations and Executive teams to extract value from data and support their data infrastructure needs.Design, build and maintain your vision of an optimal data pipeline architecture.Deploy data models that synergizes data democratization and business operations.Orchestrate the unity between our product data and external data sources, destinations and APIsBe the data life force of our company, continually improving the stages of our data infrastructure and managing the quality and consistency of our data', 'Design, build and maintain your vision of an optimal data pipeline architecture.', '4+ years of experience in Data EngineeringExtensive experience with SQL and Python scriptingExpertise with data warehousing (Redshift, BigQuery, Snowflake, Postgres), data pipelining (Airflow, AWS Data Pipeline, dbt), and data modeling.\xa0Ability to execute on ETL design, implementation, and maintenance\xa0Familiarity with API integrationsAbility to transform raw data into tangible insights through data visualization (Mode, Looker, Tableau)Passionate about PlayVS and the problem we are solvingCommitted desire to understand our customersStrong verbal and written communication skillsPossess a growth mindset: Live and breathe experimentation, data analysis, and ruthlessly prioritize for impact', 'Deploy data models that synergizes data democratization and business operations.', 'We are looking for a passionate Data Engineer to join our team. You will be responsible for building and maintaining our data infrastructure that is foundational to our amateur esports ecosystem.', 'The fastest-growing High School and Collegiate sport in America isn’t basketball or football—it’s esports—and PlayVS is the official platform for High School and Collegiate esports. We offer an incredible, full-stack esport platform—game integrations, team building tools, leagues, tournaments, and schedules—and our software products tie everything together into a cohesive experience.']",Mid-Senior level,Full-time,Information Technology,Computer Games,2021-03-24 13:05:10
Data Engineer,Harnham,San Francisco Bay Area,5 days ago,49 applicants,"['', 'SALARY', 'HOW TO APPLY', '·\xa0\xa0\xa0\xa0\xa0Automate manual data flows for repeated use and scalability', '70-75hr', 'SKILLS', '·\xa0\xa0\xa0\xa0\xa0Work independently\xa0on data projects for multiple business functions', '·\xa0\xa0\xa0\xa0Experience building SaaS products through data insights', '·\xa0\xa0\xa0\xa0\xa0Apply data flows connecting\xa0operational systems, BI systems, and the big data platform', 'THE COMPANY', '·    Must be able to bring up new architecture/design for ETL processing', '\xa0', '·\xa0\xa0\xa0\xa0Deep experience with Cloud Tools', '·\xa0\xa0\xa0\xa0Expert level\xa0with Python\xa0', 'Please register your interest by sending your CV via the Apply link on this page.', 'Are you interested in working with the leading video streaming provider? Are you advanced at reporting data insights? The team is seeking Senior Data Engineer who is able to support company stakeholders. This role focuses on tight-knit collaboration with business units in order to capture a deep understanding of the client, content, and production while aiding in the business strategy, procedures, and services.', '·    Informatica ETL', 'RESPONSIBILITIES', '·\xa0\xa0\xa0\xa0Management / Leadership experience', 'For further details or to inquire about other roles, please contact\xa0Chandler Davis-Strickland at Harnham.']",Mid-Senior level,Contract,Information Technology,Staffing and Recruiting,2021-03-24 13:05:10
Data Engineer,Langham Recruitment,"Eastern, WV",2 weeks ago,Be among the first 25 applicants,"['', 'Skills / Experience', 'Cloud Native Data Warehouse Experience (AWS Preferred)', 'Strong SQL Experience Including Tuning Skills']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Sketchy,United States,3 weeks ago,42 applicants,"['', 'Create and maintain optimal data pipeline architectureIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery re-designing infrastructure for greater scalability and usabilityBuild analytics tools that utilize the data pipeline to provide actionable insightsBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sourcesKeep our data separated and secure across national boundaries through multiple data centers', 'Sketchy is a TCG (The Chernin Group) portfolio company (joining other companies such as Headspace, Surfline Food52 and Crunchyroll) and a Reach Capital portfolio company (joining other start-up companies that bring a playfulness to learning.)', 'Sketchy is an online visual learning platform that helps students effortlessly learn and recall information through a blend of art, story, spaced repetition and memory palace techniques. Sketchy was born when four medical students began creating sketched stories to distinguish and memorize similarly named viruses, as they realized that the same learning methodologies can be used across a variety of subjects.', '3+ years experience in Data Engineering', 'Strong analytic skills related to working with unstructured datasets', 'Education:', 'Build analytics tools that utilize the data pipeline to provide actionable insights', 'Requirements:', 'Location: This role is open to remote employees in select US states: California, New York, Hawaii, Illinois, Colorado, Massachusetts, Washington and Washington, D.C.\xa0', 'Keep our data separated and secure across national boundaries through multiple data centers', 'Competitive compensation planInnovative, high growth and collaborative cultureGenerous PTO package with floating holidayFun team events (Monthly and virtual for now)Great Benefits including 99 % Coverage of Medical, dental, vision. Up to 4% match on 401k and more', 'Responsibilities:', 'Self-starter who is excited to be part of a growing startup company', 'Experience with object-oriented/object function scripting languages', 'Innovative, high growth and collaborative culture', 'Since its inception in 2013, Sketchy has become the premiere learning destination for Medical School students around the world, currently serving over 30,000 active users (or a third of the total 89,000 medical students in the United States) and an alumni base of 100,000+ students. Sketchy is creating the most engaging and effective educational service for students of higher education everywhere by combining visual storytelling with interactive learning tools that together dramatically enhance recall and knowledge acquisition.', 'Experience with relational SQL and NoSQL databases', 'Great Benefits including 99 % Coverage of Medical, dental, vision. Up to 4% match on 401k and more', 'Able to get into the weeds and propose and implement solutions without hand holding', 'Experience with data pipeline and workflow management tools', 'Create and maintain optimal data pipeline architecture', 'Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources', 'Must have experience in building reports, dashboards, and/or data modeling layers in Looker, Tableau or Mode', 'Fun team events (Monthly and virtual for now)', 'Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL)', 'Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery re-designing infrastructure for greater scalability and usability', '3+ years experience in Data EngineeringAdvanced working SQL knowledge and experience working with relational databases, query authoring (SQL)Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data storesExperience building and optimizing ‘big data’ data pipelines, architectures and data setsStrong analytic skills related to working with unstructured datasetsMust have experience in building reports, dashboards, and/or data modeling layers in Looker, Tableau or ModeExperience with relational SQL and NoSQL databasesExperience with data pipeline and workflow management toolsExperience with GCP servicesExperience with object-oriented/object function scripting languagesSelf-starter who is excited to be part of a growing startup companyAble to get into the weeds and propose and implement solutions without hand holdingAuthorization to work in the U.S. ', 'Competitive compensation plan', ""Bachelor's degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field (Master's degree in a similar field preferred but not required)"", 'Generous PTO package with floating holiday', 'Authorization to work in the U.S. ', 'Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores', 'What We Offer:', 'SketchyGroup LLC is an Equal Opportunity Employer. All applicants will receive consideration without discrimination on the basis of race, religion, color, sex, age, sexual orientation, marital status, national origin, disability or any other basis prohibited by applicable law.', 'Position overview:', 'Experience building and optimizing ‘big data’ data pipelines, architectures and data sets', 'Experience with GCP services', 'We are looking for a Data Engineer who is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The individual will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross-functional teams. You will play an integral role in helping us become more data-aware as a company and enabling data insights across our teams.']",Mid-Senior level,Full-time,Information Technology,Higher Education,2021-03-24 13:05:10
Data Engineer,Clear Channel Outdoor,"San Antonio, TX",6 days ago,Be among the first 25 applicants,"['', 'You are highly capable Data Engineer, having Designed and Built enterprise-class data platforms and integration/API solutionsYou have extensive experience in designing and building frameworks for data Ingestion and Integration Capabilities on AWSYou possess extensive experience in AWS Redshift, EMR, RDS, AWS Glue & AthenaYou have extensive experience in a Big Data technology (Hadoop, AWS Data Lake and Lake House, Map Reduce, Hive, Python, Shell Scripting)Experience in writing complex and efficient SQLExperience in Source Code Management tools like GitHub and Azure DevOpsYou have strong Analytical and Communication Skills, able to translate complex information into distilled and easily consumable conceptsYou have a solid understanding of system design, data structures, and algorithms and understand how to apply them to design pragmatic solutionsYou have experience utilizing cloud services to build, deploy, monitor and scale business critical solutions You have experimented with various development methodologies and practices (Agile/Scrum, Lean/Kanban, CI/CD) to optimize team flow and value delivery You care deeply about end user experience, clean architecture and shipping high-quality outcomesYou learn quickly and enjoy working closely together with your product team to bring value to your stakeholdersYou possess excellent verbal and written communication skills, and the ability to collaborate effectively with geographically distributed teamsYou are passionate about what you do and have a high interest in keeping up with current best-practices in your areas of expertiseYou create a culture of collaboration and a fail fast mentality, growing those around you and yourself through learning and experience', 'You create a culture of collaboration and a fail fast mentality, growing those around you and yourself through learning and experience', 'Participate in all phases of the product development lifecycle, from feature planning and estimation, through design and dev, to deployment and issue resolutionWork with a small strategically focused team to modernize our Data Architecture and ensure high resiliency and high availability of the data used to drive our business forwardProvide technical guidance to teams and leadership, ensuring our strategy and technical execution meet the needs of the businessPartner with others to design, deploy, maintain, and enhance our conceptual Data Lake and Data Consumer ModelProvide Technical Documentation and Diagrams that help inform and visualize your designs for consumption by technical and non-technical members of the CCOA businessEvaluate existing systems and architectures in order to prepare them for migration into a more modern tech stackEmbrace modern, agile software development practices and seek to optimize ways of working as a member of a cross-functional engineering teamContribute to the development of user stories that clarify what needs to be built, for whom, and whyBe accountable for the development and timely delivery of business value through technology and process design and implementationShare knowledge and learn from your team through pair programming and code review, taking and giving feedback appropriatelyEnsure production solutions are scalable, sustainable, architecturally sound and technical debt is repaid in a reasonable timeCollaborate with other teams to solve interesting and challenging problems across the company’s technology ecosystem', 'Location', 'You have experimented with various development methodologies and practices (Agile/Scrum, Lean/Kanban, CI/CD) to optimize team flow and value delivery ', 'Partner with others to design, deploy, maintain, and enhance our conceptual Data Lake and Data Consumer Model', 'About You', 'You have extensive experience in designing and building frameworks for data Ingestion and Integration Capabilities on AWS', 'You care deeply about end user experience, clean architecture and shipping high-quality outcomes', 'Job Summary', 'Evaluate existing systems and architectures in order to prepare them for migration into a more modern tech stack', 'Be accountable for the development and timely delivery of business value through technology and process design and implementation', 'Provide technical guidance to teams and leadership, ensuring our strategy and technical execution meet the needs of the business', 'You have extensive experience in a Big Data technology (Hadoop, AWS Data Lake and Lake House, Map Reduce, Hive, Python, Shell Scripting)', 'Provide Technical Documentation and Diagrams that help inform and visualize your designs for consumption by technical and non-technical members of the CCOA business', 'You have strong Analytical and Communication Skills, able to translate complex information into distilled and easily consumable concepts', 'You possess excellent verbal and written communication skills, and the ability to collaborate effectively with geographically distributed teams', 'What You’ll Do', 'You possess extensive experience in AWS Redshift, EMR, RDS, AWS Glue & Athena', 'Ensure production solutions are scalable, sustainable, architecturally sound and technical debt is repaid in a reasonable time', 'Experience in writing complex and efficient SQL', 'Embrace modern, agile software development practices and seek to optimize ways of working as a member of a cross-functional engineering team', 'You are passionate about what you do and have a high interest in keeping up with current best-practices in your areas of expertise', 'Share knowledge and learn from your team through pair programming and code review, taking and giving feedback appropriately', 'Experience in Source Code Management tools like GitHub and Azure DevOps', 'You learn quickly and enjoy working closely together with your product team to bring value to your stakeholders', 'You have a solid understanding of system design, data structures, and algorithms and understand how to apply them to design pragmatic solutions', 'You have experience utilizing cloud services to build, deploy, monitor and scale business critical solutions ', 'Participate in all phases of the product development lifecycle, from feature planning and estimation, through design and dev, to deployment and issue resolution', 'Collaborate with other teams to solve interesting and challenging problems across the company’s technology ecosystem', 'Work with a small strategically focused team to modernize our Data Architecture and ensure high resiliency and high availability of the data used to drive our business forward', 'Contribute to the development of user stories that clarify what needs to be built, for whom, and why', 'You are highly capable Data Engineer, having Designed and Built enterprise-class data platforms and integration/API solutions']",Entry level,Full-time,Information Technology,Construction,2021-03-24 13:05:10
Data Engineer,Cricut,"San Francisco, CA",5 days ago,62 applicants,"['', 'Experience in Kanban methodologies', 'Experience in Test Driven Development and CI/CD.', 'CS or CE degree or commensurate experience requiredMS SQL Server, MySQL (Aurora), MSSQL, REST API, etc.Strong understanding of scalability, performance, and reliabilityExperience with OOP frameworks, languages, design patterns, concepts and data sources such as C#/.NET, Java, Python, Kafka, SparkAbility to work on multiple areas including data pipeline ETL, data modeling & design, writing complex SQL queries, etc.', 'Hands-on expertise in one or more Amazon Web Services (AWS) technologies, such as Kinesis, S3, Redshift, Athena, Lambda', 'Job Description', 'Ability to work on multiple areas including data pipeline ETL, data modeling & design, writing complex SQL queries, etc.', 'MS SQL Server, MySQL (Aurora), MSSQL, REST API, etc.', 'Equal Opportunity: Cricut is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees and members. Applicants will be considered based on their qualifications and without regards to age, race, ethnicity, gender identity or expression, national origin, religion, physical or mental disability, protected veteran states, sex (including pregnancy), sexual orientation or any other protected characteristic protected by applicable laws, regulations or ordinances. ', 'Strong understanding of scalability, performance, and reliability', 'Preferred Skills', 'Optimize data flow and data collection for cross functional teams', 'Experience with OOP frameworks, languages, design patterns, concepts and data sources such as C#/.NET, Java, Python, Kafka, Spark', 'Hands-on expertise in one or more Amazon Web Services (AWS) technologies, such as Kinesis, S3, Redshift, Athena, LambdaExperience in Kanban methodologiesExperience in Test Driven Development and CI/CD.Demonstrated ability to develop and support large-sized international-scale software systemsExperience in expanding and optimizing data pipeline architectureOptimize data flow and data collection for cross functional teams', 'Qualifications', 'ADA: If you require reasonable accommodation during the application or selection process please do not hesitate to reach out to Cricut HR or your assigned recruiter.', 'CS or CE degree or commensurate experience required', 'Demonstrated ability to develop and support large-sized international-scale software systems', 'Experience in expanding and optimizing data pipeline architecture', 'Equal Opportunity: Cricut is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees and members. Applicants will be considered based on their qualifications and without regards to age, race, ethnicity, gender identity or expression, national origin, religion, physical or mental disability, protected veteran states, sex (including pregnancy), sexual orientation or any other protected characteristic protected by applicable laws, regulations or ordinances. ADA: If you require reasonable accommodation during the application or selection process please do not hesitate to reach out to Cricut HR or your assigned recruiter.']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer,Obsidian Security,"Newport Beach, CA",1 week ago,Be among the first 25 applicants,"['', 'Participate in the engineering life-cycle at Obsidian, including designing distributed systems, writing production code, conducting code reviews and working alongside our infrastructure and reliability teams.', 'BS/MS Computer Science or a highly quantitative discipline (MS Degree preferred)Ability with the following programming languages: Scala and PythonExtensive experience with the following software packages: SQL, NoSQL, Apache products (Spark, Hive, Airflow, Hadoop, etc.) and graph databasesFamiliarity with DevOps and AWS (S3, EMR, EC2 are a must)Successfully delivered and maintained a major cloud service with a large number of end users.Cybersecurity experience is a plus', 'You’re an engineer who is experienced designing, implementing and supporting new data processing, data sets and systems to support various advanced analytics needs from multiple sources.You enjoy solving large-scale problems and are comfortable doing incremental quality work while building brand new systems to enable future quality improvements', 'Work in a big data ecosystem to develop and maintain an analytics pipeline for acquisition, storage, and processing data types of interest to feed real-time artificial intelligent system behaviors.', 'Collaborate with the existing engineering team, data scientists, analysts and the business to understand requirements and data needs and definitions.', 'Familiarity with DevOps and AWS (S3, EMR, EC2 are a must)', 'Cybersecurity experience is a plus', 'Ability with the following programming languages: Scala and Python', 'Support and build well-engineered data systems to support analytical needs for cloud based systems (AWS) and deliver data via APIs', 'Extensive experience with the following software packages: SQL, NoSQL, Apache products (Spark, Hive, Airflow, Hadoop, etc.) and graph databases', 'Successfully delivered and maintained a major cloud service with a large number of end users.', 'You enjoy solving large-scale problems and are comfortable doing incremental quality work while building brand new systems to enable future quality improvements', 'Develop or implement tools to support analyst-driven machine learning analyses.', 'BS/MS Computer Science or a highly quantitative discipline (MS Degree preferred)', 'You’re an engineer who is experienced designing, implementing and supporting new data processing, data sets and systems to support various advanced analytics needs from multiple sources.', 'Participate in the engineering life-cycle at Obsidian, including designing distributed systems, writing production code, conducting code reviews and working alongside our infrastructure and reliability teams.Work in a big data ecosystem to develop and maintain an analytics pipeline for acquisition, storage, and processing data types of interest to feed real-time artificial intelligent system behaviors.Develop or implement tools to support analyst-driven machine learning analyses.Support and build well-engineered data systems to support analytical needs for cloud based systems (AWS) and deliver data via APIsCollaborate with the existing engineering team, data scientists, analysts and the business to understand requirements and data needs and definitions.']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer / Software Engineer,Cerner Corporation,"Kansas City, MO",2 days ago,35 applicants,"['', ' Expectations : ', 'Qualifications', ' Experience with Java, Python and Scala ', ' Bachelors degree in Computer Science, Computer Engineering or Information Systems or related field, or equivalent relevant work experience ', ' 1 year of Big data or cloud technology work experience including data analysis, data ingestion, data modeling and/or machine learning ', ' Must be currently residing in or willing to relocate to the Kansas City metro area', ' 3 years of Software engineering work experience ', ' Preferred Qualifications : ', ' Experience working with big data and big data technologies ', ' Bachelors degree in Computer Science, Computer Engineering or Information Systems or related field, or equivalent relevant work experience  3 years of Software engineering work experience  1 year of Big data or cloud technology work experience including data analysis, data ingestion, data modeling and/or machine learning ', ' Basic Qualifications : ', ' Experience with Java, Python and Scala  Experience working with big data and big data technologies ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer- 100% Remote,Emvia,US Virgin Islands,2 weeks ago,Be among the first 25 applicants,"['', 'No Prior Experience In The Energy Industry Required']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer (Python),TalentPartners,United States,,N/A,"[' Strong proficiency in Python\xa0with an emphasis in building data pipelines', ' ', ' knowledge of Application Programming Interfaces', 'Required Qualifications:', ' Experience with Atlassian products Jira and Confluence', 'Experience with version control systems (Git and Bitbucket)', ' Bachelor’s Degree in Computer Science or a related discipline', 'Nice to have:', ' Detail-oriented and document all the work', ' Nice to have:', ' Experience with Docker containerization', ' 5+ years of applicable engineering experience', ' Experience with Apache Airflow or Google Composer', ' Ability to write complex SQL to perform common types of analysis and aggregations\xa0', ' Ability to work with others from diverse skill-sets and backgrounds']",Mid-Senior level,Full-time,Information Technology,Entertainment,2021-03-24 13:05:10
Data Engineer,Wunderman Thompson Technology,"Dallas, TX",4 weeks ago,Be among the first 25 applicants,"['', 'Experience with Snowflake, Oracle, schema design in cloud environment.', 'Strong experience with structure data warehouse design and management', 'Who you are:', 'What you’ll need:', 'Build data engineering solutions | Build new enterprise level products, including, our analytical mart for use across Wunderman Thompson. This includes data wrangling, feature engineering, as well as formulation of required features for analytical consumption. ', 'Ambitious | Willing to take calculated risks, stretch yourself and your team to do new things vs. plugging into existing solutions.', 'Strong experience in building data pipeline in cloud, data integration, data processing, data standardization, data mining/wrangling', 'Cutting edge technology |', 'Collaborate |', 'What you’ll do:', 'Humble |', 'Production level PL/SQL scripting, bash scripting.', 'Minimum 2 years of experience deploying enterprise level data engineering solutions in cloud environment.', ' Data engineering and data science skill set with grounding in practical marketing applications oriented towards content, customer insights and customer experience in a digital marketing-heavy environment. Minimum 2 years of experience deploying enterprise level data engineering solutions in cloud environment. Strong experience with structure data warehouse design and management Experience with Snowflake, Oracle, schema design in cloud environment. Production level PL/SQL scripting, bash scripting. Production level experience with Python, Spark. Version Control – Git Proven track record on cloud-based infrastructure: AWS (required), IBM, GCP, Azure Strong experience in building data pipeline in cloud, data integration, data processing, data standardization, data mining/wrangling ', 'Passionate | You take great pride in your work. You approach our own and our clients’ business challenges with enthusiasm and a commitment to getting it right. You love working in health. You see data science as a way of expressing creativity.', 'Create | Build and implement a flexible data mart solution in python that enables auto build cycles and flexible runs to create data marts on the fly.', 'We rely on legitimate interest as a legal basis for processing personal information under the GDPR for purposes of recruitment and applications for employment.', 'Cutting edge technology | Cloud based engineering to manage and deploy data pipe-lines end to end from problem formulation, raw data to implementation and optimization.', 'Humble | Wear any hat that needs to be worn, and you know you do not know everything. You want to learn from others. ', 'Version Control – Git', 'Ambitious |', 'Production level experience with Python, Spark.', 'Who we are looking for:', ' Build data engineering solutions | Build new enterprise level products, including, our analytical mart for use across Wunderman Thompson. This includes data wrangling, feature engineering, as well as formulation of required features for analytical consumption.  Create | Build and implement a flexible data mart solution in python that enables auto build cycles and flexible runs to create data marts on the fly. Collaborate | You will be an active participant in the Wunderman Thompson data science community where best practices are shared, innovations are hatched, and cross-vertical collaboration with the product, data science and delivery teams. Cutting edge technology | Cloud based engineering to manage and deploy data pipe-lines end to end from problem formulation, raw data to implementation and optimization. ', 'Curious |', 'Collaborate | You will be an active participant in the Wunderman Thompson data science community where best practices are shared, innovations are hatched, and cross-vertical collaboration with the product, data science and delivery teams.', 'Build data engineering solutions |', 'Create |', 'Data engineering and data science skill set with grounding in practical marketing applications oriented towards content, customer insights and customer experience in a digital marketing-heavy environment.', 'Proven track record on cloud-based infrastructure: AWS (required), IBM, GCP, Azure', 'Curious | With an inquisitive mindset you embrace the unknown and see as an opportunity to explore and innovate.', 'Who we are:', 'Passionate |', ' Curious | With an inquisitive mindset you embrace the unknown and see as an opportunity to explore and innovate. Ambitious | Willing to take calculated risks, stretch yourself and your team to do new things vs. plugging into existing solutions. Passionate | You take great pride in your work. You approach our own and our clients’ business challenges with enthusiasm and a commitment to getting it right. You love working in health. You see data science as a way of expressing creativity. Humble | Wear any hat that needs to be worn, and you know you do not know everything. You want to learn from others.  ']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer,HighPoint Global,"Washington, DC",6 days ago,Be among the first 25 applicants,"['', 'Job Responsibilities', ' Implement, architect, and administer a JupyterHub server environment. Facilitate major machine learning projects from conception to conclusion. Collect requirements, plan implementations and integrate data sources in collaboration with stakeholders. Create understanding and build consensus with stakeholders to define project security, data architecture and operational requirements. Demonstrate strong oral and written communication skills, with the ability to communicate on technical and methodological matters with audiences of various expertise. Work onsite 100%', ' Collect requirements, plan implementations and integrate data sources in collaboration with stakeholders.', ' Positive attitude, attention to detail and great problem-solving skills', ' Anaconda, SQL Server, RStudio, Spyder experience preferred', ' Experience with scripting tools such as Python, unix shell scripts', ' Ability to obtain position of Public Trust designation', ' Significant experience with the following languages and tools: Python, R, scikit-learn', ' Demonstrate strong oral and written communication skills, with the ability to communicate on technical and methodological matters with audiences of various expertise.', 'Knowledge And Skills Requirements', ' Experience with big data platforms (Hadoop, Spark, etc)', ' Create understanding and build consensus with stakeholders to define project security, data architecture and operational requirements.', ' Facilitate major machine learning projects from conception to conclusion.', 'About Highpoint', ' Significant experience with JupyterHub, Python and R languages', ' Ability to adjust to changes in requirements, scope and direction', "" Bachelor's degree in Computer Science, Statistics, related field or equivalent experience"", ' Ability to start, manage and conclude projects independently', ' Experience with cloud service providers (AWS, Azure)', ' Implement, architect, and administer a JupyterHub server environment.', ' Should be well organized, thorough, and able to handle competing priorities', ' Work onsite 100%', 'Education And Years Of Experience Requirements', ' General knowledge of machine learning techniques and methodologies', ' Significant experience with JupyterHub, Python and R languages Significant experience with the following languages and tools: Python, R, scikit-learn Anaconda, SQL Server, RStudio, Spyder experience preferred Experience with scripting tools such as Python, unix shell scripts Experience with big data platforms (Hadoop, Spark, etc) Experience with cloud service providers (AWS, Azure) General knowledge of machine learning techniques and methodologies Should be well organized, thorough, and able to handle competing priorities Ability to start, manage and conclude projects independently Ability to adjust to changes in requirements, scope and direction Positive attitude, attention to detail and great problem-solving skills Experience in financial industry a plus Ability to obtain position of Public Trust designation', ' Experience in financial industry a plus']",Entry level,Full-time,Information Technology,Management Consulting,2021-03-24 13:05:10
Data Engineer,Root Inc.,"Columbus, OH",4 weeks ago,45 applicants,"['', 'Design solutions which help us to reach our overall goals', 'Solid SQL skills. Ability to transform data without the use of an ETL tool.', 'Our Tech Stack Includes', 'At least 3 years of experience in the insurance industry is strongly preferred', 'The Company:', 'The Team: ', 'Provide peer review for teammates on their change requests', 'At least 3 years of experience in the insurance industry is strongly preferredExperience using technologies listed above is preferredSolid SQL skills. Ability to transform data without the use of an ETL tool.Experience using version control tools like GITFamiliarity with programming languages like Ruby or PythonFamiliarity with DevOps & Agile processes', 'Work with product, actuarial, and engineering teams to understand and scope new features for our environment', 'Familiarity with programming languages like Ruby or Python', 'Help to ensure data quality and meet data delivery SLA’s', 'Familiarity with DevOps & Agile processes', 'Responsibilities', 'Design & develop sustainable, fast ETL processes using SQL', 'Experience using technologies listed above is preferred', 'Qualifications', 'Design & develop data structures that support downstream analysis', 'Create processes to identify, prioritize, and illustrate data quality issues and remediation efforts.', 'Our Progress:', 'Work with product, actuarial, and engineering teams to understand and scope new features for our environmentDesign & develop data structures that support downstream analysisDesign & develop sustainable, fast ETL processes using SQLProvide peer review for teammates on their change requestsCreate processes to identify, prioritize, and illustrate data quality issues and remediation efforts.Design solutions which help us to reach our overall goalsHelp to ensure data quality and meet data delivery SLA’s', 'Experience using version control tools like GIT']",Entry level,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Data Engineer,Veterans United Home Loans,"Columbia, MO",4 weeks ago,Be among the first 25 applicants,"['', 'Familiarity with Agile methodologies.', 'Experience with Team Foundation Services or other change management software.', 'Strong interpersonal communication skills (written and verbal).', 'Experience working with, or understanding of, document databases (MongoDB, CouchDB, etc).', 'Job Duties Will Vary Upon Assignment But May Include', 'Constantly strive to improve data accuracy and organization.', 'Partner with Product Owners and other Data Engineers to refine business requirements.', 'Careers', ' Careers ', 'Experience with a programming language(C#, Java, Python, etc).', ' Glassdoor  Careers ', 'Learn More', 'Use the latest tools to build data solutions that deliver the highest quality data to stakeholders.', 'Glassdoor', 'Support, develop and maintain technical specification documentation.', 'Thrive in a highly collaborative work setting.', ' Glassdoor ', 'Ability to manage time and multitask.Strong interpersonal communication skills (written and verbal).Proven ability to learn new concepts, tools, languages and techniques quickly.Excel in fast-paced, results-oriented environment.Thrive in a highly collaborative work setting.Familiarity with Agile methodologies.Experience collaborating and gathering requirements from both technical and business-facing teams.Proficiency in at least one ETL toolset (ideally, SSIS)Experience with Big Data technologies (Hadoop, Spark, Hive, Oozie, Presto, Kafka, etc.);Experience with a programming language(C#, Java, Python, etc).Experience working with, or understanding of, document databases (MongoDB, CouchDB, etc).Familiarity and experience with ERWIN or similar data modeling software.Experience with Team Foundation Services or other change management software.Proficient in developing complex SQL queries and designing efficient databases and tables in a relational DBMS (Microsoft SQL Server and MySQL preferred).Proven background of ETL development, including data warehousing and reporting architectures.Working knowledge of common data visualization/reporting tools (Tableau, Power BI, etc.).', 'Working knowledge of common data visualization/reporting tools (Tableau, Power BI, etc.).', 'Ability to manage time and multitask.', 'Proven ability to learn new concepts, tools, languages and techniques quickly.', 'Thoroughly and quickly analyze business requirements, translating them into a good technical data design, including conceptual, logical, and physical data models.Partner with Product Owners and other Data Engineers to refine business requirements.Use the latest tools to build data solutions that deliver the highest quality data to stakeholders.Support, develop and maintain technical specification documentation.Constantly strive to improve data accuracy and organization.', 'Thoroughly and quickly analyze business requirements, translating them into a good technical data design, including conceptual, logical, and physical data models.', 'Experience with Big Data technologies (Hadoop, Spark, Hive, Oozie, Presto, Kafka, etc.);', 'Proficient in developing complex SQL queries and designing efficient databases and tables in a relational DBMS (Microsoft SQL Server and MySQL preferred).', 'Experience collaborating and gathering requirements from both technical and business-facing teams.', 'Proven background of ETL development, including data warehousing and reporting architectures.', 'Excel in fast-paced, results-oriented environment.', 'Familiarity and experience with ERWIN or similar data modeling software.', 'Proficiency in at least one ETL toolset (ideally, SSIS)']",Entry level,Full-time,Information Technology,Financial Services,2021-03-24 13:05:10
Data Engineer,Hagerty,"Traverse City, MI",2 weeks ago,Be among the first 25 applicants,"['', 'Implement data engineering best practicesDevelop and implement robust and scalable data integration (ETL) pipelines using Python, SQL, Spark, and other AWS/Salesforce cloud solutions.Develop and implement data pipeline orchestration utilities using ApacheSupport AWS platform DevOps best practices throughout all data engineeringCreate and manage AWS resources using infrastructure-as-code bestpractices, specifically in terraform.Partner with internal and external stakeholders to collect requirements and recommend best practice solutions.Develop solutions to catalog and manage metadata to support data governance and data democratization.Develop and implement automated test cases and data reconciliation to validate ETL processes and data quality & integrity.Partner with Data Scientists to design, code, train, test, deploy and iterate machine learning algorithms and systems at scale.', 'Competitive compensation', 'Associates degree, preferably in a technical/analytical field, or relevant workAdditional 3+ years working in another role within an IT delivery team, such as a developer, business systems analyst, data analyst, quality assurance analyst, ETL developer or DBAStrong problem-solving abilities and attention to detailAbility to authentically and effectively communicate (written and verbally) with various stakeholdersAbility to create technical artifacts and documentation to support development and maintenance of data productsExperience in successful delivery of data products as productionizable software solutionsExperience or willingness to learn open source data processing technologies such as Kafka, Hadoop, Hive, Presto, Spark, GraphXExperience ensuring rigorous code development, testing, automation, and other engineering best practices.Experience in imperative (e.g., Airflow) or declarative (e.g., Informatica/Talend/Pentaho) ETL design, implementation, and maintenance.Experience cataloging and processing non-relational data.Experience or willingness to learn one or multiple of the following languages Python, Scala, or SQLFunctional knowledge of relational databases and query authoring (SQL)Experience or willingness to learn productionizing data science models in frameworks such as numpy, ML Spark, pandas, scikit-learn, tensorflow, MOA, mlpack, etc.Preferred experience in machine learning techniques such as feature engineering, features selection, supervised and unsupervised algorithms, clustering, graph analytics, and time series analysis, K-means clustering, Gaussian distribution, decision tree, etc.', 'Regular recognition, feedback, and open communication across all levels', 'Experience cataloging and processing non-relational data.', 'Experience in successful delivery of data products as productionizable software solutions', 'Experience in imperative (e.g., Airflow) or declarative (e.g., Informatica/Talend/Pentaho) ETL design, implementation, and maintenance.', 'Experience or willingness to learn one or multiple of the following languages Python, Scala, or SQL', 'To apply for this position please visit our Career site at careers.hagerty.com', 'Inclusive benefits package allowing enrollment of dependents and partners', 'Develop and implement data pipeline orchestration utilities using Apache', 'Partner with Data Scientists to design, code, train, test, deploy and iterate machine learning algorithms and systems at scale.', 'Ability to authentically and effectively communicate (written and verbally) with various stakeholders', 'Strong problem-solving abilities and attention to detail', 'Develop and implement robust and scalable data integration (ETL) pipelines using Python, SQL, Spark, and other AWS/Salesforce cloud solutions.', 'Develop and implement automated test cases and data reconciliation to validate ETL processes and data quality & integrity.', 'Support AWS platform DevOps best practices throughout all data engineering', 'This Might Describe You:', 'Company supported and employee-driven resource groups that promote diversity, career development and empowerment', 'A flexible culture that understands the importance of quality of work over quantity', ""Hagerty, an automotive lifestyle company and the world's pre-eminent membership, insurance and media organization for enthusiast vehicle owners, has an opportunity for a Data Engineer to work on our data science team. In this role you will be building and maintaining our data pipeline and scalable analytics platform. You will also be partnering with other technical and business stakeholders to develop and productionize data science models."", 'An opportunity to work with a diverse, global community of 1000+ Hagerty team members across multiple countries, united by our values of open, direct, and kind', 'Team building, bonding, mentorship and support to grow confidence, trust, and friendships', 'Experience or willingness to learn open source data processing technologies such as Kafka, Hadoop, Hive, Presto, Spark, GraphX', 'Implement data engineering best practices', ' Data Engineer', 'Associates degree, preferably in a technical/analytical field, or relevant work', 'Preferred experience in machine learning techniques such as feature engineering, features selection, supervised and unsupervised algorithms, clustering, graph analytics, and time series analysis, K-means clustering, Gaussian distribution, decision tree, etc.', 'Additional 3+ years working in another role within an IT delivery team, such as a developer, business systems analyst, data analyst, quality assurance analyst, ETL developer or DBA', 'Ability to create technical artifacts and documentation to support development and maintenance of data products', 'At Hagerty, we’re focused on building a world-class company and culture, and that starts with the people we hire. We take pride in being an equal opportunity, inclusive employer', 'Functional knowledge of relational databases and query authoring (SQL)', 'Competitive compensationInclusive benefits package allowing enrollment of dependents and partnersA flexible culture that understands the importance of quality of work over quantityAn opportunity to work with a diverse, global community of 1000+ Hagerty team members across multiple countries, united by our values of open, direct, and kindA culture of company-wide collaboration and shared successCompany supported and employee-driven resource groups that promote diversity, career development and empowermentCorporate social responsibility initiatives with global reachRegular recognition, feedback, and open communication across all levelsTeam building, bonding, mentorship and support to grow confidence, trust, and friendshipsAt Hagerty, we’re focused on building a world-class company and culture, and that starts with the people we hire. We take pride in being an equal opportunity, inclusive employer', 'Experience or willingness to learn productionizing data science models in frameworks such as numpy, ML Spark, pandas, scikit-learn, tensorflow, MOA, mlpack, etc.', 'Partner with internal and external stakeholders to collect requirements and recommend best practice solutions.', 'What You’ll Do:', 'Experience ensuring rigorous code development, testing, automation, and other engineering best practices.', 'Develop solutions to catalog and manage metadata to support data governance and data democratization.', 'Corporate social responsibility initiatives with global reach', 'A culture of company-wide collaboration and shared success', 'This position can be based remotely, or located in our Traverse City, Ann Arbor, Michigan or Dublin, OH offices.', 'We Offer:', 'Create and manage AWS resources using infrastructure-as-code bestpractices, specifically in terraform.']",Mid-Senior level,Full-time,Engineering,Automotive,2021-03-24 13:05:10
Data Engineer,Home Chef,"Chicago, IL",6 days ago,47 applicants,"['', ' Comprehensive Medical, Dental, and Vision Insurance – benefits start the 1st day of the month following your start date', 'Design optimal pipelines and architecture to meet data needs of entire organization', 'Multiple years of experience working in data engineering or adjacent fields', ' Great Work/Life Balance – We value and support each individual team member', ' Generous Parental Leave', ' Your choice of Windows or Mac laptop, plus an extra screen', ' Weekly Fooda credit', 'Strong verbal and written communication skills', 'Knowledge share with the rest of our internal data and analytics team about the various tools and systems associated with our current data infrastructure', ' Company paid Life Insurance, Short Term Disability and Long Term Disability', 'Knowledge share with the rest of our internal data and analytics team about the various tools and systems associated with our current data infrastructureWork with internal and external teams to identify pain points and areas of improvement with regards to our current data processes', ' Flexible paid time off (PTO) policy, plus sick days', ' Onsite gym', 'Organized both in terms of work management and written code', 'Identify areas of opportunity to save time or improve reliability in data processes throughout the organization', ' Newly renovated office in the historic Old Main Post Office which is located close to multiple Metra and CTA options', ' We offer flexible spending accounts (FSA) for qualified Medical, Dependent Care, Parking, or Transit expenses', ' Designing Pipelines And Processes ', 'Design processes and explore tooling to make the lives of our data developers a whole lot easier', 'Implement these data infrastructure changes through code and frameworks', 'Design optimal pipelines and architecture to meet data needs of entire organizationImplement these data infrastructure changes through code and frameworksDesign processes and explore tooling to make the lives of our data developers a whole lot easierMonitor performance of our current data infrastructure and push through sustainable fixes/improvements as neededIdentify areas of opportunity to save time or improve reliability in data processes throughout the organization', 'Work with internal and external teams to identify pain points and areas of improvement with regards to our current data processes', ' Employee discounts through Perkspot', 'High degree of familiarity with Python and command line interfaces', 'Understanding of the fundamentals of cloud storage and cloud computing', 'Multiple years of experience working in data engineering or adjacent fieldsHigh degree of familiarity with Python and command line interfacesComfort level working with version control such as GitHubUnderstanding of the fundamentals of cloud storage and cloud computingStrong verbal and written communication skillsOrganized both in terms of work management and written codeCompletion of a four-year degree program in a related field', ' Quarterly company-wide “Town Hall” meetings', "" 401k Employer match - 50% on the $1, up to 6% of the employee's earnings"", ' Casual dress in a fun, friendly and collaborative work environment', 'Comfort level working with version control such as GitHub', ' Ongoing professional development opportunities by level and function', ' Discounts on Home Chef meal kits and at Kroger stores', 'Monitor performance of our current data infrastructure and push through sustainable fixes/improvements as needed', 'Completion of a four-year degree program in a related field']",Entry level,Full-time,Information Technology,Food & Beverages,2021-03-24 13:05:10
Data Engineer,"Ursus, Inc. ","Sunnyvale, CA",4 weeks ago,Over 200 applicants,"['', 'Summary:', 'Minimum Qualifications', '1-7 years experience using python, cyclone or java', 'Location: Sunnyvale, CA/Remote', 'BS/BA in Technical Field, Computer Science or Mathematics.1-7 years experience in the data warehouse space.1-7 years experience in custom ETL design, implementation and maintenance.1-7 years experience in writing SQL statements.1-7 years experience using python, cyclone or javaAbility to analyze data to identify deliverables, gaps and inconsistencies.Communication skills including the ability to identify and communicate data driven insights.Ability in helping engineering with managing and communicating data warehouse plans to internal clients.', 'Ability in helping engineering with managing and communicating data warehouse plans to internal clients.', 'BS/BA in Technical Field, Computer Science or Mathematics.', 'Ability to analyze data to identify deliverables, gaps and inconsistencies.', 'Support data engineers with variety of data projects ranging from ETL to building dashboards to analysisInterface with engineers, product managers and product analysts to understand data needs.Build data expertise and own data quality for allocated areas of ownership.Design, build and launch new data extraction, transformation and loading processes in production.Support existing processes running in production', 'Support data engineers with variety of data projects ranging from ETL to building dashboards to analysis', 'Duration: 6 Months', 'Communication skills including the ability to identify and communicate data driven insights.', 'Support existing processes running in production', '1-7 years experience in writing SQL statements.', '1-7 years experience in the data warehouse space.', 'Design, build and launch new data extraction, transformation and loading processes in production.', 'Job Title: Data Engineer - ETL design', 'Interface with engineers, product managers and product analysts to understand data needs.', 'Build data expertise and own data quality for allocated areas of ownership.', '1-7 years experience in custom ETL design, implementation and maintenance.']",Mid-Senior level,Contract,Information Technology,Computer Software,2021-03-24 13:05:10
Data Engineer,The Judge Group,"Fort Washington, PA",3 weeks ago,Be among the first 25 applicants,"['', 'Design and develop ETL processes and data structures for the Data Warehouse following best practice procedures. Job Qualifications: Education ', 'Experience with integration with 3rd party application using Python connector api ', 'Experience in B2C, Martech, and CDP technologies and environments. ', 'Experience in ETL and ELT workflow management (Fivetran, DBT, Matillion, Snowflake Snowpipe, Stored Procedures, and Streams a plus). ', 'Familiarity with AWS Data and Analytics technologies such as Glue, Athena, Spectrum, Data Pipeline. ', '3+ years of experience with API based ELT automation framework, data manager, interface design, development, and maintenance. ', 'Supporting the Data Warehouse strategy and Business Intelligence initiatives. ', 'Location: ', 'Expert knowledge in Snowflake cloud-based data warehouse. ', 'Demonstrate capabilities to provide continuous improvements and optimizations to both cost and performance on cloud-based technologies. ', 'Contact:', 'AWS cloud experience (EC2, S3, Lambda, SQS) ', 'Hands on experience with SQL Server 2016, SSIS, SSAS. ', 'Qualifications & Requirements', ' Data Engineer ', '7+ years experience as a Data Engineer. ', 'Perform ad hoc data analysis to meet business unit data validation needs. ', '  Data Engineer 7+ years experience as a Data Engineer. 5+ years of experience driving adoption and automation of data management services and tools. 3+ years of experience with API based ELT automation framework, data manager, interface design, development, and maintenance. Expert knowledge in Snowflake cloud-based data warehouse. AWS cloud experience (EC2, S3, Lambda, SQS) Experience in ETL and ELT workflow management (Fivetran, DBT, Matillion, Snowflake Snowpipe, Stored Procedures, and Streams a plus). Demonstrate capabilities to provide continuous improvements and optimizations to both cost and performance on cloud-based technologies. Familiarity with AWS Data and Analytics technologies such as Glue, Athena, Spectrum, Data Pipeline. Experience with integration with 3rd party application using Python connector api Strong knowledge of Big Data concepts and working with both structured and unstructured data. Experience in migrations from SQL Server and Postgresql to Snowflake. Experience in B2C, Martech, and CDP technologies and environments. Hands on experience with SQL Server 2016, SSIS, SSAS. Integrate/export data following security guidelines. Supporting the Data Warehouse strategy and Business Intelligence initiatives. Perform ad hoc data analysis to meet business unit data validation needs. Design and develop ETL processes and data structures for the Data Warehouse following best practice procedures. Job Qualifications: Education Bachelor’s degree preferred.  Contact: emeltzer@judge.comThis job and many more are available through The Judge Group. Find us on the web at www.judge.com', 'Bachelor’s degree preferred. ', 'Experience in migrations from SQL Server and Postgresql to Snowflake. ', 'This job and many more are available through The Judge Group. Find us on the web at www.judge.com', '5+ years of experience driving adoption and automation of data management services and tools. ', 'Integrate/export data following security guidelines. ', 'Strong knowledge of Big Data concepts and working with both structured and unstructured data. ', 'Description: ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Advanced Tech Placement,"Atlanta, GA",,N/A,"['Description:', '', 'Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.\xa0', 'Create and maintain optimal data pipeline architecture.\xa0', 'Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.\xa0', 'Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.\xa0', 'Advanced working SQL knowledge and experience working with relational\xa0and non-relational\xa0databases, query authoring (SQL) as well as working familiarity with a variety of databases.\xa0', 'Work with data and analytics experts to strive for greater functionality in our data systems.\xa0', 'Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and ‘big data’ technologies.\xa0', 'Create and maintain optimal data pipeline architecture.\xa0Assemble large, complex data sets that meet functional/non-functional business requirements.\xa0Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\xa0Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and ‘big data’ technologies.\xa0Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.\xa0Work with stakeholders including the Executive, Product,\xa0Development,\xa0Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.\xa0Keep\xa0data separate and secure across boundaries through\xa0different\xa0regions.\xa0Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.\xa0Work with data and analytics experts to strive for greater functionality in our data systems.\xa0', 'Experience with stream-processing systems: Storm, Spark-Streaming, etc.\xa0', 'Strong\xa0communication\xa0project management and organizational skills.\xa0', 'Experience with AWS cloud services: EC2, EMR, RDS\xa0and\xa0Redshift\xa0', '\ufeffQualifications:', 'Experience building and optimizing data pipelines, architectures and data sets.\xa0', 'Assemble large, complex data sets that meet functional/non-functional business requirements.\xa0', 'Experience with data tools:\xa0Spark/Scala.\xa0', 'Responsibilities for Data Engineer:', 'Strong analytic skills related to working with unstructured datasets.\xa0', 'Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.\xa0', 'Work with stakeholders including the Executive, Product,\xa0Development,\xa0Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.\xa0', 'We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has\xa0obtained a\xa0Bachelor’s\xa0degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.\xa0They\xa0should also have experience using the following software/tools:\xa0', 'Keep\xa0data separate and secure across boundaries through\xa0different\xa0regions.\xa0', 'A successful history of manipulating, processing and extracting value from large disconnected datasets.\xa0', 'Working knowledge of message queuing, stream processing, and highly scalable data stores.\xa0', 'Experience with relational SQL and NoSQL databases, including\xa0Snowflake\xa0and\xa0MariaDB AX.\xa0', 'Experience supporting and working with cross-functional teams in a dynamic environment.\xa0', 'Build processes supporting data transformation, data structures, metadata, dependency and workload management.\xa0', 'We are looking for savvy Data Engineers to join our growing team of experts. This role is responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data engineer and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. The ideal candidate must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives. Strong candidates will thrive in the collaborative environment while also being strong individual contributors with a proactive mindset.', 'Advanced working SQL knowledge and experience working with relational\xa0and non-relational\xa0databases, query authoring (SQL) as well as working familiarity with a variety of databases.\xa0Experience building and optimizing data pipelines, architectures and data sets.\xa0Strong analytic skills related to working with unstructured datasets.\xa0Build processes supporting data transformation, data structures, metadata, dependency and workload management.\xa0A successful history of manipulating, processing and extracting value from large disconnected datasets.\xa0Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.\xa0Working knowledge of message queuing, stream processing, and highly scalable data stores.\xa0Strong\xa0communication\xa0project management and organizational skills.\xa0Experience supporting and working with cross-functional teams in a dynamic environment.\xa0We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has\xa0obtained a\xa0Bachelor’s\xa0degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.\xa0They\xa0should also have experience using the following software/tools:\xa0Experience with data tools:\xa0Spark/Scala.\xa0Experience with relational SQL and NoSQL databases, including\xa0Snowflake\xa0and\xa0MariaDB AX.\xa0Experience with AWS cloud services: EC2, EMR, RDS\xa0and\xa0Redshift\xa0Experience with stream-processing systems: Storm, Spark-Streaming, etc.\xa0Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.\xa0', 'Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\xa0']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Zennify,"Sacramento, CA",2 weeks ago,145 applicants,"['', 'Significant experience with SOAP & REST API Integration, data, and security', 'Significant experience with writing unit tests', 'Ability to work independently and be a self-starter', 'Examine and identify database structural necessities by evaluating client operations, applications, and programming', 'Bachelor’s degree in Computer Science, Information Systems,\xa0 or relevant field strongly preferred', 'Lead and mentor the data team on the project', 'Possesses strong knowledge of data modeling principles and best practices', 'Perform and/or lead project data consultants in dry run data load testing', 'Thrives in a team-based, high energy and fast-paced environmentService-oriented and innately driven to produce outstanding customer satisfaction and resultsEnjoys discovering, learning about and implementing new technologiesAnalytical and able to logically and methodically work through problemsStrong aptitude for prioritization and multitasking in a deadline-driven environmentPossess a sense of urgency with strong organizational and follow-up skills\xa0Ability to meet travel requirements, when applicable (up to 25% travel projected)', 'Ability to meet travel requirements, when applicable (up to 25% travel projected)', 'Experience overseeing team members', 'Data Engineer', 'Zennify is looking for a qualified candidate to join their team as a Data Engineer! The Data Engineer\xa0 will work directly with our customers to not only enable them to be successful but to also help guide them through high quality data driven practices and project scope.\xa0 You will work with them and our delivery teams to develop, optimize and oversee our client’s conceptual and logical data systems.\xa0', 'Qualities of the Ideal Candidate', 'A data engineer certification, e.g. Google’s Certified Professional Data Engineer, IBM Certified Data Engineer, CCP Data Engineer for Cloudera, MCSE/MCSE: Data Management and Analytics, is a plus.', ""Specializes in gathering and analyzing information and designing comprehensive solutions that are flexible and adaptable to the client's needs"", 'Understanding translation between logical data structures & physical database objects', 'Has experience with and is currently willing to be hands on contributing developer in addition to leadership responsibilities', 'Preferred Qualifications', 'Enjoys discovering, learning about and implementing new technologies', 'Perform and/or lead necessary unit testing on all developed data scripts', 'Service-oriented and innately driven to produce outstanding customer satisfaction and results', 'Ability to move fast and drive business value and results', 'Enforce sound data migration and integration best practices in order to ensure the quality of deliverables and scalability', 'Hands-on experience with data architecture tools, such as Talend, Jitterbit, Capstorm, Informatica; or others such as Hyperion, Erwin, SQL Server; etc. are helpful\xa0', 'These technologies include but are not limited to: Salesforce.com products and APIs, the Salesforce data model, Amazon Web Services (AWS), Heroku, integration/ETL technologies. You will also maintain an ongoing comprehensive understanding of data migration/integration tools, patterns and best practices.', 'Embodies Zennify culture; a team player that everyone enjoys working with', 'Leads internal initiatives; actively contributes to the Community’s knowledge and resource base', 'Able to design integrated data model based on understanding of business, and use of abstract concepts', 'Passionate about Customer Success', 'Highly detail-oriented individual with the ability to rapidly learn and take advantage of new concepts, business models, and technologies', 'Work directly with QA team to review UAT test cases in an effort to ensure accurate data is migrated and available for testing', '8+ years experience in developing technology solutions', 'Collaborate with functional and technical architect(s) to help finalize the Salesforce data model necessary for successful project delivery', 'Conduct data requirements working sessions focused on data object mapping, data transformation and planning directly with the functional architect, technical architect, and client stakeholders', 'Own and drive data architecture solutions, technology and web flows', 'Passionate about Customer SuccessAlways learning; approaches each interaction with open mind; great listener and hands-onSelf-aware and strategic thinker; proficient at building strong relationshipsSpeaks and writes with clarity, brevity, and purpose; explains area of expertise clearly and confidently to others; influences and engages C-Level with authority and confidenceConfidently and effectively facilitates and presents; ably defends point-of-view; keeps audiences engaged and delivers a clear and memorable messageProficient at collaboration and working with members of a teamAbility to move fast and drive business value and resultsEmbodies Zennify culture; a team player that everyone enjoys working withLives the company’s core values; shows integrity, transparency, and reliabilityLeads internal initiatives; actively contributes to the Community’s knowledge and resource base', 'Primary Responsibilities', 'Development and refinement of production deployment documentation related to data migrations and integrations', 'Lives the company’s core values; shows integrity, transparency, and reliability', 'Ability to reverse engineer existing data models into a conceptual, logical data model constructs', ""Bachelor’s degree in Computer Science, Information Systems,\xa0 or relevant field strongly preferred8+ years experience in developing technology solutions5+ years experience in managing external client projects from a data solution perspectiveA data engineer certification, e.g. Google’s Certified Professional Data Engineer, IBM Certified Data Engineer, CCP Data Engineer for Cloudera, MCSE/MCSE: Data Management and Analytics, is a plus.Ability to reverse engineer existing data models into a conceptual, logical data model constructsStrong conceptual and logical data modeling skillsPossesses strong knowledge of data modeling principles and best practicesExpected to be able use various data modeling tools and processes as requiredOpen minded to collaborate with various team members and able to give and handle constructive feedbackSpecializes in gathering and analyzing information and designing comprehensive solutions that are flexible and adaptable to the client's needsAbility to develop extensible and flexible data models based on long term strategy, and articulate benefit of reuseAble to design integrated data model based on understanding of business, and use of abstract conceptsExperience overseeing team membersHighly detail-oriented individual with the ability to rapidly learn and take advantage of new concepts, business models, and technologiesAbility to work independently and be a self-starterCutting edge innovator who continually studies new technologies and functionality, and is involved in projects that push the capabilities of existing technologies"", 'Work closely with Delivery Managers, Functional Solution Architects, Technical Architects and clients in order support the technology solution architecture through applying data architecture principlesExamine and identify database structural necessities by evaluating client operations, applications, and programmingConduct data requirements working sessions focused on data object mapping, data transformation and planning directly with the functional architect, technical architect, and client stakeholdersCollaborate with functional and technical architect(s) to help finalize the Salesforce data model necessary for successful project deliveryServe as a trusted advisor to the client and recommend solutions to improve new and existing database systemsBuilding analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition.Design and implement data migration and integration solutions and data models to store and retrieve dataSupport the successful delivery of the proposed technical solution, including data migration/integration scripts used in one time migrations, performing migrations, ongoing integrations, etc.Oversee the migration of data from legacy systems to new solutionsAssess database implementation procedures to ensure they comply with internal and external regulationsPrepare accurate database design and architectural design documentationMonitor the system performance by performing regular tests, troubleshooting and integrating new featuresPerform and/or lead necessary unit testing on all developed data scriptsWork directly with QA team to review UAT test cases in an effort to ensure accurate data is migrated and available for testingPerform and/or lead project data consultants in dry run data load testingDevelopment and refinement of production deployment documentation related to data migrations and integrationsLead and mentor the data team on the projectEnforce sound data migration and integration best practices in order to ensure the quality of deliverables and scalabilityIdentify and lead internal strategic initiatives to grow the Data Integration services offering; play an active role developing the knowledgebase and expertise of the Data teamOwn and drive data architecture solutions, technology and web flowsEducate staff members through training and individual supportMentor other ETL/Data team members and maintain best practices are followed within the teamOffer support by responding to system problems in a timely manner', 'Open minded to collaborate with various team members and able to give and handle constructive feedback', 'Confidently and effectively facilitates and presents; ably defends point-of-view; keeps audiences engaged and delivers a clear and memorable message', 'Strong conceptual and logical data modeling skills', 'Proficient at collaboration and working with members of a team', 'To succeed in this role, you should know how to examine new data system requirements and implement migration models. The ideal candidate will also have proven experience in data analysis and management, with excellent analytical and problem-solving abilities.', 'Design and implement data migration and integration solutions and data models to store and retrieve data', 'Experience establishing data pipelines for large data sets.', 'Leadership Skills:', 'Significant experience with Git and standard branching strategies', 'Speaks and writes with clarity, brevity, and purpose; explains area of expertise clearly and confidently to others; influences and engages C-Level with authority and confidence', 'Ability to develop extensible and flexible data models based on long term strategy, and articulate benefit of reuse', 'Hands-on experience with data architecture tools, such as Talend, Jitterbit, Capstorm, Informatica; or others such as Hyperion, Erwin, SQL Server; etc. are helpful\xa0Understanding translation between logical data structures & physical database objectsExperience with migrating data into Salesforce and understanding the Salesforce data modelExperience establishing data pipelines for large data sets.Significant Java development experienceSignificant experience with SOAP & REST API Integration, data, and securitySignificant experience with writing unit testsSignificant experience with Git and standard branching strategiesExperience using JIRA or similar software to manage user stories and workloadHas experience with and is currently willing to be hands on contributing developer in addition to leadership responsibilities', 'Self-aware and strategic thinker; proficient at building strong relationships', 'Oversee the migration of data from legacy systems to new solutions', 'Always learning; approaches each interaction with open mind; great listener and hands-on', 'Experience using JIRA or similar software to manage user stories and workload', 'Offer support by responding to system problems in a timely manner', 'Cutting edge innovator who continually studies new technologies and functionality, and is involved in projects that push the capabilities of existing technologies', 'Technical Requirements', '5+ years experience in managing external client projects from a data solution perspective', 'Mentor other ETL/Data team members and maintain best practices are followed within the team', 'Work closely with Delivery Managers, Functional Solution Architects, Technical Architects and clients in order support the technology solution architecture through applying data architecture principles', 'Serve as a trusted advisor to the client and recommend solutions to improve new and existing database systems', 'Monitor the system performance by performing regular tests, troubleshooting and integrating new features', 'Building analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition.', 'Analytical and able to logically and methodically work through problems', 'The Data Engineer is responsible for, but not limited to the following: data modeling, data migration and data integration, ETL tooling, data best practices, data movement, building and optimizing “big data” data pipelines and architectures, and the building of end to end data solutions for our customers.\xa0 Your duties may include preparing architect reports, monitoring the system, supervising system migrations, and performing root cause analysis on external and internal processes and data to identify opportunities for improvement.\xa0', 'Prepare accurate database design and architectural design documentation', 'Thrives in a team-based, high energy and fast-paced environment', 'Support the successful delivery of the proposed technical solution, including data migration/integration scripts used in one time migrations, performing migrations, ongoing integrations, etc.', 'Educate staff members through training and individual support', 'Significant Java development experience', 'Identify and lead internal strategic initiatives to grow the Data Integration services offering; play an active role developing the knowledgebase and expertise of the Data team', 'Experience with migrating data into Salesforce and understanding the Salesforce data model', 'Expected to be able use various data modeling tools and processes as required', 'Assess database implementation procedures to ensure they comply with internal and external regulations', 'Strong aptitude for prioritization and multitasking in a deadline-driven environment', 'Possess a sense of urgency with strong organizational and follow-up skills\xa0']",Mid-Senior level,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Data Engineer,ClientSolv Inc.,"Littleton, CO",2 weeks ago,29 applicants,"['', 'This 6 month contract-to- hire will be located onsite in Littleton, CO. This role will be in the office Monday-Friday during normal business hours (no remote/telecommuting options available- all work is to be performed onsite).', 'At least two years of experience (5+ years of experience for Senior) using ETL (Extract, Transform, and Load) concepts and techniques on messy data sets and large databases.', 'Ability to read, analyze, and interpret common metrics used to measure and monitor operational performance, define problems, collect data, establish facts, draw valid conclusions, and provide clear and concise communication with a wide audience of internal departments.', 'Proven ability to implement data-driven solutions in a production environment using tools such as Hadoop, Impala, Hive, NiFi, Athena, Redshift, ElasticSearch, BigTable, or Airflow.', 'Validate large amounts of data to ensure data quality in a variety of different ways depending on the data and its consumer. Key technologies may include Python and Excel.', 'Acquire big data input from numerous partners. Key technologies may include Python, Elastic Logstash, and Kafka.Normalize complicated data sources to convert potentially unusable data into a format that can be efficiently used by software and/or employees. Key technologies may include Spark, Lambda, Beam, and Flink.Aggregate data from multiple sources into a single location and format where correlation is possible. Key technologies may include SQL Server, MYSQL, Postgres, Cassandra, Impala, Kudu, and Athena.Validate large amounts of data to ensure data quality in a variety of different ways depending on the data and its consumer. Key technologies may include Python and Excel.Garner key insights from data and communicate these findings to key stakeholders to help them make data-driven decisions. Key technologies may include Tableau, Grafana, Kibana, and R.Provide analyses on data sets to identify trends, issues, and opportunities.Work with partners on efficient data processes while helping them keep their data as clean as possible.Create efficiencies and reduce resource allocation for routine tasks and procedures related to partner data management.Learn industry standards and best practices surrounding data analysis in order to continuously improve our team and systems by implementing them. Create data that is valuable for the organization, either operationally to help drive decisions or financially to gain revenue. Key technologies may include Google Analytics and Qualtrics', 'Experience using Cloud Native tools such as Kubernetes and Docker in private, public, and hybrid clouds.', 'Work with partners on efficient data processes while helping them keep their data as clean as possible.', 'The Following Additional Qualifications Are a Plus', 'Experience applying machine learning and statistical modeling using common data science techniques such as clustering, regression – logistical and linear, confidence intervals, and pattern recognition.', 'Proven ability to implement data-driven solutions in a production environment using tools such as Hadoop, Impala, Hive, NiFi, Athena, Redshift, ElasticSearch, BigTable, or Airflow.Experience using Cloud Native tools such as Kubernetes and Docker in private, public, and hybrid clouds.Experience applying machine learning and statistical modeling using common data science techniques such as clustering, regression – logistical and linear, confidence intervals, and pattern recognition.', 'At least two years of experience using one or more of the following (5+ years of experience for Senior): Java, C++, Python, Go, R, or JavaScript in a Unix/Linux environment', 'Strong communication skills to work with partners internal and external to manage data flow into our infrastructure.', 'Bachelor’s degree in Computer Science, Computer Engineering, Applied Math, Statistics, or a related technical degree.At least two years of experience (5+ years of experience for Senior) using ETL (Extract, Transform, and Load) concepts and techniques on messy data sets and large databases.At least two years of experience using one or more of the following (5+ years of experience for Senior): Java, C++, Python, Go, R, or JavaScript in a Unix/Linux environmentAt least two years of experience using Tableau.At least two years of experience with SQL-like query language and table design.Strong communication skills to work with partners internal and external to manage data flow into our infrastructure.Ability to read, analyze, and interpret common metrics used to measure and monitor operational performance, define problems, collect data, establish facts, draw valid conclusions, and provide clear and concise communication with a wide audience of internal departments.Growth mindset: Proven ability to quickly learn new concepts, processes, software, and development ideas.', 'Create data that is valuable for the organization, either operationally to help drive decisions or financially to gain revenue. Key technologies may include Google Analytics and Qualtrics', 'Primary Responsibilities Are As Follows', 'At least two years of experience using Tableau.', 'Normalize complicated data sources to convert potentially unusable data into a format that can be efficiently used by software and/or employees. Key technologies may include Spark, Lambda, Beam, and Flink.', 'Aggregate data from multiple sources into a single location and format where correlation is possible. Key technologies may include SQL Server, MYSQL, Postgres, Cassandra, Impala, Kudu, and Athena.', 'At least two years of experience with SQL-like query language and table design.', 'Bachelor’s degree in Computer Science, Computer Engineering, Applied Math, Statistics, or a related technical degree.', 'Create efficiencies and reduce resource allocation for routine tasks and procedures related to partner data management.', 'Growth mindset: Proven ability to quickly learn new concepts, processes, software, and development ideas.', 'Company Description', 'Acquire big data input from numerous partners. Key technologies may include Python, Elastic Logstash, and Kafka.', 'Learn industry standards and best practices surrounding data analysis in order to continuously improve our team and systems by implementing them. ', 'Provide analyses on data sets to identify trends, issues, and opportunities.', 'Job Description', 'Garner key insights from data and communicate these findings to key stakeholders to help them make data-driven decisions. Key technologies may include Tableau, Grafana, Kibana, and R.']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Consumer Data Engineer,WunderLand Group,"Hayward, CA",1 day ago,28 applicants,"['', 'Collaborate with agency partners to ensure Clorox’s overall KCD data architecture suits its use casesOn-going management of data architecture as our KCD continues to scale: create structure, schema design, tables, views, stored procedures, and the creation and execution of multiple database queries.Maintain appropriate documentation such as Data dictionaries and Entity Relationship diagrams (ERD).Maintain our growing data model in support of the business', 'Understand businesses’ use cases for KCD and their relative priority to help prioritize the engineering queue', ' 5+ years of API integration experience', ' 5+ years of ETL & data migration experience', 'Lead acceptance testing around data integrations', 'Continuously identify optimizations that improve business results', 'Build and automate data pipelines between Clorox’s datalake and external systems', 'What’s Required To Apply', 'Build and manage technical integration with Clorox’s privacy software to support compliance procedures for CCPA and GDPR.', 'Extremely proficient with writing complex queries and best practices', ' 5+ years working in a cross-functional team that must include some Marketing counterparts', 'Proactively manage data quality and connections via robust and reliable data validation processes. Ensure data is delivered to where it needs to be, on time, within budget and with minimal errors.Author data quality requirements and ensure they meet business and technical needsManage various data quality processes including, but not limited to, de-duplication, identity unification, data completion, etc.Ensure KCD follows appropriate legal and regulatory guidance provided by Legal and IT (CCPA/GDPR, IT Security)', '3+ years of experience working in a cloud environment, Azure or GCP preferred', 'Proactively manage data quality and connections via robust and reliable data validation processes. Ensure data is delivered to where it needs to be, on time, within budget and with minimal errors.', ' 491454', ' 5+ years of ETL & data migration experience 5+ years of API integration experience 5+ years of SQL & data warehouse or database development experience (DDL, DML, Views, Triggers, Functions, Stored Procedures) 5+ years of relational data modeling5+ years of experience with a third generation programming language such as Python or Java 5+ years working in a cross-functional team that must include some Marketing counterpartsSkills and Abilities3+ years of experience working in a cloud environment, Azure or GCP preferred 5+ years of API integration experienceExposure to NoSQL databasesExperience with large volumes of data / data warehousingExperience with database development and administration, SQL server preferred Experience with database / query optimizationExtremely proficient with writing complex queries and best practices', 'Establish and maintain data integrations between Clorox’s datalake and receiving systemsBuild and automate data pipelines between Clorox’s datalake and external systemsLead the planning of database & data warehouse objects in partnership with our agency partnersBuild and manage API integrationsUnderstand businesses’ use cases for KCD and their relative priority to help prioritize the engineering queueLead acceptance testing around data integrationsIdentify any needs for additional tools or technology; make recommendations as necessaryContinuously identify optimizations that improve business resultsBuild and manage technical integration with Clorox’s privacy software to support compliance procedures for CCPA and GDPR.', 'Maintain our growing data model in support of the business', 'Experience with large volumes of data / data warehousing', 'Build and manage API integrations', 'Collaborate with agency partners to ensure Clorox’s overall KCD data architecture suits its use cases', 'Maintain appropriate documentation such as Data dictionaries and Entity Relationship diagrams (ERD).', 'Exposure to NoSQL databases', '5+ years of experience with a third generation programming language such as Python or Java', 'Skills and Abilities', 'Lead the planning of database & data warehouse objects in partnership with our agency partners', 'Experience with database development and administration, SQL server preferred', 'Identify any needs for additional tools or technology; make recommendations as necessary', 'Author data quality requirements and ensure they meet business and technical needs', 'On-going management of data architecture as our KCD continues to scale: create structure, schema design, tables, views, stored procedures, and the creation and execution of multiple database queries.', ' 5+ years of SQL & data warehouse or database development experience (DDL, DML, Views, Triggers, Functions, Stored Procedures)', ' 5+ years of relational data modeling', 'What You’ll Be Doing', 'Manage various data quality processes including, but not limited to, de-duplication, identity unification, data completion, etc.', 'Establish and maintain data integrations between Clorox’s datalake and receiving systems', ' Experience with database / query optimization', 'Ensure KCD follows appropriate legal and regulatory guidance provided by Legal and IT (CCPA/GDPR, IT Security)']",Associate,Contract,Production,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer,"Rubio's Restaurants, Inc.","Carlsbad, CA",2 weeks ago,Be among the first 25 applicants,"['', 'Working knowledge Kubernetes container orchestration, Job Deployment, and Stored Procedure and Trigger development preferred.', 'Advanced knowledge of MS SQL Server and SQL script writing.', 'Salary Range:', 'Casual dress code, jeans are ok here.', 'SQL Programming: 5+ years (Required)Tableau 2+ years (Required)Snowflake 1+ years (Preferred)C syntax programming language 5+ years (Preferred)Kubernetes: 1-2 years (Preferred)Docker: 1-2 years (Preferred)', 'Ability to determine and resolve application bugs and feature extensions in a timely manner with reliable quality.', 'Experience with OLO online ordering platform, Punchh Loyalty Platform and Patronix Loyalty Platforms a Plus.', 'Tableau 2+ years (Required)', 'Work with Senior Software Engineers to develop and QA ETL applications.', 'SQL Programming: 5+ years (Required)', 'C syntax programming language 5+ years (Preferred)', 'Management of multiple high-profile projects.', 'Develop complex interactive/dynamic dashboards with various BI tools.', 'Kubernetes: 1-2 years (Preferred)', 'Responsibilities:', 'Advanced knowledge of MS SQL Server and SQL script writing.Tableau BI tool. Ability to create data sources, dashboards and worksheets.Snowflake. Knowledge of SnowSQL and solid understanding of how to push and pull data.Working knowledge Kubernetes container orchestration, Job Deployment, and Stored Procedure and Trigger development preferred.Working knowledge of MS SQL Server Reporting Services (SSRS).Strong analytical and problem-solving skills.Strong interpersonal and communication skills both verbal and written.Ability to multitask, effectively manage time and meet daily, monthly and annual deadlines.Ability to work effectively with all level employees, and vendors.Working knowledge of Microsoft Visual Studio.', ""Rubio's Offers:"", 'Carlsbad Corporate Office / Remote', 'Snowflake. Knowledge of SnowSQL and solid understanding of how to push and pull data.', '401K retirement savings plans with a match', 'Lead development efforts for new loyalty data modeling and analytics initiatives.Responsible for preparing analysis as well as presenting findings and recommendations. This will require working with third party vendors from time to time troubleshooting data discrepancies.Ability to determine and resolve application bugs and feature extensions in a timely manner with reliable quality.Effective reading comprehension reviewing functional and technical specifications.Develop complex interactive/dynamic dashboards with various BI tools.Work with Senior Software Engineers to develop and QA ETL applications.Demonstrate initiative with respect to learning new data modeling technologies.Improve existing data governance, analysis, and reporting processes to drive efficiency.Management of multiple high-profile projects.Collaborate with Marketing team to implement Google Analytics strategy for collecting consumer behavior data that will be integrated into Tableau dashboards.', 'Strong analytical and problem-solving skills.', 'Improve existing data governance, analysis, and reporting processes to drive efficiency.', 'Tableau BI tool. Ability to create data sources, dashboards and worksheets.', 'Demonstrate initiative with respect to learning new data modeling technologies.', 'A 9/80 schedule option (meaning every other Friday off!)Medical, Dental and Vision plans401K retirement savings plans with a matchFlexible in times for a strong work life balance.Casual dress code, jeans are ok here.', 'Docker: 1-2 years (Preferred)', 'Working knowledge of MS SQL Server Reporting Services (SSRS).', ""Bachelor's degree or 7 years’ experience in related disciplines."", 'Collaborate with Marketing team to implement Google Analytics strategy for collecting consumer behavior data that will be integrated into Tableau dashboards.', 'Qualifications', ""Bachelor's degree or 7 years’ experience in related disciplines.Experience with OLO online ordering platform, Punchh Loyalty Platform and Patronix Loyalty Platforms a Plus."", 'Flexible in times for a strong work life balance.', 'Effective reading comprehension reviewing functional and technical specifications.', 'Ability to multitask, effectively manage time and meet daily, monthly and annual deadlines.', 'Ability to work effectively with all level employees, and vendors.', 'Snowflake 1+ years (Preferred)', 'A 9/80 schedule option (meaning every other Friday off!)', 'Working knowledge of Microsoft Visual Studio.', 'Strong interpersonal and communication skills both verbal and written.', 'Responsible for preparing analysis as well as presenting findings and recommendations. This will require working with third party vendors from time to time troubleshooting data discrepancies.', 'Medical, Dental and Vision plans', 'Lead development efforts for new loyalty data modeling and analytics initiatives.']",Entry level,Full-time,Information Technology,Food Production,2021-03-24 13:05:10
Data Engineer,Blueprint Technologies,"Myrtle Point, OR",1 week ago,Be among the first 25 applicants,"['', ' At least 3-years of experience as a software development or data engineer  At least 2-years of experience with SQL, Python and/or other data collection tools & reporting  Advanced knowledge and skills with AWS is required  Experience with Pyspark or Scala is a plus (Databricks or Spark) Excellent collaboration skills to work on a team as well as independently (be self-reliant and resourceful)Excellent organization skills and able to multi-task and detailed oriented  Excellent verbal and written communication skills (must be able to write clear and concise emails for any audience, etc. ', ' Advanced knowledge and skills with AWS is required ', ' Data Engineer ', ' Experience with Pyspark or Scala is a plus (Databricks or Spark) ', 'What will I be doing?', 'By submitting your interest in this job, you agree to receive text notifications with additional steps to complete your job application. You will receive up to 6 messages from the number ""63879"". Message & data rates may apply. Please refer to our privacy policy for more information.', 'Who is Blueprint?', 'Why Blueprint?', 'What does Blueprint do?', 'Location:', 'Excellent organization skills and able to multi-task and detailed oriented ', ' FLSA - Job Classification: ', 'Qualifications', ' At least 3-years of experience as a software development or data engineer ', 'Excellent collaboration skills to work on a team as well as independently (be self-reliant and resourceful)', ' At least 2-years of experience with SQL, Python and/or other data collection tools & reporting ', ' Excellent verbal and written communication skills (must be able to write clear and concise emails for any audience, etc. ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Kokua Education,United States,2 weeks ago,36 applicants,"['', 'Perform data analysis required to troubleshoot data related issues and assist in the resolution of data issues.', 'Kokua’s mission is to empower students by matching schools with passionate guest educators who maximize substitute teaching time. By the end of high school, teacher absences result in an average of 1+ years of lost learning time, particularly in underserved communities. We solve this problem by placing dedicated substitutes (or Guest Teachers) into classrooms across the United States. We believe that proximity is power, and that one role model can change a student’s life forever. We are a driven team looking to make a lasting impact for future generations. As the world adjusts to new ways of life, inspiring role models are as important as ever, and we’re looking for a new team member to drive this mission forward.', 'BS or MS degree in Computer Science or a related technical field', '3-5 years in a data engineer role or similar role', 'Identify ways to improve data reliability, efficiency, and quality.', 'Experience maintaining data processing systems.', 'Skills in server reporting services (i.e. Tableau, Google drive, etc.), business platforms, (i.e. Salesforce, Greenhouse, Frontline), integration services, or any other data visualization tools.', 'Attractive base salary and bonus with generous benefits, commensurate or above same-level positions in education. 4 weeks of paid holidays, in addition to 7 PTO days. Position provides autonomy and a unique opportunity to scale an impact-driven organization amongst a close-knit team. Play an integral role on a team that is on a mission to help empower children so they can realize their vast potential.', 'Implement processes and systems to monitor data quality, ensuring data is always accurate and available for all business units and processes that depend on it.', 'Experience in managing and communicating data warehouse plans to internal personnel.', 'Perform a complete assessment of the current data infrastructure and business platforms.', 'This role has the option to be remote.\xa0', 'Design data integrations for current business platforms (Paylocity, Greenhouse, Salesforce, Frontline).', 'YOUR ROLE', 'PERKS AND PAY', 'If you are interested in the role, or have questions about it, please reach out to Mical at mical@kokuaed.com with “Data Engineer” in the subject line.', 'Kokua is looking for a driven, strategic, and innovative data engineer to design and implement our company’s complete data infrastructure. You will assess our current data environment and work collaboratively with various verticals within the company to identify the ideal data infrastructure that suits our business operations and needs. In addition to design and implementation, you will be asked to manage, maintain, and troubleshoot all items related to our data architecture. You are data-driven, growth-focused, and have experience in consolidating data from multiple business platforms into a centralized data warehouse. You will develop and optimize a data infrastructure that will allow for Kokua to achieve its growth potential and execute its mission of bringing communities together to empower children, particularly those growing up in marginalized communities.', 'EXPERIENCE', 'RESPONSIBILITIES', 'Perform a complete assessment of the current data infrastructure and business platforms.Design and implement a new data infrastructure that aligns with business requirements and allows for scalability.Design data integrations for current business platforms (Paylocity, Greenhouse, Salesforce, Frontline).Implement processes and systems to monitor data quality, ensuring data is always accurate and available for all business units and processes that depend on it.Identify ways to improve data reliability, efficiency, and quality.Work closely with all business verticals to develop strategy for long term data platform architecture.Perform data analysis required to troubleshoot data related issues and assist in the resolution of data issues.Work with vertical leads to develop and maintain datasets.', 'Experience in data infrastructure design and implementation.', 'BS or MS degree in Computer Science or a related technical field3-5 years in a data engineer role or similar roleExperience in data infrastructure design and implementation.Experience in managing and communicating data warehouse plans to internal personnel.Experience maintaining data processing systems.Skills in server reporting services (i.e. Tableau, Google drive, etc.), business platforms, (i.e. Salesforce, Greenhouse, Frontline), integration services, or any other data visualization tools.Expert in Microsoft office', 'Design and implement a new data infrastructure that aligns with business requirements and allows for scalability.', 'Kokua began in Chicago in 2011 and has since grown across the country. We w ere recognized by Forbes in 2016 as an up-and-coming organization making an impact in the field of education. We have exciting growth plans ahead, and this is where you come in.', 'OUR COMPANY', 'Work with vertical leads to develop and maintain datasets.', 'The ideal candidate must be a self-starter willing to proactively diagnose and troubleshoot performance and growth opportunities rather than being asked to take action. You have strong communication skills with a collaborative mindset. You are analytical, highly organized and are an innovative problem solver. You are passionate about serving children, particularly those growing up in marginalized environments. You are a team player, where no task is too small.', 'Work closely with all business verticals to develop strategy for long term data platform architecture.', 'Expert in Microsoft office']",Not Applicable,Full-time,Analyst,Education Management,2021-03-24 13:05:10
Data Engineer - Remote - New York,Provision People,"New York, NY",3 weeks ago,Be among the first 25 applicants,"['', 'Integrate new software tools for data analysis into the existing toolset.', 'Experience in analyzing and crafting efficient algorithms', 'Bachelor-degree or higher in Computer Science, Data Science, or EngineeringMust have Data analytics & pipeline experiences with Python, and Databricks/SparkMust have AWS Lambda, Batch, Serverless, ECR experiences.Must have experience with Airflow, Git, Docker / Kubernetes or similar.Experiences with Data Science languages such as Python, R, Scala.Working experiences in SQL, or Snowflake (nice to have).Working experiences in Model CI/CD such as AWS CodeDeploy, CodeCommit, SageMaker, or Azure Machine Learning.Deep understanding of algorithms and algorithmic complexityExperience in analyzing and crafting efficient algorithmsNice to have experiences with NoSQL technologies such as MongoDB, and Cassandra.Experience with cleaning, aggregating, and pre-processing data from various sources.Familiar with Linux administration (bash, network, file systems)Experience working within an Agile software development frameworkStrongly disciplined approach to software developmentA team-player who is eager to learn with strong analytical and communications skills', 'Bachelor-degree or higher in Computer Science, Data Science, or Engineering', 'Must have AWS Lambda, Batch, Serverless, ECR experiences.', 'Required Skills And Experience', 'Collaborate with the team on quick evaluation of new data sources by assisting with transfer and processing of data.', 'Identify and download public datasets like COVID tracking, temperature, census data to be used by the team for analysis.', 'Support and maintain existing production processes on Airflow and onboard new processes.', 'Deep understanding of algorithms and algorithmic complexity', 'A team-player who is eager to learn with strong analytical and communications skills', 'Experiences with Data Science languages such as Python, R, Scala.', 'Nice to have experiences with NoSQL technologies such as MongoDB, and Cassandra.', 'Must have experience with Airflow, Git, Docker / Kubernetes or similar.', 'Experience working within an Agile software development framework', 'Responsibilities', 'Work closely with Data Scientists to build data pipelines to onboard large structured and unstructured data.Construct robust scalable pipelines using Databricks and AWS Lambda, Batch to deploy applications in python / PySpark.Support and maintain existing production processes on Airflow and onboard new processes.Integrate new software tools for data analysis into the existing toolset.Collaborate with the team on quick evaluation of new data sources by assisting with transfer and processing of data.Identify and download public datasets like COVID tracking, temperature, census data to be used by the team for analysis.', 'Summary', 'Experience with cleaning, aggregating, and pre-processing data from various sources.', 'Construct robust scalable pipelines using Databricks and AWS Lambda, Batch to deploy applications in python / PySpark.', 'Familiar with Linux administration (bash, network, file systems)', 'Must have Data analytics & pipeline experiences with Python, and Databricks/Spark', 'Working experiences in Model CI/CD such as AWS CodeDeploy, CodeCommit, SageMaker, or Azure Machine Learning.', 'Strongly disciplined approach to software development', 'Work closely with Data Scientists to build data pipelines to onboard large structured and unstructured data.', 'Working experiences in SQL, or Snowflake (nice to have).']",Entry level,Full-time,Information Technology,Construction,2021-03-24 13:05:10
Data Engineer,Hired Recruiters,"Austin, TX",1 week ago,Be among the first 25 applicants,"['', 'Experience operating a production solution which supports the business', ' Help lead the team to build our data infrastructure and work with emerging technologies such as Snowflake and associated cloud services Design, implement, and operate stable, scalable, low cost solutions to flow data from production systems into the data warehouse and into end-user facing BI applications Implement processes and systems to monitor data quality, ensuring production data is always accurate and available Running machine learning experiments using best-in-class ML platforms Automate & optimize everything Leverage best in industry practices to build the next generation data ecosystem to collect, move, store and analyze data ', 'Leverage best in industry practices to build the next generation data ecosystem to collect, move, store and analyze data', 'You have worked with a variety of cloud and data solutions, such as: AWS / Azure / Google Cloud, SnowFlake, SnowPlow, Kafka, Segment.io, DataBricks, Trifecta, Hadoop, Spark, Airflow', 'What You’ll Do', 'Design, implement, and operate stable, scalable, low cost solutions to flow data from production systems into the data warehouse and into end-user facing BI applications', 'You have 5+ years of experience in the field of data engineering or related engineering experience', 'Help lead the team to build our data infrastructure and work with emerging technologies such as Snowflake and associated cloud services', 'Running machine learning experiments using best-in-class ML platforms', ' You have 5+ years of experience in the field of data engineering or related engineering experience You have recent experience building large-scale production data solutions You are familiar with data driven marketing and integrating into marketing automation solutions. You have a command of various programming languages to collect and manipulate data such as Python, R, and SQL. Experience operating a production solution which supports the business You have strong solution architecture skills and a passion for building data solutions that power the future business. You have worked with a variety of cloud and data solutions, such as: AWS / Azure / Google Cloud, SnowFlake, SnowPlow, Kafka, Segment.io, DataBricks, Trifecta, Hadoop, Spark, Airflow We are looking for people that will thrive in the collaborative environment while also being strong individual contributors with a proactive mindset. ', 'You have strong solution architecture skills and a passion for building data solutions that power the future business.', 'You have a command of various programming languages to collect and manipulate data such as Python, R, and SQL.', 'Automate & optimize everything', 'You are familiar with data driven marketing and integrating into marketing automation solutions.', 'Implement processes and systems to monitor data quality, ensuring production data is always accurate and available', 'We are looking for people that will thrive in the collaborative environment while also being strong individual contributors with a proactive mindset.', 'Who You Are', 'You have recent experience building large-scale production data solutions']",Mid-Senior level,Full-time,Engineering,Computer Software,2021-03-24 13:05:10
Data Engineer,"Frontdoor, Inc.","Denver, CO",4 weeks ago,32 applicants,"['', 'Proactively automate manual processes throughout the business for higher efficiency, robustness, and speed.', 'Desired Skills', ' Strong grasp of business elements and ability to convert requirements into database models and full-data pipeline systems.', ' Build and maintain scalable data pipelines for both batch and stream processing in a cloud-computing environment. Apply dimensional modeling to design tables and views that map business processes into an enterprise data model. Optimize database architecture by trading off storage and computation to achieve low cost and high performance.Build and support complex ETL infrastructure to deliver clean and reliable data to the organization.Support the development of new products and services via ingestion, processing, and formatting data for reporting and analytics. Interact face-to-face with business stakeholders, develop cooperative relationships, and acquire domain knowledge of the business.Proactively automate manual processes throughout the business for higher efficiency, robustness, and speed. Enforce production standards and governance best practices in the management of enterprise-level data, metrics, and reports.QualificationsBachelor’s in Computer Science, Engineering, Data Science, or related field (Masters or PhD preferred)Excellent communication and inter-personal skills. Versatile and quick learner with ability to pick up any new skills necessary to get the job done.Demonstrated strength in data modeling, ETL development, data warehousing, data pipeline and data lake creation. Extensive experience with cloud infrastructure and tools for AWS and GCP. Demonstrable proficiency in Python development and advanced SQL querying. Strong grasp of business elements and ability to convert requirements into database models and full-data pipeline systems.Experience with visualization and reporting tools, such as Looker and Tableau.3-4 years of experience in data engineering or similar work.Desired Skills Snowflake experience (highly desirable). Full stack experience, including microservice and web app development. Knowledge of big data platforms, such as Hadoop and Spark.Frontdoor is a company that’s obsessed with taking the hassle out of owning a home. With services powered by people and enabled by technology, it is the parent company of four home service plan brands: American Home Shield, HSA, Landmark and OneGuard, as well as AHS Proconnect , an on-demand membership service for home repairs and maintenance, and Streem, a technology company that enables businesses to serve customers through an enhanced augmented reality, computer vision and machine learning platform. Frontdoor serves more than two million customers across the U.S. through a network of more than 16,000 pre-qualified contractor firms that employ over 45,000 technicians. The company’s customizable home service plans help customers protect and maintain their homes from costly and unexpected breakdowns of essential home systems and appliances. With nearly 50 years of experience, the company responds to over four million service requests annually (or one request every eight seconds).For more details, visit frontdoorhome.com.Job Category: EngineeringID: R0015252', ' Interact face-to-face with business stakeholders, develop cooperative relationships, and acquire domain knowledge of the business.', '3-4 years of experience in data engineering or similar work.Desired Skills', ' Extensive experience with cloud infrastructure and tools for AWS and GCP.', ' Demonstrable proficiency in Python development and advanced SQL querying.', ' Full stack experience, including microservice and web app development.', ' Knowledge of big data platforms, such as Hadoop and Spark.Frontdoor is a company that’s obsessed with taking the hassle out of owning a home. With services powered by people and enabled by technology, it is the parent company of four home service plan brands: American Home Shield, HSA, Landmark and OneGuard, as well as AHS Proconnect , an on-demand membership service for home repairs and maintenance, and Streem, a technology company that enables businesses to serve customers through an enhanced augmented reality, computer vision and machine learning platform. Frontdoor serves more than two million customers across the U.S. through a network of more than 16,000 pre-qualified contractor firms that employ over 45,000 technicians. The company’s customizable home service plans help customers protect and maintain their homes from costly and unexpected breakdowns of essential home systems and appliances. With nearly 50 years of experience, the company responds to over four million service requests annually (or one request every eight seconds).For more details, visit frontdoorhome.com.Job Category: EngineeringID: R0015252', 'Excellent communication and inter-personal skills.', ' Optimize database architecture by trading off storage and computation to achieve low cost and high performance.', 'Responsibilities', 'Bachelor’s in Computer Science, Engineering, Data Science, or related field (Masters or PhD preferred)', 'Qualifications', ' Versatile and quick learner with ability to pick up any new skills necessary to get the job done.', ' Snowflake experience (highly desirable).', ' Apply dimensional modeling to design tables and views that map business processes into an enterprise data model.', ' Build and maintain scalable data pipelines for both batch and stream processing in a cloud-computing environment.', 'Build and support complex ETL infrastructure to deliver clean and reliable data to the organization.', 'Experience with visualization and reporting tools, such as Looker and Tableau.', ' Enforce production standards and governance best practices in the management of enterprise-level data, metrics, and reports.Qualifications', 'Demonstrated strength in data modeling, ETL development, data warehousing, data pipeline and data lake creation.', 'Frontdoor is looking for a very strong data engineer who will bring a mindset of automation and innovation to the table. In addition to sharp technical skills, this person must be a strong communicator and collaborator who can partner up with business stakeholders to understand their needs and solve their data problems. As a data engineer at Frontdoor, you will be working in a fast-paced environment and using cutting-edge cloud technologies to develop a scalable data platform that will support years of company growth.', 'Support the development of new products and services via ingestion, processing, and formatting data for reporting and analytics.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data and Analytics Engineer,MAKE Corporation,"Milwaukee, WI",5 days ago,66 applicants,"['', 'Nine paid holidays and 18 Paid Time Off days (pro-rated for the first year)', 'Work with the business users to conduct data discovery engagements.\xa0', 'Understand the importance of capturing data lineage.', 'Engage in continuous learning of our business functional areas to more effectively partner with our business SME’s', 'Seek out and apply new technologies and skills in daily work through conferences, online training, reading, participation in organizations and user groups, etc', 'Health and Dental insurance', '· Data Integration concepts and strategies: EII, ETL, EL-T, SOA, and EAI', 'Minimum of 5 - 7 years of experience in Data Solution delivery in a complex environment working collaboratively in a team setting', 'Knows when to give and take and can clearly communicate the “why” of a specific decision or approach.', '· Data Query tools: SQL, T-SQL', 'As a Data and Analytics Engineer, you will:', '· Database: SQL Server', 'Duration: 6 months Contract to Hire or PERMANENT ', 'Information technology is a core part of the company’s business strategy and plays a critical role in the growth and transformation of the firm', 'Demonstrate ability to recognize the immediate project context as well as the overall vision.', 'Location: MILWAUKEE, WI\xa0', 'Proficient in some of the following tools:', '· Business Intelligence tools: Microsoft tools (SQL Server Management Studio, SSRS, SSAS, SSIS, Power Pivot, Power Query, PowerBI), Alteryx', 'Develop and validate source to target mappings and transformation logic.\xa0', 'Data and Analytics Engineer', 'Develop best practices and communicate to teams with a focus on empowering others to implement quality value-add solutions.', 'Rate discount upon refinancing federal and/or private student loans.', 'Partner with internal business units to define information requirements and translate them into appropriate data solutions', 'Duration:', 'Free identity, financial and privacy-protection coverage for associates and their family members', '· Reference Data Management', 'Flexible work arrangements availableProfit sharing Pretax, Roth and voluntary 401(k) after-tax contributions and above-average retirement benefit with the combination of a 401(k) firm match and firm profit-sharingHigh bonus - believed to be in the 20-25% bonus range. Health and Dental insuranceFree basic term life and disability insuranceFSA and HSATuition and designation/certification reimbursement available for all regular associatesNine paid holidays and 18 Paid Time Off days (pro-rated for the first year)Free identity, financial and privacy-protection coverage for associates and their family membersRate discount upon refinancing federal and/or private student loans.And more', '· Data Warehousing concepts: Inmon, Kimball, Data Lake', 'FSA and HSA', 'Minimum of 5 - 7 years of experience in Data Solution delivery in a complex environment working collaboratively in a team settingProficient in some of the following tools:', 'Partner with project team members to translate business and functional requirements into technical designs', 'Contribute to the direction of data development, platforms, versions, and toolsets', 'Profit sharing ', 'Data Delivery', 'Take appropriate steps to ensure sensitive data is protected and secure.', 'Conversant in the following concepts:', 'What makes this opportunity great:', 'Play a contributing role in building our data and analytics practice.\xa0\xa0\xa0\xa0\xa0\xa0', 'Design and prototype a solution that brings together multiple data sources into one coherent concept and understanding.\xa0(data blending)', 'Tuition and designation/certification reimbursement available for all regular associates', '\xa0', 'Contribute to the direction of data development, platforms, versions, and toolsetsProvide awareness to IT and business team members of data management practices.Knows when to give and take and can clearly communicate the “why” of a specific decision or approach.Develop best practices and communicate to teams with a focus on empowering others to implement quality value-add solutions.Seek out and apply new technologies and skills in daily work through conferences, online training, reading, participation in organizations and user groups, etc', 'Demonstrate competence, experience, knowledge, and understanding of data management concepts, data warehousing, data integration, BI, and analytics.', 'Ability\xa0to build solid data integration solutions that feed mission-critical business processes and decisions', 'Identify and communicate project risks and impediments and proactively work with other members of the team to complete high-value deliverables as identified by business partners and team leadership', 'General Data Management', 'Information technology is a core part of the company’s business strategy and plays a critical role in the growth and transformation of the firmAbility\xa0to build solid data integration solutions that feed mission-critical business processes and decisionsAbility to work a variety of teams and leaders, proving your ability to be flexible and high adaptableUnique culture that values diverse backgrounds and perspectives while emphasizing teamwork and a strong sense of partnership', 'Ability to work a variety of teams and leaders, proving your ability to be flexible and high adaptable', 'Identify and communicate project risks and impediments and proactively work with other members of the team to complete high-value deliverables as identified by business partners and team leadershipPartner with project team members to translate business and functional requirements into technical designsPartner with the business stakeholders to develop sound information usage, access, management, and understanding. ', 'Data Engineering', 'Performs data analysis and data profiling to gain a solid understanding of the business data to collaborate effectively with the business users, business analysts, and data owners.', 'Unique culture that values diverse backgrounds and perspectives while emphasizing teamwork and a strong sense of partnership', 'Skilled in data modeling, both 3NF and dimensional, with experience in conceptual, logical, physical, and industry data modeling. Strong knowledge and experience with data architecture methodologies.', 'Flexible work arrangements available', 'Partner with the business stakeholders to develop sound information usage, access, management, and understanding. ', 'Provide awareness to IT and business team members of data management practices.', 'Pretax, Roth and voluntary 401(k) after-tax contributions and above-average retirement benefit with the combination of a 401(k) firm match and firm profit-sharing', 'Location:', 'Play a contributing role in building our data and analytics practice.\xa0\xa0\xa0\xa0\xa0\xa0Demonstrate competence, experience, knowledge, and understanding of data management concepts, data warehousing, data integration, BI, and analytics.Demonstrate ability to recognize the immediate project context as well as the overall vision.Demonstrate ability to perform abstract thinking by being able to identify higher level patterns in multiple detailed scenarios.', '#JP – BD02', 'Free basic term life and disability insurance', '· Data Modeling: ER/Studio Data Architect, 3NF and dimensional modeling', '· Data Integration tools: SSIS', 'Apply the appropriate level of modeling theory, pattern recognition, and abstractions to design a pragmatic solution that functionally meets requirements.', '· Data Management and Quality: data mapping, data profiling, metadata repository, relational data modeling, master data management', 'Skilled in data modeling, both 3NF and dimensional, with experience in conceptual, logical, physical, and industry data modeling. Strong knowledge and experience with data architecture methodologies.Apply the appropriate level of modeling theory, pattern recognition, and abstractions to design a pragmatic solution that functionally meets requirements.Partner with internal business units to define information requirements and translate them into appropriate data solutionsPerforms data analysis and data profiling to gain a solid understanding of the business data to collaborate effectively with the business users, business analysts, and data owners.Develop and validate source to target mappings and transformation logic.\xa0Understand the importance of capturing data lineage.Take appropriate steps to ensure sensitive data is protected and secure.', 'Benefits', 'What we look for:', 'Demonstrate ability to perform abstract thinking by being able to identify higher level patterns in multiple detailed scenarios.', 'Lead and Coach!', 'High bonus - believed to be in the 20-25% bonus range. ', 'Work with the business users to conduct data discovery engagements.\xa0Design and prototype a solution that brings together multiple data sources into one coherent concept and understanding.\xa0(data blending)Engage in continuous learning of our business functional areas to more effectively partner with our business SME’s', 'And more', 'Collaborate – build relationships!']",Mid-Senior level,Contract,Information Technology,Information Services,2021-03-24 13:05:10
Data Engineer / DBA,Hays,"Anaheim, CA",3 weeks ago,64 applicants,"['', '• Translate client requested features into technical requirements.', 'Visit the Hays Career Advice section to learn top tips to help you stand out from the crowd when job hunting.', 'An American Company is seeking a Data Engineer / DBA (Microsoft, Angular) in Anaheim, CA', '• Design and develop tables, views, stored procedures, indexes, functions, dictionaries, and complex ad- hoc queries for SQL Server databases.', '• Design and develop user interfaces using AngularJS best practices. - AngularJS not required at all', 'Skills & Requirements', '• Must be able to work independently, and in team environments.', 'Drug testing may be required; please contact a recruiter for more information.', '• 1+ years of development experience with object-oriented languages', '• Proficiency in Angular framework', 'Data Engineer / DBA (Microsoft, Angular) – Perm – Anaheim, CA. - $70,000-$90,00', '• Exercise critical thinking to identify opportunities to reduce manual processes through automation.', 'Hays is an Equal Opportunity Employer.', 'Role Description', '• Work with client and management team to prioritize business and reporting/data needs.', '• Communicate/coordinate with team to ensure proper implementation of company standards and procedure', 'Why Hays?', 'The end client is unable to sponsor or transfer visas for this position; all parties authorized to work in the US without sponsorship are encouraged to apply.', '• Proficiency in advanced Microsoft Excel and Visual Basic for Applications (VBA) including creating/editing macros.', ""• Bachelor's on computer science, mathematics, engineering or related field"", '• 2+ years of experience with Microsoft SQL server, including table structuring, creating queries and stored procedures', 'You will be working with a professional recruiter who has intimate knowledge of the Information Technology industry and market trends . Your Hays recruiter will lead you through a thorough screening process in order to understand your skills, experience, needs, and drivers. You will also get support on resume writing, interview tips, and career planning, so when there’s a position you really want, you’re fully prepared to get it.', 'Nervous about an upcoming interview? Unsure how to write a new resume?', '• Good communication skills – both written and oral.']",Mid-Senior level,Full-time,Information Technology,Staffing and Recruiting,2021-03-24 13:05:10
Data Engineer,Talentpair,"New York, NY",,N/A,"['', 'Once you apply, we’ll send you the full job spec, all company information and a highlight of their leadership team.', 'Our CONFIDENTIAL, pre-IPO, venture-backed, growth client is hiring a Data Engineer who will tackle hard challenges everyday! Using a mix of the latest-and-greatest tech as well as proven tools to pioneer the use of data discovery, pipelining, extraction, importation, sanitization, and visualization of massive datasets to an industry that is eager to utilize an Enterprise SaaS solution in their daily workflow. You will take on tough problems in machine learning and scalable data processing using technologies like Scala, Spark, Postgres, ElasticSearch and Docker.']",Mid-Senior level,Full-time,Information Technology,Commercial Real Estate,2021-03-24 13:05:10
Data Engineer,"Healthcare Financial, Inc.","Quincy, MA",4 weeks ago,Be among the first 25 applicants,"['', 'Job Summary', 'Essential Functions & Responsibilities', 'Minimum Requirements', 'Physical Demands', 'Responsibilities Include But Not Limited To']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,NOVATEUR TECHNOLOGIES,"Pleasanton, CA",3 weeks ago,Be among the first 25 applicants,[''],Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer ,Portico Benefit Services,"Minneapolis, MN",4 weeks ago,58 applicants,"['', 'Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.', 'Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.', 'Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.', 'Create and maintain optimal data pipeline architecture using Agile development principles', 'Strong analytic skills related to working with unstructured datasets.', 'Portico is seeking a Data Engineer to be a member of the DevOps team and deliver modern, cloud-based solutions to enable access to data for reporting, business intelligence, and operations.', 'Proficiency with one or more of the following: C#. .NET Core, Azure Functions, DevOps Pipelines, Microsoft SQL Server, Azure Cloud Infrastructure, Front End JavaScript (Vue.js, Angular, etc.)', 'BS/BA in information technology, computer science, or related field.\xa0', 'Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and ‘big data’ technologies.', 'Coach and mentor others to build their skills for data/analytics', '\xa0', 'Security background with PHI and PII a plus', 'Create and maintain optimal data pipeline architecture using Agile development principlesDesign and implement software solutions using modern, cloud-based architecturesAssemble large, complex data sets that meet functional / non-functional business requirementsIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and ‘big data’ technologies.Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.Work with data and analytics experts to strive for greater functionality in our data systems.Coach and mentor others to build their skills for data/analyticsUnderstand and comply with Portico’s Governance, Risk, and Compliance standards (e.g. internal controls, regulatory compliance, policy compliance).', 'Assemble large, complex data sets that meet functional / non-functional business requirements', 'Responsibilities', 'BS/BA in information technology, computer science, or related field.\xa05+ years of experience in a Data Engineer roleHands-on at least 2 of the following: SQL Server, Azure Data Factory, Databricks/Spark, Synapse Analytics (SQL DW), and Data LakeExperience building and optimizing data pipelines, architectures and data sets.A successful history of manipulating, processing and extracting value from large disconnected datasets.Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.Security background with PHI and PII a plusExperience supporting and working in an agile teamProficiency with one or more of the following: C#. .NET Core, Azure Functions, DevOps Pipelines, Microsoft SQL Server, Azure Cloud Infrastructure, Front End JavaScript (Vue.js, Angular, etc.)Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.Strong analytic skills related to working with unstructured datasets.Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management.Familiarity with PowerBI conceptsWorking knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.', 'Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management.', 'Qualifications', 'Familiarity with PowerBI concepts', 'Understand and comply with Portico’s Governance, Risk, and Compliance standards (e.g. internal controls, regulatory compliance, policy compliance).', 'Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.', 'A successful history of manipulating, processing and extracting value from large disconnected datasets.', 'This position is responsible for creating and optimizing data and data pipeline architecture, as well as optimizing data flow and collection.\xa0 \xa0', 'Work with data and analytics experts to strive for greater functionality in our data systems.', 'Design and implement software solutions using modern, cloud-based architectures', 'Experience building and optimizing data pipelines, architectures and data sets.', 'Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.', 'Experience supporting and working in an agile team', '5+ years of experience in a Data Engineer role', 'Hands-on at least 2 of the following: SQL Server, Azure Data Factory, Databricks/Spark, Synapse Analytics (SQL DW), and Data Lake']",Mid-Senior level,Full-time,Information Technology,Financial Services,2021-03-24 13:05:10
Data Engineer Associate,HAP Capital,"New York, NY",5 days ago,30 applicants,"['', ' Possess Grit; this job is going to be a grind, especially implementing the ETL layer as it may end up requiring some manual data entry. Demonstrate Strong Verbal and Written Communication; this will be a high-touch role, interfacing with the highest levels of the Firm. Be Goal Oriented; the individual must be excited to tackle this project, see it through to completion, and be proud of what they build. ', ' Automation platform, e.g. Airflow', 'Build a high-resolution, narrative P&L and expense accounting platform from the ground up.', 'Design and implement ETL layer for merging of trading and non-trading related cash flows.', 'Demonstrate Strong Verbal and Written Communication; this will be a high-touch role, interfacing with the highest levels of the Firm.', 'Build', 'Programming languages: SQL, Python.', ' Build a high-resolution, narrative P&L and expense accounting platform from the ground up. Design and implement ETL layer for merging of trading and non-trading related cash flows. Develop reporting infrastructure. Collaborate w/ end users (e.g. business analysts and Firm principals) on reporting prototypes. Document the platform. Train business analysts and Firm principals on reporting infrastructure. ', 'Possess Grit; this job is going to be a grind, especially implementing the ETL layer as it may end up requiring some manual data entry.', 'Collaborate', ' Undergraduate or other professional degree/certification in Computer Science, Computer Engineering, w/ a focus on Data Engineering. Programming languages: SQL, Python. Training/experience with database design and administration. ', 'Undergraduate or other professional degree/certification in Computer Science, Computer Engineering, w/ a focus on Data Engineering.', 'Develop', 'Be Goal Oriented; the individual must be excited to tackle this project, see it through to completion, and be proud of what they build.', 'Training/experience with database design and administration.', 'Requirements', 'Train business analysts and Firm principals on reporting infrastructure.', 'Develop reporting infrastructure.', 'Document', 'Train', 'Document the platform.', 'Additional Skills/experience That Will Reflect Favorably', 'Be Goal Oriented;', 'Design', 'Demonstrate Strong Verbal and Written Communication', 'Collaborate w/ end users (e.g. business analysts and Firm principals) on reporting prototypes.', 'Possess Grit', 'Automation platform, e.g. Airflow']",Entry level,Full-time,Information Technology,Capital Markets,2021-03-24 13:05:10
Junior Data Engineer,TalTeam,"New York, NY",4 weeks ago,59 applicants,"[' Perform routine functions in support of the Tech Operations data team. Run standardized queries, extracts and parses files, loads data into platforms, and ensures the highest levels of quality. Under the direction of the Tech Operations data team and following strict InfoSec procedures Run standardized queries to extract data from data lakes and other tools. Parses files per platform ingestion limitations. Loads files into platforms (e.g. SFMC, Braze) via UI or other methods. Completes quality assurance and resolves any discrepancies. Document completion and audit trail in Jira. ', 'Run standardized queries to extract data from data lakes and other tools.', 'Parses files per platform ingestion limitations.', 'Experience with quality assurance and Infosec procedures.', 'Loads files into platforms (e.g. SFMC, Braze) via UI or other methods.', 'Excellent interpersonal skills', 'Talteam Inc. is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or protected veteran status and will not be discriminated against on the basis of disability.**', ' Minimum of 2+ years scripting languages and scheduling jobs (SQL, Python) Experience with data platforms Snowflake/Databricks preferred. Experience with email/push/inapp platforms (Salesforce, Braze) preferred Experience with quality assurance and Infosec procedures. Experience with Agile software development methodology (SCRUM, Kanban). Experience with Jira preferred Flexible and willing to assist other team members Able to work in a fast-paced environment with quick turnaround Strong problem solving skills Attention to detail demonstrated in work and communication Ability to manage multiple tasks simultaneously Excellent interpersonal skills ', 'Requirements:', 'Ability to manage multiple tasks simultaneously', 'Perform routine functions in support of the Tech Operations data team.', 'Jr. Data Engineer', 'Attention to detail demonstrated in work and communication', 'Jr. Data EngineerNew York, NYResponsibilities Perform routine functions in support of the Tech Operations data team. Run standardized queries, extracts and parses files, loads data into platforms, and ensures the highest levels of quality. Under the direction of the Tech Operations data team and following strict InfoSec procedures Run standardized queries to extract data from data lakes and other tools. Parses files per platform ingestion limitations. Loads files into platforms (e.g. SFMC, Braze) via UI or other methods. Completes quality assurance and resolves any discrepancies. Document completion and audit trail in Jira.  Requirements: Minimum of 2+ years scripting languages and scheduling jobs (SQL, Python) Experience with data platforms Snowflake/Databricks preferred. Experience with email/push/inapp platforms (Salesforce, Braze) preferred Experience with quality assurance and Infosec procedures. Experience with Agile software development methodology (SCRUM, Kanban). Experience with Jira preferred Flexible and willing to assist other team members Able to work in a fast-paced environment with quick turnaround Strong problem solving skills Attention to detail demonstrated in work and communication Ability to manage multiple tasks simultaneously Excellent interpersonal skills Talteam Inc. is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or protected veteran status and will not be discriminated against on the basis of disability.**', 'Completes quality assurance and resolves any discrepancies.', 'Flexible and willing to assist other team members', 'Experience with data platforms Snowflake/Databricks preferred.', 'Experience with Agile software development methodology (SCRUM, Kanban).', 'Experience with Jira preferred', 'Run standardized queries, extracts and parses files, loads data into platforms, and ensures the highest levels of quality.', 'Responsibilities', 'Strong problem solving skills', 'Minimum of 2+ years scripting languages and scheduling jobs (SQL, Python)', 'Experience with email/push/inapp platforms (Salesforce, Braze) preferred', 'Able to work in a fast-paced environment with quick turnaround', 'Under the direction of the Tech Operations data team and following strict InfoSec procedures', 'Document completion and audit trail in Jira.']",Associate,Contract,Information Technology,Information Technology and Services,2021-03-24 13:05:10
ASSOCIATE DATA SCIENTIST ENGINEER,Gap Inc.,"San Francisco, CA",4 weeks ago,Over 200 applicants,"['', 'BA/BS in a technical or engineering field (Master’s preferred).1-3 years of experience in a data engineering or full-stack data scientist role.Strong understanding of relational databases and SQL.Solid programming foundations and proficiency with data related languages such as Python/Spark/R.Excellent communication skills. Ability to effectively communicate with both technical and non-technical audiences.', 'Partner with internal customers to understand business needs and build strong relationships with key stakeholders.', 'Partner with internal customers to understand business needs and build strong relationships with key stakeholders.Develop, deploy, and support analytic data products, such as data marts, ETL’s (extract/transform/load), functions (in Python/SQL/Spark/R), and visualizations.Navigate various data sources and efficiently locate data in a complex data ecosystem.Work closely with our data scientists to ensure production models are built using a scalable back-end.Maintain and support deployed solutions and data products.', 'This simple idea—that we all deserve to belong,\u202fand on our own terms—is core to who we are as a\u202fcompany and how we make decisions.\u202fOur team\xa0is made up of thousands of people across the globe who take risks, think big, and do good for our customers, communities, and the planet.\u202fReady to learn fast, create with audacity\u202fand lead boldly? Join our team.', 'One of the most competitive Paid Time Off plans in the industry.*', 'Employees can take up to five “on the clock” hours each month to volunteer at a charity of their choice.*', 'Excellent communication skills. Ability to effectively communicate with both technical and non-technical audiences.', 'Merchandise discount for our brands: 50% off regular-priced merchandise at Gap, Banana Republic and Old Navy, 30% off at Outlet and 25% off at Athleta for all employees.One of the most competitive Paid Time Off plans in the industry.*Employees can take up to five “on the clock” hours each month to volunteer at a charity of their choice.*Extensive 401(k) plan with company matching for contributions up to four percent of an employee’s base pay.*Employee stock purchase plan.*Medical, dental, vision and life insurance.*See more\xa0of the benefits we offer.', 'Maintain and support deployed solutions and data products.', 'Gap Inc. is an equal-opportunity employer and is committed to providing a workplace free from harassment and discrimination. We are committed to recruiting, hiring, training and promoting qualified people of all backgrounds, and make all employment decisions without regard to any protected status. We have received numerous awards for our long-held commitment to equality and will continue to foster a diverse and inclusive environment of belonging. This year, we’ve been named as one of the\xa0Best Places to Work by the Humans Rights Campaign\xa0for the fourteenth consecutive year and have been included in the\xa02019 Bloomberg Gender-Equality Index\xa0for the second year in a row.', '\xa0', 'Medical, dental, vision and life insurance.*', 'Extensive 401(k) plan with company matching for contributions up to four percent of an employee’s base pay.*', 'Solid programming foundations and proficiency with data related languages such as Python/Spark/R.', 'Develop, deploy, and support analytic data products, such as data marts, ETL’s (extract/transform/load), functions (in Python/SQL/Spark/R), and visualizations.', 'WHO YOU ARE', 'Merchandise discount for our brands: 50% off regular-priced merchandise at Gap, Banana Republic and Old Navy, 30% off at Outlet and 25% off at Athleta for all employees.', 'Work closely with our data scientists to ensure production models are built using a scalable back-end.', '1-3 years of experience in a data engineering or full-stack data scientist role.', ""WHAT YOU'LL DO"", 'See more\xa0of the benefits we offer.', 'Navigate various data sources and efficiently locate data in a complex data ecosystem.', 'ABOUT THE ROLE', 'Our brands bridge the gaps we see in the world.\u202fOld Navy democratizes style to ensure everyone\u202fhas access to quality fashion at every price point. Athleta unleashes the potential of every woman,\u202fregardless of body size, age or ethnicity. Banana\u202fRepublic believes in sustainable luxury for all. And Gap\u202finspires the world to bring individuality to modern, responsibly made\u202fessentials.\xa0', '\ufeff*For eligible employees', 'ABOUT GAP INC.', 'Employee stock purchase plan.*', 'Strong understanding of relational databases and SQL.', 'This position will be part of the Data Science Engineering team at Gap Inc, whose primary goals include building data products and infrastructure that support analytics and data-science at scale. With business users all across the company, the team works cross-functionally to ensure reports, analytics, and models are supported by a stable, efficient, and accurate back-end.', 'BA/BS in a technical or engineering field (Master’s preferred).', 'BENEFITS AT GAP INC.']",Associate,Full-time,Engineering,Retail,2021-03-24 13:05:10
Data Management and Integration Engineer,Varo Bank,"San Francisco, CA",19 hours ago,Be among the first 25 applicants,"['', 'Work closely with the Varo Data Lake team to design, build and maintain data pipelines into and out of Varo’s Data Lake using AWS Glue, Athena, S3 and Python.', 'CULTURAL ALIGNMENT', ""Previous Skills And Experiences That'll Help You Be Great"", 'Conduct code reviews in accordance with team processes and standards.', 'Stay Curious:', 'Knowledge of data governance concepts how Data Governance relates to Data Quality Management, metadata management, and MDM.', 'Provide technical capabilities in the area of data systems development including data ingestion, data curation, data storage, high-throughput data processing, analytics.', ""Bachelor's degree in Computer Science, MIS, Engineering or related field, or relevant work experience.2+ years coding experience in Python, Java or other object-oriented languages.2+ years’ experience working within the Hadoop/Spark2 ecosystem.2+ years of ETL, data modeling, warehouse and data pipelines experience.Knowledge of data governance concepts how Data Governance relates to Data Quality Management, metadata management, and MDM.Experience in data integration platforms, application integration/messaging platforms.Experience with downstream consumption patterns is a plus (reports, dashboards, API).AWS Glue, S3, RDS, REST/SOAP API, Kafka and Airflow are a plus.A passion for banking and finance.Previous experience with data quality management and data lineage tools is a plus."", 'Experience with downstream consumption patterns is a plus (reports, dashboards, API).', 'Work closely with Varo’s Data Enablement team to develop a data governance framework using leading AI driven Data Quality, MDM and lineage tools.Work closely with the Data Governance and Data Technology teams to integrate Varo’s data governance tooling with Varo’s data processing pipelines.Be responsible for development and maintenance of data governance tooling and processes that will span Varo’s platform and service all operational banking functions.Work closely with the Varo Data Lake team to design, build and maintain data pipelines into and out of Varo’s Data Lake using AWS Glue, Athena, S3 and Python.Provide technical capabilities in the area of data systems development including data ingestion, data curation, data storage, high-throughput data processing, analytics.Participate in developing and enforcing data security & access control policies and implementing effective controls for a resilient data ingestion process.Conduct code reviews in accordance with team processes and standards.', 'About The Role', 'Take Ownership:', 'OUR CORE VALUES', 'Make it Better:', ""Bachelor's degree in Computer Science, MIS, Engineering or related field, or relevant work experience."", 'Beware of fraudulent job postings!', ""What You'll Do"", 'Work closely with Varo’s Data Enablement team to develop a data governance framework using leading AI driven Data Quality, MDM and lineage tools.', '2+ years’ experience working within the Hadoop/Spark2 ecosystem.', 'About Varo', 'AWS Glue, S3, RDS, REST/SOAP API, Kafka and Airflow are a plus.', 'Experience in data integration platforms, application integration/messaging platforms.', '2+ years of ETL, data modeling, warehouse and data pipelines experience.', '2+ years coding experience in Python, Java or other object-oriented languages.', 'Learn More About Varo By Following Us', 'A passion for banking and finance.', 'Previous experience with data quality management and data lineage tools is a plus.', 'Be responsible for development and maintenance of data governance tooling and processes that will span Varo’s platform and service all operational banking functions.', 'Customers First:', 'Work closely with the Data Governance and Data Technology teams to integrate Varo’s data governance tooling with Varo’s data processing pipelines.', 'Respect:', 'Participate in developing and enforcing data security & access control policies and implementing effective controls for a resilient data ingestion process.']",Entry level,Full-time,Quality Assurance,Computer Software,2021-03-24 13:05:10
Data Engineer,"Fetch Rewards, Inc.","Chicago, IL",2 weeks ago,Be among the first 25 applicants,"['', 'Interest in building and experimenting with different tools and tech, and sharing your learnings with the broader organization', 'REQUIRED:', 'ETL process, data pipeline, and/or micro-service development experience', 'Why Join the Fetch Family?', 'Familiarity with messaging and asynchronous technologies (e.g., SQS, Kinesis, RabbitMQ, Kafka)', 'Who We Are', ' REQUIRED: Python programming skills Solid SQL skills Familiarity with Unix systems, shell scripting, and Git Experience with relational (SQL), non-relational (NoSQL), and/or object data stores (e.g., Snowflake, MongoDB, S3, HDFS, Postgres, Redis, DynamoDB) Interest in building and experimenting with different tools and tech, and sharing your learnings with the broader organization The desire to work with other teams in the organization (e.g., Development, Business Intelligence, Data Science) to build tools and solutions that support and help manage data within the Fetch ecosystem Bachelor’s degree in Computer Science (or equivalent) At least 3 years of relevant full-time work experience ', 'REQUIRED: Python programming skills', 'Big data development skills (e.g., Spark, Hadoop, MPP DW)', 'Bachelor’s degree in Computer Science (or equivalent)', 'Cloud engineering and DevOps skills (e.g., AWS, CloudFormation, Docker)', ""Love of Dogs! . . . Or just tolerance. We're a very canine-friendly workplace"", 'Fetch Rewards is an equal employment opportunity employer.', 'Familiarity with open source software and dependency management', 'Experience with relational (SQL), non-relational (NoSQL), and/or object data stores (e.g., Snowflake, MongoDB, S3, HDFS, Postgres, Redis, DynamoDB)', 'Bonus Points For', 'At least 3 years of relevant full-time work experience', 'The desire to work with other teams in the organization (e.g., Development, Business Intelligence, Data Science) to build tools and solutions that support and help manage data within the Fetch ecosystem', 'Experience with visualization tools (e.g., Tableau)', 'Solid SQL skills', 'The Role', "" Excellent written and verbal communication skills Familiarity with open source software and dependency management ETL process, data pipeline, and/or micro-service development experience Cloud engineering and DevOps skills (e.g., AWS, CloudFormation, Docker) Familiarity with messaging and asynchronous technologies (e.g., SQS, Kinesis, RabbitMQ, Kafka) Big data development skills (e.g., Spark, Hadoop, MPP DW) Experience with visualization tools (e.g., Tableau) Love of Dogs! . . . Or just tolerance. We're a very canine-friendly workplace"", 'REQUIRED: ', 'Familiarity with Unix systems, shell scripting, and Git', 'Excellent written and verbal communication skills']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer - ou4Tefwp,Ontario Systems,"Indianapolis, IN",6 days ago,Be among the first 25 applicants,[],Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,PEAK6 ,"Dallas, TX",3 weeks ago,43 applicants,"['', 'You don’t like change', 'Is motivated', 'Experience with the Microsoft BI Stack (SSIS, SSAS, SSRS) preferred', 'Make us better. Identify, advocate for, and implement solutions to improve performance and efficiencies across systems, APIs, and overnight batch processing. Develop quality code that is maintainable and avoids problems. Promote a culture for effective documentation and lessons-learned.', 'Be a great team member. Work as a member of an agile software development team to rapidly produce software. Balance both project-based and day-to-day support tasks. ', 'Make us better.', '2+ years of experience in data warehousing and ETL processes', 'Live our culture. Embrace Apex’s values as our differentiator and be an example of them every day.', 'Bachelor’s degree in Computer Science, Computer Engineering or a similar field', 'Is collaborative. ', 'Live our culture.', 'You’re not the collaborative type', 'Wants to make an impact. You’re looking to do amazing work. You value preventing problems from occurring over being caught in the chaos zone putting out fires and looking for the “hero” spotlight', ' Bachelor’s degree in Computer Science, Computer Engineering or a similar field 2+ years of Microsoft SQL Development Experience 2+ years of experience in data warehousing and ETL processes 1+ years of experience with Python Advanced problem-solving, debugging, and troubleshooting skills Excellent client support skills Proficient with version control systems, ideally GitHub Ability and willingness to learn new things (languages, tools, frameworks) quickly Experience with the Microsoft BI Stack (SSIS, SSAS, SSRS) preferred Experience with NoSQL technologies preferred Financial services background preferred ', 'See the data. Build reports, analytics and visualizations to help meet business initiatives and make decisions.', 'Learn the data. Utilize excellent analytical and problem solving techniques to understand our complex data structures and put the data to work. Participate in all phases of the development process.', 'We see tech differently. You’ll work with people who are leaders in the tech industry. We are passionate engineers dedicated to finding new and different ways to use technology to solve our customers’ problems.', '2+ years of Microsoft SQL Development Experience', 'Is passionate', 'Work the data.', 'You’re a tech snob', 'See the data.', 'Show off your work. Embrace transparency and share metrics around our levels of service with the rest of the company and our customers.', 'Wants to make an impact', ' Is passionate. You have a genuine passion for technology. You love using technology differently to maximize opportunity and impact for customers and you have a way of bringing out that same fire in the people you work with Is motivated. You’re driven to be the best – whether that’s decreasing system down time or making an innovative change to “how it’s always been done” resulting in a more efficient way of supporting the customer. You challenge yourself by setting goals and exceeding them Is collaborative. You’re excited to work with fellow engineers and big thinkers. You know how to collaborate not only within the department, but also across the organization Wants to make an impact. You’re looking to do amazing work. You value preventing problems from occurring over being caught in the chaos zone putting out fires and looking for the “hero” spotlight Strives for frictionless IT – You understand the importance of building great partnerships. You promote a seamless, smooth, user friendly and reliable environment. ', 'We see tech differently.', ' We’re a leader in the space. Apex is recognized for disrupting the financial services industry, enabling fintech standouts like Stash, Webull and Betterment. We’ve got an amazing track record of success and we foster ongoing innovation. So you get all the benefits of a proven, growing company, while enjoying a very entrepreneurial culture We see tech differently. You’ll work with people who are leaders in the tech industry. We are passionate engineers dedicated to finding new and different ways to use technology to solve our customers’ problems. Your work will have immediate impact. You’ll be able to see your direct impact on our tech department, our business, and with our clients. You won’t be just another talented engineer. ', 'You’re a tech snob. We deal with all technology – Linux and Windows; .NET, Java, and Python; SQL and noSQL (mongoDB); RabbitMQ and other open source tools and libraries. We love technology and want to work with all of it. If you’re wed to a particular tech, you may not like working for us.', 'Is collaborative. You’re excited to work with fellow engineers and big thinkers. You know how to collaborate not only within the department, but also across the organization', 'Experience with NoSQL technologies preferred', 'Is passionate. You have a genuine passion for technology. You love using technology differently to maximize opportunity and impact for customers and you have a way of bringing out that same fire in the people you work with', 'We’re a leader in the space. ', 'Strives for frictionless IT – You understand the importance of building great partnerships. You promote a seamless, smooth, user friendly and reliable environment.', 'You’re not the collaborative type. We work together to ensure the best possible solutions for our customers. We think two brains are better than one so we do most of our work together. Team work makes the dream work on this team.', 'Your work will have immediate impact.', 'Advanced problem-solving, debugging, and troubleshooting skills', ' Be a data wizard. Create, manipulate, access, and deliver data in the most efficient ways possible to business users, end-customers, 3rd-party vendors, and application developers Work the data. Create processes to load, transform, and deliver data to business users, end-customers, 3rd party vendors, and application developers. Learn the data. Utilize excellent analytical and problem solving techniques to understand our complex data structures and put the data to work. Participate in all phases of the development process. See the data. Build reports, analytics and visualizations to help meet business initiatives and make decisions. Make us better. Identify, advocate for, and implement solutions to improve performance and efficiencies across systems, APIs, and overnight batch processing. Develop quality code that is maintainable and avoids problems. Promote a culture for effective documentation and lessons-learned. Be a great team member. Work as a member of an agile software development team to rapidly produce software. Balance both project-based and day-to-day support tasks.  Show off your work. Embrace transparency and share metrics around our levels of service with the rest of the company and our customers. Live our culture. Embrace Apex’s values as our differentiator and be an example of them every day. ', 'Show off your work. ', 'Is motivated. You’re driven to be the best – whether that’s decreasing system down time or making an innovative change to “how it’s always been done” resulting in a more efficient way of supporting the customer. You challenge yourself by setting goals and exceeding them', 'Be a great team member.', 'We’re Looking For Someone Who', 'Be a data wizard. Create, manipulate, access, and deliver data in the most efficient ways possible to business users, end-customers, 3rd-party vendors, and application developers', 'Strives for frictionless IT ', 'What You’ll Do All Day', ' You don’t like change. This is not a job for someone who likes “predictable.” The job is based on the unknown which inevitably means change. If you like to know what you’re going to do every day, you may not like working on this team. You have to go with the flow here. You’re not the collaborative type. We work together to ensure the best possible solutions for our customers. We think two brains are better than one so we do most of our work together. Team work makes the dream work on this team. You’re a tech snob. We deal with all technology – Linux and Windows; .NET, Java, and Python; SQL and noSQL (mongoDB); RabbitMQ and other open source tools and libraries. We love technology and want to work with all of it. If you’re wed to a particular tech, you may not like working for us. ', 'You don’t like change. This is not a job for someone who likes “predictable.” The job is based on the unknown which inevitably means change. If you like to know what you’re going to do every day, you may not like working on this team. You have to go with the flow here.', 'Proficient with version control systems, ideally GitHub', 'The Skills You’ll Need To Succeed', 'Financial services background preferred', 'Be a data wizard.', 'And a few reasons why you may not like working for us:', 'Excellent client support skills', 'Ability and willingness to learn new things (languages, tools, frameworks) quickly', 'A Few Reasons Why You Might Love Us', 'We’re a leader in the space. Apex is recognized for disrupting the financial services industry, enabling fintech standouts like Stash, Webull and Betterment. We’ve got an amazing track record of success and we foster ongoing innovation. So you get all the benefits of a proven, growing company, while enjoying a very entrepreneurial culture', 'Work the data. Create processes to load, transform, and deliver data to business users, end-customers, 3rd party vendors, and application developers.', 'Learn the data.', 'Your work will have immediate impact. You’ll be able to see your direct impact on our tech department, our business, and with our clients. You won’t be just another talented engineer.', '1+ years of experience with Python']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Excelion Partners,"United, LA",2 weeks ago,Be among the first 25 applicants,"['', 'Preferred Experience', 'Experience', 'Key Responsibilities']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Point Card,"San Francisco, CA",2 weeks ago,62 applicants,"['', ' Experience working with AWS data products Experience with 3rd party integration tooling like Segment Experience maintaining Data Lake/Warehouse Experience supporting data analysts Experience building a data pipeline from scratch Have built a real-time data pipeline using a streaming technology like Kinesis or Kafka ', 'Create custom services and tooling when applicable', 'Implement disaster recovery procedures', ""Bachelor's degree in Computer Science, Computer Engineering, Applied Mathematics, Physics Statistics or equivalent work experience"", 'Experience building a data pipeline from scratch', 'Collaborate with engineers and data scientists on projects', 'Experience working with AWS data products', 'Requirements:', 'Expert in SQL and a scripting language like Python', 'Competitive salary, stock options, and 401K.', ' Opportunity to be part of a brand that is creating a new standard for financial services and enter at the ground floor of a fast growing, mission-driven company. Competitive salary, stock options, and 401K. Full health benefits (medical, dental, and vision insurance). Unlimited vacation policy, paid company holidays, and WFH flexible. We shut down the office at the end of the year for a winter holiday. Free Point Card membership + 10,000 ($100) monthly points Monthly stipends for continuous learning, health & wellness, and commuting.', 'Roles & Responsibilities:', 'Bonus Points (no pun intended):', 'Experience supporting data analysts', 'Full health benefits (medical, dental, and vision insurance).', "" Bachelor's degree in Computer Science, Computer Engineering, Applied Mathematics, Physics Statistics or equivalent work experience 5+ years of relevant professional experience Strong algorithmic and data modeling knowledge Experience working with a Hadoop or similar ecosystem Expert in SQL and a scripting language like Python "", 'Design and build scalable and robust data management systems', 'Point Perks:', 'Opportunity to be part of a brand that is creating a new standard for financial services and enter at the ground floor of a fast growing, mission-driven company.', ' Design and build scalable and robust data management systems Ensure that all systems meet industry best practices and business requirements Create custom services and tooling when applicable Implement disaster recovery procedures Collaborate with engineers and data scientists on projects ', 'Experience maintaining Data Lake/Warehouse', 'Have built a real-time data pipeline using a streaming technology like Kinesis or Kafka', 'Monthly stipends for continuous learning, health & wellness, and commuting.', '5+ years of relevant professional experience', 'Unlimited vacation policy, paid company holidays, and WFH flexible. We shut down the office at the end of the year for a winter holiday.', 'Experience working with a Hadoop or similar ecosystem', 'Data Engineer', 'Experience with 3rd party integration tooling like Segment', 'Ensure that all systems meet industry best practices and business requirements', 'Free Point Card membership + 10,000 ($100) monthly points', 'Strong algorithmic and data modeling knowledge']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer,Evolytics,"Kansas City, MO",3 weeks ago,Be among the first 25 applicants,"['', 'Working in big data solutions such as Hadoop, Hive, or Spark', 'Voted Coolest Office Space (Small Business) by the Kansas City Business Journal', 'Using change release processes and tools such as git', 'Create and maintain customized SQL queries to build reporting data structures.', 'Develop and implement data models necessary to build analytic solutions as defined by stakeholder requirements.', ' Working knowledge in any of the following database systems will be a plus: NoSQL, Mongo DB, Couch DB Working with clickstream web analytics tools such as Adobe Analytics (Omniture SiteCatalyst), Google Analytics, or working knowledge of the field of web analytics Knowledge of commonly-used digital metrics, analytic concepts, and online marketing channel best practices Working with data analysis tools such as SAS, Tableau, Google Data Studio, or Power BI Knowledge of business intelligence methodologies and tools Proficiency in spreadsheet and presentation technologies such as Excel, PowerPoint, or Google Docs Creating and preparing databases and tables for predictive modeling and data science applications ', 'Working with clickstream web analytics tools such as Adobe Analytics (Omniture SiteCatalyst), Google Analytics, or working knowledge of the field of web analytics', 'Learning opportunities: company-provided training, conferences and super-smart co-workers', ' Fortune’s Best Small and Medium Workplaces in 2020: #25 in US Honored for Best Places to Work in 2020 by the Kansas City Business Journal Recognized as a Great Place to Work in 2020 Voted Coolest Office Space (Small Business) by the Kansas City Business Journal Named Top Analytics Agency by the Digital Analytics Association in 2018 and again in 2020 ', 'Competitive Benefits Package including Health, Dental, Vision, and Life Insurance', 'Honored for Best Places to Work in 2020 by the Kansas City Business Journal', 'Other Experience That Is Helpful, But Not Required', 'Working with a leading analytics or relational database system, such as Redshift, Vertica, BigQuery, PostgreSQL, or MySQL', 'Proficiency in spreadsheet and presentation technologies such as Excel, PowerPoint, or Google Docs', 'Collaboration-oriented office space with plenty of room for working sessions or potlucks', 'Create, prepare, and maintain databases and tables to power reports, dashboards, predictive models, and downstream analysis.', 'CULTURE', ' Create, prepare, and maintain databases and tables to power reports, dashboards, predictive models, and downstream analysis. Plan, create, and fine-tune data pipelines and automation workflows. Design and build data infrastructure that enables actionable insights used to optimize digital marketing performance such as online advertising, social media marketing, websites, and mobile experiences. ', 'WHAT EXPERIENCE WOULD WE LIKE TO SEE?', 'Food…weekly lunches, daily snacks, fruits, beverages, unlimited coffee', ' Competitive Benefits Package including Health, Dental, Vision, and Life Insurance Great Compensation Package with Paid Time Off, Performance Bonuses and IRA Matching Contributions Opportunity to work alongside amazingly fun people who are passionate about delivering awesome in everything they do. ', 'Working knowledge in at least one of the following scripting languages: Python, Bash, Java, Scala, R, Perl, Node.js', 'Awesome team building events like a day at the Royals game or mini-golf with margaritas', 'Advanced SQL', 'Developing and implementing data transformation via ETL processes and data pipelines', ""What You'll Be Doing"", 'Opportunity to work alongside amazingly fun people who are passionate about delivering awesome in everything they do.', ' Develop processes and procedures for ingesting data from disparate sources. Build and maintain database architecture, including fine tuning and optimizing queries, data pipelines, and automation workflows. Create and maintain customized SQL queries to build reporting data structures. Develop and implement data models necessary to build analytic solutions as defined by stakeholder requirements. Validate data to determine and document any gaps between available data and requirements for reporting outputs and downstream analysis. Manage multiple client requests and detailed project activities at any one time to ensure accurate, timely and efficient reporting and analysis deliverables. ', 'Named Top Analytics Agency by the Digital Analytics Association in 2018 and again in 2020', 'Legally authorized to work in the United States without company sponsorship now or in the future', 'Design and build data infrastructure that enables actionable insights used to optimize digital marketing performance such as online advertising, social media marketing, websites, and mobile experiences.', 'Knowledge of commonly-used digital metrics, analytic concepts, and online marketing channel best practices', 'Validate data to determine and document any gaps between available data and requirements for reporting outputs and downstream analysis.', 'Recognized as a Great Place to Work in 2020', 'Great Compensation Package with Paid Time Off, Performance Bonuses and IRA Matching Contributions', 'About Evolytics', 'Developing cloud-based data solutions on Snowflake, AWS, Azure, or Google Cloud', ' Relaxed work environment: casual dress code, pool/ping pong table, treadmill desks Collaboration-oriented office space with plenty of room for working sessions or potlucks Awesome team building events like a day at the Royals game or mini-golf with margaritas Food…weekly lunches, daily snacks, fruits, beverages, unlimited coffee Learning opportunities: company-provided training, conferences and super-smart co-workers ', 'Knowledge of business intelligence methodologies and tools', 'Develop processes and procedures for ingesting data from disparate sources.', 'Minimum of a Bachelor’s degree in Computer Science, Information Systems, Business, Marketing, or a related discipline', 'Plan, create, and fine-tune data pipelines and automation workflows.', 'Manage multiple client requests and detailed project activities at any one time to ensure accurate, timely and efficient reporting and analysis deliverables.', 'Build and maintain database architecture, including fine tuning and optimizing queries, data pipelines, and automation workflows.', 'Relaxed work environment: casual dress code, pool/ping pong table, treadmill desks', 'Creating and preparing databases and tables for predictive modeling and data science applications', 'Benefits', 'Perks', 'Fortune’s Best Small and Medium Workplaces in 2020: #25 in US', ' Working knowledge in at least one of the following scripting languages: Python, Bash, Java, Scala, R, Perl, Node.js Linux command line Advanced SQL Working with a leading analytics or relational database system, such as Redshift, Vertica, BigQuery, PostgreSQL, or MySQL Developing cloud-based data solutions on Snowflake, AWS, Azure, or Google Cloud Working in big data solutions such as Hadoop, Hive, or Spark Using change release processes and tools such as git Developing and implementing data transformation via ETL processes and data pipelines Legally authorized to work in the United States without company sponsorship now or in the future Minimum of a Bachelor’s degree in Computer Science, Information Systems, Business, Marketing, or a related discipline ', 'Working with data analysis tools such as SAS, Tableau, Google Data Studio, or Power BI', 'Linux command line', 'Working knowledge in any of the following database systems will be a plus: NoSQL, Mongo DB, Couch DB', 'AWARDS']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer,Kforce Inc,"New York, NY",2 weeks ago,Be among the first 25 applicants,"['', ' Advanced reporting in SSRS and Power BI (or Tableau), including charts, drill through and sub-reports', ' Experience with developing user authentication and knowledge of security compliance is a plus', "" Bachelor's degree in Computer Science, Information Systems, or related fields"", ' Advanced data warehousing techniques using DAX', ' C# programming and debugging skills is a plus', ' Python to write ETL processes and automate repetitive tasks', ' Responsible for installing, customizing, upgrading, testing and deploying third party vendor applications', ' Pulling data from various vendor applications and feeding into data warehouse', ' Experience with SQL Server encryption is a plus', ' Experience in designing custom workflow activities using Dynamics CRM SDK', ' Create front-end applications/workflows to solve immediate as well as long-term requirements', 'Requirements', ' Participate in code reviews, developer/stand-up meetings, and technology business partner meetings', ' 5+ years of development and administration experience, including advanced TSQL, Stored Procedure, complex queries built from scratch, experience with very large databases and data warehousing/data lakes; Performance tuning and query optimization; Must be aware of modern SQL techniques', ' Writing advanced SQL queries to generate reports, maintain data warehouse and data analysis', 'Responsibilities', ""  Bachelor's degree in Computer Science, Information Systems, or related fields  5+ years of development and administration experience, including advanced TSQL, Stored Procedure, complex queries built from scratch, experience with very large databases and data warehousing/data lakes; Performance tuning and query optimization; Must be aware of modern SQL techniques  Advanced reporting in SSRS and Power BI (or Tableau), including charts, drill through and sub-reports  Advanced data warehousing techniques using DAX  Experience with SQL Server encryption is a plus  Experience in designing custom workflow activities using Dynamics CRM SDK  Experience with Dynamics 365, UiPath/RPA and Machine Learning algorithms is a plus  C# programming and debugging skills is a plus  Knowledge of API (REST, SOAP), API management and micro services is preferred  Experience with developing user authentication and knowledge of security compliance is a plus "", ' Experience with Dynamics 365, UiPath/RPA and Machine Learning algorithms is a plus', ' Implement code changes that improve stability, performance and support the scalability necessary to support future business growth; Work with team members to test and launch new features', ' Knowledge of API (REST, SOAP), API management and micro services is preferred', '  Writing advanced SQL queries to generate reports, maintain data warehouse and data analysis  Python to write ETL processes and automate repetitive tasks  Pulling data from various vendor applications and feeding into data warehouse  Create front-end applications/workflows to solve immediate as well as long-term requirements  Participate in code reviews, developer/stand-up meetings, and technology business partner meetings  Implement code changes that improve stability, performance and support the scalability necessary to support future business growth; Work with team members to test and launch new features  Responsible for installing, customizing, upgrading, testing and deploying third party vendor applications  Stay abreast of data privacy & security regulations and ensure compliance with PII, HIPAA, GDPR, PCI and regulations governing patient confidentiality and data security ', ' Stay abreast of data privacy & security regulations and ensure compliance with PII, HIPAA, GDPR, PCI and regulations governing patient confidentiality and data security']",Associate,Full-time,Information Technology,Accounting,2021-03-24 13:05:10
Data Engineer - Finance Products,Squarespace,"New York, NY",5 days ago,27 applicants,"['', 'You enjoy working with dynamic programming languages, relational databases, and distributed systems. Our platform is ever-evolving, but currently is a combination of Python, Java, Postgres, Kubernetes, Spark, Presto, Kafka, and Mongo', 'Fertility and adoption benefits', 'Equity plan for all employees', 'Experience working with financial systems is a plus', 'Up to 20 weeks of paid family leave', 'Health insurance with 100% premium covered for you and your dependent children', 'Benefits & Perks', 'Dog-friendly workplace in New York office', 'You are experienced at data modeling, storage, security, and retrieval', ""You love working directly with the people whose problems you're solving"", 'About Squarespace', '2+ years of industry experience', ' Health insurance with 100% premium covered for you and your dependent children Flexible vacation & paid time off Up to 20 weeks of paid family leave Equity plan for all employees Retirement benefits with employer match Fertility and adoption benefits Free lunch and snacks at all offices Education reimbursement Dog-friendly workplace in New York office Commuter benefit in the form of reduced tax (Ireland) and pretax (US) ', 'customer base', 'Education reimbursement', 'Guide our technical decisions', 'Encourage the technical growth of your teammates', 'Flexible vacation & paid time off', 'Retirement benefits with employer match', 'Free lunch and snacks at all offices', 'Responsibilities', "" Experience working with SQL Experience working with financial systems is a plus You love working directly with the people whose problems you're solving You are experienced at data modeling, storage, security, and retrieval You enjoy working with dynamic programming languages, relational databases, and distributed systems. Our platform is ever-evolving, but currently is a combination of Python, Java, Postgres, Kubernetes, Spark, Presto, Kafka, and Mongo You gain a deep understanding of the products and tools you work with 2+ years of industry experience "", ', but we also strive for the same in our employees. At Squarespace, we are committed to equal employment opportunity regardless of race, color, ethnicity, ancestry, religion, national origin, gender, sex, gender identity or expression, sexual orientation, age, citizenship, marital or parental status, disability, veteran status, or other class protected by applicable law. We are proud to be an equal opportunity workplace.', 'You gain a deep understanding of the products and tools you work with', 'Commuter benefit in the form of reduced tax (Ireland) and pretax (US)', 'Qualifications', ' Build and maintain data processing services Write, test, and review primarily microbatch or streaming ETL Continuously improve of our system, tests, and data quality indicators Guide our technical decisions Keep yourself up-to-date and informed about new technologies Encourage the technical growth of your teammates ', 'Keep yourself up-to-date and informed about new technologies', 'Today, more than a million people around the globe use Squarespace to share different perspectives and experiences with the world. Not only do we embrace and celebrate the diversity of our ', 'Continuously improve of our system, tests, and data quality indicators', 'Write, test, and review primarily microbatch or streaming ETL', 'Build and maintain data processing services', 'Experience working with SQL']",Mid-Senior level,Full-time,Information Technology,Investment Banking,2021-03-24 13:05:10
Data Engineer,EyeCare Partners,"Ballwin, MO",1 week ago,Be among the first 25 applicants,"['', 'Position Summary:', 'Coordinate the build and maintenance of data pipelines by third party service providers', 'Present solutions and options to leadership, project teams and other stakeholders adapting style to both technical and non-technical audiences', '3+ years of hands-on-experience in the design, development, and implementation of data solutions', 'Educate organization on available and emerging tool sets', 'Proactively monitor and resolve on-going production issues', 'Experience with data analysis, ETL, and workflow automation', ""Bachelor's degree in Computer Science, Information Systems or equivalent + 3 years related experience"", 'Work with data and business analysts to deploy and support a robust data cataloging strategy', ' Design, build and maintain data pipelines from various source systems into Snowflake Analyze data elements from various systems, data flow, dependencies, relationships and assist in designing conceptual physical and logical data models Design, build and maintain complex data sets designed to meet various business needs in the areas of reporting, advanced analytics and ad-hoc analysis Coordinate the build and maintenance of data pipelines by third party service providers Enabling and executing data migrations across systems (e.g. SQL server to Snowflake or other cloud data platforms) Development and implementation of scripts for datahub maintenance, monitoring, performance tuning Work with data and business analysts to deploy and support a robust data quality platform Work with data and business analysts to deploy and support a robust data cataloging strategy Work with various business and technical stakeholders and assist with data-related technical needs and issues Work with data and analytics teams and drive greater value from our data and analytics investments Work closely with cross-functional teams to understand and transform business requirements into scalable and manageable solutions Present solutions and options to leadership, project teams and other stakeholders adapting style to both technical and non-technical audiences Ensures teams adhere to documented design and development patterns and standards Proactively monitor and resolve on-going production issues Work closely with various technical teams to ensure consistency, quality of solutions and knowledge sharing across the enterprise Educate organization on available and emerging tool sets Ensure adherence to the approach of self-service data solutions and enable other teams with analytics solutions delivery via ‘Data as a Service’ model ', 'Analyze data elements from various systems, data flow, dependencies, relationships and assist in designing conceptual physical and logical data models', 'Enabling and executing data migrations across systems (e.g. SQL server to Snowflake or other cloud data platforms)', 'Experience with AWS cloud services: EC2, EMR, RDS, DMS', 'Experience with relational databases such as SQL Server and object relational databases such as PostgreSQL', 'Work with various business and technical stakeholders and assist with data-related technical needs and issues', 'Ensures teams adhere to documented design and development patterns and standards', 'Data Engineer – Information Technology', 'Essential Responsibilities', 'Demonstrated problem solving', 'Requirements', 'Work with data and analytics teams and drive greater value from our data and analytics investments', 'Experience with Snowflake development and support', 'Work closely with various technical teams to ensure consistency, quality of solutions and knowledge sharing across the enterprise', 'Experience working with multiple ETL/ELT tools and cloud based data hubs', 'Ensure adherence to the approach of self-service data solutions and enable other teams with analytics solutions delivery via ‘Data as a Service’ model', 'Development and implementation of scripts for datahub maintenance, monitoring, performance tuning', 'Advanced SQL knowledge with strong query writing, stored procedures skills', 'Design, build and maintain complex data sets designed to meet various business needs in the areas of reporting, advanced analytics and ad-hoc analysis', 'Experience with object-oriented/object function scripting languages: Python, Java, Scala, etc.', 'Work with data and business analysts to deploy and support a robust data quality platform', 'Experience with stream-processing systems: Storm, Spark-Streaming, etc.', 'Design, build and maintain data pipelines from various source systems into Snowflake', 'Demonstrated ability to think and work with a proactive mindset', 'A self-motivated personality with a passion for working in a fast-paced environment', 'Work closely with cross-functional teams to understand and transform business requirements into scalable and manageable solutions', "" Bachelor's degree in Computer Science, Information Systems or equivalent + 3 years related experience 3+ years of hands-on-experience in the design, development, and implementation of data solutions Advanced SQL knowledge with strong query writing, stored procedures skills Experience with Snowflake development and support Experience with object-oriented/object function scripting languages: Python, Java, Scala, etc. Experience with AWS cloud services: EC2, EMR, RDS, DMS Experience with relational databases such as SQL Server and object relational databases such as PostgreSQL Experience with stream-processing systems: Storm, Spark-Streaming, etc. Experience with data analysis, ETL, and workflow automation Experience working with multiple ETL/ELT tools and cloud based data hubs Demonstrated problem solving Demonstrated ability to think and work with a proactive mindset A self-motivated personality with a passion for working in a fast-paced environment""]",Associate,Full-time,Information Technology,Medical Practice,2021-03-24 13:05:10
Data Engineer,Progressive Insurance,"Mayfield, OH",3 weeks ago,31 applicants,"['', 'Onsite gym and healthcare at large locations', 'Diverse, inclusive and welcoming culture with Employee Resource Groups', 'Employee Status', ""Bachelor's Degree or higher in an Information Technology discipline or related field of study and minimum of one year work experience designing, programming, and supporting software programs or applications"", '401(k) with dollar-for-dollar company match up to 6%', "" Bachelor's Degree or higher in an Information Technology discipline or related field of study and minimum of one year work experience designing, programming, and supporting software programs or applications Instead of a degree, minimum of two years related work described in above bullet "", 'Gainshare bonus up to 24% of your eligible earnings; Progressive rewards each of us with an annual bonus based on company performance', 'Experience deploying or working with machine learning models. ', 'Medical, dental and vision, including free preventive care', 'Data Engineer Senior', 'Cloud experience - we use AWS (EC2, S3, Athena, Lambda, Aurora, etc.)', 'Schedule', ' Cloud experience - we use AWS (EC2, S3, Athena, Lambda, Aurora, etc.) Experience with Linux, Python, Git, Terraform, SQL, building APIs (Flask), data orchestration  Ability to develop and support web applications using a popular web framework Experience deploying or working with machine learning models.  Self-learner with ability to scope and recommend new tools (patterns, cloud services, etc.) as required by projects ', 'Experience with Linux, Python, Git, Terraform, SQL, building APIs (Flask), data orchestration ', 'Preferred Skills', 'Sponsorship for work authorization for foreign national candidates is not available for this position.', 'Must-have Qualifications', 'Primary Location', ' Gainshare bonus up to 24% of your eligible earnings; Progressive rewards each of us with an annual bonus based on company performance 401(k) with dollar-for-dollar company match up to 6% Diverse, inclusive and welcoming culture with Employee Resource Groups Career development and tuition assistance Onsite gym and healthcare at large locations Wellness programs to help you maintain a better quality of life Medical, dental and vision, including free preventive care ', 'Work From Home', 'Ability to develop and support web applications using a popular web framework', 'Wellness programs to help you maintain a better quality of life', 'Benefits', 'Career development and tuition assistance', 'Data Engineer', 'Job', 'Self-learner with ability to scope and recommend new tools (patterns, cloud services, etc.) as required by projects', 'Instead of a degree, minimum of two years related work described in above bullet']",Not Applicable,Full-time,Information Technology,Insurance,2021-03-24 13:05:10
Data Engineer (x2 Openings),Rev.io,"Atlanta, GA",1 week ago,Be among the first 25 applicants,"['', '1-2 years minimum of professional experience working with SQL (required)', 'Company Overview', 'Position Description', 'Bachelor’s Degree in Database Management, Information Technology, Computer Science, Computer Information Science, or other database and software development-focused majors (preferred)', 'Atlassian products (JIRA, Confluence, etc.)', 'Full Microsoft Stack (Visual Studio 2013, SQL Server 2014/2019, HTML5/JS)', 'Responsible for working with teammates and clients to determine innovative solutions to problems experienced when installing new customers.', 'Strong debugging skills', 'Gain exposure to large customer data sets involved in product migrations, and responsible for helping data flow into Rev.io from legacy customer systems.', 'Implement client-driven reporting and data migration needs within the Rev.io platform with exposure to user interface and database changes using SQL and ETL tools.Responsible for working with teammates and clients to determine innovative solutions to problems experienced when installing new customers.Involved in the database testing and will actively participate in workflows involving business intelligence and reporting.Gain exposure to large customer data sets involved in product migrations, and responsible for helping data flow into Rev.io from legacy customer systems.Hands-on experience working with live client systems and configuring real production environments.', 'Involved in the database testing and will actively participate in workflows involving business intelligence and reporting.', 'Hands-on experience working with live client systems and configuring real production environments.', 'Have worked with BI/Visualization tools (preferred)', 'Have worked with ETL tools for data migration (preferred)', 'Strong communicator, self-driven, and ability to meet deadlines', '1-2 years minimum of professional experience working with SQL (required)1-2 years of experience with T-SQL (preferred)Bachelor’s Degree in Database Management, Information Technology, Computer Science, Computer Information Science, or other database and software development-focused majors (preferred)Have worked with ETL tools for data migration (preferred)Have worked with BI/Visualization tools (preferred)Strong communicator, self-driven, and ability to meet deadlinesAbility to work full-time onsite daily and work with the teamStrong debugging skills', 'Implement client-driven reporting and data migration needs within the Rev.io platform with exposure to user interface and database changes using SQL and ETL tools.', '1-2 years of experience with T-SQL (preferred)', 'Python/Spark', 'Atlassian products (JIRA, Confluence, etc.)Full Microsoft Stack (Visual Studio 2013, SQL Server 2014/2019, HTML5/JS)Python/Spark', 'Ability to work full-time onsite daily and work with the team']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
GCP Data Engineer – 100% remote,Elsdon Consulting ltd,United States,5 days ago,50 applicants,"['', 'GCP Data Engineer - 100% in EST time zone', 'Eminent and prestigious consulting company', 'What you need:', 'Embrace ultimate flexibility and get exposed to unparalleled variety in your work. My client is a prestigious Google Cloud Consulting Partner who prides themselves on providing quality bespoke data analytics solutions. This company has multiple locations across the US, but they also embrace the benefits of remote work. As a result, they have consultants all across the US who work fully remote. ', 'Bespoke progression paths based on your individual strengths and ambissions', 'They are a US based firm with international outreach and a diverse enough client book to guarantee stability, as well as ensuring that each project you take on has a unique flavour. ', 'Experience with Data Warehousing and Data Lakes ', 'Software engineering experience with either Python or R', 'Certifications paid for by your employer', 'Ability to communicate with stakeholders and clients', 'Extensive experience with Google Cloud Platform, including Big Query, Data Flow and Data Proc', 'Experience working with Business Intelligence', 'ETL experience ', 'Industry-leading training programmes at all levels', 'Extensive experience with Google Cloud Platform, including Big Query, Data Flow and Data ProcExperience with Apache suite tools such as Hadoop, Spark and HiveExperience with Data Warehousing and Data Lakes Experience with Data LakesETL experience Software engineering experience with either Python or RExperience working with Business IntelligenceDrive to continually learn about cutting edge technologiesAbility to communicate with stakeholders and clients', 'Experience with Data Lakes', 'Eminent and prestigious consulting companyIndustry-leading training programmes at all levelsCertifications paid for by your employerBespoke progression paths based on your individual strengths and ambissionsSee the impact of your work on a range of businessesWork closely with experts in their fields', 'Work closely with experts in their fields', 'See the impact of your work on a range of businesses', 'Experience with Apache suite tools such as Hadoop, Spark and Hive', 'Reasons to apply:', 'Drive to continually learn about cutting edge technologies']",Mid-Senior level,Full-time,Information Technology,Staffing and Recruiting,2021-03-24 13:05:10
data engineer,IT OPENDOORS LLC,"Texas City, TX",2 weeks ago,Be among the first 25 applicants,[],Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Science Engineer,Stripe,"San Francisco, CA",3 days ago,144 applicants,"['', ' Develop unified user data schemas and tables that provide a complete view of the business across our various products such as Stripe Connect, Atlas, or Sigma Build data pipelines that track our marketing funnel from visits to onboarding to active usage of Stripe Work on our centralized experimentation platform to pipeline experiment metrics and compute descriptive statistics Improve our data visualization tooling and platform at Stripe to help the team create dynamic tools and reporting Our stack spans tools in Scala, Python, R, Javascript, React, SQL  ', 'You Will', 'Build data pipelines that track our marketing funnel from visits to onboarding to active usage of Stripe', ' 3+ Years of experience in a Data Engineering or Data Science role, with a focus on building data pipelines or conducting data intensive analysis.  A strong engineering background and are interested in data Prior experience with writing and debugging data pipelines using a distributed data framework (Hadoop/Spark/Pig etc…) An inquisitive nature in diving into data inconsistencies to pinpoint issues Knowledge of a scientific computing language (such as R or Python) and SQL The ability to communicate cross-functionally, derive requirements and architect shared datasets ', 'Our stack spans tools in Scala, Python, R, Javascript, React, SQL ', 'A strong engineering background and are interested in data', 'Knowledge of a scientific computing language (such as R or Python) and SQL', 'The ability to communicate cross-functionally, derive requirements and architect shared datasets', 'Some Things You Might Work On', 'Prior experience with writing and debugging data pipelines using a distributed data framework (Hadoop/Spark/Pig etc…)', 'Drive the collection of new data and the refinement of existing data sources, develop relationships with production engineering teams to manage our data structures as the Stripe product evolves ', 'Work on our centralized experimentation platform to pipeline experiment metrics and compute descriptive statistics', 'Help the Data Science team apply and generalize statistical and econometric models on large datasets', 'Resume', 'Develop strong subject matter expertise and manage the SLAs for those data pipelines ', 'Improve our data visualization tooling and platform at Stripe to help the team create dynamic tools and reporting', 'LinkedIn profile', 'Design, develop, and own data pipelines and models that power internal analytics for product and business teams ', 'Develop unified user data schemas and tables that provide a complete view of the business across our various products such as Stripe Connect, Atlas, or Sigma', 'You Should Include These In Your Application', '3+ Years of experience in a Data Engineering or Data Science role, with a focus on building data pipelines or conducting data intensive analysis. ', ' Identify data needs for business and product teams, understand their specific requirements for metrics and analysis, and build efficient and scalable data pipelines to enable data-driven decisions across Stripe Design, develop, and own data pipelines and models that power internal analytics for product and business teams  Help the Data Science team apply and generalize statistical and econometric models on large datasets Drive the collection of new data and the refinement of existing data sources, develop relationships with production engineering teams to manage our data structures as the Stripe product evolves  Develop strong subject matter expertise and manage the SLAs for those data pipelines  ', 'An inquisitive nature in diving into data inconsistencies to pinpoint issues', 'We’re Looking For Someone Who Has', 'Identify data needs for business and product teams, understand their specific requirements for metrics and analysis, and build efficient and scalable data pipelines to enable data-driven decisions across Stripe', ' Resume LinkedIn profile']",Mid-Senior level,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Data Engineer,Experis,"Herndon, VA",4 weeks ago,Be among the first 25 applicants,"['', 'Must be able to join tables and produce a statement / required output in SQL.', "" Bachelor's Degree in Computer Science, Information Systems, Mathematics, Statistics, or related field 5+ years of experience with Data modeling, SQL, ETL , Data Warehousing and Datalakes 5+ years of experience writing SQL scripts / Expert knowledge in an enterprise class RDBMS 2+ years of experience with enterprise-class Business Intelligence tools such as PowerBI, Tableau, Oracle BI, Penthao, etc. "", 'Experience with MPP databases such as Redshift', 'Experience with Datalake development', ' Gather requirements, prototype, sandbox (beta) test, launch, and support/enhance/maintain Tableau dashboards that directly support Data Center Operations (DCO) and Logistics Teams globally. These products must be accurate down to the individual server and component level for operational action, and will serve as the source of truth for global operations going forward. ', '5+ years of experience writing SQL scripts / Expert knowledge in an enterprise class RDBMS', 'These products must be accurate down to the individual server and component level for operational action, and will serve as the source of truth for global operations going forward.', 'Not available for C2C or visa sponsorship', '2+ years of experience with enterprise-class Business Intelligence tools such as PowerBI, Tableau, Oracle BI, Penthao, etc.', ' Must be able to join tables and produce a statement / required output in SQL. Data Visualization experience ', 'Excellent verbal/written communication & data presentation skills, including ability to succinctly summarize key findings and effectively communicate with both business and technical teams.', '5+ years of experience with Data modeling, SQL, ETL , Data Warehousing and Datalakes', 'Exposure to noSQL databases (such as DynamoDB, MongoDB)', ' Excellent verbal/written communication & data presentation skills, including ability to succinctly summarize key findings and effectively communicate with both business and technical teams. Comfortable working in a Linux environment Experience with MPP databases such as Redshift Knowledge of AWS products and services Experience with Datalake development Exposure to noSQL databases (such as DynamoDB, MongoDB) ', 'Preferred Skills', 'Responsibilities', 'Comfortable working in a Linux environment', ""Bachelor's Degree in Computer Science, Information Systems, Mathematics, Statistics, or related field"", 'Gather requirements, prototype, sandbox (beta) test, launch, and support/enhance/maintain Tableau dashboards that directly support Data Center Operations (DCO) and Logistics Teams globally.', 'Data Visualization experience', 'Knowledge of AWS products and services', 'Required Skills']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Analytics Engineer,Smartsheet,"Bellevue, WA",5 days ago,79 applicants,"['', 'Automate manual/ad hoc processes', 'You Will', 'Establish rules/processes to ensure proper access levels and consistent enrichment creation', '100% employer-paid medical, dental, and vision coverage for full-time employees', 'Knowledge of BI team priorities and company initiatives requiring analytics', ""3+ years of experience in SQL coding in a professional environmentMultiple years of experience in Snowflake's data warehouseKnowledge of the Smartsheet core and enrichment schemas in SnowflakeKnowledge of BI team priorities and company initiatives requiring analyticsStrong analytical thinking and collaborative skills in partnering with analyst and cross-functional teamsExperience with Gitlab and job scheduling apps such as Airflow and Jenkins"", 'Experience with Gitlab and job scheduling apps such as Airflow and Jenkins', 'Equal Opportunity Employer', 'Teleworking options from any registered location in the U.S. (role specific)', '401k Match to help you save for your future (50% of your contribution up to the first 6% of your eligible pay)', 'Transform raw company and 3rd party data into enrichment tables', 'Equity - Restricted Stock Units (RSUs) Equity with all offers', 'Up to 24 weeks of Parental Leave', 'Lucrative Employee Stock Purchase Program (15% discount)', 'Establish trusted singular sources of truth for operational metrics', 'Support other duties', 'Personal paid Volunteer Day to support our community', 'Monthly stipend to support your work and productivity', 'Ensure all legal/privacy/SOX requirements are met in Snowflake enrichments', 'You Have', 'Provide proper data accessibility and guidelines to analyst teams outside of BI', 'Opportunities for professional growth and development including access to Audible for Business and LinkedIn Learning online courses', 'Company Funded Perks, including a counseling membership, primary care membership, local retail discounts, and your own personal Smartsheet account', ""Multiple years of experience in Snowflake's data warehouse"", 'Knowledge of the Smartsheet core and enrichment schemas in Snowflake', '100% employer-paid medical, dental, and vision coverage for full-time employeesEquity - Restricted Stock Units (RSUs) Equity with all offersLucrative Employee Stock Purchase Program (15% discount)401k Match to help you save for your future (50% of your contribution up to the first 6% of your eligible pay)Monthly stipend to support your work and productivity15 days PTO to start, plus Flexible Sick LeaveUp to 24 weeks of Parental LeavePersonal paid Volunteer Day to support our communityOpportunities for professional growth and development including access to Audible for Business and LinkedIn Learning online coursesCompany Funded Perks, including a counseling membership, primary care membership, local retail discounts, and your own personal Smartsheet accountTeleworking options from any registered location in the U.S. (role specific)', '3+ years of experience in SQL coding in a professional environment', 'Perks & Benefits', 'Own the Snowflake enrichment code base designEstablish rules/processes to ensure proper access levels and consistent enrichment creationTransform code from core systems to an enriched usable formatCollaborate with BI team members in the creation of enrichments and data productsAutomate manual/ad hoc processesTransform raw company and 3rd party data into enrichment tablesEstablish trusted singular sources of truth for operational metricsProvide proper data accessibility and guidelines to analyst teams outside of BIEnsure all legal/privacy/SOX requirements are met in Snowflake enrichmentsSupport other duties', 'Transform code from core systems to an enriched usable format', 'Collaborate with BI team members in the creation of enrichments and data products', 'Strong analytical thinking and collaborative skills in partnering with analyst and cross-functional teams', '15 days PTO to start, plus Flexible Sick Leave', 'Own the Snowflake enrichment code base design']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer,TriCom Technical Services,"Kansas City, MO",1 week ago,36 applicants,"[' Create strategic, innovative software solutions to create customer value from data. Design and implement effective database models to store and retrieve company data. Implement and extend streaming data, E (L) TL, data cataloging, and reporting frameworks. Provide documentation to support implementation deigns. Support existing infrastructure and troubleshoot issues related to data. Research and create PoCs for new data related technologies. Analyze internal and external data sources for opportunities to drive optimization and improvement of products and services. ', 'Research and create PoCs for new data related technologies.', 'Data Engineer', '3 years of experience with C#, Java, or Scala.', 'Experience with Cloudera/Hortonworks Hadoop distribution.', 'Responsibilities', 'Support existing infrastructure and troubleshoot issues related to data.', 'Create strategic, innovative software solutions to create customer value from data.', 'Provide documentation to support implementation deigns.', 'Analyze internal and external data sources for opportunities to drive optimization and improvement of products and services.', 'Design and implement effective database models to store and retrieve company data.', 'Requirements', ' 3 years of experience with C#, Java, or Scala. Experience with advance database systems crossing relational and non-relational including MS SQL, Hive, MySQL, MongoDB, and Elasticsearch. Experience with Cloudera/Hortonworks Hadoop distribution. ', 'Implement and extend streaming data, E (L) TL, data cataloging, and reporting frameworks.', 'Experience with advance database systems crossing relational and non-relational including MS SQL, Hive, MySQL, MongoDB, and Elasticsearch.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Alteryx,"Ann Arbor, MI",3 weeks ago,91 applicants,"['', ' Strong problem-solving skills and attention to detail ', ' Ability to work with other teams across an organization ', ' Understanding of cloud infrastructure ', ' Extreme focus on detail and data quality validation ', ' Intermediate SQL experience ', ' Intermediate SQL experience  Strong problem-solving skills and attention to detail  Ability to work with geographically distributed teams  Ability to work with other teams across an organization  Strong oral and written communication ', ' Experience with Alteryx platform ', ' Knowledge of at least one modern scripting language (Python, R, etc.) ', ' Deep interest in the data and analytics market with ability to constantly evaluate new ways to enhance the telemetry system and train it to be smarter and more scalable ', ' Strong oral and written communication ', 'Benefits & Perks', ' Experience supporting or working in enterprise analytics environments ', ' Experience with Snowflake ', ' 3+ years of data engineering / development / integration experience ', ' Execute architected techniques and solutions for data collection, management and usage ', ' Take business requirements and produce data sets for efficient cross-business analysis  Extreme focus on detail and data quality validation  Deep interest in the data and analytics market with ability to constantly evaluate new ways to enhance the telemetry system and train it to be smarter and more scalable ', ' Experience with automated software testing and deployment ', ' Knowledge of at least one modern scripting language (Python, R, etc.)  Understanding of cloud infrastructure  Experience with automated software testing and deployment  Experience supporting or working in enterprise analytics environments ', ' Helpful to Have ', ' Comfortable with data modeling practices (normalizing, dimensionalizing ) ', ' BA/BS degree in Information Science, Data Analytics, Computer Science, Software Engineering or related technical field ', 'Position Details/requirements', 'Qualifications', ' Experience with Enterprise Data Warehouse development ', ' Need to Have ', ' Build, test, monitor and maintain a highly scalable data management ecosystem  Execute architected techniques and solutions for data collection, management and usage ', 'Compensation', ' Build, test, monitor and maintain a highly scalable data management ecosystem ', ' Ability to work with geographically distributed teams ', ' Take business requirements and produce data sets for efficient cross-business analysis ', ' 3+ years of data engineering / development / integration experience  Experience with Enterprise Data Warehouse development  Comfortable with data modeling practices (normalizing, dimensionalizing )  Ability to read and create ERDs ', ' BA/BS degree in Information Science, Data Analytics, Computer Science, Software Engineering or related technical field  Experience with Alteryx platform  Experience with Snowflake ', ' Ability to read and create ERDs ']",Not Applicable,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Lead Data Engineer,"Disney Parks, Experiences and Products","Lake Buena Vista, FL",1 week ago,Be among the first 25 applicants,"['', 'Production experience with DevOps Tools like: Terraform, Chef, Anisle, Docker, Jenkins.', 'Mathematical background relevant to Machine Learning concepts', 'Preferred Qualifications', 'Product ownership and strong business support in theme park ticketing space', 'Strong conceptual understanding of security at rest and in transit for big data pipelines.', 'Troubleshooting skills, ability to determine impacts, ability to resolve complex issues, and ability to exercise sound judgment and initiative in stressful situations.', 'Job Summary', 'Conceptual understanding of NoSQL databases, and ability to drive insights from unstructured data formats.', '2+ years of experience with AWS S3, Glue, MSK, Redshift, Spark, etc.', '5+ years of experience working in large Data Warehousing environments', 'Bachelor’s Degree in Computer Science, Distributed Computing, Machine Learning, Mathematics or related field', 'Additional Information', 'Strong domain knowledge of ticketing and entitlement systems. Hands-on experience with architecture, design, data models, eventing infrastructure, and implementation details', '2+ years of experience with Python development.', 'Design and develop production grade syndicated and curated data views, assist with architecture decisions, deployment and feedback.', '2+ years of experience with Python development.2+ years of experience with AWS S3, Glue, MSK, Redshift, Spark, etc.5+ years of experience in software development in an agile environment.5+ years of experience with SQL.5+ years of experience working in large Data Warehousing environmentsMathematical background relevant to Machine Learning conceptsProduction experience with DevOps Tools like: Terraform, Chef, Anisle, Docker, Jenkins.Strong conceptual understanding of security at rest and in transit for big data pipelines.Troubleshooting skills, ability to determine impacts, ability to resolve complex issues, and ability to exercise sound judgment and initiative in stressful situations.Conceptual understanding of NoSQL databases, and ability to drive insights from unstructured data formats.', 'Responsibilities', '5+ years of experience in software development in an agile environment.', 'Working with a variety of business partner groups across Disney Parks and Resorts to implement optimal technical solutions', 'Design and develop production grade syndicated and curated data views, assist with architecture decisions, deployment and feedback.Design and develop data pipelines in the AWS cloud environmentAble to debug and troubleshoot across broad technical stackProduct ownership and strong business support in theme park ticketing spaceWorking with a variety of business partner groups across Disney Parks and Resorts to implement optimal technical solutionsProvide data and system analysis needed for feature engineering on Machine Learning initiatives', 'Design and develop data pipelines in the AWS cloud environment', '5+ years of experience with SQL.', 'Provide data and system analysis needed for feature engineering on Machine Learning initiatives', 'Able to debug and troubleshoot across broad technical stack', 'Basic Qualifications']",Mid-Senior level,Full-time,Quality Assurance,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer,VanderHouwen,"Hillsboro, OR",3 weeks ago,Be among the first 25 applicants,"['', 'Description', 'Benefits', 'Data Engineer', 'Responsibilities', 'About VanderHouwen']",Entry level,Contract,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,VidMob,"Pittsfield, MA",6 days ago,Be among the first 25 applicants,"['', ' Understanding of statistics and data modeling methodologies', ' Advanced working knowledge of SQL and relational databases, MySQL preferred', ' Experience with data-related AWS services such as RDS, Redshift, and Kinesis', 'Minimum Qualifications', ' Family Leave (Maternity, Paternity)', ' Stock Option Plan', ' B.S. in computer science or equivalent experience Advanced working knowledge of SQL and relational databases, MySQL preferred Understanding of statistics and data modeling methodologies Experience collaborating with Data Scientists and Data Analysts Ability to create fast solutions to problems introduced in a changing environment with iteration towards optimal solutions Experience with data-related AWS services such as RDS, Redshift, and Kinesis Experience with programming or scripting language, preferably Python, Java, or Scala', ' Health Care Plan (Medical, Dental & Vision)', ' Manage the design and architecture of our data warehouse', 'Overview', ' Health Care Plan (Medical, Dental & Vision) Unlimited Paid Time Off (Vacation, Sick & Public Holidays) Family Leave (Maternity, Paternity) Training & Development Stock Option Plan 401k Plan', ' Work with software engineering and data science teams to design, build, and manage our application DB, machine learning components, and our data infrastructure', ' Training & Development', ' Maintain, extend, and automate reporting infrastructure', ' Experience collaborating with Data Scientists and Data Analysts', ' Unlimited Paid Time Off (Vacation, Sick & Public Holidays)', ' Create software tools to automate and manage ETL processes and dependencies', 'Responsibilities', ' Work with data insights teams to define and extract data sets for use in analysis and machine learning', ' Ability to create fast solutions to problems introduced in a changing environment with iteration towards optimal solutions', ' Work with data insights teams to define and extract data sets for use in analysis and machine learning Work with software engineering and data science teams to design, build, and manage our application DB, machine learning components, and our data infrastructure Maintain, extend, and automate reporting infrastructure Manage the design and architecture of our data warehouse Create software tools to automate and manage ETL processes and dependencies', ' Experience with programming or scripting language, preferably Python, Java, or Scala', 'Benefits', ' 401k Plan', ' B.S. in computer science or equivalent experience']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
"Lead Data Engineer, Open Source - Data Platform",Visa,"Palo Alto, CA",3 days ago,33 applicants,"['', 'Work closely with service engineering, operations, and Hadoop users to engineer applications for Visa Business projects.', 'Experience in Kubernetes based services development and have strong full stack development background using UI and API frameworks', 'Provide development lead oversight on Visa’s Hadoop Open Source Platform. Develop and help drive Open Source Hadoop based products and services.', 'Primary Responsibilities Will Include', '10 years of work experience with a Bachelor’s Degree or at least 8 years of work experience with an Advanced Degree (e.g. Masters/MBA/JD/MD) or at least 3 years of work experience with a PhD', 'Preferred Qualifications', 'Apply creative thinking/approach to determine technical solutions that further business goals and align with corporate technology strategies, keeping in mind performance, reliability, scalability, usability, security, flexibility, and cost.', 'Can navigate a Linux terminal with ease', 'Strong leadership and team player.', 'Strong customer-centric mindset', '10 years of work experience with a Bachelor’s Degree or at least 8 years of work experience with an Advanced Degree (e.g. Masters/MBA/JD/MD) or at least 3 years of work experience with a PhDMinimum 5 years of Open Source Hadoop and Big data products experience.', 'Outstanding verbal, written, presentation, facilitation, and interaction skills, including ability to effectively communicate architectural issues and concepts to multiple organization levels and executive management.', ""We're"", 'Additional Information', 'Payment industry experience is a plus.', 'Minimum 5 years of Open Source Hadoop and Big data products experience.', '12 years of work experience with a Bachelor’s Degree or at least 8 years of work experience with an Advanced Degree (e.g. Masters/MBA/JD/MD) or at least 3 years of work experience with a PhD', 'Active development experience to provide software as service on-prem and in Hybrid-Cloud. ', 'Together', 'Contribute and holistically offers the Open source Hadoop services that solves current and potential Use Case(s) for Visa Internal Business teams.', 'Qualifications', 'In-depth knowledge of the software development life cycle required', 'Strong skills on mentoring/growing junior people', 'Hands-on experience in developing and managing development of Data Integration applications for large corporations with experience in both batch and online systems.', 'Lead internal proof of concept initiatives with Architects and work on multiple enhancement on existing services.', 'Provide development lead oversight on Visa’s Hadoop Open Source Platform. Develop and help drive Open Source Hadoop based products and services.Contribute and holistically offers the Open source Hadoop services that solves current and potential Use Case(s) for Visa Internal Business teams.Contribute to the DP strategies and Open Source roadmap development to meet business objectives with existing or emerging technologies. Ability to Understands and Solves Common and Un-common User Problems by Mapping and Onboarding New Use Cases to the Open source Big data services.Innovate and Re-pivot Problems with Models and Higher Dimension Concepts to Support Current and Potential Visa Business Use Cases.Work extensively on providing Open Source Spark as a Service on Kubernetes and Presto to enable users use a data processing service connecting to Hadoop.Apply creative thinking/approach to determine technical solutions that further business goals and align with corporate technology strategies, keeping in mind performance, reliability, scalability, usability, security, flexibility, and cost.Lead internal proof of concept initiatives with Architects and work on multiple enhancement on existing services.Must love coding – prepare to spend 80% of the time on hands-on development with development teamsProvide mentorship and help team growth especially on technical side.', 'Experience in using Apache Open source projects, contributed to open source projects is a plus.', 'Understanding best practices for Big Data (in Hadoop), data warehousing, consumer analytics, knowledge management and key understanding of streaming and other NoSQL databases', 'Relational database and SQL development experience required', 'Quick learner; self-starter, detailed and thorough', 'Company Description', '12 years of work experience with a Bachelor’s Degree or at least 8 years of work experience with an Advanced Degree (e.g. Masters/MBA/JD/MD) or at least 3 years of work experience with a PhDMinimum 5 years of Open Source Hadoop and Big data products experience.Understand all aspects of our distributed systems and learn select set of services in detail. Implement solutions to solve massively distributed technology problems using open source products.Work closely with service engineering, operations, and Hadoop users to engineer applications for Visa Business projects.Active development experience to provide software as service on-prem and in Hybrid-Cloud. Experience in Kubernetes based services development and have strong full stack development background using UI and API frameworksUnderstanding best practices for Big Data (in Hadoop), data warehousing, consumer analytics, knowledge management and key understanding of streaming and other NoSQL databasesHands-on experience in developing and managing development of Data Integration applications for large corporations with experience in both batch and online systems.Experience in using Apache Open source projects, contributed to open source projects is a plus.Experience in logging, metering and alerting open source-based solution is a plus. Can navigate a Linux terminal with easeRelational database and SQL development experience requiredIn-depth knowledge of the software development life cycle requiredOutstanding verbal, written, presentation, facilitation, and interaction skills, including ability to effectively communicate architectural issues and concepts to multiple organization levels and executive management.Quick learner; self-starter, detailed and thoroughStrong customer-centric mindsetStrong leadership and team player.Strong skills on mentoring/growing junior peoplePayment industry experience is a plus.', 'Understand all aspects of our distributed systems and learn select set of services in detail. Implement solutions to solve massively distributed technology problems using open source products.', 'Provide mentorship and help team growth especially on technical side.', 'Must love coding – prepare to spend 80% of the time on hands-on development with development teams', 'Basic Qualifications', 'Contribute to the DP strategies and Open Source roadmap development to meet business objectives with existing or emerging technologies. Ability to Understands and Solves Common and Un-common User Problems by Mapping and Onboarding New Use Cases to the Open source Big data services.', 'Innovate and Re-pivot Problems with Models and Higher Dimension Concepts to Support Current and Potential Visa Business Use Cases.', 'Travel Requirements ', 'Job Description', 'Experience in logging, metering and alerting open source-based solution is a plus. ', ""You're"", 'Work extensively on providing Open Source Spark as a Service on Kubernetes and Presto to enable users use a data processing service connecting to Hadoop.', 'Mental/Physical Requirements ']",Not Applicable,Full-time,Engineering,Consumer Services,2021-03-24 13:05:10
Data Engineer,JumpCloud,"Denver, CO",6 days ago,Be among the first 25 applicants,"['', ' Experience building data pipelines and lakes in AWS Passion for learning', ' Experience building data pipelines and lakes in AWS', ' Comfortable with Linux or OSX as a desktop development environment.', ' Passion for learning', "" 1-3 years of programming experience in Golang, Python, and/or NodeJS 1-3 years of experience using cloud technologies such as AWS, Azure, or GCP 3-5 years of professional experience building enterprise applications Data operations experience using tools such as Terraform, CloudFormation and/or Salt Advanced experience working with and building RESTful APIs Willingness to learn and embrace new technologies, languages, and frameworks (we will test your skills with a take home exercise) Solid Git experience Comfortable with Linux or OSX as a desktop development environment. Database experience is a plus, including relational and non-relational databases. Strong team player that wants to win together. We are both Agile and agile, and we're a tight-knit team that's constantly working together Strong communication skills."", ' 1-3 years of programming experience in Golang, Python, and/or NodeJS', ' Solid Git experience', "" Strong team player that wants to win together. We are both Agile and agile, and we're a tight-knit team that's constantly working together"", 'About JumpCloud', ' 1-3 years of experience using cloud technologies such as AWS, Azure, or GCP', ' Willingness to learn and embrace new technologies, languages, and frameworks (we will test your skills with a take home exercise)', ' Data operations experience using tools such as Terraform, CloudFormation and/or Salt', ' Database experience is a plus, including relational and non-relational databases.', ""What You'll Be Doing"", ' Strong communication skills.', ' 3-5 years of professional experience building enterprise applications', ' Advanced experience working with and building RESTful APIs']",Entry level,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Data Engineer,Atika Technologies,"Princeton, NJ",5 days ago,Be among the first 25 applicants,"[' Engage in planning of releases / deployments together with Project Managers and Operations Team.', ' Knowledge on system / API integration and basic authentication methods (Keywords: SOAP, REST, Salesforce API, OAUTH, SSO) desired.', ' Create detailed solution designs and related specifications necessary to deliver technical solutions', ' Hold end to end accountability for business processes, integrate services horizontally across organizational units, and manage interfaces between Service Delivery projects.', ' Strong Data and Analytics Experience. Prefer experience in Advance analytics.', ' Review deployment packages with developers.', ' Create detailed solution designs and related specifications necessary to deliver technical solutions Develop, build, configure, customize and unit test solutions based on the functional and technical specifications to meet quality and performance requirements. Hold end to end accountability for business processes, integrate services horizontally across organizational units, and manage interfaces between Service Delivery projects. Engage and participate in decision making discussions with internal customer groups. Deliver detailed documentation of solutions. Ensure implemented solutions are aligned to specifications and are fit for purpose. Engage in planning of releases / deployments together with Project Managers and Operations Team. Support releases / deployments between development environments and validation / productive orgs. Review deployment packages with developers.', ' Deliver detailed documentation of solutions.', ' Must have experience with AWS, Snowflake, SQL and SPARK.', 'Dearborn MI', 'Data Engineer', ' Support releases / deployments between development environments and validation / productive orgs.', ' Demonstrated experience in using development toolkits that manages code repositories, code integration, build deployments and continuous integrations tools (Git, Bitbucket, Jenkins,).', ' Strong Data and Analytics Experience. Prefer experience in Advance analytics. Must have experience with AWS, Snowflake, SQL and SPARK. Demonstrated experience in using development toolkits that manages code repositories, code integration, build deployments and continuous integrations tools (Git, Bitbucket, Jenkins,). Knowledge on system / API integration and basic authentication methods (Keywords: SOAP, REST, Salesforce API, OAUTH, SSO) desired.', ' Ensure implemented solutions are aligned to specifications and are fit for purpose.', 'Requirements', ' Engage and participate in decision making discussions with internal customer groups.', ' Develop, build, configure, customize and unit test solutions based on the functional and technical specifications to meet quality and performance requirements.']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Windstream,"Greenville, SC",2 weeks ago,Be among the first 25 applicants,"['', 'Desired Skills', 'Job Responsibilities', 'Promotion of code through weekly code reviews into the Test and Production environments. ', 'Exposure to Business Intelligence (BI) reporting tools like Tableau', '. As our company responds to COVID-19, the safety and wellbeing of our employees, customers, partners and communities is our top priority!', 'Promotion of code through weekly code reviews into the Test and Production environments.', 'Identify and document data sources, criteria, and data mapping. ', 'Exposure to code versioning tools, such as Git ', ' Knowledge of data warehouse concepts (star schema, fact and dimension tables)  Experience working with large, disparate data sets  Excellent analytic skills associated with working on unstructured datasets  Exposure to code versioning tools, such as Git  ', 'EEO Statement', 'Other Locations', 'Exposure to scheduling tools like Airflow or Clover ', 'Management of database environment working in concert with IT resources.', 'Integration of multiple data sources and databases into one system. ', 'Exposure to Python ', 'Monitoring daily job scheduling activities. ', 'Strong SQL experience, DML/DDL (PL/SQL, T-SQL, etc). ', 'Scale and optimize performance via schema design, query tuning, and index creation. ', 'Exposure to Atlassian data stack: JIRA, Confluence, Fisheye ', 'Knowledge of data warehouse concepts (star schema, fact and dimension tables) ', 'HIRING NOW', 'Agile Planning understanding ', 'Familiarity with various functional areas pertaining to telecommunications networks eg circuit design, traffic engineering, network system design ', ' Database Administration  Strong understanding of event handling Exposure to scheduling tools like Airflow or Clover  ', 'Database Administration ', 'Troubleshoot production issues, identifying root cause and implementing sound technical resolutions in a timely manner. ', 'Positivity, and the desire to solve problems in elegant and creative ways. ', 'Strong understanding of event handling', 'Excellent analytic skills associated with working on unstructured datasets ', 'Primary Location', 'Work Locations', 'Ability to build processes that support data transformation, workload management, data structures, dependency and metadata. ', 'Drive to succeed and improve personally, and in ability to add value to the role, team, and company. ', ' Promotion of code through weekly code reviews into the Test and Production environments.  Manage projects through to completion.  Manage work through Agile tools/methodology, collaborative repositories, issue tracking platforms, and wikis.  ', ' Positivity, and the desire to solve problems in elegant and creative ways.  ', 'Minimum Requirements', 'Manage projects through to completion. ', 'Experience working with large, disparate data sets ', ' Strong SQL experience, DML/DDL (PL/SQL, T-SQL, etc).  Integration of multiple data sources and databases into one system.  ', 'Manage work through Agile tools/methodology, collaborative repositories, issue tracking platforms, and wikis. ', 'Highly organized and meticulous. ', 'Good team player and communicator. ', 'Data Engineer-Network Business Intelligence Job Description', ' Exposure to Atlassian data stack: JIRA, Confluence, Fisheye  Exposure to Python  Exposure to Business Intelligence (BI) reporting tools like Tableau Familiarity with various functional areas pertaining to telecommunications networks eg circuit design, traffic engineering, network system design  Agile Planning understanding  ', 'Job Category', 'Windstream is considered an essential business and we are ', 'Development of Extract-Transform-Load (ETL) logic to meet functional business requirements primarily using Oracle packages and scheduling software to support business intelligence needs using large, disparate data sets. ', 'Essential Skills', ' Development of Extract-Transform-Load (ETL) logic to meet functional business requirements primarily using Oracle packages and scheduling software to support business intelligence needs using large, disparate data sets.  Promotion of code through weekly code reviews into the Test and Production environments. Management of database environment working in concert with IT resources. Monitoring daily job scheduling activities.  Troubleshoot production issues, identifying root cause and implementing sound technical resolutions in a timely manner.  Scale and optimize performance via schema design, query tuning, and index creation.  Identify and document data sources, criteria, and data mapping.  ', ' Ability to build processes that support data transformation, workload management, data structures, dependency and metadata.  Drive to succeed and improve personally, and in ability to add value to the role, team, and company.  Self-starter, relentlessly curious, resourceful, collaborative, and inventive.  Good team player and communicator.  Highly organized and meticulous.  ', 'Self-starter, relentlessly curious, resourceful, collaborative, and inventive. ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
"Data Engineer, Enterprise Data Warehouse",Celonis,"Washington, DC",2 days ago,Be among the first 25 applicants,"['', '… offer attractive compensation models (best-in-class salary, stock option packages, employee referral bonus, family service, flexible working hours, …)', ' ... see people as the fundamental pillar of our success. Therefore, we invest into the personal growth and skill development of each individual alongside with the strength finder test  … offer attractive compensation models (best-in-class salary, stock option packages, employee referral bonus, family service, flexible working hours, …) ... are visionary and one of the fastest growing Software-Unicorns in the world … are experts in the field of Process Mining - the new Celonis Execution Management System provides a set of instruments and applications: the EMS offerings help companies manage every facet of execution management from analytics, to strategy and planning, management, actions and automations ... distinguish ourselves through a unique combination of innovative start-up atmosphere paired with great professionalism and self-responsible work ', 'Strong problem-solving skills and attention to detail', 'Responsible for data cataloging and maintaining data governance on created structures as well as ensuring data quality and system integrity', 'Set up and maintain core data structures that will be the basis of our internal analyses and work closely together with analysts to generate a standard set of ground truth tables and views', 'opportunity to grow', ' MBA or MS in Computer Science, Information Systems, Engineering, or related technical field 4+ years experience in business intelligence, data engineering, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets Experience with designing & optimizing queries, data sets, and data pipelines to organize, collect and standardize data across multiple sources Knowledge of data warehousing fundamentals, ETL development, and data storage principles Familiarity with BI tools and managing data sets for BI tools Strong problem-solving skills and attention to detail Ability to work with geographically distributed teams Excellent verbal and written communication skills Prior Celonis platform experience a plus Prior experience working with cloud software and API’s – Salesforce, Workday, and Financial Data experience a plus Scripting language experience a plus ', 'Be part of an innovative team that drives our Celonis on Celonis initiative and uses our software to dive into our business processes to determine root causes, quantify potential, and establish and drive improvement initiatives that make our business more efficient', 'creative, collaborative, autonomous teams ', 'Be the primary contact for the planning, development and deployment of new data sources into our internal IBC, as well as for supporting and documenting technical questions', 'PQL (Process Query Language)', 'award-winning process mining technology, ', 'direct impact ', 'Provide tiered support to internal stakeholders as necessary', '... distinguish ourselves through a unique combination of innovative start-up atmosphere paired with great professionalism and self-responsible work', 'The time is now to start your next career step at Celonis, one of the fastest growing global tech companies worldwide', 'Qualifications:', 'data-driven & AI technologies', '4+ years experience in business intelligence, data engineering, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets', 'Excellent verbal and written communication skills', 'steep learning curve, ', '... see people as the fundamental pillar of our success. Therefore, we invest into the personal growth and skill development of each individual alongside with the strength finder test ', 'MBA or MS in Computer Science, Information Systems, Engineering, or related technical field', 'Experience with designing & optimizing queries, data sets, and data pipelines to organize, collect and standardize data across multiple sources', 'Scripting language experience a plus', 'immediate value', '... are visionary and one of the fastest growing Software-Unicorns in the world', 'quickly ', 'Knowledge of data warehousing fundamentals, ETL development, and data storage principles', '… are experts in the field of Process Mining - the new Celonis Execution Management System provides a set of instruments and applications: the EMS offerings help companies manage every facet of execution management from analytics, to strategy and planning, management, actions and automations', 'At Celonis, we believe that every company can unlock their full execution capacity', 'Prior experience working with cloud software and API’s – Salesforce, Workday, and Financial Data experience a plus', ' Be part of an innovative team that drives our Celonis on Celonis initiative and uses our software to dive into our business processes to determine root causes, quantify potential, and establish and drive improvement initiatives that make our business more efficient Set up and maintain core data structures that will be the basis of our internal analyses and work closely together with analysts to generate a standard set of ground truth tables and views Be the primary contact for the planning, development and deployment of new data sources into our internal IBC, as well as for supporting and documenting technical questions Responsible for data cataloging and maintaining data governance on created structures as well as ensuring data quality and system integrity Provide tiered support to internal stakeholders as necessary ', 'We', 'Familiarity with BI tools and managing data sets for BI tools', ""What you'll do:"", 'Ability to work with geographically distributed teams', 'Prior Celonis platform experience a plus', 'Introduction', 'responsibility']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Jr. Data Engineer,Qlarion,"Reston, VA",2 weeks ago,93 applicants,"['', ' Experience with SQL a must.', ' Experience with data visualization a plus.', ' B.S. in Computer Science or Systems Engineering preferred. Other 4-year degrees with relevant experience will be considered.', ' Experience with data science a plus.', ' Strong oral and written communication skills desired.', ' Ability to work independently and in a team environment.', ' Experience with Python/R a must.', ' Experience in data analysis and problem solving required.']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,New York Power Authority,"White Plains, NY",1 week ago,Be among the first 25 applicants,"['', ' Minimum of 2 years of Data Engineering experience', ' Scripting experience with JavaScript, pySpark, Python, T-SQL or other similar languages', "" Design and develop Big data solutions using AWS, Spark and Data Bricks that are flexible, extensible, elastic, secure and reliable at large scale Work with Lead Data Engineer to provide guidance and direction to project teams ensuring compliance with coding standards and best practices Collaborate with Data Governance team to capture and manage meta data, and implement data quality rules Building and managing data pipelines, and promoting to production Continuously learn and be at the leading edge of Data Integration, Cloud, Containerization and other industry trends Work with stakeholders including product, data and business teams to assist with data-related technical issues and support their data infrastructure needs Follow Cyber security guidelines and polices to monitor the company's data security and privacy"", ' Work with Lead Data Engineer to provide guidance and direction to project teams ensuring compliance with coding standards and best practices', ' Work with stakeholders including product, data and business teams to assist with data-related technical issues and support their data infrastructure needs', ' Cloud certification (AWS/Google) is preferred', ' Bachelor of Science Degree in Computer Science or IT Engineering', ' Continuously learn and be at the leading edge of Data Integration, Cloud, Containerization and other industry trends', ' Bachelor of Science Degree in Computer Science or IT Engineering Minimum of 2 years of Data Engineering experience Experience in building micro services using AWS Lambda and similar technologies Experience with any one of the big data tools such as: Spark, Kafka, StreamSets, Hadoop Scripting experience with JavaScript, pySpark, Python, T-SQL or other similar languages Cloud certification (AWS/Google) is preferred', 'Physical Requirements', ' Experience in traditional and cloud data management components (MS SQL, RDS, Athena, or similar)', ' Experience in Collibra and Trillium is a plus', ' Experience in building micro services using AWS Lambda and similar technologies', ' Familiarity with DevOps and Agile methodologies', 'Knowledge, Skills And Abilities', ' Experience with ETL tools: Talend, Pentaho, or similar tools', ' Understanding of cloud security policies and concepts', ' Strong analytical skills', ' Building and managing data pipelines, and promoting to production', 'Responsibilities', 'Summary', "" Follow Cyber security guidelines and polices to monitor the company's data security and privacy"", ' Experience with any one of the big data tools such as: Spark, Kafka, StreamSets, Hadoop', ' Experience in traditional and cloud data management components (MS SQL, RDS, Athena, or similar) Experience in metadata driven ingestion framework, building data pipelines and data sets Familiarity with DevOps and Agile methodologies Strong analytical skills Understanding of cloud security policies and concepts Experience in Collibra and Trillium is a plus Experience with ETL tools: Talend, Pentaho, or similar tools', ' Collaborate with Data Governance team to capture and manage meta data, and implement data quality rules', 'Education, Experience and Certifications', ' Design and develop Big data solutions using AWS, Spark and Data Bricks that are flexible, extensible, elastic, secure and reliable at large scale', ' Experience in metadata driven ingestion framework, building data pipelines and data sets']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
"Data Engineer, Web Solutions Engineer, Business Technology, YouTube",YouTube,"Mountain View, CA",23 hours ago,Be among the first 25 applicants,"['', 'Working experience with protocol buffers.', 'Experience in one or more of the following: Relational/non-relational databases, SQL data pipelines, Web application development, backend system design.', 'Experience with creating system/product designs and leading them to launch.', 'Passion for data pipeline/workflow design, development and support.', 'Write and review technical documents and review code in compiled or scripted languages.', ' Design, develop, deploy and maintain scalable solutions for data processing, feed generation using C++ and SQL. Oversee data and data pipeline operations including data center migration, infrastructure migration, resource management, data pipeline configurations, and troubleshooting. Write and review technical documents and review code in compiled or scripted languages. Own and drive complex technical projects from the planning stage through to execution. Collaborate with other Web Solutions Engineers, Product Managers, and Data Analysts on data schema design, and business logic definition. ', 'About The Job', 'Oversee data and data pipeline operations including data center migration, infrastructure migration, resource management, data pipeline configurations, and troubleshooting.', 'Collaborate with other Web Solutions Engineers, Product Managers, and Data Analysts on data schema design, and business logic definition.', ""Bachelor's degree in Computer Science or related technical field or equivalent practical experience."", 'Responsibilities', ' SQL, Python, and/or Java experience. Working experience with protocol buffers. Experience with creating system/product designs and leading them to launch. Experience working with systems for collaborative code reviews, version control, continuous integration and automated testing. Basic understanding and passion for massive data processing in parallel in a distributed network computing environment. Passion for data pipeline/workflow design, development and support. ', 'Basic understanding and passion for massive data processing in parallel in a distributed network computing environment.', 'SQL, Python, and/or Java experience.', 'Design, develop, deploy and maintain scalable solutions for data processing, feed generation using C++ and SQL.', 'Own and drive complex technical projects from the planning stage through to execution.', '2 years of software development experience in C++.', 'Experience with basic Unix environment operations.', 'Experience working with systems for collaborative code reviews, version control, continuous integration and automated testing.', "" Bachelor's degree in Computer Science or related technical field or equivalent practical experience. 2 years of software development experience in C++. Experience with basic Unix environment operations. Experience in one or more of the following: Relational/non-relational databases, SQL data pipelines, Web application development, backend system design. ""]",Not Applicable,Full-time,Information Technology,Information Services,2021-03-24 13:05:10
Data Ops Engineer,NIC Inc.,"Denver, CO",3 weeks ago,44 applicants,"['', 'Collaborate with IT and business customers frequently to ensure that expected business value is delivered well.', 'All candidates must either be a U.S. Citizen or in possession of a Green Card. All candidates must be able to pass a Federal Suitability Check for a position of public trust.', 'The\xa0financial health\xa0of our people is equally important to us, so we also offer a generous 401(K) plan, a stock purchase program and educational reimbursement as well.', '\ufeffQualifications', 'DEPENDENT LIFE', 'Strong\xa0DataOps Engineer with 3+ years of Cloud Experience', 'Participate in development of documentation, technical deliverables and reports.', '2+ years’ experience with AWS Cloud Services, AWS Glue is a plus', 'NIC offers an amazing benefits package. At NIC, the\xa0health and happiness\xa0of our people aren’t coincidental, they’re essential to us. That’s why we’re proud to cover 100% of group medical premiums for employees and their families, in addition to 100% of group dental, disability and life insurance premiums to all employees beginning one month after joining the company.', 'Develop an understanding of client business processes, objectives, and solution requirements.', 'Employ common application processes, coding standards, and performance standards.', 'Participate in project workgroups with subject matter experts and stakeholders to understand specific needs.', 'Support production deployment and production support.', 'NIC is looking for a talented (2+ years) Data Ops Engineer with 3+ years of Cloud Experience to develop and support systems in an Agile DevOps environment where Java, JavaScript, Python, Angular, JSON, Postgres, MySQL, HTML5/CSS3, Jenkins, Git, VMWare, React, Redux and AWS (ELB, EC2, S3, etc.) are the prevailing frameworks and technologies.\xa0The objective of the Data Ops Engineer is to support Agile DevOps software development and deployment to deliver functionality for systems that support government operations.', 'Strong contributor in the design and development, and O&M support, of business portals and web-based systems, with little supervision.Develop using Open Source frameworks for complex system functionality.Develop an understanding of client business processes, objectives, and solution requirements.Participate in project workgroups with subject matter experts and stakeholders to understand specific needs.Employ common application processes, coding standards, and performance standards.Support production deployment and production support.Continue to learn and grow skills in additional frameworks, technologies, and technical practices to extend personal mastery as well as team capability.Research, design, develop, test, build, and coordinate the conversion and/or integration of software products based on client requirements, to include further movement into AWS cloud environment and containerized/Docker-based microservices solutions.Participate in technical feasibility analysis on potential future application projects to management.Contribute to deliverables and performance metrics.Participate in development of documentation, technical deliverables and reports.Bring an agile mindset to work on an agile team using iterative development practices and Continuous Integration and deployment techniques.O&M and production support - investigate and resolve production inquiries and issues.Communicate status to team lead/project oversight.Collaborate with IT and business customers frequently to ensure that expected business value is delivered well.', 'Degree or equivalent experience in Computer Science/Engineering or related field.Strong\xa0DataOps Engineer with 3+ years of Cloud ExperienceKnowledge of GoLang is required (testable during interview)6+ months of experience with\xa0GoLang is preferred2+ years’ experience with AWS Cloud Services, AWS Glue is a plusWorking knowledge of EMR, Spark and ETL best practicesKnowledge of Data Warehousing best practices, including Data Cataloging, Data Security and Data QualityExperience with Data Visualization is a plus (Tableau preferred)Agile development methodology experience (SAFe, LESS, Extreme, etc.).Strong analytic, organization, presentation, customer service and facilitation skills.', 'DENTAL', 'DISABILITY', 'Experience with Data Visualization is a plus (Tableau preferred)', 'Knowledge of GoLang is required (testable during interview)', '6+ months of experience with\xa0GoLang is preferred', 'Bring an agile mindset to work on an agile team using iterative development practices and Continuous Integration and deployment techniques.', 'BA or BS degree in Computer Science, Information Technology or a related field', 'EMPLOYEE STOCK PURCHASE PROGRAM', '401(K) INVESTMENT PLANEMPLOYEE STOCK PURCHASE PROGRAMEMPLOYEE EDUCATIONAL REIMBURSEMENT PROGRAM', 'Strong contributor in the design and development, and O&M support, of business portals and web-based systems, with little supervision.', '401(K) INVESTMENT PLAN', 'Required Skills:', 'Continue to learn and grow skills in additional frameworks, technologies, and technical practices to extend personal mastery as well as team capability.', 'All candidates must either be a U.S. Citizen or in possession of a Green Card. All candidates must be able to pass a Federal Suitability Check for a position of public trust', 'LIFE', 'EMPLOYEE EDUCATIONAL REIMBURSEMENT PROGRAM', 'NIC is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin', 'Research, design, develop, test, build, and coordinate the conversion and/or integration of software products based on client requirements, to include further movement into AWS cloud environment and containerized/Docker-based microservices solutions.', 'Participate in technical feasibility analysis on potential future application projects to management.', 'O&M and production support - investigate and resolve production inquiries and issues.', 'Working knowledge of EMR, Spark and ETL best practices', 'Develop using Open Source frameworks for complex system functionality.', 'EEO Statement:', 'Contribute to deliverables and performance metrics.', 'financial health\xa0', 'health and happiness\xa0', 'Knowledge of Data Warehousing best practices, including Data Cataloging, Data Security and Data Quality', 'Degree or equivalent experience in Computer Science/Engineering or related field.', 'Benefits', 'You will be hired as a full-time employee on a long-term project.', 'Agile development methodology experience (SAFe, LESS, Extreme, etc.).', 'Strong analytic, organization, presentation, customer service and facilitation skills.', 'Work is 100% remote on a long-term contract. This is an immediate need; not a contingent hire.', 'Communicate status to team lead/project oversight.', 'DENTALLIFEDEPENDENT LIFEDISABILITY']",Associate,Full-time,Engineering,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,ConsultNet,"Rockville, MD",6 days ago,Be among the first 25 applicants,"['', 'Familiarity of columnar storage formats ORC, Parquet and various compression techniques', 'Write complex SQL queries', 'Job Responsibilities', 'Provide tech expertise and leadership for analysis, design, and modeling work', 'Strong and creative analytical and problem-solving skills', 'Set clear expectations with engineering team and peers', 'Ensure security and compliance requirements are met', 'Strong communication skills', 'Required Technical Skills & Knowledge For Lead Data Engineer', 'Be delivery-focused', 'Unit testing with Junit or Scalatest', ' SPARK Hadoop, Hive, Hbase REST APIs Write complex SQL queries Performance tuning and optimization Familiarity of columnar storage formats ORC, Parquet and various compression techniques Unit testing with Junit or Scalatest Git/Maven Code Reviews Agile Testing strategy and implementation AWS Cloud experience is a plus. ', 'Believes in Scrum/Agile, and has deep experience delivering software when working on teams that use Scrum/Agile methodology', 'Code Reviews', 'Provide timely and regular updates to management', 'Testing strategy and implementation', 'Establish and implement ETL and big data processing best practices to design, test, implement, and support mission critical applications', 'Performance tuning and optimization', ' 3+ years of experience required, including some commercial/non-government work Hands on experience designing, developing, implementing, testing, and deploying large scale ETL/Data Analytics/Java/J2EE projects Believes in Scrum/Agile, and has deep experience delivering software when working on teams that use Scrum/Agile methodology Strong and creative analytical and problem-solving skills Strong communication skills Enable long-term implementation, facilitate product vision and strategy. ', 'Hadoop, Hive, Hbase', 'Ship great products! ', 'Experience & Qualifications', 'SPARK', 'Data Engineer ', 'Able to grasp complex business requirements', 'THE ROLE', 'Own the outcome', 'Git/Maven', 'Be quality-focused', 'Hands on experience designing, developing, implementing, testing, and deploying large scale ETL/Data Analytics/Java/J2EE projects', 'THE PLATFORM', ' Able to grasp complex business requirements Provide tech expertise and leadership for analysis, design, and modeling work Design, develop, implement, test, deploy, and support big data processing software in the AWS Cloud Establish and implement ETL and big data processing best practices to design, test, implement, and support mission critical applications Set clear expectations with engineering team and peers Provide timely and regular updates to management Ensure security and compliance requirements are met Be delivery-focused Be quality-focused Own the outcome Ship great products!  ', 'AWS Cloud experience is a plus.', 'Enable long-term implementation, facilitate product vision and strategy.', 'Design, develop, implement, test, deploy, and support big data processing software in the AWS Cloud', 'Job Description', '3+ years of experience required, including some commercial/non-government work', 'REST APIs', 'Agile']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Presidio,"Charlotte, NC",3 weeks ago,49 applicants,"['', 'Be able to build data pipelines and platforms using proven development tools and languages such as Eclipse, IntelliJ, Python, Scala, Spark, PySpark, AWS Glue', '(Get Acrobat Reader)', 'Ability to travel up to 30% to customer sites', '.', 'Suggest and implement optimization patterns for an existing or new Data Platform', 'Cultivating a culture of inclusion where the expression of all our differences are valued, celebrated, and add to our collective achievements.', 'www.presidio.com', 'WHY YOU SHOULD JOIN US?', 'Have proven experience in building out Data Lakes using AWS services and other CSPs', ' ', 'Work with stakeholders including executive and data teams to troubleshoot and identify issues within the Data Platform', 'Bachelor’s degree in computer science, mathematics, statistics or a similar quantitative field', 'COME BUILD YOUR FUTURE WITH PRESIDIO!', 'Required Qualifications', 'Desired Qualifications', 'Assist project team with oversight of junior team members: task delegation; technical assistance; oversight and QA', 'Identify, build, and implement optimal Data Platform', 'THE ROLE: Data Engineer', 'Experience with GCP or Azure cloud solution', ' Experience with GCP or Azure cloud solution Experience with Qubole Data Platform ', 'Ability to work independently with minimal supervision', 'Collaborate as assist with solution design', 'Ability to proactively identify performance gaps in a solution and provide guidance for improvements', 'Presidio is a VEVRAA Federal Contractor requesting priority referrals of protected veterans for its openings. State Employment Services, please provide priority referrals to ', 'Presidio is committed to Diversity, Equity, and Inclusion at the highest levels and has strengthened its drive to build and drive systemic DE&I change process across all levels of the organization.', 'To read more about discrimination protections under Federal Law, please visit: ', 'Identify and implement BI tools per architectural guidelines', 'Participate as lead engineer on projects and assist in communication/collaboration sessions with clients', 'If you have any difficulty using our online system and need an accommodation in the job application process due to a disability, please send an email to ', 'https://www1.eeoc.gov/employers/upload/eeoc_self_print_poster.pdf', 'Identify, build, and implement highly scalable Data pipeline with automation', '2+ years of engineering experience', 'Core understanding of Big Data principles and architectural patterns', 'Experience implementing Machine Learning solutions and services on AWS or GCP', 'Presidio is an Equal Opportunity / Affirmative Action Employer / VEVRAA Federal Contractor. All qualified candidates will receive consideration for this position regardless of race, color, creed, religion, national origin, age, sex, citizenship, ethnicity, veteran status, marital status, disability, sexual orientation, gender identification or any other characteristic protected by applicable federal, state and local statutes, regulations and ordinances.To read more about discrimination protections under Federal Law, please visit: https://www1.eeoc.gov/employers/upload/eeoc_self_print_poster.pdf (Get Acrobat Reader)If you have any difficulty using our online system and need an accommodation in the job application process due to a disability, please send an email to recruitment@presidio.com for assistance.Presidio is a VEVRAA Federal Contractor requesting priority referrals of protected veterans for its openings. State Employment Services, please provide priority referrals to recruitment@presidio.com.Recruitment Agencies Please NoteAgencies/3rd Parties may not solicit to any employee of Presidio. Any candidate information received from any Agency/3rd Party will be considered a gift and property of Presidio, unless the Agency/3rd Party is an Authorized Vendor of Presidio with an up-to-date Presidio Contract in hand signed by Presidio Talent Acquisition. No payment will be made to any Agency/3rd Party who is not an Authorized Vendor, nor has specific approval in writing from Presidio Talent Acquisition to engage in recruitment efforts for Presidio.', 'Build out ETL jobs per architectural guidelines which integrates within the data pipeline', 'Ability to articulate a problem and find solutions in a timely manner', 'Proven experience building or administering BI tools such as Tableau, Domo, Lookr, etc.', 'About Presidio', 'Recruitment Agencies Please Note', 'About PresidioPresidio is committed to Diversity, Equity, and Inclusion at the highest levels and has strengthened its drive to build and drive systemic DE&I change process across all levels of the organization. Cultivating a culture of inclusion where the expression of all our differences are valued, celebrated, and add to our collective achievements.Presidio is a leading global IT solutions provider assisting clients in harnessing technology innovation and simplifying IT complexity to digitally transform their businesses and drive return on IT investment. Our Digital Infrastructure, Business Analytics, Cloud, Security & Emerging solutions enable our almost 7,000 middle market, enterprise, and government clients to take advantage of new digital revenue streams, omnichannel customer experience models, and the rich data insights generated by those interactions.We serve as an extension of our clients’ IT teams, providing deep expertise and letting them focus on their core business. Within Presidio’s 40+ US offices and offices in Ireland, London, Singapore, and India, we support 2,800+ professionals, including 1,600 technical engineers. Presidio is a trusted advisor to our clients on a national level while also bringing our global scale and expertise to bear.For more information visit: www.presidio.com ', ' 2+ years of engineering experience Bachelor’s degree in computer science, mathematics, statistics or a similar quantitative field Core understanding of Big Data principles and architectural patterns Deep understanding of Batch and streaming data pipelines Have proven experience in building out Data Lakes using AWS services and other CSPs Deep understanding of AWS Data & Analytics services (Kinesis, S3, EMR, Athena, Redshift, etc.) Proven experience building or administering BI tools such as Tableau, Domo, Lookr, etc. Be able to build data pipelines and platforms using proven development tools and languages such as Eclipse, IntelliJ, Python, Scala, Spark, PySpark, AWS Glue Experience implementing Machine Learning solutions and services on AWS or GCP ', 'recruitment@presidio.com', ' for assistance.', 'For more information visit: ', 'Deep understanding of AWS Data & Analytics services (Kinesis, S3, EMR, Athena, Redshift, etc.)', 'Deep understanding of Batch and streaming data pipelines', 'Presidio is an Equal Opportunity / Affirmative Action Employer / VEVRAA Federal Contractor. All qualified candidates will receive consideration for this position regardless of race, color, creed, religion, national origin, age, sex, citizenship, ethnicity, veteran status, marital status, disability, sexual orientation, gender identification or any other characteristic protected by applicable federal, state and local statutes, regulations and ordinances.', ' Participate as lead engineer on projects and assist in communication/collaboration sessions with clients Assist project team with oversight of junior team members: task delegation; technical assistance; oversight and QA Collaborate as assist with solution design Identify, build, and implement optimal Data Platform Identify, build, and implement highly scalable Data pipeline with automation Build out ETL jobs per architectural guidelines which integrates within the data pipeline Work with stakeholders including executive and data teams to troubleshoot and identify issues within the Data Platform Suggest and implement optimization patterns for an existing or new Data Platform Identify and implement BI tools per architectural guidelines Ability to proactively identify performance gaps in a solution and provide guidance for improvements Ability to work independently with minimal supervision Ability to articulate a problem and find solutions in a timely manner Ability to travel up to 30% to customer sites ', 'Experience with Qubole Data Platform', 'About PresidioPresidio is committed to Diversity, Equity, and Inclusion at the highest levels and has strengthened its drive to build and drive systemic DE&I change process across all levels of the organization. Cultivating a culture of inclusion where the expression of all our differences are valued, celebrated, and add to our collective achievements.Presidio is a leading global IT solutions provider assisting clients in harnessing technology innovation and simplifying IT complexity to digitally transform their businesses and drive return on IT investment. Our Digital Infrastructure, Business Analytics, Cloud, Security & Emerging solutions enable our almost 7,000 middle market, enterprise, and government clients to take advantage of new digital revenue streams, omnichannel customer experience models, and the rich data insights generated by those interactions.We serve as an extension of our clients’ IT teams, providing deep expertise and letting them focus on their core business. Within Presidio’s 40+ US offices and offices in Ireland, London, Singapore, and India, we support 2,800+ professionals, including 1,600 technical engineers. Presidio is a trusted advisor to our clients on a national level while also bringing our global scale and expertise to bear.For more information visit: www.presidio.com Presidio is an Equal Opportunity / Affirmative Action Employer / VEVRAA Federal Contractor. All qualified candidates will receive consideration for this position regardless of race, color, creed, religion, national origin, age, sex, citizenship, ethnicity, veteran status, marital status, disability, sexual orientation, gender identification or any other characteristic protected by applicable federal, state and local statutes, regulations and ordinances.To read more about discrimination protections under Federal Law, please visit: https://www1.eeoc.gov/employers/upload/eeoc_self_print_poster.pdf (Get Acrobat Reader)If you have any difficulty using our online system and need an accommodation in the job application process due to a disability, please send an email to recruitment@presidio.com for assistance.Presidio is a VEVRAA Federal Contractor requesting priority referrals of protected veterans for its openings. State Employment Services, please provide priority referrals to recruitment@presidio.com.Recruitment Agencies Please NoteAgencies/3rd Parties may not solicit to any employee of Presidio. Any candidate information received from any Agency/3rd Party will be considered a gift and property of Presidio, unless the Agency/3rd Party is an Authorized Vendor of Presidio with an up-to-date Presidio Contract in hand signed by Presidio Talent Acquisition. No payment will be made to any Agency/3rd Party who is not an Authorized Vendor, nor has specific approval in writing from Presidio Talent Acquisition to engage in recruitment efforts for Presidio.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Associate Data Engineer,"UES, Inc.","Dayton, OH",2 days ago,Be among the first 25 applicants,"['BS in Mechanical Engineering or related field of study preferred, practical experience will be paramount', 'Experience with Computer-Aided Design (CAD) or Computer-Aided Engineering (CAE) files and applications', 'Requirements:', 'The associate data engineer will be trained to support the team to establish and maintain a robust naming convention for the data; ensure that the data is properly formatted; data cleansing; and ensure optimized data effectiveness so that the data intelligence can support current and future decisions and research directions by applying artificial intelligence/machine learning to the data. Some software development maybe needed (Python) to assist in streamlining the data collection, contextualization and analysis processes. Further, the associate data engineer will work closely with the software engineering team where more extensive customizations are needed.', 'Demonstrable skills in oral and written communication', 'This position is working within a government facility and requires U.S. Citizenship', 'Experience implementing or integrating Software', 'Additional Information', '\xa0', 'BS in Mechanical Engineering or related field of study preferred, practical experience will be paramountExperience with Computer-Aided Design (CAD) or Computer-Aided Engineering (CAE) files and applicationsExperience in programming in PythonDemonstrable skills in problem solving and technology risk analysisDemonstrable skills in oral and written communicationThis position is working within a government facility and requires U.S. Citizenship', 'Demonstrable skills in problem solving and technology risk analysis', 'Other functions involve designing new data and technology solutions to structure and analyze technical data.', 'Experience in programming in Python', 'Experience implementing or integrating SoftwareExperience working with software developersExperience in data science, to include data acquisition, cleansing, aggregation, analysis, and ontology', 'Experience working with software developers', 'UES, Inc. is committed to hiring and retaining a diverse workforce. We are proud to be an Equal Opportunity/Affirmative Action Employer, making decisions without regard to race, color, religion, creed, sex, sexual orientation, gender identity, marital status, national origin, age, veteran status, disability, or any other protected class. U.S. Citizenship is required for most positions.', 'Preferences:', 'Experience in data science, to include data acquisition, cleansing, aggregation, analysis, and ontology', 'UES, Inc. has an opportunity for an associate-level Data Engineer to be a critical player in the cutting edge of aerospace technology development. The Air Force in conjunction with UES has developed a contextualized data intelligence architecture for accelerated understanding, improvements, and innovation. The associate data engineer will be the primary resource to ensure that the data generated by the aerospace research team is appropriately structured, documented and archived. These aircraft structures data may include designs (CAD files), materials, fabrication and testing data.', 'UES, Inc. is an innovative science and technology company providing customers with superior research and development expertise since its inception in 1973. Our long-term success is a direct result of a strong commitment to the success of our employees. We look forward to reviewing your application.']",Mid-Senior level,Full-time,Information Technology,Research,2021-03-24 13:05:10
Data Engineer D.C.,Afiniti,"Washington, DC",1 week ago,Be among the first 25 applicants,"['', 'Expertise in MySQL or PostgreSQL', 'Proficiency in Python or any other programming language', ' Implement ETL procedures and standards using SQL/R/Talend/Python for existing and new deployments Determine specifications and implement the data pipeline according to requirements Identify issues in daily data feed and ensure data consistency and data integrity. Process, clean, and analyze data to be utilized in artificial intelligence modeling Design data models and implement effective data warehouse strategies and concepts Monitor ETL processes, perform root cause analysis on incidents and resolve data production issues Utilize advanced mathematical methods to analyze and report performance to track optimization metrics. Perform in-depth analysis on data to provide key insights to improve the call center experience. Validate models to ensure adequacy and provide recommendations for models. ', 'Identify issues in daily data feed and ensure data consistency and data integrity.', '2 - 5 years of experience in Data driven roles', 'Knowledge of Probability/Statistics', ' 2 - 5 years of experience in Data driven roles Expertise in MySQL or PostgreSQL Knowledge of Probability/Statistics Proficiency in Python or any other programming language Good communication skills ', 'Determine specifications and implement the data pipeline according to requirements', 'Perform in-depth analysis on data to provide key insights to improve the call center experience.', 'Design data models and implement effective data warehouse strategies and concepts', 'Process, clean, and analyze data to be utilized in artificial intelligence modeling', 'Good communication skills', 'Implement ETL procedures and standards using SQL/R/Talend/Python for existing and new deployments', 'Validate models to ensure adequacy and provide recommendations for models.', 'Monitor ETL processes, perform root cause analysis on incidents and resolve data production issues', 'Education And Qualifications', 'Bachelors of Engineering/Sciences in Computers/Software/Analytics', 'Utilize advanced mathematical methods to analyze and report performance to track optimization metrics.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Celonis Data Engineer,ThreeBridge Solutions,"Bridgewater, NJ",3 weeks ago,79 applicants,"['', 'APC-Consumption', 'Further activities', 'Integration Servers', 'New data models / processes', 'Creation of a new data model', ' Understand the current SQL logic to derive how the activity is created', 'Configuration Of The Complete Event Collection', 'Integration of new tables', 'Schedulers (Transformations, Extractions, ML Workbench)', 'Data Connections', ' Create / Adapt the activity and integrate it into the according event log and transformation logic', ' Gather source system knowledge to derive the new SQL logic', 'App Store', 'Configuration of the complete event collection', 'Scheduler Executions (Delta Loads, Full Loads, Extractions, Machine Learning Workbench)', 'New systems', 'Delta Loads', ' Scoping of the new project and estimation of a technical implementation timeline Derive technical requirements from the business requirements given Steering the technical integration of the project Creation of a new data model Integration of all events together with their SQL logic Build up of the data model with all joins and necessary data ', 'Responsibilities:', 'Write SQL / ML code to connect a process to Celonis', 'Data Pool Parameters (Start Dates, Document Types, etc.)', 'Duration:', 'Change Request Handling', 'Integration Of New Data - Tables', ' Adapt the current data extractions with the table to be added as well as correct filtering over global Datapool-Parameters', 'Integration of all events together with their SQL logic', 'Configure data jobs, loads and data models', ' Additional meta-data Further activities Change in activity logics New data models / processes New systems ', 'Know-how of data structures', 'Map business goals with data', ' Process Analytics Event Collection App Store Process Automation Action Engine Machine Learning Workbench', 'The Data Engineer Uses Celonis To', 'Action Engine', 'Vertica SQL skills', 'Validate data', 'Build up of the data model with all joins and necessary data', 'Change Request handling (e.g. scoping)', 'Overall System Health', 'SQL Adaption / Transformation Customization / Activity Creation', 'Change Request Can Be', ' Schedulers (Transformations, Extractions, ML Workbench) Data Pool Parameters (Start Dates, Document Types, etc.) Data Connections Data Pools Integration Servers Data Model Loads Delta Loads ', 'Steering the technical integration of the project', 'Adaption of technical analysis features', ' Map business goals with data Work with the business to understand what data they need to analyze their process. Write SQL / ML code to connect a process to Celonis Configure data jobs, loads and data models Validate data ', 'Monitoring', 'Change in activity logics', 'Scoping of the new project and estimation of a technical implementation timeline', 'Connecting And Creating New Processes / Data Models', ' Create a new view that than can be integrated into the data model with the correct join definition predefined in the SQL code to keep out unnecessary data out of the front end', 'Celonis Applications', 'License Consumption (Business Users, Analysts)', 'The Data Engineer Tasks Include', 'Additional meta-data', 'Process Analytics', ' Change Request handling (e.g. scoping) Integration of new tables SQL adaption / transformation customization Configuration of the complete event collection Adaption of technical analysis features Connecting and creating new processes / data models Monitoring ', 'Data Pools', ' Know-how of data structures Vertica SQL skills Experience with ETL processes IT-affinity ', 'Process Automation', 'Machine Learning Workbench', 'SQL adaption / transformation customization', 'Recommended Know-How', 'Work with the business to understand what data they need to analyze their process.', 'Derive technical requirements from the business requirements given', 'Connecting and creating new processes / data models', 'Event Collection', ' Getting information on the table names, description and name mapping from the source system', ' Scheduler Executions (Delta Loads, Full Loads, Extractions, Machine Learning Workbench) Data Model Loads Overall System Health APC-Consumption License Consumption (Business Users, Analysts) ', 'Data Model Loads', 'IT-affinity', 'Experience with ETL processes', ' Add the table to the data model and trigger a new load']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,true[X],"Los Angeles, CA",5 days ago,69 applicants,"['', 'Who you are:', 'Build and modify Spark jobs (in Scala) to perform various tasks, from reading Kinesis streams using Spark Streaming, to joining and aggregating huge data sets, to integrating with third party data sources', 'BS in Computer Science or related field required\xa0Strong knowledge of SQL required\xa0Strong knowledge with Spark (using Scala)\xa0Working knowledge of serialization formats and their trade-offs (columnar vs row-based) Experience debugging and optimizing Spark jobs\xa0Familiarity with database fundamentals, such as ACID, snowflake schema, normalized/denormalized data\xa0Must be a strong written and verbal communicatorFamiliarity with columnar database, key-value stores, document stores, stream\xa0processing, time series databases, data warehouses, and OLAP\xa0preferredExperience working with HDFS and S3 preferred\xa0Familiarity with Data Science tooling in Spark preferredExperience in the advertising industry and with real-time analytics is a plus\xa0', 'Experience working with HDFS and S3 preferred\xa0', ""Changing the established guidelines of an industry, especially one as rooted as digital advertising, isn't an easy or quick effort, but we believe it's the right thing to do and we want to be the ones to do it. We're looking for hungry people who are passionate about disrupting the digital media world, and agree that we can do better for viewers, advertisers and publishers out there.\xa0"", 'Develop and launch new features to adapt to evolving business needs', 'Collaborate with Data Science, Product, Research, and Engineering teams to iterate on the roadmap', ""As a Data Engineer, you will be an important part of the team that owns our data infrastructure. This is no small responsibility: As an AdTech company, data is our lifeblood, and there's a lot of it. Someone with experience ingesting, processing, storing, analyzing, and working in a big-data environment will have ample opportunity for success in this role.\xa0\xa0"", 'Be an active and engaged owner of our data infrastructure', 'A proactive problem solver', 'Must be a strong written and verbal communicator', 'A builder', 'BS in Computer Science or related field required\xa0', 'A builder\xa0— you are passionate about collecting, storing, and analyzing big data', 'A universal communicator\xa0— you are able to explain the most technical data to the least technical people without any confusion', 'We are committed to an inclusive and diverse work environment. true[X] is an equal opportunity employer. We do not discriminate based on race, color, ethnicity, ancestry, national origin, religion, sex, gender, gender identity, gender expression, sexual orientation, age, disability, veteran status, genetic information, marital status or any legally protected status.', 'Working knowledge of serialization formats and their trade-offs (columnar vs row-based) Experience debugging and optimizing Spark jobs\xa0', 'Familiarity with Data Science tooling in Spark preferred', 'Experience in the advertising industry and with real-time analytics is a plus\xa0', 'About the Company', 'Gather requirements when underspecified', 'What you’ll be doing:', '100% company-paid health, dental, and vision coverage for you and your dependents, as well as life insurance and disability coverage', '100% company-paid health, dental, and vision coverage for you and your dependents, as well as life insurance and disability coverageUnlimited paid time off - we trust your discretionOpportunities for profit sharing, bonuses, and ownership401(k) plan plus company match\xa0Cell phone reimbursement and subsidized gym membershipAnnual professional development stipend\xa0', 'What you have:', 'Maintain high standards of code quality, and encourage the same by providing constructive code reviews to collaborators', 'Benefits & Perks.', 'Opportunities for profit sharing, bonuses, and ownership', 'Familiarity with columnar database, key-value stores, document stores, stream\xa0processing, time series databases, data warehouses, and OLAP\xa0preferred', 'Troubleshoot and resolve issues, problems, and errors encountered across various systems', 'Be curious and seek to understand all aspects of our business', 'Since our founding in 2007, we have been committed to advancing three core principles in our own products and in the broader advertising industry: quality, accountability, and transparency, driven by our core belief that all are necessary for ensuring a better consumer experience. Every decision we make is guided by a deep understanding of human attention.', 'Strong knowledge with Spark (using Scala)\xa0', 'At true[X], acquired by Gimbal in 2020, our mission is to provide the best advertising experience for consumers, the best monetization for premium publishers, and the best return for brand advertisers. Across connected TV, mobile and desktop devices we empower premium publishers to create experiences that allow them to serve the widest possible audience by optimizing consumers’ time and attention, and delivering impactful results for advertisers. For brands and advertisers, true[X] delivers on our true[ATTENTION] guarantee of effective, zero-waste, high-engagement ad experiences that drive measurable brand funnel impact.', 'About the Job', 'It’s no secret that we work hard, but we also strive to create an office environment where the lines between work and play are blurred. This means we offer these great perks to help keep our team healthy, productive, and happy.', 'Build and modify Spark jobs (in Scala) to perform various tasks, from reading Kinesis streams using Spark Streaming, to joining and aggregating huge data sets, to integrating with third party data sourcesDevelop and launch new features to adapt to evolving business needsBe an active and engaged owner of our data infrastructureBe curious and seek to understand all aspects of our businessMaintain high standards of code quality, and encourage the same by providing constructive code reviews to collaboratorsTroubleshoot and resolve issues, problems, and errors encountered across various systemsCollaborate with Data Science, Product, Research, and Engineering teams to iterate on the roadmapGather requirements when underspecified', 'A proactive problem solver\xa0— you are constantly looking for ways to work smarter, delivering new efficiencies anywhere an improvement can be made', 'A universal communicator\xa0— you are able to explain the most technical data to the least technical people without any confusionA proactive problem solver\xa0— you are constantly looking for ways to work smarter, delivering new efficiencies anywhere an improvement can be madeA builder\xa0— you are passionate about collecting, storing, and analyzing big data', 'Strong knowledge of SQL required\xa0', 'Annual professional development stipend\xa0', 'Cell phone reimbursement and subsidized gym membership', 'Unlimited paid time off - we trust your discretion', '401(k) plan plus company match\xa0', 'A universal communicator\xa0', 'Familiarity with database fundamentals, such as ACID, snowflake schema, normalized/denormalized data\xa0']",Mid-Senior level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer,Urbane Systems LLC,"McLean, VA",3 weeks ago,Be among the first 25 applicants,[''],Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data/Software Engineer,Known Medicine,"Salt Lake City, UT",2 weeks ago,161 applicants,"['', 'Your Role', 'Proficiency using AWS', '3+ years experience in software engineering, data engineering, or similarBachelor’s degree in CS, Math, or similarProficiency using AWSPreferred: some data science experience', 'As a company, Known Medicine is helping oncologists pick the best treatment for cancer patients, faster. Using a patient’s own tumor sample, we use image analysis and Machine Learning to give oncologists insight into how their cells will respond to different treatments.', 'Bachelor’s degree in CS, Math, or similar', 'Help build out and productionize ML models', 'Known Medicine', 'Design, implement, and automate image storage and retrieval', 'Preferred:', 'Design, implement, and automate image storage and retrievalWork with our CTO to monitor and troubleshoot analysis of imagesHelp build out and productionize ML modelsBasic python programming of robotics systems', 'You will be the first engineer. Your responsibilities include:', '3+ years experience in software engineering, data engineering, or similar', 'Work with our CTO to monitor and troubleshoot analysis of images', 'Who You’ll Join', 'Preferred: some data science experience', 'Basic python programming of robotics systems', 'We’re a seed-stage startup backed by Y Combinator, Khosla Ventures, and other phenomenal VCs and Angel investors. You’ll be our first engineering hire, and we’re looking to build out more of our team in the next several months.\xa0', 'Our founding team consists of a biomedical engineer and an AI for drug discovery data scientist. We’re building out a fully automated pipeline in our Salt Lake City, UT lab and have research collaborations with several top cancer centers to receive and process patient samples. We offer health insurance, snacks and drinks in the office, lunch provided twice weekly, and flexible vacation/sick leave.\xa0', 'Your Background']",Not Applicable,Full-time,Biotechnology,N/A,2021-03-24 13:05:10
Associate Data Engineer,Hitachi Solutions Canada,"Chicago, IL",6 days ago,57 applicants,"['', 'Hands-on experience with data migration from legacy systems to new solutions', '401K Program with employer match for all new hires', 'Excellent communication skills with the ability to collaborate in a fluid team environment', 'Personalized career coaching and mentoring', 'Hands-on experience using Databricks/Spark', 'Hands-on experience in construction of data science and machine learning research and production pipelines', 'Hands-on experience using Python or Scala', 'In-depth understanding of database structure principlesHands-on experience using MS SQL Server, Oracle, MySQL, or similar RDBMS platformStrong analytical and problem-solving skillsHighly self‐motivated, self‐directed, and attentive to detailAbility to effectively prioritize and execute tasksAbility to adapt and learn quicklyExcellent communication skills with the ability to collaborate in a fluid team environmentWillingness to travel across the United States and Canada post-pandemic as requested by clients', 'Familiarity with data visualization tools (e.g. PowerBI, Tableau, etc)', 'Medical and Dental Benefit Package (including Long Term and Short Term Disability)', 'Job Description', 'In-depth understanding of database structure principles', 'Azure skills highly preferred, including provisioning, configuring, and developing solutions in Azure Data Lake, Azure Data Factory, Azure SQL Data Warehouse, Azure Analysis Services, etcFamiliarity with data visualization tools (e.g. PowerBI, Tableau, etc)Understanding of Data Modeling & strong skills in AnalyticsUnderstanding of Distributed Data Processing of big data batch or streaming pipelinesHands-on experience in construction of data science and machine learning research and production pipelinesHands-on experience using Databricks/SparkHands-on experience using Python or ScalaHands-on experience with data migration from legacy systems to new solutionsHands-on experience designing conceptual, logical, and physical data models using tools like ER Studio and Erwin', 'Hitachi is an Equal Employment Opportunity/Affirmative Action Employer', 'Hands-on experience using MS SQL Server, Oracle, MySQL, or similar RDBMS platform', 'Hands-on experience designing conceptual, logical, and physical data models using tools like ER Studio and Erwin', 'Ideal Candidates Possess Two Or More Of The Following', 'Medical and Dental Benefit Package (including Long Term and Short Term Disability)401K Program with employer match for all new hiresCompetitive starting salaryPersonalized career coaching and mentoring', 'Qualifications', 'Willingness to travel across the United States and Canada post-pandemic as requested by clients', 'Ability to adapt and learn quickly', 'Competitive starting salary', 'Company Description', 'Strong analytical and problem-solving skills', 'Azure skills highly preferred, including provisioning, configuring, and developing solutions in Azure Data Lake, Azure Data Factory, Azure SQL Data Warehouse, Azure Analysis Services, etc', 'Ability to effectively prioritize and execute tasks', 'Opportunity Benefits', 'Highly self‐motivated, self‐directed, and attentive to detail', 'Understanding of Distributed Data Processing of big data batch or streaming pipelines', 'Understanding of Data Modeling & strong skills in Analytics']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,WellSky,"Austin, TX",2 days ago,Be among the first 25 applicants,"['', 'Bonus Points: Go to the top of the stack if you have any Qlik certification! ', 'You will join a team of top engineers creating interactive reports and dashboards with stunning visualizations. ', 'Familiarity with ETL/ELT processes, data pipelines, data lakes, etc. ', 'Demonstrated ability to work with cross-functional teams in a fast paced, agile environment', ' Do you have what it takes? ', 'Author of code/scripts to power visualizations for complex scenarios ', 'Preferred Experience: \u202f ', 'About WellSky', ' Bonus Points: Go to the top of the stack if you have any Qlik certification!  ', ' A day in the life! ', 'Advanced SQL knowledge working with a variety of databases', 'You will be responsible for the following:\u202f ', 'Your hard work will touch the lives of real people and families navigating life and death issues with the support of our solutions. ', 'Extensive experience in designing and developing BI dashboards, using modern tools (Qlik, Tableau, Sisense, etc.) ', ' At least 3 years direct, hands on experience with Qlik Products (Sense, View, Data Catalyst) Extensive experience in designing and developing BI dashboards, using modern tools (Qlik, Tableau, Sisense, etc.)  Advanced SQL knowledge working with a variety of databases Strong command of SQL from a BI dashboard, reporting context ', 'We seek to build purpose-driven teams where comradery and compassion are coupled with a dogged pursuit of excellence. ', 'At least 3 years direct, hands on experience with Qlik Products (Sense, View, Data Catalyst)', 'Strong command of SQL from a BI dashboard, reporting context', 'Preferred Certifications', 'Bonus Points', 'C# and/or .Net Core API development, or other OO type programming language ', 'Required Experience', ' Do you stand above the rest? ', 'You will have direct impact on the users of our solutions, mainly doctors, nurses, and others on the front lines of healthcare and community services. ', ' You will join a team of top engineers creating interactive reports and dashboards with stunning visualizations.  You will have direct impact on the users of our solutions, mainly doctors, nurses, and others on the front lines of healthcare and community services.  Your hard work will touch the lives of real people and families navigating life and death issues with the support of our solutions.  We seek to build purpose-driven teams where comradery and compassion are coupled with a dogged pursuit of excellence.  ', ' C# and/or .Net Core API development, or other OO type programming language  Familiarity with ETL/ELT processes, data pipelines, data lakes, etc.  Author of code/scripts to power visualizations for complex scenarios  Demonstrated ability to work with cross-functional teams in a fast paced, agile environment ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Shelf Engine,"Seattle, WA",6 days ago,50 applicants,"['', ' About Us ', 'Have Bachelor’s degree in a STEM related field (preferred)', 'Design and implement ELT/ETL processes from 1st and 3rd party sources targeting our data warehouse and data lake, scaling to TB of data.', 'Architect, implement and evolve a data architecture that supports all aspects of the Shelf Engine business, from machine learning to analysis, and sourcing of data from 1st and 3rd party sourcesWork cross functionally to deeply understand the business needs of data systems in widely varying contexts, and produce data sets that support these needs efficiently, and that operate reliably with low latencyOwn designing and building the infrastructure for sourcing various types of data from 3rd party sourcesDesign and implement ELT/ETL processes from 1st and 3rd party sources targeting our data warehouse and data lake, scaling to TB of data.Produce new and maintain existing SQL views and other projections that are understandable and usable by various teams at Shelf who need data access for reporting, analysis and other business reasons Develop features and products for Shelf Engine to continue to reduce waste and inefficiency in the food industryDesign and implement data quality assurance mechanisms to drive excellence in data provenance, latency, and reliability, and to allow visibility of data quality state to all stakeholders.', 'Have deep empathy, for customers and for colleagues ', 'Have experience implementing ETL/ELT processes, understand the tooling landscape, and are cognizant of the tradeoffs between them', 'Have a relentless drive to solve data problems - whether that be enabling some new scenario, or track down some esoteric transformation bug, you understand the value of data quality and strive for excellence in it', 'Develop features and products for Shelf Engine to continue to reduce waste and inefficiency in the food industry', 'Have 3+ years of experience in a data engineering or related role', 'Architect, implement and evolve a data architecture that supports all aspects of the Shelf Engine business, from machine learning to analysis, and sourcing of data from 1st and 3rd party sources', 'Have deep competency with SQL, Python or other data manipulation tools', 'Excited about working at a fast-paced and mission-driven environment', 'Experience with warehouse data systems, such as Snowflake, AWS Redshift, Azure Synapse/Azure SQL Data Warehouse etc, and with tools commonly used in data orchestration and processing such as Airflow, Kubernetes, Docker, Spark, DataBricks, and data lake technologies and patterns', ' As a Data Engineer At Shelf Engine, You Will ', 'Have experience designing projections that rearrange or combine various data sources into forms that are more useful for reporting and analysis (including e.g. denormalization)', 'Design and implement data quality assurance mechanisms to drive excellence in data provenance, latency, and reliability, and to allow visibility of data quality state to all stakeholders.', 'Are authorized to work in the U.S. for any employer', 'Have 3+ years of experience in a data engineering or related roleHave deep competency with SQL, Python or other data manipulation toolsHave experience implementing ETL/ELT processes, understand the tooling landscape, and are cognizant of the tradeoffs between themHave experience designing projections that rearrange or combine various data sources into forms that are more useful for reporting and analysis (including e.g. denormalization)Experience with warehouse data systems, such as Snowflake, AWS Redshift, Azure Synapse/Azure SQL Data Warehouse etc, and with tools commonly used in data orchestration and processing such as Airflow, Kubernetes, Docker, Spark, DataBricks, and data lake technologies and patternsExperience with analytics and BI tools, we use Tableau so that would be a big plusExcited about working at a fast-paced and mission-driven environmentHave a relentless drive to solve data problems - whether that be enabling some new scenario, or track down some esoteric transformation bug, you understand the value of data quality and strive for excellence in itHave deep empathy, for customers and for colleagues Have experience working on deep data problems and with data scientists (a big plus)Have Bachelor’s degree in a STEM related field (preferred)Are authorized to work in the U.S. for any employer', 'Have experience working on deep data problems and with data scientists (a big plus)', 'Produce new and maintain existing SQL views and other projections that are understandable and usable by various teams at Shelf who need data access for reporting, analysis and other business reasons ', 'Work cross functionally to deeply understand the business needs of data systems in widely varying contexts, and produce data sets that support these needs efficiently, and that operate reliably with low latency', 'Own designing and building the infrastructure for sourcing various types of data from 3rd party sources', 'Experience with analytics and BI tools, we use Tableau so that would be a big plus']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,"StarsHR, Inc.","New York, NY",1 week ago,Be among the first 25 applicants,"['', 'Essential Skills Python AWS Lambda AWS Glue AWS Cloudwatch']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Senior Data Engineer,Remind,"San Francisco, CA",20 hours ago,Be among the first 25 applicants,"['', 'You have expert level SQL skills, but more importantly you know how to design performant and understandable datasets for the whole Remind team.', 'You are proficient in Python and familiar with at least one other programming language.', 'Define and own organization-level data architecture for a trusted, governed, dimensionally-modeled repository of data that enables the Remind team to quickly and reliably answer their questions.', 'About You', 'Protect data sources against data quality issues: work with data producers to ensure data passes acceptance tests; design, develop and maintain data quality monitoring and assurance framework; and continuously improve the processes for developing new ones, raising the level of quality expected from our work.', 'Delight data consumers (internal and external) by ensuring they have the data they need to inform decisions, where and when they need it.Define and own organization-level data architecture for a trusted, governed, dimensionally-modeled repository of data that enables the Remind team to quickly and reliably answer their questions.Partner closely with the Head of Data and the Head of Technology to define the vision and roadmap for data architecture at Remind. Protect data sources against data quality issues: work with data producers to ensure data passes acceptance tests; design, develop and maintain data quality monitoring and assurance framework; and continuously improve the processes for developing new ones, raising the level of quality expected from our work.Help democratize data at Remind: create data exploration processes and promote adoption of data sources across the company.', 'You are an expert in building complex, multistage data-pipelines with heterogeneous data sources using Spark and Redshift—with demonstrated expertise in optimizing performance. You have built scalable, performant, highly available services and understand the value of a good SLA.You are proficient in Python and familiar with at least one other programming language.You have expert level SQL skills, but more importantly you know how to design performant and understandable datasets for the whole Remind team.You are a curious and communicative person—you seek to understand before building and therefore more often build the right thing first.', ""What You'll Do"", 'Partner closely with the Head of Data and the Head of Technology to define the vision and roadmap for data architecture at Remind. ', 'Open vacation policy', 'You are an expert in building complex, multistage data-pipelines with heterogeneous data sources using Spark and Redshift—with demonstrated expertise in optimizing performance. ', 'You are fast and rigorous analytically, with a degree in engineering, math, statistics, analytics, or relevant alternative education (adult learning, on the job experience etc). ', '401K', '100% health coverage for you and your dependents', 'Compensation', 'About This Role', 'You are a curious and communicative person—you seek to understand before building and therefore more often build the right thing first.', 'You have built scalable, performant, highly available services and understand the value of a good SLA.', 'Delight data consumers (internal and external) by ensuring they have the data they need to inform decisions, where and when they need it.', 'Paid parental leave', 'Competitive salary and equity401K100% health coverage for you and your dependentsOpen vacation policyPaid parental leave', 'About The Company', 'Competitive salary and equity', 'Help democratize data at Remind: create data exploration processes and promote adoption of data sources across the company.']",Associate,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Data Engineer,Louisiana Economic Development,"Richmond, VA",3 weeks ago,Be among the first 25 applicants,[],Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer / Adobe Analytics,Systems Integration Solutions,"Cupertino, CA",2 days ago,Be among the first 25 applicants,"['', 'Hands-on experience working with raw clickstream data and data feeds\xa0', 'Proficient with SQL or other querying languages for data aggregation reporting and forensic troubleshooting\xa0', 'Strong information design instincts with an eye for visualizing reporting output\xa0', 'Document implementation processes and conduct user training to educate and train the organization and partner sites on analytics best practices\xa0', '4+ years experience leading technical analytics implementations with Adobe Marketing Cloud solutions\xa0', 'Ability to understand and extrapolate business objectives from stakeholders into actionable and measurable tracking solutions\xa0', 'Education:', 'Comfortable operating in a cross-functional capacity that requires technical acumen, strong organizational skills, eye for data visualization, and excellent communication and people skills\xa0', 'Architect and innovate custom tracking and data design solutions for creative tech or bespoke site experiences\xa0', 'Job Summary\xa0', 'Data Engineer / Adobe Analytics', 'Passionate about data and learning with an inherent curiosity to pull the thread to see where the data leads\xa0', 'Proficient in Javascript, Python, NodeJS, HTML, CSS, Regex, and Swift\xa0', 'Troubleshoot and debug data implementation, integrity, and pipeline issues\xa0', '5+ years of digital analytics experience with emphasis on architecting and implementing technical data collection solutions for enterprise level eCommerce and product marketing online properties across web and mobile experiences\xa0', 'W2 Candidates Only!!!', 'Proficient scripting skills and familiarity with Adobe APIs to programmatically govern the admin console or automate large scale queries\xa0', '4+ years of experience implementing Adobe Analytics solutions across dynamic enterprise site environments and single page applications\xa0', 'A self-starter that can take can take a project from start to finish with minimal supervision\xa0', 'Out-of-box thinker that enjoys coming up with creative tagging solutions to uniquely the company""s challenges\xa0', 'Subject matter expertise in Adobe Analytics with Adobe certification\xa0', '5+ years of experience implementing Adobe Analytics (Sitecatalyst), Adobe Mobile SDK & Adobe Target solutions\xa04+ years of experience implementing Adobe Analytics solutions across dynamic enterprise site environments and single page applications\xa0Strong familiarity with latest web technology trends and analytics tracking technologies\xa0Proficient in Javascript, Python, NodeJS, HTML, CSS, Regex, and Swift\xa0Subject matter expertise in Adobe Analytics with Adobe certification\xa0Proficient with SQL or other querying languages for data aggregation reporting and forensic troubleshooting\xa0Proficient scripting skills and familiarity with Adobe APIs to programmatically govern the admin console or automate large scale queries\xa0', 'Build Adobe Workspace areas and support ad-hoc reporting and analysis requests as required for troubleshooting analysis\xa0', 'Script tools to automate production of commonly requested reports, large scale queries, and data feeds\xa0', 'Technical skills\xa0', '5+ years of digital analytics experience with emphasis on architecting and implementing technical data collection solutions for enterprise level eCommerce and product marketing online properties across web and mobile experiences\xa04+ years experience leading technical analytics implementations with Adobe Marketing Cloud solutions\xa02+ years A/B test and multi-variate testing experience with Adobe Target\xa0Hands-on experience working with raw clickstream data and data feeds\xa0Hands-on experience with data layer design and data storage/transport protocols\xa0Business acumen and leadership skills\xa0Ability to understand and extrapolate business objectives from stakeholders into actionable and measurable tracking solutions\xa0Ability to work effectively with both business and technical stakeholders and pivot between business consultant and technical lead\xa0Comfortable operating in a cross-functional capacity that requires technical acumen, strong organizational skills, eye for data visualization, and excellent communication and people skills\xa0A self-starter that can take can take a project from start to finish with minimal supervision\xa0Passionate about data and learning with an inherent curiosity to pull the thread to see where the data leads\xa0Deep knowledge and expertise in all aspects of web analytics, A/B testing, and web design and a passion for keeping up-to-date with the latest web and analytics trends\xa0Strong information design instincts with an eye for visualizing reporting output\xa0Thrives and reacts positively in dynamic unstructured environments and can remain cool under the spotlight while juggling multiple projects\xa0Out-of-box thinker that enjoys coming up with creative tagging solutions to uniquely the company""s challenges\xa0', 'Engage with analytics stakeholders to define and translate business requirements into analytics implementation specifications\xa0', '\xa0BS in statistics, computer science, mathematics, or other quantitative discipline\xa0', 'Advise and consult on data collection best practices, opportunities, and limitations in accordance with business requirements, team best practices, and the company""s privacy policies\xa0', '\xa0', '5+ years of experience implementing Adobe Analytics (Sitecatalyst), Adobe Mobile SDK & Adobe Target solutions\xa0', 'Hands-on experience with data layer design and data storage/transport protocols\xa0', 'Responsibilities:\xa0', 'Be responsible for architecting and implementing data collection solutions to help measure and optimize visitor engagement across the company""s digital properties, and interactive experiences.\xa0', 'Work closely with front-end developers to implement and validate analytics solutions from development to production\xa0', 'Ability to work effectively with both business and technical stakeholders and pivot between business consultant and technical lead\xa0', 'Partner with Interactive Frameworks team to achieve step-function improvement in analytics library development, analytics library deployment, and data architecture design\xa0', 'Business acumen and leadership skills\xa0', 'Setup automated reporting and dashboards in Adobe Analytics, Target, or other analytics platforms as required\xa0', 'Strong familiarity with latest web technology trends and analytics tracking technologies\xa0', 'Engage with analytics stakeholders to define and translate business requirements into analytics implementation specifications\xa0Advise and consult on data collection best practices, opportunities, and limitations in accordance with business requirements, team best practices, and the company""s privacy policies\xa0Work closely with front-end developers to implement and validate analytics solutions from development to production\xa0Partner with Interactive Frameworks team to achieve step-function improvement in analytics library development, analytics library deployment, and data architecture design\xa0Architect and innovate custom tracking and data design solutions for creative tech or bespoke site experiences\xa0Troubleshoot and debug data implementation, integrity, and pipeline issues\xa0Script tools to automate production of commonly requested reports, large scale queries, and data feeds\xa0Setup automated reporting and dashboards in Adobe Analytics, Target, or other analytics platforms as required\xa0Build Adobe Workspace areas and support ad-hoc reporting and analysis requests as required for troubleshooting analysis\xa0Provide administrative and technical support for Adobe""s suite of analytics solutions and other analytics platforms as required\xa0Document implementation processes and conduct user training to educate and train the organization and partner sites on analytics best practices\xa0', 'Thrives and reacts positively in dynamic unstructured environments and can remain cool under the spotlight while juggling multiple projects\xa0', 'Key Qualifications:\xa0', 'Deep knowledge and expertise in all aspects of web analytics, A/B testing, and web design and a passion for keeping up-to-date with the latest web and analytics trends\xa0', '2+ years A/B test and multi-variate testing experience with Adobe Target\xa0', 'Provide administrative and technical support for Adobe""s suite of analytics solutions and other analytics platforms as required\xa0']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,ATMECS Inc,"United, LA",4 weeks ago,Be among the first 25 applicants,[''],Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Brace,"New York, NY",2 weeks ago,56 applicants,"['', ' Experience with any of the following technologies:', 'Airflow', 'Develop and maintain organized documentation of work', 'Role', 'Proficiency in at least one common data engineering language like Python', 'Who We Are', 'Experience with Amazon Redshift, Snowflake or similar analytics database', 'Understand the structure of the database backing our applications and suggest improvements', ' Git REST JSON ', 'Experience with modern Java (8+)', 'Experience with any of the following technologies:', 'Amazon Web Services (AWS)', 'Requirements', ' Own setup and rollout of our BI/reporting solution (Looker) including data modeling, metric definitions, and database performance optimization (Postgres) Create and own scalable core data objects and transformations that streamline analytical workflows - including the build out of robust data pipelines and back-end systems. Understand the structure of the database backing our applications and suggest improvements Influence the direction Brace should take in establishing a data warehouse and potential client-facing data products Work cross-functionally with Customer Success, Finance and Product to understand business needs and translate them into scalable data solutions Develop and maintain organized documentation of work ', 'Comfortable working in a small team, and driving it forward', 'Experience with ETL/ELT tools and the problems and concerns in such systems', ' Amazon Web Services (AWS) Amazon Glue Amazon Athena Airflow ', 'Influence the direction Brace should take in establishing a data warehouse and potential client-facing data products', 'Proficiency in SQL, especially PostgreSQL', 'REST', 'JSON', 'Amazon Athena', 'Familiarity with software development technologies such as:', 'Good understanding of cloud computing, infrastructure, database scale & performance concepts', 'Work cross-functionally with Customer Success, Finance and Product to understand business needs and translate them into scalable data solutions', 'Responsibilities', '4+ years of professional experience in data engineering, business intelligence or data science', ' 4+ years of professional experience in data engineering, business intelligence or data science Proficiency in SQL, especially PostgreSQL Experience with Amazon Redshift, Snowflake or similar analytics database Proficiency in at least one common data engineering language like Python Comfortable working in a small team, and driving it forward Experience with ETL/ELT tools and the problems and concerns in such systems Experience analyzing a wide variety of data: structured and unstructured to drive system designs and product implementations Good understanding of cloud computing, infrastructure, database scale & performance concepts Familiarity with software development technologies such as:', 'Create and own scalable core data objects and transformations that streamline analytical workflows - including the build out of robust data pipelines and back-end systems.', 'Amazon Glue', ""Where We're Going"", 'Bonus Points', 'Experience analyzing a wide variety of data: structured and unstructured to drive system designs and product implementations', 'Data Engineer', ""What We're Doing"", 'Experience with predictive modeling using statistical techniques and/or machine learning', 'Git', 'Own setup and rollout of our BI/reporting solution (Looker) including data modeling, metric definitions, and database performance optimization (Postgres)']",Entry level,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Data Analytics Engineer,DISH Network,"Englewood, CO",2 days ago,Be among the first 25 applicants,"['', ' Experience in data management and engineering capabilities ', ' Analyze business and technical requirements ', ' Understand business needs and analytics product offerings to design and build data and analytics solutions ', ' Experience in cloud technologies (like AWS) and data analytics platforms ', ' Assist in solving technical problems when they arise ', ' Experience in delivering of self-service analytics, insights, KPIs and metrics ', ' Monitor systems to ensure they meet the partner, user and business goals ', ' Experience with scheduling tools like Control-M or Airflow ', ' Experience working with cross-functional teams in a dynamic environment across different geographies and vendors ', ' 3+ years as a Data Analytics engineer in the wireless and/or telecom space esp., in Sales, Marketing, Retention, Product, Revenue and Partner Management etc  5+ years of hands-on experience with development and delivery of secure, reliable and scalable big data solutions using agile methodologies  Experience in building highly performant SQL based solutions delivered using file feeds, Data-as-a-Service APIs, streaming data analytics or BI tools like Tableau  Experience in data management and engineering capabilities  Experience in cloud technologies (like AWS) and data analytics platforms  Experience in delivering of self-service analytics, insights, KPIs and metrics  Experience working with cross-functional teams in a dynamic environment across different geographies and vendors  Experience building and maintaining data catalog with dictionary  Experience with scheduling tools like Control-M or Airflow  Ensure compliance on all deliverables by performing data audits and analysis ', 'Specific Qualifications', ' Quickly build prototypes to test out proof-of-concepts ', ' Address technical concerns, ideas and suggestions ', ' Provide technical expertise in Wireless Analytics ', ' 5+ years of hands-on experience with development and delivery of secure, reliable and scalable big data solutions using agile methodologies ', ' Provide technical expertise in Wireless Analytics  Understand business needs and analytics product offerings to design and build data and analytics solutions  Analyze business and technical requirements  Quickly build prototypes to test out proof-of-concepts  Address technical concerns, ideas and suggestions  Assist in solving technical problems when they arise  Monitor systems to ensure they meet the partner, user and business goals  Deliver solutions, features using agile delivery methodologies ', ' Experience building and maintaining data catalog with dictionary ', ' 3+ years as a Data Analytics engineer in the wireless and/or telecom space esp., in Sales, Marketing, Retention, Product, Revenue and Partner Management etc ', 'Day-to-day Job Responsibilities', ' Experience in building highly performant SQL based solutions delivered using file feeds, Data-as-a-Service APIs, streaming data analytics or BI tools like Tableau ', ' Deliver solutions, features using agile delivery methodologies ', ' Ensure compliance on all deliverables by performing data audits and analysis ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer - Data & Analytics,"Stanley Black & Decker, Inc.","New Britain, CT",2 weeks ago,29 applicants,"['', ' Career Opportunity ', 'Function', ' Familiar with software development and agile concepts. ', ' Advanced SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases. ', 'Featured Category on SBD Careers', 'Eeo', ' Experience with data modeling, data architecture design from complex data sources. ', 'EEO Statement', ' Diverse & Inclusive Culture ', 'Who We Are', ' Support and provide expertise in building and operationalizing performant data pipelines in a cloud-based data lake environment.  Perform data wrangling, cleansing, transformation, analysis and big data technologies is required.  Work with relational and unstructured data formats to create analytics-ready datasets for user friendly analytic solutions that identifies and aggregates data elements into decision models and other analytical support tools. ', '  Diverse & Inclusive Culture  : We pride ourselves on being an awesome place to work. We respect and embrace differences because that’s how the best work gets done. You’ll find we like to have fun here, too. ', ' Experience in web scraping and consuming REST based API (with JSON payload) highly preferred. ', ' MS degree in Computer Science, Engineering, Information Systems Management, or Data Science preferred  2+ years relevant experience or equivalent preferred.  Strong Experience with data, wrangling and cleansing, analysis  Advanced SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.  Strong knowledge of Python data analysis libraries including Pandas required  Fluency in big data platforms including Hadoop, MapReduce, Hive, SQL DBs, Spark  Experience with exploratory data analysis, data gathering, data mining and data analysis techniques.  Familiarity with Machine Learning concepts required.  Experience building and optimizing ‘big data’ data pipelines, in spark or hive highly preferred.  Experience in web scraping and consuming REST based API (with JSON payload) highly preferred.  Experience with data modeling, data architecture design from complex data sources.  Hands on experience with Machine Learning in python and/or spark highly preferred  Experience in optimizing big data pipelines in hive or Spark or traditional database workloads preferred.  Experience in Shell scripting, R and Snowflake a plus.  Experience with Cloud based HaaS/PaaS solutions such as AWS EMR, MS Azure preferred.  Familiar with software development and agile concepts.  Fluency in various Operating Systems fundamentals (Unix etc.) ', 'Benefits & Perks', ' Strong Experience with data, wrangling and cleansing, analysis ', ' 2+ years relevant experience or equivalent preferred. ', ' Experience in optimizing big data pipelines in hive or Spark or traditional database workloads preferred. ', '  Learning & Development  : Our lifelong learning philosophy means you’ll have access to a wealth of state-of-the-art learning resources, including our Lean Academy and online university (where you can get certificates and specializations from renowned colleges and universities). ', ' Strong knowledge of Python data analysis libraries including Pandas required ', ' Experience with Cloud based HaaS/PaaS solutions such as AWS EMR, MS Azure preferred. ', ' Fluency in various Operating Systems fundamentals (Unix etc.) ', ' Purpose-Driven Company ', 'All qualified applicants to Stanley Black & Decker are considered for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran’s status or any other protected characteristic.', '  Career Opportunity  : Career paths aren’t linear here. Being part of our global company with 60+ brands gives you the chance to grow and develop your skills along multiple career paths.   Learning & Development  : Our lifelong learning philosophy means you’ll have access to a wealth of state-of-the-art learning resources, including our Lean Academy and online university (where you can get certificates and specializations from renowned colleges and universities).   Diverse & Inclusive Culture  : We pride ourselves on being an awesome place to work. We respect and embrace differences because that’s how the best work gets done. You’ll find we like to have fun here, too.   Purpose-Driven Company  : You’ll help us continue to make positive changes in the local communities where we work and live as well as in the broader world through volunteerism, giving back and sustainable business practices. ', ' Experience with exploratory data analysis, data gathering, data mining and data analysis techniques. ', 'No. of Positions', 'Requisition Number', ' Fluency in big data platforms including Hadoop, MapReduce, Hive, SQL DBs, Spark ', 'What You’ll Also Get', ' Familiarity with Machine Learning concepts required. ', ' Data Engineer - Data & Analytics - Remote ', 'What You’ll Do', ' Experience in Shell scripting, R and Snowflake a plus. ', '  Career Opportunity  : Career paths aren’t linear here. Being part of our global company with 60+ brands gives you the chance to grow and develop your skills along multiple career paths. ', ' Hands on experience with Machine Learning in python and/or spark highly preferred ', '  Purpose-Driven Company  : You’ll help us continue to make positive changes in the local communities where we work and live as well as in the broader world through volunteerism, giving back and sustainable business practices. ', ' Work with relational and unstructured data formats to create analytics-ready datasets for user friendly analytic solutions that identifies and aggregates data elements into decision models and other analytical support tools. ', ' Experience building and optimizing ‘big data’ data pipelines, in spark or hive highly preferred. ', ' MS degree in Computer Science, Engineering, Information Systems Management, or Data Science preferred ', ' Learning & Development ', ' Support and provide expertise in building and operationalizing performant data pipelines in a cloud-based data lake environment. ', ' Perform data wrangling, cleansing, transformation, analysis and big data technologies is required. ', 'Job Description', 'Who You Are', 'Business']",Associate,Full-time,Information Technology,Construction,2021-03-24 13:05:10
Data Engineer - Internet Operations Team,Ripjar,"London, OR",16 hours ago,Be among the first 25 applicants,"['', 'Experience in a scripting language such as Javascript or PythonExperience working on complex distributed systems in a production settingETL, data modelling, and data storage experienceGood communicator and able to confidently represent Ripjar to current and prospective customersExperience with the Linux Operating SystemExperience with Spark (pyspark), Hadoop or other Big data technologiesExperience with Apache NiFiUnderstand the nuances of dealing with structured and unstructured data', 'Experience in hands-on technical deliveries, leading technical teams from design through to implementation of production systems', 'Be an expert in the configuration, implementation and collection of data from Open Source and Commercial providersDesign intelligent data ingest pipelines that allow us to ingest data and monitor its progress into our data processing environmentsWork closely with internal stakeholders and our data partners to understand the technical requirements for our data pipelinesDefine appropriate data schemas that support our data ingestion objectivesModel and transform classes of data into these schemas', 'ETL, data modelling, and data storage experience', 'Experience in a scripting language such as Javascript or PythonExperience working on complex distributed systems in a production settingETL, data modelling, and data storage experienceGood communicator and able to confidently represent Ripjar to current and prospective customersExperience with the Linux Operating SystemExperience with Spark (pyspark), Hadoop or other Big data technologiesExperience with Apache NiFiUnderstand the nuances of dealing with structured and unstructured dataBonus Technical Skills/experienceExperience with Elasticsearch, Kafka, Mongo, RedisExperience with Docker, Kubernetes, ApacheExperience in hands-on technical deliveries, leading technical teams from design through to implementation of production systemsDevOps experience (Ansible preferred)', 'Experience working on complex distributed systems in a production setting', 'Understand the nuances of dealing with structured and unstructured data', 'Define appropriate data schemas that support our data ingestion objectives', 'Be an expert in the configuration, implementation and collection of data from Open Source and Commercial providers', 'Experience with Apache NiFi', 'Experience with Elasticsearch, Kafka, Mongo, Redis', 'Requirements', 'Experience with Docker, Kubernetes, Apache', 'Bonus Technical Skills/experience', 'Experience in a scripting language such as Javascript or Python', 'Model and transform classes of data into these schemas', 'Description', 'Experience with Elasticsearch, Kafka, Mongo, RedisExperience with Docker, Kubernetes, ApacheExperience in hands-on technical deliveries, leading technical teams from design through to implementation of production systemsDevOps experience (Ansible preferred)', 'DevOps experience (Ansible preferred)', 'Good communicator and able to confidently represent Ripjar to current and prospective customers', 'As a Data Engineer In RIOT You Will', 'Experience with the Linux Operating System', 'Design intelligent data ingest pipelines that allow us to ingest data and monitor its progress into our data processing environments', 'Work closely with internal stakeholders and our data partners to understand the technical requirements for our data pipelines', 'Experience with Spark (pyspark), Hadoop or other Big data technologies']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Analytics Engineer,Breakthrough,"Green Bay, WI",1 week ago,Be among the first 25 applicants,"['', 'Experience in data ingestion and data integration strategies.', '3-5 years of experience in a similar role.', 'Own the maintenance, design and continued development of the reporting structures in close collaboration with the Data Engineering team.', 'Define new data collection and analysis processes.', 'Knowledge of Google BigQuery a plus.', 'Streamline and integrate data management and data analytics from end to end. ', 'Desired Skills and Experience:', 'Prepare data to identify patterns and trends in data sets.', 'Build, monitor and maintain reliable data pipelines for highly available reporting or data integrations.', 'Accountable for the ongoing architecture, implementation and configuration of data analytics using dbt.', 'ABOUT BREAKTHROUGH:', 'Accountable for the ongoing architecture, implementation and configuration of data analytics using dbt.Transform source system data into tested and documented datasets based on business needs.Build, monitor and maintain reliable data pipelines for highly available reporting or data integrations.Configuration of visualization tools such as Looker, Data Studio, Tableau or PowerBI.Preparation of disparate data sources for analysis.Prepare data to identify patterns and trends in data sets.Define new data collection and analysis processes.Streamline and integrate data management and data analytics from end to end. Own the maintenance, design and continued development of the reporting structures in close collaboration with the Data Engineering team.Contribute to the refinement, development and operationalization of the digital data management and analytics vision and strategy.Ensure ongoing alignment to data management best practices while developing and implementing data policies, standards, catalogs and critical metrics.', 'Ensure ongoing alignment to data management best practices while developing and implementing data policies, standards, catalogs and critical metrics.', 'Configuration of visualization tools such as Looker, Data Studio, Tableau or PowerBI.', 'Responsibilities Include:', 'WHY WORK AT BREAKTHROUGH? See our SMART, PASSIONATE, and EDGY team', 'Transform source system data into tested and documented datasets based on business needs.', 'Degree in Engineering, Computer Science or similar technical field.3-5 years of experience in a similar role.Knowledge of data modeling concepts, SQL and data warehousing.Knowledge of Google BigQuery a plus.Experience in data ingestion and data integration strategies.', 'Contribute to the refinement, development and operationalization of the digital data management and analytics vision and strategy.', 'Knowledge of data modeling concepts, SQL and data warehousing.', 'Preparation of disparate data sources for analysis.', 'Degree in Engineering, Computer Science or similar technical field.']",Mid-Senior level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer,Bill.com,"San Jose, CA",2 weeks ago,63 applicants,"['', 'What You Will Do', 'You have strong knowledge of data architecture, data modeling, and data infrastructure ecosystems.', 'About Bill.com', 'Work effectively using scrum with multiple team members to deliver analytical solutions to the business functions.', 'Experience with at least one programming language such as Python or Java is required.', 'You have the capability to synthesize business requirements and construct the technical design.', 'Demonstrated ability to build complex, scalable systems with high quality.', 'Passionate – Love what you do', 'Dedicated – To each other and the customer', 'About You', 'Bill.com Culture', 'Experience with multiple data technologies and concepts such as Airflow, Kafka, Hadoop, Hive, Spark, MapReduce, SQL, NoSQL, and Columnar databases.', 'BS/MS in Computer Science or equivalent is required.', 'Identify shared data needs across the company and build efficient and scalable pipelines to meet various needs.', 'Experience with containerized workloads, Kubernetes and infrastructure-as-code principles a big plus.', '2+ years data engineering experience in large scale data warehouse or datalake environment.', 'You have a strong background in distributed data processing and software engineering and can build high-quality, scalable data products.', 'Our Applicant Privacy Notice describes how Bill.com treats the personal information it receives from applicants.', 'You have an analytical mindset and have a passion for solving business problems using data.', 'Extensive knowledge of data engineering tools, technologies and approaches.', 'Humble – No ego', 'Automate and improve existing data processes for quicker turnaround and high productivity.', 'Fun – Celebrate the moments', 'Expertise in SQL and data analysis is required.', 'You thrive on the opportunity to collaborate with cross functional teams to create customer and business value.', 'Own the data expertise and data quality for the pipelines.', 'Design and implement data infrastructure and processing workflows required to support Data Science, Machine Learning, BI and Analytics in AWS.Build robust, efficient and reliable data pipelines consisting of diverse data sources.Design and develop real time streaming and batch processing pipeline solutions.Own the data expertise and data quality for the pipelines.Identify shared data needs across the company and build efficient and scalable pipelines to meet various needs.Work effectively using scrum with multiple team members to deliver analytical solutions to the business functions.Have a high sense of urgency to deliver projects as well as troubleshoot data pipelines/queries etc.Automate and improve existing data processes for quicker turnaround and high productivity.', 'Have a high sense of urgency to deliver projects as well as troubleshoot data pipelines/queries etc.', 'Qualifications', 'Build robust, efficient and reliable data pipelines consisting of diverse data sources.', 'You are passionate about data and the design and development of systems that enable data driven decisions.', 'Design and implement data infrastructure and processing workflows required to support Data Science, Machine Learning, BI and Analytics in AWS.', 'Authentic – We are who we are', 'You are a creative thinker and a strong problem solver with meticulous attention to detail and can tackle loosely defined problems.', 'You have excellent written and verbal communication skills with an ability to communicate in a clear, collaborative and open-minded manner and effective manner with both technical and non-technical peers.', 'You have the ability to initiate and drive projects to completion with minimal guidance in a dynamic environment.', 'Design and develop real time streaming and batch processing pipeline solutions.', 'Experience with specific AWS technologies (such as S3, EMR, and Kinesis) is a plus.', '2+ years data engineering experience in large scale data warehouse or datalake environment.BS/MS in Computer Science or equivalent is required.Extensive knowledge of data engineering tools, technologies and approaches.Demonstrated ability to build complex, scalable systems with high quality.Experience with multiple data technologies and concepts such as Airflow, Kafka, Hadoop, Hive, Spark, MapReduce, SQL, NoSQL, and Columnar databases.Expertise in SQL and data analysis is required.Experience with at least one programming language such as Python or Java is required.Experience with containerized workloads, Kubernetes and infrastructure-as-code principles a big plus.Experience with specific AWS technologies (such as S3, EMR, and Kinesis) is a plus.', 'You are passionate about data and the design and development of systems that enable data driven decisions.You thrive on the opportunity to collaborate with cross functional teams to create customer and business value.You have the capability to synthesize business requirements and construct the technical design.You have a strong background in distributed data processing and software engineering and can build high-quality, scalable data products.You have strong knowledge of data architecture, data modeling, and data infrastructure ecosystems.You have an analytical mindset and have a passion for solving business problems using data.You are a creative thinker and a strong problem solver with meticulous attention to detail and can tackle loosely defined problems.You have excellent written and verbal communication skills with an ability to communicate in a clear, collaborative and open-minded manner and effective manner with both technical and non-technical peers.You have the ability to initiate and drive projects to completion with minimal guidance in a dynamic environment.']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data engineer,Tech Observer,"Bellevue, WA",2 days ago,Be among the first 25 applicants,"['', '- Experience in a data analysis role', '- Build processes supporting data transformation, data structures, metadata, dependency and workload management.', 'SQL - BTEQ ScriptExperience Building data pipelinesExperience Data modeling', 'Experience Data modeling', '- Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AZURE technologies.', '- We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools:', 'JD Below:', '- Strong analytic skills related to working with structured or unstructured datasets.', 'Must Haves:', '- A successful history of manipulating, processing and extracting value from data files.', 'The experience youll bring.', 'Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.', '\xa0', 'Experience Building data pipelines', '- Assemble large, complex data sets that meet functional / non-functional business requirements.', '- Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.', '- Work with data and analytics experts to strive for greater functionality in our data systems.', '- Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.', '- Experience or working knowledge of SAS, Oracle APEX, Snowflake is a plus', '- Experience building and optimizing data pipelines, architectures and data sets.', '- Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.', '- Strong project management and organizational skills.', '- Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.', '- Experience with relational SQL databases, including Teradata, Oracle, SQL server.', 'SQL - BTEQ Script', '- Experience with Azure cloud services.', '- Experience supporting and working with cross-functional teams in a dynamic environment.', '- Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.', '- Create and maintain optimal data pipeline architecture,', '- Experience with object-oriented/object function scripting languages: Python, Java, etc.']",Entry level,Other,Information Technology,Banking,2021-03-24 13:05:10
Data Engineer,Holiday Inn Club Vacations,"Orlando, FL",2 weeks ago,29 applicants,"['', 'Special projects and ad-hoc reporting as required. (i.e. Assisting in data migration processes, query optimization, assist IT job troubleshooting).', 'Intermediate to advanced level of MS Office Suite', 'This position will be responsible for architecting the overall structure of data and data-related resources as an integral part of the Insights, Analytics, and Data team, as well as any other duties as required to successfully implement the data strategy. The Data Engineer role will act as a technical subject matter expert in support of several business units within our enterprise. It entails close collaboration with the Senior Analysts, and Management of the internal IAD department, as well as frequent interdepartmental collaborations. Works independently and must be extremely analytical and precise. Daily activities include, but not limited to, creating ETL processes, data modeling within SQL and Power BI, creating data dictionary definitions, complying with data governance practices and standards.', 'Bachelor’s Degree in Computer Science, Engineering, or related business area preferred3 plus years of experience with SQL Server (Tables, Views, Stored Procedures, Functions)2 plus years of experience with ETL Processes (SSIS)2 plus years of experience with data modeling and visualization (Power BI preferred)Microsoft Certifications a plusTimeshare/Hospitality industry experience a plusWritten and verbal communication and organization skills are criticalBasic understanding of data warehousing design/conceptsIntermediate to advanced level of MS Office SuiteAdvanced level of SQL proficiency preferredExperience with Power Query (M) and DAXStrong attention to detailAbility to adapt in fast-paced work environmentAbility to prioritize work to meet deadlines', 'ESSENTIAL DUTIES AND TASKS:', 'POSITION DESCRIPTION:', 'Bachelor’s Degree in Computer Science, Engineering, or related business area preferred', 'Basic understanding of data warehousing design/concepts', 'Written and verbal communication and organization skills are critical', 'Create Power BI Reporting Solutions – Modeling, Visualization, and Documentation.', 'Ability to prioritize work to meet deadlines', 'Craft, develop and manipulate MS SQL databases, tables, queries, and stored procedures.', 'Advanced level of SQL proficiency preferred', '2 plus years of experience with ETL Processes (SSIS)', '3 plus years of experience with SQL Server (Tables, Views, Stored Procedures, Functions)', 'At Holiday Inn Club Vacations, we believe in strengthening families. And we look for people who exhibit the courage, caring, and creativity to help us become the most loved brand in family travel. We’re committed to growing our people, memberships, resorts, and guest love. That’s why we need individuals who are passionate in life and bring those qualities to work every day. Do you instill confidence, trust, and respect in those around you? Do you encourage success and build relationships? If so, we’re looking for you.', 'Create Power BI Reporting Solutions – Modeling, Visualization, and Documentation.Craft, develop and manipulate MS SQL databases, tables, queries, and stored procedures.ETL Processes – mapping, coding, testing.Identify and define common data requirements for use in models and metadata.Special projects and ad-hoc reporting as required. (i.e. Assisting in data migration processes, query optimization, assist IT job troubleshooting).', 'Experience with Power Query (M) and DAX', 'ETL Processes – mapping, coding, testing.', 'Ability to adapt in fast-paced work environment', 'Microsoft Certifications a plus', 'Strong attention to detail', '2 plus years of experience with data modeling and visualization (Power BI preferred)', 'QUALIFICATIONS:', 'Timeshare/Hospitality industry experience a plus', 'Identify and define common data requirements for use in models and metadata.']",Associate,Full-time,Analyst,Hospitality,2021-03-24 13:05:10
Data Engineer,Underdog.io,New York City Metropolitan Area,2 weeks ago,29 applicants,"['', '---', ""We accept companies that offer competitive salaries, benefits, and perks. They're working on interesting technical challenges and must be respectful of your time to stay active."", ""To apply to the network, we'll ask you to fill out a 60-second web form. It's absolutely free."", 'Building an inclusive and diverse workplace is one of Underdog.io’s core values. We warmly welcome people of all backgrounds, experiences, and perspectives.', 'Our companies look for data engineers proficient in Python, Java, or C++. The ideal candidate is passionate about building clean pipelines and maintaining data products relied on by many.', 'To get started, click through to the link and drop your email.', 'The Underdog.io network is a curated group of some of the fastest growing companies in New York and San Francisco. We actively turn away more than 50% of companies that attempt to join.', ""If accepted, you'll hear directly from founders, hiring managers, and other key decision makers starting the following Monday. Our platform will hide your profile from your current employer. "", ""We're looking for a data engineer to join a company in the Underdog.io network.""]",Mid-Senior level,Full-time,Engineering,Internet,2021-03-24 13:05:10
Data Migration Engineer,ELLKAY,"Elmwood Park, NJ",6 days ago,28 applicants,"['', 'Company Culture:', 'Knowledge of healthcare data & workflow preferred', 'Assess client’s desired scope, analyze client data and design project plan.', 'This is an onsite position at our Elmwood Park HQ. Remote work may be available. ', 'Evaluate and identify opportunities to drive continuous process improvements', 'Proactively identifies issues and works on resolution plan to alleviate impact on project.', 'Essential Duties And Responsibilities', 'EMR Data Migration Specialist', 'Bachelor’s degree Computer Science, Data Analytics or related field2+ years Experience with data analysis and SQLKnowledge of healthcare data & workflow preferredStrong problem solving and analytical skillsAbility to exercise effective decision-making capabilities in a fast-paced environment.Excellent communication and organizational skills', 'Excellent communication and organizational skills', 'Performs data validation and testing to ensure accuracy', '2+ years Experience with data analysis and SQL', 'Ability to exercise effective decision-making capabilities in a fast-paced environment.', 'Coordinate deliverables with various internal departments.', 'Set and manage appropriate expectations for successful project execution.', 'ELLKAY', 'Strong problem solving and analytical skills', 'Assess client’s desired scope, analyze client data and design project plan.Proactively identifies issues and works on resolution plan to alleviate impact on project.Set up environment for EMR data Migration and performs ETL for desired databasesPerforms data validation and testing to ensure accuracyEvaluate and identify opportunities to drive continuous process improvementsManage multiple projects simultaneously and prioritize tasks to ensure timely deliveryCoordinate deliverables with various internal departments.Set and manage appropriate expectations for successful project execution.', 'Manage multiple projects simultaneously and prioritize tasks to ensure timely delivery', 'Company Description', 'Job Description', 'Bachelor’s degree Computer Science, Data Analytics or related field', 'Set up environment for EMR data Migration and performs ETL for desired databases']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Connection,"Downers Grove, IL",1 week ago,Be among the first 25 applicants,"['', '5 plus years of experience and a Bachelors Degree', 'Skills', 'Work with technical team to prioritize technical needs of solutions and take appropriate action to fulfill', 'Connection, Inc. and all of its subsidiary companies are committed to equal opportunity and proud to be affirmative action employers. All qualified applicants will receive consideration for employment, without regard to race, sex (including pregnancy), color, religion, age, national origin, ancestry, physical or mental disability status, medical condition, sexual orientation, marital status, protected veteran status, and all other characteristics protected by applicable state and federal law.', 'Analyze and understand business requirements and translate into logical data models', 'Work directly with end users, analysts, and project managers to understand business requirements and develop technical software requirements alongside other engineers', 'Experience with OLTP/OLAP database technologies (MS SQL Server, MySQL, or similar)', 'Author queries and pipelines for data extraction, movement, integration, and storage', ' Analyze and understand business requirements and translate into logical data models Author queries and pipelines for data extraction, movement, integration, and storage Design, create and maintain solutions, extensions, and integrations for applications Understand, explain, develop, and modify relational and non-relational data models; including designing, creating, and customizing along with optimizing based on system performance Design and build reporting solutions and dashboards from myriad disparate data sources Troubleshoot, diagnose, upgrade, and improve the performance of reporting and data solutions Work directly with end users, analysts, and project managers to understand business requirements and develop technical software requirements alongside other engineers Follow DevOps best practices throughout the solution development lifecycle, user acceptance testing, and production release Utilize appropriate data analysis and modeling tools and languages based on the given requirements; learn new data analysis languages and tools when necessary. Work with technical team to prioritize technical needs of solutions and take appropriate action to fulfill Additional duties and projects as assigned ', 'Competency with non-relational database technologies (MongoDB, CosmosDB)', ' 5 plus years of experience and a Bachelors Degree Hi-level of competency and experience with SQL and platform specific versions (e.g.: T-SQL) Competency with web technologies including REST/SOAP APIs, Web Services, JSON, and OData Experience with data visualization tools (Power BI, Tableau) Competency with non-relational database technologies (MongoDB, CosmosDB) Experience with OLTP/OLAP database technologies (MS SQL Server, MySQL, or similar) ', 'Troubleshoot, diagnose, upgrade, and improve the performance of reporting and data solutions', 'Responsibilities', 'Hi-level of competency and experience with SQL and platform specific versions (e.g.: T-SQL)', 'Additional duties and projects as assigned', 'Design and build reporting solutions and dashboards from myriad disparate data sources', 'Follow DevOps best practices throughout the solution development lifecycle, user acceptance testing, and production release', 'Utilize appropriate data analysis and modeling tools and languages based on the given requirements; learn new data analysis languages and tools when necessary.', 'Competency with web technologies including REST/SOAP APIs, Web Services, JSON, and OData', 'Essential Functions', 'Design, create and maintain solutions, extensions, and integrations for applications', 'Understand, explain, develop, and modify relational and non-relational data models; including designing, creating, and customizing along with optimizing based on system performance', 'Experience with data visualization tools (Power BI, Tableau)']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer-Cloud ,Paradigm Technology,"Woonsocket, RI",1 week ago,62 applicants,"['', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience implementing Machine Learning models and building highly scalable and high availability systems', 'Preferred Qualifications: ', 'Location: Woonsocket, RI 02895 (can work remotely, due to COVID-19)', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa05+ years of hands-on experience with “big data” platforms, including Kubernetes OR Spark, as well as experience with Traditional RDBMS (Oracle, Teradata etc.,)', 'Position:', 'Position: Data Engineer-Cloud', 'Learn more at www.pt-corp.com', 'Preferred Qualifications', 'Platforms', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Analytics', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Cloud', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience operating in distributed environments, including cloud (Azure, GCP, AWS etc.)', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Familiarity with building data pipelines, data modeling, architecture, and governance concepts', 'Responsibilities:', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa05+ years of experience in the following programming languages: Python, shell scripting, SQL (preferably Snowflake, Teradata, and PL/SQL) and Java, or Scala', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience building, launching, and maintaining complex analytics pipelines in production', 'Qualifications:', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Proficiency with tools to automate workflow and data pipelines (e.g., GitLab, Jenkins, Control-M)', '(can work remotely, due to COVID-19)', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Big Data', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Ability to design and build a framework to orchestrate data pipelines and Machine Learning models', ""·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Master's Degree in Data Science or Business Analytics"", '\xa0', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Languages: Kubectl, SnowSQL, Python, PySpark, Scala', 'Languages', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa05+ years of professional IT and Business Analytics experience', 'Paradigm offerings include:', 'Location:', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Project Leadership', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Platforms: Snowflake, Kubernetes, Kubeflow, TensorFlow, Azure Cloud, Teradata', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Aggregate a huge amount of data and information from large numbers of sources to discover patterns and features necessary to build machine learning models', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Design and implement end-to-end solutions using Machine Learning, Optimization, and other advanced computer science technologies, and own live deployments', 'With nearly 200 employees nationwide, Paradigm Technology is a boutique consulting company and strategic solutions partner.', 'Company Description', 'Company Description:', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0SnowPro certification', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Familiarity with data modeling, data architecture, and governance concepts', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Exposure to Healthcare Domain knowledge']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,RemoteHub,"Home, KS",1 week ago,Be among the first 25 applicants,"['', ' Data Studio Job', ' SQL Azure, Synapse (SQL Datawarehouse)', 'Solid Familiarity On The Following Tools', ' Azure Data Lake, Blob Storage', ' Develop and unit test assigned features to meet product requirements.', ' Expert knowledge of SQL and of relational database systems and concepts.', ' Azure Data Factory', ' Understand business processes, logical data models and relational database implementations for data analysis.', ' Experience designing and building scalable and robust data pipelines to enable data-driven decisions for the business.', ' PowerBI', ' Develop, implement and tune ETL processes.', ' Ensure documentation of all project artefacts are accurate and current.', ' Develop and implement solutions for data quality validation and continuous improvement.', ' Drive our data platform and help evolve our technology stack and development best practices', ' Databricks', ' Gathering technical requirement from customer and enable the right team to develop and implement it. Experience designing and building scalable and robust data pipelines to enable data-driven decisions for the business. Understand business processes, logical data models and relational database implementations for data analysis. Build data expertise and implement own data quality test cases for required areas. Expert knowledge of SQL and of relational database systems and concepts. Expert knowledge in Azure Data Factory, Databricks Script, Scope Scripts, Cosmos etc. Demonstrated strength in data modeling, ETL development, and data warehousing Develop, implement and tune ETL processes. Experience analyzing data to discover opportunities and address gaps. Develop and maintain data pipelines including solutions for data collection, management, and usage. Develop and implement solutions for data quality validation and continuous improvement. Drive our data platform and help evolve our technology stack and development best practices Develop and unit test assigned features to meet product requirements. Working knowledge of data quality approaches and techniques. Programming language experience (C#, Python, Scala Spark.) is a plus. Build visualizations in PowerBI to help derive meaningful insights from data. Maintain and enhance our data and computation platform up and running. Accountable for ensuring that project tasks are accomplished to schedule at acceptable level of quality; or that appropriate contingencies are implemented. Ensure documentation of all project artefacts are accurate and current. Ensure all projects are delivered successfully and according to Engagement statement of work process, methodology, and quality standards.', ' Accountable for ensuring that project tasks are accomplished to schedule at acceptable level of quality; or that appropriate contingencies are implemented.', ' Experience analyzing data to discover opportunities and address gaps.', ' Build data expertise and implement own data quality test cases for required areas.', ' Build visualizations in PowerBI to help derive meaningful insights from data.', ' Ensure all projects are delivered successfully and according to Engagement statement of work process, methodology, and quality standards.', ' Gathering technical requirement from customer and enable the right team to develop and implement it.', ' Data Quality management. Test Cases etc.', ' Working knowledge of data quality approaches and techniques.', ' SQL Azure, Synapse (SQL Datawarehouse) Azure Data Factory Kusto Scope Scripts, Cosmos PowerBI Azure Data Lake, Blob Storage Databricks Data Quality management. Test Cases etc. Data Studio Job', ' Demonstrated strength in data modeling, ETL development, and data warehousing', ' Expert knowledge in Azure Data Factory, Databricks Script, Scope Scripts, Cosmos etc.', ' Kusto', ' Apply for this Data Engineer position', ' Maintain and enhance our data and computation platform up and running.', ' Scope Scripts, Cosmos', ' Develop and maintain data pipelines including solutions for data collection, management, and usage.', ' Programming language experience (C#, Python, Scala Spark.) is a plus.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer - Personalization (Remote Eligible - Americas),Spotify,"Boston, MA",2 weeks ago,126 applicants,"['', 'You understand the value of partnership within teams.', 'Collaborate with other software engineers, ML experts and stakeholders, taking learning and leadership opportunities that will arise every single day.', 'Build large-scale batch and real-time data pipelines with data processing frameworks like Scalding, Scio, Storm, Spark and the Google Cloud Platform.Use best practices in continuous integration and delivery.Help drive optimization, testing and tooling to improve data quality.Collaborate with other software engineers, ML experts and stakeholders, taking learning and leadership opportunities that will arise every single day.Work in multi-functional agile teams to continuously experiment, iterate and deliver on new product objectives!', 'Prefer an office to work from home instead? Not a problem! We have plenty of options for your working preferences. Find more information about our Work From Anywhere options here .', 'Help drive optimization, testing and tooling to improve data quality.', 'We are proud to foster a workplace free from discrimination. We strongly believe that diversity of experience, perspectives, and background will lead to a better environment for our employees and a better product for our users and our creators. This is something we value deeply and we encourage everyone to come be a part of changing the way the world listens to music. ', 'Working hours? We operate within the Eastern Standard time zone for collaboration and ask that all be located that time zone. ', 'You know how to work with high volume heterogeneous data, preferably with distributed systems such as Hadoop, BigTable, and Cassandra.', 'Work in multi-functional agile teams to continuously experiment, iterate and deliver on new product objectives!', 'Use best practices in continuous integration and delivery.', 'You know how to work with high volume heterogeneous data, preferably with distributed systems such as Hadoop, BigTable, and Cassandra.You know how to write distributed, high-volume services in Java or Scala.You are knowledgeable about data modeling, data access, and data storage techniques.You appreciate agile software processes, data-driven development, reliability, and responsible experimentation.You understand the value of partnership within teams.We are proud to foster a workplace free from discrimination. We strongly believe that diversity of experience, perspectives, and background will lead to a better environment for our employees and a better product for our users and our creators. This is something we value deeply and we encourage everyone to come be a part of changing the way the world listens to music. ', 'You are knowledgeable about data modeling, data access, and data storage techniques.', 'We are a distributed workforce enabling our band members to find a work mode that is best for them!', ""What You'll Do"", 'You know how to write distributed, high-volume services in Java or Scala.', 'Where in the world? For this role, it can be within the Americas region in which we have a work location and is within working hours. ', 'Build large-scale batch and real-time data pipelines with data processing frameworks like Scalding, Scio, Storm, Spark and the Google Cloud Platform.', 'We are a distributed workforce enabling our band members to find a work mode that is best for them!Where in the world? For this role, it can be within the Americas region in which we have a work location and is within working hours. Working hours? We operate within the Eastern Standard time zone for collaboration and ask that all be located that time zone. Prefer an office to work from home instead? Not a problem! We have plenty of options for your working preferences. Find more information about our Work From Anywhere options here .', ""Where You'll Be"", 'Who You Are', 'You appreciate agile software processes, data-driven development, reliability, and responsible experimentation.']",Not Applicable,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer - Intelligent Forecasting,DICK'S Sporting Goods,"Coraopolis, PA",1 week ago,Be among the first 25 applicants,"['', 'Experience with Agile Development and Agile Deployment tools and versioning using Git or similar tools\xa0', 'Experience with Cloud Identity and Access Management for data on public cloud.', 'You would partner with merchandising, supply-chain, pricing, and product development business teams to build the next-generation of data insights. Work alongside data science teams building AI/ML models, platform engineers creating services that support you, and software developers creating unique experiences.', 'Any Public Cloud certification focused on Cloud Engineering', 'Work with stakeholders including the product, data and architecture teams to assist with data-related technical issues and support their data infrastructure needs.', 'Hands-on experience building, managing, and automating data pipelines', 'Some experience applying security and privacy to how you manage data.', 'Experience building data pipelines on modern public cloud services like Snowflake, AWS, GCP, or Azure.', 'A good grip of data structures, relationships, integration patterns, and algorithms.', '3+ years of experience being close to the business and delivering value through it as part of a team.', 'Three to five years of experience in\xa0Data Engineering, AI/ML Engineer Integration, Data Modeling', 'Work as part of a team building the data ingestion, products, pipelines, and tooling supporting our data products driving merchandising, supply-chain, pricing, and product development initiaves.Work with stakeholders including the product, data and architecture teams to assist with data-related technical issues and support their data infrastructure needs.Provide proactive design, operational support, and governance for privacy and security policy for data.', 'Job Duties & Responsibilities', 'What you will bring:', 'Experience with message queuing, stream processing, and highly scalable ‘big data’ data stores (Kafka, Pub/Sub)', 'Some knowledge or exposure to supporting AI/ML engineering and integrating data to model development, management and serving.', 'We are looking for a Data Engineer who wants to be a part of a team solving the ingestion, enrichment, and activation of data to drive better development, forecasting, allocation, and insights into the products we sell. In this role, you will work with the core of how we use inventory, transaction, and historical forecast data working with modern cloud and open source technology to transform the future of sport.', 'Any Public Cloud certification focused on Data Engineering or Data Science', 'Proficient with SQL, Spark, and other common Query languages', 'Experience with AI/ML workflow frameworks like Kubeflow, Azure AI/ML, AWS Sagemaker, etc.', ""Bachelor's Degree in\xa0Computer Science, Software Engineering, Information Systems or Information Technology or related field required, or equivalent experienceThree to five years of experience in\xa0Data Engineering, AI/ML Engineer Integration, Data ModelingAny Public Cloud certification focused on Data Engineering or Data ScienceAny Public Cloud certification focused on Cloud EngineeringExperience building data pipelines on modern public cloud services like Snowflake, AWS, GCP, or Azure.Experience with message queuing, stream processing, and highly scalable ‘big data’ data stores (Kafka, Pub/Sub)Proficient with SQL, Spark, and other common Query languagesExperience with Cloud Identity and Access Management for data on public cloud.Proficient with object-oriented programming and scripting languages (Python, Java, etc..)Experience with continuous integration/continuous delivery (CI/CD) pipelines (Jenkins, Concourse, Azure DevOps)Experience with relational databases (Oracle, SQL Server, etc..) as well as NoSQL database technologies (MongoDB, BigTable, Cassandra, etc..)Experience with AI/ML workflow frameworks like Kubeflow, Azure AI/ML, AWS Sagemaker, etc.Proficient in developing, maintaining and interacting with APIsExperience with Agile Development and Agile Deployment tools and versioning using Git or similar tools\xa0Proficient in Linux/Unix environments"", 'Proficient in Linux/Unix environments', 'This role builds new data products, pipelines, APIs, materialized views, services, and tools. They are fascinated with data and the modern ways of creating rich, scalable data products driving how we work.', 'A grasp of the importance of common data platform patterns and how they relate like Data Lake, Data Mesh, Data Catalog, Tagging, Stream Processing, etc.', 'Experience with continuous integration/continuous delivery (CI/CD) pipelines (Jenkins, Concourse, Azure DevOps)', 'Provide proactive design, operational support, and governance for privacy and security policy for data.', 'At Dicks Sporting Goods, we are creating the future of sport driven by powerful data products and platforms that serve our Athletes and Teammates.', 'Experience and love for Python, Spark, SQL, or other standard data scripting languagesHands-on experience building, managing, and automating data pipelinesFamiliarity and appreciation for modern public cloud data services from AWS, GCP, or Azure3+ years of experience being close to the business and delivering value through it as part of a team.Some knowledge or exposure to supporting AI/ML engineering and integrating data to model development, management and serving.A good grip of data structures, relationships, integration patterns, and algorithms.You understand the importance of Kafka, Snowpipe, or Kinesis for real-time needsSome experience applying security and privacy to how you manage data.A grasp of the importance of common data platform patterns and how they relate like Data Lake, Data Mesh, Data Catalog, Tagging, Stream Processing, etc.', 'Qualifications', 'Experience and love for Python, Spark, SQL, or other standard data scripting languages', 'Proficient in developing, maintaining and interacting with APIs', 'Proficient with object-oriented programming and scripting languages (Python, Java, etc..)', 'Experience with relational databases (Oracle, SQL Server, etc..) as well as NoSQL database technologies (MongoDB, BigTable, Cassandra, etc..)', 'You understand the importance of Kafka, Snowpipe, or Kinesis for real-time needs', 'Work as part of a team building the data ingestion, products, pipelines, and tooling supporting our data products driving merchandising, supply-chain, pricing, and product development initiaves.', 'We are open to this role being remote.\xa0', 'Familiarity and appreciation for modern public cloud data services from AWS, GCP, or Azure', ""Bachelor's Degree in\xa0Computer Science, Software Engineering, Information Systems or Information Technology or related field required, or equivalent experience""]",Associate,Full-time,Information Technology,Retail,2021-03-24 13:05:10
Data Engineer,General Mills,"Minneapolis, MN",2 weeks ago,30 applicants,"['', 'Minimum 2 years of IT experience, 3+ preferredCloud data experience', 'Company Overview', 'Ability to research, plan, organize, lead, and implement new processes or technology', 'Key Responsibilities', 'Bachelor’s Degree; Computer Science, MIS, or Engineering preferredMinimum 2 years of IT experience, 3+ preferredCloud data experienceStrong understanding of Hadoop fundamentalsDatabase development experience using Oracle, SQL Server, SAP BW or SAP HANAJob Scheduling experienceProcess mindset with experience creating, documenting and implementing standard processesDevelopment experience using Hive and/or SparkEffective verbal and written communication and influencing skills.Effective analytical and technical skills.Ability to work in a team environmentAbility to research, plan, organize, lead, and implement new processes or technology', 'Minimum Qualifications', 'Ability to work in a team environment', 'Preferred Qualifications', 'Job Scheduling experience', 'Collaboratively troubleshoot technical and performance issues in the big data ecosystem', 'Familiarity with Kafka ', 'Act as a key technical leader within General Mills', 'Strong understanding of Hadoop fundamentals', 'Overview', 'Bachelor’s Degree; Computer Science, MIS, or Engineering preferred', 'Experience with agile techniques or methods', 'Effective analytical and technical skills.', 'Python, Scala or Java development experience', 'Database development experience using Oracle, SQL Server, SAP BW or SAP HANA', 'Participate in the evaluation, implementation and deployment of emerging tools & process in the big data space.', 'Design, create, code, and support a variety of ETL solutions (potentially including but not limited to: Talend Studio, Python, Scala, Kafka, SAP Data Services, or others)', 'Partner with business analysts and solutions architects to deliver business initiatives.', 'Python, Scala or Java development experienceFamiliarity with Kafka Familiarity with the Linux operating systemExperience with agile techniques or methods', 'Familiarity with the Linux operating system', 'Generate and implement your own ideas on how to improve the operational and strategic health of big data ecosystem', 'Effective verbal and written communication and influencing skills.', 'Process mindset with experience creating, documenting and implementing standard processes', 'Development experience using Hive and/or Spark', 'Act as a key technical leader within General MillsDesign, create, code, and support a variety of ETL solutions (potentially including but not limited to: Talend Studio, Python, Scala, Kafka, SAP Data Services, or others)Generate and implement your own ideas on how to improve the operational and strategic health of big data ecosystemParticipate in the evaluation, implementation and deployment of emerging tools & process in the big data space.Partner with business analysts and solutions architects to deliver business initiatives.Collaboratively troubleshoot technical and performance issues in the big data ecosystem']",Entry level,Full-time,Information Technology,Consumer Goods,2021-03-24 13:05:10
ETL Data Engineer,"TalentBurst, an Inc 5000 company","Costa Mesa, CA",4 weeks ago,175 applicants,"['', 'Pyspark & SQL', 'Acquire an understanding of existing data, perform analysis and provide ETL solutions', 'As per the Hiring Manager:', 'Experience with business intelligence or reporting tools (preferably Tableau and Alteryx)', 'Expertise in SQL and Python scripting', 'Must-Have:', 'Minimum 4-year experience in BI development and support.4 years of experience with data warehouse concepts and methodologiesMinimum 3+ Years of experience with Pyspark & SQLExpertise in SQL and Python scriptingExperience with business intelligence or reporting tools (preferably Tableau and Alteryx)Experience with AWS environment will be a plusExcellent written and oral communication skillsHighly motivated self-starter, detail and quality-oriented, and able to work independentlyEducation: BS degree or higher in MIS or engineering fields\xa0', 'Responsibilities:', 'ETL Data Engineer is responsible for the design, development, delivery, and support of ETL solutions using PySpark and SQL.', 'Work with Engineering teams to explore and understand new data being introduced to front end application', 'Nice to Have:', 'Qualifications:', 'Participate in requirement gathering, solution design, and implementation of data warehouse and reporting projects.', 'This role is 50% Focused on Machine learning & 50% Focused on ETL DevelopmentMust-Have: PySpark,\xa0Strong SQL, PythonNice to Have: Data Warehouse, Tableau, Alteryx, AWS', 'This role is 50% Focused on Machine learning & 50% Focused on ETL Development', '\xa0', 'Nice to Have: Data Warehouse, Tableau, Alteryx, AWS', 'Education: BS degree or higher in MIS or engineering fields\xa0', 'Minimum 3+ Years of experience with Pyspark & SQL', 'Responsibilities:\xa0', 'Highly motivated self-starter, detail and quality-oriented, and able to work independently', 'Duration: 5 Months Contract', 'Position: ETL Data Engineer\xa0(Only W2)', 'ETL Data Engineer is responsible for the design, development, delivery, and support of ETL solutions using PySpark and SQL.Work as part of a team to support business analytics for growing online consumer subscription service.Acquire an understanding of existing data, perform analysis and provide ETL solutionsDevelop and troubleshoot PySpark jobs in AWS environmentSupport and enhance existing ETL jobs in SQLParticipate in requirement gathering, solution design, and implementation of data warehouse and reporting projects.Work with Engineering teams to explore and understand new data being introduced to front end applicationCreate and maintain ETL specifications and process documentation to produce the required data deliverablesTroubleshoot and resolve data, system, and performance issues.\xa0', 'Support and enhance existing ETL jobs in SQL', 'Minimum 4-year experience in BI development and support.', 'Location: Fully Remote OR Costa Mesa, CA', 'Experience with AWS environment will be a plus', 'Must-Have: PySpark (Minimum 2 Years) & Strong SQL (5+ Years)', 'Excellent written and oral communication skills', 'Create and maintain ETL specifications and process documentation to produce the required data deliverables', 'PySpark (Minimum 2 Years) & Strong SQL (5+ Years)', 'Work as part of a team to support business analytics for growing online consumer subscription service.', 'Must-Have: PySpark,\xa0Strong SQL, Python', 'Troubleshoot and resolve data, system, and performance issues.\xa0', 'Develop and troubleshoot PySpark jobs in AWS environment', '4 years of experience with data warehouse concepts and methodologies']",Mid-Senior level,Contract,Engineering,Staffing and Recruiting,2021-03-24 13:05:10
Data Engineer,Centene Corporation,"Charlotte, NC",1 week ago,Be among the first 25 applicants,"['', ' Contribute to the development and maintenance of real-time processing applications Contribute to the creation and maintenance of optimal data pipeline architectures Conduct maintenance and support for core infrastructure health, system upgrades, monitoring, CI/CD and logging Research streaming best practices and proper stream architecture Collaborate with team members to better understand existing data requirements and validation rules Analyze trends in data sets and contribute to the development of algorithms in order to improve upon the usefulness of raw data ', 'Collaborate with team members to better understand existing data requirements and validation rules', 'Position Purpose', 'Conduct maintenance and support for core infrastructure health, system upgrades, monitoring, CI/CD and logging', 'Contribute to the creation and maintenance of optimal data pipeline architectures', 'Research streaming best practices and proper stream architecture', 'Education/Experience', 'Contribute to the development and maintenance of real-time processing applications', 'Centene is an equal opportunity employer that is committed to diversity, and values the ways in which we are different. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, veteran status, or other characteristic protected by applicable law.', 'Analyze trends in data sets and contribute to the development of algorithms in order to improve upon the usefulness of raw data']",Not Applicable,Full-time,Information Technology,Hospital & Health Care,2021-03-24 13:05:10
Data Engineer,Homesite Insurance,"Phoenix, AZ",1 month ago,Be among the first 25 applicants,"['', 'Insurance industry experience a plus, but not required.', 'Knowledge, Skills And Competencies', 'Experience with cloud computing APIs (Amazon Web Services preferred).', 'Test, design, and implement process automation techniques to support efficiencies.', 'Other Job Experiences Desired', 'Experience with cloud computing services (Amazon Web Services like Lambda, S3, CloudWatch, ECS, and RDS preferred)', 'A mastery of best practices in coding, code organization, data transformation, ETL, process automation, and APIs.', 'Lead efforts to design, develop and implement databases enhancement to improve efficiency and streamline the use for analytics, business analysis and data governance. ', 'Expertise with Microsoft SQL Server (including SSIS, SSRS) including 2-5 years of experience. ', 'Motivated individual with strong analytic, problem solving, and troubleshooting skills.', 'Compensation may vary based on the job level and your geographic work location.', 'Foster relationships with colleagues to identify and explore new sources of data.', 'Expertise in relational databases (such as MS SQL Server, MySQL and Aurora) concepts.', ' Clearly communicate complex findings to colleagues influencing outcomes and communicating the value add of the data engineering work being completed.', 'Knowledge of one version control system, preferably Git is required.', 'Responsibilities', 'Ability to comprehensively understand data sources, elements and relationships in both business and technical terms.', 'Develop a mastery of the corporate data structure to be able to perform ETL operations from data tables existing in the company.', 'Lead efforts to design, develop and implement databases enhancement to improve efficiency and streamline the use for analytics, business analysis and data governance. Develop a mastery of the corporate data structure to be able to perform ETL operations from data tables existing in the company.Test, design, and implement process automation techniques to support efficiencies. Clearly communicate complex findings to colleagues influencing outcomes and communicating the value add of the data engineering work being completed.Foster relationships with colleagues to identify and explore new sources of data.', 'Insurance industry experience a plus, but not required.Experience with cloud computing APIs (Amazon Web Services preferred).Experience with cloud computing services (Amazon Web Services like Lambda, S3, CloudWatch, ECS, and RDS preferred)', 'A mastery of best practices in coding, code organization, data transformation, ETL, process automation, and APIs.Expertise in relational databases (such as MS SQL Server, MySQL and Aurora) concepts.Ability to comprehensively understand data sources, elements and relationships in both business and technical terms.Expertise with Microsoft SQL Server (including SSIS, SSRS) including 2-5 years of experience. Knowledge of one version control system, preferably Git is required.Bachelor’s degree in Computer Science, Computer Engineering, Information Technology/Systems, related field, or equivalent experience.Motivated individual with strong analytic, problem solving, and troubleshooting skills.', 'Bachelor’s degree in Computer Science, Computer Engineering, Information Technology/Systems, related field, or equivalent experience.']",Entry level,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Quant Data Engineer,C2R Ventures,"Boston, MA",2 days ago,191 applicants,"['', '5+ years of hands on oriented programming language (Python, Spark etc.)', 'Highly successful and growing quant trading firm seeking a Sr Data Engineer for a leadership/hands-on role.\xa0You will help build out the cloud-based future platform and its leading-edge capabilities.\xa0', 'ETL and ELT processes for large data sets', '\ufeffQualifications', 'Experience with building cloud services', 'Engineering or Computer Science degree', 'Engineering or Computer Science degree5+ years of hands on oriented programming language (Python, Spark etc.)SQL Server, T-SQL, data warehousing conceptsETL and ELT processes for large data setsExperience with building cloud services', 'SQL Server, T-SQL, data warehousing concepts', '\xa0']",Mid-Senior level,Full-time,Information Technology,Staffing and Recruiting,2021-03-24 13:05:10
Data Engineer,Evolent Health,"Arlington, VA",1 month ago,Be among the first 25 applicants,"['', 'globally distributed', ' Software and application development  Microsoft Data Storage Technologies - specifically SQL Server 2017+  Microsoft SSIS processing engine', "" Bachelor's degree in Computer Science or related field"", ' Microsoft SSIS processing engine', 'Ability to work in a fast-paced entrepreneurial environment with small focused team of engineers to deliver solutions', 'Evolent Health is an equal opportunity employer and considers all qualified applicants equally without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran status, or disability status.', 'Experience in various data formats used in healthcare - NCPDP, HL7, X12 and XML', 'Technical Requirements', 'Experience or qualifications within Azure Synapse or relative components (High Value)Experience in medical, pharmacy, provider and laboratory domains (High Value)Experience in various data formats used in healthcare - NCPDP, HL7, X12 and XMLExperience with MicroStrategyExperience creating system documentation for both data and business processes flowsAbility to work in a fast-paced entrepreneurial environment with small focused team of engineers to deliver solutions', ' Ability to work both independently, and as part of a globally distributed team of technical and non-technical colleagues ', 'Experience creating system documentation for both data and business processes flows', 'The Experience You’ll Need (Required)', ' Strong attention to detail, analytical thinking, and outstanding problem-solving skills ', 'Experience with MicroStrategy', ' Ability to partner collaboratively with stakeholders', ' Microsoft Data Storage Technologies - specifically SQL Server 2017+ ', 'Your Future Evolves Here', ' Strong written and verbal communication skills ', ' Three or more years of experience with: Software and application development  Microsoft Data Storage Technologies - specifically SQL Server 2017+  Microsoft SSIS processing engine', 'It’s Time For A Change…', ' Software and application development ', "" Bachelor's degree in Computer Science or related field Strong attention to detail, analytical thinking, and outstanding problem-solving skills  Ability to work both independently, and as part of a globally distributed team of technical and non-technical colleagues  Strong written and verbal communication skills  Three or more years of experience with: Software and application development  Microsoft Data Storage Technologies - specifically SQL Server 2017+  Microsoft SSIS processing engine Ability to partner collaboratively with stakeholders"", 'Experience in medical, pharmacy, provider and laboratory domains (High Value)', 'Finishing Touches (Preferred)', 'What You’ll Be Doing', 'Experience or qualifications within Azure Synapse or relative components (High Value)', 'Local candidates (Chicago, IL or Arlington, VA) are preferred for this role. We ask that candidates located outside of the Chicago or Arlington areas be willing to work the normal business hours that align with our local teams.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Karat,Greater Seattle Area,3 weeks ago,81 applicants,"['', 'Support our engineering teams by reviewing code and designs', 'Collaborate with Software Engineers, Data Scientists, and Data Analysts to design, implement and scale our data processing and analysis platforms', 'Design, build and deploy efficient and reliable cloud-based data pipelines to move data across a number of platforms and disparate data sources', 'Our Data Engineers work directly with analysts, scientists, and stakeholder teams to build the infrastructure, reporting, and product features that power insights at Karat. Data Engineers at Karat design and build the data infrastructure needed to make interviews more predictive. They also engineer the systems and processes needed to derive meaning from this data. Working on the Data and Delivery engineering teams you will shape the future of how our company ingests, analyzes, and talks about data.', 'The ideal candidate for this role is comfortable designing and building software for real customers, manipulating data and wielding statistics, providing technical and product recommendations, working cross-functionally to support ideas from inception to launch, and capable of presenting and being a thought partner across the company.\xa0', 'Experience with and a knack for communicating technical work to stakeholders', 'Paid parental leave\xa0', 'About You:', 'Strong data analysis and scientific thinking skills', 'Core Responsibilities:', 'Level up the skills of everyone around you through direct mentorship, brown bags, clean code, and setting a strong example.\xa0', 'Benefits of joining Karat:', 'Experience with data visualization and presentation, familiar with data analysis and BI tools', 'We value a diverse workforce: people of color, women, and LGBTQIA+ individuals are strongly encouraged to apply.', 'Experience with Amazon Web Services', 'Competitive salary and benefits', 'Bachelor’s degree in Computer Science or a related field (or equivalent experience)', '\xa0', 'As we collect richer data about user interactions during interviews, you will drive the collection and refinement of our data to increase the signal captured in every minute of our interviews and maximize the value provided to our customers.', '2+ years of experience in data engineering, software engineering, or other related roles, with a focus on building data pipelines and conducting data analysis', 'While doing meaningful work is rewarding in itself, we also offer the following programs and benefits for all our full-time employees:', '2+ years of experience in data engineering, software engineering, or other related roles, with a focus on building data pipelines and conducting data analysisExperience with development best practices, including version control, code reviews, and documentationBachelor’s degree in Computer Science or a related field (or equivalent experience)Experience with Amazon Web ServicesExperience with and a knack for communicating technical work to stakeholdersExperience with data visualization and presentation, familiar with data analysis and BI toolsStrong data analysis and scientific thinking skillsExperience with Python, Docker and job orchestration frameworks such as Apache AirflowSignificant experience with relational databases with a strong knowledge of SQL and data modeling best principles', 'Nurture the data conversations and culture at Karat by educating technical and non-technical teammates about data literacy and usage.', 'Collaborate with Software Engineers, Data Scientists, and Data Analysts to design, implement and scale our data processing and analysis platformsDesign, build and deploy efficient and reliable cloud-based data pipelines to move data across a number of platforms and disparate data sourcesIdentify data needs for business and product teams, understand their metrics and analysis requirements and build solutions to enable data-driven decisionsProtect our data sources against quality issues. Design and develop data quality monitoring.As we collect richer data about user interactions during interviews, you will drive the collection and refinement of our data to increase the signal captured in every minute of our interviews and maximize the value provided to our customers.Support our engineering teams by reviewing code and designsNurture the data conversations and culture at Karat by educating technical and non-technical teammates about data literacy and usage.Level up the skills of everyone around you through direct mentorship, brown bags, clean code, and setting a strong example.\xa0', 'Significant experience with relational databases with a strong knowledge of SQL and data modeling best principles', 'Statement of Non-Discrimination:', 'Experience with development best practices, including version control, code reviews, and documentation', 'Flexible vacation and paid company holidays', 'Experience with Python, Docker and job orchestration frameworks such as Apache Airflow', 'Medical / dental / vision insurance\xa0', 'Protect our data sources against quality issues. Design and develop data quality monitoring.', 'Identify data needs for business and product teams, understand their metrics and analysis requirements and build solutions to enable data-driven decisions', 'State-of-the-art equipment for your work station', 'In keeping with our beliefs and goals, no employee or applicant will face discrimination or harassment based on: race, color, ancestry, national origin, religion, age, gender, marital/domestic partner status, sexual orientation, gender identity or expression, disability status, or veteran status. Above and beyond discrimination and harassment based on “protected categories,” we also strive to prevent other subtler forms of inappropriate behavior (i.e., stereotyping) from ever gaining a foothold in our office. Whether blatant or hidden, barriers to success have no place at Karat.', 'Competitive salary and benefitsMedical / dental / vision insurance\xa0Flexible vacation and paid company holidaysPaid parental leave\xa0State-of-the-art equipment for your work station']",Mid-Senior level,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Data Engineer,Peachy,"New York, NY",3 weeks ago,61 applicants,"['', 'Competitive compensation', 'Strong written and verbal communication skills, the ability to collaborate with technical and non-technical stakeholders', 'Implement best practices for organizing and maintaining clean data', 'Create end-to-end data pipelines, working with a variety of internal and external sources for custom data integrations', 'Contribute to technical leadership of the team through data architecture and data collection decisions company-wide', "" What You'll Do "", 'Experience working with LookML', ' Bonus Points ', 'Experience with continuous integration and delivery via messaging and queueing protocols', 'Experience with Google Cloud Platform and AWS', '4+ years of experience as a Data Engineer or in a similar role', 'Collaborate with key stakeholders in a fast-paced work environment', 'Health & Wellness Perks', 'The opportunity to join a high-growth company at an early stage', 'Strong understanding of relational databases such as PostgreSQL, MySQL or MSSQL (experience using BigQuery is a plus)', 'Design and manage our relational database', 'Conduct data modeling and ensure the timely delivery of data reporting initiatives to senior management', 'Experience maintaining data cleanliness and streamlining large data sets', 'Complimentary Peachy services and products', 'Health & Wellness PerksCompetitive compensationComplimentary Peachy services and productsThe opportunity to join a high-growth company at an early stageThe ability to impact the growth of the company, we value employee feedback & suggestions!', 'Collaborate with key stakeholders in a fast-paced work environmentDesign and manage our relational databaseBuild, maintain, and optimize the backend of our business intelligence tool (Looker)Develop data quality testing and processes to ensure our data infrastructure will be accurate, robust and observableCreate end-to-end data pipelines, working with a variety of internal and external sources for custom data integrationsImplement best practices for organizing and maintaining clean dataContribute to technical leadership of the team through data architecture and data collection decisions company-wideConduct data modeling and ensure the timely delivery of data reporting initiatives to senior management', 'Experience working with LookMLExperience with continuous integration and delivery via messaging and queueing protocolsEnthusiastic about working with complex data sets to solve problems and propose scalable solutionsFlexible, excited to work in an evolving high growth start-up environment', ' Who You Are ', 'Enthusiastic about working with complex data sets to solve problems and propose scalable solutions', 'Flexible, excited to work in an evolving high growth start-up environment', 'Experience with coding languages such as Python, C++, Java, or C#', 'Education, training or experience equivalent to a BS (or higher, e.g., MS, or PHD) in Computer Science or a related technical field involving coding (e.g., physics or mathematics)Strong understanding of relational databases such as PostgreSQL, MySQL or MSSQL (experience using BigQuery is a plus)Experience with coding languages such as Python, C++, Java, or C#Experience with Google Cloud Platform and AWSExperience maintaining data cleanliness and streamlining large data setsKnowledgeable in data management and data storage principlesStrong written and verbal communication skills, the ability to collaborate with technical and non-technical stakeholders4+ years of experience as a Data Engineer or in a similar role', 'Knowledgeable in data management and data storage principles', 'Build, maintain, and optimize the backend of our business intelligence tool (Looker)', 'Education, training or experience equivalent to a BS (or higher, e.g., MS, or PHD) in Computer Science or a related technical field involving coding (e.g., physics or mathematics)', 'The ability to impact the growth of the company, we value employee feedback & suggestions!', 'Develop data quality testing and processes to ensure our data infrastructure will be accurate, robust and observable']",Entry level,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
SQL Data Engineer,Sequoia Consulting Group,"New York, NY",2 weeks ago,Be among the first 25 applicants,"['', 'Ability to succeed in a dynamic, Agile environment Strong prioritization and time-management skills Dedication to team goals that include support of live 24/7 production systems A consummate collaborator, able to establish good relationships with technical, product, and business owners A champion of quality, able to QA and vouch for the integrity of the report output Maintaining business partner engagement and setting expectations Assessing current processes and recommending changes as needed Documenting and communicating technical specifications to ensure that proper and optimized techniques, queries, data standards, and final outputs are understood and incorporated into data and analytics processes Participate in business analysis activities to gather required reporting and dashboard requirements Translate business requirements into specifications that will be used to implement the required user-friendly environments, reports and dashboards, built from potentially multiple data sources ', 'Assess fitness-for-purpose of existing data model and processes ', 'Design and implement effective database solutions and models to store and retrieve company data ', 'Working familiarity with Salesforce (SFDC) ', 'A consummate collaborator, able to establish good relationships with technical, product, and business owners ', '3+ years’ experience in data modeling and architecting, ETL, data engineering, or BI fields with concentration on data transformations and data modeling ', 'Assess database implementation procedures to ensure they follow internal and external regulations ', 'What Does the Role Entail?', 'Lead all aspects of the migration of data from legacy systems to new solutions ', 'Monitor the system performance by performing regular tests, fixing, and integrating new features ', 'Useful Skills And Experience', 'Recommend solutions to improve new and existing database systems ', 'Maintaining business partner engagement and setting expectations ', 'Ability to succeed in a dynamic, Agile environment ', 'Assessing current processes and recommending changes as needed ', 'Assess fitness-for-purpose of existing data model and processes Design conceptual and logical data models and flowcharts Design and implement effective database solutions and models to store and retrieve company data Development of reporting solutions to meet the operational and executive needs of the platform Examine and identify database structural necessities by evaluating client operations, applications, and programming Optimize new and current database systems Assess database implementation procedures to ensure they follow internal and external regulations Install and organize information systems to guarantee company functionality Prepare accurate database design and architecture reports for management and executive teams Lead all aspects of the migration of data from legacy systems to new solutions Monitor the system performance by performing regular tests, fixing, and integrating new features Recommend solutions to improve new and existing database systems ', 'Extensive hands-on experience working with SQL and Python for the purposes of data modeling and ETL ', 'Design conceptual and logical data models and flowcharts ', 'Understanding of various data extraction and transformation techniques ', ""What You'll Do"", 'Development of reporting solutions to meet the operational and executive needs of the platform ', 'Sequoia’s Culture – Our most important asset', 'Knowledge of medical, dental, LTD/STD/life insurance concepts and data Understanding of various data extraction and transformation techniques Working familiarity with Salesforce (SFDC) Knowledge of Mulesoft is a bonus Familiar with data visualization standard methodologies\u202f ', 'Soft Skills', 'SQL', 'Advanced working knowledge and ability to write complex SQL queries in MySQL, Snowflake, and Salesforce (SFDC) environments ', 'Compensation & Benefits', 'A champion of quality, able to QA and vouch for the integrity of the report output ', 'Translate business requirements into specifications that will be used to implement the required user-friendly environments, reports and dashboards, built from potentially multiple data sources ', 'Knowledge of medical, dental, LTD/STD/life insurance concepts and data ', 'Examine and identify database structural necessities by evaluating client operations, applications, and programming ', 'Prepare accurate database design and architecture reports for management and executive teams ', 'Optimize new and current database systems ', 'Participate in business analysis activities to gather required reporting and dashboard requirements ', 'Required Skills & Experience', 'Data Engineer ', 'Knowledge of Mulesoft is a bonus ', 'Dedication to team goals that include support of live 24/7 production systems ', 'Strong prioritization and time-management skills ', 'Familiar with data visualization standard methodologies\u202f ', 'Install and organize information systems to guarantee company functionality ', 'Advanced working knowledge and ability to write complex SQL queries in MySQL, Snowflake, and Salesforce (SFDC) environments Extensive hands-on experience working with SQL and Python for the purposes of data modeling and ETL Strong familiarity with Kimball, OLAP, and EDW data design methodologies, especially for healthcare and benefits datasets 3+ years’ experience in data modeling and architecting, ETL, data engineering, or BI fields with concentration on data transformations and data modeling ', 'Strong familiarity with Kimball, OLAP, and EDW data design methodologies, especially for healthcare and benefits datasets ', 'Documenting and communicating technical specifications to ensure that proper and optimized techniques, queries, data standards, and final outputs are understood and incorporated into data and analytics processes ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Serigor Inc,"Fort Washington, MD",3 weeks ago,26 applicants,"['Experience in ETL and ELT workflow management (Fivetran, DBT, Matillion, Snowflake Snowpipe, Stored Procedures, and Streams a plus).', 'Duration: Contract to hire', '7+ years’ experience as a Data Engineer.', 'Duration: ', 'Familiarity with AWS Data and Analytics technologies such as Glue, Athena, Spectrum, Data Pipeline.', 'Experience with integration with 3rd party application using Python connector api', '3+ years of experience with API based ELT automation framework, data manager, interface design, development, and maintenance.', 'Responsibilities:', 'Hands on experience with SQL Server 2016, SSIS, SSAS.', 'Strong knowledge of Big Data concepts and working with both structured and unstructured data.', 'Expert knowledge in Snowflake cloud-based data warehouse.', 'Location: ', 'Demonstrate capabilities to provide continuous improvements and optimizations to both cost and performance on cloud-based technologies.', 'Experience in migrations from SQL Server and Postgresql to Snowflake.', 'Job Title: Data Engineer', 'Perform ad hoc data analysis to meet business unit data validation needs.', '\xa0', 'Supporting the Data Warehouse strategy and Business Intelligence initiatives.', 'AWS cloud experience (EC2, S3, Lambda, SQS)', 'Job Title: ', '7+ years’ experience as a Data Engineer.5+ years of experience driving adoption and automation of data management services and tools.3+ years of experience with API based ELT automation framework, data manager, interface design, development, and maintenance.Expert knowledge in Snowflake cloud-based data warehouse.AWS cloud experience (EC2, S3, Lambda, SQS)Experience in ETL and ELT workflow management (Fivetran, DBT, Matillion, Snowflake Snowpipe, Stored Procedures, and Streams a plus).Demonstrate capabilities to provide continuous improvements and optimizations to both cost and performance on cloud-based technologies.Familiarity with AWS Data and Analytics technologies such as Glue, Athena, Spectrum, Data Pipeline.Experience with integration with 3rd party application using Python connector apiStrong knowledge of Big Data concepts and working with both structured and unstructured data.Experience in migrations from SQL Server and Postgresql to Snowflake.Experience in B2C, Martech, and CDP technologies and environments.Hands on experience with SQL Server 2016, SSIS, SSAS.Integrate/export data following security guidelines.Supporting the Data Warehouse strategy and Business Intelligence initiatives.Perform ad hoc data analysis to meet business unit data validation needs.Design and develop ETL processes and data structures for the Data Warehouse following best practice procedures.', 'Integrate/export data following security guidelines.', 'Design and develop ETL processes and data structures for the Data Warehouse following best practice procedures.', 'Location: Remote to start, eventually onsite in Fort Washington, PA', 'Experience in B2C, Martech, and CDP technologies and environments.', '5+ years of experience driving adoption and automation of data management services and tools.']",Entry level,Contract,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Vida Health,"San Francisco, CA",2 weeks ago,61 applicants,"['', 'Self-service BI tools such as Datastudio, Looker, Amplitude, and Tableau', 'ETL Tools such as DBT, Matillion, Snowplow, Fivetran, Stitch, and Talend', 'Our data strategy is Polychronic by Design, meaning we leverage population data insights to help members with multiple interrelated chronic conditions throughout their lifetime. ', 'Weekly meetups with team members across the country through our #connectandcommit program', 'We’re a distributed company, so you can work from most US states (We still have a HQ office in San Francisco)', 'Access to a Vida Health Coach and the full Vida App', 'Experience with cloud platforms such as AWS or Google CloudAdvanced knowledge of: Data Warehouses, SQL, Python, REST APIsTrack record of standardizing data for analysis on BI tools or delivering ML applicationsExperience managing enterprise data exchanges, Analytics ETL, or ML Ops.Has a relentless focus on delivering maximum value to the end userHas an ownership mindset and is excited about monitoring and alerting on their systemsHas at least 5+ years of relevant work experience in Data Engineering or similar roles', 'We seek to recruit, develop and retain the most talented people from a diverse candidate pool. We don’t just accept differences — we celebrate them, we support them, and we thrive on them for the benefit of our employees, our platform and those we serve. Vida is committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures.', 'Define and meet service-level agreements (SLAs) for data pipeline processes and ML powered APIs', 'We’re a distributed company, so you can work from most US states (We still have a HQ office in San Francisco)Access to a Vida Health Coach and the full Vida AppNew hire home office stipendMonthly wellness benefitTraining and leadership development programsWeekly meetups with team members across the country through our #connectandcommit programQuarterly All Company EventsQuarterly Team Based Connection OpportunitiesSignificant opportunities for growth and development as the business grows', 'Database tools such as BigQuery, Datastore, and Firestore', 'PERKS', 'Commuter and Parking Benefits ', 'Medical, Dental, Vision, Disability and Life Insurance (We cover 100% of your premium and 80% for your dependents)', 'Training and leadership development programs', 'Design the foundational layer of Vida Health’s data environment to make data standardized and reusableManage data pipelines that collect and transform data from multiple sources to support ML models, analysis, and reportingManage services that make results from ML models and data analysis accessible to other services and app featuresDesign and build data pipelines to replace manual tasks such as data cleaning and ingestion of data from new sourcesWork with Data Analysts and Data Scientists to design data architectures that will improve the productivity and velocity of both teams’ workflowsMonitor cost of data pipelines and data products and continuously search for ways to reduce the cost of those systemsMonitor the status of data pipelines and data products and promptly inform relevant teams of errors and outagesDefine and meet service-level agreements (SLAs) for data pipeline processes and ML powered APIs', 'Has a relentless focus on delivering maximum value to the end user', 'Flexible PTO Policy', 'Experience with cloud platforms such as AWS or Google Cloud', 'Asynchronous systems such as Kafka, RabbitMQ, PubSub, and Kinesis', 'Quarterly All Company Events', 'HIPAA and healthcare data such as Medical and RX claims', 'Has an ownership mindset and is excited about monitoring and alerting on their systems', '10 Paid Company Holidays', 'Quarterly Team Based Connection Opportunities', 'Bonus Skills', 'Database tools such as BigQuery, Datastore, and FirestoreAsynchronous systems such as Kafka, RabbitMQ, PubSub, and KinesisWorkflow orchestration services such as Apache AirflowETL Tools such as DBT, Matillion, Snowplow, Fivetran, Stitch, and TalendSelf-service BI tools such as Datastudio, Looker, Amplitude, and TableauHIPAA and healthcare data such as Medical and RX claimsManagement and/or team lead experience preferred.', 'Experience managing enterprise data exchanges, Analytics ETL, or ML Ops.', 'About The Job', 'Work with Data Analysts and Data Scientists to design data architectures that will improve the productivity and velocity of both teams’ workflows', 'Track record of standardizing data for analysis on BI tools or delivering ML applications', 'Workflow orchestration services such as Apache Airflow', 'Healthcare FSA Plan', 'Advanced knowledge of: Data Warehouses, SQL, Python, REST APIs', 'About Us', 'Monitor cost of data pipelines and data products and continuously search for ways to reduce the cost of those systems', 'Competitive compensation with meaningful stock optionsMedical, Dental, Vision, Disability and Life Insurance (We cover 100% of your premium and 80% for your dependents)Healthcare FSA PlanDependent Care FSA PlansCommuter and Parking Benefits 401K ProgramFlexible PTO PolicyPaid Parental Leave10 Paid Company Holidays', 'Design and build data pipelines to replace manual tasks such as data cleaning and ingestion of data from new sources', 'Responsibilities', 'Monthly wellness benefit', 'Design the foundational layer of Vida Health’s data environment to make data standardized and reusable', 'Management and/or team lead experience preferred.', 'Has at least 5+ years of relevant work experience in Data Engineering or similar roles', 'Qualifications', 'Manage data pipelines that collect and transform data from multiple sources to support ML models, analysis, and reporting', 'Vida is proud to be an Equal Employment Opportunity and Affirmative Action employer.', 'Monitor the status of data pipelines and data products and promptly inform relevant teams of errors and outages', 'Benefits', 'Significant opportunities for growth and development as the business grows', 'Manage services that make results from ML models and data analysis accessible to other services and app features', 'Dependent Care FSA Plans', '401K Program', 'New hire home office stipend', 'Paid Parental Leave', 'Vida is authorized to do business in many, but not all, states. If you are not located in or able to work from a state where Vida is registered, you will not be eligible for employment. Please speak with your recruiter to learn more about where Vida is registered.About The JobWe are looking for a Data Engineer who can help us build, manage, and optimize Vida Health’s data pipelines within our Google Cloud Platform (GCP) environment. You would be the first Data Engineer at Vida, which is an important role because we are a very data-driven company. From measuring Vida’s impact on health care costs to correlating member activities with successful health outcomes, you’ll have a substantial impact on projects whose results can lead to changes in product design and member experience. Our data strategy is Polychronic by Design, meaning we leverage population data insights to help members with multiple interrelated chronic conditions throughout their lifetime. Because Vida is a startup, you’ll have the opportunity to work on a diverse set of projects that involve multiple parts of our data infrastructure. One of your responsibilities will be to connect all our sources of data such as claims data and app data, while making the data structured and accessible for analysis and other ETL pipelines. You’ll also work on optimizing our ETL pipelines, including our ML pipelines which deliver recommendations and predictions that influence the member and health provider’s experience on the platform.You’ll have the pleasure of working with a team who is excited about providing care to people living with chronic conditions. We also have experts in health care who are an invaluable resource for learning more about the health domain. We hope that you’ll consider embarking on this journey into polychronic health care with us.ResponsibilitiesDesign the foundational layer of Vida Health’s data environment to make data standardized and reusableManage data pipelines that collect and transform data from multiple sources to support ML models, analysis, and reportingManage services that make results from ML models and data analysis accessible to other services and app featuresDesign and build data pipelines to replace manual tasks such as data cleaning and ingestion of data from new sourcesWork with Data Analysts and Data Scientists to design data architectures that will improve the productivity and velocity of both teams’ workflowsMonitor cost of data pipelines and data products and continuously search for ways to reduce the cost of those systemsMonitor the status of data pipelines and data products and promptly inform relevant teams of errors and outagesDefine and meet service-level agreements (SLAs) for data pipeline processes and ML powered APIsQualificationsExperience with cloud platforms such as AWS or Google CloudAdvanced knowledge of: Data Warehouses, SQL, Python, REST APIsTrack record of standardizing data for analysis on BI tools or delivering ML applicationsExperience managing enterprise data exchanges, Analytics ETL, or ML Ops.Has a relentless focus on delivering maximum value to the end userHas an ownership mindset and is excited about monitoring and alerting on their systemsHas at least 5+ years of relevant work experience in Data Engineering or similar rolesBonus SkillsDatabase tools such as BigQuery, Datastore, and FirestoreAsynchronous systems such as Kafka, RabbitMQ, PubSub, and KinesisWorkflow orchestration services such as Apache AirflowETL Tools such as DBT, Matillion, Snowplow, Fivetran, Stitch, and TalendSelf-service BI tools such as Datastudio, Looker, Amplitude, and TableauHIPAA and healthcare data such as Medical and RX claimsManagement and/or team lead experience preferred.BenefitsCompetitive compensation with meaningful stock optionsMedical, Dental, Vision, Disability and Life Insurance (We cover 100% of your premium and 80% for your dependents)Healthcare FSA PlanDependent Care FSA PlansCommuter and Parking Benefits 401K ProgramFlexible PTO PolicyPaid Parental Leave10 Paid Company HolidaysPERKSWe’re a distributed company, so you can work from most US states (We still have a HQ office in San Francisco)Access to a Vida Health Coach and the full Vida AppNew hire home office stipendMonthly wellness benefitTraining and leadership development programsWeekly meetups with team members across the country through our #connectandcommit programQuarterly All Company EventsQuarterly Team Based Connection OpportunitiesSignificant opportunities for growth and development as the business growsVida is proud to be an Equal Employment Opportunity and Affirmative Action employer.Diversity is more than a commitment at Vida—it is the foundation of what we do. All qualified applicants will receive consideration for employment without regard to race, color, ancestry, religion, gender, gender identity or expression, sexual orientation, marital status, national origin, genetics, disability, age, or Veteran status. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law.We seek to recruit, develop and retain the most talented people from a diverse candidate pool. We don’t just accept differences — we celebrate them, we support them, and we thrive on them for the benefit of our employees, our platform and those we serve. Vida is committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures.', 'Competitive compensation with meaningful stock options', 'Diversity is more than a commitment at Vida—it is the foundation of what we do. All qualified applicants will receive consideration for employment without regard to race, color, ancestry, religion, gender, gender identity or expression, sexual orientation, marital status, national origin, genetics, disability, age, or Veteran status. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Ocher Technology Group,"Atlanta, GA",1 week ago,Be among the first 25 applicants,[],Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Global Technical Talent,"Plano, TX",1 day ago,Be among the first 25 applicants,"['Job Description', 'Azure: DevOps, pipeline, blob storage, Containers, Key Vault', 'No exp with Snowflake is fine as long as SQL knowledge is good.', 'Restful API knowledge is good.', 'Candidate should be very good with SQL (joins, grouping, aggregation, windowing, CTE)', 'Remote India', 'Data Engineer', 'Able to implement Airflow as Azure pipelines are bottleneck .. so any working knowledge on implementing Airflow on Azure will be good', 'Data Engr offshore 3 C#, Python, Data Pipelines, Strong SQL skills', 'Remote IndiaC# is the main programming language (mainly .net core). Restful API knowledge is good.Candidate should be very good with SQL (joins, grouping, aggregation, windowing, CTE)DBT is needed and knowledge of SCD will be helpful.No exp with Snowflake is fine as long as SQL knowledge is good.Azure: DevOps, pipeline, blob storage, Containers, Key VaultAble to implement Airflow as Azure pipelines are bottleneck .. so any working knowledge on implementing Airflow on Azure will be goodPython with\xa0Data\xa0processing skills using\xa0Pandas, NumPy libraryOther areas: Gitflow, Agile, SFTP, PowerShell scripting, MS office, Excel (formula), Good presentation and demo skillsData Engr offshore 3 C#, Python, Data Pipelines, Strong SQL skills', 'Python with\xa0Data\xa0processing skills using\xa0Pandas, NumPy library', 'C# is the main programming language (mainly .net core). ', 'Other areas: Gitflow, Agile, SFTP, PowerShell scripting, MS office, Excel (formula), Good presentation and demo skills', 'DBT is needed and knowledge of SCD will be helpful.', '\xa0Job Description']",Entry level,Contract,Information Technology,Insurance,2021-03-24 13:05:10
Lead Data Engineer,The Knot Worldwide,"Washington, DC",2 weeks ago,32 applicants,"['', 'ABOUT THE ROLE AND OUR TEAM:', 'Document built processes and data content and publish needed information to our data dictionary', 'SUCCESSFUL LEAD DATA ENGINEERS HAVE:', 'A Bachelor’s Degree in Computer Science, Engineering, Information Systems, or a related field is preferred. An advanced degree or professional licensing is a plus.', 'Experience developing advanced data pipelines and streaming channels', 'Experience with Big Data Solutions such as Snowflake, Redshift, Google BigQuery and or Hadoop', 'Experience with at least one Business Intelligence tool such as Qlik and or Metabase', 'You Love Our Users. You keep our global community at the center of everything you do.', 'DESIRED SKILLS/EXPERIENCE:', 'Analyze, propose and seek to establish data permissions and profile segmentation aligning to standards such as Personas', 'Proven and demonstrated ability to work under pressure', 'Experience with advanced Ralph Kimball dimensional modelling architecture', 'WHAT WE DO MATTERS:', 'Support project efforts that seek to reduce technical debt, example: (Multiple data stores that are redundant)', 'Ability to troubleshoot complex integration issues', 'Experience with ETL / ELT platforms such as Matillion, Dell Boomi, Luigi, Informatica, Talend, and or SSIS', 'Participate in maintaining quality and governance standards ', 'Programming experience with Ruby, Python, Scala, Shell Scripting', ' You Dream Big. You iterate and experiment to drive innovation. You Love Our Users. You keep our global community at the center of everything you do. You Do the Right Thing. You strengthen your team through respect, fairness, and inclusion. You Hustle Every Day. You favor urgency and own your outcomes.  You Win Together. People are at the heart of our success and you play as a team. ', 'Excellent communication skills, both verbal and written', 'Minimum of 7 years experience with SQL and advanced relational, operational, object-oriented and data warehouse development', 'Introduce a consistent change management process that aligns with our standards; examples are: new field, object and data creation', 'Conduct research and make recommendations on data requirements, products and data services', 'Collaborate with Data Engineering leadership on data standardization and integration best practices', 'Present ideas, expectations and information in a concise well-organized way', 'Working knowledge of data movement, monitoring and management technologies ', 'Demonstrate self-confidence, energy and enthusiasm', 'Ability to ramp up quickly and fully exploit platform advanced feature sets', 'Manage time well, whilst correctly prioritizing tasks', 'Resolve Ad-hoc data questions from the organization in a timely and accurate manner', ' A Bachelor’s Degree in Computer Science, Engineering, Information Systems, or a related field is preferred. An advanced degree or professional licensing is a plus. Programming experience and a demonstrated interest in data trends, data analysis, quality, movement, measurement, profiling, reporting and analytics Proven and demonstrated ability to work under pressure Working knowledge of data movement, monitoring and management technologies  Minimum of 7 years experience with SQL and advanced relational, operational, object-oriented and data warehouse development Hands-on experience with at least one of the following databases (MYSQL, PostgreSQL, Snowflake, MSSQL, Redshift, MongoDB, Couchbase)  Minimum of 5 years experience actively working with AWS services (S3, Lambda, SNS, EC2, Load Balancers, Routing tables, EMR, VDI / PCI networking) Excellent communication skills, both verbal and written Demonstrate self-confidence, energy and enthusiasm Present ideas, expectations and information in a concise well-organized way Conduct research and make recommendations on data requirements, products and data services Demonstrated ability to act in a lead role Ability to ramp up in a short time on new technologies Ability to troubleshoot complex integration issues Manage time well, whilst correctly prioritizing tasks ', 'Ability to ramp up in a short time on new technologies', 'You Win Together. People are at the heart of our success and you play as a team.', 'Extract, load and transform data from The Knot Worldwide and other sources to our DataWarehouse for the consumption of our end users.', 'Architect and develop advanced solutions for integrating between disparate systems which include cloud-based sources, API and AWS Services', 'You Dream Big. You iterate and experiment to drive innovation.', 'You Do the Right Thing. You strengthen your team through respect, fairness, and inclusion.', 'Continuous improvement on current ETL processes to ensure accuracy, timeliness and scalability as data volumes grow.', 'WHAT WE LOVE ABOUT YOU:', 'Programming experience and a demonstrated interest in data trends, data analysis, quality, movement, measurement, profiling, reporting and analytics', ' Extract, load and transform data from The Knot Worldwide and other sources to our DataWarehouse for the consumption of our end users. Architect and develop advanced solutions for integrating between disparate systems which include cloud-based sources, API and AWS Services Maintaining current ETL processes and resolving daily ETL job failures as they arise Resolve Ad-hoc data questions from the organization in a timely and accurate manner Continuous improvement on current ETL processes to ensure accuracy, timeliness and scalability as data volumes grow. Identify through documented or current issues, data structural problems that increase inconsistencies and reduce quality and performance within the enterprise set of systems and the related data stores  Bring awareness and communicate standards across the data landscape Introduce a consistent change management process that aligns with our standards; examples are: new field, object and data creation Support project efforts that seek to reduce technical debt, example: (Multiple data stores that are redundant) Analyze, propose and seek to establish data permissions and profile segmentation aligning to standards such as Personas Document built processes and data content and publish needed information to our data dictionary Provide guidance to team members with respect to best practices Participate in maintaining quality and governance standards  Collaborate with Data Engineering leadership on data standardization and integration best practices ', 'Maintaining current ETL processes and resolving daily ETL job failures as they arise', 'RESPONSIBILITIES: ', 'Bring awareness and communicate standards across the data landscape', 'Hands-on experience with at least one of the following databases (MYSQL, PostgreSQL, Snowflake, MSSQL, Redshift, MongoDB, Couchbase) ', 'Provide guidance to team members with respect to best practices', 'You Hustle Every Day. You favor urgency and own your outcomes. ', 'Minimum of 5 years experience actively working with AWS services (S3, Lambda, SNS, EC2, Load Balancers, Routing tables, EMR, VDI / PCI networking)', 'Demonstrated ability to act in a lead role', 'Experience in database OLTP schema design and architecture models', 'WHAT YOU LOVE ABOUT US:', ' Experience with advanced Ralph Kimball dimensional modelling architecture Experience in database OLTP schema design and architecture models Experience developing advanced data pipelines and streaming channels Experience with at least one Business Intelligence tool such as Qlik and or Metabase Experience with Big Data Solutions such as Snowflake, Redshift, Google BigQuery and or Hadoop Experience with ETL / ELT platforms such as Matillion, Dell Boomi, Luigi, Informatica, Talend, and or SSIS Programming experience with Ruby, Python, Scala, Shell Scripting Ability to ramp up quickly and fully exploit platform advanced feature sets ', 'Identify through documented or current issues, data structural problems that increase inconsistencies and reduce quality and performance within the enterprise set of systems and the related data stores ']",Associate,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Solutions Engineer,Epsilon,"Chicago, IL",3 weeks ago,69 applicants,"['', 'Bachelor’s Degree in Computer Science or equivalent degree is required.', 'Airflow', 'Ability to handle complex products', 'Strong understanding of Disaster Recovery and Business Continuity solutions', 'Experience with Python', 'Ideal candidate can lead in the areas of solution design, code development, quality assurance, data modeling, business intelligence, cross team communication, and application maintenance.', 'Bachelor’s Degree in Computer Science or equivalent degree is required.5 – 10 years of business analysis experience around database marketing technologies and data management, and technical understanding in these areas.Strong experience in Basic and Advanced SQL writing and tuning .Experience with PythonStrong understanding of Disaster Recovery and Business Continuity solutionsExperience with scheduling applications with complex interdependencies, preferably AirflowGood experience in working with geographically and culturally diverse teamsFamiliarity with complex data lake environments that span OLTP, MPP and Hadoop platformsExcellent written and verbal communication skills.Ability to handle complex productsExcellent Analytical and problem-solving skillsAbility to diagnose and troubleshoot problems quickly', 'What You’ll Need', 'Strong experience in Basic and Advanced SQL writing and tuning .', 'Ability to diagnose and troubleshoot problems quickly', 'Excellent Analytical and problem-solving skills', 'Great People, Deserve Great Benefits', 'Familiarity with complex data lake environments that span OLTP, MPP and Hadoop platforms', 'Good experience in working with geographically and culturally diverse teams', '5 – 10 years of business analysis experience around database marketing technologies and data management, and technical understanding in these areas.', 'What You’ll Do', ' Lead, design and code solutions on and off database for ensuring application access to enable data driven decision making for the company’s multi-faceted ad serving operations.Working closely with Engineering resources across the globe to ensure enterprise data warehouse solutions and assets are actionable, accessible and evolving in lockstep with the needs of the ever-changing business model.Ideal candidate can lead in the areas of solution design, code development, quality assurance, data modeling, business intelligence, cross team communication, and application maintenance.', 'Working closely with Engineering resources across the globe to ensure enterprise data warehouse solutions and assets are actionable, accessible and evolving in lockstep with the needs of the ever-changing business model.', 'Experience with scheduling applications with complex interdependencies, preferably Airflow', 'Company Description', 'Excellent written and verbal communication skills.', ' Lead, design and code solutions on and off database for ensuring application access to enable data driven decision making for the company’s multi-faceted ad serving operations.', 'Job Description']",Not Applicable,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Senior Data Engineer,EvolutionIQ,"New York, NY",1 day ago,Be among the first 25 applicants,"['', ' Open to giving and receiving critical feedback', ' Other ', ' Setup a reliable continuous model deployment infrastructure', ' Expert in Python 3 and Pandas or equivalent data manipulation library', ' Designed and Implemented ML platform used for building and shipping multiple products Owned, deployed and maintained ETL pipelines in production Experience in deploying & maintaining distributed learning frameworks (TensorFlow preferred) Extensive experience in deploying systems on GCP or AWS Expert developer who uses writes clean, efficient, easy to understand code with unit tests, functional design patterns Expert in Python 3 and Pandas or equivalent data manipulation library Expert data architect eager to solve complex, multi dimensional big data problems with advanced strategic leverage of databases, data assets and DAG pipeline Excellent document writing skills Extreme creativity and resourcefulness, appetite to solve previously unsolved problems', ' Work with Data Scientists to map ML workflows to a scalable platform', ' Refactor core algorithms for improved efficiency, readability and test coverage', ' Holds self to extremely high standards, and inspires others to do the same', ' Expert developer who uses writes clean, efficient, easy to understand code with unit tests, functional design patterns', ' Enthusiasm for team work and pair work', ' Work with Data Scientists to map ML workflows to a scalable platform Setup a reliable continuous model deployment infrastructure Setup centralized data storage to support ease of data access and EDA Setup reliable and secure client data ingestion Refactor core algorithms for improved efficiency, readability and test coverage', ' Extreme creativity and resourcefulness, appetite to solve previously unsolved problems', ' Setup centralized data storage to support ease of data access and EDA', ' Extreme self-starter and self-motivator Holds self to extremely high standards, and inspires others to do the same Open to giving and receiving critical feedback Able to handle the ups and downs of early startup life Enthusiasm for team work and pair work Kind, empathetic, polite and professional', ' Experience in deploying & maintaining distributed learning frameworks (TensorFlow preferred)', ' Expert data architect eager to solve complex, multi dimensional big data problems with advanced strategic leverage of databases, data assets and DAG pipeline', ' Owned, deployed and maintained ETL pipelines in production', ' Able to handle the ups and downs of early startup life', ' Setup reliable and secure client data ingestion', ' Extensive experience in deploying systems on GCP or AWS', ' Designed and Implemented ML platform used for building and shipping multiple products', ' Extreme self-starter and self-motivator', ' Kind, empathetic, polite and professional', ' Excellent document writing skills']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Entry Level Data Engineer,"KGS Technology Group, Inc","Foster City, CA",4 days ago,Be among the first 25 applicants,"['', ' experience in building data pipelines in cloud (Azure) and big data environment', ' PL/SQL, Python.', ' be responsible for designing, building and supporting components and its pipelines.', ' Has basic knowledge with big data platforms like Hadoop, Hive, or Phoenix, as well as knowledge in parallel programming, and distributed computing frameworks like Spark.', ' Candidate having Analytical skills with core experience', ' Data Warehousing, Azure Candidate having Analytical skills with core experience PL/SQL, Python. Bi Tools Power BI, Business Objects Designs, develops, tests and delivers offerings for Cognitive Systems using the latest technologies. Works in an Agile, collaborative environment to understand stakeholder requirements. Designs, codes, and tests innovative component-level solutions in areas such as base operating systems, machine learning, computational linguistics, and Natural Language Processing (NLP), advanced and semantic information search, extraction, induction, classification and exploration. Ensures that the implemented solutions are unit tested and ready to be integrated into their product. be responsible for designing, building and supporting components and its pipelines. work in a collaborative environment with other data engineers, data scientists, and software engineers to achieve important goals. experience in building data pipelines in cloud (Azure) and big data environment worked on data lake, distributed db and nosql databases Has basic knowledge with big data platforms like Hadoop, Hive, or Phoenix, as well as knowledge in parallel programming, and distributed computing frameworks like Spark. Research and implement cutting edge solutions to solve challenges related to ETL/ELT, data processing, and analytics.', ' Designs, codes, and tests innovative component-level solutions in areas such as base operating systems, machine learning, computational linguistics, and Natural Language Processing (NLP), advanced and semantic information search, extraction, induction, classification and exploration.', ' Ensures that the implemented solutions are unit tested and ready to be integrated into their product.', ' Research and implement cutting edge solutions to solve challenges related to ETL/ELT, data processing, and analytics.', ' Bi Tools Power BI, Business Objects', ' work in a collaborative environment with other data engineers, data scientists, and software engineers to achieve important goals.', ' Designs, develops, tests and delivers offerings for Cognitive Systems using the latest technologies.', ' Data Warehousing, Azure', 'Job Description', ' Works in an Agile, collaborative environment to understand stakeholder requirements.', ' worked on data lake, distributed db and nosql databases']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Senior Data Engineer,WaFd Bank,"Seattle, WA",20 hours ago,Be among the first 25 applicants,"['', 'Tracking data consumption patterns.', 'Experience in working with large, heterogeneous datasets in building and optimizing data pipelines, pipeline architectures and integrated datasets using traditional data integration technologies. These should include ETL/ELT, data replication/CDC, and data virtualization.', 'Strong experience with advanced analytics tools for Object-oriented/object function scripting using languages such as R, Python, Java, others.', 'Experience in working with data governance/data quality and data security teams and specifically data stewards and security officers in moving data pipelines into production with appropriate data quality, governance and security standards and certification.', 'Learning and using modern data preparation, integration and AI-enabled metadata management tools and techniques.', 'Strong experience in profiling source data with little or no documentation and documenting data quality', 'Educate and train: The data engineer should be curious and knowledgeable about new data initiatives and how to address them. This includes applying their data and/or domain understanding in addressing new data requirements. They will also be responsible for proposing appropriate and innovative data ingestion, preparation, integration and operationalization techniques in optimally addressing these data requirements. The data engineer will be required to train counterparts such as data analysts, business users or any data consumers in these data pipelining and preparation techniques, which make it easier for them to integrate and consume the data they need for their own use cases.', 'Demonstrated success in working with large, heterogeneous datasets to extract business value using tools such as AWS Data Pipeline, Airflow or Segment to reduce or even automate parts of the tedious data preparation tasks.', 'Performing intelligent sampling and caching.', 'At least six years or more of work experience in data management disciplines including data integration, modeling, optimization and data quality, and/or other areas directly relevant to data engineering responsibilities and tasks.At least three years of experience working in cross-functional teams and collaborating with business stakeholders in support of a departmental and/or multi-departmental data management and analytics initiative.', 'Job Summary', 'Become a data and analytics evangelist: The data engineer will be considered a blend of data and analytics ""evangelist,"" ""data guru"" and ""fixer."" This role will promote the available data and analytics capabilities and expertise to business unit leaders and educate them in leveraging these capabilities in achieving their business goals', 'Experience on NoSQL/Hadoop oriented databases like MongoDB, Cassandra, others for nonrelational databases are a plus.', 'At least three years of experience working in cross-functional teams and collaborating with business stakeholders in support of a departmental and/or multi-departmental data management and analytics initiative.', 'Requirements', 'Basic experience working with popular data discovery, analytics and BI software tools like Tableau, Qlik, PowerBI and others for semantic-layer-based data discovery.', 'Learning and using modern data preparation, integration and AI-enabled metadata management tools and techniques.Tracking data consumption patterns.Performing intelligent sampling and caching.Monitoring schema changes.Recommending -- or sometimes even automating -- existing and future integration flows.', 'Previous Experience', 'Strong experience in working with Oracle packages and data pump, ability to troubleshoot and reverse-engineer exiting processes, fix and improve data logic and query performance.', 'Strong experience in working with data science teams in refining and optimizing data science and machine learning models and algorithms.', 'Strong experience with database programming languages including SQL, PL/SQL, others for relational databases and certifications.', 'Strong experience in creating, documenting and improving data models from business specification', 'Strong ability to design, build and manage data pipelines for data structures encompassing data transformation, data models, schemas, metadata and workload management. The ability to work with both IT and business in integrating analytics and data science output into business processes and workflows.', 'Weekly Hours: 40', 'Responsibilities', 'Collaborate across departments: The newly hired data engineer will need strong collaboration skills in order to work with varied stakeholders within the organization. In particular, the data engineer will work in close relationship with the data architect and with business (data) analysts in refining their data requirements for various data and analytics initiatives and their data consumption requirements.', 'The ideal candidate will have a combination of IT skills, data governance skills, analytics skills and Banking knowledge with a technical or computer science degree.', 'Recommending -- or sometimes even automating -- existing and future integration flows.', 'Build data pipelines: Manage data pipelines consisting of a series of stages through which data flows (for example, from data sources or endpoints of acquisition to integration to consumption for specific use cases). These data pipelines have to be created, maintained and optimized as workloads move from development to production for specific use cases. Architecting, creating and maintaining data pipelines will be the primary responsibility of the data engineer.', 'Education and Training', 'Adept in agile methodologies and capable of applying DevOps and increasingly DataOps principles to data pipelines to improve the communication, integration, reuse and automation of data flows between data managers and consumers across an organization', 'Participate in ensuring compliance and governance during data use: Data engineers should work with data governance team and data stewards within this team and participate in vetting and promoting content created in the business and by data architects to the curated data catalog for governed reuse.', 'At least six years or more of work experience in data management disciplines including data integration, modeling, optimization and data quality, and/or other areas directly relevant to data engineering responsibilities and tasks.', ""A bachelor's or master's degree in computer science, statistics, applied mathematics, data management, information systems, information science or a related quantitative field or equivalent work experience is required."", ""A bachelor's or master's degree in computer science, statistics, applied mathematics, data management, information systems, information science or a related quantitative field or equivalent work experience is required.The ideal candidate will have a combination of IT skills, data governance skills, analytics skills and Banking knowledge with a technical or computer science degree."", 'Weekly Hours', 'Strong experience with advanced analytics tools for Object-oriented/object function scripting using languages such as R, Python, Java, others.Strong ability to design, build and manage data pipelines for data structures encompassing data transformation, data models, schemas, metadata and workload management. The ability to work with both IT and business in integrating analytics and data science output into business processes and workflows.Strong experience with database programming languages including SQL, PL/SQL, others for relational databases and certifications.Experience on NoSQL/Hadoop oriented databases like MongoDB, Cassandra, others for nonrelational databases are a plus.Experience in working with large, heterogeneous datasets in building and optimizing data pipelines, pipeline architectures and integrated datasets using traditional data integration technologies. These should include ETL/ELT, data replication/CDC, and data virtualization.Strong experience in working with Oracle packages and data pump, ability to troubleshoot and reverse-engineer exiting processes, fix and improve data logic and query performance.Strong experience in creating, documenting and improving data models from business specificationStrong experience in profiling source data with little or no documentation and documenting data qualityBasic experience working with popular data discovery, analytics and BI software tools like Tableau, Qlik, PowerBI and others for semantic-layer-based data discovery.Strong experience in working with data science teams in refining and optimizing data science and machine learning models and algorithms.Demonstrated success in working with large, heterogeneous datasets to extract business value using tools such as AWS Data Pipeline, Airflow or Segment to reduce or even automate parts of the tedious data preparation tasks.Experience in working with data governance/data quality and data security teams and specifically data stewards and security officers in moving data pipelines into production with appropriate data quality, governance and security standards and certification.Adept in agile methodologies and capable of applying DevOps and increasingly DataOps principles to data pipelines to improve the communication, integration, reuse and automation of data flows between data managers and consumers across an organization', 'Technical And Business Knowledge/Skills', 'Job Description', 'Drive Automation through effective metadata management: The data engineer will be responsible for using innovative and modern tools, techniques and architectures to partially or completely automate the most-common, repeatable and tedious data preparation and integration tasks in order to minimize manual and error-prone processes and improve productivity. The data engineer will also need to assist with renovating the data management infrastructure to drive automation in data integration and management.Learning and using modern data preparation, integration and AI-enabled metadata management tools and techniques.Tracking data consumption patterns.Performing intelligent sampling and caching.Monitoring schema changes.Recommending -- or sometimes even automating -- existing and future integration flows.', 'Monitoring schema changes.', 'Build data pipelines: Manage data pipelines consisting of a series of stages through which data flows (for example, from data sources or endpoints of acquisition to integration to consumption for specific use cases). These data pipelines have to be created, maintained and optimized as workloads move from development to production for specific use cases. Architecting, creating and maintaining data pipelines will be the primary responsibility of the data engineer.Drive Automation through effective metadata management: The data engineer will be responsible for using innovative and modern tools, techniques and architectures to partially or completely automate the most-common, repeatable and tedious data preparation and integration tasks in order to minimize manual and error-prone processes and improve productivity. The data engineer will also need to assist with renovating the data management infrastructure to drive automation in data integration and management.Learning and using modern data preparation, integration and AI-enabled metadata management tools and techniques.Tracking data consumption patterns.Performing intelligent sampling and caching.Monitoring schema changes.Recommending -- or sometimes even automating -- existing and future integration flows.Collaborate across departments: The newly hired data engineer will need strong collaboration skills in order to work with varied stakeholders within the organization. In particular, the data engineer will work in close relationship with the data architect and with business (data) analysts in refining their data requirements for various data and analytics initiatives and their data consumption requirements.Educate and train: The data engineer should be curious and knowledgeable about new data initiatives and how to address them. This includes applying their data and/or domain understanding in addressing new data requirements. They will also be responsible for proposing appropriate and innovative data ingestion, preparation, integration and operationalization techniques in optimally addressing these data requirements. The data engineer will be required to train counterparts such as data analysts, business users or any data consumers in these data pipelining and preparation techniques, which make it easier for them to integrate and consume the data they need for their own use cases.Participate in ensuring compliance and governance during data use: Data engineers should work with data governance team and data stewards within this team and participate in vetting and promoting content created in the business and by data architects to the curated data catalog for governed reuse.Become a data and analytics evangelist: The data engineer will be considered a blend of data and analytics ""evangelist,"" ""data guru"" and ""fixer."" This role will promote the available data and analytics capabilities and expertise to business unit leaders and educate them in leveraging these capabilities in achieving their business goals']",Associate,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Data Engineer,AAA-The Auto Club Group,"Minneapolis, MN",1 week ago,Be among the first 25 applicants,"['', 'Insurance or Banking industry backgroundMaster’s degree in Mathematics, Computer Science, Statistics, or related quantitative field\xa0Experience:5+ years of experience in an analytics or business intelligence setting, including experience using one or more of the following analytic tools: SAD, SQL, R, Python, SPSSWorking with Internal Audit or on an audit data analytics teamDashboard design and development; building intuitive interfaces, infographics, and visualization to tell stories with dataAbility to complete advanced statistical analysisProactively monitoring daily processes, performance strategies,and results to ensure consistent coverage over business strategies and goalsFamiliarity with agile development methodologiesExperience with IBM Cognos TM1 or PeopleSoft', 'Offers recommendations for new data analytic techniques and methodologies.', ""Bachelor's degree in Mathematics, Computer Science, Statistics, or related quantitative field\xa0"", 'Important Note: The above statements describe the principal and essential functions, but not all functions that may be inherent in the job.\xa0This job requires the ability to perform duties contained in the job description for this position, including, but not limited to, the above requirements.\xa0Reasonable accommodations will be made for otherwise qualified applicants, as needed, to enable them to fulfill these requirements.', 'Developing positive relationships with IT and business intelligence departments and personnel', 'Working with PC software applications (e.g., Word, Excel, Visio)', 'Interpreting and understanding business strategies and goals to find emerging issues and root-causes for anomalies', 'Dashboard design and development; building intuitive interfaces, infographics, and visualization to tell stories with data', 'Understanding of database structures, theories, principles, and practices;', 'Master’s degree in Mathematics, Computer Science, Statistics, or related quantitative field', '\xa0\xa0', 'Designing and delivering reports, visualizations, or presentations that are understandable for both technical and non-technical audiences, and explain the importance and relevance of results to business requirements;', 'Familiarity with agile development methodologies', 'Extensive experience in:', 'Experience with IBM Cognos TM1 or PeopleSoft', 'Knowledge and Skills:', 'Complex data analysis', 'Proactively monitoring daily processes, performance strategies,and results to ensure consistent coverage over business strategies and goals', 'Experience:', 'Work in a temperature-controlled office environment.\xa0Occasional (less than 10% of work time) travel required.', 'Working within an Internal Audit department, analyzes large, diverse data sets to drive better business decisions and to identify business problems, in the context of financial, regulatory, reputational, and operational risk.', '(21000065)', 'Requirements gathering and documentation;', 'Solid understanding of data capture, data mapping, and data cleansing', 'Work Environment:', '\xa0', 'Applying analytic skills to solve problems creatively; Ability to investigate and determine root causes of issues; Strong analytical problem solving, requirements facilitation, and collaboration skills; Consistently looks for better ways to achieve business results; recognizes problems as opportunities for process improvement', 'Description', 'Primary Duties and Responsibilities:', 'The Auto Club Group (ACG) provides membership, travel, insurance and financial services offerings to approximately 9 million members and customers across 11 states and 2 U.S. territories through the AAA, Meemic and Fremont brands. ACG belongs to the national AAA federation and is the second largest AAA club in North America.\xa0', 'Extracting and manipulating large data sets in Oracle databases and mainframe sequential files for analysis, including integration of diverse data sources', 'Troubleshooting data issues; data reconciliation experience with consistent attention to detail', 'Data Analytics Lead - Audit', 'Conducts research and special studies when needed; Performs ad hoc analysis and provides meaningful reporting to operations and the Internal Audit leadership team.', 'Qualifications', 'The Auto Club Group offers a competitive compensation and benefits package including a base salary with performance based incentives; medical/dental/vision insurance, 401(k), generous time off, a complimentary AAA Membership and much more!', 'Extracting and manipulating large data sets in Oracle databases and mainframe sequential files for analysis, including integration of diverse data sourcesInterpreting and understanding business strategies and goals to find emerging issues and root-causes for anomaliesComplex data analysisRequirements gathering and documentation;Troubleshooting data issues; data reconciliation experience with consistent attention to detailDesigning and delivering reports, visualizations, or presentations that are understandable for both technical and non-technical audiences, and explain the importance and relevance of results to business requirements;Translating high level business goals into the tasks and technical specifications needed to perform meaningful data analysis;Developing positive relationships with IT and business intelligence departments and personnelWorking with PC software applications (e.g., Word, Excel, Visio)\xa0\xa0Knowledge and Skills:Extensive knowledge and skills:Mathematical/statistical, reasoning, and logic skills; Understanding of statistical concepts (e.g. hypothesis testing, regression analysis)Data design principlesUnderstanding of database structures, theories, principles, and practices;Solid understanding of data capture, data mapping, and data cleansingApplying analytic skills to solve problems creatively; Ability to investigate and determine root causes of issues; Strong analytical problem solving, requirements facilitation, and collaboration skills; Consistently looks for better ways to achieve business results; recognizes problems as opportunities for process improvementSelf-motivated, flexible, organized, and able to perform with minimal supervision; Pro-actively make recommendations as issues and opportunities ariseDemonstrated ability to communicate, verbally and in writing, complex information to others in an understandable way', 'Demonstrated ability to communicate, verbally and in writing, complex information to others in an understandable way', 'Required Qualifications:', '3+ years in an analytics or business intelligence setting,\xa0including experience using one or more of the following analytic tools: SAS, SQL, R, Python, SPSS', 'The Auto Club Group, and all of its affiliated companies, is an equal opportunity/affirmative action employer.\xa0All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, gender identity, sexual orientation, national origin, disability or protected veteran status.', 'Data Analytics Lead - Audit\xa0-\xa0(21000065)', 'Mines data, performs quantitative analysis and creates clear and actionable narratives about the business.', 'Ability to complete advanced statistical analysis', 'Mathematical/statistical, reasoning, and logic skills; Understanding of statistical concepts (e.g. hypothesis testing, regression analysis)', '-', 'Preferred Qualifications:', 'Extensive knowledge and skills:', 'Translating high level business goals into the tasks and technical specifications needed to perform meaningful data analysis;', 'Working with Internal Audit or on an audit data analytics team', '5+ years of experience in an analytics or business intelligence setting, including experience using one or more of the following analytic tools: SAD, SQL, R, Python, SPSS', 'Data design principles', 'Self-motivated, flexible, organized, and able to perform with minimal supervision; Pro-actively make recommendations as issues and opportunities arise', 'Insurance or Banking industry background', 'Performs analysis and produces meaningful reports on business policies, processes, and performance with a focus on risk and control.']",Mid-Senior level,Full-time,Information Technology,Insurance,2021-03-24 13:05:10
Data Engineer,The Climate Corporation,"San Francisco, CA",2 weeks ago,Be among the first 25 applicants,"['', 'What You Will Do', 'Experience in developing models / explores / dashboards in Looker', '2+ years of scripting experience', 'Inspire one another', 'Applicant must be willing to provide 24x7 on call support one week per month', 'Build systems to answer business questions in a timely fashion and expand our product features', 'Preferred Qualifications', 'Learn More About Our Team And Our Mission', '2+ years of experience with dimensional data modeling & schema design in Data Warehouses.', 'A stocked kitchen with a large assortment of snacks & drinks to get you through the day', 'Design and develop new systems and tools to enable folks to consume and understand data faster', 'Rapidly prototype new analytics views and work directly with stakeholders across multiple functions (Science, Marketing, Sales, Risk, Finance, Product)', 'Superb medical, dental, vision, life, disability benefits, and a 401k matching program', 'Leave a mark on the world', 'Be direct and transparent', 'Superb medical, dental, vision, life, disability benefits, and a 401k matching programA stocked kitchen with a large assortment of snacks & drinks to get you through the dayEncouragement to get out of the office and into the field with agents and farmers to see first-hand how our products are being usedWe take part and offer various workshops, conferences, meet-up groups, tech-talks, and hack-a-thons to encourage participation and growth in both community involvement and career development', 'Schema design in data warehouses, working directly with SQL to profile data, generate analytics', 'Encouragement to get out of the office and into the field with agents and farmers to see first-hand how our products are being used', 'Experience working in a cloud deployment such as AWS is a plus', 'Champion data warehousing best practices', '2+ years of SQL and dynamic or static programming languages experience as applied to ETL/ELT tools (Informatics, Kettle, Talend, etc.)', 'Help design and build a Business Analytics Warehouse. Build and maintain the core data model, ETL / ELT, core data metrics and data quality.', 'Experience with relational databases or NoSql', 'What We Offer', 'Excellent communication skills including the ability to identify and communicate data driven insights', 'B.S. or B.A. in computer science, math, economics, engineering or other technical field2+ years of SQL and dynamic or static programming languages experience as applied to ETL/ELT tools (Informatics, Kettle, Talend, etc.)Performing shell scripting in a Linux/Unix environmentPerforming dimensional data modelingSchema design in data warehouses, working directly with SQL to profile data, generate analyticsDevelop infrastructure in AWS Cloud environment to inform on key metrics, recommend changesExperience with relational databases or NoSqlExperience in developing models / explores / dashboards in LookerApplicant must be willing to provide 24x7 on call support one week per month', 'Inspire one anotherInnovate in all we doLeave a mark on the worldFind the possible in the impossibleBe direct and transparent', 'Work closely with other departments to gather new data and leverage existing data to make our products better for us and our users', 'Experience with massive scale relational databases (MPP) is a big plus (Vertica / Redshift / Teradata / MemSQL).', 'We take part and offer various workshops, conferences, meet-up groups, tech-talks, and hack-a-thons to encourage participation and growth in both community involvement and career development', 'Innovate in all we do', 'Help design and build a Business Analytics Warehouse. Build and maintain the core data model, ETL / ELT, core data metrics and data quality.Rapidly prototype new analytics views and work directly with stakeholders across multiple functions (Science, Marketing, Sales, Risk, Finance, Product)Champion data warehousing best practicesBuild systems to answer business questions in a timely fashion and expand our product featuresArchitect, build and launch new data models that provide intuitive analytics to business usersDevelop infrastructure to inform on key metrics, recommend changes and predict future resultsWork closely with other departments to gather new data and leverage existing data to make our products better for us and our usersBuild data expertise and own data quality for the pipelines you buildDesign and develop new systems and tools to enable folks to consume and understand data fasterProvide expert advice and education in the usage and interpretation of data systems to the business users', 'Architect, build and launch new data models that provide intuitive analytics to business users', 'Performing dimensional data modeling', 'Performing shell scripting in a Linux/Unix environment', 'Develop infrastructure in AWS Cloud environment to inform on key metrics, recommend changes', 'Build data expertise and own data quality for the pipelines you build', 'Find the possible in the impossible', '2+ years of experience with dimensional data modeling & schema design in Data Warehouses.2+ years of scripting experienceExperience with massive scale relational databases (MPP) is a big plus (Vertica / Redshift / Teradata / MemSQL).Experience working in a cloud deployment such as AWS is a plusExcellent communication skills including the ability to identify and communicate data driven insights', 'B.S. or B.A. in computer science, math, economics, engineering or other technical field', 'Position Overview', 'Basic Qualifications', 'Develop infrastructure to inform on key metrics, recommend changes and predict future results', 'Provide expert advice and education in the usage and interpretation of data systems to the business users']",Not Applicable,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Data Analytics Engineer,SEGULA Technologies,"Ann Arbor, MI",2 weeks ago,169 applicants,"['', '1+ years of experience working in Cloud technologies (AWS/Azure/Google)', 'Gather data and prepare reports to communicate trends', 'Requirements', 'Responsibilities', 'Cleanup data to accurately identify patterns', 'Focus on overall efficiency for product development using machine learning', 'Gather data and prepare reports to communicate trendsCleanup data to accurately identify patternsCollaborate with Engineers to improve tools and processesFocus on overall efficiency for product development using machine learningApply data science skills and predictive analytics for team applicationsDevelop exploratory analysis to propose clear ideas and proof of concept', 'Collaborate with Engineers to improve tools and processes', 'Develop exploratory analysis to propose clear ideas and proof of concept', 'Job Description', 'MSc in Computer Science, Electrical Engineering, or similar preferred', 'Apply data science skills and predictive analytics for team applications', '1+ years of experience in Data Science/Machine Learning and statistics', 'Knowledge of data managemenr and visualization with SQL, Tableau or PowerBI', 'Programming skills in Python, Spark or R', 'MSc in Computer Science, Electrical Engineering, or similar preferred1+ years of experience in Data Science/Machine Learning and statistics1+ years of experience working in Cloud technologies (AWS/Azure/Google)Knowledge of data managemenr and visualization with SQL, Tableau or PowerBIProgramming skills in Python, Spark or R']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer Support,"ALTA IT Services, LLC","Chicago, IL",1 day ago,Be among the first 25 applicants,"['None', 'Travel And Telecommuting', 'Data Engineer Support', 'Model Operations; data engineering & support. Making changes to models vs building from ground', 'No', 'contract', 'ALTA is supporting a contract opportunity for a client in Chicago or Atlanta with a Data Engineer Support opportunity.', 'For immediate interest, please send your resume ', 'Python, Scala, Spark, Hadoop, Shell Scripting. Teradata a plus Limited Data Engineer, not as complicated', 'Chicago', ' or ', 'to ddegiuseppe@altaits.com', 'Atlanta']",Entry level,Contract,Information Technology,Information Technology and Services,2021-03-24 13:05:10
AWS Data Engineer,Capgemini,"San Francisco, CA",2 days ago,Be among the first 25 applicants,"['', ' Solid grasp of database engineering and design', ' Must be hands-on coding capable in at least a core Language skill of (Python, Java or Scala) with Spark', ' Expertise in working with Distributed DW and Cloud Services (like Snowflake, Redshift, AWS, etc.) via scripted pipelines for at least 2 years', 'Required Skills And Experience', ' Very high test score – Python, SQL, DW Concepts & Logic', 'Required Technical Skills', ' Variety of EC2, EMR, RDS, Redshift, Snowflake', ' Nice to have: Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores', 'Organization', ' Should have progressing knowledge in Business Analysis, Business Knowledge, Software Engineering, Testing, Data Management, Architecture Knowledge and Technical Solution Design.', ' Unit Test & Document deliverables', ' AWS Cloud knowledge', ' Very high test score – Python, SQL, DW Concepts & Logic a core Language skill of (Python, Java or Scala) with Spark Variety of EC2, EMR, RDS, Redshift, Snowflake Strong SQL Other SQL based databases, like Oracle, SQL Server, etc. AWS Cloud knowledge Nice to have: Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores', 'Schedule', ' a core Language skill of (Python, Java or Scala) with Spark', ' Qualification: 3-7 years (2 years min relevant experience in the role) , Bachelor’s Degree.', ' Identify bottlenecks and bugs in the system and develop scalable solutions', ' Handling large and complex sets of XML, JSON, and CSV from various sources and databases', ' Capacity to successfully manage a pipeline of duties with minimal supervision', ' This role intersects with ‘Big Data’ stack to enable varied Analytics, ML, etc. Not just ‘DW’ type workload', 'Primary Location', ' Strong SQL', ' Business Domains like Sales & Marketing, Direct to Consumer, Adsales of particular interest', 'About Capgemini', ' Other SQL based databases, like Oracle, SQL Server, etc.', ' Certification: Should have or seeking SE Level 1.', 'Job', '6+ years of experience in data pipeline engineering for both batch and streaming applications', ' Qualification: 3-7 years (2 years min relevant experience in the role) , Bachelor’s Degree. Certification: Should have or seeking SE Level 1. Should have progressing knowledge in Business Analysis, Business Knowledge, Software Engineering, Testing, Data Management, Architecture Knowledge and Technical Solution Design.', ' Leveraged frameworks & orchestration like Airflow as required for ETL pipeline']",Not Applicable,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data engineer,"Metasys Technologies, Inc.","Bristol, CT",3 weeks ago,50 applicants,[''],Entry level,Contract,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Commonwealth Financial Network,"Waltham, MA",6 days ago,Be among the first 25 applicants,"['', 'About Commonwealth', ' Troubleshooting and tuning complex SQL statements and making recommendations on indexing strategies ', 'Key Responsibilities', ' Ability to write, troubleshoot, and performance-tune stored procedures and ad-hoc scripts ', 'Core Strengths', ' Picture Yourself Here ', ' Fulfilling data requests when the data elements are not yet built in the data warehouse ', ' Experience with data warehousing ', ' Financial sector experience a plus ', ' Performing code reviews, learning from peers, and sharing your knowledge in a positive, friendly way ', ' Solid understanding of relational modeling concepts  Ability to write, troubleshoot, and performance-tune stored procedures and ad-hoc scripts  Experience with data warehousing ', ' Creating workflows and transformations with SQL Server Integration Services ', ' 5–10 years of experience ', ' Solid understanding of relational modeling concepts ', ' Building the schema and SQL code behind our complex suite of web-based applications ', ' Ensuring that data is stored efficiently and can be retrieved quickly within our databases and servers that integrate as an ecosystem to serve our clients’ needs ', ' The Fine Print ', ' Experience with ETL tools such as SSIS and Azure Data Factory  5–10 years of experience  Experience working with Microsoft Azure Cloud a plus  Financial sector experience a plus ', ' Collaborating with a dynamic team of web developers, business analysts, and product owners to build the best applications in the financial industry ', ' Experience with ETL tools such as SSIS and Azure Data Factory ', ' Building ETL processes for data warehouse and data hub to enable business users to self-serve various data analytics ', ' Experience working with Microsoft Azure Cloud a plus ', ' Building ETL processes for data warehouse and data hub to enable business users to self-serve various data analytics  Fulfilling data requests when the data elements are not yet built in the data warehouse  Building the schema and SQL code behind our complex suite of web-based applications  Creating workflows and transformations with SQL Server Integration Services  Ensuring that data is stored efficiently and can be retrieved quickly within our databases and servers that integrate as an ecosystem to serve our clients’ needs  Troubleshooting and tuning complex SQL statements and making recommendations on indexing strategies  Working on a variety of projects and systems, from big to small and complex to simple  Collaborating with a dynamic team of web developers, business analysts, and product owners to build the best applications in the financial industry  Performing code reviews, learning from peers, and sharing your knowledge in a positive, friendly way ', 'Additional Skills And Knowledge', ' Working on a variety of projects and systems, from big to small and complex to simple ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer (Python/PySpark/SAS),Technology Ventures,"McLean, VA",2 weeks ago,80 applicants,"['', 'Strong quantitative skills (statistics, econometrics, linear algebra)', 'Solid understanding of software design principles', 'BS in Computer Science or equivalent experience', 'A strong understanding of SQL', 'Verify that the Python version of the SAS code is equivalent to the SAS version. This involves running both processes, comparing the output, and resolving any differences.', 'Experience writing automated unit, integration, regression, performance and acceptance tests', 'At least 3 years of experience developing production Python codeA strong understanding of Pandas and PySparkA strong understanding of SQLExperience with SASSolid understanding of software design principles', 'Optimize the Python code to reduce the runtime.', 'Peer review code and automated tests, help team members with design and implementation challenges.', 'Qualifications:', 'Enhance the Python process to be fault-tolerant and contain checkpoints to make rerunning a subset of the process more efficient.', 'Experience with SAS', 'BS in Computer Science or equivalent experienceExperience with cloud computing and storage services, particularly AWS EMRExperience writing automated unit, integration, regression, performance and acceptance testsStrong quantitative skills (statistics, econometrics, linear algebra)', 'Leverage PySpark and AWS EMR to parallelize the process and reduce the runtime.', '1) Please describe your experience converting SAS code to Python (Pandas / PySpark)? What were the major challenges?', 'A strong understanding of Pandas and PySpark', '2) Please describe a project where you wrote production Pandas or PySpark code. How did you test it?', 'Write automated tests for Python code.', 'Translate existing SAS code into Python code. We are using both Pandas data frames and PySpark data frames so knowledge of both is required.Verify that the Python version of the SAS code is equivalent to the SAS version. This involves running both processes, comparing the output, and resolving any differences.Leverage PySpark and AWS EMR to parallelize the process and reduce the runtime.Optimize the Python code to reduce the runtime.Enhance the Python process to be fault-tolerant and contain checkpoints to make rerunning a subset of the process more efficient.Write automated tests for Python code.Peer review code and automated tests, help team members with design and implementation challenges.', 'Translate existing SAS code into Python code. We are using both Pandas data frames and PySpark data frames so knowledge of both is required.', 'At least 3 years of experience developing production Python code', 'Preferred Skills:', 'Experience with cloud computing and storage services, particularly AWS EMR']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer - Junior Level,"Community Tax, LLC","Chicago, IL",1 week ago,93 applicants,"['', 'Some experience and proficiency with Power BI', 'Build and flesh out data models in our database for use in regular and/or automated reporting and analysis', 'Convey your work and results to a wide variety of internal and external stakeholders - nerds and non-nerds', 'Our Mission', 'Experience with some flavor of data visualization', 'You like to formulate the right questions versus just seeking answers to make sure the best and smartest decisions get made', 'Clean data and build business rules to ensure proper trigger functions within the newly implemented CRM', 'Offer insight to all aspects of the organization - Sales, marketing, servicing and finance', 'Comfort in a complex data environment and understanding of data structures', 'Advanced proficiency with Excel', 'Experience supporting business operations in an analytics capacity', 'Build, monitor and maintain reliable data pipelines for highly available reporting or data integrations.', 'Support the business with ad hoc reporting', 'Solid communication skills', 'Passion for automation', 'Work with a variety of data sources - extracting knowledge and actionable information from massive datasetsBuild and flesh out data models in our database for use in regular and/or automated reporting and analysisWrangle and Scrub data through SQL and Python, to produce clean accurate data utilized in various projects.Clean data and build business rules to ensure proper trigger functions within the newly implemented CRMBuild, monitor and maintain reliable data pipelines for highly available reporting or data integrations.Be a passionate problem solver - breaking down problems and developing analytical insightsConvey your work and results to a wide variety of internal and external stakeholders - nerds and non-nerdsEvaluate operations for inefficiencies and identify areas where you can create, automate, and develop tools (SQL-based or otherwise)Diagnose data-related bugs and ensure they are resolved in a timely mannerSupport the business with ad hoc reportingContinuously strive for a deeper understanding of our business driversOffer insight to all aspects of the organization - Sales, marketing, servicing and finance', 'Work with a variety of data sources - extracting knowledge and actionable information from massive datasets', 'Evaluate operations for inefficiencies and identify areas where you can create, automate, and develop tools (SQL-based or otherwise)', ""The business sense to understand why you're pulling data and whether it seems accurate...not just pulling it"", 'Be a passionate problem solver - breaking down problems and developing analytical insights', 'What You Will Learn To Do', 'Diagnose data-related bugs and ensure they are resolved in a timely manner', 'Wrangle and Scrub data through SQL and Python, to produce clean accurate data utilized in various projects.', 'Continuously strive for a deeper understanding of our business drivers', 'Familiarity with transactional and data warehouse environments', 'Education in Business information systems or related technical field or equivalent work experience', 'A love for insights, innate curiosity, and a deep desire to find ways to create value', 'What You Bring To The Table', ""Strong relevant academic foundation with related internship, capstone or freelance gig experienceEducation in Business information systems or related technical field or equivalent work experienceExperience supporting business operations in an analytics capacityA love for insights, innate curiosity, and a deep desire to find ways to create valueSolid communication skillsPassion for automationComfort in a complex data environment and understanding of data structuresIntermediate to advanced proficiency with SQL and PythonAdvanced proficiency with ExcelExperience with some flavor of data visualizationSome experience and proficiency with Power BIFamiliarity with transactional and data warehouse environmentsThe business sense to understand why you're pulling data and whether it seems accurate...not just pulling itYou like to formulate the right questions versus just seeking answers to make sure the best and smartest decisions get made"", 'Intermediate to advanced proficiency with SQL and Python', 'Strong relevant academic foundation with related internship, capstone or freelance gig experience']",Associate,Internship,Information Technology,Insurance,2021-03-24 13:05:10
Data Engineer,Shift4 Payments,United States,2 weeks ago,25 applicants,"['', 'Job Summary:', 'Bachelor of Science degree and/or relevant work experience', 'Basic VB & C# experience a plus.', 'We are looking for a Data Engineer to join our Information Technology Group. This position will engage in an Agile-based SDLC to complete data layer requirements (includes but not limited to table creation, stored procedure creation and updates, ETL processes) as well as acquire a deep understanding of business processes and flows to assist with reporting / business intelligence tasks. The Data Engineer will work in a team comprised of front / backend developers, business analysts, QA engineers, and will follow the Project Manager’s lead for sprint goals and deliverables.', 'Experience with an Agile SDLC a plus.', 'Education:', 'Experience with Jira and Confluence', 'Excellent written communication skills, particularly in the realm of technical documentation', 'Working with stakeholders and Technology Group to fine tune outputs to ensure all requirements are met for reports', 'Experience working with API build-outs and data layer requirements ', 'Responsibilities:', 'Shift4 Payments provides\xa0equal employment opportunities\xa0(EEO) to all employees and applicants for\xa0employment', 'Qualifications:', 'Ensuring code standards are followed on all code', 'Shift4 Payments provides\xa0equal employment opportunities\xa0(EEO) to all employees and applicants for\xa0employment\xa0without regard to race, color, religion, sex, national origin, age, disability or genetics.', 'Shift4 Payments is the leader in secure payment processing solutions. The company’s groundbreaking technologies help power the top software providers in numerous verticals, including hospitality, retail, F&B, e-commerce, lodging, gaming, and many more. Shift4’s family of software brands includes Harbortouch, Restaurant Manager, POSitouch, and Future POS — with additional integrations to 300+ POS/PMS systems across every industry. With an expansive global footprint that includes eight offices across the U.S. and Europe and over 8,000 sales partners, the company securely processes more than a billion transactions annually for nearly 200,000 businesses, representing over $100 billion in payments each year. For additional information, visit www.shift4.com.', 'Experience with SSIS and SSRS essential. SSAS preferred but not required.', '3+ years of experience with SQL Server.', 'Working closely on related issues with internal business units', 'without regard to race, color, religion, sex, national origin, age, disability or genetics.', ""Bachelor's degree in Computer Science or equivalent work experience"", '\xa0', 'Helping design, develop, debug and optimize stored procedures and views producing data suitable for reporting purposes', 'Company Background:', 'Helping design, develop, debug and optimize stored procedures and views producing data suitable for reporting purposesEnsuring code standards are followed on all codeRefactoring existing code based on existing DBs and modifying it for newer modelsValidating data, performing cross reference checking between source data and outputsWorking with stakeholders and Technology Group to fine tune outputs to ensure all requirements are met for reportsWorking closely on related issues with internal business units', 'Validating data, performing cross reference checking between source data and outputs', ""Bachelor's degree in Computer Science or equivalent work experience3+ years of experience with SQL Server.Experience working with API build-outs and data layer requirements Experience with SSIS and SSRS essential. SSAS preferred but not required.Thorough understanding of SDLC and how it pertains to the database.Experience with Jira and ConfluenceExcellent written communication skills, particularly in the realm of technical documentationBasic VB & C# experience a plus.Experience with an Agile SDLC a plus.Ability to communicate technical issues to all levels"", 'Data Engineer', 'Refactoring existing code based on existing DBs and modifying it for newer models', 'Ability to communicate technical issues to all levels', 'We are looking for individuals that are extremely self-sufficient, available to work flexible hours, & hold themselves to the highest standards of professionalism. We will be evaluating candidates based on how they interview, prior experiences, functional knowledge, and references.', 'Thorough understanding of SDLC and how it pertains to the database.']",Mid-Senior level,Full-time,Information Technology,Financial Services,2021-03-24 13:05:10
Data Engineer,Sharecare,"Atlanta, GA",4 weeks ago,Be among the first 25 applicants,"['', ' Analyze and interpret complex data on all target systems, coordinating with internal and external stakeholders to validate requirements', "" 3+ years' experience with Python mandatory. Web and HTML a plus."", ' Document technical and system specifications for all ETL processes', ' Excellent written and verbal communication skills to lead meetings with technical peers regarding the solution designs Ability to work in team environment Ability to interface with client and/or Product Management', ' Responsible for populating the data warehouse and all related extraction, transformation and load of data functions', ' Design and develop all data mapping techniques for all data models in systems', 'Job Summary', "" 3+ years' experience with data warehousing architecture concepts and ETL techniques"", ' Support all facets of the business in data analysis and data operations', ' Build architecture and libraries to be leveraged across the enterprise', ' Ability to work in team environment', ' Experience interacting with RESTful service APIs. REST implementation is a plus.', ' Responsible for designing, implementing and testing Data pipelines using Python Responsible for populating the data warehouse and all related extraction, transformation and load of data functions Support all facets of the business in data analysis and data operations Design and develop all data mapping techniques for all data models in systems Document technical and system specifications for all ETL processes Analyze and interpret complex data on all target systems, coordinating with internal and external stakeholders to validate requirements Build architecture and libraries to be leveraged across the enterprise Occasionally publish written documentation that relates to the technology solutions. Perform root cause analysis, resolve and validate all production data issues Exhibit a passion for clean code and simple solutions to complex problems', ' Occasionally publish written documentation that relates to the technology solutions.', ' Exhibit a passion for clean code and simple solutions to complex problems', 'Qualifications', ' Perform root cause analysis, resolve and validate all production data issues', ' Ability to interface with client and/or Product Management', ' Experience with Amazon cloud technologies, Unix shell scripts and GIT is desired', "" 3+ years' experience with database technologies such as SQL, Vertica, Mongo and/or Elastic"", ' Responsible for designing, implementing and testing Data pipelines using Python', 'Essential Functions', 'Specific Skills/Attributes', ' Excellent written and verbal communication skills to lead meetings with technical peers regarding the solution designs', "" Bachelor's degree (or higher) in Computer Science or related field 3+ years' experience with Python mandatory. Web and HTML a plus. Knowledge and experience with Agile development practices Experience interacting with RESTful service APIs. REST implementation is a plus. 3+ years' experience with database technologies such as SQL, Vertica, Mongo and/or Elastic 3+ years' experience with data warehousing architecture concepts and ETL techniques Experience with Amazon cloud technologies, Unix shell scripts and GIT is desired"", "" Bachelor's degree (or higher) in Computer Science or related field"", ' Knowledge and experience with Agile development practices']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,ADT,"Boca Raton, FL",2 weeks ago,31 applicants,"['', '4-year college degree in either analytics, mathematics, operations or computer science, statistics, or similar highly quantitative degree required with a graduate degree preferred', 'Be capable of working closely with the business to define and build implementation strategies to drive business results and measure value creation', 'Own end-to-end predictive modeling processes including ETL, model creation, calibration, cross-validation, and maximizing model accuracy and performance', 'Company Overview', 'Analyze ADT customer operational, demographic, 3rd party, and/or big data ', 'Passion for delivering the ideal customer experience', 'Develop presentations to illustrate data insights, present key findings and drive key decision-making', 'Experience with data extraction, cleansing and normalization of data inconsistencies', 'Outstanding analytical skills; comfortable working with, interpreting and presenting data', 'Utilize correlations, clustering, profiling and other statistical analyses to identify customer insights and drivers affecting retention, engagement & satisfaction', 'Provide innovative ideas on how to streamline and optimize new and existing processes and results', 'P Osition Summary', 'Expertise and proven proficiency with SQL and/or Oracle query languages', 'Education', 'Skills And Capabilities', 'Coachable, strong desire to learn, and enthusiasm for creating customers for life', 'Experience', ' 5+ year’s directly applicable experience ', 'Efficiency and comfort with querying and manipulating large data sets', ' Experience with data extraction, cleansing and normalization of data inconsistencies Analyze ADT customer operational, demographic, 3rd party, and/or big data  Utilize correlations, clustering, profiling and other statistical analyses to identify customer insights and drivers affecting retention, engagement & satisfaction Own end-to-end predictive modeling processes including ETL, model creation, calibration, cross-validation, and maximizing model accuracy and performance Develop, maintain, and continuously improve propensity models under the mentorship of senior team members  Have the capability to utilize advanced analytical tools, including statistical modeling and having the ability to interpret and translate the findings into business strategy and results  Develop recommendations including Operational changes based on customer insights Be capable of working closely with the business to define and build implementation strategies to drive business results and measure value creation Develop presentations to illustrate data insights, present key findings and drive key decision-making Provide innovative ideas on how to streamline and optimize new and existing processes and results ', 'Position Responsibilities', 'Experience working within a big data environment preferred ', '5+ year’s directly applicable experience', 'Develop recommendations including Operational changes based on customer insights', 'Experience using ML tools such as H2O.ai, Alteryx, DataRobot, or equivalent tools preferred', 'Have the capability to utilize advanced analytical tools, including statistical modeling and having the ability to interpret and translate the findings into business strategy and results ', ' 4-year college degree in either analytics, mathematics, operations or computer science, statistics, or similar highly quantitative degree required with a graduate degree preferred ', 'Experience with Tableau, OBIEE, or equivalent tools', 'Solid sense of business acumen alongside strong verbal and written communication skills', ' Expertise and proven proficiency with SQL and/or Oracle query languages Expertise and working proficiency with Python and/or R programming languages Experience with Tableau, OBIEE, or equivalent tools Experience using ML tools such as H2O.ai, Alteryx, DataRobot, or equivalent tools preferred Efficiency and comfort with querying and manipulating large data sets Ability to develop working tables and data marts that can be referenced and leveraged across multiple users  Experience working within a big data environment preferred  Outstanding analytical skills; comfortable working with, interpreting and presenting data Solid sense of business acumen alongside strong verbal and written communication skills Passion for delivering the ideal customer experience Coachable, strong desire to learn, and enthusiasm for creating customers for life ', ' ADT LLC is an Equal Employment Opportunity (EEO) employer. We are committed to having a diverse and inclusive workforce and do our best to foster a culture and environment where every employee feels valued. Our goal is to serve our customers and help save lives. We can achieve this goal when we have the best talent working in an environment where employees feel included and recognized. Visit us online at jobs.adt.com to learn more.', 'Expertise and working proficiency with Python and/or R programming languages', 'Develop, maintain, and continuously improve propensity models under the mentorship of senior team members ', 'Ability to develop working tables and data marts that can be referenced and leveraged across multiple users ']",Entry level,Full-time,Information Technology,Construction,2021-03-24 13:05:10
Data Engineer,BitTitan,"Bellevue, WA",1 month ago,Be among the first 25 applicants,"['', 'Research relevant emerging technologies and tools ', 'Work as part of a team to design and develop analytics platform solutions ', 'Establishes brand and team as subject matter experts in advanced analytics across departments ', 'Ability to work independently in ambiguous environment', 'Strong skills\u202fin\u202fcommon analysis tools\u202fsuch as\u202fExcel, Power BI, Tableau or similar ', 'We would love to speak with you if you have: ', 'Design, build and deploy BI solutions ', 'In This Job, You Will', 'Proficient with SSIS or Azure Data Factory ', 'Build and maintain company-wide KPI dashboards ', 'Strong knowledge of database, storage, collection and aggregation models, techniques, and technologies and how to apply them in business ', 'Serve as a subject matter expert in Azure analytics platforms and assist others with data collection and analysis ', '2+ years data engineering/data warehousing experience ', ' Work as part of a team to design and develop analytics platform solutions  Implement Azure data services including Azure Data Lake, Azure Data Factory and Azure Analysis Services  Design and develop scalable data ingestion frameworks to transform a wide variety of datasets  Research, analyze, recommend, and select technical approaches for solving challenging and complex development and integration problems  Serve as a subject matter expert in Azure analytics platforms and assist others with data collection and analysis  Introduces incremental improvements to data analysis, visualization, and presentation techniques to communicate discoveries  Research relevant emerging technologies and tools  Establishes brand and team as subject matter experts in advanced analytics across departments  Collaborate with data stakeholders that require reporting data and ensure that datasets are in place and are used consistently internally/externally  Provide guidance on data governance, security, and privacy  Translate business needs into technical specifications  Design, build and deploy BI solutions  Build and maintain company-wide KPI dashboards  Other strategic projects as assigned  ', 'Design and develop scalable data ingestion frameworks to transform a wide variety of datasets ', 'Collaborate with data stakeholders that require reporting data and ensure that datasets are in place and are used consistently internally/externally ', 'Quick Overview', 'Provide guidance on data governance, security, and privacy ', 'Other strategic projects as assigned ', '2+ years real-word experience implementing Azure analytics platforms ', 'Translate business needs into technical specifications ', 'Ability to write complex SQL queries ', 'Introduces incremental improvements to data analysis, visualization, and presentation techniques to communicate discoveries ', ' 2+ years data engineering/data warehousing experience  2+ years real-word experience implementing Azure analytics platforms  Strong knowledge of database, storage, collection and aggregation models, techniques, and technologies and how to apply them in business  Ability to apply such methods to solve business problems using one or more Azure data services in combination with building data pipelines, data streams, and system integration  Working knowledge of programming and scripting languages such as C#, Java, Python, and PowerShell  Proficient with SSIS or Azure Data Factory  Strong skills\u202fin\u202fcommon analysis tools\u202fsuch as\u202fExcel, Power BI, Tableau or similar  Ability to write complex SQL queries  Ability to work independently in ambiguous environment ', 'Ability to apply such methods to solve business problems using one or more Azure data services in combination with building data pipelines, data streams, and system integration ', 'Implement Azure data services including Azure Data Lake, Azure Data Factory and Azure Analysis Services ', 'Research, analyze, recommend, and select technical approaches for solving challenging and complex development and integration problems ', 'Working knowledge of programming and scripting languages such as C#, Java, Python, and PowerShell ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer (BigQuery/Tableau),"Cook Systems, Inc.","Memphis, TN",7 days ago,99 applicants,"['', 'Required:', 'Ability to translate business ask to technical requirements.', 'COOK SYSTEMS INTERNATIONAL has an IMMEDIATE need for a Data Engineer with Google BigQuery and Tableau', 'To be successful in this role candidate must be able to work effectively in a fluid, fast-paced environment while maintaining good communication with management and team members and be able to mentor entry level analysts. Ability to develop experimental and analytic plans for data modeling processes, use of strong baselines, ability to accurately determine cause and effect relations. Understanding of relevant statistical measures such as confidence intervals, significance of error measurements, development and evaluation data sets, etc. Track record of diving into data to Client hidden patterns and of conducting error/deviation analysis. Design and develop data visualizations, complex reports and dashboards based on business requirements using complex SQL/Tableau. Must have Business Intelligence experience using data warehouse tools such as Business Objects preferred. Extensive experience solving analytical problems using quantitative approaches, operations research and optimization algorithms. Comfort manipulating and analyzing complex, high-volume, high dimensionality data from varying sources.\xa0', 'In this role, candidate is expected to provide technical expertise and support in the design, development, and implementation of Audience centric Dashboards.', 'Experience in Digital technology, Google Big query and AWS is a plus.', ""Requires Bachelor's degree in Computer Information Systems, or other related field and 6 year experience in an interactive marketing role with emphasis in analytics; or a Master's degree, preferably in Analytics, and 4 years' experience in an interactive marketing role with emphasis in analytics."", 'e/o/e', 'Should have experience in Data Mapping and Data analytics along with ability to recommend solutions to optimize the SQL queries for visualizations as well as organizing and storing, the large scale data.', 'This is a technical role and requires the candidate to have expertise in relational databases like SQL Server and Tableau Visualization with at least 4 plus years of experience.', 'Responsible for designing, developing and implementing data models with Statistical and Machine learning expertise. Responsible for Data Mining and Analysis from company databases to drive optimization and improvement of Client and Media Dashboards.', '\xa0']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Excella,"Arlington, VA",2 weeks ago,Be among the first 25 applicants,"['', 'Starting day one, every employee is bonus eligible and receives 15 days of paid vacation, 6 federal holidays, and 4 floating holidays.', 'Developing and managing data processes to ensure that data is available and usableCreation and automation of data pipelines and platformsManaging and monitoring data quality via automated testing frameworks (Data Driven Testing, TDD, etc.) Working closely with Architects, Data Scientists, and DevOps to design, build, test, deliver, and maintain sustainable and highly scalable data solutionsResearching data acquisition and evaluating suitability Integration of data management solutions into client environment Actively managing risks to data and ensuring there is a data recovery planBuilding data repositories such as data warehouses, data lakes, and operational data stores, etc.', 'Building data repositories such as data warehouses, data lakes, and operational data stores, etc.', 'Developing and managing data processes to ensure that data is available and usable', 'You\'ll work with great people who love what they do our team includes published authors, certified trainers, and internationally renowned speakers.We have a ""bring your own device"" workplace and will share the cost of a new computer of your choice -- Mac or PC. It\'s up to you.We\'ll invest in your career by providing 3 days of paid professional development every year, including travel and registration fees to attend classes and conferences, in addition to tuition assistance for degrees and certifications.Starting day one, every employee is bonus eligible and receives 15 days of paid vacation, 6 federal holidays, and 4 floating holidays.You can bike, drive, or metro to work -- our commute reimbursement plan has you covered.You\'ll have fun! We hold monthly social events all year long, including a summer event for you and your family.', 'Use of scripting languages, preferably Python', ""Here's What You Can Expect From Us"", ""We'll invest in your career by providing 3 days of paid professional development every year, including travel and registration fees to attend classes and conferences, in addition to tuition assistance for degrees and certifications."", 'Actively managing risks to data and ensuring there is a data recovery plan', 'You can bike, drive, or metro to work -- our commute reimbursement plan has you covered.', 'Excella is an equal opportunity/affirmative action employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected veteran status, age, or any other characteristic protected by law.', 'Creating secure, performant, and well-modeled data stores', 'Creating robust and extensible data pipelines for production systemsUse of cloud platforms, preferably AWSCreating secure, performant, and well-modeled data storesCommon analytical platform architectural patterns (Star Schema, data integration patterns, ABAC, data quality frameworks etc.)Data lake design patterns and technology options (schema on read, metadata capture, search framework)Use of scripting languages, preferably PythonFamiliarity with NoSQL databasesSource code version control management using git', ""You'll have fun! We hold monthly social events all year long, including a summer event for you and your family."", '3+ years relevant professional work experience.Experience and expertise in the following', 'Experience and expertise in the following', 'Managing and monitoring data quality via automated testing frameworks (Data Driven Testing, TDD, etc.) ', 'Familiarity with NoSQL databases', 'Use of cloud platforms, preferably AWS', 'Project experience using the Scrum or Kanban framework.', 'Aptitude and desire for learning new technologies.', ""You'll work with great people who love what they do our team includes published authors, certified trainers, and internationally renowned speakers."", '3+ years relevant professional work experience.', 'Professionalism; to include written and oral communication - the ability to communicate collaboratively in front of a whiteboard. An ability to understand your audience and adjust your communication style to fit', 'Working closely with Architects, Data Scientists, and DevOps to design, build, test, deliver, and maintain sustainable and highly scalable data solutions', 'Integration of data management solutions into client environment ', 'Technically savvy, entrepreneurial spirit who thrives in environments that reward self-initiative and resourcefulness.', 'Researching data acquisition and evaluating suitability ', 'Creation and automation of data pipelines and platforms', 'Data lake design patterns and technology options (schema on read, metadata capture, search framework)', 'Creating robust and extensible data pipelines for production systems', 'We have a ""bring your own device"" workplace and will share the cost of a new computer of your choice -- Mac or PC. It\'s up to you.', 'Common analytical platform architectural patterns (Star Schema, data integration patterns, ABAC, data quality frameworks etc.)', 'Source code version control management using git']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
"Data Reporting Engineer, Data Products",Flatiron Health,"New York, NY",2 weeks ago,52 applicants,"['', 'Collaborate with customers and synthesize feedback to design and prototype new reporting products', 'Back-up child care', 'Work/life autonomy via flexible work hours and flexible paid time off', ' Write and edit custom SQL queries to generate customer-facing dashboards and reports for community oncology clinics designed to meet their clinical and operational needs Analyze reporting requests to develop solutions addressing common needs across oncology practices Build and maintain data pipelines that power parts of our analytics product Collaborate with customers and synthesize feedback to design and prototype new reporting products Work with platform software engineers to improve the data infrastructure that powers clinical analytics  Maintain cross-functional relationships with customer-facing teams and continually enhance team efficiency ', ' Database performance and interpretation of query execution plans Database scripting and Extract, Transform and Load processes ', 'Maintain cross-functional relationships with customer-facing teams and continually enhance team efficiency', 'Generous parental leave (16 weeks for either parent)', 'Hackathons for all employees (not just our engineers!)', 'Write and edit custom SQL queries to generate customer-facing dashboards and reports for community oncology clinics designed to meet their clinical and operational needs', 'Professional development benefit for attending conferences, industry events and external courses', 'You have experience visualizing data in Looker dashboards', 'Database scripting and Extract, Transform and Load processes', 'You have experience with T-SQL and building reports in SQL Server Reporting Services', 'You are organized with strong prioritization and communication skills', 'Flatiron University training curriculum which includes presentation skills, meeting mastery, coding languages and more', 'Work with platform software engineers to improve the data infrastructure that powers clinical analytics ', 'We Offer', 'Database performance and interpretation of query execution plans', 'Flatiron-sponsored fitness classes', 'What You’ll Do', 'You thrive in a cross-functional environment ', 'Extra Credit', 'Analyze reporting requests to develop solutions addressing common needs across oncology practices', 'You have experience with:', 'You have experience working with Visual Studio, C#, .Net, python, or spark', 'Career coaching opportunities', ' You have experience with:', 'Why You Should Join Our Team', ' Flatiron University training curriculum which includes presentation skills, meeting mastery, coding languages and more Career coaching opportunities Hackathons for all employees (not just our engineers!) Professional development benefit for attending conferences, industry events and external courses Work/life autonomy via flexible work hours and flexible paid time off Employee Resource Groups (ERGs) that encourage our employees to share their unique experiences and perspectives Generous parental leave (16 weeks for either parent) Back-up child care Flatiron-sponsored fitness classes ', 'Build and maintain data pipelines that power parts of our analytics product', 'You love working with engineering teams to develop analytics content that complements new application features and workflows', 'You have healthcare industry knowledge/context (especially oncology-specific knowledge)', 'Employee Resource Groups (ERGs) that encourage our employees to share their unique experiences and perspectives', 'Who You Are', ' You have healthcare industry knowledge/context (especially oncology-specific knowledge) You have experience with T-SQL and building reports in SQL Server Reporting Services You have experience visualizing data in Looker dashboards You have experience working with Visual Studio, C#, .Net, python, or spark ']",Not Applicable,Full-time,Research,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Saggezza,"Chicago, IL",2 weeks ago,141 applicants,"['', 'Proficiency in modern data tools and cloud technologies would be advantageous, including but not limited to Spark, Hadoop, Tableau, AWS, Azure, Kafka, GCP & IBM Cloud.', 'Saggezza is an Equal Employment Opportunity Employer:', 'What You’ll Definitely Need', 'Entrepreneurial spirit:', 'Working hands-on with SQL/SQL server & Python to deliver analytics to clients. ', '5+ years of practical hands-on experience working within data modeling, data extraction, data manipulation, and data warehousing concepts.', 'What We’d Love To See', ""A Bachelor's in Computer Science, Information Technology, Mathematics, Engineering or an equivalent field.5+ years of practical hands-on experience working within data modeling, data extraction, data manipulation, and data warehousing concepts.Minimum of 5+ years of practical hands-on experience working with SQL/SQL Server and Python for analytics and data science purposes as well as working in a large data warehousing environment.3+ years of working with data modeling and entity-relationship diagramsExpert level experience writing complex SQL queries, including but not limited to stored procedures, functions, views, and triggers.Knowledge of indexes and how they can be used to enhance query performance.Proficiency in modern data tools and cloud technologies would be advantageous, including but not limited to Spark, Hadoop, Tableau, AWS, Azure, Kafka, GCP & IBM Cloud.Analytical mindset and business acumen. Self-motivated, individual contributor.Great communication and data-oriented personality with strong problem-solving skills."", 'At Saggezza, we are fortunate to have a strong mentorship program that provides every one of our employees the ability to thrive professionally and personally.', 'Great communication and data-oriented personality with strong problem-solving skills.', 'Why Join Our Team?', 'Experience with common data science toolkits, such as R, Weka, NumPy, MatLab, etc (Depending on specific project requirements).', 'Diverse culture, experiences, and skills.', 'Experience with analytic modeling in a scripting language (Python, R, etc.).', ""A Bachelor's in Computer Science, Information Technology, Mathematics, Engineering or an equivalent field."", 'We are not hierarchical but operate as a flat surface where every opinion matters, ideas are cultivated and innovation is encouraged.', 'We welcome innovators with entrepreneurial spirits to grow with our team. ', 'Built-In Top Places to Work in Chicago 2020', 'Examining and reporting results to stakeholders in leadership, technology, marketing, sales, and product teams.', 'Consulting Magazine - Fastest Growing Firms 2019', 'Diverse culture, experiences, and skills.Our nurturing and supportive environment fosters collaboration across the entire organization.We are not hierarchical but operate as a flat surface where every opinion matters, ideas are cultivated and innovation is encouraged.At Saggezza, we are fortunate to have a strong mentorship program that provides every one of our employees the ability to thrive professionally and personally.We are only as good as our people. Saggezza, Italian for wisdom, is rooted from the perspective that knowledge is power. We create thought-leaders who are constantly exposed and trained in different technologies in the ever-evolving world of software development.We welcome innovators with entrepreneurial spirits to grow with our team. Consulting Magazine - Fastest Growing Firms 2019Built-In Top Places to Work in Chicago 2020Best and Brightest Companies in the Nation 2019 and 2020, Best and Brightest Companies in Milwaukee 2020 and Best and Brightest Companies in Chicago 20202020 Inc. 5000 List - Honored as one of the fastest-growing private companies in America ', 'Working across multiple clients and industries to add value and strategic insights within data & analytics.', 'We are only as good as our people. Saggezza, Italian for wisdom, is rooted from the perspective that knowledge is power. We create thought-leaders who are constantly exposed and trained in different technologies in the ever-evolving world of software development.', 'Problem-solving skills: ', 'Best and Brightest Companies in the Nation 2019 and 2020, Best and Brightest Companies in Milwaukee 2020 and Best and Brightest Companies in Chicago 2020', 'Our nurturing and supportive environment fosters collaboration across the entire organization.', 'Drive: ', 'Analytical mindset and business acumen. Self-motivated, individual contributor.', 'Experience with analytic modeling in a scripting language (Python, R, etc.).An understanding of machine learning techniques and algorithms, such as k-NN, Naive Bayes, SVM, Decision Forests, etc.Experience with common data science toolkits, such as R, Weka, NumPy, MatLab, etc (Depending on specific project requirements).Cloud certification, or any certification related to database or BI Tools.', 'Providing proficiency in analyzing data and formulating insights/conclusions. ', 'Entrepreneurial spirit: We seek individuals who enjoy contributing to the growth of an organization and who show commitment to the success of their team.Problem-solving skills: Individuals at our company have well-honed analytical skills coupled with business acumen to structure problems, deliver solutions, and communicate insights.Drive: Our team sets ambitious goals and seeks energetic professionals, enjoy a fast pace environment, and thrive in taking on responsibility.', 'Minimum of 5+ years of practical hands-on experience working with SQL/SQL Server and Python for analytics and data science purposes as well as working in a large data warehousing environment.', 'Knowledge of indexes and how they can be used to enhance query performance.', 'Cleaning and preparing data for analysis and processing', 'Helping clients reach solutions by utilizing data management & operations, data quality & governance, cloud transformation, self-service analytics & visualization, and data intelligence. ', 'Expert level experience writing complex SQL queries, including but not limited to stored procedures, functions, views, and triggers.', 'Working across multiple clients and industries to add value and strategic insights within data & analytics.Cleaning and preparing data for analysis and processingProviding proficiency in analyzing data and formulating insights/conclusions. Building, developing and maintaining reporting systems that support key business decisions. Helping clients reach solutions by utilizing data management & operations, data quality & governance, cloud transformation, self-service analytics & visualization, and data intelligence. Working hands-on with SQL/SQL server & Python to deliver analytics to clients. Examining and reporting results to stakeholders in leadership, technology, marketing, sales, and product teams.', 'Data Engineer', 'Building, developing and maintaining reporting systems that support key business decisions. ', 'Drive: Our team sets ambitious goals and seeks energetic professionals, enjoy a fast pace environment, and thrive in taking on responsibility.', 'Entrepreneurial spirit: We seek individuals who enjoy contributing to the growth of an organization and who show commitment to the success of their team.', '2020 Inc. 5000 List - Honored as one of the fastest-growing private companies in America ', '3+ years of working with data modeling and entity-relationship diagrams', 'Problem-solving skills: Individuals at our company have well-honed analytical skills coupled with business acumen to structure problems, deliver solutions, and communicate insights.', 'Cloud certification, or any certification related to database or BI Tools.', 'An understanding of machine learning techniques and algorithms, such as k-NN, Naive Bayes, SVM, Decision Forests, etc.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,mParticle,United States,3 weeks ago,96 applicants,"['', 'Work with business stakeholders and product managers to define product requirementsArchitect, implement, and support scalable/reliable data pipelines and data applications in a cost efficient wayBuild, maintain, and document automated ETL pipelinesContinuously monitor and optimize the pipelines and data schemasBuild automated alerting to improve efficiency of our team’s operations, including but not limited to time series forecasting, anomaly detection, text classifications, etc.', 'Build, maintain, and document automated ETL pipelines', 'Experience in building scalable and distributed data pipelines for analytics processes and/or training machine learning models', 'Here at mParticle we embrace the differences that make us unique. We are dedicated to building an inclusive environment that fosters respect and celebrates an array of backgrounds and perspectives.', 'Ability to learn quickly and display solid analytical/engineering thinking', 'Employment opportunities are available to all applicants without regard to race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.', 'We are seeking someone who wants to be a contributor in a small, dynamic work environment, loves a challenge, and has a strong balance of technical and people skills. As a Data Engineer, you will be a member of our backend engineering team and collaborate to help specify, design, and develop data pipelines/applications meeting company and product requirements. You will help evaluate and choose service protocols and architectures, write high quality, maintainable code in a fast-paced startup environment with tight schedules, and be fully responsible for ensuring quality and proper deployment of the written software.', '\ufeffDesired Experience', 'Founded in 2013, mParticle is the leading customer data platform that unlocks the full power of data for businesses. The company empowers brands to accelerate their growth strategy to keep pace with their customers by providing the most advanced data platform for web and apps across all devices in the marketplace. A trusted partner among renowned brands such as Airbnb, Foursquare, Hulu, King, and Spotify among many others, the mParticle platform has grown to manage over 1 billion mobile users each month, capturing over $5 billion in ecommerce transactions and processes over 250 billion API calls. Recognized as one of Crain’s 100 Best Places to Work in New York City and named to Gartner’s “Cool Vendors in Mobile App Development” list, mParticle has 45 employees and is headquartered in New York City with offices in San Francisco, Florida, Seattle and London.', 'Build automated alerting to improve efficiency of our team’s operations, including but not limited to time series forecasting, anomaly detection, text classifications, etc.', ""We're looking for a talented and technically well-rounded person who loves to tackle complex problems and is passionate about building scalable and reliable data pipelines and applications, e.g., BI reporting, data transformations/integrations, machine learning, etc."", '1+ years of proven success working in backend of large-scale software developmentBS/MS in Computer Science or related fieldExpertise in SQL-like languages and toolsAbility to learn quickly and display solid analytical/engineering thinkingExperience in building scalable and distributed data pipelines for analytics processes and/or training machine learning modelsAble to design and develop quality cloud-based systems and operate them in an automated fashionDemonstrable experience in taking projects from spec to releaseWorking knowledge of Druid, Fivetran, AWS (Redshift), Looker, Spark, Luigi/Airflow, etc..', 'Expertise in SQL-like languages and tools', 'Able to design and develop quality cloud-based systems and operate them in an automated fashion', '*At this time, mParticle is unable to sponsor visas for this role, unfortunately.', 'BS/MS in Computer Science or related field', 'Working knowledge of Druid, Fivetran, AWS (Redshift), Looker, Spark, Luigi/Airflow, etc..', 'Continuously monitor and optimize the pipelines and data schemas', 'Responsibilities', 'Demonstrable experience in taking projects from spec to release', 'Architect, implement, and support scalable/reliable data pipelines and data applications in a cost efficient way', '1+ years of proven success working in backend of large-scale software development', 'Work with business stakeholders and product managers to define product requirements', 'About mParticle', 'Here at mParticle, everyone is equal. We\xa0believe strongly in our values\xa0and are looking for someone who demonstrates empathy and sincerity to all roles and teammates. Our clients include marketing and engineering functions for some of the largest apps in the world and our platform processes nearly one-third of the world’s smartphone traffic.']",Mid-Senior level,Full-time,Engineering,Internet,2021-03-24 13:05:10
Data Engineer (DE - Client Data Engineering) - Data Design & Cura,Goldman Sachs,"Dallas, TX",4 weeks ago,36 applicants,"['', ' Deep understanding of multidimensionality of data, data curation and data quality, such as traceability, security, performance latency and correctness across supply and demand processes', ' RESPONSIBILITIES AND QUALIFICATIONS ', 'About Goldman Sachs', 'Preferred Qualifications', ' In-depth knowledge of relational and columnar SQL databases, including database design', ' Excellent communications skills and the ability to work with subject matter expert to extract critical business concepts', ' Strong analytical and problem solving skills', ' A Bachelor’s degree (Masters preferred) in a computational field (Computer Science, Applied Mathematics, Engineering, or in a related quantitative discipline)', ' Engage with data consumers and producers in order to design appropriate models to suit all needs', ' Extensive knowledge and proven experience applying domain driven design to build complex business applications', ' Ability to collaborate effectively across global teams and communicate complex ideas in a simple manner', ' 2-3 years of relevant work experience in a team-focused environment', ' Experience with the Hadoop eco-system (HDFS, Spark)', ' Independent thinker, willing to engage, challenge or learn', ' Working knowledge of more than one programming language (Python, Java, C++, C#, etc.)', ' Strong work ethic, a sense of ownership and urgency', 'How You Will Fulfill Your Potential', ' Ability to stay commercially focused and to always push for quantifiable commercial impact', ' 2-3 years of relevant work experience in a team-focused environment A Bachelor’s degree (Masters preferred) in a computational field (Computer Science, Applied Mathematics, Engineering, or in a related quantitative discipline) Working knowledge of more than one programming language (Python, Java, C++, C#, etc.) Extensive knowledge and proven experience applying domain driven design to build complex business applications Deep understanding of multidimensionality of data, data curation and data quality, such as traceability, security, performance latency and correctness across supply and demand processes In-depth knowledge of relational and columnar SQL databases, including database design General knowledge of business processes, data flows and the quantitative models that generate or consume data Excellent communications skills and the ability to work with subject matter expert to extract critical business concepts Independent thinker, willing to engage, challenge or learn Ability to stay commercially focused and to always push for quantifiable commercial impact Strong work ethic, a sense of ownership and urgency Strong analytical and problem solving skills Ability to collaborate effectively across global teams and communicate complex ideas in a simple manner', ' Evaluate, select and acquire new internal & external data sets that contribute to business decision making', ' Engineer streaming data processing pipelines', 'Skills And Experience We Are Looking For', ' Financial Services industry experience Experience with the Hadoop eco-system (HDFS, Spark)', ' Deploy modern data management tools to curate our most important data sets, models and processes, while identifying areas for process automation and further efficiencies Evaluate, select and acquire new internal & external data sets that contribute to business decision making Engineer streaming data processing pipelines Drive adoption of Cloud technology for data processing and warehousing Engage with data consumers and producers in order to design appropriate models to suit all needs', ' Financial Services industry experience', ' Drive adoption of Cloud technology for data processing and warehousing', ' General knowledge of business processes, data flows and the quantitative models that generate or consume data', ' Deploy modern data management tools to curate our most important data sets, models and processes, while identifying areas for process automation and further efficiencies']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Senior Data Engineer,Appriss Retail,"Colorado, United States",1 day ago,Be among the first 25 applicants,"['', 'Functions And Responsibilities', ' Design, develop and debug ETL components utilizing AWS services  Meet the delivery expectations of the Agile Project Management methodology (1-week Data Engineering Sprint cycles)  Develop clear and concise documentation regarding technical solutions and willingness to share knowledge with teammates via “Lunch and Learns”.  Work with internal and external customers to prove requirements have been met.  Work with team to improve processes and procedures.  Work with team to make sure that 24/7 support coverage is available for On Call Rotation in areas of primary responsibility.  Other duties as assigned.', ' Meet the delivery expectations of the Agile Project Management methodology (1-week Data Engineering Sprint cycles)', ' Other duties as assigned.', 'Minimum Requirements -Education', ' Proficient developing in SQL, ideally PostgreSQL on relational database systems', ' Experience integrating AWS services with the boto3 Software Development Kit', ' Exposure to maintaining software on Linux based environments', ' You are someone who will contribute to the team’s success by completing tasks assigned on time, helping others when needed and asking for help when needed.', 'About You', ' Ability to work in an Agile environment include comfort using JIRA', ' You are someone who demonstrates the ability to communicate with both technical and non-technical professionals in an accurate and kind manner.', ' Bachelor’s Degree in a computer-related field', ' Excellent written and verbal communication skills', ' Experience with Data Visualization', ' Experience with containerization and container orchestration technologies such as Docker, Swarm, or Kubernetes', ' 3-5 years total software and relational database development experience.  2 years with a strong demonstrated ability to develop and maintain ETL solutions, ideally using Python and various Application Programming Interfaces (API)  2 of those years with a strong demonstrated ability to leverage and maintain data models in a data warehouse or data lake  Proficient developing in SQL, ideally PostgreSQL on relational database systems  Experience with containerization and container orchestration technologies such as Docker, Swarm, or Kubernetes  Ability to work in an Agile environment include comfort using JIRA  Self-motivated with a thirst to work in a dynamic and fast-pace environment leveraging modern technologies.  Understand how developer contributions are used to achieve individual goals and organizational Objectives and Key Results  Ability to explore multiple technologies at once including but not limited to Python, Ruby, NoSQL and various AWS services  Excellent written and verbal communication skills', ' Experience with Tableau, Matillion, Kubernetes', ' Ability to explore multiple technologies at once including but not limited to Python, Ruby, NoSQL and various AWS services', ' You are someone able to understand the needs of intra-company teams that depend on your work and strive to exceed expectations.', 'Physical And Mental Requirements', ' Work with team to improve processes and procedures.', ' 3-5 years total software and relational database development experience.', ' 2 years with a strong demonstrated ability to develop and maintain ETL solutions, ideally using Python and various Application Programming Interfaces (API)', ' Understand how developer contributions are used to achieve individual goals and organizational Objectives and Key Results', ' 2 of those years with a strong demonstrated ability to leverage and maintain data models in a data warehouse or data lake', 'Other', ' Develop clear and concise documentation regarding technical solutions and willingness to share knowledge with teammates via “Lunch and Learns”.', ' You are someone who can identify missing requirements and roadblocks to task completion and is able to perform research in technologies that enable the completion of team tasks.', ' Disclaimer', ' Experience integrating AWS services with the boto3 Software Development Kit  Exposure to a big data environment where scalability is a prime concept  Exposure to maintaining software on Linux based environments  Experience with Tableau, Matillion, Kubernetes  Experience utilizing Web Services, APIs and SDKs  Experience with Data Visualization', ' Work with internal and external customers to prove requirements have been met.', ' OR Bachelor’s Degree in any field and equivalent experience.', ' You are passionate about ETL, Data Modeling, Business Intelligence and Python Development.', ' Work with team to make sure that 24/7 support coverage is available for On Call Rotation in areas of primary responsibility.', 'Summary', ' Exposure to a big data environment where scalability is a prime concept', ' Experience utilizing Web Services, APIs and SDKs', 'Knowledge, Skills, Abilities, Experience, Or Characteristics', 'Helpful / Preferred', ' Self-motivated with a thirst to work in a dynamic and fast-pace environment leveraging modern technologies.', ' Design, develop and debug ETL components utilizing AWS services', ' You are someone willing and able to share skills in areas of strength and comfortable working with others or doing research into new technologies to complete team objectives in areas where you may lack experience.', ' You are passionate about ETL, Data Modeling, Business Intelligence and Python Development.  You are someone who will contribute to the team’s success by completing tasks assigned on time, helping others when needed and asking for help when needed.  You are someone who demonstrates the ability to communicate with both technical and non-technical professionals in an accurate and kind manner.  You are someone who can identify missing requirements and roadblocks to task completion and is able to perform research in technologies that enable the completion of team tasks.  You are someone willing and able to share skills in areas of strength and comfortable working with others or doing research into new technologies to complete team objectives in areas where you may lack experience.  You are someone able to understand the needs of intra-company teams that depend on your work and strive to exceed expectations.', ' Bachelor’s Degree in a computer-related field  OR Bachelor’s Degree in any field and equivalent experience.']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
AWS Data/Data Bricks Engineer,NLB Services,"Thousand Oaks, CA",22 hours ago,Be among the first 25 applicants,"['', '  8-10 years of Experience in a Large EDW project\xa0', 'Job Description:', 'Should have exposure to AWS environment.', 'Should have Life science or healthcare domain Knowledge', 'Good Knowledge in SQL', 'Deep Knowledge of any ETL Tool like Informatica\xa0\xa0', '\xa0\xa0Understand the requirements from the customer , Good Technical knowledge in AWS , Understand the requirements and program the code accordingly , work Closely with Customer to understand their Acceptance Criteria and Coordinate with them on testing , Work with Offshore and coordinate with them in the Project as this is Onsite – Offshore execution', 'Programming Experience in Databricks Python', ' ']",Associate,Contract,Business Development,Information Technology and Services,2021-03-24 13:05:10
Associate Data Engineer,EAB,"Richmond, VA",1 week ago,Be among the first 25 applicants,"['', 'Partner with multiple stakeholders including clients, new product development, BI engineers to develop scalable standard schemas', 'Dynamic growth opportunities with merit-based promotion philosophy', 'Experience working in an AGILE environment', ' Bachelor’s or Master’s degree in Computer Science or Computer Engineering Experience working with relational or multi-dimensional databases Experience developing logical data models within a data warehouse Experience developing ETL processes Demonstrated mastery in one or more SQL variants: PostgreSQL, MySQL, Oracle, SQL Server, or DB2 Demonstrated mastery in database concepts and large-scale database implementations and design patterns Proven ability to work with users to define requirements and business issues Excellent analytic and troubleshooting skills Strong written and oral communication skills  ', 'Ideal Qualifications', 'Demonstrated mastery in one or more SQL variants: PostgreSQL, MySQL, Oracle, SQL Server, or DB2', 'Support operations by identifying, researching and resolving performance and production issues', 'Codify high-performing SQL for efficient data transformation', 'About EAB', 'Primary Responsibilities', '401(k) retirement plan with company match', ' Medical, dental, and vision insurance; dependents and domestic partners eligible 401(k) retirement plan with company match 20+ days of PTO annually, in addition to paid firm holidays Daytime leave policy for community service or fitness activities (up to 10 hours a month each) Paid parental leave for birthing or non-birthing parents Phase Back to Work program for employees returning from parental leave Infertility treatment coverage and adoption or surrogacy assistance Wellness programs including gym discounts and incentives to promote healthy living Dynamic growth opportunities with merit-based promotion philosophy Benefits kick in day one, see the full details here. ', 'Work with clients to research and conduct business information flow studies', 'Associate Data Engineer', 'Medical, dental, and vision insurance; dependents and domestic partners eligible', 'Daytime leave policy for community service or fitness activities (up to 10 hours a month each)', 'Benefits kick in day one, see the full details here.', 'Excellent analytic and troubleshooting skills', 'The Role In Brief', 'GIT expertise ', 'Phase Back to Work program for employees returning from parental leave', 'Proven ability to work with users to define requirements and business issues', ' Responsible for data modeling and schema design that will range across multiple business domains within higher education Partner with multiple stakeholders including clients, new product development, BI engineers to develop scalable standard schemas Work with clients to research and conduct business information flow studies Codify high-performing SQL for efficient data transformation Coordinate work with external teams to ensure a smooth development process Support operations by identifying, researching and resolving performance and production issues ', 'Responsible for data modeling and schema design that will range across multiple business domains within higher education', '20+ days of PTO annually, in addition to paid firm holidays', 'Experience developing ETL processes', 'Experience working with relational or multi-dimensional databases', ' Experience working in an AGILE environment Experience developing commercial software products Experience with AWS infrastructure (Lambda, Python, Aurora DB) Familiar with Snowflake database GIT expertise  ', 'Bachelor’s or Master’s degree in Computer Science or Computer Engineering', 'Wellness programs including gym discounts and incentives to promote healthy living', 'Benefits', 'Coordinate work with external teams to ensure a smooth development process', 'Strong written and oral communication skills ', 'Experience with AWS infrastructure (Lambda, Python, Aurora DB)', 'Basic Qualifications', 'Experience developing logical data models within a data warehouse', 'Demonstrated mastery in database concepts and large-scale database implementations and design patterns', 'Familiar with Snowflake database', 'Infertility treatment coverage and adoption or surrogacy assistance', 'Paid parental leave for birthing or non-birthing parents', 'Experience developing commercial software products']",Associate,Part-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Junior Data Integration Engineer,Lexia Learning,"Concord, MA",4 weeks ago,Be among the first 25 applicants,"['', ' Parking & transit benefits ', 'Junior Data Integration Engineer ', 'and much more!', 'Core Responsibilities', 'Continually improves technical knowledge and service skills', ' Ability to work with customers at all levels of technical expertise and provide support accordingly ', ' Ability to speak Spanish a plus ', 'Provides roster services and data integrations support, Lexia product support, networking, hardware/software, and other technical trouble-shooting', 'Responsibilities As Part Of The Customer Success Team', ' Comprehensive health care benefits  401K with 100% matching up to 4% of salary  Flexible vacation policy and 12 paid holidays  Legal assistance  Tuition reimbursement  Parking & transit benefits  Caregiver & family support  Adoption assistance  Pet insurance ', 'Junior Data Integration Engineer', ' Legal assistance ', ' At Rosetta Stone we speak, learn, and interact differently, we embrace and thrive on these differences! We deeply benefit from the diversity that each individual has to offer. We are dedicated to fostering a culture that celebrates unique backgrounds, ideas and experiences. Rosetta Stone is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, national origin, age, disability, pregnancy, veteran status or any other status protected by federal, state or local laws.', ' Knowledge of data integration methods; previous knowledge of Clever or Class Link integration is a plus ', 'helping people change their lives through the power of language and literacy education', ' Minimum 1-2 years experience in technical customer support ', ' Experience supporting Software as a Service, web browsers, networking principles, application support, hardware device support such as phone, tablet, laptop and desktops. ', ' Must be self-directed and pro-active ', ' Adoption assistance ', 'Thinks strategically about customer needs and partners with development team to fix defects, new feature requests, improvements for the user experience, etc.Expert on all Lexia products, and latest hardware, software, and networking technologiesContinually improves technical knowledge and service skillsContinually improves company knowledge of products and servicesWorks closely with internal teams to ensure great customer experience, including tracking and reporting on technical issues for customers Has direct impact in making our customers successful through increasing their product understanding ', ' Comprehensive health care benefits ', 'Handles manual data integration and provisioning troubleshooting cases, identifying and escalating any issues to the Senior Data Integration Engineer as neededProvides technical support to customers via phone, chat, email, video conferencing, and other channelsProvides roster services and data integrations support, Lexia product support, networking, hardware/software, and other technical trouble-shooting', 'Expert on all Lexia products, and latest hardware, software, and networking technologies', 'We Are Customer Success', ' Flexible vacation policy and 12 paid holidays ', ' Ability to quickly learn new features and particulars of software applications ', 'Qualifications', 'At Rosetta Stone we speak, learn, and interact differently, we embrace and thrive on these differences! We deeply benefit from the diversity that each individual has to offer. We are dedicated to fostering a culture that celebrates unique backgrounds, ideas and experiences. Rosetta Stone is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, national origin, age, disability, pregnancy, veteran status or any other status protected by federal, state or local laws.', 'Handles manual data integration and provisioning troubleshooting cases, identifying and escalating any issues to the Senior Data Integration Engineer as needed', ' Demonstrated ability to work collaboratively ', ' Experience with Excel, SQL, and basic database concepts ', ' Excellent communication skills (written and verbal) ', 'Continually improves company knowledge of products and services', ' Familiarity with case tracking and customer relationship management software (preferably Salesforce) ', 'Benefits', ' 401K with 100% matching up to 4% of salary ', ' Caregiver & family support ', ' Minimum 1-2 years experience in technical customer support  Demonstrated phone, email and chat skills successfully supporting customers, particularly with technical questions.  Experience with Excel, SQL, and basic database concepts  Knowledge of data integration methods; previous knowledge of Clever or Class Link integration is a plus  Experience supporting Software as a Service, web browsers, networking principles, application support, hardware device support such as phone, tablet, laptop and desktops.  Comfortable in a fast paced and changing environment  Ability to quickly learn new features and particulars of software applications  Excellent communication skills (written and verbal)  Demonstrated ability to work collaboratively  Ability to work with customers at all levels of technical expertise and provide support accordingly  Must be self-directed and pro-active  Familiarity with case tracking and customer relationship management software (preferably Salesforce)  Education or EdTech experience preferred  Ability to speak Spanish a plus ', ' Tuition reimbursement ', ' Pet insurance ', 'Position Overview', 'Works closely with internal teams to ensure great customer experience, including tracking and reporting on technical issues for customers', 'Working With Us', ' Comfortable in a fast paced and changing environment ', ' Education or EdTech experience preferred ', ' Demonstrated phone, email and chat skills successfully supporting customers, particularly with technical questions. ', ' Has direct impact in making our customers successful through increasing their product understanding ', 'Thinks strategically about customer needs and partners with development team to fix defects, new feature requests, improvements for the user experience, etc.', 'Provides technical support to customers via phone, chat, email, video conferencing, and other channels']",Associate,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Software Engineer // Data,Brandfolder,"Denver, CO",3 weeks ago,45 applicants,"['', 'Legally eligible to work in the U.S. on an ongoing basis', 'Strong knack for picking up new tools and technologies, continuously learning', 'Generous paid time off', 'Parental leave policy', 'Leverage observability and monitoring tooling such as Rollbar and Datadog for delivering reliable data pipelines', 'Advanced skills in Python and SQL', 'Opportunities for professional growth and development', '5+ years of experience in data engineering rolesPositive attitude and the ability to thrive in a fast-paced environmentThe drive to take initiative and own high-impact projects from day oneAdvanced skills in Python and SQLExperience with BigQuery, Airflow, Google Cloud Storage, and PostgresExperience developing, maintaining, and monitoring data/ETL pipelinesStrong knack for picking up new tools and technologies, continuously learningHave a passion for troubleshooting and finding solutions in a production environmentLegally eligible to work in the U.S. on an ongoing basis', 'Equity - Restricted Stock Units (RSUs) Equity with all offers', 'Medical and dental insurance, 100% paid by Brandfolder', 'Lucrative Employee Stock Purchase Program (15% discount)', 'Requirements', 'Medical and dental insurance, 100% paid by BrandfolderEquity - Restricted Stock Units (RSUs) Equity with all offersLucrative Employee Stock Purchase Program (15% discount)401k Match (50% up to 6%) to help you save for your futureGenerous paid time offParental leave policyOpportunities for professional growth and developmentIconic office location in RiNo, bike and light rail friendly', 'Design, construct, install, test and maintain data management systems, especially the ETL pipeline, currently run through Fivetran', 'The drive to take initiative and own high-impact projects from day one', 'About Brandfolder', 'Experience with BigQuery, Airflow, Google Cloud Storage, and Postgres', 'Experience developing, maintaining, and monitoring data/ETL pipelines', '5+ years of experience in data engineering roles', 'Responsibilities', 'Partner with data scientists and software engineers to develop a best-in-class data architecture, prioritizing scalability, reliability, and functionality.', 'Iconic office location in RiNo, bike and light rail friendly', 'Develop set processes for data mining, data modeling, and data production.', '401k Match (50% up to 6%) to help you save for your future', 'Company Description', 'Ensure that all data systems meet the business/company requirements as well as industry practices, and are optimized for cost effectiveness.', 'Partner with data scientists and software engineers to develop a best-in-class data architecture, prioritizing scalability, reliability, and functionality.Leverage observability and monitoring tooling such as Rollbar and Datadog for delivering reliable data pipelinesEnsure that all data systems meet the business/company requirements as well as industry practices, and are optimized for cost effectiveness.Develop set processes for data mining, data modeling, and data production.Design, construct, install, test and maintain data management systems, especially the ETL pipeline, currently run through Fivetran', 'Positive attitude and the ability to thrive in a fast-paced environment', 'Job Description', 'Have a passion for troubleshooting and finding solutions in a production environment']",Mid-Senior level,Full-time,Engineering,Marketing and Advertising,2021-03-24 13:05:10
Data Analytics Engineer,Software AG,"Houston, TX",4 weeks ago,Be among the first 25 applicants,"['', ' discover potential areas of improvement ', ' Collaborate with R&D and Product Development in defining requirements for new features for TrendMiner ', 'Supported By The Customer Success Manager, Our Product Team And The Customer Success Team You Ensure a Professional And Smooth Integration Of The Added Value Of Analytics Into Successful Operations At Our Customers Using Our Own TrendMiner', ' The employees are smart, friendly and welcoming ', ' Parental Leave ', '  You support our customers in the use of TrendMiner as their prime analytics toolbox   Team up with process technologists and improvement teams of our customers to:  discover potential areas of improvement   do data analysis to find the root-cause of identified issues   suggest and implement analytics solutions   support and train users how to use TrendMiner     Support the sales process by giving demonstrations of TrendMiner   Collaborate with R&D and Product Development in defining requirements for new features for TrendMiner   Collaborate with Customer Success Manager and Sales to find new business leads and opportunities   Transfer knowledge to the team with respect to improvement results and the way the improvement is achieved  ', ' do data analysis to find the root-cause of identified issues ', ' Transfer knowledge to the team with respect to improvement results and the way the improvement is achieved ', 'What To Expect', ' suggest and implement analytics solutions ', 'Culture', ' 14 paid holidays (including a floating holiday and a community service day) ', '  Minimal level of education BSc in Engineering (Chemical, Biotechnology, Mathematical, etc.)   Experience within the use of analytics, preferably in process industry   Knowledge of analytical tools (eg. Matlab, R, Python)   2-5 years of Experience in process engineering   Full professional proficiency in English, Spanish is a plus   Excellent communication skills, customer focus and result oriented   Willingness to travel to collaborate with customers within US and abroad   Motivational and inspirational personality   Pro-active personality with hands-on attitude  ', ' Corporate discount purchase programs ', '  You support our customers in the use of TrendMiner as their prime analytics toolbox   Team up with process technologists and improvement teams of our customers to:  discover potential areas of improvement   do data analysis to find the root-cause of identified issues   suggest and implement analytics solutions   support and train users how to use TrendMiner     Support the sales process by giving demonstrations of TrendMiner   Collaborate with R&D and Product Development in defining requirements for new features for TrendMiner   Collaborate with Customer Success Manager and Sales to find new business leads and opportunities   Transfer knowledge to the team with respect to improvement results and the way the improvement is achieved   ', ' IND123', '  discover potential areas of improvement   do data analysis to find the root-cause of identified issues   suggest and implement analytics solutions   support and train users how to use TrendMiner  ', '   You support our customers in the use of TrendMiner as their prime analytics toolbox   Team up with process technologists and improvement teams of our customers to:  discover potential areas of improvement   do data analysis to find the root-cause of identified issues   suggest and implement analytics solutions   support and train users how to use TrendMiner     Support the sales process by giving demonstrations of TrendMiner   Collaborate with R&D and Product Development in defining requirements for new features for TrendMiner   Collaborate with Customer Success Manager and Sales to find new business leads and opportunities   Transfer knowledge to the team with respect to improvement results and the way the improvement is achieved    ', ' Pet Insurance ', ' 401(k) Plan with up to 5% employer match ', ' Excellent communication skills, customer focus and result oriented ', ' support and train users how to use TrendMiner ', '  The employees are smart, friendly and welcoming   Health Insurance starts your first day on the job   6 weeks of PTO – 3 weeks of vacation and 3 weeks of sick leave   14 paid holidays (including a floating holiday and a community service day)   401(k) Plan with up to 5% employer match   Parental Leave   Corporate discount purchase programs   Group legal plans   Pet Insurance   And MANY more  ', '   Minimal level of education BSc in Engineering (Chemical, Biotechnology, Mathematical, etc.)   Experience within the use of analytics, preferably in process industry   Knowledge of analytical tools (eg. Matlab, R, Python)   2-5 years of Experience in process engineering   Full professional proficiency in English, Spanish is a plus   Excellent communication skills, customer focus and result oriented   Willingness to travel to collaborate with customers within US and abroad   Motivational and inspirational personality   Pro-active personality with hands-on attitude    ', ' Collaborate with Customer Success Manager and Sales to find new business leads and opportunities ', ' Full professional proficiency in English, Spanish is a plus ', ' Your Qualities ', ' And MANY more ', ' Group legal plans ', ' Support the sales process by giving demonstrations of TrendMiner ', '  Minimal level of education BSc in Engineering (Chemical, Biotechnology, Mathematical, etc.)   Experience within the use of analytics, preferably in process industry   Knowledge of analytical tools (eg. Matlab, R, Python)   2-5 years of Experience in process engineering   Full professional proficiency in English, Spanish is a plus   Excellent communication skills, customer focus and result oriented   Willingness to travel to collaborate with customers within US and abroad   Motivational and inspirational personality   Pro-active personality with hands-on attitude   ', ' 6 weeks of PTO – 3 weeks of vacation and 3 weeks of sick leave ', ' Motivational and inspirational personality ', ' Team up with process technologists and improvement teams of our customers to:  discover potential areas of improvement   do data analysis to find the root-cause of identified issues   suggest and implement analytics solutions   support and train users how to use TrendMiner   ', 'Your Challenge ', ' Willingness to travel to collaborate with customers within US and abroad ', ' Health Insurance starts your first day on the job ', ' Knowledge of analytical tools (eg. Matlab, R, Python) ', ' You support our customers in the use of TrendMiner as their prime analytics toolbox ', ' Experience within the use of analytics, preferably in process industry ', ' Minimal level of education BSc in Engineering (Chemical, Biotechnology, Mathematical, etc.) ', ' The Company ', ' 2-5 years of Experience in process engineering ', ' Pro-active personality with hands-on attitude ']",Entry level,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Data Engineer II,CDK Global,"San Jose, CA",2 days ago,Be among the first 25 applicants,"['Work with product owners to identify and iron out upcoming business needs and develop technical backlog to answer those needs in a timely manner.', 'Evolve existing framework to support new scalability requirements as well as new functionality needed.', 'Summary:', 'Experience programming in Python, Java, writing ETL applications', 'Education:', 'If you have worked on bringing a large software product to market, or have a desire to gain this experience, this role might be perfect for you.', 'Power the PossibilitiesThe CDK Global technology team is looking for collaborative innovators who are passionate about making their mark on emerging enterprise software products. We’re building and developing cloud technology for the automotive retail industrythat will change the landscape for automotive dealers, original equipment manufacturers (OEMs) and the customers they serve.Be Part of Something BiggerEach year, more than three percent of the U.S. gross domestic product (GDP) is attributed to the auto industry, which flows through our customer, the auto dealer. It’s time you joined an evolving marketplace where research and developmentinvestment is measured in the tens of billions. It’s time you were a part of something bigger.We’re expanding our workforce – engineers, architects, developers and more – onboarding early adopters who can optimize, pivot and keep pace with ever-evolving development roadmaps and applications.Join Our TeamGrowth potential, flexibility and material impact on the success and quality of a next-gen, enterprise software product make CDK an excellent choice for those who thrive in challenging, fast-paced engineering environments.The possibilities for impact are endless. We have exceptional opportunities to evolve our industry by driving change through new technology.', 'Qualified candidates will generally have 3+ years of software development experience, including:', 'We focus on building quality software in an agile and results-oriented environment. ', 'Experience in application design and implementation using agile practices & TDD', 'Knowledge in SQL and understanding relational database', 'Experience:', 'Power the Possibilities', 'If you’re ready for high-impact, you’re ready for CDK.', 'Experience with Kafka, Hadoop, and NoSQL datastore is a plus', 'At CDK, we pride ourselves on having a diverse workforce. We value and celebrate the uniqueness of individuals and the different perspectives they provide. We offer equal opportunity employment regardless of race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability status, age, marital status, or protected veteran status.', '3+ years of software development', 'A BS or MS in Computer Science or equivalent education/experience', 'Specific responsibilities include:', '5+ years of software development experience or Masters + 2yrs in CS.Experience programming in Python, Java, writing ETL applicationsExperience with Amazon AWS services such as S3, Lambda, StepFunctionsKnowledge in SQL and understanding relational databaseExperience designing robust scalable applicationsExperience in application design and implementation using agile practices & TDDExperience with highly scalable / distributed systems desiredExperience with Kafka, Hadoop, and NoSQL datastore is a plusEnthusiasm for solving interesting and complicated problems.', 'Experience with highly scalable / distributed systems desired', 'Experience with Amazon AWS services such as S3, Lambda, StepFunctions', 'Join Our Team', 'Experience designing robust scalable applications', 'Enthusiasm for solving interesting and complicated problems.', 'Evolve existing framework to support new scalability requirements as well as new functionality needed.Work with the team to drive big data solutions.Work with product owners to identify and iron out upcoming business needs and develop technical backlog to answer those needs in a timely manner.', '5+ years of software development experience or Masters + 2yrs in CS.', 'Be Part of Something Bigger', ""The most important thing to us about you is that you have a passion for working on cool stuff and can work well with cool people. We love the energy shown in your projects (and those side projects you do, 'just for you') and we love that you can get in a room with amazing developers and learn and teach and contribute and grow. Our Agile, collaborative approach is important to everyone here."", 'CDK Global knows you have passions outside of work.  You have family, friends, sporting events, and lots of things going on.   That’s why we offer a comprehensive benefits package to not only take care of you but your family as well.   All of our benefits are effective the first day of employment including 401K matching, paid time off to re-energize, donate your time to volunteer in your community, and tuition reimbursement to name a few.', 'Work with the team to drive big data solutions.']",Not Applicable,Full-time,Information Technology,Computer Hardware,2021-03-24 13:05:10
Senior Data Engineer,REI Systems,"Alexandria, VA",7 days ago,Be among the first 25 applicants,"['', 'Desired Skills', 'Research and identify root cause/data issues.', 'Experience enhancing the performance of high-traffic sites', 'Perform data validation and quality assurance.', 'Experience supporting the US Patent and Trademark Office', 'Experience with Amazon Web Services (AWS)', 'Experience delivering solutions using Agile delivery practicesExperience with Amazon Web Services (AWS)Familiarity with revision control systems such as Git/GithubExperience enhancing the performance of high-traffic sitesExperience supporting the US Patent and Trademark Office', 'Federal government experience is strongly preferred.', 'Understand the business needs and using the knowledge of the data, prepare meaningful reports using datasets available through existing systems and datasets external to the customer unit.', 'Identify gaps in the system and communicate them to the solution project leads/program manager.', 'Understand the customers’ business processes, the underlying data, and cross-cutting data sets for various systems within the customers’ business unit.', 'Perform periodic status check-ins with the internal delivery/program manager.', 'Experience delivering solutions using Agile delivery practices', 'Document the unmet data needs of stakeholders for reporting purposes.', 'Ensure data integrity between the different cross cut reports produced through the system.', 'Responsibilities', 'Qualifications', 'Other similar professional duties maybe assigned as neededPerform hands-on work with data analysis, validation and quality assurance, while working as a full-time on-site staff at the customer facility. Candidate must have at 5-7 years of professional and proven experience in a related role.Understand the customers’ business processes, the underlying data, and cross-cutting data sets for various systems within the customers’ business unit.Understand the business needs and using the knowledge of the data, prepare meaningful reports using datasets available through existing systems and datasets external to the customer unit.Support dissemination and distribution of data and reports.Perform data validation and quality assurance.Ensure data integrity between the different cross cut reports produced through the system.Research and identify root cause/data issues.Support data analysis to study outliers and to understand the health of new programs within the customer unit.Ensure timely delivery of reports, and manage conflicting priorities and customer expectations.Perform periodic check-ins with the customer unit management for status and feedback.Perform periodic status check-ins with the internal delivery/program manager.Identify gaps in the system and communicate them to the solution project leads/program manager.Document the unmet data needs of stakeholders for reporting purposes.Expertise in one or more areas of Business Intelligence, Data Analytics, Decision Support or Business Analysis is required.Federal government experience is strongly preferred.', 'Familiarity with revision control systems such as Git/Github', 'Perform periodic check-ins with the customer unit management for status and feedback.', 'Support dissemination and distribution of data and reports.', 'Expertise in one or more areas of Business Intelligence, Data Analytics, Decision Support or Business Analysis is required.', 'Ensure timely delivery of reports, and manage conflicting priorities and customer expectations.', 'Perform hands-on work with data analysis, validation and quality assurance, while working as a full-time on-site staff at the customer facility. Candidate must have at 5-7 years of professional and proven experience in a related role.', 'Other similar professional duties maybe assigned as needed', 'Support data analysis to study outliers and to understand the health of new programs within the customer unit.', 'Required Skills']",Mid-Senior level,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Associate Data Engineer - Big Data Platforms,"Lowe's Companies, Inc.","Charlotte, NC",5 days ago,Be among the first 25 applicants,"['', 'About Lowe’s In The Community', 'Assists with translating business requirements and specifications into modules and Platform solutions with guidance from senior colleagues; provides insight into recommendations for technical solutions that meet design and functional needsSupports the development, configuration, or modification of integrated business and/or enterprise application solutions within various computing environments by leveraging various software development methodologies and programming languagesAssists in the implementation and maintenance of business data solutions to ensure successful deployment of released applications with guidance from senior colleagues as appropriateSupports systems integration testing (SIT) and user acceptance testing (UAT) with guidance from senior colleagues to ensure quality software deploymentSupports in all software development end-to-end product lifecycle phases by applying an understanding of company methodologies, policies, standards, and controlsUnderstands Computer Science and/or Computer Engineering fundamentals. Learning software architecture; actively seeks knowledge and applies to platform applicationsDrives the adoption of new technologies by researching innovative technical trends and developmentsSolves technical problems; solutions may need refinement and/or feedback from more senior level engineers', 'Performing application installs, upgrades, - patching and troubleshooting efforts', 'Key Responsibilities', 'Minimum Qualifications', 'Perform cluster maintenance, user provisioning, automation of routine tasks, troubleshooting of failed jobs, configure and maintain security policies', 'Preferred Qualifications', 'Job Summary', 'Supports systems integration testing (SIT) and user acceptance testing (UAT) with guidance from senior colleagues to ensure quality software deployment', '1 year of experience with Web Services', 'Experience with application and integration middleware', 'Platform Engineering Responsibilities', 'About Lowe’s', 'Administering Hadoop clusters in DTQ and Production environments', 'Supports in all software development end-to-end product lifecycle phases by applying an understanding of company methodologies, policies, standards, and controls', 'Assists with translating business requirements and specifications into modules and Platform solutions with guidance from senior colleagues; provides insight into recommendations for technical solutions that meet design and functional needs', '1 year of experience developing and implementing business systems within an organization', 'Understands Computer Science and/or Computer Engineering fundamentals. Learning software architecture; actively seeks knowledge and applies to platform applications', '1 year of experience in Hadoop, NO-SQL, RDBMS, Teradata, MicroStrategy or any Cloud Bigdata components', 'Solves technical problems; solutions may need refinement and/or feedback from more senior level engineers', '1 year of experience working with defect or incident tracking software', 'Experience with database technologies', 'Drives the adoption of new technologies by researching innovative technical trends and developments', 'Administering Hadoop clusters in DTQ and Production environmentsPerforming application installs, upgrades, - patching and troubleshooting effortsPerform cluster maintenance, user provisioning, automation of routine tasks, troubleshooting of failed jobs, configure and maintain security policiesHelp team to create the conceptual, logical and physical design for hybrid cloud-based solutions for infrastructure and platforms', ""Bachelor\\'s degree in Engineering, Computer Science, CIS, or related field (or equivalent work experience in a related field)"", '1 year of experience in software development or a related field', 'In most cases Lowe’s will not be able to provide sponsorship for roles located in the Tech Hub', 'Help team to create the conceptual, logical and physical design for hybrid cloud-based solutions for infrastructure and platforms', ""In most cases Lowe’s will not be able to provide sponsorship for roles located in the Tech HubMaster\\'s degree in Computer Science, CIS, or related field1 year of experience in software development or a related field1 year of experience developing and implementing business systems within an organization1 year of experience working with defect or incident tracking software1 year of experience writing technical documentation in a software development environment1 year of experience with Web ServicesExperience with application and integration middlewareExperience with database technologies1 year of experience in Hadoop, NO-SQL, RDBMS, Teradata, MicroStrategy or any Cloud Bigdata components"", 'Supports the development, configuration, or modification of integrated business and/or enterprise application solutions within various computing environments by leveraging various software development methodologies and programming languages', ""Master\\'s degree in Computer Science, CIS, or related field"", 'Assists in the implementation and maintenance of business data solutions to ensure successful deployment of released applications with guidance from senior colleagues as appropriate', '1 year of experience writing technical documentation in a software development environment']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Grant Thornton LLP,"Arlington, VA",6 days ago,Be among the first 25 applicants,"['', 'Primary Location', 'Databricks Experience Strongly Preferred', 'Work Locations']",Not Applicable,Full-time,Information Technology,Accounting,2021-03-24 13:05:10
Data Governance Engineer,DraftKings Inc.,"Boston, MA",2 days ago,Be among the first 25 applicants,"['', 'Bachelor’s degree in Computer Science, Computer Engineering, MIS, Data Engineering, or a related field. 3 years of professional experience in software programming, software development, data engineering, or a related role.Familiarity with the software development life cycle (SDLC) and change management.Development experience using languages such as Python and work in AWS.Relational database experience including schema design and SQL.Knowledge of Data Governance principles and practices. Experience with data catalogs, business glossaries, data governance, and metadata management.Ability to understand all data interdependencies in connection with business requirements.', 'Who are we a good fit for? ', 'Apply now.', 'Knowledge of Data Governance principles and practices. ', 'Bachelor’s degree in Computer Science, Computer Engineering, MIS, Data Engineering, or a related field. ', 'Support data catalog, data lineage, and other Data Governance efforts.', 'Build the possibilities at DraftKings.', 'Conduct ongoing review of Data Governance artifacts for necessary controls and technical information.', 'Continually improve the integrity of governance and data through the delivery of validated, accurate sources.', 'What You’ll Do As a Data Governance Engineer', 'Ability to understand all data interdependencies in connection with business requirements.', 'Experience with data catalogs, business glossaries, data governance, and metadata management.', 'Development experience using languages such as Python and work in AWS.', 'Assist in the integration of Data Governance controls in existing business and technical processes.', 'We are DraftKings.', 'Lead and support enterprise metadata identification and cataloging efforts.', ""Design, develop and support scripts, APIs, and other solutions to facilitate integration across tools and data sources.Build solutions to serve compliance customers' requests. Assist in the integration of Data Governance controls in existing business and technical processes.Conduct ongoing review of Data Governance artifacts for necessary controls and technical information.Lead and support enterprise metadata identification and cataloging efforts.Continually improve the integrity of governance and data through the delivery of validated, accurate sources.Support data catalog, data lineage, and other Data Governance efforts."", '3 years of professional experience in software programming, software development, data engineering, or a related role.', 'What Skills You Will Use', 'Join Us!', 'Design, develop and support scripts, APIs, and other solutions to facilitate integration across tools and data sources.', 'Relational database experience including schema design and SQL.', ""Build solutions to serve compliance customers' requests. "", 'Familiarity with the software development life cycle (SDLC) and change management.']",Entry level,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Data Engineer/Analyst,"Centurion Consulting Group, LLC","Windsor Mill, MD",5 days ago,Be among the first 25 applicants,"['', 'Key Required Skills', 'Desired Skills', 'Position Details', 'Education', 'Required']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Alterra Mountain Company,"Denver, CO",4 weeks ago,31 applicants,"['', 'ETL job scheduling tools/techniques, job control, exception handling, logging, and monitoring. ', 'PHYSICAL REQUIREMENTS ', 'Mechanical tendencies and a curiosity to know how things work and how to make them better. ', 'While performing the duties of this job, the employee is regularly required to walk, talk, see, hear, and operate a computer and other office productivity machinery. ', 'Alterra Mtn Co Shared Services Inc. and its affiliates are equal opportunity employers and maintain drug-free workplaces. All employees and candidates are reminded that Alterra Mtn Co Shared Services Inc. and its affiliates adhere to all applicable labor and employment laws, and State, County, and City-specific labor and employment regulations, where applicable.', 'Ensure proper governance of enterprise data assets including data access at the subject and row level, enforcement of data privacy (PII), protection of financial data (PCI), and country specific treatment and regional storage of data. ', ' ', 'Provide input to and execute development of logical and physical enterprise data models; enterprise master data and reference data models; metadata models; and data catalogs. Support the governance and stewarding of master and reference data. ', 'Collaborative and mentoring work style. ', 'Data Architecture, Data Management Services, Data Governance, Data Quality processes and Data Lifecycle. ', 'This job description is not an exhaustive list of all functions and responsibilities that an employee may be required to perform in this position. Alterra Mtn Co Shared Services Inc. and its affiliates reserve the right to modify, increase, decrease, suspend, and or eliminate any of the essential duties and/or the position in its entirety. ', 'Technical development, testing and deployment of T-SQL ETL pipelines and target data warehouse schemas for Alterra’s Unified Data Platform. Contribute to the design, implementation, maintenance, enhancement, monitoring and governance of enterprise data repositories Develop data processing code with a focus on consistency, reliability, and accuracy Profile inbound data and work with subject matter experts to ensure data is conformed to enterprise standards Contribute to the technical standards and data dictionaries Ensure data ingestion processes catalog and tag arriving data and provide data life cycle and version management across landing, near-term archive, long term cold storage, and data destruction events based on corporate security, compliance, and data retention policies. Ensure data pipelines provide appropriate access security, encryption (at rest and in motion) and data masking/stripping based on content and corporate security and compliance guidelines. Ensure proper governance of enterprise data assets including data access at the subject and row level, enforcement of data privacy (PII), protection of financial data (PCI), and country specific treatment and regional storage of data. Collaborate with teams that manage operational data masters and execute the design and development of data mastering processes. Provide input to and execute development of logical and physical enterprise data models; enterprise master data and reference data models; metadata models; and data catalogs. Support the governance and stewarding of master and reference data. Support ongoing development and code reviews of data acquisition, data movement, data cleansing, data transformation, data mapping, data quality screens, ETL jobs and schedules, and other ETL and data integration activities. Ensure that ETL jobs are scheduled, monitored and generate detailed logs to support ongoing diagnostics, exception processing, and audit trails for compliance. Manage the packaging of code assets, models, configurations, schemas and migration instructions to support updates to development, test and production environments. Support data integration and business intelligence teams. Provide input to and execute development and enforcement of naming conventions for enterprise data assets including data models; database, schema, table, view, index, trigger, stored proc/function names; object level storage container names and paths; file names; and ETL scripts. Monitor and provide input into the enterprise IT architecture including cloud infrastructure and connectivity; database architecture and connectivity; external data connectivity (S/FTP, APIs); security, backup and DR as these subject areas relate to the enterprise information architecture. Provide input to and execute development of design patterns and best practices for data integration and data analysis across the enterprise. ', 'Cloud-based data platforms such as Azure and AWS; cloud security (accounts, users, groups, roles); logging and monitoring; lambdas and functions; data versioning and life cycle management; infrastructure-as-code. ', 'Monitor and provide input into the enterprise IT architecture including cloud infrastructure and connectivity; database architecture and connectivity; external data connectivity (S/FTP, APIs); security, backup and DR as these subject areas relate to the enterprise information architecture. ', 'Experience with master data management ', 'WORKING CONDITIONS ', 'Code and infrastructure-as-code testing techniques including unit, integration, system, performance/stress, and acceptance tests. ', 'General Responsibilities', 'Provide input to and execute development and enforcement of naming conventions for enterprise data assets including data models; database, schema, table, view, index, trigger, stored proc/function names; object level storage container names and paths; file names; and ETL scripts. ', 'Indoor/Outdoor: While performing the duties of this job, the employee may be exposed to harsh and varying outside weather conditions. ', 'This position is located in Colorado, and the work is primarily in Denver, CO and, as such, employment in this position is subject to the labor and employment laws of the state of Colorado.', '5+ years working with SQL, preferably MSSQL 3+ years working with SQL in a data warehouse environment 3+ years building data platforms (architecture, storage, management, monitoring) 3+ years developing in a formal SDLC environment, using GitHub, Jira, Confluence or similarly tools 3+ years building information architectures 3+ years data profiling & data mining 1+ years hands-on experience with Azure Business Process Management process and application experience Experience with Informatica & SSIS is a plus Experience with metadata applications and solutions Experience with master data management Strong project management and organizational skills. Collaborative and mentoring work style. Strong analytic skills and hands-on attention to detail. Mechanical tendencies and a curiosity to know how things work and how to make them better. Passion for statistics & analytics, process engineering, and information management. Loves being hands-on in a fast-paced, entrepreneurial environment. ', 'Master data management using mapping tables and/or commercial MDM tools. ', 'Conceptual understanding of advanced analytic and machine learning data processing requirements. ', 'To perform this job successfully, an individual must be able to perform each essential duty satisfactorily.  Reasonable accommodations may be made to enable individuals with disabilities to perform the essential duties.', 'Support data integration and business intelligence teams. ', 'Design and construction of information architectures that enable well-integrated transactional, collaborative and analytical systems. ', 'The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job.  Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. ', 'WORKING CONDITIONS', 'Hazardous Materials/Noise: The noise level in the work place is usually moderate. ', 'Various data connectivity techniques including FTP and APIs, ', 'To perform this job successfully, an individual must be able to perform each essential duty satisfactorily.  Reasonable accommodations may be made to enable individuals with disabilities to perform the essential duties. ', 'Experience with metadata applications and solutions ', 'Additional experiences in the following areas are beneficial: ', 'General Responsibilities ', 'This position is located in Colorado, and the work is primarily in Denver, CO and, as such, employment in this position is subject to the labor and employment laws of the state of Colorado. ', 'Strong project management and organizational skills. ', 'SDLC concepts including application lifecycle management, release management, and optionally continuous delivery. ', 'PowerBI, MSFT SSRS and other reporting & visualization tools ', 'Commercial ETL tools such as SSIS, Talend, Informatica and/or the use of 3GL languages such as Python to implement ETL services. ', '3+ years building data platforms (architecture, storage, management, monitoring) ', 'POSITION SUMMARY', 'Employment with Alterra Mtn Co Shared Services Inc. or any of its affiliates is “at will” meaning either party may terminate the employment relationship at any time with or without cause and with or without notice. ', 'Undergraduate Major preference: Business, IS/IT, Computer Science, related field and/or experience Commercial ETL tools such as SSIS, Talend, Informatica and/or the use of 3GL languages such as Python to implement ETL services. Design and construction of information architectures that enable well-integrated transactional, collaborative and analytical systems. Schema-on-read query paradigm, columnar file formats such as Parquet/AVRO, compression and partitioning techniques. SDLC concepts including application lifecycle management, release management, and optionally continuous delivery. Data Architecture, Data Management Services, Data Governance, Data Quality processes and Data Lifecycle. SQL databases, DDL, DML, DQL, DCL, and TCL, triggers, stored procedures/functions, indexes, sequences, table schema types, transactions, and replication. Enterprise level data modeling at the logical and physical levels for 3NF, star schema, slowly changing data. Version management via a version control system/repository. Data cataloging and metadata management. Master data management using mapping tables and/or commercial MDM tools. ETL job scheduling tools/techniques, job control, exception handling, logging, and monitoring. ETL workflow design including change data capture, transformations, mapping, and data quality screens. Various data connectivity techniques including FTP and APIs, Cloud-based data platforms such as Azure and AWS; cloud security (accounts, users, groups, roles); logging and monitoring; lambdas and functions; data versioning and life cycle management; infrastructure-as-code. ', '3+ years data profiling & data mining ', 'Enterprise level data modeling at the logical and physical levels for 3NF, star schema, slowly changing data. ', 'Strong analytic skills and hands-on attention to detail. ', '1+ years hands-on experience with Azure ', 'Develop data processing code with a focus on consistency, reliability, and accuracy ', 'Data cataloging and metadata management. ', 'KNOWLEDGE, SKILL AND ABILITY REQUIREMENTS', 'Ensure that ETL jobs are scheduled, monitored and generate detailed logs to support ongoing diagnostics, exception processing, and audit trails for compliance. ', ' ESSENTIAL DUTIES', 'PHYSICAL REQUIREMENTS', 'Employment with Alterra Mtn Co Shared Services Inc. or any of its affiliates is “at will” meaning either party may terminate the employment relationship at any time with or without cause and with or without notice.', 'Alterra is looking for a Strong SQL Server database developer with advanced query and stored procedure experience for ETL/ELT use cases to join our Unified Data Platform team. Bonus for experience with ETL tools and recent version of SQL Server 2017 or higher or Azure SQL Database.  You will be primarily responsible for profiling source data, building new T-SQL transformations, defining and building target data schemas in support of strategic data projects. ', 'PowerBI, MSFT SSRS and other reporting & visualization tools Code and infrastructure-as-code testing techniques including unit, integration, system, performance/stress, and acceptance tests. Database schema migration techniques. Conceptual understanding of advanced analytic and machine learning data processing requirements. Serverless and microservice architectures and techniques. Organizing data at scale including data lakes, data marts, and data warehouses. ', 'This job description is not an express or implied contract, guarantee, promise, or covenant of employment for any set term or duration, or for termination only for cause. ', 'Passion for statistics & analytics, process engineering, and information management. ', 'Profile inbound data and work with subject matter experts to ensure data is conformed to enterprise standards ', 'Business Process Management process and application experience ', 'Experience with Informatica & SSIS is a plus ', 'Manage the packaging of code assets, models, configurations, schemas and migration instructions to support updates to development, test and production environments. ', 'POSITION SUMMARY ', 'This job description is not an express or implied contract, guarantee, promise, or covenant of employment for any set term or duration, or for termination only for cause.', 'Education & Experience', 'Version management via a version control system/repository. ', 'EDUCATION & EXPERIENCE REQUIREMENTS', 'Undergraduate Major preference: Business, IS/IT, Computer Science, related field and/or experience ', '3+ years building information architectures ', 'Schema-on-read query paradigm, columnar file formats such as Parquet/AVRO, compression and partitioning techniques. ', 'Database schema migration techniques. ', 'Ensure data ingestion processes catalog and tag arriving data and provide data life cycle and version management across landing, near-term archive, long term cold storage, and data destruction events based on corporate security, compliance, and data retention policies. ', 'Contribute to the technical standards and data dictionaries ', 'Collaborate with teams that manage operational data masters and execute the design and development of data mastering processes. ', 'Technical development, testing and deployment of T-SQL ETL pipelines and target data warehouse schemas for Alterra’s Unified Data Platform. ', '5+ years working with SQL, preferably MSSQL ', 'Support ongoing development and code reviews of data acquisition, data movement, data cleansing, data transformation, data mapping, data quality screens, ETL jobs and schedules, and other ETL and data integration activities. ', '3+ years developing in a formal SDLC environment, using GitHub, Jira, Confluence or similarly tools ', 'Contribute to the design, implementation, maintenance, enhancement, monitoring and governance of enterprise data repositories ', 'Organizing data at scale including data lakes, data marts, and data warehouses. ', 'Loves being hands-on in a fast-paced, entrepreneurial environment. ', 'ETL workflow design including change data capture, transformations, mapping, and data quality screens. ', 'This job description is not an exhaustive list of all functions and responsibilities that an employee may be required to perform in this position. Alterra Mtn Co Shared Services Inc. and its affiliates reserve the right to modify, increase, decrease, suspend, and or eliminate any of the essential duties and/or the position in its entirety.', '3+ years working with SQL in a data warehouse environment ', 'SQL databases, DDL, DML, DQL, DCL, and TCL, triggers, stored procedures/functions, indexes, sequences, table schema types, transactions, and replication. ', 'Serverless and microservice architectures and techniques. ', 'Education & Experience ', 'Ensure data pipelines provide appropriate access security, encryption (at rest and in motion) and data masking/stripping based on content and corporate security and compliance guidelines. ', 'Provide input to and execute development of design patterns and best practices for data integration and data analysis across the enterprise. ']",Mid-Senior level,Full-time,Information Technology,Hospitality,2021-03-24 13:05:10
Data Engineer,ADM,"Erlanger, KY",4 weeks ago,Be among the first 25 applicants,"['', ' Strong collaboration skills working with design and development teams ', ' Establish and manage collaborations engaging business units to develop novel data analytic approaches and coordinated decision science solutions ', ' Experience with metadata management discipline and practices ', ' Self-starter who is organized , communicative, quick learner, and team-oriented ', ' Establishing and maintaining information security standards ', ' 1-3 years of work experience or equivalent academic background in data processing using Python ', ' Knowledge in programming scripting languages like Java, Scala and Python ', ' Experience delivering Cloud based Data Solutions in Azure ', ' Working knowledge of BI architecture, data warehousing concepts, and data integration standard methodologies ', ' Mapping data ecosystems and creating data model diagrams ', ' Experience working with Big Data technologies (Spark, Kafka, DataBricks, Hive, or equivalent) ', ' Understanding of software development principles such as project architecture, version control (Git), test-driven development, etc. ', ' 5+ years of hands-on experience crafting and implementing data and analytics solutions ', ' Work with data warehouse and data integration teams to ensure successful delivery of enterprise data warehouse solutions ', ' Fluency in SQL ', ' High accountability with a demonstrated ability to deliver ', ' Motivated, demonstrates initiative and leadership ', ' Automating data flows with resilient, production-grade code ', ' This is an exempt level position. ', 'Essential Job Functions', ' 4-year Bachelor’s degree or equivalent in IT, Computer Science, science, engineering, statistics, programming or mathematical field ', ' Creating custom queries, scripts, and job runs for ad hoc data processing and/or data investigation in Python ', ' Monitor and troubleshoot data issues in solution pipelines ', ' Learn new groundbreaking data engineering and analytic tools as needed ', ' Possesses a professional attitude ', ' Understanding and navigating a wide array of source data systems (enterprise data warehouses, relational databases, IT systems, in house and COTS applications, documents, APIs, unstructured data, big data, NoSQL databases, etc.) ', ' Lead development of strategy in an environment rich in complex biological, environmental, operational, global economic, and business data ', 'Data Engineer- Erlanger, KY ', ' Strong written communication skills including functional design documentation ', ' Prepare and present ideas and recommendations to colleagues and management ', ' Required education: ', ' Work simultaneously on multiple projects without sacrificing delivery  Work with data warehouse and data integration teams to ensure successful delivery of enterprise data warehouse solutions  Have natural curiosity and real passion for data investigation, talent for strategic leadership, interest in digital product development, and experience in people management  Lead development of strategy in an environment rich in complex biological, environmental, operational, global economic, and business data  Establish and manage collaborations engaging business units to develop novel data analytic approaches and coordinated decision science solutions  Self-starter who is organized , communicative, quick learner, and team-oriented  Prepare and present ideas and recommendations to colleagues and management  Document solutions through high/low level design documentation  Learn new groundbreaking data engineering and analytic tools as needed ', ' 5+ years of hands-on experience crafting and implementing data and analytics solutions  1-3 years of work experience or equivalent academic background in data processing using Python  Fluency in SQL  Experience delivering Cloud based Data Solutions in Azure  Experience working with Big Data technologies (Spark, Kafka, DataBricks, Hive, or equivalent)  Knowledge in programming scripting languages like Java, Scala and Python  Experience in delivering solutions using iterative development methodologies like Agile, KanBan, DevOps, etc.  Working knowledge of BI architecture, data warehousing concepts, and data integration standard methodologies  Experience with metadata management discipline and practices  High accountability with a demonstrated ability to deliver  Strong written communication skills including functional design documentation  Strong collaboration skills working with design and development teams  Motivated, demonstrates initiative and leadership  Able to critically think and be solution-driven with strong interpersonal skills and able to work easily with team members  Great organizational, time management and problem-solving skills  Possesses a professional attitude ', ' Document solutions through high/low level design documentation ', ' Work simultaneously on multiple projects without sacrificing delivery ', ' Experience in delivering solutions using iterative development methodologies like Agile, KanBan, DevOps, etc. ', ' Analyzing data and developing insights (e.g., via data visualization tools like Excel/Power BI/Tableau ', 'EEO', ' Able to critically think and be solution-driven with strong interpersonal skills and able to work easily with team members ', ' Developing prototypes and proof of concepts visualizations/dashboards for the selected solutions ', ' Have natural curiosity and real passion for data investigation, talent for strategic leadership, interest in digital product development, and experience in people management ', ' Great organizational, time management and problem-solving skills ', ' Creating custom queries, scripts, and job runs for ad hoc data processing and/or data investigation in Python  Understanding and navigating a wide array of source data systems (enterprise data warehouses, relational databases, IT systems, in house and COTS applications, documents, APIs, unstructured data, big data, NoSQL databases, etc.)  Understanding of software development principles such as project architecture, version control (Git), test-driven development, etc.  Automating data flows with resilient, production-grade code  Monitor and troubleshoot data issues in solution pipelines  Developing prototypes and proof of concepts visualizations/dashboards for the selected solutions  Establishing and maintaining information security standards  Mapping data ecosystems and creating data model diagrams  Extending traditional ETL solutions (Informatica, Alteryx, SQL, etc.)  Analyzing data and developing insights (e.g., via data visualization tools like Excel/Power BI/Tableau ', ' Extending traditional ETL solutions (Informatica, Alteryx, SQL, etc.) ', 'Additional Responsibilities Of This Role Will Include The Following', 'About ADM', 'Position Summary', ' Job Requirements: ']",Not Applicable,Full-time,Information Technology,Food Production,2021-03-24 13:05:10
Senior Data Engineer,Beyond Finance,"Chicago, IL",2 days ago,Be among the first 25 applicants,"['', 'Experience in setting up configuration-as-code tools such as Ansible, Terraform, or Chef.', 'What We Look For', ""What You'll Do\xa0"", '5-8 years of experience working on building services or data integrations; Airflow experience is a plus', 'Design and build data pipelines while thinking of creating standards and common tools to be as productive as possible', 'Experience working with cloud platforms such as AWS, Azure and GCP', 'Beyond Finance is an exciting next generation FinTech company headquartered in Houston with additional offices in San Diego and Chicago. We provide financial products that are easy to use and accessible that aim to improve people’s lives. As the consumer debt market continues to grow in the United States, we have made it our objective to create a transparent and fair debt management service that is customized to the consumer’s individual situation.', 'Good knowledge of a relational database platform such as MySQL, Postgres, Redshift, Snowflake, Oracle or SQL Server; any cloud database technologies is a plus', 'About The Role', ""Bachelor's degree in Computer Science or other technical field or equivalent work experience5-8 years of experience working on building services or data integrations; Airflow experience is a plusSolid Python coding and scripting experienceGood knowledge of a relational database platform such as MySQL, Postgres, Redshift, Snowflake, Oracle or SQL Server; any cloud database technologies is a plusExperience working with cloud platforms such as AWS, Azure and GCPExposure in developing batch systems and real time streaming platforms and a sense of the benefits of each approach.Excellent SQL experience working through complex queriesExperience in setting up configuration-as-code tools such as Ansible, Terraform, or Chef.Ability to work in a fast-paced environment where continuous innovation is desired, and ambiguity is the normExperience with agile or Kanban development methodologies"", 'Experience with agile or Kanban development methodologies', 'Think about how to handle security issues surrounding access, PII, and business-sensitive data.', 'As a Data Engineer you will work on developing a modern data pipeline integrating data from a variety of data sources in order to bring the data into the analytics platform using highly scalable and extensible pipeline technologies.\xa0\xa0You’ll be owning small to medium sized projects which ultimately will help the business grow and make more informed decisions.', 'Own small to medium sized projects independently', ""Bachelor's degree in Computer Science or other technical field or equivalent work experience"", 'Beyond Finance is committed to employee health and safety. We’ve built and implemented a plan to conduct remote interviewing and onboarding. Although we’d like to get back to the office one day, we’re committed to doing so in a safe, responsible manner to ensure minimum risk for our employees and their families', 'Excellent SQL experience working through complex queries', 'Exposure in developing batch systems and real time streaming platforms and a sense of the benefits of each approach.', 'Ability to work in a fast-paced environment where continuous innovation is desired, and ambiguity is the norm', 'Learn how to take deep dives into technologies and troubleshoot issues to understand the root cause', 'Be a team player in contributing your thoughts and ideas to the overall goals of the team', 'Solid Python coding and scripting experience', 'Hiring During the Pandemic', 'Design for scalability and robustness of availability ensuring the data pipelines work or provide a clear error when they can’t proceed.', 'Work with the latest cutting edge technologies which include AWS, Snowflake, Airflow and Terraform', 'Work closely with the Business Intelligence team to deliver them the data they need to answer business questions.', 'Integrate all our data sources under one data warehouse which will allow our business to examine what is going on in endless ways.', 'Our leadership team has extensive experience and expertise in the financial services and debt management marketplace. We are reinventing the market by investing in innovative technologies to best serve our customers. Our workforce is a community made up of celebrated, diverse backgrounds working towards the common objective of developing something transformational for the debt settlement industry.', 'Own small to medium sized projects independentlyWork with the latest cutting edge technologies which include AWS, Snowflake, Airflow and TerraformIntegrate all our data sources under one data warehouse which will allow our business to examine what is going on in endless ways.Be a team player in contributing your thoughts and ideas to the overall goals of the teamWork closely with the Business Intelligence team to deliver them the data they need to answer business questions.Design and build data pipelines while thinking of creating standards and common tools to be as productive as possibleLearn how to take deep dives into technologies and troubleshoot issues to understand the root causeThink about how to handle security issues surrounding access, PII, and business-sensitive data.Design for scalability and robustness of availability ensuring the data pipelines work or provide a clear error when they can’t proceed.']",Director,Full-time,Information Technology,Financial Services,2021-03-24 13:05:10
Data Engineer I,Memorial Health System (IL),"Springfield, IL",4 weeks ago,Be among the first 25 applicants,"['Develops fit for purpose, resilient, scalable and future-proof data services to meet user needs.', 'Other Knowledge/Skills/Abilities:', 'Education:', 'Has a working knowledge of a range of coding tools and languages. Specifically must be well versed in SQL Management Studio and TSQL, and have an awareness or ability to learn Oracle SQL Developer, PL-SQL, and Postgres SQL. Must also be able to identify the applicability of, and appropriate utilize these various coding and programming languages, often with the assistance of senior data engineers or the data engineering manager. Understands security, accessibility and version control.', 'Has a working knowledge of handling large and diverse datasets, parsing and understanding the data, and then building, managing and operationalizing data pipelines. Must be able to, independently or with guidance, provision datasets quickly across multiple, distributed and often diverse datasets within and outside the organizational boundaries.Has a working knowledge of a range of coding tools and languages. Specifically must be well versed in SQL Management Studio and TSQL, and have an awareness or ability to learn Oracle SQL Developer, PL-SQL, and Postgres SQL. Must also be able to identify the applicability of, and appropriate utilize these various coding and programming languages, often with the assistance of senior data engineers or the data engineering manager. Understands security, accessibility and version control.Will work side by side with domain experts in business groups and with data scientists and analysts to frame the business problem, integrate the needed data, and determine the best way to provision that data on demand, often with the assistance of senior data engineers or the data engineering manager.Working experience with cloud technologies is not required, but is preferred. Is able to communicate effectively across organizational and technical boundaries, making complex and technical information and language simple and accessible for non-technical audiences.Is aware of and keeps up to date with advances in digital analytics tools and data manipulation products. Has an awareness of the process and or team resources available to logs, analyze and manage problems in order to identify and implement the appropriate solution. Ensures that the problem is fixed, often with the assistance of senior data engineers or the data engineering manager.Have an awareness of the skills and concepts required to build, populate and manage visualization and/or reporting assets. Tableau experience is preferred. Demonstrated problem solving skills and ability to consistently exercise sound judgment and initiative in all circumstances, including very stressful situations. Demonstrated effective written and verbal communication in performing assigned duties.Demonstrated ability to communicate effectively with all levels of staff both in and outside of IT essential.', 'Builds, manages and optimizes data pipelines and moves them into production.', 'Under the direct supervision of the manager of data engineering, responsible for engineering, testing and maintaining the functionality and integrity of data pipelines and other data assets in order to make data accessible and useful for data analytics consumers throughout Memorial Health System. ', 'Troubleshoots problems and tunes for performance. ', 'Integrates and separates data feeds in order to map, produce, transform and test new data assets.', 'Has a working knowledge of handling large and diverse datasets, parsing and understanding the data, and then building, managing and operationalizing data pipelines. Must be able to, independently or with guidance, provision datasets quickly across multiple, distributed and often diverse datasets within and outside the organizational boundaries.', 'Performs other related work as required or requested.', 'Experience:', 'Designs, writes and iterates code from prototype to production-ready. ', 'Licensure/Certification/Registry:', 'Demonstrated effective written and verbal communication in performing assigned duties.', '\xa0', 'Certifications in SQL or Data Engineering (such as those offered by Microsoft or Google) preferred', 'Education equivalent to an Associate’s degree in computer science, statistics, applied mathematics, data management, information systems, information science or a related quantitative field is required. Bachelor’s degree is preferred.3 Years of Applied Data Engineering experience required in lieu of educational requirements. ', 'Will work side by side with domain experts in business groups and with data scientists and analysts to frame the business problem, integrate the needed data, and determine the best way to provision that data on demand, often with the assistance of senior data engineers or the data engineering manager.', 'Collects, collates, cleanses and synthesizes data to derive meaningful and actionable insights.', 'Demonstrated problem solving skills and ability to consistently exercise sound judgment and initiative in all circumstances, including very stressful situations. ', 'Is aware of and keeps up to date with advances in digital analytics tools and data manipulation products. ', 'Develops and manages data asset documentation.', 'Participates in development opportunities including but not limited to quality and leadership classes as determined by manager and employee. (MHS, vendor, or scholastic provider; member of professional society).', '3 Years of Applied Data Engineering experience required in lieu of educational requirements. ', 'Is able to communicate effectively across organizational and technical boundaries, making complex and technical information and language simple and accessible for non-technical audiences.', 'Education equivalent to an Associate’s degree in computer science, statistics, applied mathematics, data management, information systems, information science or a related quantitative field is required. Bachelor’s degree is preferred.', 'At least one year or more of work experience in data management disciplines including data integration, report writing, data quality, and/or other areas directly relevant to data engineering responsibilities and tasks.', 'Have an awareness of the skills and concepts required to build, populate and manage visualization and/or reporting assets. Tableau experience is preferred. ', 'Has an awareness of the process and or team resources available to logs, analyze and manage problems in order to identify and implement the appropriate solution. Ensures that the problem is fixed, often with the assistance of senior data engineers or the data engineering manager.', 'Working experience with cloud technologies is not required, but is preferred. ', 'Demonstrated ability to communicate effectively with all levels of staff both in and outside of IT essential.', 'Designs, writes and deploys data visualization and data reporting assets. ', 'At least one year or more of work experience in data management disciplines including data integration, report writing, data quality, and/or other areas directly relevant to data engineering responsibilities and tasks.At least one year of experience working in cross-functional teams and collaborating with business stakeholders.', 'At least one year of experience working in cross-functional teams and collaborating with business stakeholders.']",Entry level,Full-time,Information Technology,Nonprofit Organization Management,2021-03-24 13:05:10
Analytics Data Engineer (Remote) - Operations,Progressive Leasing,"Draper, UT",4 days ago,35 applicants,"['', 'Work in a self-directed but not isolated manner, proactively collaborating with the Operations Analyst team to drive business valueEnsure data pipelines to operations analysis environments are stable, controlled, and accurateQuickly deliver new application data to the data warehouse for data analystsUnderstand data flow from application to data warehouse so you can guarantee data delivery at the requisite velocity for near-real time dashboardsCollaborate with the operations analysts to source and make available data that drives business valueActively work on migrating from SQL Server to the new cloud data storage platform (combination of Databricks’ DeltaLake & Snowflake on AzureSupport legacy SQL Server stored procedures while migrating to a combination of Python, Airflow, DBT, Spark, and Snowflake code', 'Tuition Reimbursement', 'Job required equipment and services', 'Employee Stock Purchase Program', 'Progressive Leasing welcomes and encourages diversity in the workplace. We do not discriminate in any aspect of employment on the basis of race, color, religion, national origin, ancestry, gender, sexual orientation, gender identity and/or expression, age, veteran status, disability, or any other characteristic protected by federal, state, or local employment discrimination laws where Progressive Leasing does business.', 'Ensure data pipelines to operations analysis environments are stable, controlled, and accurate', 'Competitive Compensation + Eligible for STI Full Health Benefits; Medical/Dental/Vision/Life Insurance + Paid Parental LeaveCompany Matched 401kPaid Time Off + Paid Holidays + Paid Volunteer HoursDiversity Alliance Resource GroupsEmployee Stock Purchase ProgramTuition ReimbursementCharitable Gift Matching Job required equipment and services', 'Actively work on migrating from SQL Server to the new cloud data storage platform (combination of Databricks’ DeltaLake & Snowflake on Azure', 'Support legacy SQL Server stored procedures while migrating to a combination of Python, Airflow, DBT, Spark, and Snowflake code', 'Who We Are', 'Full Health Benefits; Medical/Dental/Vision/Life Insurance + Paid Parental Leave', 'Ability to understand and explain multiple data storage technologies at a high level', 'Required Qualifications', '2+ years of professional experience in data & analytics or software engineering ', ' Analytics Data Engineer (Remote) - Operations,', 'This role can be performed remotely anywhere in the US or from one of our office locations in Draper, UT or Glendale, AZ.', 'Solid knowledge of SQL and some experience in 1 other programming language (Python, R, Java, C#, PowerShell, Bash, etc.)', 'Work in a self-directed but not isolated manner, proactively collaborating with the Operations Analyst team to drive business value', 'Charitable Gift Matching ', 'Competitive Compensation + Eligible for STI ', 'Bachelor’s degree in a technical field (Engineering, Computer Science, Information Systems, etc.)2+ years of professional experience in data & analytics or software engineering 1+ years moving data into a data lake & transforming it for downstream analysisAbility to understand and explain multiple data storage technologies at a high levelKnowledge of tuning and performance optimization techniques in 1 technology (Hadoop, SQL Server, Cosmos DB, Snowflake, Azure Data Lake Gen2, etc.)Solid knowledge of SQL and some experience in 1 other programming language (Python, R, Java, C#, PowerShell, Bash, etc.)Experience writing data pipelines and ETL code, ideally on a major cloud provider (GCP, AWS, Azure, etc.)', 'What We Offer', 'Company Matched 401k', 'Quickly deliver new application data to the data warehouse for data analysts', 'Knowledge of tuning and performance optimization techniques in 1 technology (Hadoop, SQL Server, Cosmos DB, Snowflake, Azure Data Lake Gen2, etc.)', '1+ years moving data into a data lake & transforming it for downstream analysis', 'Collaborate with the operations analysts to source and make available data that drives business value', 'Understand data flow from application to data warehouse so you can guarantee data delivery at the requisite velocity for near-real time dashboards', 'Diversity Alliance Resource Groups', 'Bachelor’s degree in a technical field (Engineering, Computer Science, Information Systems, etc.)', 'THE DETAILS', 'Paid Time Off + Paid Holidays + Paid Volunteer Hours', 'Essential Functions', 'Experience writing data pipelines and ETL code, ideally on a major cloud provider (GCP, AWS, Azure, etc.)']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer | Restaurantes,Rappi,"Location, WV",2 days ago,Be among the first 25 applicants,"['', 'ETL, ELT, datapipelines.', 'Airflow', ' Responsibilities ', 'Build and maintain the ETLs from Production to the analytics platform.', 'SparkSQLPython: Flask, pandas, pyspark, sklearn, numpy, keras, tensorflowETL, ELT, datapipelines.Data lakes, datawarehouse y datamarkets.ETL orchestrator', ' Required ', 'Work together with developers, tech leads and solution architects to build applications.', 'Spark', 'SQL', 'AirflowJava/Kotlin, Scala Golang, Snowflake, R, BigQuery, MLFLow o similares', 'Python: Flask, pandas, pyspark, sklearn, numpy, keras, tensorflow', 'Strive to get everything working on automatic ways.', 'Build and maintain Datalake / DW and Datamarts for the different use cases.', 'ETL orchestrator', ' Plus ', 'The result of your work will also allow you to improve the user experience on scalable and high availability platforms, contributing to the key differential of each business.', 'Make the right data accessible in the right way to the different groups of users.', 'Work together with developers, tech leads and solution architects to build applications.Improve existing structures by adding new functionalities or proposing technological updates so that as a result, the impact of your contribution is significant in the core of the business.The result of your work will also allow you to improve the user experience on scalable and high availability platforms, contributing to the key differential of each business.Build and maintain the ETLs from Production to the analytics platform.Build and maintain Datalake / DW and Datamarts for the different use cases.Make the right data accessible in the right way to the different groups of users.Strive to get everything working on automatic ways.Investigate and test new technologies that could improve our processes.Provide a secure data environment to comply with regulations on different countries. ', 'Data lakes, datawarehouse y datamarkets.', 'Provide a secure data environment to comply with regulations on different countries. ', 'Java/Kotlin, Scala Golang, Snowflake, R, BigQuery, MLFLow o similares', 'Improve existing structures by adding new functionalities or proposing technological updates so that as a result, the impact of your contribution is significant in the core of the business.', 'Investigate and test new technologies that could improve our processes.']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Analytics Engineer,Novanta Inc.,"Will-More, VA",7 days ago,34 applicants,"['', 'None', 'Convert raw data into consumable information applying business logic and utilizing clean engineering workflows', '3+ years - Experience with MS SQL Server and Snowflake', 'Motivated individual with strong analytic, problem solving, and troubleshooting skills', ' Bachelor’s degree in Information Technology ', 'Experienced building data warehouse infrastructure and BI tables', 'Provide exceptional customer service to stakeholders through project execution and timely delivery of solutions', 'Automate standard report creation and sharing using tools or scripts', 'Primary Responsibilities', 'Interface with business customers to gather data and metrics requirements, then driving analytic projects to solve complex challengesDraw insights from data and clearly communicate findings to stakeholders and external customersProvide exceptional customer service to stakeholders through project execution and timely delivery of solutions', 'This position can be based from a Remote home office anywhere in the U.S.', 'Job Summary', 'Communication', 'Analyze assigned projects for data quality issues. Troubleshoot and resolve issues as they arise.', ' Bachelor’s degree in Information Technology 3+ years - Experience with MS SQL Server and Snowflake3+ years - Experience with ETL/ELT Tools (ex. API, Informatica)3+ years - Experience using Power BI, Tableau, or similar data visualization toolExpert SQL Fluency (Well versed in CTEs and window functions)Demonstrated ability in data modeling, ETL/ELT, data pipelines, EDWExperienced building data warehouse infrastructure and BI tablesMotivated individual with strong analytic, problem solving, and troubleshooting skills', 'Support the acquisition of external data sets, interpreting data layouts, structures, fields, and values to incorporate new data into the core analytics database', 'Draw insights from data and clearly communicate findings to stakeholders and external customers', 'Design, develop and maintain scaled, automated, user-friendly systems, reports, dashboards, etc.', 'Demonstrated ability in data modeling, ETL/ELT, data pipelines, EDW', 'Interface with business customers to gather data and metrics requirements, then driving analytic projects to solve complex challenges', 'Company Overview:', 'Design, develop and maintain scaled, automated, user-friendly systems, reports, dashboards, etc.Write complex, production-quality (i.e., accurate, performant, and maintainable) data transformation code to solve the needs of analysts, and business stakeholders (ex. MS SQL Server, Oracle, and Snowflake)Analyze assigned projects for data quality issues. Troubleshoot and resolve issues as they arise.Automate standard report creation and sharing using tools or scriptsConvert raw data into consumable information applying business logic and utilizing clean engineering workflowsEnsure that data, systems, architecture, business logic, and metrics are well-documentedSupport the acquisition of external data sets, interpreting data layouts, structures, fields, and values to incorporate new data into the core analytics databaseServe as a catalyst for sharing knowledge, information, and ideas throughout the company as it relates to business intelligence', 'Ensure that data, systems, architecture, business logic, and metrics are well-documented', 'Business Unit Overview', '3+ years - Experience with ETL/ELT Tools (ex. API, Informatica)', 'Write complex, production-quality (i.e., accurate, performant, and maintainable) data transformation code to solve the needs of analysts, and business stakeholders (ex. MS SQL Server, Oracle, and Snowflake)', 'Serve as a catalyst for sharing knowledge, information, and ideas throughout the company as it relates to business intelligence', 'Less than 20%', '3+ years - Experience using Power BI, Tableau, or similar data visualization tool', 'Expert SQL Fluency (Well versed in CTEs and window functions)', 'Required Experience, Education, Skills And Competencies']",Entry level,Full-time,Information Technology,Electrical/Electronic Manufacturing,2021-03-24 13:05:10
Data Engineer,MUFG,"Tempe, AZ",5 days ago,Be among the first 25 applicants,"['', ' Expertise on BigData and ETL technologies like Hive, Spark, AWS Glue, Redshift and other distributed systems. Strong expertise in relational SQL and NoSQL databases, including Postgres, Amazon RDS, DynamoDB etc.  Experience with object-oriented/object function scripting languages: Python, Java, etc.  Strong knowledge on cloud and distributed storage systems like S3, HDFS etc.  Experience with data pipeline and workflow tools: like Syncort.  Experience with stream-processing systems: Kafka, AWS Kinesis, Apache Storm, Spark-Streaming, etc. is a plus. A successful history of manipulating, processing and extracting value from large disconnected datasets with ETL and Data engineering know how of SQL, Informatica PowerCenter or similar.  ', 'Metadata management', 'Work with partners including the Business, Infrastructure and Design teams to assist with data-related technical issues and support their data infrastructure needs.', 'Experience with data pipeline and workflow tools: like Syncort. ', 'Awareness of data governance aspects like metadata, business glossaries, data controls, data protection, canonical models, etc.', 'Experience within a high integrity, and/or regulated environment (government, healthcare, financial sectors, etc.)', 'Expertise on BigData and ETL technologies like Hive, Spark, AWS Glue, Redshift and other distributed systems.', ' 7-10 years of meaningful technical experience, with at least 5+ years of experience in design, development and delivery of critical data solutions in large complex IT environment, poses Experienced level skills in 3 or more of the following areas: Data Warehouse, Data Mart and Data Vaults Data Backup / Restore, Replication, Disaster Recovery Data field encryption and tokenization Application design / develop / test experience with RDBMS. Knowledge in NoSQL is a plus.  Database Administration experience with Relational and NoSQL databases  Metadata management   Data Services solution design and implementation experiences in on-premise or cloud native environment, poses Expert level skills in 4 or more of the following areas: Expertise on BigData and ETL technologies like Hive, Spark, AWS Glue, Redshift and other distributed systems. Strong expertise in relational SQL and NoSQL databases, including Postgres, Amazon RDS, DynamoDB etc.  Experience with object-oriented/object function scripting languages: Python, Java, etc.  Strong knowledge on cloud and distributed storage systems like S3, HDFS etc.  Experience with data pipeline and workflow tools: like Syncort.  Experience with stream-processing systems: Kafka, AWS Kinesis, Apache Storm, Spark-Streaming, etc. is a plus. A successful history of manipulating, processing and extracting value from large disconnected datasets with ETL and Data engineering know how of SQL, Informatica PowerCenter or similar.    Experience with cloud services platform for Data Management and Integration Awareness of data governance aspects like metadata, business glossaries, data controls, data protection, canonical models, etc. Experience to Develop, Deploy and Manage application in Cloud (AWS, GCP etc.) Strong scripting experience with automating processes and deployments using tools such as scripting (bash, python etc.) Familiar with DevOps toolchain, i.e. BitBucket, JIRA, Jenkins Pipeline, Artifactory or Nexus, GIT, Ansible and experienced in automate and deploy n-tier application stack in cloud native environments Excellent data & system analysis, data mapping, and data profiling skills Demonstrate good understanding of modern, cloud-native application models and patterns Excellent collaboration skills and a passion for problem solving, with the ability to work alternative coverage schedules Strong verbal and written communication skills required due to the dynamic nature of collaboration with leadership, customers, and other engineering teams Bachelor’s degree in Computer Science, or a related field ', 'Build processes supporting data transformation, data structures, metadata, dependency and workload management.', 'Core Banking Transformation Program', 'Data Warehouse, Data Mart and Data Vaults', 'Has the ability to write infrastructure, application and data test cases and participate in code review sessions.', 'Be a data authority to strive for greater functionality in our data systems.', 'Job Posting', 'Bachelor’s degree in Computer Science, or a related field', 'Demonstrate good understanding of modern, cloud-native application models and patterns', 'Work closely with architecture teams to select, design, develop and implement optimized solutions and practices', 'We are proud to be an Equal Opportunity / Affirmative Action Employer and committed to leveraging the diverse backgrounds, perspectives, and experience of our workforce to create opportunities for our colleagues and our business. We do not discriminate in employment decisions on the basis of any protected category.', 'Develop and deliver ongoing releases using tiered data pipelines and continuous integration tools like Jenkins', 'Experience with cloud services platform for Data Management and Integration', 'Program Summary', 'Schedule', 'Required Education & Certifications', 'Experience to Develop, Deploy and Manage application in Cloud (AWS, GCP etc.)', 'Shift', 'Proficient in usage of distributed revision control systems with branching, tagging (git). Create and maintain release and update processes using open source build tools', 'Role Summary', ""Bachelor's degree in computer science or related field, or equivalent professional experience"", '7-10 years of meaningful technical experience, with at least 5+ years of experience in design, development and delivery of critical data solutions in large complex IT environment, poses Experienced level skills in 3 or more of the following areas: Data Warehouse, Data Mart and Data Vaults Data Backup / Restore, Replication, Disaster Recovery Data field encryption and tokenization Application design / develop / test experience with RDBMS. Knowledge in NoSQL is a plus.  Database Administration experience with Relational and NoSQL databases  Metadata management  ', 'Desired Knowledge, Skills, And Experience', 'Strong knowledge on cloud and distributed storage systems like S3, HDFS etc. ', 'Strong scripting experience with automating processes and deployments using tools such as scripting (bash, python etc.)', 'Experience with stream-processing systems: Kafka, AWS Kinesis, Apache Storm, Spark-Streaming, etc. is a plus.', 'Analyze complex data / data models and focus on the data research of multi-functional requirements, source and target data model analysis to develop and support the end-to-end data mapping effort', 'Excellent data & system analysis, data mapping, and data profiling skills', 'Gather and process large, complex, raw data sets at scale (including writing data pipelines, scripts, calling APIs, write SQL queries, etc.) that meet functional / non-functional business requirements.', 'Data field encryption and tokenization', "" Bachelor's degree in computer science or related field, or equivalent professional experience "", 'A conviction is not an absolute bar to employment. Factors such as the age of the offense, evidence of rehabilitation, seriousness of violation, and job relatedness are considered in all employment decisions. Additionally, it’s the bank’s policy to only inquire into a candidate’s criminal history after an offer has been made. Federal law prohibits banks from employing individuals who have been convicted of, or received a pretrial diversion for, certain offenses.', 'Primary Location', ' Data Warehouse, Data Mart and Data Vaults Data Backup / Restore, Replication, Disaster Recovery Data field encryption and tokenization Application design / develop / test experience with RDBMS. Knowledge in NoSQL is a plus.  Database Administration experience with Relational and NoSQL databases  Metadata management ', 'Partner with Risk Management and Security team to identify the standards and required controls, and lead the design, build, and rollout secured and compliant data services to support MUFG critical business applications and workload', 'Technology Modernization Program', 'Provide Level 3 support for troubleshooting and services restoration in Production', 'The above statements are intended to describe the general nature and level of the work being performed. They are not intended to be construed as an exhaustive list of all responsibilities, duties, and skills required of personnel so classified', 'Data Services solution design and implementation experiences in on-premise or cloud native environment, poses Expert level skills in 4 or more of the following areas: Expertise on BigData and ETL technologies like Hive, Spark, AWS Glue, Redshift and other distributed systems. Strong expertise in relational SQL and NoSQL databases, including Postgres, Amazon RDS, DynamoDB etc.  Experience with object-oriented/object function scripting languages: Python, Java, etc.  Strong knowledge on cloud and distributed storage systems like S3, HDFS etc.  Experience with data pipeline and workflow tools: like Syncort.  Experience with stream-processing systems: Kafka, AWS Kinesis, Apache Storm, Spark-Streaming, etc. is a plus. A successful history of manipulating, processing and extracting value from large disconnected datasets with ETL and Data engineering know how of SQL, Informatica PowerCenter or similar.   ', 'Excellent collaboration skills and a passion for problem solving, with the ability to work alternative coverage schedules', 'Create and maintain optimal data pipeline architecture, responsibilities include the design, implementation, and continuous delivery of a sophisticated data pipeline supporting development and operations', 'Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.', 'Application design / develop / test experience with RDBMS. Knowledge in NoSQL is a plus. ', 'Participate in software and system performance analysis and tuning, service capacity planning and demand forecasting', 'Database Administration experience with Relational and NoSQL databases ', 'Major Responsibilities', 'Data Governance, Infrastructure & Reporting Program', ' Experience within a high integrity, and/or regulated environment (government, healthcare, financial sectors, etc.) AWS professional level certifications is preferred but not required  ', 'Familiar with DevOps toolchain, i.e. BitBucket, JIRA, Jenkins Pipeline, Artifactory or Nexus, GIT, Ansible and experienced in automate and deploy n-tier application stack in cloud native environments', 'Strong verbal and written communication skills required due to the dynamic nature of collaboration with leadership, customers, and other engineering teams', ' Core Banking Transformation Program Data Governance, Infrastructure & Reporting Program Technology Modernization Program ', 'Partner with application and DBA teams to experiment, design, develop and deliver on-premise as well as cloud native solutions and services, and power the digital transformations across business units', 'Performance analysis and tuning of infrastructure and data processing', 'Job', 'Required Knowledge, Skills, And Experience', 'Data Backup / Restore, Replication, Disaster Recovery', 'Strong expertise in relational SQL and NoSQL databases, including Postgres, Amazon RDS, DynamoDB etc. ', 'Embrace Infrastructure-as-Code, and leverage Continuous Integration / Continuous Delivery Pipelines to run the full data service lifecycle from release of data service offerings into production through the retirement thereof', 'Experience with object-oriented/object function scripting languages: Python, Java, etc. ', ' Work closely with architecture teams to select, design, develop and implement optimized solutions and practices Create and maintain optimal data pipeline architecture, responsibilities include the design, implementation, and continuous delivery of a sophisticated data pipeline supporting development and operations Gather and process large, complex, raw data sets at scale (including writing data pipelines, scripts, calling APIs, write SQL queries, etc.) that meet functional / non-functional business requirements. Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. Analyze complex data / data models and focus on the data research of multi-functional requirements, source and target data model analysis to develop and support the end-to-end data mapping effort Build processes supporting data transformation, data structures, metadata, dependency and workload management. Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using Programming language like Python, Hive/Spark, SQL/NoSQL, pipeline tools and technologies. Proficient in usage of distributed revision control systems with branching, tagging (git). Create and maintain release and update processes using open source build tools Develop and deliver ongoing releases using tiered data pipelines and continuous integration tools like Jenkins Solid experience with environment and deployment automation, infrastructure-as-code, deployment data pipeline specification and development. Work with partners including the Business, Infrastructure and Design teams to assist with data-related technical issues and support their data infrastructure needs. Be a data authority to strive for greater functionality in our data systems. Responsible for production readiness and all operational aspects of the new data services that will support critical MUFG applications Partner with Risk Management and Security team to identify the standards and required controls, and lead the design, build, and rollout secured and compliant data services to support MUFG critical business applications and workload Partner with application and DBA teams to experiment, design, develop and deliver on-premise as well as cloud native solutions and services, and power the digital transformations across business units Embrace Infrastructure-as-Code, and leverage Continuous Integration / Continuous Delivery Pipelines to run the full data service lifecycle from release of data service offerings into production through the retirement thereof Participate in software and system performance analysis and tuning, service capacity planning and demand forecasting Has the ability to write infrastructure, application and data test cases and participate in code review sessions. Performance analysis and tuning of infrastructure and data processing Provide Level 3 support for troubleshooting and services restoration in Production ', 'Transformation Program', 'A successful history of manipulating, processing and extracting value from large disconnected datasets with ETL and Data engineering know how of SQL, Informatica PowerCenter or similar. ', 'Responsible for production readiness and all operational aspects of the new data services that will support critical MUFG applications', 'AWS professional level certifications is preferred but not required ', 'Solid experience with environment and deployment automation, infrastructure-as-code, deployment data pipeline specification and development.', 'Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using Programming language like Python, Hive/Spark, SQL/NoSQL, pipeline tools and technologies.']",Entry level,Full-time,Strategy/Planning,Financial Services,2021-03-24 13:05:10
Data Engineer,Corestaff Services,"Creve Coeur, MO",6 days ago,Be among the first 25 applicants,"['', 'data analysis skills utilizing either Spotfire and/or Tableau', ' Visualization-data analysis skills utilizing either Spotfire and/or Tableau', ' Lorien ', 'Database systems and management of large data sets ', 'Strong Python skills with the ability to manipulate data', 'Data Engineer- 6737', ' In addition to strong technical skills, the successful candidate must excel in time management; and have a proven track record in problem solution leveraging complex data.', "" Bachelor\\'s degree or higher in Computer Science, Data Science, Data Analytics, Ag/Life Sciences, or related field"", ' Understanding of Database systems and management of large data sets ', 'manipulate data', 'Considered a Plus For The Data Engineer', ' Experience working with multidisciplinary research and field teams', 'Job Overview', ' Develop key metrics and data visualizations that will enable prescriptive operations and optimize pipeline processes and transparency through increased data visibility', ""Use Python and SQL to manipulate data to support the project Develop key metrics and data visualizations that will enable prescriptive operations and optimize pipeline processes and transparency through increased data visibilityRequirements For The Data Engineer Bachelor\\'s degree or higher in Computer Science, Data Science, Data Analytics, Ag/Life Sciences, or related fieldStrong Python skills with the ability to manipulate dataStrong SQL querying skillsGood data management skills Visualization-data analysis skills utilizing either Spotfire and/or Tableau In addition to strong technical skills, the successful candidate must excel in time management; and have a proven track record in problem solution leveraging complex data. Ideal candidates will have a strong developmental desire to continually learn new technical skills to remain on the cutting edge of capabilities for data analysis and visualization.Considered a Plus For The Data Engineer Understanding of Database systems and management of large data sets  Experience working with multidisciplinary research and field teams Ability to construct a storyline, perform analyses, and create presentations to effectively communicate complex concepts and recommendations Experience and skills with project management and communication of strategy with both technical and non-technical audiences.Additional Info: Develop Business Intelligence Visualization SQL R Metrics Create Tools Manipulation Data Acquisition is a plus “Employer will not sponsor applicants for work visas for this position.”Please apply online or email ian.mulloy@lorienglobal.com. If you don’t meet these requirements, but are interested in other Impellam NA, Corestaff Services or Lorien opportunities, please register with us online at ess.impellam.com . Lorien is an Equal Opportunity Employer - All qualified applicants will receive consideration without regard to race, color, religion, gender, national origin, age, disability, veteran status, or any other factor determined to be unlawful under applicable law."", ' Experience and skills with project management and communication of strategy with both technical and non-technical audiences.', 'Strong Python', 'solution leveraging complex data', 'Good data management skills', 'Requirements For The Data Engineer', ' “Employer will not sponsor applicants for work visas for this position.”', ' strong Python and SQL skills', ' Ability to construct a storyline, perform analyses, and create presentations to effectively communicate complex concepts and recommendations', 'Use Python and SQL to manipulate data to support the project', ' Ideal candidates will have a strong developmental desire to continually learn new technical skills to remain on the cutting edge of capabilities for data analysis and visualization.', 'Strong SQL querying skills', 'Develop key metrics and data visualizations', 'Due to contractual obligations candidates must be on our W2. Depending on skill level candidates can work 100% remote, but will need to travel to the office once every few months.', 'Responsibilities For The Data Engineer']",Entry level,Contract,Information Technology,Construction,2021-03-24 13:05:10
Data Infrastructure Engineer,Snowflake,"San Mateo, CA",2 weeks ago,99 applicants,"['', 'Experience programming in at least one of the following languages: Java, Scala, or Python.', 'Experience with cloud environments (e.g. AWS, Azure, or GCP), or resource management systems (e.g. Kubernetes)', 'Minimal Qualifications', 'Preferred Qualifications', 'Develop best practices around observability and data processing, and implement the changes to make those practices a reality.', ""Bachelor's degree in Computer Science, a related technical field involving software engineering, or equivalent practical experience."", "" Bachelor's degree in Computer Science, a related technical field involving software engineering, or equivalent practical experience. Experience programming in at least one of the following languages: Java, Scala, or Python. Experience working with databases (e.g. SQL) and distributed big data infrastructure like Presto, Airflow, Hadoop, Spark, and Snowflake. Systematic problem-solving methods, effective communication skills. "", 'Collaborate cross functionally to develop, test, deploy, and scale new solutions.', 'Experience improving efficiency, scalability, and stability of data systems.', 'The position may require access to U.S. export-controlled technologies or technical data.', 'Systematic problem-solving methods, effective communication skills.', 'Experience with alerting, monitoring and remediation automation in a large scale distributed environment', 'Cloud Infrastructure Engineers ', 'Experience working with databases (e.g. SQL) and distributed big data infrastructure like Presto, Airflow, Hadoop, Spark, and Snowflake.', 'Responsibilities', ' The position may require access to U.S. export-controlled technologies or technical data. Employment with Snowflake is contingent on Snowflake either verifying that you may legally access U.S. export-controlled technologies and technical data, or successfully obtaining by July 1, 2021, any necessary license or other authorization from the U.S. Government to allow you to have the ability to access U.S. export-controlled technology and technical data. ', 'Implement observability systems to track data quality and consistency.', "" Experience working cross-functionally to establish an overall data architecture for a company's needs, building data pipelines, and establishing best data practices. Experience with cloud environments (e.g. AWS, Azure, or GCP), or resource management systems (e.g. Kubernetes) Experience improving efficiency, scalability, and stability of data systems. Experience with alerting, monitoring and remediation automation in a large scale distributed environment "", ' Collaborate cross functionally to develop, test, deploy, and scale new solutions. Design, implement, test, and deploy the tools that allow other members of data science and analytics teams to easily write and run effective data pipelines. Develop best practices around observability and data processing, and implement the changes to make those practices a reality. Implement observability systems to track data quality and consistency. ', 'Employment with Snowflake is contingent on Snowflake either verifying that you may legally access U.S. export-controlled technologies and technical data, or successfully obtaining by July 1, 2021, any necessary license or other authorization from the U.S. Government to allow you to have the ability to access U.S. export-controlled technology and technical data.', 'Mandatory Requirements For The Role', 'Design, implement, test, and deploy the tools that allow other members of data science and analytics teams to easily write and run effective data pipelines.', ""Experience working cross-functionally to establish an overall data architecture for a company's needs, building data pipelines, and establishing best data practices.""]",Entry level,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Data engineer,Perficient,"Wilmington, DE",2 weeks ago,Be among the first 25 applicants,"['', ' Manages ETL: use programming and tools for data ingestion, configure pipelines, apply transformations and decoding, integrate and fuse data, move and securely deliver ', ' Experience with relational SQL and NoSQL databases ', ' Creates POC processes in Dev/QA and works with Software Engineers to productionize and align best coding and process practices that maximize efficiency, speed, stability, system resources and capabilities ', ' Document code artifacts and participate in developing user documentation and run books ', ' Work with data engineering team to define and develop data ingestion, validation, transformation and data engineering code. ', ' Awareness of and compliance with: data privacy, security, legal and contractual guidelines ', ' Incorporates data quality and privacy checks/alerts to minimize bad data being consumed by end users, models and dashboards, and to protect customer data; revises checks for new data issues ', ' Leverages frameworks in place with big data tools: Hadoop, Spark, Python, Kafka, etc. ', ' Works closely with Architects to align systems, tools and applications being utilized with business use case and performance requirements ', ' Experts of the data and its application by users ', 'Overview', ' Responsible for data architecture including sources, table structures, physical models ', ' Experts of the data and its application by users  Understands data landscape and environments: sources, elements, update freq, completeness, stewards/contacts, platforms  Manages ETL: use programming and tools for data ingestion, configure pipelines, apply transformations and decoding, integrate and fuse data, move and securely deliver  Translates business requirements to build repeatable, sustainable, efficient, coded processes that can be productionized by Software Engineers and readily modified by other Data Engineers  Creates POC processes in Dev/QA and works with Software Engineers to productionize and align best coding and process practices that maximize efficiency, speed, stability, system resources and capabilities  Leverages frameworks in place with big data tools: Hadoop, Spark, Python, Kafka, etc.  Experience with relational SQL and NoSQL databases  Awareness of and compliance with: data privacy, security, legal and contractual guidelines  Incorporates data quality and privacy checks/alerts to minimize bad data being consumed by end users, models and dashboards, and to protect customer data; revises checks for new data issues  Maintains feedback loop with Data Stewards on data issues, standards, fit for use (Data Stewardship is a subset of data engineering which would include responsibilities like data curation)  Validates data products and pipelines are functioning as expected following system or application upgrades, source changes, etc.  Responsible for data architecture including sources, table structures, physical models  Works closely with Architects to align systems, tools and applications being utilized with business use case and performance requirements  Communicates with end users to set expectations and ensure alignment around data accuracy, completeness, timeliness and consistency  Provides data product support and maintenance  Establishes, tracks and monitors KPIs related to specific data products and deliverables ', ' Establishes, tracks and monitors KPIs related to specific data products and deliverables ', ' Participate in design sessions, demos and prototype sessions, testing and training workshops with business users and other IT associates ', ' Work with data engineering team to define and develop data ingestion, validation, transformation and data engineering code.  Develop open source platform components using Spark, Scala, Java, Oozie, Hive and other components  Document code artifacts and participate in developing user documentation and run books  Troubleshoot deployment to various environments and provide test support.  Participate in design sessions, demos and prototype sessions, testing and training workshops with business users and other IT associates ', ' Will you now, or in the future, require sponsorship for employment visa status (e.g. H-1B visa status)?', 'Job Overview', 'Preferred Skills And Education', ' Understands data landscape and environments: sources, elements, update freq, completeness, stewards/contacts, platforms ', ' Validates data products and pipelines are functioning as expected following system or application upgrades, source changes, etc. ', 'More About Perficient', 'Responsibilities', ' Communicates with end users to set expectations and ensure alignment around data accuracy, completeness, timeliness and consistency ', 'Data Engineer with Spark exp.', 'Qualifications', ' Provides data product support and maintenance ', ' Translates business requirements to build repeatable, sustainable, efficient, coded processes that can be productionized by Software Engineers and readily modified by other Data Engineers ', ' Are you legally authorized to work in the United States?', ' Develop open source platform components using Spark, Scala, Java, Oozie, Hive and other components ', ' Troubleshoot deployment to various environments and provide test support. ', ' Maintains feedback loop with Data Stewards on data issues, standards, fit for use (Data Stewardship is a subset of data engineering which would include responsibilities like data curation) ', ' Disclaimer: ']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
"Data ""Golang"" Engineer",Rose International,United States,,N/A,"['', 'If you share our values, you should have:', 'At least 2 years experience with Go', '• Experience data modeling for large scale databases, either relational or NoSQL', '• Familiarity with creating and maintaining containerized application deployments with a platform like Docker', '• A proven ability to build and maintain cloud based infrastructure on a major cloud provider like AWS, Azure or Google Cloud Platform', '• Experience with stream processing using Apache Kafka', 'Duration:\xa012 months\xa0', '• At least 2 years experience with Go', '• A level of comfort with Unit Testing and Test Driven Development methodologies', 'Duration:\xa012 months', 'Title: Data ""Golang"" Engineer', '• Proven experience (2 years) building and maintaining data-intensive APIs using a RESTful approach']",Mid-Senior level,Contract,Information Technology,Chemicals,2021-03-24 13:05:10
Data Engineer II,NOV,"Conroe, TX",2 weeks ago,Be among the first 25 applicants,"['Write and deploy REST APIs for internal consumption.', 'Write data aggregation and integration scripts, e.g. ETL, Stream Analytics, or MapReduce.', 'Competence in a database query language.', ' Work with customers to identify opportunities for new data driven products and ways to improve existing products. Research data science and software engineering techniques used within the energy industry and in general. Design quality, scalable data models for business processes in SQL, NoSQL, or dimensional databases. Write data aggregation and integration scripts, e.g. ETL, Stream Analytics, or MapReduce. Apply statistical models to classify data and develop new key performance indicators. Deliver data to consumers through Power BI connectors and dashboards. Write and deploy REST APIs for internal consumption. Apply domain-driven design to complex engineering models when applicable. ', 'Report regularly on project progress.', 'Minimum of five years’ experience as a software engineer or equivalent postgraduate education.', 'Demonstrated organizational skills.', 'Work with customers to identify opportunities for new data driven products and ways to improve existing products.', 'Competence in general purpose programming.', 'Experience with machine learning techniques.', 'Design quality, scalable data models for business processes in SQL, NoSQL, or dimensional databases.', ' Examine how new software requirement fits into Divisional and Corporate strategies and plans. Proactively generate documentation for both customers and developers, e.g. readme, swagger, pydoc. Maintain version control of all code and automate deployments with CI/CD pipeline. Report regularly on project progress. Perform and/or coordinate project commercialization requirements such as User Manual development; technical support requirements; and training collateral. Champion the use of technical software tools throughout the organization. ', 'Proactively generate documentation for both customers and developers, e.g. readme, swagger, pydoc.', 'Maintain version control of all code and automate deployments with CI/CD pipeline.', 'Experience in the energy industry.', 'Bachelor of Science in a quantitative discipline.', 'Education And Experience', ' Bachelor of Science in a quantitative discipline. Minimum of five years’ experience as a software engineer or equivalent postgraduate education. Demonstrated organizational skills. Demonstrated teamwork skills. Competence in general purpose programming. Competence in a database query language. Knowledge of algorithms and software design. Excellent written and verbal communication skills. Demonstrated ability to apply agile methodology to software development. ', 'Research data science and software engineering techniques used within the energy industry and in general.', 'Apply domain-driven design to complex engineering models when applicable.', 'Demonstrated teamwork skills.', 'Experience with business intelligence tools.', 'Experience designing relational and dimensional data models.', ' Experience in the energy industry. Knowledge of mechanics and/or material science. Experience with business intelligence tools. Experience with machine learning techniques. ', ' Experience or interest in functional programming. Strong quantitative skills (mathematics/statistics). Experience designing relational and dimensional data models.', 'Knowledge of mechanics and/or material science.', 'Champion the use of technical software tools throughout the organization.', 'General Requirements', 'Knowledge of algorithms and software design.', 'Examine how new software requirement fits into Divisional and Corporate strategies and plans.', 'Deliver data to consumers through Power BI connectors and dashboards.', 'Primary Responsilities', 'Experience or interest in functional programming.', 'Excellent written and verbal communication skills.', 'Strong quantitative skills (mathematics/statistics).', 'Apply statistical models to classify data and develop new key performance indicators.', 'Job Description', 'Preferred Experience', 'Perform and/or coordinate project commercialization requirements such as User Manual development; technical support requirements; and training collateral.', 'Demonstrated ability to apply agile methodology to software development.']",Entry level,Full-time,Information Technology,Oil & Energy,2021-03-24 13:05:10
Lead Data Engineer,All Turtles,"New York, NY",2 weeks ago,36 applicants,"['', ""You'll be responsible for:"", ' 5+ years of data engineering experience. Expert level skills in Python and SQL. Experience with Jupyter Notebook or similar tools is a plus. Extensive experience with data warehousing and data visualization tools. Experience building batch data pipelines and data warehouses on cloud computing platforms such as AWS or GCP, working with MPP databases such as BigQuery, and using workflow management tools such as Apache Airflow. A solid foundation in statistics. Excellent communications skills. Flexibility and comfort working in a dynamic, fast-growing environment with minimal oversight, documentation and process. ', '5+ years of data engineering experience.', 'A solid foundation in statistics.', 'Working with business and product leaders to understand the questions they need to be able to answer.', 'Excellent communications skills.', 'Creating dashboards and visualizations.', ' Working with business and product leaders to understand the questions they need to be able to answer. Designing, implementing and maintaining the analytics infrastructure that supports our product & business decision making. Working with product development teams to instrument their applications to gather the required data. Creating dashboards and visualizations. ', ""Ideally, you'll have:"", 'Extensive experience with data warehousing and data visualization tools.', 'Expert level skills in Python and SQL. Experience with Jupyter Notebook or similar tools is a plus.', 'Experience building batch data pipelines and data warehouses on cloud computing platforms such as AWS or GCP, working with MPP databases such as BigQuery, and using workflow management tools such as Apache Airflow.', 'Working with product development teams to instrument their applications to gather the required data.', 'Flexibility and comfort working in a dynamic, fast-growing environment with minimal oversight, documentation and process.', 'Designing, implementing and maintaining the analytics infrastructure that supports our product & business decision making.']",Associate,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
"Data engineer, Senior",Biobot Analytics,"Myrtle Point, OR",5 days ago,Be among the first 25 applicants,"['', 'Practical understanding of version control and software collaboration best practices. ', 'Design and build a robust data infrastructure that unlocks Biobot’s ability to leverage data at all levels of the organization. Working with our data science and lab teams, design frameworks for building data analysis pipelines across a variety of product offerings and data types. Lead database and codebase design decisions for existing and new data pipelines.Working closely with our product and engineering team, integrate Biobot’s data into our customer-facing software platform and internal and external APIs. Establish ETL processes to ingest external datasets that our data science and visualization teams can use to add context to Biobot’s wastewater data.Implement best practices for version control, code review, testing, and collaboration in a remote working environment.Manage two junior data engineers and provide leadership across product, data, and engineering teams.Provide strategic insights to company leadership as we expand Biobot’s ""data as a product"" model.', 'Skills And Qualifications', 'Implement best practices for version control, code review, testing, and collaboration in a remote working environment.', 'Working with our data science and lab teams, design frameworks for building data analysis pipelines across a variety of product offerings and data types. Lead database and codebase design decisions for existing and new data pipelines.', 'Experience working with and designing for cloud-based infrastructures, ideally AWS (S3, EC2, Redshift, lambda and step functions, etc). Proficiency designing and implementing ELT/ETL mechanisms, SQL, stored procedures, resilient data pipelines to ingest structured and semi-structured datasets using files, API, streams, etc.', 'Experience working with biological or other scientific data is a plus. Curiosity about the science behind Biobot’s data is important for the role.', 'Ability to lead data governance and data profiling efforts to ensure data quality and proper metadata documentation for data lineage.', 'Roles And Responsibilities', 'Experience leading a technical team or mentoring junior engineers, preferably in a formal capacity.', 'Expert in Python & pandas and proficiency with object oriented programming and test development.', '5+ years of experience in data modeling and architecture. Proficiency in SQL database technologies, ideally Postgres.', 'Working closely with our product and engineering team, integrate Biobot’s data into our customer-facing software platform and internal and external APIs. ', 'Demonstrated experience designing data pipelines from scratch, including robust data validations, automations, logging, and tests. ', 'Manage two junior data engineers and provide leadership across product, data, and engineering teams.', 'Knowledge of cutting-edge data engineering tools and technologies, and an ability to evaluate trade-offs between approaches to suit our startup needs.', '7+ years experience as a data engineer or software engineer in a data-focused company. Experience working in a startup environment is a plus. 5+ years of experience in data modeling and architecture. Proficiency in SQL database technologies, ideally Postgres.Demonstrated experience designing data pipelines from scratch, including robust data validations, automations, logging, and tests. Expert in Python & pandas and proficiency with object oriented programming and test development.Practical understanding of version control and software collaboration best practices. Experience leading a technical team or mentoring junior engineers, preferably in a formal capacity.Willingness and ability to work in a rapidly changing environment, responding to customer feedback and evolving product priorities and adapting codebases accordingly.Experience working with and designing for cloud-based infrastructures, ideally AWS (S3, EC2, Redshift, lambda and step functions, etc). Proficiency designing and implementing ELT/ETL mechanisms, SQL, stored procedures, resilient data pipelines to ingest structured and semi-structured datasets using files, API, streams, etc.Ability to lead data governance and data profiling efforts to ensure data quality and proper metadata documentation for data lineage.Knowledge of cutting-edge data engineering tools and technologies, and an ability to evaluate trade-offs between approaches to suit our startup needs.Experience working with biological or other scientific data is a plus. Curiosity about the science behind Biobot’s data is important for the role.', 'Design and build a robust data infrastructure that unlocks Biobot’s ability to leverage data at all levels of the organization. ', 'Provide strategic insights to company leadership as we expand Biobot’s ""data as a product"" model.', '7+ years experience as a data engineer or software engineer in a data-focused company. Experience working in a startup environment is a plus. ', 'Establish ETL processes to ingest external datasets that our data science and visualization teams can use to add context to Biobot’s wastewater data.', 'By submitting your interest in this job, you agree to receive text notifications with additional steps to complete your job application. You will receive up to 6 messages from the number ""63879"". Message & data rates may apply. Please refer to our privacy policy for more information.', 'Willingness and ability to work in a rapidly changing environment, responding to customer feedback and evolving product priorities and adapting codebases accordingly.']",Associate,Full-time,Information Technology,Hospital & Health Care,2021-03-24 13:05:10
Data Engineer,Bangor Savings Bank,"Bangor, ME",1 week ago,Be among the first 25 applicants,"['', 'Are a successful, vibrant, and innovative company.', 'Apply Bank standards and industry best practices to the ongoing management of the database infrastructure and related technologies.', 'Ensure the highest levels of availability and performance within BI systems and infrastructure.', 'Demonstrate ownership of database and related technologies and all issues that arise with them.', 'Believe in autonomy & initiative taking.', 'Believe every interaction is an opportunity to provide a “You Matter More” experience.', 'Perform data profiling of source data to identify data quality issues and anomalies, business knowledge embedded in data, gathering of natural keys, and metadata information.', 'Analytical approach to problem solving and process improvement.', 'Care most about our employees, our customers and our communities.', 'Have a smart, experienced, and diverse leadership team that wants to do it right & is open to new ideas.', 'Are adaptable and flexible, aren’t afraid of change, open to new ideas, take on new challenges, handle pressure, adjust plans to meet changing needs.', 'Are a technologically and data-driven business.', 'Relevant technical certification(s) strongly preferred.', 'About The Role', 'Support and improve production data integration system and environment.', 'You’ll Love Working at Bangor Savings Bank Because We… ', 'Are a successful, vibrant, and innovative company.Care most about our employees, our customers and our communities.Believe every interaction is an opportunity to provide a “You Matter More” experience.Believe in autonomy & initiative taking.Are a technologically and data-driven business.Have a smart, experienced, and diverse leadership team that wants to do it right & is open to new ideas.Have a beautiful new campus in Bangor, Maine.', 'Support and improve production data integration system and environment.Understand the data architecture needs and data structures in the source systems and business processes.Design data marts for business units and collaborate with development teams during the implementation.Collaborate with internal & external data consumers to understand their data needs and drive towards unifying collections of data requirements for key data elements across the organization.Document and maintain documentation related data mapping and other data design artifacts that encompass data specifications, business & transformation rules.Collaborate with vendors and internal developers in requirements gathering sessions with stakeholders to determine user needs and capture data requirements.Translate business requirements and data needs into solutions easily used for reporting, scorecards and dashboards.Apply Bank standards and industry best practices to the ongoing management of the database infrastructure and related technologies.Demonstrate ownership of database and related technologies and all issues that arise with them.Ensure the highest levels of availability and performance within BI systems and infrastructure.Perform BI Administrative functions as requested.', 'What You Bring To The Table Is…', 'We’re Excited About You Because You…', 'Document and maintain documentation related data mapping and other data design artifacts that encompass data specifications, business & transformation rules.', 'Are analytical, observe processes and trends; make recommendations for process changes that help achieve departmental and individual goals.', 'Understand the data architecture needs and data structures in the source systems and business processes.', 'Have relevant employment experience and demonstrated abilities as a Data Engineer.', 'Experience working with multi-format files likes JSON, XML, CSV, Flat, etc.', 'Hands-on experience with data modeling techniques, including with star schemas and contemporary ETL strategies.', 'You’re Excited About This Opportunity Because You Will…', 'Experience in one or more programming languages like Python, JavaScript, C#, Java, etc.', 'Take initiative and independent action, operate as a pro-active self-starter, act on opportunities, and practice self-development.', 'Have relevant employment experience and demonstrated abilities as a Data Engineer.Have the desire and ability to maintain knowledge and skill currency within the fast-changing technological realm.', 'Perform BI Administrative functions as requested.', 'Collaborate with internal & external data consumers to understand their data needs and drive towards unifying collections of data requirements for key data elements across the organization.', 'Design data marts for business units and collaborate with development teams during the implementation.', 'About Us', 'Strong knowledge of relational and multi-dimensional databases.', 'Have a strong organization system and use that system for the improvement and advancement of personal and team goals.', 'Design and develop enterprise and departmental business intelligence and data warehousing solutions.', 'A BS or MS degree in Computer Science or a related technical field or relevant work experience in the field.', 'Strong documentation skills, to include proficiency with MS-Word, MS-Excel and MS-Visio.', 'Have good interpersonal skills including active listening skills and negotiation techniques.', 'Have demonstrated experience using SSIS, Talend or other similar ETL tools in a data warehouse environment.', 'Willingness and ability to maintain knowledge regarding relevant current and emerging technologies and industry trends and best practices.', 'Are analytical, observe processes and trends; make recommendations for process changes that help achieve departmental and individual goals.Take initiative and independent action, operate as a pro-active self-starter, act on opportunities, and practice self-development.Are adaptable and flexible, aren’t afraid of change, open to new ideas, take on new challenges, handle pressure, adjust plans to meet changing needs.Have integrity and ethics to deal with others in a straightforward, honest manner, are accountable for you own actions, maintain confidentiality, support company values, and convey news good or bad.Have good interpersonal skills including active listening skills and negotiation techniques.Have the vision and values of BSB!Have a strong organization system and use that system for the improvement and advancement of personal and team goals.', 'Expert-level knowledge of modern databases and their related toolsets, reporting packages, and underlying technologies.', 'Develop and perform unit, system, performance and regression testing on ETL mappings.', 'Design, build, enhance, and maintain ETL processes to make new and existing data sources.', 'Collaborate with project leads, business analysts, end users and third-party contacts to design, implement and test data warehouse applications.', 'A BS or MS degree in Computer Science or a related technical field or relevant work experience in the field.Experience and implementation of Data Architecture, Data Lake, Data Marts, Operational Data Store, Analytical systems & Metadata management initiatives.Experience with schema design and dimensional data modeling.Experience in one or more programming languages like Python, JavaScript, C#, Java, etc.Experience working with APIs like REST APIs, SDKs and CLI tools as part of ETL provisioning.Experience working with multi-format files likes JSON, XML, CSV, Flat, etc.Relevant technical certification(s) strongly preferred.Exceptional troubleshooting abilities.Strong verbal and written communication skills.Strong documentation skills, to include proficiency with MS-Word, MS-Excel and MS-Visio.Expert-level knowledge of modern databases and their related toolsets, reporting packages, and underlying technologies.Strong knowledge of SQL development, performance tuning, index management.Hands-on experience with data modeling techniques, including with star schemas and contemporary ETL strategies.Strong knowledge of relational and multi-dimensional databases.Analytical approach to problem solving and process improvement.Willingness and ability to maintain knowledge regarding relevant current and emerging technologies and industry trends and best practices.', 'Have demonstrated experience using SSIS, Talend or other similar ETL tools in a data warehouse environment.Have a high proficiency in a MS SQL environment.', 'Translate business requirements and data needs into solutions easily used for reporting, scorecards and dashboards.', 'Collaborate with vendors and internal developers in requirements gathering sessions with stakeholders to determine user needs and capture data requirements.', 'Have the vision and values of BSB!', 'Have the desire and ability to maintain knowledge and skill currency within the fast-changing technological realm.', 'Experience and implementation of Data Architecture, Data Lake, Data Marts, Operational Data Store, Analytical systems & Metadata management initiatives.', 'Exceptional troubleshooting abilities.', 'By submitting your interest in this job, you agree to receive text notifications with additional steps to complete your job application. You will receive up to 6 messages from the number ""63879"". Message & data rates may apply. Please refer to our privacy policy for more information.', 'Collaborate with project leads, business analysts, end users and third-party contacts to design, implement and test data warehouse applications.Design, build, enhance, and maintain ETL processes to make new and existing data sources.Develop and perform unit, system, performance and regression testing on ETL mappings.Design and develop enterprise and departmental business intelligence and data warehousing solutions.Interact with end users and business analysts to understand reporting/dashboard requirements.Perform data profiling of source data to identify data quality issues and anomalies, business knowledge embedded in data, gathering of natural keys, and metadata information.', 'Have integrity and ethics to deal with others in a straightforward, honest manner, are accountable for you own actions, maintain confidentiality, support company values, and convey news good or bad.', 'Experience with schema design and dimensional data modeling.', 'Experience working with APIs like REST APIs, SDKs and CLI tools as part of ETL provisioning.', 'Strong verbal and written communication skills.', 'Interact with end users and business analysts to understand reporting/dashboard requirements.', 'Have a high proficiency in a MS SQL environment.', 'Strong knowledge of SQL development, performance tuning, index management.', 'Have a beautiful new campus in Bangor, Maine.']",Entry level,Internship,Information Technology,Banking,2021-03-24 13:05:10
Data Engineer,Brady Corporation,Greater Milwaukee,2 weeks ago,Be among the first 25 applicants,"['', 'Desired Knowledge, Skills & Abilities', 'Lead design sessions to develop ETL logic that meets business and data scientists’ requirements.', ""Create and build robust data structures to support end user's analysis and decision-making across multiple business verticals. This includes both end-to-end architecting and business solutions."", 'Position Summary:\xa0Brady IDS BI is interested in hiring a data engineer to work closely with business leaders, the data scientist team, and corporate IT to create data pipelines to support data-driven business decision-making processes.', 'Contribute to and care about data quality efforts, ensuring data accuracy and transparency for our stakeholders.', 'Bachelor’s degree in CIS, MIS, IT, or related Database Theory degree; or equivalent experience', 'Position Reports To:\xa0', 'Provide support in fixing database and data quality issues via reverse engineering.', 'Drive data best practices and contribute to the development of overall data strategy and roadmap.', 'Required Knowledge, Skills & Abilities', 'Excellent database tuning techniques and practices, including partitioning, performance tuning, and working with application development teams for enhanced performance', 'Bachelor’s degree in CIS, MIS, IT, or related Database Theory degree; or equivalent experience4+ years of experience in the development and maintenance of complex data systemsWorking knowledge of emerging database technologies such as Elastic Search, Big Query, Mongo or equivalent.Excellent database tuning techniques and practices, including partitioning, performance tuning, and working with application development teams for enhanced performanceAt least 1 year of Spark experience.Advanced experience leveraging various strategies for ingesting, modelling, processing, and persisting data as well as excellent analytical and problem-solving skills.Troubleshoot live site issues, engage appropriate parties, and drive through to resolution', 'Cloud Experience preferred (e.g. AWS, Azure, Google Cloud Platform, etc.). Demonstrated Azure experience with Data Factory, Analysis Services, SQL Managed Instance, Synapse, and DevOps.', 'Master’s degree, MBA or Ph.D. degree is a plus.', 'Position Summary:\xa0', 'Master’s degree, MBA or Ph.D. degree is a plus.Cloud Experience preferred (e.g. AWS, Azure, Google Cloud Platform, etc.). Demonstrated Azure experience with Data Factory, Analysis Services, SQL Managed Instance, Synapse, and DevOps.', 'Working knowledge of emerging database technologies such as Elastic Search, Big Query, Mongo or equivalent.', ""Lead design sessions to develop ETL logic that meets business and data scientists’ requirements.Create and build robust data structures to support end user's analysis and decision-making across multiple business verticals. This includes both end-to-end architecting and business solutions.Provide mentoring on database systems to other development and business teams.Monitor data warehouse eco-system and identify opportunities to make enhancements.Provide support in fixing database and data quality issues via reverse engineering.Contribute to and care about data quality efforts, ensuring data accuracy and transparency for our stakeholders.Participate in brainstorming sessions and contribute ideas to our technology, algorithms, and products.Adhere to timelines and excel in a fast-paced, high-energy environment.Drive data best practices and contribute to the development of overall data strategy and roadmap."", 'Participate in brainstorming sessions and contribute ideas to our technology, algorithms, and products.', 'Monitor data warehouse eco-system and identify opportunities to make enhancements.', '\ufeff', 'Adhere to timelines and excel in a fast-paced, high-energy environment.', 'Troubleshoot live site issues, engage appropriate parties, and drive through to resolution', 'Provide mentoring on database systems to other development and business teams.', 'At least 1 year of Spark experience.', 'Advanced experience leveraging various strategies for ingesting, modelling, processing, and persisting data as well as excellent analytical and problem-solving skills.', 'Essential Duties and Responsibilities:', '4+ years of experience in the development and maintenance of complex data systems', 'Position Reports To:\xa0\xa0Director of Business Intelligence', 'The successful candidate will have strong verbal and written communication skills and effectively communicate with the client and internal team. A strong understanding of databases, SQL, cloud technologies, and modern data integration and orchestration tools.']",Mid-Senior level,Full-time,Analyst,Printing,2021-03-24 13:05:10
Data Engineer - Melbourne,NOVEL,"Melbourne, FL",2 weeks ago,Be among the first 25 applicants,"['', 'Benefits And Perks', 'Novel Engineering offers a competitive employment package. This includes competitive compensation, vacation, dental, vision, employee profit sharing plan, flexible schedules, and a professional yet relaxed culture that gives you the opportunity to work in a team-oriented environment, innovate with co-workers, and thrive as an individual.', ""Bachelor's in computer science computer engineering or other related fields3+ years’ experience in data engineering and software developmentMust Have: Strong programming skills (python) - solid fundamentals of computer science such as data structures, time/space complexity analysisMust Have: Hands-on experience with Apache Spark (Pyspark)Must Have: Hands-on experience with Apache CassandraMust Have: Hands-on experience with Apache KafkaExperience with file formats such as Parquet and AVROKnowledge of Delta LakeFluent and experienced with Linux OS (and some bash scripting knowledge)Previous experience with AWS tools such as S3, EMR, EC2, DynamoDBGood understanding about CRM data (e.g., Salesforce), the objects, and the relationships among them Position is subject to pre-employment drug and random drug and alcohol testing."", ""Bachelor's in computer science computer engineering or other related fields"", 'Must Have: Hands-on experience with Apache Cassandra', 'Must Have: Hands-on experience with Apache Kafka', 'Company Overview', 'Must Have: Hands-on experience with Apache Spark (Pyspark)', 'Knowledge of Delta Lake', 'Fluent and experienced with Linux OS (and some bash scripting knowledge)', 'Position is subject to pre-employment drug and random drug and alcohol testing.', 'Good understanding about CRM data (e.g., Salesforce), the objects, and the relationships among them ', '3+ years’ experience in data engineering and software development', 'Job Description', 'Must Have: Strong programming skills (python) - solid fundamentals of computer science such as data structures, time/space complexity analysis', 'Previous experience with AWS tools such as S3, EMR, EC2, DynamoDB', 'Experience with file formats such as Parquet and AVRO']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Finecast,"New York, NY",3 weeks ago,Be among the first 25 applicants,"['', 'Your Qualifications', '4+ Proven experience using GCP services in architecting PaaS solutions.', 'When applicable, apply AI/ML tools to handle complex problems', 'Understand and evaluate existing business processes, workflows, and supporting systems to inform how to implement requirements', 'Design solutions for both small data (Cloud SQL) and Big Data (BigQuery, BigTable) business cases', 'Good exposure to Big Table, Big Query, Data Proc/Dataflow/ML API’s etc.', 'At least 4+ years of experience hands on experience in cloud native architecture design, implementation of distributed, fault tolerant enterprise applications for Cloud.Experience in application migration to GCP cloud4+ Proven experience using GCP services in architecting PaaS solutions.Should be a certified GCP Professional Cloud Architect.Technical Skills:Deep understanding of Cloud Native and fundamentals of GCP.Deep knowledge and understanding of PaaS and IaaS features in Cloud.Hands on experience in GCP services i.e. GCE, GKE, GAE, GCS, Cloud SQL, VPC, Resource Manager, Stack Driver, Cloud CDN, Cloud IAM, and Cloud PUB/SUB.Adequate knowledge on Google Anthos.Hands on Experience on Cloud Deployment Manager and Cloud Build services in GCP.Good exposure to Big Table, Big Query, Data Proc/Dataflow/ML API’s etc.Experience automation and provisioning of cloud environments using API’s, CLI and scripts.Experience in deploy, manage, and scale applications.Experience in programming languages like Java & Python, R, and SQL. Hands on experience in scripting languages like Perl, Shell etc.', 'Deep understanding of Cloud Native and fundamentals of GCP.', 'At least 4+ years of experience hands on experience in cloud native architecture design, implementation of distributed, fault tolerant enterprise applications for Cloud.', 'Experience in programming languages like Java & Python, R, and SQL. ', 'Hands on experience in GCP services i.e. GCE, GKE, GAE, GCS, Cloud SQL, VPC, Resource Manager, Stack Driver, Cloud CDN, Cloud IAM, and Cloud PUB/SUB.', 'Understand technology trends and the practical application of existing, new, and emerging technologies to enable new and evolving business and operating models.', 'Experience automation and provisioning of cloud environments using API’s, CLI and scripts.', 'Lead the analysis of the current GCP practices and recommend solutions for improvement.', 'Document technology architecture design and analysis work', 'About Groupm', 'Responsible for design, development, implementation, operation improvement and debug cloud environments in GCP and Cloud Management Platform', 'Define high-level migration plans to address the gaps between the current and future state, typically in sync with the IT budgeting or other capital planning processes.', 'Your Impact', 'Should be a certified GCP Professional Cloud Architect.', 'Understand and evaluate existing business processes, workflows, and supporting systems to inform how to implement requirementsDesign solutions for both small data (Cloud SQL) and Big Data (BigQuery, BigTable) business casesWhen applicable, apply AI/ML tools to handle complex problemsPlan and implement integrations with large data sets and existing software solutionsResponsible for design, development, implementation, operation improvement and debug cloud environments in GCP and Cloud Management PlatformLead the analysis of the current GCP practices and recommend solutions for improvement.Oversee, or consult on, technology implementation and modification activities (for example, projects), particularly for new or shared infrastructure solutions.Define high-level migration plans to address the gaps between the current and future state, typically in sync with the IT budgeting or other capital planning processes.Understand technology trends and the practical application of existing, new, and emerging technologies to enable new and evolving business and operating models.Document technology architecture design and analysis work', 'Experience in deploy, manage, and scale applications.', 'Deep knowledge and understanding of PaaS and IaaS features in Cloud.', 'Hands on experience in scripting languages like Perl, Shell etc.', 'Hands on Experience on Cloud Deployment Manager and Cloud Build services in GCP.', 'Adequate knowledge on Google Anthos.', 'Experience in application migration to GCP cloud', 'GroupM and all its affiliates embrace and celebrate diversity, inclusivity, and equal opportunity.\u202fWe are committed to building a team that represents a variety of backgrounds, perspectives, and skills. We are a worldwide media agency network that represents global clients. The more inclusive we are, the more great work we can create together.', 'The Role', 'Oversee, or consult on, technology implementation and modification activities (for example, projects), particularly for new or shared infrastructure solutions.', 'Technical Skills:', 'Plan and implement integrations with large data sets and existing software solutions']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Analyst/Engineer,The Laughton Team,"Peoria, AZ",1 week ago,Be among the first 25 applicants,"['', ' Snowflake, Teradata or alike Ab Initio or Informatica or Datastage ', 'Experience with writing complex SQL queries', '$70,000 - $80,000', 'Get To Know Us', 'Snowflake, Teradata or alike', 'Career Advancement: Training, coaching and development, as well as growth within the organization', 'Preferred Qualifications', 'Experience with transforming business requirements into technical specification for data designs and solutions', 'Experience in data science and mining', '2+ years Data analysis, business analysis, data application programming, ETL development, next generation databases, or related', ' Preferred Bachelor’s degree in Information Technology. 2+ years Data analysis, business analysis, data application programming, ETL development, next generation databases, or related In lieu of education, 4+ years of experience in Data analysis, business analysis, data application programming, ETL development, next generation databases, or related ', ' Experience with writing complex SQL queries Experience with Cloud-based technologies:', ' AWS, Snowflake API Nation, Integromat, Zapier Big Data stacks/ecosystem including Kafka, Spark, Python, NoSQL ', 'Ability to identify, analyze and solve complex problems', 'Qualifications You’ll Need', 'Ensures data governance policies are followed by implementing and validating data lineage, quality checks, classification, etc.', ' Competitive compensation package Health Insurance: We offer a variety of comprehensive medical, dental, and vision plans with low out-of-pocket expenses Paid time off including holidays and sick leave Career Advancement: Training, coaching and development, as well as growth within the organization ', 'Preferred Bachelor’s degree in Information Technology.', ' Sponsorship is not available for this position', 'Paid time off including holidays and sick leave', 'Ability to work within a team environment as well as independently', 'AWS, Snowflake', 'Experience in agile process and technology', 'Competitive compensation package', 'Understanding of relational databases, data integration tools:', 'Excellent attention to detail', 'Provides support for deployed data applications and analytical models; Identifies data problems and guides issue resolutions', 'What We Offer', 'Strong team player who is able to work across multiple functions and disciplines', 'Strong oral and written communication skills; Demonstrated ability to clearly articulate information or solution', ' Partner with business and technology teams to develop data applications. Identify business data ingestion and processing frameworks. Coordinates and obtains data application requirements from the business. Translates business requirements to development teams. Assist with development and testing processes. Coordinate data application and model deployments and validations. Assists with the development of models, analytic processes, and reports; Provides guidance on the development of data consumption processes Ensures data governance policies are followed by implementing and validating data lineage, quality checks, classification, etc. Provides support for deployed data applications and analytical models; Identifies data problems and guides issue resolutions Provides data and technical consulting during data application design; Provide technical consulting on data composition and data engineering Demonstrated experience with integration technologies and how to leverage them into data mapping between systems Experience with transforming business requirements into technical specification for data designs and solutions Experience in agile process and technology Strong oral and written communication skills; Demonstrated ability to clearly articulate information or solution Experience leading a high performing work team Good time management skills (i.e. works efficiently) Excellent attention to detail Ability to work within a team environment as well as independently Ability to identify, analyze and solve complex problems Strong team player who is able to work across multiple functions and disciplines Expertise with the design and development of ETL data solutions Understanding of relational databases, data integration tools:', 'In lieu of education, 4+ years of experience in Data analysis, business analysis, data application programming, ETL development, next generation databases, or related', 'Good time management skills (i.e. works efficiently)', 'Health Insurance: We offer a variety of comprehensive medical, dental, and vision plans with low out-of-pocket expenses', 'We invite you to join our mission', 'Experience leading a high performing work team', 'Compensation', 'Major Responsibilities', ' $70,000 - $80,000 ', 'Provides data and technical consulting during data application design; Provide technical consulting on data composition and data engineering', 'Assists with the development of models, analytic processes, and reports; Provides guidance on the development of data consumption processes', 'Work Authorization/security Clearance Requirements', 'Expertise with the design and development of ETL data solutions', 'Demonstrated experience with integration technologies and how to leverage them into data mapping between systems', 'Experience with Cloud-based technologies:', 'Expertise in enabling business intelligence solutions through data integration; including roles that span the complete BI lifecycle, from strategy to ETL to report implementation', 'Basic Qualifications', 'Big Data stacks/ecosystem including Kafka, Spark, Python, NoSQL', 'Sponsorship is not available for this position', 'Ab Initio or Informatica or Datastage', 'Partner with business and technology teams to develop data applications. Identify business data ingestion and processing frameworks. Coordinates and obtains data application requirements from the business. Translates business requirements to development teams. Assist with development and testing processes. Coordinate data application and model deployments and validations.', 'API Nation, Integromat, Zapier']",Entry level,Full-time,Information Technology,Real Estate,2021-03-24 13:05:10
Data Engineer,Fluidigm Corporation,"South San Francisco, CA",2 weeks ago,46 applicants,"['', ' Build processes supporting data transformation, data structures, metadata, dependency and workload management. ', ' Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. ', ' Would you like to join an innovative team creating technology to power groundbreaking insights in academic, clinical, pharma and biotech research? ', ' Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and ‘big data’ technologies as necessary ', ' Strong project management and organizational skills. ', ' Experience with big data tools like Hadoop, Tableau, AWS cloud services like EC2 and Redshift is a plus ', ' A successful history of manipulating, processing and extracting value from large disconnected datasets. ', 'At Fluidigm We Are Also Building a Positive Culture Where Our People Can Do The Best Work Of Their Careers, Informed And Influenced By Our Core Values', ' Familiarity with Dynamics AX, Salesforce, Azure cloud, and Synapse is a strong plus ', ' Experience supporting and working with cross-functional teams in a dynamic environment. ', ' Experience with object-oriented/object function scripting languages such as Python, Java, JavaScript is a plus ', ' Step up. ', ' Create and maintain optimal data pipeline architecture ', ' Work with cross functional stakeholders to design and architect analytics solutions and data warehouses as necessary ', ' Experience building and optimizing data pipelines, architectures and data sets. ', ' Build and test functional prototypes for BI, data discovery, and analytics solutions. ', ' Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases ', ' Create what customers need next. ', ' Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores. ', ' Collaborate and learn. ', ' Create what customers need next.  Drive to make a difference.  Collaborate and learn.  Step up. ', ' Data Engineer ', ' Evaluate datasets utilizing statistics, data mining, and predictive analytics to gain additional business insights ', ' Strong Power BI experience is required ', ' Create and maintain visualization, data sets, data models using Power BI ', 'Responsibilities', ' Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. ', 'Qualifications', ' Work with stakeholders to assist with data-related technical issues and support data infrastructure needs ', ' 5+ years of experience in a Data Engineer role, who has attained a Bachelor’s degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. ', ' Assemble large, complex data sets that meet functional / non-functional business requirements. ', ' Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics. ', ' Strong analytic skills related to working with datasets. ', ' 5+ years of experience in a Data Engineer role, who has attained a Bachelor’s degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.  Strong Power BI experience is required  Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases  Familiarity with Dynamics AX, Salesforce, Azure cloud, and Synapse is a strong plus  Experience building and optimizing data pipelines, architectures and data sets.  Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.  Strong analytic skills related to working with datasets.  Build processes supporting data transformation, data structures, metadata, dependency and workload management.  A successful history of manipulating, processing and extracting value from large disconnected datasets.  Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.  Strong project management and organizational skills.  Experience supporting and working with cross-functional teams in a dynamic environment.  Experience with big data tools like Hadoop, Tableau, AWS cloud services like EC2 and Redshift is a plus  Experience with object-oriented/object function scripting languages such as Python, Java, JavaScript is a plus ', ' Drive to make a difference. ', 'Job Description', ' Create and maintain visualization, data sets, data models using Power BI  Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.  Build and test functional prototypes for BI, data discovery, and analytics solutions.  Assemble large, complex data sets that meet functional / non-functional business requirements.  Evaluate datasets utilizing statistics, data mining, and predictive analytics to gain additional business insights  Work with cross functional stakeholders to design and architect analytics solutions and data warehouses as necessary  Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and ‘big data’ technologies as necessary  Create and maintain optimal data pipeline architecture  Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.  Work with stakeholders to assist with data-related technical issues and support data infrastructure needs ', 'Position Summary']",Entry level,Full-time,Information Technology,Medical Devices,2021-03-24 13:05:10
Data Engineer,"Ashley Ellis, Inc","Dallas, TX",3 weeks ago,Be among the first 25 applicants,"['Ability to clearly communicate capabilities, opportunities, and recommendations to both technical and nontechnical audiences', 'www.paladininc.com.', 'Expertise in translating business requirements to project design, development, and execution', 'Skills & Qualifications', 'Troubleshoot & determine best resolution for data issues and anomalies', 'Expertise in Data Analysis, Data Profiling, and SQL Tuning', 'Prepare high-level ETL mapping specifications.', 'Work Location:\xa0', 'Duration: ', 'Work with team leads to prioritize business and information needs', 'For more information or to view other opportunities, visit us at www.paladininc.com.', 'For more information or to view other opportunities, visit us at', 'Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy', 'Advanced SQL skills - Adept at queries, report writing and presenting findings', ""Bachelor's degree (BA/BS) in a related field such as information systems, mathematics, or computer science or equivalent work experience. "", ""Database Performance Tuning, Database Management, Requirements Analysis, Software Development Fundamentals, Problem Solving, Documentation Skills, Verbal Communication, Data Maintenance, Database Security, Promoting Process Improvement, System Administration.Advanced SQL skills - Adept at queries, report writing and presenting findingsExpertise in Data Analysis, Data Profiling, and SQL TuningExpertise in translating business requirements to project design, development, and executionStrong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracyAbility to clearly communicate capabilities, opportunities, and recommendations to both technical and nontechnical audiencesBachelor's degree (BA/BS) in a related field such as information systems, mathematics, or computer science or equivalent work experience. Requires technical and business knowledge in multiple disciplines/processes. Typically has 5-7+ years of relevant work experience. Consideration given to equivalent combination of education and experience.\xa0"", 'Interpret and analyze data from various source systems to support data integration and data reporting needs.', 'Job Title:\xa0', 'Identify, analyze, and interpret trends or patterns in complex data sets', 'Typically has 5-7+ years of relevant work experience. Consideration given to equivalent combination of education and experience.\xa0', 'Database Performance Tuning, Database Management, Requirements Analysis, Software Development Fundamentals, Problem Solving, Documentation Skills, Verbal Communication, Data Maintenance, Database Security, Promoting Process Improvement, System Administration.', 'Requires technical and business knowledge in multiple disciplines/processes. ', 'Education/Experience Required:\xa0E', 'Skills & Qualifications:', 'Job Title:\xa0Data EngineerWork Location:\xa0Dallas, TXDuration: 10 month contract with option to extend\xa0Education/Experience Required:\xa0Expertise in Data Analysis, Data Profiling, and SQL Tuning; Advanced SQL skills - Adept at queries, report writing and presenting findingsJob Description & Responsibilities:', 'Develop complex code data scripts (Primarily SQL) for ETL', 'Job Description & Responsibilities', 'Manage exploratory data analysis to support database and dashboard development, as well as advanced analytics efforts', 'Interpret and analyze data from various source systems to support data integration and data reporting needs.Identify, analyze, and interpret trends or patterns in complex data setsWork with team leads to prioritize business and information needsPrepare high-level ETL mapping specifications.Develop complex code data scripts (Primarily SQL) for ETLTroubleshoot & determine best resolution for data issues and anomaliesManage exploratory data analysis to support database and dashboard development, as well as advanced analytics efforts', 'Job Description & Responsibilities:']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer/Data Engineering Manager,Lawrence Harvey,"Chicago, IL",4 weeks ago,98 applicants,"['', 'Bonus points if:\xa0', 'AWS Solution Architect or Azure Architect or GCP architect certification.', 'Data Engineering Consultant', 'Design and build real-time analytics solutions and work alongside data architects', 'They are looking for a Spark or Cloud Data engineering professionals who are also client-facing. ', 'Minimum 1 year of designing and building streaming data ingestion, analysis and processing pipelines using Kafka, Kafka Streams, Spark Streaming and similar cloud native technologies.', 'Translate objectives into a scalable solution that meets end customers’ needs within deadlines.', 'AWS Solution Architect or Azure Architect or GCP architect certification.Experience designing and building Big Data ETL pipelines using Talend or Informatica technologiesExperience building Business Data Catalogs or Data Marketplaces for powering business analytics using technologies such as Alation, Collibra, Informatica or custom solutions.', 'Responsibilities:', 'Develop Cloud Native architecture', 'Design and build real-time analytics solutions and work alongside data architectsDevelop Cloud Native architecturePrototype and Test end to end data supply chainDevelop use cases that drive business value for clientsProvide architecture support to the data scientists.Translate objectives into a scalable solution that meets end customers’ needs within deadlines.', 'Provide architecture support to the data scientists.', '\xa0', ""Bachelor's or Master’s in Computer Science, Engineering, Technical Science2+ years designing, implementing large scale data pipelines for data curation, feature engineering and machine learningExperience using Python, Spark, pySpark, Java, or Scala; either on AWS or AzureMinimum 1 year of designing and building streaming data ingestion, analysis and processing pipelines using Kafka, Kafka Streams, Spark Streaming and similar cloud native technologies."", 'Bonus points if:', 'Qualifications', ""Bachelor's or Master’s in Computer Science, Engineering, Technical Science"", 'My client is a world-leading Fortune Global 500 consultancy with leading capabilities in Digital, Cloud and Security. They employ over 500,000 people and serve clients in more than 120 countries. As of 2020, they work with 91 of the Fortune Global 100 and have appearances on Fortune’s “World’s Most Admired Companies” numerous times.', 'Prototype and Test end to end data supply chain', 'Experience using Python, Spark, pySpark, Java, or Scala; either on AWS or Azure', 'Experience building Business Data Catalogs or Data Marketplaces for powering business analytics using technologies such as Alation, Collibra, Informatica or custom solutions.', 'Also there Is a Data Manager position available as well!', '2+ years designing, implementing large scale data pipelines for data curation, feature engineering and machine learning', 'Develop use cases that drive business value for clients', 'Experience designing and building Big Data ETL pipelines using Talend or Informatica technologies', 'They are looking for a Data Engineering Consultant to join their Applied Intelligence team. The team is one of the world’s largest team of data scientists, data engineers, and experts in machine learning and AI.\xa0']",Associate,Full-time,Information Technology,Management Consulting,2021-03-24 13:05:10
ETL /Data Engineer,Allwyn Corporation,"Washington, DC",4 weeks ago,Be among the first 25 applicants,"['', 'Minimum Requirements', 'Responsibilities']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Lead Data Engineer ,BICP,"Portland, OR",2 weeks ago,28 applicants,"['', 'Experience developing solutions in SnowflakeExperience with workload automation tools such as Airflow, Autosys.Kubernetes, Lambda, Spark Streaming', 'Strong experience developing with PySpark, preferably leveraging AWS EMR managed service', 'Experience developing with scripting languages such as Shell and Python', '3+ years of experience with data engineering with emphasis on data analytics and reporting', 'Key Skills:', 'Expert experience with SQL and Relational database engineering (Oracle, SQL Server, Teradata)— expert-level SQL abilities', 'Experience developing solutions in Snowflake', 'Nice to Have:', 'If you’re looking to join an organization where there is tremendous growth opportunity, that operates with transparency and with a highly collaborative approach then BICP could be a great place to accelerate your career. We offer excellent compensation and these roles will be 100% remote for for foreseeable future.', 'Ability to partner with business and technology team members, to understand business requirements and translate those into value-add technology solutions', 'Experience with workload automation tools such as Airflow, Autosys.', 'Kubernetes, Lambda, Spark Streaming', '3+ years of experience with data engineering with emphasis on data analytics and reportingExperience developing with scripting languages such as Shell and PythonStrong experience developing with PySpark, preferably leveraging AWS EMR managed serviceExpert experience with SQL and Relational database engineering (Oracle, SQL Server, Teradata)— expert-level SQL abilitiesExperience with agile delivery methodologies– Scrum, SAFe, Extreme ProgrammingExperience working with source-code management tools such as GitHub and JenkinsAbility to partner with business and technology team members, to understand business requirements and translate those into value-add technology solutions', 'BICP, a market leader in BI, Advanced Analytics and Innovation Platforms, is looking to hire two (2) Lead Data Engineers to support new cloud transformation initiatives at our long standing Fortune 100 Retail client in Beaverton, OR. We currently have a team on-site supporting multiple groups with their drive to deploy NextGen platforms and applications capable of powering real time analytics. Ideal candidate will have 3+ years Data Engineering experience in large enterprise environments dealing with massive volumes of data. Consultant will have prior experience supporting cloud based, analytics-driven transformation efforts from legacy to NextGen platforms. Sr. Data Engineer will be expected to Develop and support data solutions in support of reporting and analytics requirements; Engage with product owner, technology lead, report developers, product analysts, and business partners to understand capability requirements and develop data solutions based on product backlog priorities. Additional key skills and qualifications below:', 'About BICP', 'Experience working with source-code management tools such as GitHub and Jenkins', 'BICP is a leading-edge consulting firm focused on delivering innovative and transformative BI&A, Advanced Analytics and Big Data solutions to our customers.\xa0With deep experience across a diverse ecosystem of NextGen platforms, Analytics and Data Science applications we have the required product ambiguity and expertise to deliver truly best in breed solutions customized to our client’s specific business needs.', 'Experience with agile delivery methodologies– Scrum, SAFe, Extreme Programming']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Micasa Global,"Houston, TX",2 weeks ago,Be among the first 25 applicants,"['Job Description', '']",Entry level,Contract,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,"Solidus Technical Solutions, Inc.","Hanscom AFB, MA",22 hours ago,Be among the first 25 applicants,"['', 'Responsibilities', 'Experience implementing and building event driven architectures - Familiarity with event driven finite state machines', 'High proficiency in SQL to include schema design, data definition, and advanced queries', ' Experience implementing and building event driven architectures - Familiarity with event driven finite state machines High proficiency in SQL to include schema design, data definition, and advanced queries Experience with MPP data warehouses Experience with scripting languages for automating repetitive tasks - Experience with creating automated data pipelines for complex systems - Extensive linux server management background Can prototype visualizations with lightweight data visualization suites - Excellent verbal and written communications skills along with the ability to present technical data and approaches to both technical and non-technical audiences ', 'Can prototype visualizations with lightweight data visualization suites - Excellent verbal and written communications skills along with the ability to present technical data and approaches to both technical and non-technical audiences', 'Experience with MPP data warehouses', 'Experience with scripting languages for automating repetitive tasks - Experience with creating automated data pipelines for complex systems - Extensive linux server management background']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Spectraforce Technologies,"Seattle, WA",2 weeks ago,Be among the first 25 applicants,"['', "" Leverage operational standards for monitoring and alerting provided by Client's Enterprise Data Platform team.."", 'Knowledge And Skill Requirements', 'Duration: ', ' AWS Certifications on either a Developer or Architect is preferred but not required', ' Demonstrated technical background in ETL development and data warehousing concepts is preferred', ' Demonstrated experience writing optimized SQL queries across large data sets', 'Equal Opportunity Employer: ', ' For the full list of our open positions, please check www.spectraforce.com', 'Primary Responsibilities (Essential Functions)', ' Work with other data engineers and marketers to identify and scope of the requests and define implementation plans.', 'Job Title: Data Engineer', 'Benefits: ', 'Experience', ' Build solutions that scale as our data volumes grow exponentially.', ' BA/BS in Computer Science or equivalent', 'Seattle, WA 98121', ' Regular, predictable job attendance.', ' Experience with Snowflake is highly preferred', ' Develop data quality automations and unit tests to ensure the accuracy of the data delivered to the Analysts and Business Customers.', 'Location:', ' Experience with containers and container orchestration tools such as Docker and Kubernetes are nice to have', ' Experience with CI/CD pipelines are nice to have', ' Build robust scalable data processing and data integration pipelines using Python and SQL.', ' Develop data models that support analytical models used by Client.', ' 3+ years professional experience in Software Engineering role', '  For the full list of our open positions, please check www.spectraforce.com', ' Experience with Git', ' Participate in agile development ceremonies, working to deliver within a sprint.', ' Demonstrated experience writing Python', 'Position Summary', ' 8 Months (Possible Temp to Perm) ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,JupiterOne,Salt Lake City Metropolitan Area,4 weeks ago,Be among the first 25 applicants,"['', 'Apache Hadoop', 'Experience working with graph databases', 'Serverless (AWS Lambda and AWS Fargate)', 'Python', 'Apache Spark', 'Able to guide other engineers on data engineering best practices', 'Able to make practical decisions regarding data storage and retrieval', 'Apache Hive', 'Redis', 'SQL', 'JVM languages (Java, Scala, etc.)', 'Elasticsearch', 'Experience with DevOps automation and Infrastructure as Code with tools like Terrafom or CloudFormation', 'GraphQL', '3+ years of experience in a Data Engineering roleExperience with big data tools such as Hadoop, Athena, Apache Hive, and Apache SparkAble to write production code that is reliable and easy to supportAble to guide other engineers on data engineering best practicesUnderstands the importance of maintaining customer data privacy and security complianceAble to make practical decisions regarding data storage and retrievalAble to help architect a data ingestion and query pipeline that enables efficient and cost efficient data science use cases', 'Node.js', 'Able to write production code that is reliable and easy to support', 'Willing to explore new approaches to solving problems and challenging the status quo', 'TypeScript', 'Experience with big data tools such as Hadoop, Athena, Apache Hive, and Apache Spark', 'Docker', 'Understands the importance of maintaining customer data privacy and security compliance', 'Able to help architect a data ingestion and query pipeline that enables efficient and cost efficient data science use cases', '3+ years of experience in a Data Engineering role', 'Eager to improve processes via automation', 'JVM languages (Java, Scala, etc.)PythonSQLNode.jsTypeScriptApache SparkApache HiveApache HadoopServerless (AWS Lambda and AWS Fargate)DockerAWS (Athena, Neptune, Lambda, API Gateway, DynamoDB, Kinesis, SQS, S3, Comprehend, etc.)GraphQLElasticsearchRedisTerraform', 'Experience working with micro-services', 'Able to work independently', 'Terraform', 'Able to work independentlyExperience working with graph databasesExperience working with micro-servicesExperience with DevOps automation and Infrastructure as Code with tools like Terrafom or CloudFormationWilling to explore new approaches to solving problems and challenging the status quoEager to improve processes via automation', ' Key Qualifications ', 'AWS (Athena, Neptune, Lambda, API Gateway, DynamoDB, Kinesis, SQS, S3, Comprehend, etc.)']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
"Associate Data Engineer, Commercial Analytics",EAB,"Washington, DC",1 day ago,27 applicants,"['', ' Medical, dental, and vision insurance; dependents and domestic partners eligible ', ' Leverage best practices in continuous integration and delivery. ', ' Infertility treatment coverage and adoption or surrogacy assistance ', 'Associate', '  Bachelor’s degree, ideally in an analytical/quantitative field (e.g. computer science, statistics, etc.).   2+ years of experience using SQL to manage complex and disparate data.   Strong interpersonal skills, including the ability to interpret nuanced, contextual requests.   Self-starter with proven ability to stay organized and multi-task in a fast-paced environment.   Strong desire to solve challenging problems, and to constantly improve technical/analytical skills by learning new methods.  ', ' Dynamic growth opportunities with merit-based promotion philosophy ', ' Strong project management skills. ', 'Ideal Qualifications', ' 20+ days of PTO annually, in addition to paid firm holidays ', ' Use SQL to develop & maintain efficient and reliable data pipelines that support advanced analytics. ', 'About EAB', 'Primary Responsibilities', ' Phase Back to Work program for employees returning from parental leave ', '  Experience with Salesforce or other customer relationship management (CRM) systems.   Strong project management skills.   Programming experience with R and/or Python.   Experience in server/database administration.  ', '  Consistent with our belief that our employees are our most valuable resource, EAB offers a competitive and inclusive benefits package.   Medical, dental, and vision insurance; dependents and domestic partners eligible   401(k) retirement plan with company match   20+ days of PTO annually, in addition to paid firm holidays   Daytime leave policy for community service or fitness activities (up to 10 hours a month each)   Paid parental leave for birthing and non-birthing parents   Phase Back to Work program for employees returning from parental leave   Infertility treatment coverage and adoption or surrogacy assistance   Wellness programs including gym discounts and incentives to promote healthy living   Dynamic growth opportunities with merit-based promotion philosophy   Benefits kick in day one, see the full details here.  ', ' Strong desire to solve challenging problems, and to constantly improve technical/analytical skills by learning new methods. ', ' Consistent with our belief that our employees are our most valuable resource, EAB offers a competitive and inclusive benefits package. ', ' Benefits kick in day one, see the full details here. ', ' Wellness programs including gym discounts and incentives to promote healthy living ', 'Commercial Analytics', ' Programming experience with R and/or Python. ', ' Make commercial performance data simpler and easier to use for analysts and data scientists. ', 'The Role In Brief', ' 401(k) retirement plan with company match ', ' Paid parental leave for birthing and non-birthing parents ', ' Collaborate with your colleagues to understand team needs and discover the best solutions. ', ' Source, map, and integrate external data sets in to existing infrastructure, including data from newly acquired business units. ', ' Strong interpersonal skills, including the ability to interpret nuanced, contextual requests. ', ' Self-starter with proven ability to stay organized and multi-task in a fast-paced environment. ', ' 2+ years of experience using SQL to manage complex and disparate data. ', 'Benefits', 'Data Engineer', ' Help drive optimization, testing and tooling to improve data quality. ', '  Use SQL to develop & maintain efficient and reliable data pipelines that support advanced analytics.   Make commercial performance data simpler and easier to use for analysts and data scientists.   Source, map, and integrate external data sets in to existing infrastructure, including data from newly acquired business units.   Collaborate with your colleagues to understand team needs and discover the best solutions.   Leverage best practices in continuous integration and delivery.   Help drive optimization, testing and tooling to improve data quality.  ', 'Basic Qualifications', ' Bachelor’s degree, ideally in an analytical/quantitative field (e.g. computer science, statistics, etc.). ', ' Experience in server/database administration. ', ' Daytime leave policy for community service or fitness activities (up to 10 hours a month each) ', ' Experience with Salesforce or other customer relationship management (CRM) systems. ']",Associate,Full-time,Education,Education Management,2021-03-24 13:05:10
Data Engineer,FLYR Labs (We're Hiring),"San Francisco, CA",2 weeks ago,130 applicants,"['', 'At Our SFHQ', 'At FLYR, we are proud to be an equal opportunity workplace and embrace a commitment to have our teams better reflect the world around us as we scale the business.', '• Generous PTO policy and paid holidays', 'Engage with customers from data discovery to ETL development, to data QA/QC metrics determination and delivery SLAs.\xa0', 'Experience working with large-scale, complicated datasets.', 'Cirrus™, our Revenue Operating System, is deployed by major commercial airlines both internationally and domestically. Cirrus leverages an extensive data- and deep learning infrastructure that was built from the ground up to solve airlines’ most complex commercial challenges.', '• Dog-friendly Candidate Selection Process', '• Technical Assignment/Deep Dive - A brief technical assignment touching the necessary skills listed above provided on a take-home basis to be utilized during the onsite.', 'Previous experience building and operating data transformation pipelines.', '• High career growth potential', ""Founder Alex Mans started FLYR in 2013, initially with the intent to remove inefficiencies in airline pricing for travelers. Since then, we've shifted focus to address the forecasting and pricing problems at the core of major airlines, and we have built a team with a wide range of experiences in enterprise technology, aviation, and entrepreneurship. We are a dynamic and fast-growing travel tech start-up headquartered in the SoMa neighborhood in San Francisco with a European development hub in Krakow, Poland."", 'Develop and implement best practices and standardized, repeatable processes for implementing enterprise-grade data transformation/ingest pipelines, with an eye toward scalability.\xa0', 'Quantitative work/education background (computer science or equivalent).', 'About FLYR’s product', ""Collaborate closely with FLYR's Poland-based data platform team to define tool requirements to support onboarding and ingest of new data sources."", 'Hands-on experience with cloud computing services, Airflow, and BigQuery.', '• Initial Screening - Active candidates are screened against the criteria listed above.', 'Our hyper-accurate contextual forecasts enable the most effective scheduling, marketing, and leadership decisions while directly managing the pricing for billions of dollars worth of product and revenue.', 'Learn our customers’ business needs and apply business acumen to ensure the success of highly technical projects.\xa0', 'Quantitative work/education background (computer science or equivalent).Ability to ship production-quality Python code.Hands-on experience with cloud computing services, Airflow, and BigQuery.Previous experience building and operating data transformation pipelines.Experience working with large-scale, complicated datasets.Experience with data warehouses, ETL automation, BI visualization tools, and cloud-based data management tools.Advanced SQL. You know your way around analytical and aggregate functions, complex joins, window functions, and are confident in wrangling all types of data in SQL.Can identify impediments to customer onboarding and propose improvements to processes. Can work with Product, Data Science, and Data Platform teams to define product and platform capabilities to improve customer onboarding.Ability to clearly communicate status, blockers, risks, and dependencies to FLYR and customer stakeholders.', 'Ability to ship production-quality Python code.', ""Complete complex customer ingests and data warehousing projects.Develop and implement best practices and standardized, repeatable processes for implementing enterprise-grade data transformation/ingest pipelines, with an eye toward scalability.\xa0Engage with customers from data discovery to ETL development, to data QA/QC metrics determination and delivery SLAs.\xa0Develop and implement best practices and standardized, repeatable processes for implementing enterprise-grade data transformation/ingest pipelines, with an eye toward scalability.\xa0Learn our customers’ business needs and apply business acumen to ensure the success of highly technical projects.\xa0Collaborate closely with FLYR's Poland-based data platform team to define tool requirements to support onboarding and ingest of new data sources.Defining platform data validation test suites.\xa0Provide mentorship and training to newer crew members."", '• Equity in Series B startup with high growth potential', '• Comprehensive healthcare plans (Choice of PPO & HMO available)', '• 401K with company match', '• Herman Miller chairs and Autonomous SmartDesks', 'Experience with data warehouses, ETL automation, BI visualization tools, and cloud-based data management tools.', 'Our Selection Process:', ""While the effects of COVID-19 on commercial aviation (our primary client base) have been widely publicized, demand from airlines around the globe has exceeded all expectations. On the back of incredible outperformance compared to legacy incumbents in our space, we are on track for an exceptionally strong 2021 and beyond. While the road to travel recovery isn’t easy, you won't find any other industry as dedicated to continuous improvement and reinvention."", 'Can identify impediments to customer onboarding and propose improvements to processes. Can work with Product, Data Science, and Data Platform teams to define product and platform capabilities to improve customer onboarding.', '\xa0', 'Our Vision', 'Complete complex customer ingests and data warehousing projects.', 'Starting with airlines, we provide the Cirrus Revenue Operating System™ that reshapes how travel and transportations businesses plan their commercial operation. We displace legacy data, forecasting, pricing, and reporting solutions with a single enterprise SaaS platform that leverages the latest advancements in deep learning, cloud computing, and user experience.', '• Introductory Call - A FLYR crew member will reach out for an initial call to learn more about each other.', 'What will your destination look like?', '• Flexible working arrangements — full-time remote eligible from anywhere in US', '• Onsite/Virtual Onsite - A series of meetings with the hiring team/team members to finalize our evaluation.', 'Defining platform data validation test suites.\xa0', 'Ability to clearly communicate status, blockers, risks, and dependencies to FLYR and customer stakeholders.', 'Our Commitment to Fairness', ""• Follow-up Call - A more in-depth call focusing more on the specifics of candidates' finance backgrounds."", 'Advanced SQL. You know your way around analytical and aggregate functions, complex joins, window functions, and are confident in wrangling all types of data in SQL.', 'We are on a path to become the single largest provider of commercial intelligence and automation across the travel and transportation industry.', '• Follow-up Call - A hiring manager at FLYR will have a follow-up, in-depth conversation with candidates.', 'What can you bring on this trip?', 'Provide mentorship and training to newer crew members.', 'Impact of COVID-19', 'Benefits ', '• Bright, modern office in excellent SoMa location']",Mid-Senior level,Full-time,Information Technology,Internet,2021-03-24 13:05:10
Data Engineer,Pondurance,United States,4 weeks ago,29 applicants,"['', 'Degree or pursuing Degree in Computer Science, Engineering, or a related technical discipline and/or equivalent experiencePrior development experience requiredStrong technical and analytical skills', 'Database programming, SQL, etc.', 'Data science and machine learning experienceDatabase programming, SQL, etc.Domain-Specific Language designAgile DevelopmentExperience creating or contributing to open source projects.Experience with statistical programming and graphingGPU programming experience, CUDA, etc.', 'Scripting languages and data science experience, python, ruby, etc.', 'our customers', 'Systems programming experience and concurrent programming, Rust preferredExperience with API design and maintenance, REST or similar.Strong data modeling skillsScripting languages and data science experience, python, ruby, etc.Familiarity with message queues, KafkaComfortable with Git/version control workflows.Familiarity with Continuous Integration (CI), Github Actions, etc.Highly organized, able to multitask, the ability to work individually, within a team, and with other groups.', 'GPU programming experience, CUDA, etc.', 'Strong technical and analytical skills', 'Are you ready to join a team of passionate, dedicated professionals who wake up every day to make the digital world a better place?\xa0', 'Data science and machine learning experience', 'About The Role', 'Domain-Specific Language design', 'Strong data modeling skills', 'Experience creating or contributing to open source projects.', 'Familiarity with Continuous Integration (CI), Github Actions, etc.', 'Systems programming experience and concurrent programming, Rust preferred', 'Pondurance is an Equal Opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex including sexual orientation and gender identity, national origin, disability, protected veteran status, or any other characteristic protected by applicable federal, state, or local law.', ""What You'll Need"", 'Highly organized, able to multitask, the ability to work individually, within a team, and with other groups.', 'We are committed to building an inclusive culture of teamwork that embraces the diversity of our people and reflects the diversity of the communities in which we work, the customers, agencies and organizations we serve, and the enable us to deliver on our mission while allowing our employees to live a balanced life.', 'Our mission at Pondurance is to ensure that every organization is able to detect and respond to cyber threats – regardless of size, industry or current in-house capabilities. We believe AI and automation alone aren’t enough, you also need experienced human intervention because behind every nefarious cyber-attack is a person with their own mission. We combine our advanced threat intelligence platform with decades of human intelligence to speed detection and response and contain cybersecurity threats quickly to ultimately decrease risk to\xa0our customers\xa0mission - and we need you to help us.\xa0', 'Experience with API design and maintenance, REST or similar.', 'Prior development experience required', 'Skills And Experience', 'The purpose of the role is for the chosen candidate to understand the existing set of technologies and assist in the conception, architecture, design, and quality of data processing tools and algorithms. This position will work closely with software engineers to provide the data back to customers, and also with the Security Analysts to ensure applicability of enhancements to detect and address challenging problems in cybersecurity.', 'Let’s redefine the security and cyber risk landscape together.\xa0', 'We strive to provide an environment where high performing teams apply their diverse perspectives,\xa0to make informed decisions and collectively solve industry and customer problems.\xa0In so, we can attract and retain talent from all backgrounds and create an environment where everyone feels empowered to bring their full, authentic selves to work.', 'Nice To Have, but always a plus', 'We believe our people are what makes us different. As team members in Pondurance, we offer flexible work arrangements to help our people manage their personal and professional lives in the complex remote world we live in. We believe in transparency and fairness in all relationships and that trust and empathy towards our clients and partners, towards each other and within our communities is the best foundation for success.\xa0', 'Comfortable with Git/version control workflows.', 'Agile Development', 'Familiarity with message queues, Kafka', 'Degree or pursuing Degree in Computer Science, Engineering, or a related technical discipline and/or equivalent experience', 'As a Data Engineer on the Pondurance Data Pipeline Team, you will build, manage, and maintain the Managed Detection and Response Data Pipeline. The pipeline is responsible for collecting, processing, and enhancing security-related data used to monitor and protect our customer’s environments. ', 'Experience with statistical programming and graphing']",Mid-Senior level,Full-time,Information Technology,Computer & Network Security,2021-03-24 13:05:10
Data Engineer - Job ID 12852,Infor,United States,2 weeks ago,45 applicants,"['', 'What You Will Need', 'Bachelors (or foreign equivalent) in Computer Science, Electrical Engineering or a related quantitative discipline from an accredited university.Demonstrated real-world experience in building and orchestrating big data pipelines of structured and unstructured data sets.Experience building data-centric analytics solutions.Understanding of basic data science models: Linear regression, Logistic regression, SVM, K-mean, Decision trees, etc.Good data modeling and problem-solving skills.Proficiency in development using a JVM-based language. Scala is preferred.Hands-on experience with frameworks in the Big Data ecosystem. Spark, Delta Lake, and Neo4j are preferred.A strong collaborative mindset with great interpersonal skills to help solve complex business problems.', 'Who is Infor? ', 'A Day In The Life Typically Includes', 'Preferred Qualifications', 'Learn and adopt new technologies fast.', 'Advanced proficiency in Apache Spark and Scala.Real-world experience with cloud-based server monitoring data and workflows strongly desired.Familiarity with AI/ML techniques such as boosted models, tree-based models, neural networks, etc.Demonstrated real-world experience in mathematical modeling, data science methodologies, and coding.Hands-on experience with one major cloud platform is preferred.Excellent communication skills (e.g., the ability to communicate effectively and efficiently with a broad range of audiences.)A proven desire to continue learning new technologies and techniques.', 'What Will Put You Ahead?', 'Location: Remote', 'Demonstrated real-world experience in mathematical modeling, data science methodologies, and coding.', 'Good data modeling and problem-solving skills.', 'Bachelors (or foreign equivalent) in Computer Science, Electrical Engineering or a related quantitative discipline from an accredited university.', 'Build reusable, scalable, and reliable big data pipelinesSupport all facets of machine learning based software deploymentsAdd high-value data analytics capabilities to existing Infor products.Timely deliver projects and products.Learn and adopt new technologies fast.Be a team player.', 'Add high-value data analytics capabilities to existing Infor products.', ':', 'Familiarity with AI/ML techniques such as boosted models, tree-based models, neural networks, etc.', 'Support all facets of machine learning based software deployments', 'Watch to find out!', 'S', 'A', 'De', 'About Infor', 'Demonstrated real-world experience in building and orchestrating big data pipelines of structured and unstructured data sets.', 'Timely deliver projects and products.', 'Excellent communication skills (e.g., the ability to communicate effectively and efficiently with a broad range of audiences.)', 'Be a team player.', 'A proven desire to continue learning new technologies and techniques.', 'Advanced proficiency in Apache Spark and Scala.', 'Experience building data-centric analytics solutions.', 'Proficiency in development using a JVM-based language. Scala is preferred.', 'I', 'Data Engineer', 'Basic Qualifications', 'Hands-on experience with frameworks in the Big Data ecosystem. Spark, Delta Lake, and Neo4j are preferred.', 'Infor Values', 'Hands-on experience with one major cloud platform is preferred.', 'Understanding of basic data science models: Linear regression, Logistic regression, SVM, K-mean, Decision trees, etc.', 'A strong collaborative mindset with great interpersonal skills to help solve complex business problems.', 'Real-world experience with cloud-based server monitoring data and workflows strongly desired.', 'Position Summary', 'Build reusable, scalable, and reliable big data pipelines']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer Intern,"DigiCert, Inc.","Lehi, UT",2 days ago,Be among the first 25 applicants,"['', 'Familiarity with the below:', 'object-oriented/object function scripting languages: Python, Go, etc.', 'Identify bottlenecks and implement fixes to maintain optimum pipeline health', 'Develop end-end data integration solutions', 'Knowledge of CI/CD engineering concepts', ' big data tools: Spark, Kafka, etc. pipeline and workflow management tools: Luigi, Airflow, etc. stream-processing systems: Storm, Spark-Streaming, etc. object-oriented/object function scripting languages: Python, Go, etc. ', 'Experience using GIT for version control', 'Key Responsibilities', 'Pursuing Bachelor’s degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field', 'Share team responsibilities for all aspects involved with maintaining a high level of uptime', 'Build processes supporting data transformation, data structures, metadata, dependency and workload management.', 'Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvements', 'Preferred Qualifications', 'This position will mainly interact with internal Data Engineering Infrastructure', 'LOCATION', 'Design and implement processes supporting data transformation, data structures, metadata, dependency and workload management.', 'Work with data and analytics experts to strive for greater functionality in our data systems', 'Working knowledge of large-scale data pipelines', 'POSITION', 'pipeline and workflow management tools: Luigi, Airflow, etc.', 'Digicert Benefits', 'Experience in defining and implementing testing framework', 'Requirements', 'stream-processing systems: Storm, Spark-Streaming, etc.', 'Working knowledge of message queuing, stream processing, and highly scalable data stores', 'big data tools: Spark, Kafka, etc.', ' Experience working on Data Engineering projects. Pursuing Bachelor’s degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field Knowledge of CI/CD engineering concepts Working knowledge of large-scale data pipelines Familiarity with the below:', 'About Digicert', 'Experience with MariaDB AX/MPP', ' Experience with MariaDB AX/MPP Experience with CDC, Debezium Experience with Kubernetes, Nutanix-Karbon Experience in defining and implementing testing framework Experience with relational SQL and NoSQL databases: MariaDB, PostgreSQL, Cassandra Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvements Working knowledge of message queuing, stream processing, and highly scalable data stores ', ' This position will mainly interact with internal Data Engineering Infrastructure Design and implement processes supporting data transformation, data structures, metadata, dependency and workload management. Identify bottlenecks and implement fixes to maintain optimum pipeline health Build processes supporting data transformation, data structures, metadata, dependency and workload management. Develop end-end data integration solutions Share team responsibilities for all aspects involved with maintaining a high level of uptime Work with data and analytics experts to strive for greater functionality in our data systems ', 'Knowledge of building containerized applications', 'Experience with CDC, Debezium', 'Experience with Kubernetes, Nutanix-Karbon', 'Experience working on Data Engineering projects.', 'Experience with relational SQL and NoSQL databases: MariaDB, PostgreSQL, Cassandra']",Internship,Internship,Information Technology,Information Technology and Services,2021-03-24 13:05:10
"Lead, Data Engineer ",Macy's,New York City Metropolitan Area,5 days ago,Be among the first 25 applicants,"['', 'Work with counterparts from Tech to build frameworks that integrate data pipelines and machine learning models that facilitate use by data scientists for priority use cases; Enterprise Data and Analytics team focused on ""last mile"" transformations on select data required for use cases', 'Job Overview:', 'Compliance with relevant laws and regulations , in partnership with Legal/Privacy', 'BA/BS degree required\xa0', 'Support business users in identifying the correct data sets and providing easy to use tools to pull data', 'Work with Data Architect to implement the data models, standards and quality rules', 'Quantity, type, and quality of databases and pipelines, in partnership with TechnologyCompliance with relevant laws and regulations , in partnership with Legal/PrivacyAutomation of data cleansing and harmonization processes in refined/trusted zones', 'Culture', 'Key Performance Indicators', ""Embody data-driven culture at Macy's"", 'Familiarity with data architecture, modeling and security', ""Macy's, Inc is building an Enterprise Data & Analytics team to further grow our capabilities in support of our mission to be a data-led, customer-centric company. This team will focus on accelerating impact from analytics, coordinating an enterprise-wide roadmap, and ensuring proper data governance and management. As a colleague on this team, the Lead, Data Scientist will help lead the charge to execute on our vision to build profitable lifetime customer relationships by embedding data & analytics at the heart of everything we do.\xa0\xa0\xa0\xa0\xa0"", 'Essential Functions:', 'Adhere to processes to ensure data pulled from various sources meets quality standards, is curated and enhanced for analytical use and there is a ""single source of truth""Work with counterparts from Tech to build frameworks that integrate data pipelines and machine learning models that facilitate use by data scientists for priority use cases; Enterprise Data and Analytics team focused on ""last mile"" transformations on select data required for use casesMaintain database structure and standard definitions for business users across Macy\'sWork with data architects to build the foundational extract / load / transform process and regularly review the architecture and recommend effectiveness improvementsCollaborate with Technology to future-proof data & analytics software, tools and code to reduce risk and support pipeline ownersWork with Legal and Privacy teams to adhere to data privacy and security standardsWork with Data Architect to implement the data models, standards and quality rulesWork with the Data Science team to understand data formatting and sourcing needs to enable them to build out use cases as efficiently as possible', 'Qualifications:', ""Maintain database structure and standard definitions for business users across Macy's"", 'Work with the Data Science team to understand data formatting and sourcing needs to enable them to build out use cases as efficiently as possible', '\xa0', ""Community builder to foster data and analytics culture and to support data-driven thinking and discussions across priority analytics use caseEmbody data-driven culture at Macy'sBolster awareness, knowledge and conviction around data-driven practices and behaviors, increasing digital IQ of Business UnitsProactively raise data issues and take action to remediateSupport business users in identifying the correct data sets and providing easy to use tools to pull data"", 'Work with data architects to build the foundational extract / load / transform process and regularly review the architecture and recommend effectiveness improvements', 'Data Management & Preparation ', 'Work with Legal and Privacy teams to adhere to data privacy and security standards', 'Experience in distributed computing and enterprise environments', 'Proactively raise data issues and take action to remediate', ""This job description is not all inclusive. Macy’s Inc. reserves the right to amend this job description at any time. Macy's Inc. is an Equal Opportunity Employer, committed to a diverse and inclusive work environment.\xa0"", '5+ years of work experience, and at least 3 years of demonstrable experience in the role’s relevant technologies and field of expertise', 'Community builder to foster data and analytics culture and to support data-driven thinking and discussions across priority analytics use case', 'Bolster awareness, knowledge and conviction around data-driven practices and behaviors, increasing digital IQ of Business Units', 'The Lead Data Engineer will develop, test and maintain architecture, including databases and processing systems, to support a robust analytical pipeline that facilitates priority analytics use cases. The Lead Data Engineer will ensure these data processing capabilities meet business requirements, use case and user needs while providing reliable, efficient and quality data. ', 'Ability to write production level code, assess databases and leverage Big Data technologies on Google Cloud Platform to support downstream analytics', 'Automation of data cleansing and harmonization processes in refined/trusted zones', 'BA/BS degree required\xa05+ years of work experience, and at least 3 years of demonstrable experience in the role’s relevant technologies and field of expertiseAbility to write production level code, assess databases and leverage Big Data technologies on Google Cloud Platform to support downstream analyticsFamiliarity with data architecture, modeling and securityExperience in distributed computing and enterprise environmentsExcellent written and verbal communication skills with ability to read, write and interpret business and technical documents', 'Adhere to processes to ensure data pulled from various sources meets quality standards, is curated and enhanced for analytical use and there is a ""single source of truth""', 'Collaborate with Technology to future-proof data & analytics software, tools and code to reduce risk and support pipeline owners', 'Quantity, type, and quality of databases and pipelines, in partnership with Technology', 'Excellent written and verbal communication skills with ability to read, write and interpret business and technical documents']",Associate,Full-time,Science,Consumer Goods,2021-03-24 13:05:10
Platform Data Engineer (Remote),CrowdStrike,"Sunnyvale, CA",2 days ago,46 applicants,"['', 'Market leader in compensation and equity awardsCompetitive vacation policyComprehensive health benefitsPaid parental leave, including adoptionFlexible work environmentWellness programsStocked fridges, coffee, soda, and lots of treats', 'Market leader in compensation and equity awards', 'Created automated / scalable infrastructure and pipelines for teams in the past.', 'Flexible work environment', 'BS degree in Computer Science or related field.5+ years of relevant work experience.Good knowledge of some (or all) of AWS, Python, Golang, Kafka , Spark, Airflow, ECS, Kubernetes, etc to build infrastructure that can ingest and analyze billions of events per day.Good knowledge of distributed system design and associated tradeoffs.Good knowledge of CI / CD and associated best practices.Familiarity with Docker-based development and orchestration.', 'Familiarity with Docker-based development and orchestration.', 'BS degree in Computer Science or related field.', 'What You’ll Need', 'About The Role', 'Prior experience with Spinnaker, Relational DBs, or KV Stores.', 'Comprehensive health benefits', 'Competitive vacation policy', '5+ years of relevant work experience.', 'Contributed to the open source community (GitHub, Stack Overflow, blogging).', 'Good knowledge of CI / CD and associated best practices.', 'Prior experience in the cybersecurity or intelligence fields', 'Created automated / scalable infrastructure and pipelines for teams in the past.Contributed to the open source community (GitHub, Stack Overflow, blogging).Prior experience with Spinnaker, Relational DBs, or KV Stores.Prior experience in the cybersecurity or intelligence fields', 'Stocked fridges, coffee, soda, and lots of treats', 'Good knowledge of distributed system design and associated tradeoffs.', 'Good knowledge of some (or all) of AWS, Python, Golang, Kafka , Spark, Airflow, ECS, Kubernetes, etc to build infrastructure that can ingest and analyze billions of events per day.', 'Paid parental leave, including adoption', 'Wellness programs', 'Benefits Of Working At CrowdStrike', 'Also open for India Pune location.', 'Bonus points if you have…']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
IT Lead Data Lake Engineer,Zimmer Biomet,United States,,N/A,"['', 'Certification on MS-Azure architecture will be a plus', '5+ years’ relevant experience; OR Bachelor’s with Master’s degree in business, IT or related field with 3+ years’ relevant experience', 'Recognize problems and opportunities and provide value-added business solutions', 'Experience and understanding of Medical Device industry preferred', 'Travel Requirements', 'Zimmer Biomet is embarking on the next exciting chapter in its journey as a hi-tech Medtech company.', 'Experience programming/scripting to support Big Data engagements is a plus.', 'Expected Areas of Competence', 'Experience working effectively in a matrix organization and ambiguous work environment', 'Experience and core deep foundational and functional knowledge on big data Framework/Integration components and best practices', 'Good to have hands on Hadoop technologies including Hive, Sqoop, YARN, HBase, HDFS, SPARK, and/or Map Reduce', 'EOE/M/F/Vet/Disability', 'Experience working with cross functional teams within IT on security, Infrastructure on leading the GDPR, HIPAA compliance frameworks.', 'As a Zimmer Biomet team member, you will share in our commitment to providing mobility and renewed life to people around the world. This is why we offer you a competitive rewards package that includes medical, dental, vision, life and disability insurance, wellness incentives, employee assistance programs as well as paid time off for vacation and holidays.', 'Our teams develop advanced technologies such as our ROSA® robotics, our smartphone app mymobility®, our OrthoIntel Orthopedic Intelligence Platform, and more to improve patient outcomes; our patients experience next level care through our pioneering innovative technology solutions. This is part of the reason why Zimmer Biomet was selected as the 2019 Medtech Company of the Year; and, both 2018, 2019 & 2020 100 Best Places to Work in IT.', 'Having background on SAP is plus', 'Knowledge of Data Integration technologies like: Alteryx, SAP Data Services, SDI, and HANA Cloud Integration, etc. is plus', 'Ability to look for solution gaps and provide technical recommendationsProven track record in designing, developing and implementing Big Data or Data Lake solutions including Data warehouseKnowledge of Data Integration technologies like: Alteryx, SAP Data Services, SDI, and HANA Cloud Integration, etc. is plusExperience programming/scripting to support Big Data engagements is a plus.Predictive modeling with R/Python or using Data science tools like Data bricks is plusGood to have hands on Hadoop technologies including Hive, Sqoop, YARN, HBase, HDFS, SPARK, and/or Map ReduceHaving background on SAP is plusCertification on MS-Azure architecture will be a plus', 'Job Summary', 'Compensation Range 95,400 - 112,200', 'Participate in the design, implementation and support of Big Data, Analytics and Cloud solutions', 'Experience working on governing methodology for multi cloud environments.', 'Education/Experience Requirements', 'Translate business goals and requirements into Big data solutions through participation in blueprint workshops, facilitating decisions on best practice solutions, guiding configuration and providing Go-Live support', 'Zimmer Biomet is looking for diverse and talented individuals to join our Information Technology Team. The IT Lead Analyst, Data Lake Engineer – Architect is an experienced individual who can effectively Architect and implement solutions to collect, store, analyze and visualize data (volume, velocity, variety) for Big Data/Data Lake/Data Science projects. The role requires understanding of detail business requirements and IT strategy. The developer leverages in-depth knowledge of Big Data, Hadoop, Cloud, and Analytics solutions, capabilities and expertise, modern approach to provide business users with the best solutions using Cloud and Big Data technology.', ""Bachelor's degree in Information Technology or related field required.5+ years’ relevant experience; OR Bachelor’s with Master’s degree in business, IT or related field with 3+ years’ relevant experienceExperience leading requirements analyses to understand BI, self-service data preparations/discovery, and data requirements in detail.Experience working effectively in a matrix organization and ambiguous work environmentExperience and understanding of Medical Device industry preferredExperience working with data from various ERPs, 3rd-parties, public, unstructured and streaming data is mustExperience in driving Big Data use cases across Business and IT groupsExperience on Microsoft Azure or similar big tier cloud environments is plus. Need to have ability on Big Data and Cloud models with capability to design private and public workspaces.Experience working with cross functional teams within IT on security, Infrastructure on leading the GDPR, HIPAA compliance frameworks.Experience to acquire, ingest, Automate and model structure and unstructured data for data exploration/mining.Experience working on governing methodology for multi cloud environments.Experience and core deep foundational and functional knowledge on big data Framework/Integration components and best practices"", 'Up to 10%', 'Experience on Microsoft Azure or similar big tier cloud environments is plus. Need to have ability on Big Data and Cloud models with capability to design private and public workspaces.', 'Experience working with data from various ERPs, 3rd-parties, public, unstructured and streaming data is must', 'Principal Duties and Responsibilities', 'Participate in team knowledge sharing, design reviews, Analytics CoE operations, etc.', 'Experience to acquire, ingest, Automate and model structure and unstructured data for data exploration/mining.', 'Zimmer Biomet is a world leader in musculoskeletal health solutions. Our team members are part of a company with a heritage of leadership, a focus on shaping the future, and a mission dedicated to alleviating pain and improving the quality of life for people around the world.', 'Experience in driving Big Data use cases across Business and IT groups', 'Experience leading requirements analyses to understand BI, self-service data preparations/discovery, and data requirements in detail.', 'Translate business goals and requirements into Big data solutions through participation in blueprint workshops, facilitating decisions on best practice solutions, guiding configuration and providing Go-Live supportParticipate in the design, implementation and support of Big Data, Analytics and Cloud solutionsConduct and lead Big Data and Analytics-related discussions with customersParticipate in team knowledge sharing, design reviews, Analytics CoE operations, etc.Recognize problems and opportunities and provide value-added business solutionsInteract with both business and technical stakeholders of clients to provide a sound technical solution', 'Conduct and lead Big Data and Analytics-related discussions with customers', 'Predictive modeling with R/Python or using Data science tools like Data bricks is plus', 'At Zimmer Biomet, we believe in The Power of Us, which means that we are stronger together. We are committed to creating an environment where every team member feels included, respected, empowered, and celebrated.', 'Proven track record in designing, developing and implementing Big Data or Data Lake solutions including Data warehouse', ""Bachelor's degree in Information Technology or related field required."", 'Ability to look for solution gaps and provide technical recommendations', 'Interact with both business and technical stakeholders of clients to provide a sound technical solution']",Mid-Senior level,Full-time,Information Technology,Medical Devices,2021-03-24 13:05:10
Data Engineer,eTeam,"Hartford, CT",2 days ago,30 applicants,"['', 'They will perform hands-on development to create, enhance and maintain data solutions enabling seamless integration and flow of data across our data ecosystem.', 'Experience building and optimizing big data data pipelines, architectures and data sets.', 'Experience with data modeling, data architecture design and leveraging large-scale data ingest from complex data sources', 'The Senior Data Engineer will support and provide expertise in data ingestion, wrangling, cleansing, technologies. In this role they will work with relational and unstructured data formats to create analytics-ready datasets for analytic solutions.', 'Strong knowledge of data pipelining software e.g., Talend, Informatica', 'Responsibilities:', 'Develop real-time and batch data ingestion and stream-analytic solutions leveraging technologies such as Kafka, Apache Spark, Java, NoSQL DBs, AWS EMR.', 'The senior data engineer will partner with the Data Analytics team to understand their data needs and build data pipelines using cutting edge technologies.', 'Qualifications:', 'Strong knowledge of analysis tools such as Python, R, Spark or SAS, Shell scripting, R/Spark on Hadoop or Cassandra preferred.', 'Strong experience in data ingestion, gathering, wrangling and cleansing tools such as Apache NiFI, Kylo, Scripting, Power BI, Tableau and/or Qlik', 'Develop data driven solutions utilizing current and next generation technologies to meet evolving business needs.', 'These projects will include designing and developing data ingestion and processing/transformation frameworks leveraging open source tools such as Python, Spark, pySpark, etc.', 'Strong experience in data ingestion, gathering, wrangling and cleansing tools such as Apache NiFI, Kylo, Scripting, Power BI, Tableau and/or QlikExperience with data modeling, data architecture design and leveraging large-scale data ingest from complex data sourcesExperience building and optimizing big data data pipelines, architectures and data sets.Advanced SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.Strong knowledge of analysis tools such as Python, R, Spark or SAS, Shell scripting, R/Spark on Hadoop or Cassandra preferred.Strong knowledge of data pipelining software e.g., Talend, Informatica', 'Develop custom cloud-based data pipeline.', 'Translating data and technology requirements into our ETL / ELT architecture.Develop real-time and batch data ingestion and stream-analytic solutions leveraging technologies such as Kafka, Apache Spark, Java, NoSQL DBs, AWS EMR.Develop data driven solutions utilizing current and next generation technologies to meet evolving business needs.Develop custom cloud-based data pipeline.Provide support for deployed data applications and analytical models by identifying data problems and guiding issue resolution with partner data engineers and source data providers.Provide subject matter expertise in the analysis, preparation of specifications and plans for the development of data processes.', 'Provide support for deployed data applications and analytical models by identifying data problems and guiding issue resolution with partner data engineers and source data providers.', 'Provide subject matter expertise in the analysis, preparation of specifications and plans for the development of data processes.', 'The Senior Data Engineer will support and provide expertise in data ingestion, wrangling, cleansing, technologies. In this role they will work with relational and unstructured data formats to create analytics-ready datasets for analytic solutions.The senior data engineer will partner with the Data Analytics team to understand their data needs and build data pipelines using cutting edge technologies.They will perform hands-on development to create, enhance and maintain data solutions enabling seamless integration and flow of data across our data ecosystem.These projects will include designing and developing data ingestion and processing/transformation frameworks leveraging open source tools such as Python, Spark, pySpark, etc.', 'Advanced SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.', 'Translating data and technology requirements into our ETL / ELT architecture.']",Entry level,Contract,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Alight Solutions,"America, IL",3 weeks ago,Be among the first 25 applicants,"['', 'Knowledge Of Following', 'Scala', 'Data oriented foundational technologies', 'Background Check Required', 'Analytical skills when working with unstructured data sets', 'Knowledge on manipulating, processing, and extracting value from large and disconnected data sets', 'PythonJavaC++Scala', 'Dev Testing', 'Building analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency', 'Python', 'Knowledge of query authoring, relational databases, and a familiarity with a variety of databases', 'Diversity Statement ', 'Building required infrastructure for optimal extraction, transformation and loading of data from various data sources using Cloud and Big data technologies', 'Java', 'Assembling large, complex sets of data that meet non-functional and functional business requirementsActivities include creating and maintaining AI/ML enabled data pipeline for analytics and insightsIdentifying, designing and implementing process improvements for greater scalability, optimizing data delivery, and automating manual processesBuilding required infrastructure for optimal extraction, transformation and loading of data from various data sources using Cloud and Big data technologiesBuilding analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiencyWorking with stakeholders including data, design, product and executive teams and assisting them with data-related technical issues', 'Working with stakeholders including data, design, product and executive teams and assisting them with data-related technical issues', 'Disclaimer', 'Activities include creating and maintaining AI/ML enabled data pipeline for analytics and insights', 'Knowledge of Hadoop, Kafka, and Spark', 'Equal Employment Opportunity', 'C++', 'Advanced working knowledge of SQL (writing and debugging)', 'Identifying, designing and implementing process improvements for greater scalability, optimizing data delivery, and automating manual processes', 'Authorization to Work in the United States', 'Data oriented foundational technologiesAdvanced working knowledge of SQL (writing and debugging)Knowledge of query authoring, relational databases, and a familiarity with a variety of databasesKnowledge of Hadoop, Kafka, and SparkAnalytical skills when working with unstructured data setsKnowledge of cloud-based data solutions (e.g., AWS, EC2, EMR, RDS, and Redshift)Knowledge on manipulating, processing, and extracting value from large and disconnected data setsDev Testing', 'Reasonable Accommodations', 'Assembling large, complex sets of data that meet non-functional and functional business requirements', 'Knowledge of cloud-based data solutions (e.g., AWS, EC2, EMR, RDS, and Redshift)']",Not Applicable,Other,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer II,RetailMeNot,"Austin, TX",1 week ago,Be among the first 25 applicants,"['', 'You recognize that your success depends upon enabling your fellow team members to succeed; taking time to help others energizes you', 'Develop ETL processes that ensure data is accurate and available within SLAs', 'Implement data system for both real-time and warehouse applicationsDevelop ETL processes that ensure data is accurate and available within SLAsEnhance data models by developing integrations with business partnersSeek opportunities for performance improvement and implement optimizationsCreate dashboards that provide insight into the health of data integrations, ETL processes and data sets', 'You have 3+ years work experienceYou are skilled using Python, Linux, Docker, Git, and Amazon Web Services (or have translatable experience with similar toolsets)', 'Create dashboards that provide insight into the health of data integrations, ETL processes and data sets', ' Cell phone & gym membership reimbursements', ' Performance based rewards & recognition for your hard work and service', 'Who We Are', 'You are able to accurately estimate tasks, identify dependencies and dedicatedly solve problems to ensure commitments are met', 'You have extensive SQL experience on a variety of RDBMS, and enjoy optimizing queries as well as designing efficient data models', 'You enjoy gathering requirements from non-technical coworkers and delivering solutions that meet their needs and exceed their expectations', 'Include, But Are Not Limited To The Following', 'We have an open environment where engineers are given a lot of responsibility and the freedom to make a huge impact.', ' Open & flexible PTO', ""What You'll Do"", ' Long Term Incentive Plan', 'You have developed scalable solutions using both SQL and NoSQL (Hadoop) databases. Working with data sets comprised of millions or billions of records is comfortableYou strive to identify simple solutions to complex problems, can identify a minimal viable product and enjoy iterative development', ""You have a Bachelor's degree in computer science or equivalent STEM field, or equivalent work experience"", 'Implement data system for both real-time and warehouse applications', ' Very competitive benefits packages, including best-in-class parental leave', 'You have a work ethic that inspires your fellow team members to give their best', 'About Us', 'U.S. Equal Employment Opportunity/Affirmative Action Information', 'We believe in giving prizes, bonuses, and recognition for doing what you enjoy.', 'Some rewards do not apply to contract workers or interns.About UsRetailMeNot, Inc. is a leading savings destination bringing people and the things they lovetogether through savings with retailers, brands, restaurants and pharmacies. RetailMeNotmakes everyday life more affordable through online and in-store coupon codes, cash backoffers, and the RetailMeNot Deal Finder™ browser extension. Savings are also provided inconsumers’ mailboxes through the RetailMeNot Everyday™ direct mail package.To learn more, visit http://www.retailmenot.com/corp or follow @RetailMeNot on social media.U.S. Equal Employment Opportunity/Affirmative Action InformationAt RetailMeNot we celebrate difference. We are committed to ensuring an environment of mutual respect for every employee and proud to be an an equal employment opportunity employer who does not discriminate against any person because of race, color, creed, religion, gender, gender identity, gender expression, national origin, citizenship, age, sex, sexual orientation, pregnancy, marital status, ancestry, physical or mental disability, military or veteran status, or any other characteristic protected by law. We believe a diverse and inclusive workplace is central to our success and actively seek to recruit, develop and retain the most talented people from a diverse pool of candidates. You are being given the opportunity to provide the following information in order to help us align with federal and state Equal Employment Opportunity/Affirmative Action record keeping, reporting, and other legal requirements.', 'We have an open environment where engineers are given a lot of responsibility and the freedom to make a huge impact.We have lots of smart people to work with and learn from.We work on large scale challenges with a variety of technologies and believe in an ever-growing diversity of technology platforms.We believe in giving prizes, bonuses, and recognition for doing what you enjoy.', 'Enhance data models by developing integrations with business partners', 'Rewards*', 'We work on large scale challenges with a variety of technologies and believe in an ever-growing diversity of technology platforms.', ""You have a Bachelor's degree in computer science or equivalent STEM field, or equivalent work experienceYou have 3+ years work experienceYou are skilled using Python, Linux, Docker, Git, and Amazon Web Services (or have translatable experience with similar toolsets)You have extensive SQL experience on a variety of RDBMS, and enjoy optimizing queries as well as designing efficient data modelsYou have developed scalable solutions using both SQL and NoSQL (Hadoop) databases. Working with data sets comprised of millions or billions of records is comfortableYou strive to identify simple solutions to complex problems, can identify a minimal viable product and enjoy iterative developmentYou are able to accurately estimate tasks, identify dependencies and dedicatedly solve problems to ensure commitments are metYou recognize that your success depends upon enabling your fellow team members to succeed; taking time to help others energizes youYou enjoy gathering requirements from non-technical coworkers and delivering solutions that meet their needs and exceed their expectationsYou derive satisfaction from enabling the business to succeed and delighting coworkers, not building technology for its own sakeYou have a work ethic that inspires your fellow team members to give their best"", ' Competitive base & bonus packages; salary negotiable', 'We have lots of smart people to work with and learn from.', 'Who You Are', 'You derive satisfaction from enabling the business to succeed and delighting coworkers, not building technology for its own sake', 'Seek opportunities for performance improvement and implement optimizations']",Not Applicable,Full-time,Information Technology,Internet,2021-03-24 13:05:10
Intern Data Engineer,Arm,"Austin, TX",6 days ago,73 applicants,"['', 'Exploiting Virtualization and cloud technologies such as AWS, Docker, OpenStack and Kubernetes.', 'Compiled programming language such as Java, C, C++ or Go', 'Have an active interest in Open Source Software', 'Desired Skills And Experience', 'Essential skills and experience', 'Interpreted programming language such as Python, Ruby or Perl', 'Experience in developing and deploying services under Virtualization and cloud technologies', 'Software build and test solutions such as Jenkins, Bamboo and Artifactory', 'Self-motivated', 'A focus on personal achievement and responsibility ', ' What can you expect to work on?', 'Web application development.', 'Data engineering (ETL) development.', 'Compiled programming language such as Java, C, C++ or GoInterpreted programming language such as Python, Ruby or Perl', 'Developing workflow tooling under industry large scale clustered compute and cloud-based environments.Software build and test solutions such as Jenkins, Bamboo and ArtifactoryExploiting Virtualization and cloud technologies such as AWS, Docker, OpenStack and Kubernetes.Web application development.Data engineering (ETL) development.', 'Job Requirements', 'Familiarity with Linux and source control systems', 'Experience in developing and deploying services under Virtualization and cloud technologiesSelf-motivatedPassionate about software developmentHave an active interest in Open Source SoftwareA focus on personal achievement and responsibility ', 'Developing workflow tooling under industry large scale clustered compute and cloud-based environments.', 'Job Description', 'A demonstrable level of experience in at least one ', 'Passionate about software development']",Internship,Full-time,Information Technology,Semiconductors,2021-03-24 13:05:10
Data Engineer,Agile Software,"United, LA",3 weeks ago,Be among the first 25 applicants,"['', 'Preferred Skills (not Required)']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
"Sr Data Engineer, Data Science",Twitter,"Seattle, WA",4 weeks ago,72 applicants,"['', 'Support your colleagues by reviewing code and designs.', 'Design, develop, and launch extremely efficient and reliable data pipelines to move data and to provide intuitive analytics to our partner teams.', 'Collaborate with other engineers and Data Scientists to discover the best solutions.', 'As such, you will ', 'Diagnose and solve issues in our existing data pipelines and envision and build their successors.', 'Company Description', 'Design, develop, and launch extremely efficient and reliable data pipelines to move data and to provide intuitive analytics to our partner teams.Make Twitter-scale data more discoverable and easy to use for Data Scientists and Analysts across the company.Collaborate with other engineers and Data Scientists to discover the best solutions.Support your colleagues by reviewing code and designs.Diagnose and solve issues in our existing data pipelines and envision and build their successors.', 'Job Description', 'Make Twitter-scale data more discoverable and easy to use for Data Scientists and Analysts across the company.']",Not Applicable,Full-time,Information Technology,Internet,2021-03-24 13:05:10
Lead Data Engineer,"Informed Systems, Inc.","San Francisco, CA",1 day ago,Be among the first 25 applicants,"['', 'Ability to write thorough, scalable and clear documentation.', '5+ years of industry experience as a product manager, data scientist, or similar role in Series B or later start-ups.', 'Develop and automate reports, iteratively build and prototype dashboards to provide insights at scale, solving for business and customer priorities.', 'Develop SQL queries and tables. Define the data warehousing strategy.', 'Maintain and improve data security practices to protect and destroy NPPI/PII.', '5+ years experience working with data warehouses, developing customized ETLs, schema design, and writing SQL queries.', 'About You', 'The ability to communicate cross-functionally, derive requirements and architect solutions.', 'Two weeks of paid time-off in addition to seven paid company holidays', 'Work with Engineering leadership to make decisions on building vs buying components of the data warehousing and analytics solution.', 'Advanced knowledge of a scientific computing language (such as R or Python) and SQL, and familiarity with at least one object oriented language.', ' Dig into documents and data belonging to millions of consumer loan applicants. Capture data and structure it in schemas that are easily query-able so that banks have a better understanding of their borrowers. Build ETL pipelines that enable for regular reporting on SLAs as well as deep analytical research into the contents of loan jackets. Develop SQL queries and tables. Define the data warehousing strategy. Develop and automate reports, iteratively build and prototype dashboards to provide insights at scale, solving for business and customer priorities. Triage data pipeline issues and drive their resolution. Implement and operate our cloud-based data infrastructure. Work with Engineering leadership to make decisions on building vs buying components of the data warehousing and analytics solution. Maintain and improve data security practices to protect and destroy NPPI/PII. ', ' 5+ years of industry experience as a product manager, data scientist, or similar role in Series B or later start-ups. 5+ years experience working with data warehouses, developing customized ETLs, schema design, and writing SQL queries. Fluency in data analysis and communication, including defining a metrics strategy, conducting exploratory data analysis, and crafting data-driven reports and visualizations. BS/MS/PhD in Computer Science, Engineering, or equivalent technical or analytical field. Advanced knowledge of a scientific computing language (such as R or Python) and SQL, and familiarity with at least one object oriented language. Ability to thrive in a start-up, self-motivate, and own projects end-to-end. Ability to write thorough, scalable and clear documentation. An inquisitive nature in diving into data inconsistencies to pinpoint issues. The ability to communicate cross-functionally, derive requirements and architect solutions. ', 'Competitive salary and stock options', 'Ruby on Rails', 'JavaScript/React/Redux', 'Daily lunch catered to our sunny office in SOMA (across from Caltrain!)', 'Responsibilities', 'Fluency in data analysis and communication, including defining a metrics strategy, conducting exploratory data analysis, and crafting data-driven reports and visualizations.', 'BS/MS/PhD in Computer Science, Engineering, or equivalent technical or analytical field.', 'Dig into documents and data belonging to millions of consumer loan applicants. Capture data and structure it in schemas that are easily query-able so that banks have a better understanding of their borrowers.', 'Healthcare, dental, and vision partially paid by company', 'Build ETL pipelines that enable for regular reporting on SLAs as well as deep analytical research into the contents of loan jackets.', ' JavaScript/React/Redux Ruby on Rails Postgres ', ' Competitive salary and stock options Daily lunch catered to our sunny office in SOMA (across from Caltrain!) Healthcare, dental, and vision partially paid by company Two weeks of paid time-off in addition to seven paid company holidays ', 'Ability to thrive in a start-up, self-motivate, and own projects end-to-end.', 'Postgres', 'Triage data pipeline issues and drive their resolution. Implement and operate our cloud-based data infrastructure.', 'An inquisitive nature in diving into data inconsistencies to pinpoint issues.']",Associate,Full-time,Engineering,Computer Software,2021-03-24 13:05:10
Data Engineer,May Mobility,"Ann Arbor, MI",6 days ago,Be among the first 25 applicants,"['', ' Experience in an object oriented programming language, such as C++, Python, or Java', ' B.S. Degree in Computer Science, Computer Engineering, or an equivalent degree and 2+ years of industry experience Hands-on experience with distributed technology such as Kafka, Spark, Spark Streaming, Storm, Flink, Cassandra Strong working knowledge of data structures and algorithms Experience in an object oriented programming language, such as C++, Python, or Java Attention to detail and rigorous testing methodology Written and verbal communication skills Experience with robotics, automotive engineering, or start-ups is not required Ability to undergo a driving record check', ' Ability to undergo a driving record check', ' Contribute to designing and implement data models for optimal storage and retrieval meeting requirements of stakeholders with different needs', ' Experience building and managing large-scale data-processing pipelines in a cloud environment', ' Daily catered lunches and snacks', 'Required Qualifications', 'Desirable Qualifications', ' Build state-of-art data distribution, storage and analysis platforms powering experiences for internal and external customers', ' Meaningful stock incentives and equity refresh program', ' Expertise in Python, C/C++ or Java', ' Participate in new technology introduction initiatives for modern data tools and industry best practices', ' Strong working knowledge of data structures and algorithms', ' Attention to detail and rigorous testing methodology', ' Written and verbal communication skills', ' B.S. Degree in Computer Science, Computer Engineering, or an equivalent degree and 2+ years of industry experience', ' Build state-of-art data distribution, storage and analysis platforms powering experiences for internal and external customers Manage and scale our real-time and historical data pipelines to enable our fleet to operate and facilitate continuous development of our system Contribute to designing and implement data models for optimal storage and retrieval meeting requirements of stakeholders with different needs Define, build, and expand libraries and APIs for managing, searching, and analyzing vehicle datasets with internal and external partners Participate in new technology introduction initiatives for modern data tools and industry best practices', ' Unlimited vacation / company paid holidays', ' Competitive salary and benefits (medical / dental / vision / 401k)', ' Competitive salary and benefits (medical / dental / vision / 401k) Meaningful stock incentives and equity refresh program Unlimited vacation / company paid holidays Daily catered lunches and snacks Paid parental leave', ' M.S. Degree in Computer Science, Computer Engineering and 2+ years of industry experience', ' Working knowledge of telemetry systems and real-time data processing', ' Define, build, and expand libraries and APIs for managing, searching, and analyzing vehicle datasets with internal and external partners', ' Hands-on experience with distributed technology such as Kafka, Spark, Spark Streaming, Storm, Flink, Cassandra', ' Experience with robotics, automotive engineering, or start-ups is not required', ' M.S. Degree in Computer Science, Computer Engineering and 2+ years of industry experience Expertise in Python, C/C++ or Java Experience building and managing large-scale data-processing pipelines in a cloud environment Working knowledge of telemetry systems and real-time data processing', ' Paid parental leave', 'About May Mobility', ' Manage and scale our real-time and historical data pipelines to enable our fleet to operate and facilitate continuous development of our system']",Entry level,Full-time,Information Technology,Transportation/Trucking/Railroad,2021-03-24 13:05:10
Data Engineer,"Spneedel Technologies, Inc.","McLean, VA",1 week ago,Be among the first 25 applicants,"['Job Description', '']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
ETL Data Engineer (W2),Sharp Decisions,"Costa Mesa, CA",,N/A,"['', 'remote (PST hours)', '3 years of experience with data warehouse concepts and methodologies', 'Create and maintain ETL specifications and process documentations to produce the required data deliverables', 'Experience with business intelligence or reporting tools (preferably Tableau and Alteryx)', 'Expertise in SQL and Python scripting', 'Acquire understanding on existing data, perform analysis and provide ETL solutions', 'Participate in requirement gathering, solution design and implementation of data warehouse and reporting projects.', '*W2 Only.\xa0', 'Highly motivated self-starter, detail and quality oriented and able to work independently', 'Work with Engineering teams to explore and understand new data being introduced to front end application', 'A client of Sharp Decisions Inc. is actively looking to bring on an ETL Data Engineer to be based in Costa Mesa, CA.\xa0This position\xa0is on a 5 month\xa0contract basis with possible extension. Client is open to remote (PST hours). PySpark and SQL are required.\xa0', 'Minimum 2+\xa0year experience in BI development and support', 'JOB DETAILS:', 'ETL Data Engineer is responsible for design, develop, deliver and support ETL solutions using PySpark and SQL.', 'ETL Data Engineer ', 'Must have PySpark and SQL', 'Must have PySpark and SQLMinimum 2+\xa0year experience in BI development and support3 years of experience with data warehouse concepts and methodologiesExpertise in SQL and Python scriptingExperience with business intelligence or reporting tools (preferably Tableau and Alteryx)Experience with AWS environment will be a plusExcellent written and oral communication skillsHighly motivated self-starter, detail and quality oriented and able to work independentlyEducation: BS degree or higher in MIS or engineering fields', 'Support and enhance existing ETL jobs in SQL', 'contract', 'Education: BS degree or higher in MIS or engineering fields', 'Troubleshoot and resolve data, system, and performance issues.', 'Experience with AWS environment will be a plus', 'PySpark and SQL are required', 'Excellent written and oral communication skills', 'QUALIFICATIONS:', 'Work as part of a team to support business analytics for growing online consumer subscription service.', 'Work as part of a team to support business analytics for growing online consumer subscription service.Acquire understanding on existing data, perform analysis and provide ETL solutionsDevelop and troubleshoot PySpark jobs in AWS environmentSupport and enhance existing ETL jobs in SQLParticipate in requirement gathering, solution design and implementation of data warehouse and reporting projects.Work with Engineering teams to explore and understand new data being introduced to front end applicationCreate and maintain ETL specifications and process documentations to produce the required data deliverablesTroubleshoot and resolve data, system, and performance issues.', 'Develop and troubleshoot PySpark jobs in AWS environment']",Mid-Senior level,Contract,Engineering,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Equity Residential,"Chicago, IL",6 days ago,Be among the first 25 applicants,"['', ' Organized and Confident. You are flexible, composed, and able to prioritize multiple tasks and deadlines simultaneously, while confidently interacting with a variety of individuals, across all levels of the organization. You handle pressure well and do so with confidence.', ' Experience with Azure or other cloud providers (AWS, GCP) is strongly preferred', "" Bachelor's Degree required, preferably in Computer Science, Mathematics, Statistics, Finance or related technical field."", ' Interact with stakeholders and functional subject matter experts to understand and gather requirements to develop effective data models that can be translated into business insights reports/analyses', 'Who You Are', ' Partner with IT and Functional Teams on internal/external data integration and acquisition', ' Assist in standardizing our data management best practices, including codebase management, work and issue tracking, testing and quality control/assurance measures, data dictionaries, and a documentation hub for both production level code and ad hoc analyses.', ' Research modern technologies and frameworks to discover uses for new and existing data.', ' Lead the effort to democratize data at EQR, providing the right endpoints for the right user (e.g. report, API, table, etc.)', ' Build and deploy pipelines to ingest and transform in our rapidly growing data platform', ""What You'll Do"", ' Demonstrated experience using software engineering best practices like Continuous Integration/ Deployment to deliver complex software projects', ' Knowledgeable, Analytical, and Solution-Oriented. Without a doubt, you\'ve got strong quantitative skills and are comfortable analyzing large data sets, spotting trends and patterns, and synthesizing relevant observations. You use a hypothesis-driven approach to engage in analysis that will deliver on your client questions. You like thinking outside the box to come up with innovative points of view on new challenges, relying on your previous analytic work and experience to help guide you along the way. Results-Oriented. You demonstrate an inherent sense of urgency to drive great results, while being precise in executing your work. You are facile with creating and communicating a clear project plan, tracking progress, and keeping your business partners in the loop along the way. Intellectually Curious. You\'re inherently interested in the ""why"" so that you can identify opportunities that represent unconventional solutions to the problems you are trying to solve. Strong Communicator. Your writing and speaking skills are concise, articulate, and effective, providing an ability to interact with all levels/various teams across the organization, be understood, and develop trust and rapport within the organization. Technologically Savvy. You exhibit mastery of SQL along with a strong skill set in one or more general programming languages (Python, Java, Scala, C++) A Trusted Team Player. You enjoy partnering with others and build constructive working relationships that foster the collaboration necessary to deliver great results. You are accountable to your teammates and follow through on commitments. Organized and Confident. You are flexible, composed, and able to prioritize multiple tasks and deadlines simultaneously, while confidently interacting with a variety of individuals, across all levels of the organization. You handle pressure well and do so with confidence.', ' Create data models and data processes, providing the right format and structure for use case solutions.', ' Results-Oriented. You demonstrate an inherent sense of urgency to drive great results, while being precise in executing your work. You are facile with creating and communicating a clear project plan, tracking progress, and keeping your business partners in the loop along the way.', "" Knowledgeable, Analytical, and Solution-Oriented. Without a doubt, you've got strong quantitative skills and are comfortable analyzing large data sets, spotting trends and patterns, and synthesizing relevant observations. You use a hypothesis-driven approach to engage in analysis that will deliver on your client questions. You like thinking outside the box to come up with innovative points of view on new challenges, relying on your previous analytic work and experience to help guide you along the way."", "" Must be available for overnight travel (approximately 10%) to any of Equity's major core markets, as required."", ' Produce clean, well-tested, and documented code', ' A Trusted Team Player. You enjoy partnering with others and build constructive working relationships that foster the collaboration necessary to deliver great results. You are accountable to your teammates and follow through on commitments.', ' Experience writing software in one or more languages: Python, Java, C++', ' Team oriented and flexible with a proven track record in collaborating with multiple stakeholders.', ' Lead the effort to democratize data at EQR, providing the right endpoints for the right user (e.g. report, API, table, etc.) Assist in standardizing our data management best practices, including codebase management, work and issue tracking, testing and quality control/assurance measures, data dictionaries, and a documentation hub for both production level code and ad hoc analyses. Interact with stakeholders and functional subject matter experts to understand and gather requirements to develop effective data models that can be translated into business insights reports/analyses Create data models and data processes, providing the right format and structure for use case solutions. Build and deploy pipelines to ingest and transform in our rapidly growing data platform Partner with IT and Functional Teams on internal/external data integration and acquisition Research modern technologies and frameworks to discover uses for new and existing data. Propose and effect changes to our data generation processes Produce clean, well-tested, and documented code', ' 3+ years developing end-to-end Business Intelligence or Analytic solutions (i.e. data modeling, ELT/ELT, reporting/analysis)', ' 3+ years utilizing SQL based data management systems (preferred NoSQL experience as well)', ' Propose and effect changes to our data generation processes', ' Authorization to work in the US (without need for Visa sponsorship from employer) is required.', ' Strong Communicator. Your writing and speaking skills are concise, articulate, and effective, providing an ability to interact with all levels/various teams across the organization, be understood, and develop trust and rapport within the organization.', ' Technologically Savvy. You exhibit mastery of SQL along with a strong skill set in one or more general programming languages (Python, Java, Scala, C++)', ' Intellectually Curious. You\'re inherently interested in the ""why"" so that you can identify opportunities that represent unconventional solutions to the problems you are trying to solve.', ' Experience working with data warehouses, including technical architectures, infrastructure components, large-scale ETL/ELT pipelines, automation, database optimization, and reporting/analytic tools and environments.', ' Preferred experience with data platforms and technologies such as Kafka, Presto, Delta Lake, Spark, etc.', ' Demonstrated experience working with diverse data sets and frameworks across multiple domains', 'Previous Experience & Requirements', "" Bachelor's Degree required, preferably in Computer Science, Mathematics, Statistics, Finance or related technical field. 3+ years developing end-to-end Business Intelligence or Analytic solutions (i.e. data modeling, ELT/ELT, reporting/analysis) Experience working with data warehouses, including technical architectures, infrastructure components, large-scale ETL/ELT pipelines, automation, database optimization, and reporting/analytic tools and environments. Experience writing software in one or more languages: Python, Java, C++ 3+ years utilizing SQL based data management systems (preferred NoSQL experience as well) Experience with Azure or other cloud providers (AWS, GCP) is strongly preferred Preferred experience with data platforms and technologies such as Kafka, Presto, Delta Lake, Spark, etc. Demonstrated experience working with diverse data sets and frameworks across multiple domains Demonstrated experience using software engineering best practices like Continuous Integration/ Deployment to deliver complex software projects Team oriented and flexible with a proven track record in collaborating with multiple stakeholders. Must be available for overnight travel (approximately 10%) to any of Equity's major core markets, as required. Authorization to work in the US (without need for Visa sponsorship from employer) is required.""]",Entry level,Full-time,Information Technology,Real Estate,2021-03-24 13:05:10
Data Engineer,SpartanNash,"Byron Center, MI",3 weeks ago,Be among the first 25 applicants,"['', 'Provide technical guidance to the applications development team regarding database design to reduce risk of design issues. Interface with appropriate IT sub-departments and/or cross functional business areas to address concerns and resolve issues in a timely manner.', 'Programming PL/SQL and ETL development experience is necessary, Microsoft Azure (preferred)', 'Participate in the development of project scope and milestones to meet assigned project requirements. Partner cross-functionally to meet deadlines and audit databases/reports, and work with management on critical issues.', ""Bachelor's Degree (Required) Computer Science Information Technology, Business Administration or related field or equivalent combination of education and/or experience"", ""Bachelor's Degree (Required) Computer Science Information Technology, Business Administration or related field or equivalent combination of education and/or experienceFive (5) years of Information Technology experience in database modeling/relational database administration. Experience with and knowledge of creating Data Warehouse dimensional models, data modeling tools, principles and practices and related configuration management concepts.Programming PL/SQL and ETL development experience is necessary, Microsoft Azure (preferred)Experience creating logical and physical data models to optimize database design – experience in Erwin a plus.Experience profiling and retrieving data from business applications using a wide variety of database platforms.Experience with modern ETL/ELT tools (Talend/Matillion/Datastage) preferably in a cloud environment.Experience with version control systems (Git/SCCS)Ability to identify, troubleshoot, and resolve data integrity and performance issuesBackground supporting retail or wholesale distribution would be advantageous"", 'Maintain data reliability and integrity to eliminate data redundancy and to streamline the reporting platform. Maintain current knowledge of organization data uses to recommend improvements and maintain data integrity.', 'Five (5) years of Information Technology experience in database modeling/relational database administration. Experience with and knowledge of creating Data Warehouse dimensional models, data modeling tools, principles and practices and related configuration management concepts.', 'Utilize modeling tools to build and analyze databases and to develop logical and physical models to include relationships, constraints, attributes, and other modeling information to understand impact for proposed systems, and setup approved systems. Identify, troubleshoot and resolve data integrity and performance issues.', 'Experience creating logical and physical data models to optimize database design – experience in Erwin a plus.', 'Experience with modern ETL/ELT tools (Talend/Matillion/Datastage) preferably in a cloud environment.', 'Gather and document business requirements and convert requirements into testable designs; and work with the business and application teams to refine design requirements on assigned projects.', 'Create and maintain various database versions to model data to optimize database design and share relational data across various applications. Document data model standards and database diagrams. Work with cross-functional departments to coordinate the database changes in a timely and accurate manner.', 'Location:', 'Create and maintain various database versions to model data to optimize database design and share relational data across various applications. Document data model standards and database diagrams. Work with cross-functional departments to coordinate the database changes in a timely and accurate manner.Utilize modeling tools to build and analyze databases and to develop logical and physical models to include relationships, constraints, attributes, and other modeling information to understand impact for proposed systems, and setup approved systems. Identify, troubleshoot and resolve data integrity and performance issues.Design data model structures that enhance the application performance through the development of aggregation and summarization structures. Document model structures (i.e., keys, indexes, constraints) and partner with cross-functional IT (i.e., Data Warehouse/Database Administration) for the process of data normalization and physical data base design.Gather and document business requirements and convert requirements into testable designs; and work with the business and application teams to refine design requirements on assigned projects.Maintain data reliability and integrity to eliminate data redundancy and to streamline the reporting platform. Maintain current knowledge of organization data uses to recommend improvements and maintain data integrity.Establish and maintain common data definitions (i.e., naming guidelines, standard abbreviations, naming conventions) for database users to eliminate data redundancy and improve data integrity. Document data definitions and coordinate data models, dictionaries, and other database documentation across multiple applications.Provide technical guidance to the applications development team regarding database design to reduce risk of design issues. Interface with appropriate IT sub-departments and/or cross functional business areas to address concerns and resolve issues in a timely manner.Participate in the development of project scope and milestones to meet assigned project requirements. Partner cross-functionally to meet deadlines and audit databases/reports, and work with management on critical issues.', 'Background supporting retail or wholesale distribution would be advantageous', 'Ability to identify, troubleshoot, and resolve data integrity and performance issues', 'Design data model structures that enhance the application performance through the development of aggregation and summarization structures. Document model structures (i.e., keys, indexes, constraints) and partner with cross-functional IT (i.e., Data Warehouse/Database Administration) for the process of data normalization and physical data base design.', 'Experience with version control systems (Git/SCCS)', 'Here’s What You’ll Need', 'Experience profiling and retrieving data from business applications using a wide variety of database platforms.', 'Establish and maintain common data definitions (i.e., naming guidelines, standard abbreviations, naming conventions) for database users to eliminate data redundancy and improve data integrity. Document data definitions and coordinate data models, dictionaries, and other database documentation across multiple applications.', 'Job Description']",Entry level,Full-time,Information Technology,Food & Beverages,2021-03-24 13:05:10
Data Engineer,"SoftPros, Inc.","Irving, TX",2 weeks ago,Be among the first 25 applicants,[],Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
REMOTE - Data Engineer,ClearSense,US Virgin Islands,2 weeks ago,Be among the first 25 applicants,"['', 'Skills', 'Qualifications']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer / Data Integration,ArborMetrix,"Ann Arbor, MI",4 weeks ago,45 applicants,"['', 'Knowledge of web technologies such as RESTful APIs', 'Experience in using data integration / ETL tools to construct production-quality, real-time interfaces', 'Minimum Qualifications', 'Writing PL/SQL functions and packages to automate workflows in a Postgres environment linked with web technologies', 'Preferred Qualifications', 'Focus and initiative in learning and enhancing existing products and future enhancements', 'Experience with or knowledge of Amazon Web Services', 'Participating in the design, implementation, and validation of a robust, scalable, next-generation Data Integration EngineImplementing robust ETL solutions that integrate heterogeneous healthcare data feeds using tools such as Apache NiFi, Mirth (JavaScript), CloverDX, PostgreSQL, Python, and Linux shell scriptingBuilding direct interfaces to hospitals and EMR vendors utilizing industry standards such as HL7 2.x, C-CDA, and FHIR as well as implementing bespoke interfaces that consume formats such as XML, JSON, and delimited filesUtilizing APIs and web services to permit bi-directional flows of data to and from our partner organizationsMonitoring operational ETL processes, triaging and resolving issues as they arise to meet SLAsWriting PL/SQL functions and packages to automate workflows in a Postgres environment linked with web technologiesApplying industry best practices of software design, implementation, testing, and deployment, with a particular emphasis on quality assurance, fault tolerance, and component reuseMaintaining data standards (including adherence to HIPAA), documenting work, and championing product and process improvementsCollaborating with an innovative team in an Agile/Scrum environment', 'Ability to manage multiple deliverables with competing deadlines in a fast-paced environment', 'Experience in using data integration / ETL tools to construct production-quality, real-time interfacesStrong understanding of relational databases, data modeling / architecture, and SQL query design / optimizationFamiliarity with a variety of structured and unstructured file formatsKnowledge of web technologies such as RESTful APIsExperience using SQL to manage data as well as proficiency in at least one scripting language (e.g. JavaScript, Python, Linux shell script)Focus and initiative in learning and enhancing existing products and future enhancementsAbility to manage multiple deliverables with competing deadlines in a fast-paced environmentStrong communication and problem-solving skillsBachelor’s degree in a quantitative field such as Computer Science or Health Informatics', 'Master’s degree in a quantitative field such as Computer Science or Health Informatics', 'Familiarity with a variety of structured and unstructured file formats', 'Experience using SQL to manage data as well as proficiency in at least one scripting language (e.g. JavaScript, Python, Linux shell script)', 'Project experience involving direct integration with EMR systems', 'Participating in the design, implementation, and validation of a robust, scalable, next-generation Data Integration Engine', 'Monitoring operational ETL processes, triaging and resolving issues as they arise to meet SLAs', 'Familiarity with healthcare data integration technologies (e.g. HL7 2.x, C-CDA, and FHIR)', 'Collaborating with an innovative team in an Agile/Scrum environment', 'Master’s degree in a quantitative field such as Computer Science or Health InformaticsFamiliarity with healthcare data integration technologies (e.g. HL7 2.x, C-CDA, and FHIR)Project experience involving direct integration with EMR systemsExperience with or knowledge of Amazon Web Services', 'Utilizing APIs and web services to permit bi-directional flows of data to and from our partner organizations', 'Strong communication and problem-solving skills', 'Building direct interfaces to hospitals and EMR vendors utilizing industry standards such as HL7 2.x, C-CDA, and FHIR as well as implementing bespoke interfaces that consume formats such as XML, JSON, and delimited files', 'Strong understanding of relational databases, data modeling / architecture, and SQL query design / optimization', 'Bachelor’s degree in a quantitative field such as Computer Science or Health Informatics', 'Company Description', 'Implementing robust ETL solutions that integrate heterogeneous healthcare data feeds using tools such as Apache NiFi, Mirth (JavaScript), CloverDX, PostgreSQL, Python, and Linux shell scripting', 'Job Description', 'Maintaining data standards (including adherence to HIPAA), documenting work, and championing product and process improvements', 'Applying industry best practices of software design, implementation, testing, and deployment, with a particular emphasis on quality assurance, fault tolerance, and component reuse']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Platform Engineer,THE TIFIN GROUP,"Boulder, CO",27 minutes ago,43 applicants,"['', 'Expertise in data model design with sensitivity to usage patterns and goals – schema, scalability, immutability, idempotency, etc.', 'The TIFIN Group is proud to be an equal opportunity workplace and values the multitude of talents and perspectives that a diverse workforce brings. All qualified applicants will receive consideration for employment without regard to race, national origin, religion, age, color, sex, sexual orientation, gender identity, disability, or protected veteran status.', 'Our companies have experienced promising growth in 2020 and as we enter into the next stage of expansion, we are looking to leverage the combined data from all of our platforms in order to build a more complete picture of what investors want. We are looking for an exceptional and ambitious Data Platform Engineer to join this new world-class data team. The ideal candidate is excited to build and optimize a distributed data platform to be used for machine learning, and is an experienced data wrangler who enjoys optimizing data systems and building them from the ground up.\xa0', 'Make magic for our users. We center around the voice of the customer. With deep empathy for our clients, we create technology that transforms investor experiences.\xa0', 'Ability to thrive in a highly demanding, entrepreneurial, and fast-paced environment\xa0', 'Teamwork for Teamwin. We believe in win together, learn together. We fly in formation. We cover each other’s backs. We inspire each other with our energy and attitude.\xa0', 'Competitive base salary, health benefits, unlimited PTO and performance-linked variable compensation.', 'This is a greenfield project so you must be a motivated and tenacious self-starter who is comfortable interpreting technology requirements, based on business requirements, and implementing them with maximum impact to the users. You must be results- and execution-oriented. You will be expected to develop technology in a way that minimizes technology debt while maximizing the customer experience. This is a rare opportunity to join an early-stage startup founded under a broader fintech platform, called The TIFIN Group, at a post-product, pre-revenue stage.', 'A local presence in Denver or Boulder is preferred', 'Degree in Computer Science, related field or equivalent experience', 'Design data platforms and tools that abstract implementation details for developers, analysts, and data scientists, enabling data transit and storage “as a service”', 'Create a shared understanding. We communicate with radical candor, precision and compassion to create a shared understanding. We challenge, but once a decision is made, commit fully. We listen attentively, speak candidly.', 'Make magic for our users', 'BENEFITS:', 'THE ROLE:', 'Assemble, splice, and merge large, complex data sets that meet functional / non-functional business requirements', 'Expertise in various big data technologies both open source and cloud native, AWS preferred (Kafka/Kinesis, Presto/Athena, Spark/EMR, Airflow, Hive)', 'A top performer with a proactive approach who has a “doer” & problem-solver mentality\xa0', 'Create a shared understanding.', ""WHAT YOU'LL BE DOING:"", 'Teamwork for Teamwin', 'The TIFIN Group, which starts, invests and operates first-in-category fintech companies in investment management, investment advisory, and personal finance. The TIFIN Group companies are shaping the future of investment experience. Our fintech solutions are powered by science, world-class technology, investment intelligence and algorithmic expertise.', 'Track record of choosing the right transit, storage, and analytical technology to simplify and optimize user experience', 'Assemble, splice, and merge large, complex data sets that meet functional / non-functional business requirementsBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologiesDesign data platforms and tools that abstract implementation details for developers, analysts, and data scientists, enabling data transit and storage “as a service”Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leaderWork with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needsKeep our data separated and secure across national boundaries through multiple data centers and AWS regions', 'Keep our data separated and secure across national boundaries through multiple data centers and AWS regions', 'Highly flexible, good tolerance for ambiguity, and able to quickly adapt to changing priorities', 'Ability to work comprehensively with a variety of databases (Postgres, SQL, MongoDB, etc.)', 'Real-world experience developing highly scalable solutions using micro-service architecture designed to democratize data to everyone in the organization', '5+ years of increasing responsibility in technical data roles\xa0', 'Grow at the edge. We are driven by personal growth. We get out of our comfort one and keep egos aside to find our genius zones. We strive to be the best we can possibly be. No excuses.\xa0', 'WHO WE ARE:', 'Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.', 'Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader', 'An exceptional team player with strong communication skills', 'Disrupt with creative solutions', 'WHO YOU ARE:', 'Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies', 'Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs', 'Strong coding skills in Python', 'Disrupt with creative solutions. We believe that disruptive innovation begins with curiosity and creativity. We challenge the status quo and problem solve to find new answers.\xa0', 'Grow at the edge', 'Create a shared understanding. We communicate with radical candor, precision and compassion to create a shared understanding. We challenge, but once a decision is made, commit fully. We listen attentively, speak candidly.Teamwork for Teamwin. We believe in win together, learn together. We fly in formation. We cover each other’s backs. We inspire each other with our energy and attitude.\xa0Make magic for our users. We center around the voice of the customer. With deep empathy for our clients, we create technology that transforms investor experiences.\xa0Grow at the edge. We are driven by personal growth. We get out of our comfort one and keep egos aside to find our genius zones. We strive to be the best we can possibly be. No excuses.\xa0Disrupt with creative solutions. We believe that disruptive innovation begins with curiosity and creativity. We challenge the status quo and problem solve to find new answers.\xa0', 'OUR VALUES:', 'Degree in Computer Science, related field or equivalent experience5+ years of increasing responsibility in technical data roles\xa0Real-world experience developing highly scalable solutions using micro-service architecture designed to democratize data to everyone in the organizationStrong coding skills in PythonExpertise in various big data technologies both open source and cloud native, AWS preferred (Kafka/Kinesis, Presto/Athena, Spark/EMR, Airflow, Hive)Ability to work comprehensively with a variety of databases (Postgres, SQL, MongoDB, etc.)Expertise in data model design with sensitivity to usage patterns and goals – schema, scalability, immutability, idempotency, etc.Track record of choosing the right transit, storage, and analytical technology to simplify and optimize user experienceAbility to thrive in a highly demanding, entrepreneurial, and fast-paced environment\xa0Highly flexible, good tolerance for ambiguity, and able to quickly adapt to changing prioritiesA top performer with a proactive approach who has a “doer” & problem-solver mentality\xa0An exceptional team player with strong communication skillsA local presence in Denver or Boulder is preferred']",Mid-Senior level,Full-time,Engineering,Venture Capital & Private Equity,2021-03-24 13:05:10
Data Engineer,Sagent,United States,2 weeks ago,Be among the first 25 applicants,"['', 'About the Role:\xa0\xa0\xa0', '7+ years in working collaboratively and communicating comfortably on multi-discipline development teams.', 'Job Related Experience:', 'Strong English language written and verbal skills.', 'About our Business', 'Experience in working with data sourced from both relational and non-relational (e.g. mainframe) systems.', 'Essential Job Responsibilities', 'For every individual and family, one of their biggest life purchasing decisions involve buying a house. We are the company working behind the scenes with our clients to create industry leading technology that ensures the experience is secure and easy for consumers. Each year, we are proud to help millions of people realize their home ownership dreams. Our clients, who are Blue Chip lenders and high-growth brand name disruptors trust us to help them address and anticipate the ever-evolving lending landscape. Over the years, we earned the industry’s choice for scalable performance and innovative technology that connects lenders with the people that they serve\xa0\xa0\xa0\xa0', '•\xa0Understanding of development methodologies', 'Position is remote.', '•\xa0Document and communicate technical designs, standards and processes as needed.', '•\xa0Work on a development team to help plan, implement and support enterprise applications.\xa0', 'Job Related Experience:\xa0', '•\xa0Minimum of 7 years’ experience as a data engineer\xa0', 'Essential Job Responsibilities:\xa0\xa0', 'Experience in implementation and deployment of code\xa0within a Continuous Integration/Continuous Deployment (CI/CD) discipline and repository, such as Azure DevOPS or TFS.', ""2+ years’ experience in construction and implementation of data workflows in Azure Data Factory (ADF). This includes having implemented at least one system using ADF into production.7+ years' experience in working on data-integration projects, including operational datastores, data marts, and data warehouses.7+ years’ experience in construction, implementation,\xa0unit testing, and production support of ETL/ELT solutions using tools such as SSIS, Informatica, Talend, or native database code.7+ years in working collaboratively and communicating comfortably on multi-discipline development teams.7+ years’ experience in working with terabyte-sized\xa0databases in engines such as Snowflake, Azure SQL, SQL Server, or Oracle. This includes the demonstrated ability to design and construct performant load processes, perform query optimization, and understand database and database design fundamentals that impact functionality and performance.7+ years’ experience in construction and maintenance of stored procedures.Experience in implementation and deployment of code\xa0within a Continuous Integration/Continuous Deployment (CI/CD) discipline and repository, such as Azure DevOPS or TFS.Experience in working with data sourced from both relational and non-relational (e.g. mainframe) systems.Strong English language written and verbal skills."", '•\xa0Work with others to fine tune application performance.\xa0', 'Additional Skills/Knowledge:\xa0\xa0', ""7+ years' experience in working on data-integration projects, including operational datastores, data marts, and data warehouses."", 'Required Qualifications:\xa0\xa0\xa0\xa0', '7+ years’ experience in construction and maintenance of stored procedures.', 'About the Role:\xa0', '7+ years’ experience in working with terabyte-sized\xa0databases in engines such as Snowflake, Azure SQL, SQL Server, or Oracle. This includes the demonstrated ability to design and construct performant load processes, perform query optimization, and understand database and database design fundamentals that impact functionality and performance.', 'This is one of the most exciting times to join our team because of a recent joint venture agreement between our previous parent company Fiserv and private equity firm Warburg Pincus. With this agreement, we are backed by both the legacy of a Fortune 500 company that has been voted one of the World’s Most Admired Companies and the growth expertise of a leading investment group.\xa0\xa0\xa0', '•\xa0Design, develop and test new system capabilities.', '•\xa0Bachelor’s degree in Computer Science or equivalent education in a related discipline is required.\xa0\xa0\xa0', '•\xa0Collaborate with application architect and business analysts to understand requirements.', 'Job Related Experience:\xa0\xa0\xa0', 'Additional Skills/Knowledge', 'Required Qualifications:\xa0\xa0', 'About the Role', 'Extensive experience with full-lifecycle development (i.e. research, design, coding, testing, debugging, etc.)\xa0', 'Sagent Lending Technologies is seeking a Software Development Engineer to help delight our borrowers by providing top notch client support. The team is building our next-generation borrower facing portals and servicing systems. The systems will be used by lenders and servicers in the consumer and mortgage lending markets in the United States. As a Software Development Engineer, you will be working on a delivery team using modern technologies, tools and frameworks to develop advanced, enterprise business components mainframe and Microsoft platforms.\xa0\xa0\xa0', 'Required Qualifications:', 'About our Business:\xa0\xa0\xa0', '2+ years’ experience in construction and implementation of data workflows in Azure Data Factory (ADF). This includes having implemented at least one system using ADF into production.', '•\xa0Continuously enhance skills by learning and applying relevant technologies and patterns.\xa0', 'Essential Job Responsibilities:\xa0\xa0\xa0\xa0', 'About our Business:', '•\xa0Troubleshoot and resolve software bugs and deployment issues', '7+ years’ experience in construction, implementation,\xa0unit testing, and production support of ETL/ELT solutions using tools such as SSIS, Informatica, Talend, or native database code.', '•\xa0Perform unit and integration testing to ensure application quality.\xa0']",Associate,Full-time,Information Technology,Financial Services,2021-03-24 13:05:10
Data Engineer - Portland,Nectar Markets,"Portland, OR",7 days ago,Be among the first 25 applicants,"['', 'Excellent written and verbal communication', 'HTML and CSS/Bootstrap', 'EEO Statement', 'Who We Are', 'Microsoft Excel', 'NoSQL (MongoDB) ', 'Ability to code in the following languages/packages:Python (Pandas, Plotly/Matplotlib, SQLAlchemy, and sci-kit-learn)SQL (PostgreSQL, MS SQL Server)', 'About The Role', 'Reports To', 'Munging Nectar’s data into an accessible, dependable database.Ensuring data reliability and accuracy by building processes and controls around data compilation and reporting. (ETL /Data Governance)Assisting Data Analysts in providing accurate and timely reporting to management and other stakeholders.Automating reoccurring tasks through the development of production code and data-pipelines.Assisting in the establishment of Nectar’s Code Repository.Collaborating in the architecting and implementation of a Data Warehouse/Database(s). ', 'Other visualization software (with demonstrated competence)', 'Excellent written and verbal communicationMicrosoft ExcelDatabasing/Data Warehousing – Structures / Architecture / GovernanceBuilding/Accessing APIsAbility to code in the following languages/packages:Python (Pandas, Plotly/Matplotlib, SQLAlchemy, and sci-kit-learn)SQL (PostgreSQL, MS SQL Server)Desired but not required:VBANoSQL (MongoDB) JS (D3, Plotly/Matplotlib, and/or Leaflet)HTML and CSS/BootstrapExperience with at least one of the following:PowerPointTableauPower BIOther visualization software (with demonstrated competence)Ability to present information/data in layman terms to reach a broader audience.Ability to present information/data in highly technical terms to communicate with subject matter experts.', 'Ensuring data reliability and accuracy by building processes and controls around data compilation and reporting. (ETL /Data Governance)', 'Experience with at least one of the following:PowerPointTableauPower BIOther visualization software (with demonstrated competence)', 'SQL (PostgreSQL, MS SQL Server)', 'VBA', 'PowerPointTableauPower BIOther visualization software (with demonstrated competence)', 'Desired but not required:VBANoSQL (MongoDB) JS (D3, Plotly/Matplotlib, and/or Leaflet)HTML and CSS/Bootstrap', 'Collaborating in the architecting and implementation of a Data Warehouse/Database(s). ', 'Assisting in the establishment of Nectar’s Code Repository.', 'Munging Nectar’s data into an accessible, dependable database.', 'Assisting Data Analysts in providing accurate and timely reporting to management and other stakeholders.', 'JS (D3, Plotly/Matplotlib, and/or Leaflet)', 'Building/Accessing APIs', 'Ability to present information/data in highly technical terms to communicate with subject matter experts.', 'Tableau', 'PowerPoint', 'Python (Pandas, Plotly/Matplotlib, SQLAlchemy, and sci-kit-learn)SQL (PostgreSQL, MS SQL Server)', 'Python (Pandas, Plotly/Matplotlib, SQLAlchemy, and sci-kit-learn)', 'Qualifications', 'Ability to present information/data in layman terms to reach a broader audience.', 'Power BI', 'Compensation And Benefits', 'Job Title', 'Databasing/Data Warehousing – Structures / Architecture / Governance', 'Automating reoccurring tasks through the development of production code and data-pipelines.', 'Who You Are', 'VBANoSQL (MongoDB) JS (D3, Plotly/Matplotlib, and/or Leaflet)HTML and CSS/Bootstrap']",Entry level,Full-time,Information Technology,Construction,2021-03-24 13:05:10
Data Engineer,Kofi Group,"Austin, TX",2 weeks ago,Be among the first 25 applicants,[''],Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,ICONMA,"McLean, VA",1 month ago,Be among the first 25 applicants,"['', 'Building new pipelines & integrating with other platfroms', 'At least 3 years of experience in open source programming languages for large scale data analysis (Spark, Hadoop, HDFS, AVRO)', ' AML Space Apps primarily untiled on spark, Scala, python & AWS data from upstream, involved maintenance and data pipelines of million+ records ', 'At least 2 years of Java, Scala or Python development for modern data engineering', 'Fixing data pipeline automation.', 'At least 4 years of professional work experience in data management, data warehousing or unstructured data environments', 'data from upstream, involved maintenance and data pipelines of million+ records', 'Apps primarily untiled on spark, Scala, python & AWS', ' At least 4 years of professional work experience in data management, data warehousing or unstructured data environments At least 3 years of experience in open source programming languages for large scale data analysis (Spark, Hadoop, HDFS, AVRO) At least 2 years of Java, Scala or Python development for modern data engineering At least 1 years of experience with Real Time data stream platforms (Kafka, Spark Streaming) At least 1 year of experience working with cloud data capabilities (Amazon AWS, Microsoft Azure) ', 'ETL Processing through scala & spark', 'Role Info', 'Data Engineer ', ' Hands on Data Engineer role with strong Scala & Spark background Enhancing Data Pipelines  Fixing data pipeline automation. Building new pipelines & integrating with other platfroms ETL Processing through scala & spark ', 'Top Skills', 'AML Space', 'Team Info', 'Data Engineer', 'Enhancing Data Pipelines ', 'At least 1 years of experience with Real Time data stream platforms (Kafka, Spark Streaming)', 'Duration 12 + Months ', 'Location: McLean, VA', 'Job Description', 'At least 1 year of experience working with cloud data capabilities (Amazon AWS, Microsoft Azure)', 'Hands on Data Engineer role with strong Scala & Spark background']",Entry level,Contract,Information Technology,Financial Services,2021-03-24 13:05:10
Data Engineer,Millennium Corporation,"Washington, DC",4 days ago,Be among the first 25 applicants,"[""Assist with Business Development activities as required to support Millennium's strategic business objectives, which may include but not limited to participation in technical interviews, creation of technical documentation, general proposal writing support and proposal color reviews."", 'Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.', '10% travel ', 'Identify, design, and implement internal process improvements automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.', 'Must be comfortable with prolonged periods of sitting at a desk and working on a computer.', 'Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.', 'Create and maintain optimal data pipeline architecture,', 'Candidate must have an active Secret Clearance ', 'Assemble large, complex data sets that meet functional / non-functional business requirements.', 'Candidate must have an active Secret Clearance Bachelor’s degree in Computer Science, Engineering, Mathematics or other business-related field with eight (8) years of relevant experience in data engineering, data management and transformation OR High School Diploma with at least eighteen (18) years of relevant experience.', 'active Secret Clearance.', 'Serve as lead on projects.', 'Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.', 'Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.', 'Bachelor’s degree in Computer Science, Engineering, Mathematics or other business-related field with eight (8) years of relevant experience in data engineering, data management and transformation OR High School Diploma with at least eighteen (18) years of relevant experience.', 'The Data Engineer Will', 'Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.', 'Data Engineer to work remote from the DC Metro area. ', 'Work with data and analytics experts to strive for greater functionality in our data systems.', 'Create and maintain optimal data pipeline architecture,Assemble large, complex data sets that meet functional / non-functional business requirements.Identify, design, and implement internal process improvements automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.Work with data and analytics experts to strive for greater functionality in our data systems.Serve as lead on projects.', 'Must be comfortable with prolonged periods of sitting at a desk and working on a computer.Must be able to lift up to 10-15 pounds at a time.', 'Must be able to lift up to 10-15 pounds at a time.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer - Rochester,Performix,"Rochester, MN",4 weeks ago,Be among the first 25 applicants,"[' Design and maintain data pipelines and services using best practice for ETL/ELT, data management and data governance. Analyze raw data sources and data transformation requirements. Perform data modelling against large datasets for peak requirements. Identify, design and implement process improvement solutions that automate manual processes and leverage standard frameworks and methodologies. Understand and incorporate data quality principals that ensure optimal performance, impact and user experience. Create and document functional and technical specifications. Perform ongoing research to explore new features, versions and related technologies, and provide recommendations to enhance our offerings Cloud Data Engineer for Google Cloud Project', ' Create and document functional and technical specifications.', 'Skills Required', "" Bachelor's degree in Computer Science, Information Technology or related field; OR equivalent 3+ years of experience. 3+ years of hands-on experience programming in SQL. 2+ years of experience building and maintaining automated data pipelines and data assets using batch and/or streaming processes"", ' Design and maintain data pipelines and services using best practice for ETL/ELT, data management and data governance.', ' Understand and incorporate data quality principals that ensure optimal performance, impact and user experience.', ' Analyze raw data sources and data transformation requirements.', 'Education', 'Duties', ' 3+ years of hands-on experience programming in SQL. 2+ years of experience building and maintaining automated data pipelines and data assets using batch and/or streaming processes', ' ENGINEER', ' Cloud Data Engineer for Google Cloud Project', ' Perform ongoing research to explore new features, versions and related technologies, and provide recommendations to enhance our offerings', "" Bachelor's degree in Computer Science, Information Technology or related field; OR equivalent 3+ years of experience."", ' GCP', ' SQL', ' Perform data modelling against large datasets for peak requirements.', ' GCP ENGINEER SQL', ' Identify, design and implement process improvement solutions that automate manual processes and leverage standard frameworks and methodologies.', 'Job Description']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,CyberCoders,"Cambridge, MA",4 days ago,Be among the first 25 applicants,"['', 'Your Right to Work', ' Improve our data systems performance and reliability against steadily increasing loads and varieties of work', ' Rrelational databases, tuning and ETL: Oracle, Postgres, MySQL, etc', ' Telemetry and monitoring experience: Splunk, Elasticsearch, etc', ' Work creatively to derive data from existing sets to enable new capabilities', 'Please do NOT change the email subject line in any way. You must keep the JobID: linkedin : PT3-1619197 -- in the email subject line for your application to be considered.***', 'Email Your Resume In Word To', ' Empower internal users with tools that allow them to service their own needs in collecting, querying and analyzing data', ""At Least 2 Years' Experience With a Combination Of"", ' Programming experience: Python, NodeJS, PHP, etc', ' Rrelational databases, tuning and ETL: Oracle, Postgres, MySQL, etc AWS data systems: S3, RDS, DynamoDB, Aurora, Redshift, etc Programming experience: Python, NodeJS, PHP, etc Telemetry and monitoring experience: Splunk, Elasticsearch, etc', ' Help design, construct and optimize systems to aggregate, transform and process large amounts of data from relational, columnar and other kinds of data stores Empower internal users with tools that allow them to service their own needs in collecting, querying and analyzing data Improve our data systems performance and reliability against steadily increasing loads and varieties of work Work creatively to derive data from existing sets to enable new capabilities Employ machine learning tools to enable new product features and outcomes', ' Employ machine learning tools to enable new product features and outcomes', 'CyberCoders, Inc is proud to be an Equal Opportunity Employer', ' Help design, construct and optimize systems to aggregate, transform and process large amounts of data from relational, columnar and other kinds of data stores', ' AWS data systems: S3, RDS, DynamoDB, Aurora, Redshift, etc']",Entry level,Full-time,Information Technology,Hospital & Health Care,2021-03-24 13:05:10
Data Engineer/ Architect,nfolks,"United, LA",2 days ago,Be among the first 25 applicants,[''],Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Platform Engineer,New York Life Insurance Company,"New York, NY",2 days ago,Be among the first 25 applicants,"['', 'Competencies:', 'Education:', 'Solid experience in standing up enterprise practices for Big Data, Analytics, Self-Service', 'Capable of working with open source software, debugging issues and working with vendors toward effective resolution', 'Desired Qualifications:', 'Thinks strategically - sets overall direction for solution design and delivery for enterprise platforms aligned to the data & analytics strategy', 'Proficient with Unix/Linux (building/assembling packages, shell scripts, configuration management and OS tuning)', 'Knowledge of enabling Kerberos and best practices for securing data a plus', 'Excellent communication skills, both written and verbal in conveying technical design and approach for delivering technical solution', 'Pragmatic in his/her approach, delivering incrementally and demonstrating value', 'Minimum Bachelor’s Degree in relevant field; Master’s Degree a plus', ""Experience with Cloudera's Data Platform (CDP), Hortonworks Data Platform (HDP), and the AWS stack is highly preferred"", 'Primary Responsibilities', 'Understand data related initiatives within New York Life and engineer optimal designs and solutions', ""Proven expertise with AWS technologies (EC2, S3, Glue, Athena, RedShift, CloudFormation, LakeFormation, Terraform, etc.).\xa0AWS certifications highly preferredSolid understanding of Hadoop technologies (YARN, Hive, MR, Tez, Spark, etc.)Proficient with Unix/Linux (building/assembling packages, shell scripts, configuration management and OS tuning)Knowledge of\xa0configuration management/automation tooling (Puppet/Chef/Salt/Terraform)Experience with Java, Python and API'sKnowledge of enabling Kerberos and best practices for securing data a plusExperience working with Vendors/Open Source in the Data and Cloud ecosystemKnowledge of the open source community (opening issues, tracking issues and identifying problematic issues ahead of time by tracking open JIRA issues in the community)Understanding of Networking (tracing, packet capture, etc.)"", 'Results Driven - sets aggressive goals and is accountable for continuously driving improved outcomes, leading change and ensuring high standards', ""The Enterprise Data Management organization is tasked with creating and delivering a technology strategy for New York Life's Data & Analytics.\xa0The continued evolution of this data strategy requires a Data Platform Engineer with AWS expertise to help drive the next phase of the journey!\xa0"", 'Drive knowledge management practices for key enterprise data platforms and collaborate with team members', 'Experience working with Vendors/Open Source in the Data and Cloud ecosystem', 'Knowledge of the open source community (opening issues, tracking issues and identifying problematic issues ahead of time by tracking open JIRA issues in the community)', 'Stay current and informed on emerging technologies and new techniques to refine and improve overall delivery', '8+ years in a variety of technology – especially AWS, Big Data, Linux, Web, and Databases', 'Accountable for designing and delivering against New York Life’s data technology strategy', ""Experience with Java, Python and API's"", 'Understanding of Networking (tracing, packet capture, etc.)', 'Accountable for designing and delivering against New York Life’s data technology strategyWork with a team of engineers and developers to deliver against the overall technology data strategyEnsure enterprise data platforms are standardized, optimized, available, reliable, consistent, accessible and secure to support business and technology needsUnderstand data related initiatives within New York Life and engineer optimal designs and solutionsDrive knowledge management practices for key enterprise data platforms and collaborate with team membersCollaborate with peers across Enterprise Data Management, to deliver on the overarching strategyDevelop framework, metrics and reporting to ensure progress can be measured, evaluated and continually improvedStay current and informed on emerging technologies and new techniques to refine and improve overall delivery', '\xa0', 'Required Skills:', 'Ability to help train/develop junior resources on the team', 'Role Location: New York City (Primary), Clinton, NJ,\xa0Dallas, TX,\xa0Jersey City, NJ, White Plains, NYC also an option.', ""8+ years in a variety of technology – especially AWS, Big Data, Linux, Web, and DatabasesExperience with Cloudera's Data Platform (CDP), Hortonworks Data Platform (HDP), and the AWS stack is highly preferredDeep expertise in data related tools including latest data solutions (i.e. Big Data, Cloud, In Memory Analytics, etc.)Hands-on experience with Hadoop, NoSQL DBs, Big Data warehouses, and insights on when to recommend a particular solutionSolid experience in standing up enterprise practices for Big Data, Analytics, Self-ServiceProven track record for identifying, architecting and building new technology solutions to solve complex business problemsCapable of working with open source software, debugging issues and working with vendors toward effective resolution"", 'Proven expertise with AWS technologies (EC2, S3, Glue, Athena, RedShift, CloudFormation, LakeFormation, Terraform, etc.).\xa0AWS certifications highly preferred', 'Thinks strategically - sets overall direction for solution design and delivery for enterprise platforms aligned to the data & analytics strategyResults Driven - sets aggressive goals and is accountable for continuously driving improved outcomes, leading change and ensuring high standardsExcellent communication skills, both written and verbal in conveying technical design and approach for delivering technical solutionPragmatic in his/her approach, delivering incrementally and demonstrating valueAbility to help train/develop junior resources on the teamOther competencies: critical thinker, adaptable, self-starter, demonstrates sound judgment', 'Solid understanding of Hadoop technologies (YARN, Hive, MR, Tez, Spark, etc.)', 'Other competencies: critical thinker, adaptable, self-starter, demonstrates sound judgment', 'Serving as a Data Platform Engineer, this\xa0provides a unique opportunity to help shape and influence one of the most strategic initiatives at New York Life.\xa0The position will be responsible for building enterprise data management platforms both on-premise and in the cloud. It is expected that this engineer will architect and build out the core data platforms for the enterprise. This role will require an advanced skill-set across a variety of technologies. This individual will often have to learn on their own and remain on the cusp of new technologies in the Big Data, Analytics, and Cloud space.', 'Proven track record for identifying, architecting and building new technology solutions to solve complex business problems', 'Deep expertise in data related tools including latest data solutions (i.e. Big Data, Cloud, In Memory Analytics, etc.)', 'Hands-on experience with Hadoop, NoSQL DBs, Big Data warehouses, and insights on when to recommend a particular solution', 'Develop framework, metrics and reporting to ensure progress can be measured, evaluated and continually improved', 'Collaborate with peers across Enterprise Data Management, to deliver on the overarching strategy', 'Ensure enterprise data platforms are standardized, optimized, available, reliable, consistent, accessible and secure to support business and technology needs', 'Knowledge of\xa0configuration management/automation tooling (Puppet/Chef/Salt/Terraform)', 'Work with a team of engineers and developers to deliver against the overall technology data strategy']",Mid-Senior level,Full-time,Product Management,Financial Services,2021-03-24 13:05:10
Data Engineer,DAT Freight & Analytics,"Denver, CO",7 days ago,25 applicants,"['', 'The range of responsibilities of the Data Engineer are broad and range from traditional ETL & SQL solutions to building cloud infrastructure and serverless data processing solutions (e.g. AWS Lambdas) for sophisticated machine learning models. In this role you will be part of a supportive, collaborative, and diverse team responsible for exploring, making recommendations, and implementing enterprise data solutions primarily in our snowflake data warehouse.', 'Leverage AWS to deliver efficient, cloud-native solutions', 'Working closely with product teams and data scientists to determine how to capture, persist, and maintain data', 'Experience modeling star schemas and other summary objects based on business requirements', 'Analyzing\xa0and recommending alternate persistence solutions\xa0', 'Building distributed, scalable, and reliable data pipelines that ingest and process data at scale with ever diminishing time processing windows', '\xa0\xa0', 'DAT Software and Analytics offers competitive compensation and an excellent benefit package that includes medical, dental, and vision coverage, flexible savings accounts, 401K, Life and AD&D insurance, a comprehensive Paid Leave program, and a Tuition Reimbursement program.', 'Gathering requirements from analysts and the larger business community to optimize, unify, and simplify data according to business needs', 'Strong SQL Proficiency with Snowflake including analytic queries and other complex use cases', 'Experience with SQL Query Tuning, data investigation, and optimization', 'DAT Software and Analytics embraces the value of a diverse workforce, and believes it is a core strength of our company that we encourage those values in every DAT employee, at every level of our organization, regardless of tenure or rank. We provide equal employment opportunities (EEO) to all employees and applicants without regard to race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, disability, genetic information, marital status, amnesty, or status as a covered veteran in accordance with applicable federal, state, and local laws.', 'Working either independently on projects or in collaborative teams depending upon the needs of the effort', 'Serving as a resource for the Data Sciences team by partnering with them to make production quality and supportable solutions\xa0', 'Scripting experience with Powershell and Python', 'DAT Software and Analytics is a next-generation SaaS technology company that has been at the leading edge of innovation in transportation supply chain logistics for 43 years. We continue to transform the industry year over year, by deploying a suite of software solutions to millions of customers every day - customers who depend on DAT for the most relevant data and most accurate insights to help them make smarter business decisions and run their companies more profitably. We operate the largest marketplace of its kind in North America, with 226 million freight posts in 2020, and a database of $110 billion of annual global shipment market transaction data. DAT is based in Beaverton, OR, with offices in Colorado, Missouri, Texas, and Bangalore, India.', 'a Data Engineer', 'Understanding of Oracle, SQL Server, Mongo and other database systems', 'The Skills You’ll Need', 'Experience with AWS tools and infrastructure.\xa0Particularly in regards to s3 storage, data pipelines, and serverless AWS compute services (e.g. Lambdas)', 'Bonus Skills', 'For additional information, see www.DAT.com/company', 'Understanding of modern data warehouse architectures such as data vault', 'DAT Software & Analytics is seeking a Data Engineer to join our Data and Engineering team in Denver, Colorado.', '\xa0', 'Exceptional ELT/ETL background with data transformation and operational experience\xa0\xa0', 'About DAT', 'Identifying opportunities in our data landscape where we need data enrichment', 'What You’ll Do', 'Working closely with product teams and data scientists to determine how to capture, persist, and maintain dataBuilding distributed, scalable, and reliable data pipelines that ingest and process data at scale with ever diminishing time processing windowsLeverage AWS to deliver efficient, cloud-native solutionsIdentifying opportunities in our data landscape where we need data enrichmentIdentifying and make recommendations on data storage solutions using alternate\xa0technologiesAnalyzing\xa0and recommending alternate persistence solutions\xa0Serving as a resource for the Data Sciences team by partnering with them to make production quality and supportable solutions\xa0Gathering requirements from analysts and the larger business community to optimize, unify, and simplify data according to business needsWorking either independently on projects or in collaborative teams depending upon the needs of the effort', 'Exceptional ELT/ETL background with data transformation and operational experience\xa0\xa0Strong SQL Proficiency with Snowflake including analytic queries and other complex use casesExperience with SQL Query Tuning, data investigation, and optimizationExperience with Snowflake cloud data warehouse administration.\xa0Experience modeling star schemas and other summary objects based on business requirementsExperience building large data stores, modern data warehouse, or advanced data processing solutionsExperience with AWS tools and infrastructure.\xa0Particularly in regards to s3 storage, data pipelines, and serverless AWS compute services (e.g. Lambdas)Deployment pipeline and infrastructure as code and deployment pipeline experience on a cloud platform such as AWS', 'All referrals and résumés are managed exclusively through the Human Resources Department.', 'Experience with Snowflake cloud data warehouse administration.\xa0', 'DAT’s industry dominant network of carriers, brokers and shippers makes our data an immensely valuable asset to the industry.\xa0', 'DAT Software and Analytics will not consider unsolicited résumés from vendors including search firms, fee-based referral services, and/or recruitment agencies.', 'Experience with WhereScape RED ELT Software', 'Understanding of Oracle, SQL Server, Mongo and other database systemsScripting experience with Powershell and PythonExperience with WhereScape RED ELT SoftwareUnderstanding of modern data warehouse architectures such as data vault', 'Deployment pipeline and infrastructure as code and deployment pipeline experience on a cloud platform such as AWS', 'Identifying and make recommendations on data storage solutions using alternate\xa0technologies', 'Experience building large data stores, modern data warehouse, or advanced data processing solutions']",Mid-Senior level,Full-time,Engineering,Computer Software,2021-03-24 13:05:10
Data Engineer- ETL/ Informatica,Mastech Digital,"Austin, MN",6 days ago,49 applicants,"['', '·Provide issue resolution and maintenance for a large variety of business unit solutions already existing in the enterprise data warehouse', 'What client is looking for:', '·Works directly with business units to understand their analytics use cases and gather requirements for an analytics solution', 'OBIEE/ OBIA- Nice to have the knowledge and basics', '·Engineers and performance tunes Informatica ETLs to move data from a variety of source systems and file types to fit into dimensional data models', 'ONLY VISA INDEPENDENTS', 'Loctaion: Austin, MN- Post COVID -Onsite- Relocation Assistance Provided', 'Responsibilities', '·Utilizes advanced SQL within Informatica ETLs when necessary to achieve proper metric calculations or derive dimension attributes', 'Titlle: Data Engineer- ETL/Informatica', 'Strong PL/SQL development with core Data Warehousing experience- Specific to ETL- Informatica ', 'Duration: 4 Months to Hire', '·Engineers data models for dimensions and facts within the staging and warehouse layer of our enterprise data warehouse', '·Engineers schedule and orchestration for batch and near-real time data loads into the enterprise data warehouse', '·Engineers dashboards within the enterprise business intelligence platform containing reports and visualization that have intelligent user interface design and flow for the business including adequate performance']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer - Chattanooga,FreightWaves,"Chattanooga, TN",2 weeks ago,Be among the first 25 applicants,"['', 'An excellent start-up work environment, flat hierarchies, and short decision paths.', 'Annual life achievement bonus of $2000 for having a baby, buying a house, or getting married (max one per year) ', 'Gym Membership (or virtual membership while COVID is still a part of our daily lives)', 'What We Are Looking For', 'Build and support data pipelines from various sources (API, FTP, streaming, cloud storage buckets, etc.) with data in various formats (JSON, CSV, tab delimited, XML, etc.)', 'Experience with AWS, Snowflake or GCP and strong working knowledge of the platforms’ services for supporting data pipelines and analytics', 'Discount on Ford Vehicles ', 'Stock options', '401k with up to 3.5% match', 'Our Benefits', 'Student-loan reimbursement ', 'Audible or Kindle Unlimited subscription ', 'Work directly with data providers to better understand source data', 'Identify ways to improve application and data reliability, resilience, efficiency and quality', 'Strong in SQL and Python', 'An excellent start-up work environment, flat hierarchies, and short decision paths.Competitive salaryWork from homeA generous benefits package including 100% employer-paid health, dental, vision and Life insurance, STD, LTDStock options401k with up to 3.5% matchTraining programs and career development opportunities.Student-loan reimbursement Annual life achievement bonus of $2000 for having a baby, buying a house, or getting married (max one per year) No set days off Vacation policy (our team take time off as needed with supervisor approval)Gym Membership (or virtual membership while COVID is still a part of our daily lives)Audible or Kindle Unlimited subscription FreightWaves strives for sustainability. We offset our carbon emissions.Discount on Ford Vehicles ', 'Competitive salary', 'What You Will Be Doing', 'QA datasets for data integrity and data quality', 'Must have a “cloud first” mentality and embrace the paradigm of cloud services and microservices', 'A generous benefits package including 100% employer-paid health, dental, vision and Life insurance, STD, LTD', 'Build and support data pipelines from various sources (API, FTP, streaming, cloud storage buckets, etc.) with data in various formats (JSON, CSV, tab delimited, XML, etc.)Work closely with our Data Science team to build enriched data sources to better support detailed analysis and customer supportQA datasets for data integrity and data qualityIdentify ways to improve application and data reliability, resilience, efficiency and qualityWork directly with data providers to better understand source data', 'Must have a “cloud first” mentality and embrace the paradigm of cloud services and microservicesExperience with AWS, Snowflake or GCP and strong working knowledge of the platforms’ services for supporting data pipelines and analyticsStrong in SQL and PythonComfortable on the Linux/Unix and Windows command lineComfortable with Git/GitHub and Git workflows', 'Training programs and career development opportunities.', 'Work from home', 'FreightWaves strives for sustainability. We offset our carbon emissions.', 'Comfortable on the Linux/Unix and Windows command line', 'Comfortable with Git/GitHub and Git workflows', 'No set days off Vacation policy (our team take time off as needed with supervisor approval)', 'Work closely with our Data Science team to build enriched data sources to better support detailed analysis and customer support']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer II,Expedia Group,"Chicago, IL",4 days ago,Be among the first 25 applicants,"['', 'Experience And Qualification', 'Responsibilities']",Mid-Senior level,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Data Engineer,Elder Research,"Arlington, VA",6 days ago,Be among the first 25 applicants,"['', 'Desired Skills', 'Selected Technologies: (A combination of experience with some of the following is ', 'Communicate clearly both verbally and in writing to teammates and clients ', 'Participate in every stage of the engineering lifecycle, from ideation and requirements gathering through implementation, testing, deployment, and maintenance ', 'Cloud platform development and SaaS ', 'Ability to consider both long-term stability and scalability while taking a user-focused approach to development and deployment. ', 'Bachelor’s Degree in Computer Science, Engineering, Information Systems or related technical discipline. A Master’s degree may be substituted for up to two (2) years of experience. A PhD may be substituted for up to five (5) years of experience.', 'Previous experience supporting projects with US Customs and Border ProtectionData manipulation, SQL, relational databases, and/or NoSQL databases – experience as a DBA is a huge plus Cloud platform development and SaaS DevOps – infrastructure, continuous integration and automation, packaging and deployment Consulting experience is a plus', 'Ability and the willingness to tailor applications to a client’s business goals using an iterative methodology. ', 'Travel to and work on-site at clients both local and non-local. Number of days at client site vary depending on project requirements', 'Ability to support the development of organization-wide data models for use in designing and building integrated, shared software and database systems;', 'Perform other technical tasks as needed, including writing project reports, managing, implementing, and/or maintaining technical infrastructure, etc. ', 'Ability to perform functional and data requirements analysis, and implementation of data engineering projects, analyze customer requirements and provide solution recommendations.', 'required) ', 'Demonstrate knowledge of information engineering methodologies, process improvement, and performance measurement', 'Work collaboratively with data scientists, business consultants, and software engineers to create and deploy dynamic data applications that help our customers make meaningful business decisions. Develop and deploy robust data pipelines and end-to-end systems Participate in every stage of the engineering lifecycle, from ideation and requirements gathering through implementation, testing, deployment, and maintenance Provide leadership and coordination for certain stages of the engineering lifecycle as needed Perform other technical tasks as needed, including writing project reports, managing, implementing, and/or maintaining technical infrastructure, etc. Ability and the willingness to tailor applications to a client’s business goals using an iterative methodology. Ability to consider both long-term stability and scalability while taking a user-focused approach to development and deployment. Communicate clearly both verbally and in writing to teammates and clients Ability to work independently in a collaborative, dynamic, cross-functional environment Travel to and work on-site at clients both local and non-local. Number of days at client site vary depending on project requirements', 'Data Engineer ', 'DevOps – infrastructure, continuous integration and automation, packaging and deployment ', 'Work collaboratively with data scientists, business consultants, and software engineers to create and deploy dynamic data applications that help our customers make meaningful business decisions. ', 'Provide leadership and coordination for certain stages of the engineering lifecycle as needed ', 'Previous experience supporting projects with US Customs and Border Protection', 'Data manipulation, SQL, relational databases, and/or NoSQL databases – experience as a DBA is a huge plus ', 'Eight (8) years relevant experience data architecture, Computer Science, Engineering, Information Systems or related technical discipline in applied data science research or big data analytics.Bachelor’s Degree in Computer Science, Engineering, Information Systems or related technical discipline. A Master’s degree may be substituted for up to two (2) years of experience. A PhD may be substituted for up to five (5) years of experience.Ability to perform functional and data requirements analysis, and implementation of data engineering projects, analyze customer requirements and provide solution recommendations.Demonstrate knowledge of information engineering methodologies, process improvement, and performance measurementAbility to support the development of organization-wide data models for use in designing and building integrated, shared software and database systems;Is currently CBP Cleared or have an active/current TS Clearance. ', 'Is currently CBP Cleared or have an active/current TS Clearance. ', 'Eight (8) years relevant experience data architecture, Computer Science, Engineering, Information Systems or related technical discipline in applied data science research or big data analytics.', 'Data Engineer', 'Essential Functions', 'By submitting your interest in this job, you agree to receive text notifications with additional steps to complete your job application. You will receive up to 6 messages from the number ""63879"". Message & data rates may apply. Please refer to our privacy policy for more information.', 'Develop and deploy robust data pipelines and end-to-end systems ', 'Job Description', 'About Elder Research, Inc.', 'Consulting experience is a plus', 'Ability to work independently in a collaborative, dynamic, cross-functional environment ', 'Required Skills']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Sr. Data Integration Engineer,"Medifast, Inc","Baltimore, MD",1 day ago,Be among the first 25 applicants,"['', ' Plan data integration process by developing common definitions of sourced data, designing common keys in physical data structure, establishing data integration specifications, examining data applications etc. ', ' Design, specify, document and propose integration & data solutions, considering current and future needs for enterprise data sources, stores, security, infrastructure and performance. ', ' Analyze and study various data sources by interviewing different users, defining & validating data objects and identifying the relationship among them.  Plan data integration process by developing common definitions of sourced data, designing common keys in physical data structure, establishing data integration specifications, examining data applications etc. ', ' Improves data integration by designing and evaluating new data interchange formats; improves physical design; rewrites data policy, standards, and procedures  Assess and support different middleware tools for data integration, transformation, and routing.  Design, specify, document and propose integration & data solutions, considering current and future needs for enterprise data sources, stores, security, infrastructure and performance.  Familiar with some of these designing and documentation tools like Lucid Chart, Visio, Power Point Presentation, Jira and Confluence ', ' Bachelor’s Degree in Computer Science, Information Systems, Engineering, or closely related field or equivalent related professional experience and an Associate’s Degree. Additional Certifications preferred. ', 'Leadership', 'Design & Documentation', ' Working with Data Scientists and developers, build integrations, tools and scripts that take advantage of the Insights to support organizational strategic, clinical, ad-hoc and regulatory imperatives.  Collaborate with agile team in the planning, design, development, testing and deployment of new Integrations, BI Reports, and enhancements to existing features.  Self-driven and responsible for successful delivery of project from initiation to execution. ', 'Overview', ' Lead or participate in multiple projects by completing and updating project documentation; managing project scope; adjusting schedules when necessary; determining daily priorities; ensuring efficient and on-time delivery of project tasks and milestones; following proper escalation paths; and managing customer relationships and all stakeholders ', ' Collaborate with agile team in the planning, design, development, testing and deployment of new Integrations, BI Reports, and enhancements to existing features. ', 'Analysis & Planning', ' Improves data integration by designing and evaluating new data interchange formats; improves physical design; rewrites data policy, standards, and procedures ', ' Assess and support different middleware tools for data integration, transformation, and routing. ', ' Self-driven and responsible for successful delivery of project from initiation to execution. ', 'Responsibilities', ' 5+ years in a data engineering role with demonstrable experience in data integration and data warehouse projects  Robust 5+ experience building Integrations with some of the leading ETL Tools & Platforms - SSIS, AWS Lambda, Informatica ETL, Mulesoft Anypoint Platform, Attunity/Qlik Replicate etc.  2+ years of experience working on Amazon Cloud platform & services – Amazon S3, Amazon RedShift, Amazon Redshift Spectrum, AWS Glue, AWS DMS etc.  Experienced to comprehend and transform different data formats – CSV, XML, JSON, SOAP, JMS  Bachelor’s Degree in Computer Science, Information Systems, Engineering, or closely related field or equivalent related professional experience and an Associate’s Degree. Additional Certifications preferred.  Excellent analytical, problem solving and conceptual thinking skills  Any past work experience in CPG industry would be desirable', ' 5+ years in a data engineering role with demonstrable experience in data integration and data warehouse projects ', 'Qualifications', ' Familiar with some of these designing and documentation tools like Lucid Chart, Visio, Power Point Presentation, Jira and Confluence ', ' Analyze and study various data sources by interviewing different users, defining & validating data objects and identifying the relationship among them. ', ' Experienced to comprehend and transform different data formats – CSV, XML, JSON, SOAP, JMS ', ' Working with Data Scientists and developers, build integrations, tools and scripts that take advantage of the Insights to support organizational strategic, clinical, ad-hoc and regulatory imperatives. ', ' 2+ years of experience working on Amazon Cloud platform & services – Amazon S3, Amazon RedShift, Amazon Redshift Spectrum, AWS Glue, AWS DMS etc. ', 'Support Integration Tools & Platforms', ' Robust 5+ experience building Integrations with some of the leading ETL Tools & Platforms - SSIS, AWS Lambda, Informatica ETL, Mulesoft Anypoint Platform, Attunity/Qlik Replicate etc. ', ' Any past work experience in CPG industry would be desirable', ' Excellent analytical, problem solving and conceptual thinking skills ']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Coupa Software,"Reno, NV",2 weeks ago,Be among the first 25 applicants,"['', 'Experience interacting with APIs, utilizing SDKs, and other tools', 'Data stores infrastructure management: ensure that data lake, scheduled jobs, query and exploration tool and connections with visualization tools are meeting business requirements while remaining cost-effective', '3+ years of technical hands-on experience with Data Warehouses such as AWS RDS PostgreSQL, Snowflake, AWS Redshift etc.', 'Experience with Workato and Fivetran or other integration (Mulesoft, Informatica) tools required', 'Working experience with AWS resources strongly preferred (e.g. S3, Glue, Athena, Lambda, RDS, EC2, Spark, CloudFormation)', '2. Focus On Results', 'Develop and maintain star schemas and assist Data Analysts with the maintenance and development of BI-serving models', 'Collaborate with Data Architecture IT team to internally serve Coupa business units with the highest quality, best-in-breed solutions for BI infrastructures, applications integrations, task automation and any relevant data automation needs', '3+ years of technical hands-on experience with Data Warehouses such as AWS RDS PostgreSQL, Snowflake, AWS Redshift etc.Working experience with AWS resources strongly preferred (e.g. S3, Glue, Athena, Lambda, RDS, EC2, Spark, CloudFormation)Coding proficiency in at least one modern programming language (Python, Ruby, Java, etc.)Proven track record of sharing outcomes through written communication, including an ability to effectively communicate with both business and technical teamsStrong interpersonal skills and the ability to communicate complex technology solutions to leadership, gain alignment, and drive progressExperience with Workato and Fivetran or other integration (Mulesoft, Informatica) tools requiredAdvanced knowledge of PostGres SQLExperience interacting with APIs, utilizing SDKs, and other tools', 'Build and maintain the infrastructure required for extraction, loading, transformation and storage of data from multiple data sources using custom scripted as well as SaaS technologies', 'Collaborate with Data Architecture IT team to internally serve Coupa business units with the highest quality, best-in-breed solutions for BI infrastructures, applications integrations, task automation and any relevant data automation needsBuild and maintain the infrastructure required for extraction, loading, transformation and storage of data from multiple data sources using custom scripted as well as SaaS technologiesBuild processes supporting data transformation, data structures, metadata, dependencies and model managementDevelop and maintain star schemas and assist Data Analysts with the maintenance and development of BI-serving modelsData stores infrastructure management: ensure that data lake, scheduled jobs, query and exploration tool and connections with visualization tools are meeting business requirements while remaining cost-effectiveManage and maintain the connections between different data consuming tools and data storesCreate and maintain tools to monitor data quality and integrityPerform periodic health checks and audits on all data stores, ETLs, ELTs and data consumption connectionsFully utilize the resources available in our data stores infrastructure to optimize operations and maximize data security and reliability', 'Create and maintain tools to monitor data quality and integrity', '1. Ensure Customer Success', 'Perform periodic health checks and audits on all data stores, ETLs, ELTs and data consumption connections', 'Requirements: ', 'Build processes supporting data transformation, data structures, metadata, dependencies and model management', 'Strong interpersonal skills and the ability to communicate complex technology solutions to leadership, gain alignment, and drive progress', 'Coding proficiency in at least one modern programming language (Python, Ruby, Java, etc.)', 'Proven track record of sharing outcomes through written communication, including an ability to effectively communicate with both business and technical teams', 'Responsibilities: ', '3. Strive For Excellence', 'Advanced knowledge of PostGres SQL', 'Manage and maintain the connections between different data consuming tools and data stores', 'Fully utilize the resources available in our data stores infrastructure to optimize operations and maximize data security and reliability', 'Please be advised, inquiries or resumes from recruiters will not be accepted.']",Not Applicable,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Lead Data Quality Engineer (Remote),Wondersign®,"San Diego, CA",1 week ago,Be among the first 25 applicants,"['', 'Experience with big data systems and BI products', 'Assist with all metrics level reporting', 'Strong Analytical skills ', 'Company supports professional development for all team members', ""Bachelor's Degree in a technical field, Data Science, Statistics or Mathematics "", 'Offering Freedom & Flexibility. ', 'Conduct exploratory data analysis from complex data sources', 'Who We Are', ""Overall 5-6 years of experience in Software Quality Assurance role including Data Management and TestingExperience with one or more programming languages like Python or RExperience writing complex SQL queries is a mustExperience with big data systems and BI productsKnowledge of statistical analysis techniquesStrong Analytical skills Exposure to AWS Lambda and S3Experience with source control and dependency management softwareAbility to lead projects and manage processExperience with conducting and running defect triageBachelor's Degree in a technical field, Data Science, Statistics or Mathematics "", 'This position will require the following physical requirements; sitting (75%), walking (15%), standing (10%), lifting up to 10 pounds.This position will require the following mental requirements; Ability to reason through problems to reach solutions, troubleshooting ability, effective written and verbal communications skills and ability to see, type, speak on phone and work with various departments within the company.', 'Experience with one or more programming languages like Python or R', 'Exposure to AWS Lambda and S3', 'Essential Physical & Mental Requirements', 'Integrate and load data to the backend database', 'Lead projects and assignments', 'Experience with source control and dependency management software', 'Top-notch tech/equipment/software you need to do your job', 'Physical Demands/Work Environment', 'Experience writing complex SQL queries is a must', 'Identify gaps in the quality assurance, data management, and testing processes, generate solutions and drive continuous improvement', 'Company pays 100% of your medical, dental and vision insurance, 80% for your family', 'Short and long term disability insurance ', 'Life insurance ', 'Generate reports of findings and represent to the business', 'Ability to lead projects and manage process', 'Being Truthful & Inclusive. ', 'This position will require the following mental requirements; Ability to reason through problems to reach solutions, troubleshooting ability, effective written and verbal communications skills and ability to see, type, speak on phone and work with various departments within the company.', 'The Perks', 'This position will require the following physical requirements; sitting (75%), walking (15%), standing (10%), lifting up to 10 pounds.', ""Perform quality assurance and quality control duties for Wondersign's Data Management and data development life cycleCreate and maintain test scripts to ensure data services and programs meet acceptance criteriaParticipate in manual testing and troubleshooting when requiredDevelop Validation & Verification plans against the requirementLead projects and assignmentsIdentify gaps in the quality assurance, data management, and testing processes, generate solutions and drive continuous improvementEnsure the intent of requirement is satisfiedRecommend technical strategies to contribute to the overall effectiveness and qualityConduct exploratory data analysis from complex data sourcesGenerate reports of findings and represent to the businessTrack and adequately report defects using appropriate toolsIntegrate and load data to the backend databaseIdentify patterns and trends in data setsAssist with all metrics level reportingCommunicate with internal and external customers to establish acceptance criteria "", 'Attractive compensation and PTO policy', 'Tackling Exciting Challenges. ', 'Track and adequately report defects using appropriate tools', 'Responsibilities', 'Experience with conducting and running defect triage', ""Here's How We Work"", 'Communicate with internal and external customers to establish acceptance criteria ', 'Qualifications', 'Secondary Accountabilities', 'Overall 5-6 years of experience in Software Quality Assurance role including Data Management and Testing', 'Other Duties', 'Attractive compensation and PTO policyCompany pays 100% of your medical, dental and vision insurance, 80% for your familyShort and long term disability insurance Life insurance Company supports professional development for all team membersTop-notch tech/equipment/software you need to do your jobFree on-site gym access (San Diego office)', 'Ensure the intent of requirement is satisfied', 'Interested? Submit your resume and any supporting paperwork today! :)', ""Perform quality assurance and quality control duties for Wondersign's Data Management and data development life cycle"", 'Knowledge of statistical analysis techniques', 'Free on-site gym access (San Diego office)', 'Develop Validation & Verification plans against the requirement', 'Identify patterns and trends in data sets', 'Create and maintain test scripts to ensure data services and programs meet acceptance criteria', 'Recommend technical strategies to contribute to the overall effectiveness and quality', 'This position will require the following mental requirements; while performing the duties of this job, the employee is regularly exposed to high pressure to high-stress situations. Employee works in a typical office (or home office) environment and is occasionally exposed to moving mechanical office equipment. The noise level in the work environment is usually moderate. Some travel to job sites and/or offices is required. Must be able to travel and work extended schedule as needed.', 'Taking Ownership. ', 'Job Description', 'Additional Physical & Mental Requirements', 'Participate in manual testing and troubleshooting when required']",Associate,Full-time,Quality Assurance,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,"Aptima, Inc.","Dayton, OH",2 weeks ago,Be among the first 25 applicants,"['', 'Continue to develop and expand software knowledge with acquiring new skill sets and experience through the course of development', 'Software development in a team environment (peer reviews, unit testing, configuration management, defect tracking, etc.)Excellent project team and customer-focused written and oral communication skillsProven technical project leadership at individual and team levelProficiency in C#, JavaScript, Python, Angular, and other programming languagesExposure to statistical methods, especially those related to exploratory data analysisUser interface development for Desktop and/or Web applicationsFamiliarity with one or more relational (SQL Server) and/or NoSQL (Elasticsearch) data storage technologiesFamiliarity with one or more data analytics tools (e.g., Python, R, Matlab)Developing innovative dashboard, data analytics, and data telemetry displaysExperience working with unstructured data sets comprised of multiple disparate data sources', 'BA, BS, or equivalent in Computer Science or related degree preferred', 'Aptima, Inc. participates in the US Government E-Verify Program. For more information, click on\xa0http://www.uscis.gov/e-verify.\xa0', 'Coordinate with customers and internal stakeholders regarding requirements for transition and further scientific and technical development of the technologyMaintain high quality software development leveraging internal and industry standardsCoordinate software teams on projects leading architecture and design of software systemsDerive business knowledge from large data repositoriesContinue to develop and expand software knowledge with acquiring new skill sets and experience through the course of development', 'Maintain high quality software development leveraging internal and industry standards', 'Developing innovative dashboard, data analytics, and data telemetry displays', 'Who We Are', 'Experience working with unstructured data sets comprised of multiple disparate data sources', 'Working Conditions (ADA accommodations may be sought)', 'Familiarity with one or more data analytics tools (e.g., Python, R, Matlab)', 'U.S. Citizenship required or the ability to obtain a U.S. Security Clearance', 'Exposure to statistical methods, especially those related to exploratory data analysis', 'Derive business knowledge from large data repositories', 'Proficiency in C#, JavaScript, Python, Angular, and other programming languages', 'Secret Clearance needed', ':', '\xa0', ""For 25 years, Aptima's mission has been to improve and optimize performance in mission-critical, technology-intensive settings. We apply deep expertise in how humans think, learn, and perform to today's challenges. Whether for fighter pilots functioning in the cockpit, medical staff in the ICU, or teams collaborating across distributed networks, our solutions help measure, assess, inform, and augment human performance in defense, intel, aviation, law enforcement, and healthcare."", 'All applicants selected will be subject to a background investigation and must meet eligibility requirements for access to classified information.', ""As a Data Engineer, you will work within Aptima's Performance Assessment Technologies division and have an active role in leading development of tools for assessing and augmenting human performance within a fast-paced, collaborative team. You will work within the entire engineering lifecycle, including software requirements analysis, detailed design, implementation and test of individual software modules along with development and testing of software systems and software applications and will use these tools to derive insights from data.\xa0Other responsibilities include:"", 'Coordinate with customers and internal stakeholders regarding requirements for transition and further scientific and technical development of the technology', '10% travel possible', 'What You’ll Do', 'Aptima, Inc. is an equal opportunity employer dedicated to non-discrimination in employment. We select the most qualified individual for the job based on job-related qualifications regardless of race, color, age, sex, religion, national origin, disability, ancestry, marital status, credit history, sexual orientation, arrest and court record, genetic information, veteran status or any other status protected by federal, state or other applicable laws.', 'Minimum Requirements', 'Coordinate software teams on projects leading architecture and design of software systems', 'Excellent project team and customer-focused written and oral communication skills', 'Proven technical project leadership at individual and team level', 'Experience Needed', 'Software development in a team environment (peer reviews, unit testing, configuration management, defect tracking, etc.)', 'Familiarity with one or more relational (SQL Server) and/or NoSQL (Elasticsearch) data storage technologies', 'User interface development for Desktop and/or Web applications', 'EOE MINORITIES/FEMALES/PROTECTED VETERANS/DISABLED', '8+ years of professional experience in related technical field', 'U.S. Citizenship required or the ability to obtain a U.S. Security ClearanceSecret Clearance neededBA, BS, or equivalent in Computer Science or related degree preferred8+ years of professional experience in related technical field']",Mid-Senior level,Full-time,Business Development,Defense & Space,2021-03-24 13:05:10
Data Infrastructure Engineer,Urgently Roadside Assistance,United States,1 week ago,Be among the first 25 applicants,"['', 'Understand our platform development environment and philosophy.Understand our cloud architecture and applications’ infrastructure.Understand our engineering teams’ work culture.', 'Technical Expertise:\xa0', 'YOUR MISSION:\xa0Your mission is to manage and optimize our cloud data platform. This means you will be responsible for working on a variety of data projects which include orchestrating our data pipelines using modern big data tools/stack and engineering our existing transactional processing systems.', 'Build CI/CD pipelines to deploy and monitor data pipelines\xa0Architect and build data infrastructure not available off-the-shelf.Build deployment tools to provide blue/green and zero downtime for our servicesEnsure data flow and integrity between systems. Champion the flow of data across all our systems from end-to-end ensuring consistency across the chain.', 'Architect and build data infrastructure not available off-the-shelf.', 'Problem Solving: ', 'Experience deploying and maintaining\xa0data warehousing technologies such as Amazon Redshift and Google BigQuery', 'For more information, visit www.geturgently.com', 'Manager = You’ll report to the Technical Manager Platform Infrastructure', '\xa0The result? ', '4) Ongoing...\xa0', 'Experience deploying and maintaining EMR, Glue, Athena, RDS', 'Location = Great news!\xa0You have the option of working from anywhere in the U.S.!\xa0\xa0\xa0Manager = You’ll report to the Technical Manager Platform InfrastructureCompensation = Commensurate with experience for a company of Urgently’s sizeBenefits = At Urgently, we have awesome benefits!\xa0We cover 100% of the cost of your dental and vision plans and we also provide short term and long term disability and life insurance to you all free of charge!\xa0We have two medical plans - a base plan that has a Health Savings account add on and a PPO option (you do have to pay for these).\xa0You’ll have 12 holidays off and unlimited paid time off..\xa0We match 100% on the first 3% you contribute to our 401(k)and then 50% of the next 2% you contribute. So, if you contribute 5% of your paycheck, we’ll match 4% of that.\xa0Free money!', 'Applicants with disabilities may be entitled to reasonable accommodation under the Americans with Disabilities Act and similar applicable state laws. Please let us know if you need assistance completing any forms or to otherwise participate in the application process', 'Understand our cloud architecture and applications’ infrastructure.', 'Follow AWS best practices to deploy services\xa0', 'YOUR LEGACY:\xa0Your legacy will include ensuring that every Urgently team across our growing organization is able to access data from any source within minutes, none of our data is ever lost and build immutable data infrastructure.\xa0Your efforts will ensure that Urgently becomes the world’s leading mobility assistance platform.\xa0The result? \xa0You helped Urgently become the world’s leading mobility assistance company.\xa0\xa0', 'THE NITTY GRITTY:', 'You pride yourself on working collaboratively with all of your teammates.\xa0You are transparent in your communication and proactively share what others need to be aware of.', ""Bachelor's Degree in Computer Science or related field"", 'Team Member: \xa0', 'Monitor our operations and security\xa0', 'You have been involved in deploying data pipelines that involved collecting/streaming, storing and processing (ETL) the data for various business use cases.\xa0You have extensively worked on implementing Jenkins Pipelines\xa0You have\xa0been an integral part of a team working with structured, semi-structured and unstructured large data sets from real time/batch streaming data feeds.', 'Proficient in Python\xa0', 'Work with different teams to make data available for reporting and analytics.\xa0', '\xa0', ""At Urgently, we don’t just accept difference - we celebrate it, we support it, and we thrive on it for the benefit of our employees, our customers and the communities where we operate. We are an equal opportunity employer which means that employment and advancement at Urgently is based on a person's merit and qualifications.\xa0"", 'In depth and demonstrable knowledge of AWS cloud\xa0', 'Experience with deploying\xa0messaging and data pipeline tools like Apache Kafka, Amazon Kinesis etc.', 'We do not discriminate against any applicant or employee on the basis of race, ethnicity, color, national origin, ancestry, citizenship, religion, creed, sex (including pregnancy, childbirth, breastfeeding or related medical conditions), sexual orientation, gender, gender identity or expression, physical or mental disability, or any other characteristic protected by federal, state or local law.\xa0', 'Build CI/CD pipelines to deploy and monitor data pipelines\xa0', 'You are known for proactively solving problems before they can become real problems.\xa0You are the kind of person who is constantly upgrading your skill sets and is always looking for ways to enhance the data platforms you’re working on.\xa0\xa0\xa0', 'Knowledge of Airflow or other workflow management systems in a distributed setup', 'Fine tune our performance with a focus on high availability and scalability.', 'OUR FAIR HIRING PRACTICES:', 'You have\xa0been an integral part of a team working with structured, semi-structured and unstructured large data sets from real time/batch streaming data feeds.', 'Experience in writing\xa0Terraform/CloudFormation templates', 'Team Member: ', 'Understand our engineering teams’ work culture.', 'Benefits = At Urgently, we have awesome benefits!\xa0We cover 100% of the cost of your dental and vision plans and we also provide short term and long term disability and life insurance to you all free of charge!\xa0We have two medical plans - a base plan that has a Health Savings account add on and a PPO option (you do have to pay for these).\xa0You’ll have 12 holidays off and unlimited paid time off..\xa0We match 100% on the first 3% you contribute to our 401(k)and then 50% of the next 2% you contribute. So, if you contribute 5% of your paycheck, we’ll match 4% of that.\xa0Free money!', ""Bachelor's Degree in Computer Science or related fieldIn depth and demonstrable knowledge of AWS cloud\xa0Proficient in Jenkins CI/CD pipelines\xa0Experience deploying and maintaining EMR, Glue, Athena, RDSExperience deploying and maintaining\xa0data warehousing technologies such as Amazon Redshift and Google BigQueryExperience with deploying\xa0messaging and data pipeline tools like Apache Kafka, Amazon Kinesis etc.Proficient in Python\xa0Experience in writing\xa0Terraform/CloudFormation templatesExperience using and/or implementing modern observability tooling such as Prometheus, InfluxDB, Grafana, Logstash, Kibana or Jaeger.\xa0"", 'Experience using and/or implementing modern observability tooling such as Prometheus, InfluxDB, Grafana, Logstash, Kibana or Jaeger.\xa0', 'Build deployment tools to provide blue/green and zero downtime for our services', 'Understand our platform development environment and philosophy.', 'Industry Experience:\xa0', 'YOUR MISSION:\xa0', 'Nice to Haves:', '2) First 6 months:', 'You should also know that we adhere to these same principles in all aspects of employment, including, but not limited to, recruitment, hiring, job assignment, compensation, promotion, access to benefits and training, discipline, and termination of employment.\xa0', 'Ensure data flow and integrity between systems. Champion the flow of data across all our systems from end-to-end ensuring consistency across the chain.', 'You have been involved in deploying data pipelines that involved collecting/streaming, storing and processing (ETL) the data for various business use cases.\xa0', 'You have extensively worked on implementing Jenkins Pipelines\xa0', 'Compensation = Commensurate with experience for a company of Urgently’s size', '1) First 3 months:', 'Proficient in Jenkins CI/CD pipelines\xa0', 'YOUR LEGACY:\xa0', 'Monitor table schemas (i.e. partitions, compression, distribution) to minimize costs and achieve maximize performanceFine tune our performance with a focus on high availability and scalability.Work with different teams to make data available for reporting and analytics.\xa0Monitor our operations and security\xa0Follow AWS best practices to deploy services\xa0', 'Location = Great news!\xa0You have the option of working from anywhere in the U.S.!\xa0\xa0\xa0', 'WHO YOU ARE:', 'WHAT YOU’LL BE RESPONSIBLE FOR:', 'Monitor table schemas (i.e. partitions, compression, distribution) to minimize costs and achieve maximize performance']",Mid-Senior level,Full-time,Information Technology,Automotive,2021-03-24 13:05:10
Lead Data Engineer,goHUNT,"Las Vegas, NV",1 week ago,27 applicants,"['', 'Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.', 'Experience supporting and working with cross-functional teams in a dynamic environment.', 'Working knowledge of message queuing, stream processing, and highly scalable structured/relational data stores.', 'Hunters secure more tags and have more success because we put the best technology and gear into their hands. Our tech cuts through hunting’s complexities to uncover opportunities that most folks don’t even know exist, and we provide the expertise and gear needed to get it done in the field. That means more money for conservation and more support for wildlife management, preserving public lands for generations to come.', 'Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.', 'Build processes supporting data transformation, data structures, metadata, dependency and workload management.', 'Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.', 'Qualifications\xa0', 'Assemble large, complex data sets that meet functional / non-functional business requirements.', 'Create data tools for analytics and data science projects for building and optimizing our product into an innovative industry leader.', 'Create and maintain optimal data pipeline architecture.Assemble large, complex data sets that meet functional / non-functional business requirements.Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS Data technologies.Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.Work with stakeholders including the Executive, Product, GIS and Design teams to assist with data-related technical issues and support their data infrastructure needs.Create data tools for analytics and data science projects for building and optimizing our product into an innovative industry leader.Work with data and analytics experts to strive for greater functionality in our data systems.', 'About goHUNT', 'Experience with relational SQL and NoSQL databases, including MySQL Postgres and Dynamodb.', 'Work with stakeholders including the Executive, Product, GIS and Design teams to assist with data-related technical issues and support their data infrastructure needs.', 'Experience with big data tools: Hadoop/Spark, Kafka, etc.', 'We support conservation through hunting.', 'Job Overview', '\xa0', 'Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.Experience building and optimizing data pipelines, architectures and data sets.Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.Build processes supporting data transformation, data structures, metadata, dependency and workload management.A successful history of manipulating, processing and extracting value from large disconnected datasets.Working knowledge of message queuing, stream processing, and highly scalable structured/relational data stores.Strong project management and organizational skills.Experience supporting and working with cross-functional teams in a dynamic environment.We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools:Experience with relational SQL and NoSQL databases, including MySQL Postgres and Dynamodb.Experience with AWS data pipeline and workflow management tools.Experience with AWS cloud services: EC2, EMR, RDS,Experience with stream-processing systems: Storm, Spark-Streaming, etc.Experience with object-oriented/object function scripting languages: Python and Java.Experience with big data tools: Hadoop/Spark, Kafka, etc.', 'We are looking for a savvy Lead Data Engineer to join our growing team of experts. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will work closely with our software developers, GIS architects and data analysts on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives. The position is full-time and is based in our Las Vegas office reporting directly to the Chief Technology Officer.\xa0', 'Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS Data technologies.', 'We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools:', 'Strong project management and organizational skills.', 'Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.', 'A successful history of manipulating, processing and extracting value from large disconnected datasets.', 'Work with data and analytics experts to strive for greater functionality in our data systems.', 'Experience with stream-processing systems: Storm, Spark-Streaming, etc.', 'Experience building and optimizing data pipelines, architectures and data sets.', 'Vision & Mission', 'Experience with AWS cloud services: EC2, EMR, RDS,', 'Experience with object-oriented/object function scripting languages: Python and Java.', 'Responsibilities for Data Engineer', ""goHUNT is a rapidly growing company that puts the best technology and gear into the hands of hunters and anyone who loves the outdoors. We've taken over the Western big game hunting industry by giving ethical, fair-chase hunters better tools to pursue their passion. We take pride in staying mission-focused while still enjoying ourselves and offering a world-class customer experience. If you’re someone with grit and integrity, we’d love to have you on our team."", 'Create and maintain optimal data pipeline architecture.', 'Experience with AWS data pipeline and workflow management tools.']",Associate,Full-time,Information Technology,Internet,2021-03-24 13:05:10
Data Engineer (-SK),Intel Corporation,"Folsom, CA",2 weeks ago,45 applicants,"['', ""Help us build out an optimal data infrastructure from the ground up. Currently, the team's primary data infrastructure is comprised of SQL servers."", 'Minimum 1+ years of experience in data engineering.', 'Programming experience, ideally in SQL, Python, Spark, Kafka or Java, and a willingness to learn new programming languages to meet goals and objectives.', 'Data Engineer Roles And Responsibilities', 'Proficient understanding of distributed computing principles.', ""This position is associated with the sale of Intel's NAND memory and storage business to SK hynix"", 'Other Locations', 'Support business decisions with ad hoc analysis as needed.', 'A solid track record of data management showing your flawless execution and attention to detail.Programming experience, ideally in SQL, Python, Spark, Kafka or Java, and a willingness to learn new programming languages to meet goals and objectives.Knowledge of data cleaning, wrangling, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks.Knowledge of data mining, machine learning, natural language processing, or information retrieval.Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources.Explore new alternatives or options to solve data mining issues, and utilize a combination of industry best practices, data innovations and your experience to get the job done.Experience in production support and troubleshooting.A passion for documenting work and sharing knowledge systematically', ""Help us build out an optimal data infrastructure from the ground up. Currently, the team's primary data infrastructure is comprised of SQL servers.Work closely with BI Experts, Data Scientists, and Data Governance Analysts to ensure infrastructure meets all needs.Proficient understanding of distributed computing principles.Experience with NoSQL and SQL databases, such as HBase, Cassandra, MongoDB, MySQL, Oracle, PostgreSQL, etc.Design, build, manage and optimize data pipelines for data structures encompassing data transformation, data models, schemas, metadata, data quality, and workload management.Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, web scraping, calling APIs, write SQL queries, etc.Process unstructured data into a form suitable for analysis.Support business decisions with ad hoc analysis as needed.Monitoring data performance and modifying infrastructure as needed.Define data retention policies. Participate in standups and design reviews.Document and share knowledge through the creation of comprehensive standards.Breakdown business features into technical stories and approaches.Create proof of concepts and prototypes."", 'Experience with NoSQL and SQL databases, such as HBase, Cassandra, MongoDB, MySQL, Oracle, PostgreSQL, etc.', 'Create proof of concepts and prototypes.', 'A passion for documenting work and sharing knowledge systematically', 'Breakdown business features into technical stories and approaches.', 'Work closely with BI Experts, Data Scientists, and Data Governance Analysts to ensure infrastructure meets all needs.', 'Explore new alternatives or options to solve data mining issues, and utilize a combination of industry best practices, data innovations and your experience to get the job done.', 'A solid track record of data management showing your flawless execution and attention to detail.', 'Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, web scraping, calling APIs, write SQL queries, etc.', 'Process unstructured data into a form suitable for analysis.', 'Define data retention policies. Participate in standups and design reviews.', ""Bachelor's Degree or more in Computer Science or a related field."", 'Experience in production support and troubleshooting.', 'Inside this Business Group', ""Bachelor's Degree or more in Computer Science or a related field.Minimum 1+ years of experience in data engineering."", 'Qualifications', 'Posting Statement', 'Minimum Requirements', 'Document and share knowledge through the creation of comprehensive standards.', 'Monitoring data performance and modifying infrastructure as needed.', 'Knowledge of data cleaning, wrangling, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks.', 'Design, build, manage and optimize data pipelines for data structures encompassing data transformation, data models, schemas, metadata, data quality, and workload management.', 'Knowledge of data mining, machine learning, natural language processing, or information retrieval.', 'Preferred Experience And Skill', 'Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources.', 'Job Description']",Entry level,Full-time,Information Technology,Electrical/Electronic Manufacturing,2021-03-24 13:05:10
Data Engineer – Information Security,Irvine Technology Corporation,"Burbank, CA",2 days ago,40 applicants,"['', 'Please send your resume to\xa0Hannah Xu, Technical Recruiter ', 'Experience applying information security components, principles, practices and procedures', 'Irvine Technology Corporation (ITC)\xa0', 'Proven knowledge of data center infrastructure technologies: Windows and Linux OS, clustering, data storage, middleware, monitoring technology', 'Irvine Technology Corporation (ITC)\xa0is a leading provider of technology and staffing solutions for IT, Security, Engineering, and Interactive Design disciplines servicing startups to enterprise clients, nationally. We pride ourselves in the ability to introduce you to our intimate network of business and technology leaders – bringing you opportunity coupled with personal growth, and professional development!\xa0\xa0Join us. Let us catapult your career!', 'Perform data analysis and implement security input into data warehousePerform project mapping, data modeling, Python statistical programming and translating output into elastic searchProduce and review process and procedural documentation, including knowledgebase articles, workflows, and overview presentations', 'Irvine Technology Corporation provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics. In addition to federal law requirements, Irvine Technology Corporation complies with applicable state and local laws governing non-discrimination in employment in every location in which the company has facilities.', 'Extensive experience in SQL, T-SQL with advanced analytic SQL functions on MS SQL Server database platform', 'Produce and review process and procedural documentation, including knowledgebase articles, workflows, and overview presentations', 'Qualifications:', 'ABOUT US', 'IMMEDIATE NEED for a Data Engineer – Information Security ', 'Join us. Let us catapult your career!', 'IMMEDIATE NEED for a Data Engineer – Information Security to join an entertainment company for a long-term contract position that can sit at ether their Burbank, Orlando or Seattle office. This position is remote capable with occasional onsite days for special projects. The ideal candidate will have experience applying information security to data systems. ', '\xa0', 'Experience in production environment and enterprise level environment', 'Data Engineer – Information Security', 'Responsibilities:\xa0', 'One or more InfoSec certifications, such as CISSP or GIAC', 'Let us help you secure an interview!', 'Responsibilities:\xa0\xa0', 'Perform data analysis and implement security input into data warehouse', 'Knowledge of Amazon RDS, public hosting service providers, cloud security', 'Extensive experience in SQL, T-SQL with advanced analytic SQL functions on MS SQL Server database platformExperience applying information security components, principles, practices and proceduresProven knowledge of data center infrastructure technologies: Windows and Linux OS, clustering, data storage, middleware, monitoring technologyKnowledge of Amazon RDS, public hosting service providers, cloud securityExperience in production environment and enterprise level environmentOne or more InfoSec certifications, such as CISSP or GIAC', 'Please send your resume to\xa0Hannah Xu, Technical Recruiter for immediate consideration. Let us help you secure an interview!', 'Perform project mapping, data modeling, Python statistical programming and translating output into elastic search']",Mid-Senior level,Contract,Information Technology,Entertainment,2021-03-24 13:05:10
Data Engineer,"InvestEdge, Inc.",United States,2 weeks ago,Be among the first 25 applicants,"['', 'T-SQL (DDL, Stored Process, Views, CTEs, etc.', 'To apply send resume and cover letter to careers@investedge.com with the job title in the subject line.\xa0', 'Qualifications: ', 'Serve as a subject matter expert on a large in-house enterprise database model.', 'T-SQL (DDL, Stored Process, Views, CTEs, etc.VCS (Git, SVN, etc.)', 'Responsibilities:', 'Additional Skills that are highly favorable for this position ', 'Overview', 'InvestEdge is a leading provider of innovative advisor solutions to financial institutions.', 'Company Profile:', 'Qualifications:', ""\xa0The candidate will work with InvestEdge's senior data architect to implement new solutions as well as improve existing ones. They should be comfortable working with large data sets and large database footprints and should understand concepts such as performance tuning, SQL Injection, and data security best practices."", 'VCS (Git, SVN, etc.)', '\xa0\xa0\xa0Additional Skills that are highly favorable for this position ', 'Debug and troubleshoot performance issues in database code.', '\xa0', 'Location: Remote/Work from Home', 'Candidates with a CPA/CFA or other financial services background are considered the ideal candidate.\xa0Candidates who understand web technologies and can program in other languages in addition to SQL will be preferred. JavaScript/ES6/NodeJS preferred.VS Code, SSMS, and other IDEs.CI/CD experience with integrating database changes into deployment models.', 'CI/CD experience with integrating database changes into deployment models.', 'Our integrated solutions provide a breadth of tools that simplify complicated wealth management processes and reduce overall operational risk. Using the integrated solutions automates key front office functions like easy-to-use portfolio management, trading/rebalancing, performance measurement, reporting, compliance/fiduciary monitoring, and data aggregation tools.', 'Candidates who understand web technologies and can program in other languages in addition to SQL will be preferred. JavaScript/ES6/NodeJS preferred.', 'Work with the senior data architect to implement data routines in a financial services environment.', 'Location:', 'VS Code, SSMS, and other IDEs.', 'Candidates with a CPA/CFA or other financial services background are considered the ideal candidate.\xa0', 'Understand the business domain of the application.', 'Create new queries, views, and stored procedures for a large existing relational data set.', 'Work with the senior data architect to implement data routines in a financial services environment.Create new queries, views, and stored procedures for a large existing relational data set.Debug and troubleshoot logical issues in database code.Debug and troubleshoot performance issues in database code.Serve as a subject matter expert on a large in-house enterprise database model.Understand the business domain of the application.Work in an Agile environment on a cross-functional team.', 'Debug and troubleshoot logical issues in database code.', 'InvestEdge is seeking a database specialist with expert knowledge in relational data modeling, querying, and data analysis. The candidate should have expert knowledge of working with MSSQL and Postgres databases, and can write complex queries, stored procedures, and views. This person has likely worked in a senior data developer or data architect role, and also understands database administration concepts such as indexing strategies, backup and fault-tolerance strategies, and how to organize and secure data at rest. An ideal candidate would also have a background in financial services and understand how financial markets work. A CPA/CFA with the ability to write advanced SQL should be a shoe-in.', 'Position Summary', 'Work in an Agile environment on a cross-functional team.']",Entry level,Full-time,Information Technology,Financial Services,2021-03-24 13:05:10
Data Engineer,Guild Mortgage,"San Diego, CA",1 week ago,Be among the first 25 applicants,"['', 'Audio/Visual: Ability to accurately interpret sounds and associated meanings at a volume consistent with interpersonal conversation. Regularly required to accurately perceive, distinguish and interpret information received visually and through audio; e.g., words, numbers and other data broadcasted aloud/viewed on a screen, as well as print and other media.', 'Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.', 'Keep our Personally Identifiable Information (PII) data separated and secured through table splitting, data masking, access control restrictions and redundancy (high availability across multiple zones).', 'Focus on the continual improvement of policies, procedures, and processes falling under scope of authority.', 'Position Summary', 'Proven experience with querying and analyzing data from SQL relational databases such as AuroraDB, IBM DB2, MemSQL, MS SQL Server, MySQL, Postgres, Redshift or Snowflake, and NoSQL Databases such as Cassandra, DynamoDB, Redis, or MongoDB', 'Guild offers a pleasant work environment, competitive compensation and excellent benefits package; including medical, dental, vision, life insurance, AD&D, LTD and 401(k) with employer match.', 'Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.', 'Ability to think critically, including the ability to evaluate facts and data to draw conclusions, determine the downstream impact of decisions and associated risks.', 'Create data tools for data scientist and analytics team members that assist them in building and optimizing data analysis and reporting.', 'Strong knowledge of Kafka, Sparks, ESB and/or other messaging queues, real-time data integration & stream processing technologies.', 'Problem solver with an ability to work as a team towards a solution.', 'Guild Mortgage Company, closing loans and opening doors since 1960. As a mortgage banking firm we are dedicated to serving the home owner/buyer. Our goal is to provide affordable home financing for our customers, utilizing the best terms available while providing a level of professionalism and service unsurpassed in the lending industry.', 'Highly proficient in building and optimizing Big Data pipelines, architecture and data sets.', 'Physical: ', 'Perform other duties as assigned.', 'Self-starter with the demonstrated ability to learn/adapt to new technologies and techniques.', 'Experience with data science languages such as R, and Python as well as general purpose languages such as Java, Scala, C# or JavaScript.', 'Assemble large, complex data sets that meet functional / non-functional business requirements.', 'Ethical, with a commitment to company values.', 'Strong detail orientation and highly organized with proven ability to lead effectively and drive results in a matrixed management environment.', 'Proven experience with data lake and warehouse best practices and leading products in the marketplace.', 'Strong data governance skills to ensure highest quality of data is made available for data analysis and accuracy in reporting.', ""Bachelor's degree, BS in Statistics, Computer Science, Data Science or related quantitative field is required, along with a minimum of five years’ experience in Data Engineer related role(s) and at least three of those years spent in a senior technical level role(s) required.Highly proficient in building and optimizing Big Data pipelines, architecture and data sets.Highly proficient with data loading, processing and data warehouse design techniques: star or snowflake schema designs, etc.Proven experience with data lake and warehouse best practices and leading products in the marketplace.Expert at creating data integrations using Extract, Transform and Load (ETL) tools and modern data pipes.Strong knowledge of Kafka, Sparks, ESB and/or other messaging queues, real-time data integration & stream processing technologies.Proven experience with querying and analyzing data from SQL relational databases such as AuroraDB, IBM DB2, MemSQL, MS SQL Server, MySQL, Postgres, Redshift or Snowflake, and NoSQL Databases such as Cassandra, DynamoDB, Redis, or MongoDBExperience with data science languages such as R, and Python as well as general purpose languages such as Java, Scala, C# or JavaScript.Experience collecting structured, semi-structured and unstructured data in various popular formats and sourced from internal core systems as well as 3rd\xa0partner providers such as Google Analytics, Facebook Insights, Zillow, CoreLogic, MLS Data, Public Records, and Property DataExperience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.Strong analytical skills to build processes supporting data transformation, data structures, metadata, dependency and workload management.Strong data governance skills to ensure highest quality of data is made available for data analysis and accuracy in reporting.Problem solver with an ability to work as a team towards a solution.Ability to prioritize multiple tasks in a deadline-driven environment, strong sense of urgency and responsiveness.Strong detail orientation and highly organized with proven ability to lead effectively and drive results in a matrixed management environment.Ability to think critically, including the ability to evaluate facts and data to draw conclusions, determine the downstream impact of decisions and associated risks.Excellent verbal and written communication skills plus demonstrate strong leadership capabilities.Strong interpersonal and team building skills.Self-starter with the demonstrated ability to learn/adapt to new technologies and techniques.Ethical, with a commitment to company values."", 'Requirements', 'The Data Engineer is a key technical position and plays an important role in the organization by performing a number of activities related to the company’s Information Technology functions.\xa0The role is primarily responsible, under limited supervision, for executing the data strategy and analysis efforts within the company’s enterprise data platform by expanding and optimizing data and data pipeline architecture, as well as optimizing data flow and collection from cross functional areas. Responsibilities include supporting software developer, database architects, data analysts and data scientists on various data initiatives as well as ensuring optimal data delivery architecture is maintained as needs grow.', 'Strong analytical skills to build processes supporting data transformation, data structures, metadata, dependency and workload management.', 'Manual Dexterity:', '\xa0', 'Stay abreast of latest technology trends and participate in high-level decisions impacting the direction of Information Technology function.', 'Strong interpersonal and team building skills.', 'Physical: Work is primarily sedentary; mobility in an office setting.', 'Drive strong communications, partnerships, and stakeholder management with senior leaders, functional managers and staff.', 'Partner with key stakeholders, including the business unit leaders, Product, Data & Technology teams, to assist with their data-related technical issues and support their data infrastructure needs by collecting data from primary sources and optimizing the data architecture, improving quality of sourced data, and ensuring consistent delivery of data to key stakeholders in a timely manner.', 'Highly proficient with data loading, processing and data warehouse design techniques: star or snowflake schema designs, etc.', 'Partner with key stakeholders, including the business unit leaders, Product, Data & Technology teams, to assist with their data-related technical issues and support their data infrastructure needs by collecting data from primary sources and optimizing the data architecture, improving quality of sourced data, and ensuring consistent delivery of data to key stakeholders in a timely manner.Extract, Load & Transform (ELT) data from various primary data sources (internal & 3rd\xa0party) into the data lake to create optimal data pipeline architecture.Assemble large, complex data sets that meet functional / non-functional business requirements.Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and leading ELT tools.Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.Keep our Personally Identifiable Information (PII) data separated and secured through table splitting, data masking, access control restrictions and redundancy (high availability across multiple zones).Create data tools for data scientist and analytics team members that assist them in building and optimizing data analysis and reporting.Improve the quality of data used for analysis by assessing the accuracy of new data sources and the effectiveness of new data gathering techniques.Develop processes and tools to monitor and analyze data warehouse performance and data accuracy.Contribute to the team of data professionals by using expertise to answer questions and sharing repeatable design patterns with less experienced teammates, enhancing skillsets and competencies of team members, and sharing technical knowledge throughout the team.Participate in stakeholder reviews, and design sessions.Provide data, reports, and information to management as needed.Identify, track, and monitor trends and avoidable technology-related errors; work across functions to develop complex solutions, improvements, and stop-gaps.Focus on the continual improvement of policies, procedures, and processes falling under scope of authority.Use expertise to resolve high level issues that cannot be solved by teammates.Stay abreast of latest technology trends and participate in high-level decisions impacting the direction of Information Technology function.Consistently monitor and model platform usage, database sizes, compute resources and third-party costs to ensure that the data team’s spend is as cost-efficient as possible.Champion data transformation to new ways of working and generating insights.Drive strong communications, partnerships, and stakeholder management with senior leaders, functional managers and staff.Perform other duties as assigned.', 'Identify, track, and monitor trends and avoidable technology-related errors; work across functions to develop complex solutions, improvements, and stop-gaps.', 'Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and leading ELT tools.', 'Manual Dexterity:\xa0Frequent use of computer keyboard and mouse.', 'Develop processes and tools to monitor and analyze data warehouse performance and data accuracy.', 'Expert at creating data integrations using Extract, Transform and Load (ETL) tools and modern data pipes.', 'Environmental:\xa0Office environment – no substantial exposure to adverse environmental conditions.', 'Contribute to the team of data professionals by using expertise to answer questions and sharing repeatable design patterns with less experienced teammates, enhancing skillsets and competencies of team members, and sharing technical knowledge throughout the team.', 'Qualifications', 'Environmental:\xa0', 'Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.', 'Champion data transformation to new ways of working and generating insights.', 'Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.', 'Use expertise to resolve high level issues that cannot be solved by teammates.', ""Bachelor's degree, BS in Statistics, Computer Science, Data Science or related quantitative field is required, along with a minimum of five years’ experience in Data Engineer related role(s) and at least three of those years spent in a senior technical level role(s) required."", 'Participate in stakeholder reviews, and design sessions.', 'Extract, Load & Transform (ELT) data from various primary data sources (internal & 3rd\xa0party) into the data lake to create optimal data pipeline architecture.', 'Travel: Infrequent based on company events and/or relevant conferences or training', 'Ability to prioritize multiple tasks in a deadline-driven environment, strong sense of urgency and responsiveness.', 'Audio/Visual:', 'Travel:', 'Essential Functions', 'Provide data, reports, and information to management as needed.', 'Experience collecting structured, semi-structured and unstructured data in various popular formats and sourced from internal core systems as well as 3rd\xa0partner providers such as Google Analytics, Facebook Insights, Zillow, CoreLogic, MLS Data, Public Records, and Property Data', 'Consistently monitor and model platform usage, database sizes, compute resources and third-party costs to ensure that the data team’s spend is as cost-efficient as possible.', 'Guild Mortgage Company', 'Guild Mortgage Company is an Equal Opportunity Employer.', 'Excellent verbal and written communication skills plus demonstrate strong leadership capabilities.', 'Improve the quality of data used for analysis by assessing the accuracy of new data sources and the effectiveness of new data gathering techniques.']",Mid-Senior level,Full-time,Information Technology,Financial Services,2021-03-24 13:05:10
Data Engineer,A Cloud Guru,"Austin, TX",4 weeks ago,25 applicants,"['', ' 4 weeks PTO, plus 10 sick days, and holidays. Because even when your office is your living room, we all need time to unplug.   Remotely awesome. Get $500 to level up your home office, monthly snack boxes, free Headspace access, weekly lunch funds, and $50 monthly for internet.   Human connection. Get to know the Gurus with good times and get-togethers inspired by our values, virtual happy hours, lunchtime trivia, or a socially distanced drive-in movie.   Gender-neutral paid parental leave. Expanding your family? We offer 12 weeks of gender-neutral paid parental leave, and reimburse up to $10,000 for eligible adoption expenses.   $1,000 continuing education budget. All Gurus get $250 a quarter to spend on personal development, and 2 hours each week reserved for learning something new.  ', ' 2+ years of development experience with Python or similar scripting language ', ' Explore and contribute to discussions around technologies under consideration, such as Snowflake, Kappa/Lambda architecture, Delta Lakes and Data Vault ', 'to teach the world to cloud. ', ' Knowledge of software engineering best practices with experience with implementing CI/CD, monitoring & alerting for production systems ', ' Coach and mentor other team members ', ' Collaborate with the Analytics team on transformation processes to populate data models ', 'What makes the Engineering team awesome...', ' Remotely awesome. Get $500 to level up your home office, monthly snack boxes, free Headspace access, weekly lunch funds, and $50 monthly for internet. ', ' Experience with complex data structures and No-SQL databases ', ' $1,000 continuing education budget. All Gurus get $250 a quarter to spend on personal development, and 2 hours each week reserved for learning something new. ', ' Gender-neutral paid parental leave. Expanding your family? We offer 12 weeks of gender-neutral paid parental leave, and reimburse up to $10,000 for eligible adoption expenses. ', ' Experience with ETL development, metadata management, and data quality ', ""Hello, we're A Cloud Guru"", ' Experience with open source orchestration platforms (e.g. Airflow) ', ' Be an essential part of designing and building ACG’s new data platform, as we evolve the existing databases into a cutting-edge solution to meet the needs of our 2021 data plans and beyond ', ' Discover opportunities for data acquisition and explore new ways of using existing data ', 'Remotely awesome.', 'What you bring to the table', ' Recommend ways to improve data reliability, efficiency and quality of the data platform and optimise for performance, scalability and cost ', '  Be an essential part of designing and building ACG’s new data platform, as we evolve the existing databases into a cutting-edge solution to meet the needs of our 2021 data plans and beyond   Explore and contribute to discussions around technologies under consideration, such as Snowflake, Kappa/Lambda architecture, Delta Lakes and Data Vault   Develop, test and maintain existing architecture, including databases, data pipelines and large-scale processing systems   Collaborate with the Analytics team on transformation processes to populate data models   Recommend ways to improve data reliability, efficiency and quality of the data platform and optimise for performance, scalability and cost   Discover opportunities for data acquisition and explore new ways of using existing data   Identify gaps in data processes and drive improvements   Coach and mentor other team members  ', 'Human connection.', '  2+ years of Data Engineering, Data Warehousing, or related experience   2+ years of development experience with Python or similar scripting language   2+ years of SQL experience, including experience with schema design and dimensional data modelling   Experience working with AWS services such as DynamoDB, Glue, Lambda, Step Functions, S3, CloudFormation or Redshift   Experience with ETL development, metadata management, and data quality   Knowledge of software engineering best practices with experience with implementing CI/CD, monitoring & alerting for production systems   Experience with complex data structures and No-SQL databases   Experience with open source orchestration platforms (e.g. Airflow)  ', ' Identify gaps in data processes and drive improvements ', ' Experience working with AWS services such as DynamoDB, Glue, Lambda, Step Functions, S3, CloudFormation or Redshift ', 'As a Data Engineer at ACG, you’ll get to:', ' Develop, test and maintain existing architecture, including databases, data pipelines and large-scale processing systems ', '4 weeks PTO, plus 10 sick days, and holidays.', 'The Data Engineer role ', ' 2+ years of SQL experience, including experience with schema design and dimensional data modelling ', ' Human connection. Get to know the Gurus with good times and get-togethers inspired by our values, virtual happy hours, lunchtime trivia, or a socially distanced drive-in movie. ', 'What’s the interview process like at ACG?', ' 2+ years of Data Engineering, Data Warehousing, or related experience ', '$1,000 continuing education budget.', '4 weeks PTO, plus 10 sick days, and holidays. Because even when your office is your living room, we all need time to unplug. ', 'Gender-neutral paid parental leave.', 'More than a job']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Ingest Engineer,Jacobs,"Hanover, MD",3 weeks ago,Be among the first 25 applicants,"['', 'Communicate with stakeholders internal and external to the agency in a professional manner via email, telephone, email, VTC, or face-to-face. Participate in technical exchange meetings at local sites or at customer sites in MD, DC, and VA; document meetings, identify and assign actions, and follow up and follow through on all actions.', 'Familiarity with agency technologies directly or related to the IC-GovCloud environment, which includes NiFi/EDA/DDS, the GHOSTMACHINE/Analytic Environment and the Data Warehouse platforms (Content Warehouse (CWH) and Event Warehouse (EWH)), RKS, and associated APIs, Copilot, TheCatalog, Trusted Data Format (TDF), ASDF, ABAC, and Agency tools available for IC access, such as DataXplorer and ICON, GATEKEEPER, CASPORT/AccessIT!, and MWS Registry.', 'Be solution-oriented, a self-starter, and have excellent verbal and written communications skills.', 'Partner with IC data owners to use the Data Services Wizard (DSW) tool to complete an IC Ingest Request (similar to DMR).', 'Other Essential Functions', 'Contribute to team initiatives which include sprint activities using Jira, Confluence revisions for team pages, marketing, training materials, and answering actions and data calls from the customer or organizational leadership.', 'Interact with IC data owners, end users, analysts, and technical SMEs regarding the identification and characterization of their data and help to determine mission use cases and how data will be used within the IC-GovCloud environment.', ' Work with multiple stakeholders across the IC to ingest their data into the IC-GovCloud and manage end-to-end data ingest initiatives. Interact with IC data owners, end users, analysts, and technical SMEs regarding the identification and characterization of their data and help to determine mission use cases and how data will be used within the IC-GovCloud environment. Examine customer-provided data samples to understand format, validate content to ensure viability for ingest into the IC-GovCloud environment. Explore mission use of IC data with the data provider as well a reaching out to partners to assess data usefulness and data fusion possibilities. Partner with IC data owners to use the Data Services Wizard (DSW) tool to complete an IC Ingest Request (similar to DMR). Communicate with stakeholders internal and external to the agency in a professional manner via email, telephone, email, VTC, or face-to-face. Participate in technical exchange meetings at local sites or at customer sites in MD, DC, and VA; document meetings, identify and assign actions, and follow up and follow through on all actions. Contribute to team initiatives which include sprint activities using Jira, Confluence revisions for team pages, marketing, training materials, and answering actions and data calls from the customer or organizational leadership. ', 'Explore mission use of IC data with the data provider as well a reaching out to partners to assess data usefulness and data fusion possibilities.', 'Your mission is our mission.', 'You must have an active TS/SCI with a polygraph', 'Ability to understand multi-faceted processes involving multiple systems and services, and personnel.', 'Your Impact', 'Examine customer-provided data samples to understand format, validate content to ensure viability for ingest into the IC-GovCloud environment.', 'Work with multiple stakeholders across the IC to ingest their data into the IC-GovCloud and manage end-to-end data ingest initiatives.', 'Ability to conduct themselves professionally with the team, customers, and internal and external agency partners, and to clearly communicate with the customer, stakeholders, and system or service providers.', 'Solid systems engineering background in architecture/dataflow, developing strategies and solutions to customer data flow problems, and working multiple initiatives at once.', 'Essential Functions', 'Here’s What You’ll Need', 'Minimum 12 years of Systems Engineering/Technical role and a Bachelor’s degree is required or 20 years in a technical role or 26 years total without a degree ', ' You must have an active TS/SCI with a polygraph Minimum 12 years of Systems Engineering/Technical role and a Bachelor’s degree is required or 20 years in a technical role or 26 years total without a degree  Solid systems engineering background in architecture/dataflow, developing strategies and solutions to customer data flow problems, and working multiple initiatives at once. Ability to understand multi-faceted processes involving multiple systems and services, and personnel. Familiarity with agency technologies directly or related to the IC-GovCloud environment, which includes NiFi/EDA/DDS, the GHOSTMACHINE/Analytic Environment and the Data Warehouse platforms (Content Warehouse (CWH) and Event Warehouse (EWH)), RKS, and associated APIs, Copilot, TheCatalog, Trusted Data Format (TDF), ASDF, ABAC, and Agency tools available for IC access, such as DataXplorer and ICON, GATEKEEPER, CASPORT/AccessIT!, and MWS Registry. Be solution-oriented, a self-starter, and have excellent verbal and written communications skills. Ability to conduct themselves professionally with the team, customers, and internal and external agency partners, and to clearly communicate with the customer, stakeholders, and system or service providers.']",Not Applicable,Full-time,Information Technology,Civil Engineering,2021-03-24 13:05:10
Data Engineer,Dunhill Professional Search,"United, LA",4 weeks ago,Be among the first 25 applicants,"['', 'Minimum Qualifications', 'Other Job Specific Skills', 'Qualifications']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Senior Data Engineer,Freedom Financial Network,"Tempe, AZ",19 hours ago,Be among the first 25 applicants,"['', 'get better', 'Get Better (every day): ', 'Event-driven architecture and messaging frameworks (Pub/Sub, Kafka, and/or RabbitMQ)', '9 paid holidays & 5 sick days', 'Work with analytics, data science, and wider engineering teams to help with automating data analysis and visualization needs, advise on transformation processes to populate data models, and explore ways to design and develop data infrastructure', 'Build a scalable technology platform to support a growing business', '10 years’ experience building large scale, real-time distributed systems with solid coding, problem-solving, and design skillsExperience working in highly collaborative teamsAbility to demonstrate a team-first attitude towards software developmentHands on Experience with Python, Java, SQLExperience working with various data stores: SQL, NoSQL, and/or key-value storeMature engineering practices (CI/CD, testing, secure coding, etc)Experience with, or willingness to learn:', 'Deliver high-quality code to production, consistently', 'COLLABORATE (with everybody): We strive to work together toward a common purpose by proactively sharing information and inviting participation. We recognize the perspective of various groups and embrace a healthy, constructive debate.', 'Who We Are', 'Paid time off for volunteer work and on your birthday', 'COLLABORATE (with everybody): ', 'Cloud infrastructure (Google Cloud Platform, AWS, and/or Azure)', 'Experience working with various data stores: SQL, NoSQL, and/or key-value store', 'Ability to demonstrate a team-first attitude towards software development', 'Experience with, or willingness to learn:', '10 years’ experience building large scale, real-time distributed systems with solid coding, problem-solving, and design skills', 'Fast, continued growth – there’s a lot of opportunity for advancement', 'Lead product requirements and advanced analytics requirements gathering efforts, interfacing with Data Engineering, Data Scientists, Analytics, and stakeholder teams ', 'The Opportunity', 'Bachelor’s in Computer Science or related field, or relevant experience in software development', 'Event-driven architecture and messaging frameworks (Pub/Sub, Kafka, and/or RabbitMQ)Cloud infrastructure (Google Cloud Platform, AWS, and/or Azure)', 'Requirements/Characteristics', 'Care (for everyone): We show compassion and contribute to the well-being and growth of those around us. We only pursue products that improve the financial lives of our clients.', 'Experience working in highly collaborative teams', 'Hands on Experience with Python, Java, SQL', 'Education', 'Care (for everyone): ', 'Care (for everyone): We show compassion and contribute to the well-being and growth of those around us. We only pursue products that improve the financial lives of our clients.Act with Integrity (every time): We take the right action even when it is hard and even when no one is watching. We treat our employees, clients, and communities the way they wish to be treated.Get Better (every day): We innovate, iterate, and improve each day. We are creative, take thoughtful risks, and ultimately learn and recover from failures.COLLABORATE (with everybody): We strive to work together toward a common purpose by proactively sharing information and inviting participation. We recognize the perspective of various groups and embrace a healthy, constructive debate.', 'Benefits start within 30 days', 'Description', 'Act with Integrity (every time): ', 'Use technology to solve business problems and drive positive outcomes', 'Voted the Best Place to Work multiple times by our employees, most recently #1 in Phoenix for the 2nd year in a row!', 'care', 'Act with Integrity (every time): We take the right action even when it is hard and even when no one is watching. We treat our employees, clients, and communities the way they wish to be treated.', 'Lead product requirements and advanced analytics requirements gathering efforts, interfacing with Data Engineering, Data Scientists, Analytics, and stakeholder teams Work with analytics, data science, and wider engineering teams to help with automating data analysis and visualization needs, advise on transformation processes to populate data models, and explore ways to design and develop data infrastructureUse technology to solve business problems and drive positive outcomesBuild a scalable technology platform to support a growing businessDeliver high-quality code to production, consistentlyIntegration with various third-party systemsPlay a key role in building out a large scale distributed and event based Data Platform that serves as an underpinning for all of Freedom’s businesses and productsRequirements/Characteristics10 years’ experience building large scale, real-time distributed systems with solid coding, problem-solving, and design skillsExperience working in highly collaborative teamsAbility to demonstrate a team-first attitude towards software developmentHands on Experience with Python, Java, SQLExperience working with various data stores: SQL, NoSQL, and/or key-value storeMature engineering practices (CI/CD, testing, secure coding, etc)Experience with, or willingness to learn:Event-driven architecture and messaging frameworks (Pub/Sub, Kafka, and/or RabbitMQ)Cloud infrastructure (Google Cloud Platform, AWS, and/or Azure)EducationBachelor’s in Computer Science or related field, or relevant experience in software developmentCULTURAL FIT (Our Core Values)Care (for everyone): We show compassion and contribute to the well-being and growth of those around us. We only pursue products that improve the financial lives of our clients.Act with Integrity (every time): We take the right action even when it is hard and even when no one is watching. We treat our employees, clients, and communities the way they wish to be treated.Get Better (every day): We innovate, iterate, and improve each day. We are creative, take thoughtful risks, and ultimately learn and recover from failures.COLLABORATE (with everybody): We strive to work together toward a common purpose by proactively sharing information and inviting participation. We recognize the perspective of various groups and embrace a healthy, constructive debate.WHY JOIN THE FREEDOM FAMILY?Fast, continued growth – there’s a lot of opportunity for advancementVoted the Best Place to Work multiple times by our employees, most recently #1 in Phoenix for the 2nd year in a row!Benefits start within 30 days401k with employer match3 weeks’ paid vacation (increased with tenure)9 paid holidays & 5 sick daysPaid time off for volunteer work and on your birthday', 'Integration with various third-party systems', '401k with employer match', 'Fast, continued growth – there’s a lot of opportunity for advancementVoted the Best Place to Work multiple times by our employees, most recently #1 in Phoenix for the 2nd year in a row!Benefits start within 30 days401k with employer match3 weeks’ paid vacation (increased with tenure)9 paid holidays & 5 sick daysPaid time off for volunteer work and on your birthday', 'Mature engineering practices (CI/CD, testing, secure coding, etc)', 'collaborate', 'Get Better (every day): We innovate, iterate, and improve each day. We are creative, take thoughtful risks, and ultimately learn and recover from failures.', 'WHY JOIN THE FREEDOM FAMILY?', '3 weeks’ paid vacation (increased with tenure)', 'The Role', 'act with integrity', 'Play a key role in building out a large scale distributed and event based Data Platform that serves as an underpinning for all of Freedom’s businesses and products', 'CULTURAL FIT (Our Core Values)']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Trove,"Brisbane, CA",5 days ago,78 applicants,"['', ' Experience with the specific tools we currently use:  Experience working with machine learning systems ', 'About Trove Recommerce', 'You have experience architecting a CI/CD pipeline for containers and microservices.', 'You have strong communication skills, including communicating complex technical information to a non-technical audience.', 'Participate in ensuring code and data quality within the data team. Partner with analysts to fine-tune queries around large, complex data sets. ', 'Evolve tools and processes to enable the data team to monitor daily execution, diagnose and log issues, and fix pipelines to ensure SLAs are met with internal stakeholders', 'You have a product-focused mindset. You enjoy digging deep into business requirements and architecting systems that will scale and extend to accommodate those needs.', 'About The Data Engineer', 'Bonus points if you...', 'You thrive on autonomy and have experience driving long-term, cross-functional projects to completion. ', ' Interface with data engineers, data scientists, and all data stakeholders to understand their needs and promote best practices Design the data infrastructure architecture to enable delivery of machine learning insights at scale Extend and optimize the current ELT pipeline to help assemble complex data sets to address a diverse set of business and data analytics requests. We use off the shelf solutions like Stitch and Fivetran where we can, but occasionally need to build custom integrations as well.  Evolve tools and processes to enable the data team to monitor daily execution, diagnose and log issues, and fix pipelines to ensure SLAs are met with internal stakeholders Vet tools and technologies for the most viable solution for each problem at hand. Manage tools and data vendors involved. Participate in ensuring code and data quality within the data team. Partner with analysts to fine-tune queries around large, complex data sets.  ', 'Partner with Data Scientists to design, develop and implement scalable container-based architecture for delivering batch and online machine learning inference.', 'You have experience building and optimizing cloud data pipelines, architectures and data sets.', 'You have strong overall programming skills, and are able to write modular, maintainable code (preferably Python).', 'Interface with data engineers, data scientists, and all data stakeholders to understand their needs and promote best practices', ' You are energized by the thought of working across a wide array of technologies, project types, and stakeholders.  You have a product-focused mindset. You enjoy digging deep into business requirements and architecting systems that will scale and extend to accommodate those needs. You have experience building and optimizing cloud data pipelines, architectures and data sets. You have strong overall programming skills, and are able to write modular, maintainable code (preferably Python). You have a deep understanding of SQL, dimensional modeling, and analytical data warehouses, like Redshift or BigQuery. You have experience architecting a CI/CD pipeline for containers and microservices. You take a pragmatic approach to decision making and design choices.  You have strong communication skills, including communicating complex technical information to a non-technical audience. You thrive on autonomy and have experience driving long-term, cross-functional projects to completion.  ', 'You have a deep understanding of SQL, dimensional modeling, and analytical data warehouses, like Redshift or BigQuery.', ' Partner with Data Scientists to design, develop and implement scalable container-based architecture for delivering batch and online machine learning inference. Work with a cross-functional team to design and implement a secure data sharing platform that will allow us to push customer data, including PII, to our partner brands and retailers. Partner with full stack engineers to build data tools, such as an experimentation platform, for analytics and data scientist team members. ', 'Design the data infrastructure architecture to enable delivery of machine learning insights at scale', 'Vet tools and technologies for the most viable solution for each problem at hand. Manage tools and data vendors involved.', 'Experience working with machine learning systems', 'As for necessities, ...', 'Responsibilities Include…', 'Partner with full stack engineers to build data tools, such as an experimentation platform, for analytics and data scientist team members.', 'Projects might include…', 'You take a pragmatic approach to decision making and design choices. ', 'Extend and optimize the current ELT pipeline to help assemble complex data sets to address a diverse set of business and data analytics requests. We use off the shelf solutions like Stitch and Fivetran where we can, but occasionally need to build custom integrations as well. ', 'You are energized by the thought of working across a wide array of technologies, project types, and stakeholders. ', 'Work with a cross-functional team to design and implement a secure data sharing platform that will allow us to push customer data, including PII, to our partner brands and retailers.', 'Experience with the specific tools we currently use: ', 'Trove is proud to be an equal opportunity company and is an affirmative action employer. We believe that diversity and gender balance will help us reach our potential. We are committed to equal opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, or veteran status. We will consider qualified applicants with arrest and conviction records for employment.']",Entry level,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Data Engineer I,VisitPay,"Boise, ID",1 week ago,Be among the first 25 applicants,"['', 'Apply for this Position', 'Competencies For Success', ' Prefers a collaborative environment and is comfortable working with others and giving and receiving feedback ', ' Wants to be part of a high-growth, high-innovation company that will revolutionize a market ', ' Thank You ', ' Boise, ID or remote.  We will only consider applicants who reside within the U.S.  Sponsorship is not available for this position. ', 'Key Activities', ' Design, build, deploy and support complex data pipelines that drive both internal and client-facing products  Identify, design, and implement internal process improvements through automation, optimizing data delivery, redesigning infrastructure for greater scalability, etc.  Develop fully automated data pipelines (CICD)  Collaborate across multiple cross-functional teams including product, software development, QA, and end users.  Work effectively on a product-driven agile team and collaborate with other data teams to drive organization wide efficiencies. ', ' Hands-on experience developing ETL/data integration solutions ', ' 1+ years of SQL development or 3+ years of SQL query development/analysis/QA  Experience with object-oriented programming in Python, C# or .Net  Hands-on experience developing ETL/data integration solutions  Experience working with large, complex data sets  Excellent analytical, conceptual, troubleshooting, and problem-solving skills  Some PowerShell programming skills  Experience with version control (GIT)  BS in Computer Science, Information Systems, or related field ', ' Work effectively on a product-driven agile team and collaborate with other data teams to drive organization wide efficiencies. ', ' Experience working with Azure Databricks ', ' Sponsorship is not available for this position. ', ' Experience working with large, complex data sets ', ' Some PowerShell programming skills ', ' Excellent analytical, conceptual, troubleshooting, and problem-solving skills ', ' Experience working with Azure Databricks  Experience working in a secure HIPAA/PHI environment  Experience with hospital and/or financial systems ', ' Thrives on challenges and loves learning ', ' Is self-driven, diligent, and enjoys solving problems ', ' 1+ years of SQL development or 3+ years of SQL query development/analysis/QA ', ' Experience with object-oriented programming in Python, C# or .Net ', ' Experience with hospital and/or financial systems ', ' Develop fully automated data pipelines (CICD) ', ' Thrives on challenges and loves learning  Is self-driven, diligent, and enjoys solving problems  Wants to be part of a high-growth, high-innovation company that will revolutionize a market  Prefers a collaborative environment and is comfortable working with others and giving and receiving feedback ', ' We will only consider applicants who reside within the U.S. ', ' Experience working in a secure HIPAA/PHI environment ', 'Qualifications', 'Summary Of Position', ' Boise, ID or remote. ', ' Identify, design, and implement internal process improvements through automation, optimizing data delivery, redesigning infrastructure for greater scalability, etc. ', ' BS in Computer Science, Information Systems, or related field ', ' Experience with version control (GIT) ', ' Collaborate across multiple cross-functional teams including product, software development, QA, and end users. ', 'By submitting your interest in this job, you agree to receive text notifications with additional steps to complete your job application. You will receive up to 6 messages from the number ""63879"". Message & data rates may apply. Please refer to our privacy policy for more information.', ' Design, build, deploy and support complex data pipelines that drive both internal and client-facing products ', 'Desired Experience']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Lead Data Engineer,Target,"Minneapolis, MN",4 weeks ago,Be among the first 25 applicants,"['', 'https://corporate.target.com/careers/', 'Description', 'Americans With Disabilities Act (ADA)']",Associate,Full-time,Information Technology,Retail,2021-03-24 13:05:10
"Data Engineer, eCommerce",Anheuser-Busch InBev,"New York, NY",,N/A,"['', 'Familiarity with containerization technologies, e.g., Docker, Kubernetes\xa0', 'The Data Engineer will create, oversee and maintain commercial and external data pipelines working closely with our commercial, product and technology leads to standardize our commercial and external data. This person should have experience building data integrations to sales and ERP systems such as SAP, maintaining centralized data warehouses and data dictionaries for multiple commercial stakeholders.\xa0', 'Working knowledge of “Big Data” technologies such as MapReduce and/or Spark\xa0', 'Have experience\u202fworking with\u202fboth\u202fhighly technical\u202fand non-technical profiles on a day-to-day basis\xa0', 'Spark/PySpark\xa0', 'You Will', 'Experience working for an international organization with teams distributed across geographies/timezones\xa0', 'Take ownership of understanding the business and operational problems at hand and how to best solve them through a data-driven approach\xa0', 'Prioritize and find the most efficient path towards solving complex, ambiguous business problems with data, keeping a mindset of simplicity, robustness and speed above all\xa0', 'About the Team', 'Familiarity with Data Governance and related concepts, e.g., lineage, quality, integrity, security\xa0', 'ReTool\xa0', 'Be a key leader in our global organization, working fluidly across ABI’s global and local teams and functions (sales, finance, marketing, product, IT, etc.) to make data-driven initiatives successful in an efficient way\xa0', 'We’re rethinking the way AB InBev does business with its retail customers and creating digital experiences to serve them. You will be joining a new digital organization within AB InBev consisting of digital strategy,\u202fproduct, design, analytics, operations and engineering. This organization is responsible for building the products and platforms that transform our traditional sales operations across the world.\xa0', 'About You', 'Our Technology\xa0', 'Have experience operating in a production level data warehouse and/or data lake\xa0', 'API design expertise', 'About the Team\xa0', 'Experience structuring data from unstructured data sources and maintaining SLAs for delivery\xa0', 'BA/BS degree (Computer Science, Software/Computer Engineering, Information Systems, Statistics, or similar technical field)\xa0Working knowledge of “Big Data” technologies such as MapReduce and/or Spark\xa0Experience building cloud-based solutions (AWS, Azure, GCP)\xa0Understanding of CI/CD and DevOps best practices\xa0Familiarity with Data Governance and related concepts, e.g., lineage, quality, integrity, security\xa0Familiarity with containerization technologies, e.g., Docker, Kubernetes\xa0Experience with workflow orchestration tools, e.g., Airflow, Luigi\xa0Functional programming experience\xa0Familiarity with ML development lifecycle and related tools/libraries\xa0Experience building streaming data pipelines using Kafka, Kinesis, Spark, and/or Flink\xa0API design expertise', 'Required Qualifications\xa0', 'Required Qualifications', 'Create and maintain optimal data pipeline architecture, – own all data sources (transactional, internal and external data\u202fwithin our e-commerce organization) and ensure their accuracy to maintain commercial functions in our digital platforms\xa0', 'Our team is in search of a Data Engineer to be a leader in our Global Revenue Management & Commercial organization and help build, and maintain our data engineering infrastructure geared towards solving business problems and provide commercial value for ABI. This position reports directly to the Global Director of eCommerce Data Engineering.', 'Azure (Active Directory, Data Factory, Data Share, DevOps, Event Hub, Key Vault, Storage accounts/Blob containers/ADLS Gen2)\xa0Snowflake\xa0Databricks\xa0Spark/PySpark\xa0ReTool\xa0PowerBI\xa0New Relic', 'Experience building streaming data pipelines using Kafka, Kinesis, Spark, and/or Flink\xa0', 'Desired Qualifications', 'PowerBI\xa0', 'Databricks\xa0', 'Experience with Data Warehouses such as Google Big Query, Snowflake, or Redshift\xa0', 'About AB InBev\xa0', 'About You\xa0', 'BA/BS degree (Computer Science, Software/Computer Engineering, Information Systems, Statistics, or similar technical field)\xa0', 'Have experience\u202fworking in a central analytics or data strategy role within a large enterprise\u202fenvironment\xa0', 'New Relic', 'Understanding of CI/CD and DevOps best practices\xa0', 'About BEES ', 'Experience managing the full end-to-end pipeline for data warehousing and data pipelines of commercial data for multiple analytics and BI stakeholders\xa0', 'Main stakeholders will include sales functions (commercial reporting, promo analytics, and sales algorithms) and customer data product leadership.\xa0', 'You Will\xa0', 'About AB InBev', 'Have strong communication skills: able to\u202fclearly understand business objectives\u202fand translate them to an end to end data strategy and execution\xa0Have experience\u202fworking in a central analytics or data strategy role within a large enterprise\u202fenvironment\xa0Have experience operating in a production level data warehouse and/or data lake\xa0Live and breathe data – providing the best, most correct data is your obsession\xa0Have experience\u202fworking with\u202fboth\u202fhighly technical\u202fand non-technical profiles on a day-to-day basis\xa0', 'Azure (Active Directory, Data Factory, Data Share, DevOps, Event Hub, Key Vault, Storage accounts/Blob containers/ADLS Gen2)\xa0', 'About the Job\xa0', 'AB InBev is the leading global brewer and one of the world’s top 5 consumer product companies. With over 500 beer brands we’re number one or two in many of the world’s top beer markets: North America, Latin America, Europe, Asia, and Africa.\xa0', 'Experience with workflow orchestration tools, e.g., Airflow, Luigi\xa0', 'Familiarity with ML development lifecycle and related tools/libraries\xa0', 'BEES, a part of the AB InBev family, is a digital organization within ABI building a platform to improve the ways retailers run their businesses and interact with the world’s largest brewer & other suppliers. We provide transactional and educational resources to SMB retailers across the world to help reduce the overhead of their day-to-day operations and make their businesses more profitable. Today more than 1 million SMB retailers across 18 countries use the BEES platform to source manage their business.', 'Our Technology', 'Experience working in a full Data Engineering team: QA, Data Engineers, Data Analysts, DBAs, etc.\xa0', 'Create and maintain optimal data pipeline architecture, – own all data sources (transactional, internal and external data\u202fwithin our e-commerce organization) and ensure their accuracy to maintain commercial functions in our digital platforms\xa0Take ownership of understanding the business and operational problems at hand and how to best solve them through a data-driven approach\xa0Execute, and maintain the strategy for our data warehousing and pipelines to cater to specific business objectives, from implementation to maintenance\xa0Be a key leader in our global organization, working fluidly across ABI’s global and local teams and functions (sales, finance, marketing, product, IT, etc.) to make data-driven initiatives successful in an efficient way\xa0Prioritize and find the most efficient path towards solving complex, ambiguous business problems with data, keeping a mindset of simplicity, robustness and speed above all\xa0', 'About the Job', 'Desired Qualifications\xa0', 'Familiarity with SQL\xa0', 'Functional programming experience\xa0', 'Experience building cloud-based solutions (AWS, Azure, GCP)\xa0', 'Live and breathe data – providing the best, most correct data is your obsession\xa0', 'Expertise in Python, C++, and/or a JVM language\xa0', 'Have strong communication skills: able to\u202fclearly understand business objectives\u202fand translate them to an end to end data strategy and execution\xa0', 'Snowflake\xa0', 'Execute, and maintain the strategy for our data warehousing and pipelines to cater to specific business objectives, from implementation to maintenance\xa0', 'Expertise in Python, C++, and/or a JVM language\xa0Familiarity with SQL\xa0Experience with Data Warehouses such as Google Big Query, Snowflake, or Redshift\xa0Experience managing the full end-to-end pipeline for data warehousing and data pipelines of commercial data for multiple analytics and BI stakeholders\xa0Experience structuring data from unstructured data sources and maintaining SLAs for delivery\xa0Experience working in a full Data Engineering team: QA, Data Engineers, Data Analysts, DBAs, etc.\xa0Experience working for an international organization with teams distributed across geographies/timezones\xa0']",Mid-Senior level,Full-time,Analyst,Consumer Goods,2021-03-24 13:05:10
Data Engineer,Centerfield,"Los Angeles, CA",5 days ago,33 applicants,"['', 'Strong foundation in SQL coding and experience with ETL processes', 'Amazon Web Services (S3, SQS, Redshift, DocumentDB, etc.) ', 'Bonus Points…', 'Experience with Google BigQuery and Google Analytics', 'Life at Centerfield...', 'Centerfield Media is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, age, religion, sex, national origin, disability, protected veteran status, or any other characteristic protected under federal, state or local law.', 'Experience with Python', 'Work with relevant stakeholders to deliver appropriate BI, data warehousing, reporting, and analytical infrastructure required to support Centerfield’s assets', 'BI tools like Tableau, PowerBI, or Microstrategy, etc.Experience with NoSQL databases like MongoDB, DynamoDB, Druid, etc.Amazon Web Services (S3, SQS, Redshift, DocumentDB, etc.) Experience with Python', 'Free onsite gym + locker rooms', 'Award winning culture & unprecedented team spirit (featured in LA Business Journal & Built In LA)', 'Career growth – we enjoy promoting from within!', 'BI tools like Tableau, PowerBI, or Microstrategy, etc.', 'Competitive salary + quarterly bonus', 'Outside patio overlooking Playa Vista + cabanas, firepits & working grills', '2-4 years working in a Data Engineer, BI Engineer, or Data Warehousing Engineer roleStrong experience with any ETL tool like Talend or SSIS or Informatica, etc.Experience with Google BigQuery and Google AnalyticsAbility to lead projects individually and deliver them on timeStrong experience in performance tuning techniquesExperience with real-time streaming implementation and architectureExperience building reports and data visualization with any BI tools like Tableau, Power BI, etc.Strong foundation in SQL coding and experience with ETL processes', 'Competitive salary + quarterly bonusUnlimited PTO – take a break when you need it!Industry leading medical, dental, and vision plans + generous parental leave401(k) company match plan – fully vested day 1Outside patio overlooking Playa Vista + cabanas, firepits & working grillsMonthly happy hours, catered lunches + daily food trucksAward winning culture & unprecedented team spirit (featured in LA Business Journal & Built In LA)Fully stocked kitchens with snacks & drinksBreakroom supplied with games, couches, workout equipment + weekly in-office exercise classes hosted by professional instructors (yoga, kickboxing & circuit training)Free onsite gym + locker roomsPaid charity and volunteer days (local mentor programs, adopt a pet, beach cleanup, etc.)Monthly team outings (ball games, casino night, hikes, etc.)Career growth – we enjoy promoting from within!', 'Strong experience in performance tuning techniques', 'Monthly team outings (ball games, casino night, hikes, etc.)', 'Paid charity and volunteer days (local mentor programs, adopt a pet, beach cleanup, etc.)', ""Hi, We're Centerfield. "", 'Fully stocked kitchens with snacks & drinks', 'Experience with real-time streaming implementation and architecture', '2-4 years working in a Data Engineer, BI Engineer, or Data Warehousing Engineer role', 'Ability to lead projects individually and deliver them on time', 'To learn more, visit us Here . ', 'Help to implement maintenance strategy for all datasetsWork with relevant stakeholders to deliver appropriate BI, data warehousing, reporting, and analytical infrastructure required to support Centerfield’s assetsOwn problems from end-to-end, so that you can best collect, extract, and clean the data', 'Strong experience with any ETL tool like Talend or SSIS or Informatica, etc.', 'Industry leading medical, dental, and vision plans + generous parental leave', 'Breakroom supplied with games, couches, workout equipment + weekly in-office exercise classes hosted by professional instructors (yoga, kickboxing & circuit training)', 'Experience building reports and data visualization with any BI tools like Tableau, Power BI, etc.', ""How You'll Contribute..."", 'Own problems from end-to-end, so that you can best collect, extract, and clean the data', 'The Opportunity...', ""What We're Looking For..."", 'Experience with NoSQL databases like MongoDB, DynamoDB, Druid, etc.', '401(k) company match plan – fully vested day 1', 'Help to implement maintenance strategy for all datasets', 'Unlimited PTO – take a break when you need it!', 'Monthly happy hours, catered lunches + daily food trucks']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer,First Republic Bank,"San Francisco, CA",3 weeks ago,29 applicants,"['', ' Hands-On experience with data pipeline design and development.', ""What You'll Do As a Data Engineer"", ' Experience with both SQL and NoSQL as well as their relevant data modeling patterns', ' Demonstrated experience working in large-scale data environments which included real-time and batch processing requirements.', ' Perform detailed analysis to troubleshoot and resolve identified issues and maintain data integrity', ' Bachelors or Master degree in information technology, computer science or data science Strong skills in python and knowledge of various frameworks like pandas, pyspark. Experience in building cloud native data lakes, pipelines and stream processing Experience with cloud services preferably AWS and Snowflake. Background in data science, analytics, or data mining. Familiar with Data Virtualization concepts ideally with Denodo/Composite experience. Familiar with Data Virtualization concepts ideally with Denodo/Composite experience. Experience in DevSecOps and automation using CICD tools and process Proven history of learning and implementing new technology in fast moving environment. Hands-On experience with data pipeline design and development. Experience with both SQL and NoSQL as well as their relevant data modeling patterns Demonstrated experience working in large-scale data environments which included real-time and batch processing requirements. Familiar with Data Virtualization concepts ideally with Denodo/Composite experience.', ' Familiar with Data Virtualization concepts ideally with Denodo/Composite experience.', ' Apply data science skills to model data for quality verification', ' Must be able to communicate effectively via telephone and in person.', 'Description', ' Design, develop and maintain various data model for regulatory and corporate domain Develop Pipelines, ensuring the best practice are implemented for data governance, data quality, data lineage and data cleansing. Apply data science skills to model data for quality verification Perform detailed analysis to troubleshoot and resolve identified issues and maintain data integrity Responsible for driving and managing data source integration between various vendor systems.', ' Must be able to review and analyze data reports and manuals; must be computer proficient. Must be able to communicate effectively via telephone and in person.', 'Responsibilities', 'Qualifications', 'Job Demands', ' Must be able to review and analyze data reports and manuals; must be computer proficient.', ' Proven history of learning and implementing new technology in fast moving environment.', ' Develop Pipelines, ensuring the best practice are implemented for data governance, data quality, data lineage and data cleansing.', ' Strong skills in python and knowledge of various frameworks like pandas, pyspark.', ' Experience in DevSecOps and automation using CICD tools and process', ' Experience with cloud services preferably AWS and Snowflake.', ' Design, develop and maintain various data model for regulatory and corporate domain', ' Experience in building cloud native data lakes, pipelines and stream processing', ' Bachelors or Master degree in information technology, computer science or data science', ' Responsible for driving and managing data source integration between various vendor systems.', ' Background in data science, analytics, or data mining.']",Not Applicable,Full-time,Information Technology,Banking,2021-03-24 13:05:10
Data Engineer,TechDigital Corporation,"Plainsboro Center, NJ",,N/A,"['', 'Work on a Cloud based Datawarehouse like AWS and work with transactional and unstructured data to develop interfaces for downstream reporting and data sciences projects.', 'Design and build modern Data Pipelines, Data Streams and Data Service Application Programming Interfaces (APIs).', 'Create data ingestion procedures using Python, Spark, Lambda, Glue etc.', 'Develops and builds out new API integrations to support continuing increases in data volume and complexity.', 'Collaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.', 'Manage Data Lake and Analytical Applications which is hosted in AWS environment like Redshift, EMR and EC2.', 'Experience handling un-structured data, leveraging data streaming and developing data pipelines driven by events and queues.', 'Design and build modern Data Pipelines, Data Streams and Data Service Application Programming Interfaces (APIs). Develops and builds out new API integrations to support continuing increases in data volume and complexity. Have Prior Experience in Integrating External data feeds like Twitter, Facebook and 3rd party data sets into their data warehouses. Work on a Cloud based Datawarehouse like AWS and work with transactional and unstructured data to develop interfaces for downstream reporting and data sciences projects. Works closely with all business units and engineering teams to develop strategy for long term data platform architecture. Manage Data Lake and Analytical Applications which is hosted in AWS environment like Redshift, EMR and EC2. Create data ingestion procedures using Python, Spark, Lambda, Glue etc. Collaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization. Experience handling un-structured data, leveraging data streaming and developing data pipelines driven by events and queues.', 'Have Prior Experience in Integrating External data feeds like Twitter, Facebook and 3rd party data sets into their data warehouses.', 'Works closely with all business units and engineering teams to develop strategy for long term data platform architecture.']",Entry level,Contract,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer-Remote,Stefanini Group,"San Francisco, CA",4 days ago,Be among the first 25 applicants,"['Demonstrated technical expertise across a variety of technology platforms and solutions.', '3 or more years of relevant technical experience, at senior level.Bachelor""s degree in Computer Science, Information Systems, or other related technical field or equivalent work.Experience in using Terraform to manage AWS Programmable Infrastructures.Must have architected and implemented the Cloud Infrastructure Automation scripts to create and maintain various target environments like Dev, Stage, QA, Integration and Production in AWS environments.The Infrastructure must include security roles and permissions, Cloud networking assets like VPC, Subnets, Routing Tables, Access Controls lists, storage assets like S3 buckets, creating lambda functions & layers, provisioning other AWS services like Redshift, DynamoDB etc.Experience with advanced features like S3 backends and State file locks in Terraform.Experienced in implementing Data and Advanced Analytics solutions, or related experience in the Cloud.Experience in developing an end to end AWS native platform for building Data lakes ( S3, Glue (Crawlers, ETL, Catalog), IAM, CodePipeline, CodeCommit, CloudTrail, CloudWatch, AWS Config, Guard Duty, Secrets Manager, KMS, EC2, Data Visualization Tool like Tableau run on an EC2 or AWS Quicksight, Athena.Hands on programming skills in JAVA or Python or Scala or other scripting language.Working knowledge of Amazon Web Services.Experience in Continuous Integration, Continuous Delivery, and Continuous Deployment software tools to support, enhance and grow our CI and CD capabilities.Understanding of security design for enterprise software systemsLooking for W2 Candidates for this role.', 'Knowledge of high-availability, load-balancing and failover configurations across application, infrastructure, and platform.', 'Strong analytical, collaboration, leadership, critical thinking, multitasking, and time management skills.', 'Experience in Continuous Integration, Continuous Delivery, and Continuous Deployment software tools to support, enhance and grow our CI and CD capabilities.', 'Experience with advanced features like S3 backends and State file locks in Terraform.', 'Ability to work effectively independently or in a team environment.', 'You will provide technical leadership in the planning, design, and implementation of cloud-based infrastructure systems with both traditional and non-traditional infrastructures. ', 'Proven ability to write clear and concise communications: technical documents, design documents, specifications.', 'Demonstrated technical expertise across a variety of technology platforms and solutions.Ability to work effectively independently or in a team environment.Strong analytical, collaboration, leadership, critical thinking, multitasking, and time management skills.Ability to influence successfully in a highly matrix or virtual organization.Demonstrates independent thinking and decision-making abilities.Ability to quickly receive and process information.Proven ability to write clear and concise communications: technical documents, design documents, specifications.', 'In this role, you will get an opportunity to broadly apply your engineering skills across various technology solutions, as well as build your skills in other areas by being exposed to various aspects of product delivery from inception, through design, build, and deployment.You will be working multi-functionally with Product Managers, Architects, Engineers, and Customer teams in a rapidly evolving environment.You will be developing Infrastructure as Code to launch server instances, install and configure software, amongst other things.You will provide technical leadership in the planning, design, and implementation of cloud-based infrastructure systems with both traditional and non-traditional infrastructures. ', 'Looking for W2 Candidates for this role.', 'Experienced in implementing Data and Advanced Analytics solutions, or related experience in the Cloud.', 'Background in data security, governance and cybersecurity solutions.', 'Bachelor""s degree in Computer Science, Information Systems, or other related technical field or equivalent work.', 'Stefanini is looking for a Data Engineer for Remote Location.', 'Must have architected and implemented the Cloud Infrastructure Automation scripts to create and maintain various target environments like Dev, Stage, QA, Integration and Production in AWS environments.', 'The Infrastructure must include security roles and permissions, Cloud networking assets like VPC, Subnets, Routing Tables, Access Controls lists, storage assets like S3 buckets, creating lambda functions & layers, provisioning other AWS services like Redshift, DynamoDB etc.', 'Experience with and/or working knowledge the Financial Industry, Government Agencies, Federal Reserve Bank Lines of Business (LoBs) Applications.', 'Knowledge of high-availability, load-balancing and failover configurations across application, infrastructure, and platform.Experience with and/or working knowledge the Financial Industry, Government Agencies, Federal Reserve Bank Lines of Business (LoBs) Applications.Working experience with Kubernetes, ConnectDirect, etc.Practical experience and knowledge of Service Oriented Architecture (SOA), Mircoservices and API Management.Background in data security, governance and cybersecurity solutions.', 'Practical experience and knowledge of Service Oriented Architecture (SOA), Mircoservices and API Management.', 'Preferred Skills', 'Required Skills:', 'Ability to influence successfully in a highly matrix or virtual organization.', 'Demonstrates independent thinking and decision-making abilities.', 'You will be working multi-functionally with Product Managers, Architects, Engineers, and Customer teams in a rapidly evolving environment.', 'In this role, you will get an opportunity to broadly apply your engineering skills across various technology solutions, as well as build your skills in other areas by being exposed to various aspects of product delivery from inception, through design, build, and deployment.', 'Responsibilities', 'Ability to quickly receive and process information.', '3 or more years of relevant technical experience, at senior level.', 'Hands on programming skills in JAVA or Python or Scala or other scripting language.', 'Working knowledge of Amazon Web Services.', 'Qualifications', 'Experience in developing an end to end AWS native platform for building Data lakes ( S3, Glue (Crawlers, ETL, Catalog), IAM, CodePipeline, CodeCommit, CloudTrail, CloudWatch, AWS Config, Guard Duty, Secrets Manager, KMS, EC2, Data Visualization Tool like Tableau run on an EC2 or AWS Quicksight, Athena.', 'You will be developing Infrastructure as Code to launch server instances, install and configure software, amongst other things.', 'Working experience with Kubernetes, ConnectDirect, etc.', 'Job Requirements', 'Experience in using Terraform to manage AWS Programmable Infrastructures.', 'Understanding of security design for enterprise software systems']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer (CTH),Blackstone Talent Group,"Los Angeles, CA",2 weeks ago,83 applicants,"['Blackstone Talent Group is a division of Blackstone Technology Group, a global IT services and solutions firm that implements technological solutions across commercial industry verticals and the US Federal Government. Blackstone’s global talent augmentation practice was founded in 1998. Blackstone Talent Group has offices in San Francisco, Denver, Houston, Colorado Springs, and Washington, DC. We specialize in providing clients the best talent across a variety of industries and sectors.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Strong experience in authoring, scheduling, and\xa0monitoring of workflows (Apache Airflow related technologies)', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience in design, build and operationalization of\xa0big data pipelines on Cloud Technologies.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Advanced SQL programming skills', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Participate in design and code reviews\xa0', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Hands-on experience with Apache Airflow or Google Composer', 'Our cutting-edge Media client is looking for a Data Engineer to build and maintain optimized and highly available data pipelines.', ""·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Bachelor's degree in\xa0Computer Science or equivalent experience in a related field"", 'Primary Duties and Key Responsibilities:', 'Job Description:', 'MUST BE ABLE TO WORK ON OUR W2', 'Blackstone Talent Group, an award-winning technology consulting and talent agency is seeking a Data Engineer to join our team at our client’s site in Fort Lauderdale, FL – San Francisco, CA, LA or New York. Role is fully remote!', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Be part of the on-call rotation supporting our SLA’s', 'MUST HAVE: Airflow, Python, AWS or Docker experience.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Apache Airflow or Similar (Luigi, Apache NiFi, Jenkins, AWS Step Functions, and Pachyderm are the most popular alternatives and competitors to Airflow)', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Familiar with a NoSQL database such as MongoDB', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Google Cloud Certified - Professional Data Engineer certification would be a plus', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Familiar with Atlassian products Jira and Confluence', '\xa0', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Five years of\xa0hands-on experience working in data warehousing or data engineering environment', 'EOE of Minorities/Females/Veterans/Disabilities', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience in\xa0ingestion of data from external APIs and data stores', 'Required Qualifications:', '(Luigi, Apache NiFi, Jenkins, AWS Step Functions, and Pachyderm are the most popular alternatives and competitors to Airflow)', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Develop real-time data processing applications using Google Cloud', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience developing data solutions on GCP or AWS', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Strong communication &\xa0interpersonal skills', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Knowledge of Git, Jinja2, Docker, Bitbucket, and Bamboo', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Design and develop highly scalable and reliable data engineering pipelines to process large volumes of data across many data sources', '\xa0\xa0Preferred Qualifications:', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Familiar with version control systems (Git and Bitbucket)', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Develop and promote best practices in data engineering', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Identify, design and implement internal process improvements by automating manual processes and optimizing data delivery']",Mid-Senior level,Contract,Advertising,Broadcast Media,2021-03-24 13:05:10
Data Engineer,"Enterprise Knowledge, LLC","Arlington, VA",2 weeks ago,Be among the first 25 applicants,"['', 'Experience with the graph data structure such as Knowledge Graphs or other graph analytics use cases', '“We are an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.”', 'Experience with multiple data structures and tools such as relational databases, graph databases, document stores, search indexes, etc. and multiple data serialization formats such as CSV, JSON, Parquet, or HDFS.', 'Able to carefully identify and understand client needs in order to design practical solutions that surpass their expectations.', 'Washington Business Journal', 'Proficiency in at least one database query language such as SQL, SPARQL, or Gremlin', 'Working knowledge or experience with implementing Machine Learning workflows including Exploratory Data Analysis, Feature Engineering, Model Training, and Cross Validation.', 'Inc. Magazine’s 2020 Best Workplaces!', 'benefits', ' Experience in development using leading data manipulation frameworks, especially Python or R, and associated libraries such as pandas and dplyr. Able to carefully identify and understand client needs in order to design practical solutions that surpass their expectations. Minimum of 3 years of experience developing data pipelines or data products and services such as microservices or APIs. Experience implementing data transformation pipelines using data orchestration or ETL workflows such as with tools like Apache Airflow or AWS Step Functions. Experience with multiple data structures and tools such as relational databases, graph databases, document stores, search indexes, etc. and multiple data serialization formats such as CSV, JSON, Parquet, or HDFS. Proficiency in at least one database query language such as SQL, SPARQL, or Gremlin ', 'Preferred Skills', 'Implementation of distributed data processing frameworks such as Apache Spark, Kolas, or Dask.', ' Experience with the graph data structure such as Knowledge Graphs or other graph analytics use cases Working knowledge or experience with implementing Machine Learning workflows including Exploratory Data Analysis, Feature Engineering, Model Training, and Cross Validation. Implementation of distributed data processing frameworks such as Apache Spark, Kolas, or Dask. Experience building, deploying, and supporting cloud-based infrastructure such as AWS and understanding of key cloud/IT principles such as networking, permissions, basic server management, and Infrastructure as code (IaC) templating. ', 'Experience in development using leading data manipulation frameworks, especially Python or R, and associated libraries such as pandas and dplyr.', 'Experience implementing data transformation pipelines using data orchestration or ETL workflows such as with tools like Apache Airflow or AWS Step Functions.', 'Minimum of 3 years of experience developing data pipelines or data products and services such as microservices or APIs.', 'Experience building, deploying, and supporting cloud-based infrastructure such as AWS and understanding of key cloud/IT principles such as networking, permissions, basic server management, and Infrastructure as code (IaC) templating.', 'Required Skills']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Architect/Engineer,Health IQ,United States,4 weeks ago,79 applicants,"['', 'Hyper-growth means hyper opportunity for employees! While many companies hire managers externally or use a tenure system, we pride ourselves on our cultural value of meritocracy. This means that as we grow and new positions are created, we look to promote our top performers from within. Today over 80% of our managers are promoted from internal roles.\xa0', 'Join the Health Conscious Workplace of the Future\xa0', 'Maintain and refactor existing schema to maximize data usability and consistency across different business functions', '6. Excellent benefits ', 'Health IQ is adding a Data Architect/Engineer to its growing platform engineering team. As a Data Architect/Engineer, you will work with clients, team members, department heads and 3rd party data providers, to develop, maintain, and enhance our data engineering capabilities in support of our data and predictive analytic offerings to the insurance marketplace.\xa0The ideal candidate demonstrates a curious analytical mind with the ability to understand business objectives, ask insightful questions, and be detail-oriented in implementation.', 'To make the world a healthier place, we started in our backyard. We created a health-conscious environment that allows each of our employees to reach their personal health goals. Below are a few of the employee-led programs that make working at Health IQ truly unique.\xa0', 'Database/Query performance tuning', 'BS/MS/Ph.D.PhD in a technical field - e.g. Computer Science, Math, Statistics, Economics4+ years in Data Engineering, key experiences include:Writing in SQL, Python, R and shell scripting in a version control systemData visualization tools like Looker, Periscope, Tableau, Power BISet up of real-time data pipelines and associated tools: Hadoop, Spark, KafkaWorkflow management, orchestration and batch processing: Apache Airflow, AWS Step Functions, AWS BatchCluster management: JuypterHub, Apache Zeppelin, Docker, Kubernetes, Linux, CICDDatabase/Query performance tuningModern data warehousing solutions like Redshift, Snowflake and BigQueryVarious data structures and formats: XML, JSON, Parquet, ORC, Avro, CSVData mining and machine learningELT/ETL/ETLT technologies like DBT, Fivetran, ETLeapInsurance industry knowledge or experience with insurance data a plus', 'What we’re looking for:', 'Are You Ready to Join The Movement?\xa0', 'Writing in SQL, Python, R and shell scripting in a version control system', '4+ years in Data Engineering, key experiences include:', 'Set up of real-time data pipelines and associated tools: Hadoop, Spark, Kafka', 'Build data pipelines and fix performance bottlenecks to visualize data real-time', ""Health IQ is the internet's fastest growing online Insurance Company in the US. In the last few years, we’ve gone from 0 to $24B in coverage, 0 to 230 employees, 0 to $139MM in venture capital raised. Why is our product selling so fast? We have a data advantage. Health IQ spent 6+ years gathering the science and the proprietary data from our popular Health IQ test (taken 10.2 million times) to convince insurance carriers to give lower rates on life insurance for vegans, marathoners, triathletes, well-controlled diabetics, yogis, Crossfitters, and more. These special rates are exclusive to us saving consumers thousands of dollars each and rewarding them for living a healthy lifestyle.\xa0"", 'Modern data warehousing solutions like Redshift, Snowflake and BigQuery', 'Insurance industry knowledge or experience with insurance data a plus', 'Join the Health Conscious Workplace of the Future', 'Build tools to support common data science functions (feature extraction, experimentation, funnel metrics, etc…)', 'Instead of the usual ping pong table, we’ve dedicated space for our employees to enjoy yoga, spin bikes, exercise equipment, and other wellness activities. We believe a healthy body is at the core of a healthy mind, so whether it’s time for a walk, using the exercise equipment, or daily meditation, we make it easy.\xa0', 'Anyone who has tried to stay healthy knows it’s hard to stick to your particular nutritional goals throughout the day with soda, chips, and sugary treats all around you. So we provide our employees with quality food and snack options, like a limitless supply of organic nuts, fruits, and veggies instead.\xa0', 'Architect data solutions to solve business problems while developing a long term data architecture roadmap', '\xa0', 'Cluster management: JuypterHub, Apache Zeppelin, Docker, Kubernetes, Linux, CICD', 'Stay up to date on industry and job-related trends and best practices, including reading relevant publications, articles, blogs, etc.', 'Health IQ believes that everyone can make an impact, and we are proud to be an equal opportunity employer committed to providing employment opportunity regardless of sex, race, creed, color, gender, religion, marital status, domestic partner status, age, national origin or ancestry, physical or mental disability, medical condition, sexual orientation, pregnancy, military or veteran status, citizenship status, and genetic information. If you require an accommodation to complete the application or the interview process, please contact talent@healthiq.com.\xa0', 'Build and maintain test coverage over key transforms and develop alerting systems', ""We pay 100% of our employees' costs toward medical, dental, and vision insurance.\xa0"", 'Are You Ready to Join The Movement?', 'Many of our employees are current or former athletes who are competitive and like to win. They motivate each other to do their personal best every day, and together we win as a team and as a company!\xa0', 'Data Architect/Engineer', '4. Like-Minded Coworkers ', 'Architect data solutions to solve business problems while developing a long term data architecture roadmapBuild data pipelines and fix performance bottlenecks to visualize data real-timeMaintain and refactor existing schema to maximize data usability and consistency across different business functionsBuild and maintain test coverage over key transforms and develop alerting systemsBuild tools to support common data science functions (feature extraction, experimentation, funnel metrics, etc…)Stay up to date on industry and job-related trends and best practices, including reading relevant publications, articles, blogs, etc.', 'Various data structures and formats: XML, JSON, Parquet, ORC, Avro, CSV', 'Data mining and machine learning', 'To learn more visit https://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdf', ""Ever dream of coming to work in your casual fitness attire? Well, that's how we roll at Health IQ! Be comfortable and let your fitness fashion shine!\xa0"", 'ELT/ETL/ETLT technologies like DBT, Fivetran, ETLeap', 'At the end of the day, we are a business-minded insurance company and we use analytics to measure our success and drive our business. Coming to work and working hard however is much more fun when you are surrounded by like-minded people who are motivated by the same personal goals as you. Our employees are making friends that will last a lifetime.\xa0', '3. Casual Office Attire ', '2. Optional Fitness Time ', 'BS/MS/Ph.D.PhD in a technical field - e.g. Computer Science, Math, Statistics, Economics', 'Data visualization tools like Looker, Periscope, Tableau, Power BI', '1. Nutritionally Supportive Environment ', 'Workflow management, orchestration and batch processing: Apache Airflow, AWS Step Functions, AWS Batch', 'What you will be doing:', '5. Motivate and Compete ']",Associate,Full-time,Engineering,Insurance,2021-03-24 13:05:10
Lead Data Engineer,SPIN,"San Francisco, CA",20 hours ago,Be among the first 25 applicants,"['', 'Deep understanding of distributed systems', 'Monthly cell phone bill stipend', 'Scaling up our data infrastructure to meet business needs', 'Build and maintain our data warehouse and data pipelines', 'Worked with Cloud-based architecture such as AWS or Google Cloud', 'Benefits & Perks', 'About The Role', 'Competitive health benefits', 'Wellness perk for salaried roles', 'Minimum of 7+ years of relevant experience in building and architecting data solutions', 'Experience with realtime data streaming infrastructures like AWS Kinesis, Spark or Kafka (Confluent Platform a plus)', 'Deploy sophisticated analytics programs, machine learning, and statistical methods', 'Work cross-functionally with our product, business, finance, and engineering teams', 'Minimum of 7+ years of relevant experience in building and architecting data solutionsDeep understanding of distributed systemsExpert in SQL and high-level languages such as Python, Java, or ScalaBuilt and maintained data warehouses and ETL pipelinesExperience with Data modeling and warehouse designYou have worked with big data solutions like Redshift, Snowflake, Hadoop or HiveExperience with realtime data streaming infrastructures like AWS Kinesis, Spark or Kafka (Confluent Platform a plus)Worked with Cloud-based architecture such as AWS or Google CloudYou will be working with BigQuery, DBT, Airflow, Python, Fivetran, Kafka, git and Looker, and we welcome new ideas.', 'Opportunity to join a fast-growing startup and help shape and establish the company’s industry leadershipCompetitive health benefitsUnlimited PTO for salaried rolesPre-tax commuter benefitsMonthly cell phone bill stipendWellness perk for salaried roles', 'Unlimited PTO for salaried roles', 'Build and maintain our data warehouse and data pipelinesScaling up our data infrastructure to meet business needsDeploy sophisticated analytics programs, machine learning, and statistical methodsWork cross-functionally with our product, business, finance, and engineering teams', 'Expert in SQL and high-level languages such as Python, Java, or Scala', 'Responsibilities', 'You have worked with big data solutions like Redshift, Snowflake, Hadoop or Hive', 'Qualifications', 'Built and maintained data warehouses and ETL pipelines', 'You will be working with BigQuery, DBT, Airflow, Python, Fivetran, Kafka, git and Looker, and we welcome new ideas.', 'About Spin', 'Pre-tax commuter benefits', 'Opportunity to join a fast-growing startup and help shape and establish the company’s industry leadership', 'Experience with Data modeling and warehouse design']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Info Services,"Bristol, CT",1 week ago,Be among the first 25 applicants,[''],Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer ,The Adecco Group,"Jacksonville, FL",3 weeks ago,75 applicants,"['Knowledge of building and optimizing data pipelines, architectures and data sets.', 'We are the workforce experts delivering staffing and career service solutions to organizations and individuals across all industries. Collectively we harness the power of some of the greatest talent in the world. That talent and expertise allows us to do business globally and act locally with deep knowledge in niche areas.\xa0', 'Participating in special projects and performs other duties as assigned.', 'MINIMUM EDUCATION & EXPERIENCE REQUIREMENTS:', 'Implements internal process improvements, automates manual processes, optimizes data delivery and re-designs infrastructure for greater scalability, etc.', 'The Company will consider for employment qualified applicants with arrest and conviction records.', 'KNOWLEDGE, SKILLS & ABILITIES REQUIREMENTS:', 'The Data Engineer is responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. Experienced data pipeline builder and data wrangler that optimizes data systems and building them from the ground up. Supports our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects.', 'Build processes supporting data transformation, data structures, metadata, dependency and workload management.', 'Adecco Group North America, through an impressive portfolio of staffing industry leading brands including Accounting Principals, Adecco General Staffing, Adia, Ajilon, Entegee, Lee Hecht Harrison, Modis, Paladin, Parker+Lynch, Pontoon, Special Counsel and Soliant is the world’s leading provider of Human Resources solutions.\xa0', 'Works with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.', 'Bachelor’s degree in Computer Science, Statistics, Informatics, Information Systems or a related field plus two (2) years minimum experience in a Data Engineering or similar business intelligence, data analytics or supply chain management role required.MBA or Graduate degree in Computer Science, Statistics, Informatics, Information Systems or a related field in lieu of experience accepted.', 'Keeps our data separated and secure across national boundaries through multiple data centers and regions.', 'Strong analytic skills related to working with unstructured datasets.', 'ESSENTIAL DUTIES & RESPONSIBILITIES:', 'Working knowledge of message queuing, stream processing, and highly scalable data stores.', 'Ability to perform root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.', 'SUMMARY:', '\xa0', 'Builds data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.', 'Every day, we have more than 100,000 associates on assignment, 30,000 colleagues working internally to support more than 10,000 clients in the United States and Canada. Ensuring our business units are prepared to deliver outstanding service to our associates and clients, the Adecco Group North America team provides a strong infrastructure through our corporate and shared services teams.\xa0', 'Works with data and analytics experts to strive for greater functionality in our data systems.', 'Creates and maintains optimal data pipeline architecture.Assembles large, complex data sets that meet functional and non-functional business requirements.Implements internal process improvements, automates manual processes, optimizes data delivery and re-designs infrastructure for greater scalability, etc.Operates infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and relevant Azure technologies like Data Factory, Databricks, and similar.Implement analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.Works with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.Keeps our data separated and secure across national boundaries through multiple data centers and regions.Builds data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.Works with data and analytics experts to strive for greater functionality in our data systems.Participating in special projects and performs other duties as assigned.', 'Operates infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and relevant Azure technologies like Data Factory, Databricks, and similar.', 'Responsibilities', 'Working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.', 'Working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.Working knowledge of Scala, Python or R.Knowledge of building and optimizing data pipelines, architectures and data sets.Ability to perform root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.Strong analytic skills related to working with unstructured datasets.Build processes supporting data transformation, data structures, metadata, dependency and workload management.A successful history of manipulating, processing and extracting value from large disconnected datasets.Working knowledge of message queuing, stream processing, and highly scalable data stores.Strong project management and organizational skills.Ability to support and work with cross-functional teams in a dynamic environment.', 'Qualifications', 'Strong project management and organizational skills.', 'MBA or Graduate degree in Computer Science, Statistics, Informatics, Information Systems or a related field in lieu of experience accepted.', 'A successful history of manipulating, processing and extracting value from large disconnected datasets.', 'Implement analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.', 'Equal Opportunity Employer Minorities/Women/Veterans/Disabled', 'Assembles large, complex data sets that meet functional and non-functional business requirements.', 'COMPANY OVERVIEW:\xa0', 'Ability to support and work with cross-functional teams in a dynamic environment.', 'Working knowledge of Scala, Python or R.', 'Creates and maintains optimal data pipeline architecture.', 'Bachelor’s degree in Computer Science, Statistics, Informatics, Information Systems or a related field plus two (2) years minimum experience in a Data Engineering or similar business intelligence, data analytics or supply chain management role required.']",Entry level,Full-time,Information Technology,Information Services,2021-03-24 13:05:10
Quantitative Data Engineer,Jacobs Levy Equity Management,"Florham Park, NJ",1 week ago,76 applicants,"['Broad knowledge of database concepts with proficiency in SQL and stored      procedures, preferably with Microsoft SQL Server ', 'Jacobs Levy Equity Management, an institutional asset manager located in Florham Park, NJ, is seeking a motivated Quantitative Data Engineer to design and implement our proprietary quantitative investment systems. ', '2+ years of solid coding experience in Python, Julia, C++, C# ', ' Implement, enhance, and manage quantitative models  Design and improve proprietary data repository and financial data      platforms  Automate and support the Extract, Transform, and Load (ETL) processes      from various market data vendors  Develop and manage reporting and performance analytics platforms\xa0\xa0 ', 'Be part of a team that leads our future.  You will be a key player on the Technology team and will research, design, code, test and deploy projects. \xa0The Quantitative Data Engineer will work closely with quantitative research and portfolio management professionals to implement new ideas. ', 'Jacobs Levy Equity Management focuses exclusively on managing equity portfolios for institutional clients.  We encourage intellectual curiosity and offer a good work/life balance, competitive compensation, and a collegial environment.', 'Design and improve proprietary data repository and financial data      platforms ', 'Automate and support the Extract, Transform, and Load (ETL) processes      from various market data vendors ', 'Experience in processing large and complex datasets ', 'An advanced knowledge of math and statistics ', 'Responsibilities Include: ', 'Develop and manage reporting and performance analytics platforms\xa0\xa0 ', 'Implement, enhance, and manage quantitative models ', 'MS/PhD in Computer Science, Engineering, Statistics, or related      discipline with excellent academic credentials ', 'Strong knowledge of financial equity data, a plus with experience in      Bloomberg, Thomson Reuters, Compustat, and CapIQ data ', ' ', 'Position Qualifications: ', ' MS/PhD in Computer Science, Engineering, Statistics, or related      discipline with excellent academic credentials  Strong knowledge of financial equity data, a plus with experience in      Bloomberg, Thomson Reuters, Compustat, and CapIQ data  Broad knowledge of database concepts with proficiency in SQL and stored      procedures, preferably with Microsoft SQL Server  2+ years of solid coding experience in Python, Julia, C++, C#  Experience in processing large and complex datasets  An advanced knowledge of math and statistics ']",Mid-Senior level,Full-time,Information Technology,Financial Services,2021-03-24 13:05:10
Data Engineer,City of Olathe,"Olathe, KS",3 weeks ago,Be among the first 25 applicants,"['', 'Experience supporting and working with cross-functional teams in a dynamic environment.', 'Experience using the following software/tools:Three-Five (3-5) years recent experience with RDBMS, specifically SSMS or Oracle.One-Three (1-3) year recent experience with report design and building, specifically SSRS.One-Three (1-3) years recent experience with business intelligence software, specifically PowerBI.One-Three (1-3) years recent experience with ESRI geographic information system software, specifically Esri ArcHub and spatial analysis.One-Three (1-3) years of experience with Python, specifically ArcPy.One-Three (1-3) years of experience with web scripting languages and technology such as PHP, JavaScript, HTML, HTML5, CSS, CSS3 (preferred).Comfort and experience working with Windows or Unix?based (Unix, Mac, Linux) operating systems.', 'Key Responsibilities', 'One-Three (1-3) years recent experience with business intelligence software, specifically PowerBI.', 'Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.', 'Build processes supporting data transformation, data structures, metadata, dependency and workload management.', 'Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.', 'Provide data and analysis support and services, including data visualizations, to strive for greater functionality in our data systems and inform stakeholders about the way government is working.', 'Assemble large, complex data sets that meet functional / non-functional business requirements.', 'Strong analytic skills related to working with unstructured datasets.', 'Build analytics tools that utilize the data pipeline to provide actionable insights into consumer behaviors and future expectations for services, operational efficiency, and other key business performance metrics.', 'One-Three (1-3) year recent experience with report design and building, specifically SSRS.', 'Create, develop and maintain optimal data pipeline architecture for the purposes of research and operations to internal stakeholders, residents, Community Partners, Chamber of Commerce, Businesses and other City partners.', 'Comfort and experience working with Windows or Unix?based (Unix, Mac, Linux) operating systems.', 'Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.Strong analytic skills related to working with unstructured datasets.Build processes supporting data transformation, data structures, metadata, dependency and workload management.A successful history of manipulating, processing and extracting value from large disconnected datasets.Strong project management and organizational skills.Experience supporting and working with cross-functional teams in a dynamic environment.Experience using the following software/tools:Three-Five (3-5) years recent experience with RDBMS, specifically SSMS or Oracle.One-Three (1-3) year recent experience with report design and building, specifically SSRS.One-Three (1-3) years recent experience with business intelligence software, specifically PowerBI.One-Three (1-3) years recent experience with ESRI geographic information system software, specifically Esri ArcHub and spatial analysis.One-Three (1-3) years of experience with Python, specifically ArcPy.One-Three (1-3) years of experience with web scripting languages and technology such as PHP, JavaScript, HTML, HTML5, CSS, CSS3 (preferred).Comfort and experience working with Windows or Unix?based (Unix, Mac, Linux) operating systems.A Bachelor’s degree from a fully accredited institution in Computer Sciences, Statistics, Mathematics, Physics, Business Administration, or another quantitative field. Master’s Degree preferred.', 'One-Three (1-3) years of experience with web scripting languages and technology such as PHP, JavaScript, HTML, HTML5, CSS, CSS3 (preferred).', 'Description', 'Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, RDBMS (SSMS), PowerBI, ESRI geographic information system (Esri ArcHub), etc. ', 'Strong project management and organizational skills.', 'One-Three (1-3) years recent experience with ESRI geographic information system software, specifically Esri ArcHub and spatial analysis.', 'Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.', 'A successful history of manipulating, processing and extracting value from large disconnected datasets.', 'Work with stakeholders including IT, Knowledge Management/GIS and Performance Management teams to assist with data-related technical issues and support data infrastructure needs.', 'Three-Five (3-5) years recent experience with RDBMS, specifically SSMS or Oracle.One-Three (1-3) year recent experience with report design and building, specifically SSRS.One-Three (1-3) years recent experience with business intelligence software, specifically PowerBI.One-Three (1-3) years recent experience with ESRI geographic information system software, specifically Esri ArcHub and spatial analysis.One-Three (1-3) years of experience with Python, specifically ArcPy.One-Three (1-3) years of experience with web scripting languages and technology such as PHP, JavaScript, HTML, HTML5, CSS, CSS3 (preferred).Comfort and experience working with Windows or Unix?based (Unix, Mac, Linux) operating systems.', 'One-Three (1-3) years of experience with Python, specifically ArcPy.', 'Create, develop and maintain optimal data pipeline architecture for the purposes of research and operations to internal stakeholders, residents, Community Partners, Chamber of Commerce, Businesses and other City partners.Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Assemble large, complex data sets that meet functional / non-functional business requirements.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, RDBMS (SSMS), PowerBI, ESRI geographic information system (Esri ArcHub), etc. Build analytics tools that utilize the data pipeline to provide actionable insights into consumer behaviors and future expectations for services, operational efficiency, and other key business performance metrics.Work with stakeholders including IT, Knowledge Management/GIS and Performance Management teams to assist with data-related technical issues and support data infrastructure needs.Provide data and analysis support and services, including data visualizations, to strive for greater functionality in our data systems and inform stakeholders about the way government is working.Perform other related tasks and duties as assigned or required.', 'Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.', 'Perform other related tasks and duties as assigned or required.', 'Three-Five (3-5) years recent experience with RDBMS, specifically SSMS or Oracle.', 'A Bachelor’s degree from a fully accredited institution in Computer Sciences, Statistics, Mathematics, Physics, Business Administration, or another quantitative field. Master’s Degree preferred.']",Entry level,Full-time,Information Technology,Construction,2021-03-24 13:05:10
Data Engineer,Numerator,"Chicago, IL",2 weeks ago,51 applicants,"['', 'An opportunity to have an impact in a technologically data driven company.', 'We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law.', 'Skills & Requirements', 'Experience with a data pipeline scheduling framework (Airflow)', 'An inclusive and collaborative company culture - we work in an open environment while working together to get things done, and adapt to the changing needs as they come.', 'Terraform and/or ansible (or similar) for infrastructure deployment', 'Ownership over platforms and environments of an industry leading product.', 'Advanced proficiency in Python (data structures, algorithms, object oriented programming, using APIs)Experience administering a cloud data warehouse (Redshift, Snowflake, Vertica)', 'Knowledge of software engineering best practices across the development lifecycle, coding standards, code reviews, source management, build processes, testing, and operations', 'Experience with schema design and dimensional data modeling', 'Strong support for career growth, including mentorship programs, leadership training, access to conferences and employee resource groups.', 'Amazon Web Services (EC2, DMS, RDS) experience', 'Great benefits package including health/vision/dental, unlimited PTO, 401k matching, travel reimbursement and more.', ' An inclusive and collaborative company culture - we work in an open environment while working together to get things done, and adapt to the changing needs as they come. An opportunity to have an impact in a technologically data driven company. Ownership over platforms and environments of an industry leading product. Market competitive total compensation package. Volunteer time off and charitable donation matching. Strong support for career growth, including mentorship programs, leadership training, access to conferences and employee resource groups. Regular hackathons to build your own projects and Engineering Lunch and Learns. Great benefits package including health/vision/dental, unlimited PTO, 401k matching, travel reimbursement and more. ', 'Collaborate with product and engineering teams to take requirements from prototype to production', 'Develop expertise in the different upstream data stores and systems across Numerator', 'Design, develop and maintain data integration pipelines for Numerators growing data sets and product offerings', 'What We Offer You', 'Expert in SQL, including advanced analytical queries, window functions, CTEs and query optimization', 'Exceptional candidates will have', 'Knowledge of and experience implementing data security and governance best practices', 'Build data validation testing frameworks to ensure high data quality and integrity', 'Ability to work with team members located in multiple geographies and time zones.', ' Bachelors degree in Computer Science or related field of study required; Masters degree preferred 5 + years of experience in the data warehouse space Knowledge of software engineering best practices across the development lifecycle, coding standards, code reviews, source management, build processes, testing, and operations Knowledge of and experience implementing data security and governance best practices Expert in SQL, including advanced analytical queries, window functions, CTEs and query optimization Advanced proficiency in Python (data structures, algorithms, object oriented programming, using APIs)Experience administering a cloud data warehouse (Redshift, Snowflake, Vertica) Experience with a data pipeline scheduling framework (Airflow) Experience with schema design and dimensional data modeling ', ' Develop expertise in the different upstream data stores and systems across Numerator Design, develop and maintain data integration pipelines for Numerators growing data sets and product offerings Collaborate with product and engineering teams to take requirements from prototype to production Build data validation testing frameworks to ensure high data quality and integrity Write and maintain documentation on data pipelines and schemas ', 'Volunteer time off and charitable donation matching.', 'Write and maintain documentation on data pipelines and schemas', '5 + years of experience in the data warehouse space', 'Market competitive total compensation package.', ' Amazon Web Services (EC2, DMS, RDS) experience Terraform and/or ansible (or similar) for infrastructure deployment Airflow -- Experience building and monitoring DAGs, developing custom operators and using script templating solutions Experience supporting production systems and developing on-call/incident management playbooks Ability to work with team members located in multiple geographies and time zones. Curious and interested in learning about the latest in data warehouse technology Interest and willingness to mentor junior team members ', 'Curious and interested in learning about the latest in data warehouse technology Interest and willingness to mentor junior team members', 'Regular hackathons to build your own projects and Engineering Lunch and Learns.', 'Airflow -- Experience building and monitoring DAGs, developing custom operators and using script templating solutions', 'Duties/Responsibilities Include', 'Bachelors degree in Computer Science or related field of study required; Masters degree preferred', 'Job Description', 'Experience supporting production systems and developing on-call/incident management playbooks']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,FutureSoftIT,"Farmington Hills, MI",1 week ago,35 applicants,"['', 'AWS cloud services: EC2, S3, SQS, Lambda', ' You will be bringing new data into their data warehouse (Snowflake), and help integrate the data into other systems (i.e. Salesforce Marketing Cloud, Adobe Analytics, etc). You will also utilize real time data and analytics (i.e. e-mail campaigns that take consumers to the website). Get data from the systems, read the source files from source systems, do ETL of data, write ETL logic, and load into Data-warehouse. Read files with S3 buckets using python to load into the database Create and maintain optimal data pipeline Help build out data lake Assemble large, complex data sets that meet functional / non-functional business requirements. Interact with the business to understand the data requirements, and what data they need, and also work with the IT teams to gather additional data. Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases AWS cloud services: EC2, S3, SQS, Lambda', 'Location', 'AWS', 'Role', 'SQS', 'Assemble large, complex data sets that meet functional / non-functional business requirements.', 'Keys', ' S3 buckets SQS Lambda ', 'AWS and associated technologies such as: S3 buckets SQS Lambda  ', 'SQL experience - create SQL queries from scratch/modify ', 'S3 buckets', 'Responsibilities', '5-7 years of experience in Data Engineering', 'Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases', ' 5-7 years of experience in Data Engineering Python Coding (MUST HAVE) AWS and associated technologies such as: S3 buckets SQS Lambda   SQL experience - create SQL queries from scratch/modify  Relational database experience ', 'Relational database experience', 'You will be bringing new data into their data warehouse (Snowflake), and help integrate the data into other systems (i.e. Salesforce Marketing Cloud, Adobe Analytics, etc). You will also utilize real time data and analytics (i.e. e-mail campaigns that take consumers to the website).', 'Help build out data lake', 'Read files with S3 buckets using python to load into the database', 'Interact with the business to understand the data requirements, and what data they need, and also work with the IT teams to gather additional data.', 'Lambda', 'Create and maintain optimal data pipeline', 'SQL experience - ', 'Python Coding (MUST HAVE)', 'Get data from the systems, read the source files from source systems, do ETL of data, write ETL logic, and load into Data-warehouse.']",Entry level,Contract,Information Technology,Electrical/Electronic Manufacturing,2021-03-24 13:05:10
Data Engineer,GNS Healthcare,"Somerville, MA",1 month ago,Be among the first 25 applicants,"['', 'Company Overview', 'Experience with data processing frameworks such as Apache Spark, Beam, Dataflow, Crunch, Scalding, Storm, Hive and BigQuery ', 'Supports projects including specific epidemiology, health outcomes and other observational studies to better understand disease natural history, prevalence, comorbidities, treatment patterns, and health and safety outcomes in ‘real world’ patient populations. ', 'Experience developing data frames', 'Background in statistics, biostatistics, public health, research design, health economics, or other related quantitative healthcare field. ', 'Assists team members with the design and development of RWD analyses and predictive models. ', 'An expert in cloud data warehousing tools (e.g. Snowflake, Amazon RedShift, BigQuery, Microsoft SQL Server, Oracle, PostgreSQL, or equivalent) and ELT tools (e.g. Stitch, Fivetran, DBT, Glue). You thrive on building modern, cloud-native data pipelines and operations.', 'Background in statistics, biostatistics, public health, research design, health economics, or other related quantitative healthcare field. Experience with backend web (API) development, Kubernetes, Docker, and Tableau Experience developing data framesExperience with data processing frameworks such as Apache Spark, Beam, Dataflow, Crunch, Scalding, Storm, Hive and BigQuery Experience extracting data from REST APIs and parallel processing large datasets', 'Functions as a healthcare data subject matter expert to support the design, development, testing, implementation, and support of clinical information and intelligence solutions. ', 'Experience using Git/Bitbucket and working on shared code repositories', 'Experience working in healthcare, life sciences, and/or with diverse healthcare data sets (e.g. medical, pharmacy claims, and lab results) ', 'Equal Employment Opportunity', 'Experience using a scripting language (Python, R, Java, Scala, C++, C# and Bash/PowerShell) to automate processes and support proprietary software. Machine learning, R, and python skills highly preferred. ', 'Company Culture', 'Responsibilities', 'Supports production solutions, the ongoing updates, and maintenance of our reference data sources. ', 'Qualifications', 'Experience with backend web (API) development, Kubernetes, Docker, and Tableau ', 'Designs and builds consistent, reproducible, and testable ETL pipelines to ingest, normalize, and store data from large healthcare datasets, from clinical trials, to claims, and EHRs.', 'Experience extracting data from REST APIs and parallel processing large datasets', 'Experience with industry standard measures and code sets (e.g. ICD-10 codes, ICD-9 codes, HCPCS codes, CPT codes, HEDIS metrics, ETGs, HCCs, DRGs, etc.) and publicly available sources (e.g. HCUP) ', 'Experience using Tableau or other in-app data visualization platforms ', 'in silico ', '3-5 years of data engineering experience with a thorough understanding of data lake architectures.An expert in cloud data warehousing tools (e.g. Snowflake, Amazon RedShift, BigQuery, Microsoft SQL Server, Oracle, PostgreSQL, or equivalent) and ELT tools (e.g. Stitch, Fivetran, DBT, Glue). You thrive on building modern, cloud-native data pipelines and operations.Fluent in SQL scripting Experience working in healthcare, life sciences, and/or with diverse healthcare data sets (e.g. medical, pharmacy claims, and lab results) Experience with industry standard measures and code sets (e.g. ICD-10 codes, ICD-9 codes, HCPCS codes, CPT codes, HEDIS metrics, ETGs, HCCs, DRGs, etc.) and publicly available sources (e.g. HCUP) You thrive on mapping and designing ingestion and transformation of data from multiple sources, creating a cohesive data asset and Common Data Model (CDM)Experience using a scripting language (Python, R, Java, Scala, C++, C# and Bash/PowerShell) to automate processes and support proprietary software. Machine learning, R, and python skills highly preferred. Experience using Git/Bitbucket and working on shared code repositoriesExperience using Tableau or other in-app data visualization platforms ', '3-5 years of data engineering experience with a thorough understanding of data lake architectures.', 'Nice To Have Skills', 'Fluent in SQL scripting ', 'You thrive on mapping and designing ingestion and transformation of data from multiple sources, creating a cohesive data asset and Common Data Model (CDM)', 'Assists team members with the design and development of RWD analyses and predictive models. Designs and builds consistent, reproducible, and testable ETL pipelines to ingest, normalize, and store data from large healthcare datasets, from clinical trials, to claims, and EHRs.Supports projects including specific epidemiology, health outcomes and other observational studies to better understand disease natural history, prevalence, comorbidities, treatment patterns, and health and safety outcomes in ‘real world’ patient populations. Functions as a healthcare data subject matter expert to support the design, development, testing, implementation, and support of clinical information and intelligence solutions. Provides complete documentation and communication of all processes, methods, and results. Supports production solutions, the ongoing updates, and maintenance of our reference data sources. ', 'Provides complete documentation and communication of all processes, methods, and results. ']",Mid-Senior level,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Data Engineer,GTL,"Pittsburgh, PA",4 weeks ago,Be among the first 25 applicants,"['', 'Assist in maintaining efficient operation of reports, visualizations and ETL processes within the GTL back-office', 'Support proper data governance processes and policies; implement and/or validate data lineage, quality checks, classification, etc.', 'Solid knowledge of distributed computing', 'GTL, an innovation leader in correctional technology, education solutions that assist in rehabilitating inmates, and payment services solutions for government.\u202f GTL leads the fields of correctional technology, education, and government payment services with visionary solutions and customized products that integrate seamlessly to deliver security, financial value, and operational efficiencies while aiding inmate rehabilitation and reducing recidivism rates.\u202f ', 'Experience building APIs and Webhooks', 'Partner with internal business stakeholders to engineer batch and real-time streaming data pipelines', 'Document your work, system architecture and processes in JIRA, Confluence and other documentation tools.', 'Respond to user requests and document work done on service tickets in ticketing system', 'Partner with DBA and other developers to harness application data and design database structures to obtain best results from the data', 'Responsibilities Include', 'GTL is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee on the basis of race, color, religion, creed, national origin or ancestry, sex, pregnancy or pregnancy-related condition, age, physical or mental disability, veteran or military status, genetic information, sexual orientation, marital status, or any other legally recognized protected basis under federal, state or local laws, regulations or ordinances. The information collected by this application is solely to determine suitability for employment, verify identity and maintain employment statistics on applicants.', 'Generate tracking and monitoring tools to validate data', 'A minimum of 2 years of related experience, to include a minimum of 1 year of work as a Data Engineer, BI Engineer, or Data Warehouse Engineer', 'Required Qualifications', 'Desired Qualifications', '4 Year College Degree in Computer Science, Mathematics, Statistics or a similar quantitative field or comparable experience in lieu of degree', 'Knowledge of Containerization and experience in Kubernetes.', 'Experience with columnar databases like Snowflake, Redshift', 'Familiarity with Business Intelligence concepts', 'Solid analytical and organizational skills, with ability to analyze processes', 'office', 'Troubleshoot data issues, validate result sets, and implement process improvements', 'Experience with relational databases like Oracle, Microsoft SQL Server, MySQL', 'Write GitLab CI/CD Pipelines for infrastructure and for unit testing and deploying code', 'Have a quality mindset and work hard to prevent bugs through unit testing, test-driven development, version control, continuous integration and deployment', ' Partner with internal business stakeholders to engineer batch and real-time streaming data pipelines Partner with DBA and other developers to harness application data and design database structures to obtain best results from the data Develop and maintain data pipelines in Apache Airflow and Apache Spark Write GitLab CI/CD Pipelines for infrastructure and for unit testing and deploying code Assist in maintaining efficient operation of reports, visualizations and ETL processes within the GTL back-office Resolve defects/bugs during QA testing, pre-production, production, and post-release patches Have a quality mindset and work hard to prevent bugs through unit testing, test-driven development, version control, continuous integration and deployment Troubleshoot data issues, validate result sets, and implement process improvements Use acquired information to help build out a big data schema and infrastructure Contribute to the design and architecture of the project Conduct design and code reviews  Operate within Agile Development environment and apply the methodologies Generate tracking and monitoring tools to validate data Review and respond to system alerts in internal and external alerting and metrics systems Document your work, system architecture and processes in JIRA, Confluence and other documentation tools. Respond to user requests and document work done on service tickets in ticketing system Support proper data governance processes and policies; implement and/or validate data lineage, quality checks, classification, etc. ', 'Falls Church, VA', 'Contribute to the design and architecture of the project', 'Conduct design and code reviews ', ' 4 Year College Degree in Computer Science, Mathematics, Statistics or a similar quantitative field or comparable experience in lieu of degree A minimum of 2 years of related experience, to include a minimum of 1 year of work as a Data Engineer, BI Engineer, or Data Warehouse Engineer Proficient to expert level skills in SQL Proficient to expert level skills in Python Experience with relational databases like Oracle, Microsoft SQL Server, MySQL Experience in developing and maintaining ETL processes Experience with AWS services like S3, EMR, EC2 or Azure equivalents Solid analytical and organizational skills, with ability to analyze processes Excellent presentation and communication skills Familiarity with Business Intelligence concepts Solid knowledge of distributed computing ', 'Experience with Informatica ETL software', 'Resolve defects/bugs during QA testing, pre-production, production, and post-release patches', 'Use acquired information to help build out a big data schema and infrastructure', 'Proficient to expert level skills in Python', 'Experience with AWS services like S3, EMR, EC2 or Azure equivalents', 'Experience in developing and maintaining ETL processes', 'Experience working with Business Intelligence software such as Looker, Business Objects and PowerBI', ' Experience in orchestrating data pipelines using Apache Airflow, Snowflake and Spark Knowledge of Containerization and experience in Kubernetes. Experience building APIs and Webhooks Experience with columnar databases like Snowflake, Redshift Experience working with Business Intelligence software such as Looker, Business Objects and PowerBI Experience with Informatica ETL software ', 'Proficient to expert level skills in SQL', 'Develop and maintain data pipelines in Apache Airflow and Apache Spark', 'Review and respond to system alerts in internal and external alerting and metrics systems', 'Experience in orchestrating data pipelines using Apache Airflow, Snowflake and Spark', 'Excellent presentation and communication skills', 'Pittsburg, PA office', 'Operate within Agile Development environment and apply the methodologies']",Mid-Senior level,Full-time,Information Technology,Telecommunications,2021-03-24 13:05:10
Data Engineer/Analyst,"Software Tech Enterprises, Inc.","Baltimore, MD",6 days ago,Be among the first 25 applicants,"['', 'Good understanding with BI development life cycle, data architect, metadata management, data marts and related methodologies.', 'Strong interpersonal skills with ability to collaborate with others effectively and efficiently.', 'Possesses a strong passion for data analytics.', 'Able to multi-task under tight deadlines.', 'STE', 'Experience in BI related fields with progressively increased level of responsibility in data analysis, data modeling, BI development, database development and SQL experiences.Good understanding with BI development life cycle, data architect, metadata management, data marts and related methodologies.Experience in data modeling for operational and data warehousingExperience with data lake, big data, AWS and cloud-based databases, MPP database technologies such as Greenplum and PostgreSQL is a plus familiar with data replication methodology.Experience in analyzing and building reports / dashboards using Tableau is advantageous Proficient in discussing the project architecture with solution architects and Enterprise architect teams.Possesses a strong passion for data analytics.Excellent troubleshooting and problem-solving skills Outstanding SQL skills Proficient with Erwin Data Modeler tools to build data models.Ability to create model from scratch and modify existing complex models as per the Enterprise standards.Able to multi-task under tight deadlines.Ability to work with multiple teams and users for more than one project, at the same time.Strong interpersonal skills with ability to collaborate with others effectively and efficiently.Excellent oral and written communication skills are mandatory', 'Detailed Skills Requirement ', 'Software Tech Enterprises', 'Education', 'Ability to create model from scratch and modify existing complex models as per the Enterprise standards.', 'Excellent oral and written communication skills are mandatory', 'Data Engineer/Analyst. ', 'Excellent troubleshooting and problem-solving skills Outstanding SQL skills Proficient with Erwin Data Modeler tools to build data models.', 'Key Required Skills: ', 'Experience in analyzing and building reports / dashboards using Tableau is advantageous Proficient in discussing the project architecture with solution architects and Enterprise architect teams.', 'Software Tech Enterprises (STE)', 'Experience in BI related fields with progressively increased level of responsibility in data analysis, data modeling, BI development, database development and SQL experiences.', 'Work Location: ', 'Ability to work with multiple teams and users for more than one project, at the same time.', 'Experience with data lake, big data, AWS and cloud-based databases, MPP database technologies such as Greenplum and PostgreSQL is a plus familiar with data replication methodology.', 'Experience in data modeling for operational and data warehousing', 'Required Skills']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Resiliency LLC,"Chantilly, VA",4 weeks ago,Be among the first 25 applicants,"['', 'Ability to multi-task', 'Working experience with ETL processing', 'Creative thinker', ' Working knowledge of entity resolution systems Experience with messages systems like Kafka Experience with NoSQL and/or graph databases like MongoDB or ArangoDB Any of the following databases: SQL, MongoDB, Oracle, Postgres Working experience with ETL processing Working experience with data workflow products like StreamSets or NiFi Working experience with Python RESTful API services, JDBC Experience with Hadoop and Hive/Impala Experience with Cloudera Data Science Workbench is a plus Understanding of pySpark Leadership experience Creative thinker Ability to multi-task Excellent use and understanding of data engineering concepts, principles, and theories ', 'Working knowledge of entity resolution systems', 'Leads technical tasks for small teams or projects', 'Design, develop, implement and maintain data ingestion process from various disparate datasets using StreamSets (experience with StreamSets not mandatory)', ' Support data science team by designing, developing and implementing scalable ETL process for disparate datasets into a Hadoop infrastructure Design, develop, implement and maintain data ingestion process from various disparate datasets using StreamSets (experience with StreamSets not mandatory) Develop processes to identify data drift and malformed records Develop technical documentation and standard operating procedures Mentor new and junior data engineers Leads technical tasks for small teams or projects', 'Mentor new and junior data engineers', 'Understanding of pySpark Leadership experience', 'Responsibilities', 'Working experience with data workflow products like StreamSets or NiFi', 'Clearance Requirements', 'Excellent use and understanding of data engineering concepts, principles, and theories', 'Working experience with Python RESTful API services, JDBC', 'Experience with Hadoop and Hive/Impala', 'Experience with NoSQL and/or graph databases like MongoDB or ArangoDB', 'Develop processes to identify data drift and malformed records', 'Basic Qualifications', 'Develop technical documentation and standard operating procedures', 'Experience with messages systems like Kafka', 'Experience with Cloudera Data Science Workbench is a plus', 'Support data science team by designing, developing and implementing scalable ETL process for disparate datasets into a Hadoop infrastructure', 'Any of the following databases: SQL, MongoDB, Oracle, Postgres']",Mid-Senior level,Full-time,Information Technology,Defense & Space,2021-03-24 13:05:10
"Lead, Data Engineer",Macy's,"New York, NY",2 weeks ago,Be among the first 25 applicants,"['', 'Work with counterparts from Tech to build frameworks that integrate data pipelines and machine learning models that facilitate use by data scientists for priority use cases; Enterprise Data and Analytics team focused on ""last mile"" transformations on select data required for use cases', 'Compliance with relevant laws and regulations , in partnership with Legal/Privacy', 'PC Ch. 9-3500', 'Qualifications And Competencies', ' here. Candidates for positions in Philadelphia ONLY can review their rights and the Company’s obligations under ', "" Community builder to foster data and analytics culture and to support data-driven thinking and discussions across priority analytics use case Embody data-driven culture at Macy's Bolster awareness, knowledge and conviction around data-driven practices and behaviors, increasing digital IQ of Business Units Proactively raise data issues and take action to remediate Support business users in identifying the correct data sets and providing easy to use tools to pull data "", ' Quantity, type, and quality of databases and pipelines, in partnership with Technology Compliance with relevant laws and regulations , in partnership with Legal/Privacy Automation of data cleansing and harmonization processes in refined/trusted zones ', '3-5 years related experience', 'Support business users in identifying the correct data sets and providing easy to use tools to pull data', 'Work with Data Architect to implement the data models, standards and quality rules', 'Ability and desire to take product/project ownership', 'Culture', 'Data Management & Preparation', 'Key Performance Indicators', 'SFPC Art. 49', 'This job description is not all-inclusive, and Macy’s Inc. reserves the right to amend this job description at any time. Macy’s Inc. is an Equal Opportunity Employer and is committed to a diverse and inclusive work environment. Candidates for positions in San Francisco ONLY can review their rights and the Company’s obligations under ', ""Embody data-driven culture at Macy's"", 'Familiarity with data architecture, modeling and security', 'Able to juggle multiple projects - can identify primary and secondary objectives, prioritize time and communicate timeline to team members', ' here. Candidates for positions in Los Angeles ONLY can review their rights and the Company’s obligations under ', 'LA MC Ch. XVIII Art. 9', ""Maintain database structure and standard definitions for business users across Macy's"", 'Ability to think creatively, strategically and technically', 'Ability to work a flexible schedule based on department and Company needs.', 'Job Overview', 'Work with the Data Science team to understand data formatting and sourcing needs to enable them to build out use cases as efficiently as possible', "" Bachelor's Degree from a 4-year college or university 3-5 years related experience "", 'Work with data architects to build the foundational extract / load / transform process and regularly review the architecture and recommend effectiveness improvements', ' 5+ years of work experience, and at least 3 years of demonstrable experience in the role’s relevant technologies and field of expertise Ability to write production level code, assess databases and leverage Big Data technologies on Google Cloud Platform to support downstream analytics Familiarity with data architecture, modeling and security Experience in distributed computing and enterprise environment Ability to effectively share technical information, communicate technical issues and solutions to all levels of business Able to juggle multiple projects - can identify primary and secondary objectives, prioritize time and communicate timeline to team members Ability and desire to take product/project ownership Ability to think creatively, strategically and technically Ability to work a flexible schedule based on department and Company needs. ', ""Bachelor's Degree from a 4-year college or university"", 'Work with Legal and Privacy teams to adhere to data privacy and security standards', 'Ability to effectively share technical information, communicate technical issues and solutions to all levels of business', 'Proactively raise data issues and take action to remediate', 'About', ' Adhere to processes to ensure data pulled from various sources meets quality standards, is curated and enhanced for analytical use and there is a ""single source of truth"" Work with counterparts from Tech to build frameworks that integrate data pipelines and machine learning models that facilitate use by data scientists for priority use cases; Enterprise Data and Analytics team focused on ""last mile"" transformations on select data required for use cases Maintain database structure and standard definitions for business users across Macy\'s Work with data architects to build the foundational extract / load / transform process and regularly review the architecture and recommend effectiveness improvements Collaborate with Technology to future-proof data & analytics software, tools and code to reduce risk and support pipeline owners Work with Legal and Privacy teams to adhere to data privacy and security standards Work with Data Architect to implement the data models, standards and quality rules Work with the Data Science team to understand data formatting and sourcing needs to enable them to build out use cases as efficiently as possible ', '5+ years of work experience, and at least 3 years of demonstrable experience in the role’s relevant technologies and field of expertise', 'Community builder to foster data and analytics culture and to support data-driven thinking and discussions across priority analytics use case', 'Bolster awareness, knowledge and conviction around data-driven practices and behaviors, increasing digital IQ of Business Units', 'Ability to write production level code, assess databases and leverage Big Data technologies on Google Cloud Platform to support downstream analytics', 'Essential Functions', 'Automation of data cleansing and harmonization processes in refined/trusted zones', ' here.', 'Collaborate with Technology to future-proof data & analytics software, tools and code to reduce risk and support pipeline owners', 'Adhere to processes to ensure data pulled from various sources meets quality standards, is curated and enhanced for analytical use and there is a ""single source of truth""', 'Quantity, type, and quality of databases and pipelines, in partnership with Technology', 'Experience in distributed computing and enterprise environment']",Not Applicable,Full-time,Information Technology,Apparel & Fashion,2021-03-24 13:05:10
Python Data Engineer,Cbit Technologies,"United, LA",2 weeks ago,Be among the first 25 applicants,[''],Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Junior Data Engineer,NFI,"Chicago, IL",2 weeks ago,38 applicants,"['', 'Take ownership of the production release and operation of your services', 'Comfortable using a Mac and working in a Linux environment', 'Implement the domain model in Elixir services being careful to avoid unnecessary coupling', 'Document the domain model as it evolves', 'Product', 'Experience using Git', 'Data Technologies', '2+ years of professional software development experience (or comparable)', 'Modern RDBMS experience mandatory', ' 2+ years of professional software development experience (or comparable) Modern RDBMS experience mandatory Elixir and ecto_sql experience strongly preferred Familiarity with DDD principles is a huge plus R or python experience a plus Comfortable using a Mac and working in a Linux environment Experience using Git ', 'Integrate with 3rd party APIs', 'Collaborate with domain experts, product managers, and other developers to refine a data model following DDD principles', 'Requirements', 'Familiarity with DDD principles is a huge plus', 'R or python experience a plus', ' Collaborate with legacy system stakeholders, domain experts, product managers, and other developers to manipulate existing data sources into purpose-driven models Collaborate with domain experts, product managers, and other developers to refine a data model following DDD principles Document the domain model as it evolves Implement the domain model in Elixir services being careful to avoid unnecessary coupling Implement unit tests and integration tests Integrate with 3rd party APIs Take ownership of the production release and operation of your services ', 'Implement unit tests and integration tests', 'Design', 'Collaborate with legacy system stakeholders, domain experts, product managers, and other developers to manipulate existing data sources into purpose-driven models', 'Essential Duties & Responsibilities', 'Elixir and ecto_sql experience strongly preferred']",Associate,Full-time,Information Technology,Construction,2021-03-24 13:05:10
"Manager, Data Engineer",Ally,"Charlotte, NC",2 weeks ago,Be among the first 25 applicants,"['', 'Building a Family:', 'Building a Family: adoption, surrogacy, and fertility support as well as parental and caregiver leave, back-up child and adult/elder day care program and child care discounts.', 'Work-Life Integration: other benefits including LifeMatters® Employee Assistance Program, subsidized and discounted Weight Watchers® program and other employee discount programs.', 'Strong verbal and written communication skills', 'Who We Are', 'Solutions analysis and design, balancing technical and business factors.', 'Strong knowledge of DevSecOps pipeline creation, maturity, and operations', 'Ally and Your Career', 'Mid-level business partner engagement; management of relationship and inter-group planning among technology leadership/peers.', 'Ability to multi-task and manage through complexity', ' Time Away: competitive holiday and flexible paid-time-off, including time off for volunteering and voting. Planning for the Future: plan for the near and long term with an industry-leading 401K retirement savings plan with matching and company contributions, student loan and 529 educational assistance programs, tuition reimbursement, and other financial well-being programs. Supporting your Health & Well-being: flexible health and insurance options including dental and vision, pre-tax Health Savings Account with employer contributions and a total well-being program that helps you and your family stay on track physically, socially, emotionally and financially. Building a Family: adoption, surrogacy, and fertility support as well as parental and caregiver leave, back-up child and adult/elder day care program and child care discounts. Work-Life Integration: other benefits including LifeMatters® Employee Assistance Program, subsidized and discounted Weight Watchers® program and other employee discount programs. ', 'Time Away: competitive holiday and flexible paid-time-off, including time off for volunteering and voting.', 'Strong knowledge and experience with one or more SDLC methodologies and/or principles such as Agile Scrum or Kanban, or other related Agile methods', ""Bachelor's degree in Computer Science, Information Systems, or other relevant field of study."", 'The Opportunity', 'The Skills You Bring', 'Planning for the Future:', 'Experience related to data warehousing design and development', 'The Work Itself', 'Vendor management for service and software delivery.', 'Supporting your Health & Well-being:', 'Strong analytic and problem-solving skills', 'Management of team progression and maturity and Agile delivery and DevOps/CI-CD automation.', 'Time Away', 'Ability to lead, mentor and manage individual contributors', 'Management of team capacity, talent, learning & growth, and financials.', 'Supporting your Health & Well-being: flexible health and insurance options including dental and vision, pre-tax Health Savings Account with employer contributions and a total well-being program that helps you and your family stay on track physically, socially, emotionally and financially.', "" Bachelor's degree in Computer Science, Information Systems, or other relevant field of study. 5+ years of data engineering and data analysis experience.Must have experience building Data Warehouse based on DV-2 Data Vaulting model Experience related to data warehousing design and development Strong knowledge and experience with one or more SDLC methodologies and/or principles such as Agile Scrum or Kanban, or other related Agile methods Strong knowledge of DevSecOps pipeline creation, maturity, and operations Ability to lead, mentor and manage individual contributors Strong requirements gathering and design abilities Strong analytic and problem-solving skills Strong verbal and written communication skills Ability to multi-task and manage through complexity "", 'Must have experience building Data Warehouse based on DV-2 Data Vaulting model', 'How We’ll Have Your Back', '5+ years of data engineering and data analysis experience.', ' Managing engineers and analytics professionals to design and deliver solutions in an Agile context.  Management of team progression and maturity and Agile delivery and DevOps/CI-CD automation. Solutions analysis and design, balancing technical and business factors. High-level and low-level design of real time and batch system interfaces and data enrichment pipelines in a cloud-native data ecosystem. Troubleshooting and support of data & analytics solutions end-to-end. Promoting and enforcing design and development standards and best practices. Mid-level business partner engagement; management of relationship and inter-group planning among technology leadership/peers. Vendor management for service and software delivery. Management of team capacity, talent, learning & growth, and financials. ', 'High-level and low-level design of real time and batch system interfaces and data enrichment pipelines in a cloud-native data ecosystem.', 'Strong requirements gathering and design abilities', 'Planning for the Future: plan for the near and long term with an industry-leading 401K retirement savings plan with matching and company contributions, student loan and 529 educational assistance programs, tuition reimbursement, and other financial well-being programs.', 'Troubleshooting and support of data & analytics solutions end-to-end.', 'Work-Life Integration: ', 'Promoting and enforcing design and development standards and best practices.', 'Managing engineers and analytics professionals to design and deliver solutions in an Agile context. ']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Certilytics,"Louisville, KY",4 weeks ago,Be among the first 25 applicants,"['', ' Provable SQL knowledge and experience working with a variety of databases Successful history of manipulating, transforming, and extracting value from large data sets Strong project management and organizational skills Knowledge of programming languages (Python and Java) Ability to work with various functional teams in a dynamic environment Excellent written and verbal communication Strong collaborative drive Experience designing, building, and maintaining big data pipelines Ability to function autonomously and with partners and teammates across several time zones Technical mindset and desire to learn new tools and approaches Experience working with healthcare data desired Experience with big data tools: Hadoop, Hive, Spark, Yarn desired ', 'Respond to operational data pipeline failures with a sense of ownership and an eye toward continuous improvement', 'Sedentary work that primarily involves sitting/standing in-front of a computer', 'Generate accurate and effective documentation', 'Excellent written and verbal communication', 'Identify, design, implement, and own internal process improvements', 'Other duties as assigned', 'Optimize and troubleshoot complex joins across massive data sets (billions of records)', 'Create and maintain data pipelines that support internal and external clients', 'Work with data experts to strive for greater functionality in our data platform', 'Technical mindset and desire to learn new tools and approaches', 'Strong project management and organizational skills', 'Evaluate logs across a wide range of complex systems', 'Successful history of manipulating, transforming, and extracting value from large data sets', 'Assemble accurate large, complex data sets that meet business requirements', ' Ability to work remotely without interruptions Sedentary work that primarily involves sitting/standing in-front of a computer ', 'Responsibilities/Accountabilities', 'Provable SQL knowledge and experience working with a variety of databases', 'Experience designing, building, and maintaining big data pipelines', 'Experience with big data tools: Hadoop, Hive, Spark, Yarn desired', 'Knowledge of programming languages (Python and Java)', 'Ability to work with various functional teams in a dynamic environment', 'Ability to function autonomously and with partners and teammates across several time zones', 'Automate manual processes, and optimize data delivery and ingestion', ' Create and maintain data pipelines that support internal and external clients Optimize and troubleshoot complex joins across massive data sets (billions of records) Identify, design, implement, and own internal process improvements Automate manual processes, and optimize data delivery and ingestion Respond to operational data pipeline failures with a sense of ownership and an eye toward continuous improvement Assemble accurate large, complex data sets that meet business requirements Evaluate logs across a wide range of complex systems Work with data experts to strive for greater functionality in our data platform Generate accurate and effective documentation Other duties as assigned ', 'Ability to work remotely without interruptions', 'Experience working with healthcare data desired', 'Strong collaborative drive', 'Required Skills']",Not Applicable,Full-time,Engineering,Mechanical or Industrial Engineering,2021-03-24 13:05:10
Data Engineer - Development - Remote ,"Oncology Analytics, Inc.",United States,5 days ago,Be among the first 25 applicants,"['', 'Skills:\xa0Analyzing information, software design, software documentation, and testing are required skills, as well as general programming skills and software development fundamentals, development process, and requirements. Outstanding written and verbal communication skills and comfortable presenting ideas to peers and across the company.', 'Social and Community events monthly with your peers', 'Job Summary\xa0', '4-7 years’ experience with big data platforms like Azure Data Lakes, Data Lakes Analytics, Azure Machine Learning, Snowflake, CosmosDB, Synapse, Databricks, Spark/PySpark, Kafka, Hadoop, and Cloudera.', 'Key Works:\xa0Oncology Analytics, Data Engineer,\xa0Azure, Data Lakes, Blob, Azure Services, Python, .NET, Cloud, Pipelines, Remote, Work from Home', 'Oncology Analytics provides health plans, providers, and patients with a data-driven, utilization management solution that delivers real-world, evidence-based analytics focused exclusively for oncology. Used by physicians to support almost 5 million health plan members in the US and Puerto Rico, the Oncology Analytics e-Prior Authorization platform covers the full spectrum of therapeutics, across all cancer types and stages, including chemotherapy, radiation therapy, precision medicine, targeted therapy, and supportive care.', 'Own the relationship with internal teams to provide data and servicesIntegrate NLP and other data together using automated pipelineRedesign current flows to increase throughput and streamline data for\xa0downstream consumers', 'Compensation/Benefits:', 'Attractive compensation package, including a competitive base, bonus incentives, and company equity.Full benefits package starting day 1Retirement savings starting within your first month.Social and Community events monthly with your peersCareer growth with an industry innovator.', 'Work with members of the Data Engineering, DevOps and other teams in order to understand gaps in automation and fill as needed', 'About Oncology Analytics', '3-5 years’ experience working with automation using cloud based tools and services.', 'The Data Engineer works in an agile environment while demonstrating a strong skillset with both SQL and NOSQL solutions as well as possessing strong technical acumen with Python, Spark, JSON working in a healthcare environment. This individual will leverage the cloud (Azure) for setting up pipelines in enterprise data warehousing, business intelligence, and data wrangling with ETL/ELT combined with knowledge of healthcare data and clinical coding systems. This role will be involved in making technology choices to support the overall OA data pipeline build and data management services.', 'Microsoft Azure based tools and services', '3-5 years’ of experience in custom ETL design, implementation/maintenance, data warehouse, schema design, and data modeling.', 'Key Works:\xa0', 'Experience:', '4-7 years’ experience working with Python or .NET and Spark development', 'Implement and support platforms that can work with large datasets and unstructured data', 'Redesign current flows to increase throughput and streamline data for\xa0downstream consumers', 'Bachelor’s degree in Computer Science or relevant experience required.\xa04-7 years’ experience in using SQL and/or NOSQL databases in a healthcare enterprise environment.4-7 years’ experience in a cloud environment with Azure strongly preferred, will consider experience AWS,\xa0GCS or Private Cloud as well4-7 years’ experience working with Python or .NET and Spark development4-7 years’ experience with big data platforms like Azure Data Lakes, Data Lakes Analytics, Azure Machine Learning, Snowflake, CosmosDB, Synapse, Databricks, Spark/PySpark, Kafka, Hadoop, and Cloudera.3-5 years’ of experience working with Microsoft tools such as Windows servers, SQL server, .NET framework and .NET core.3-5 years’ of experience working with Analysis Services (SSAS), Reporting Services (SSRS), Integration Services (SSIS).3-5 years’ of experience in custom ETL design, implementation/maintenance, data warehouse, schema design, and data modeling.3-5 years’ experience working with automation using cloud based tools and services.Any knowledge in healthcare data is a plus', 'Integrate NLP and other data together using automated pipeline', 'Leverage native Azure tools such as Synapse, Lake Storage, PySpark Notebook, Databricks, and Azure Data Factory to build ETL/ELT jobs from Synapse in order to automate data flows', 'Retirement savings starting within your first month.', 'Develop reusable/scalable code and pipelines process for data collection, ingestion, extraction, transformation, and operation usingMicrosoft Azure based tools and servicesBuild unit and full testing mechanisms to assure quality of code to be implemented in pipelinesMaintain and refactor existing code to maximize data usability and consistency across different business functionsImplement and support platforms that can work with large datasets and unstructured dataLeverage Azure Data Lake/Delta Lake and/or blob storage to prepare large unstructured datasets for consumption by data science or clinical staffLeverage native Azure tools such as Synapse, Lake Storage, PySpark Notebook, Databricks, and Azure Data Factory to build ETL/ELT jobs from Synapse in order to automate data flowsWork with members of the Data Engineering, DevOps and other teams in order to understand gaps in automation and fill as neededDevelop and design solutions by studying information needs; conferring with users; studying flow, data usage, and work processes; investigating problem areas; and following the Agile development lifecycle', 'Attractive compensation package, including a competitive base, bonus incentives, and company equity.', 'Responsibilities', '4-7 years’ experience in using SQL and/or NOSQL databases in a healthcare enterprise environment.', '1.\xa0Design, develop, test, and implement data pipeline', 'Full benefits package starting day 1', 'Own the relationship with internal teams to provide data and services', 'Career growth with an industry innovator.', 'Develop reusable/scalable code and pipelines process for data collection, ingestion, extraction, transformation, and operation using', 'Skills:', 'Develop and design solutions by studying information needs; conferring with users; studying flow, data usage, and work processes; investigating problem areas; and following the Agile development lifecycle', 'Bachelor’s degree in Computer Science or relevant experience required.\xa0', '2.\xa0Build data expertise and own data quality for allocated areas of ownership', 'Any knowledge in healthcare data is a plus', 'Maintain and refactor existing code to maximize data usability and consistency across different business functions', 'Leverage Azure Data Lake/Delta Lake and/or blob storage to prepare large unstructured datasets for consumption by data science or clinical staff', '3-5 years’ of experience working with Microsoft tools such as Windows servers, SQL server, .NET framework and .NET core.', 'Build unit and full testing mechanisms to assure quality of code to be implemented in pipelines', '3-5 years’ of experience working with Analysis Services (SSAS), Reporting Services (SSRS), Integration Services (SSIS).', '4-7 years’ experience in a cloud environment with Azure strongly preferred, will consider experience AWS,\xa0GCS or Private Cloud as well']",Mid-Senior level,Full-time,Engineering,Hospital & Health Care,2021-03-24 13:05:10
Data Engineer,Mozilla,"Maryland, United States",17 hours ago,Be among the first 25 applicants,"['', 'You will work with other data engineers to design and maintain scalable data models and ETL pipelines.', 'We Have Multiple Openings For The Following', 'Proficiency with one or more of the programming languages used by our teams (SQL, Python, Java, Rust).', 'Ability to work collaboratively with a distributed team.', 'Commitment to diversity, equity, inclusion, and belonging', ' Strong CS fundamentals: data structures, algorithms, etc. Proficiency with one or more of the programming languages used by our teams (SQL, Python, Java, Rust). Ability to work collaboratively with a distributed team. Ability to write and speak English well. ', 'You have experience with data systems: Databases, message queues, batch and stream processing', 'You will help design, build, and improve the infrastructure for ingesting, storing, and transforming data at a scale of tens of terabytes per day.', 'Specific Skills/Experience', 'You have experience with highly scalable distributed systems hosted on cloud providers (e.g. Google Cloud Platform)', 'You have a working knowledge of web development technologies: HTML, Javascript, CSS, HTTP.', 'General Professional Requirements', 'You will help design and build systems to monitor and analyze data from Mozilla’s products. See https://telemetry.mozilla.org', 'Ability to write and speak English well.', ' You have experience with data systems: Databases, message queues, batch and stream processing You have experience with highly scalable distributed systems hosted on cloud providers (e.g. Google Cloud Platform) You have a working knowledge of web development technologies: HTML, Javascript, CSS, HTTP. ', 'About Mozilla', 'You will work with data scientists to answer questions and guide product decisions.', 'Strong CS fundamentals: data structures, algorithms, etc.', 'The Role', ' You will help design, build, and improve the infrastructure for ingesting, storing, and transforming data at a scale of tens of terabytes per day. You will help design and build systems to monitor and analyze data from Mozilla’s products. See https://telemetry.mozilla.org You will work with other data engineers to design and maintain scalable data models and ETL pipelines. You will work with data scientists to answer questions and guide product decisions. ']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Apple,"Cupertino, CA",5 days ago,42 applicants,"['', 'Description', 'Education & Experience', 'Summary', 'Key Qualifications']",Not Applicable,Full-time,Information Technology,Consumer Electronics,2021-03-24 13:05:10
Data Engineer,Lockheed Martin,"Arlington, VA",1 week ago,Be among the first 25 applicants,"['', ' Must be a US Citizen', ' Enable data pipelines to provide clean, automated, reproducible data sets for consumption by AI/ML workloads', ' Maintain data systems performance by identifying and resolving production and application development problems; calculating optimum values for parameters; evaluating, integrating, and installing new releases', ' Integrate data into machine learning libraries and operational frameworks into an end-to-end environment', ' Work with AI/ML practitioners to solve complex problems and create unique solutions', ' Work with AI/ML practitioners to solve complex problems and create unique solutions Work through all stages of a data solution lifecycle, e.g., analyze/profile data, create conceptual, logical and physical data model designs, architect and design ETL, testing, reporting and analytics Responsible for collecting, ingesting, processing, storing, and virtualizing large datasets from a wide variety of data sources and stakeholders Enable data pipelines to provide clean, automated, reproducible data sets for consumption by AI/ML workloads Integrate data into machine learning libraries and operational frameworks into an end-to-end environment Maintain data systems performance by identifying and resolving production and application development problems; calculating optimum values for parameters; evaluating, integrating, and installing new releases Participate in on-call rotations to monitor and resolve production issues during off-hours Provides ongoing support, monitoring, and maintenance of deployed solutions Collaborate with data architects, data scientists, data analysts, and system engineers to develop a solution across the entire data processing pipeline Must be a US Citizen', ' Provides ongoing support, monitoring, and maintenance of deployed solutions', ' Collaborate with data architects, data scientists, data analysts, and system engineers to develop a solution across the entire data processing pipeline', ' Responsible for collecting, ingesting, processing, storing, and virtualizing large datasets from a wide variety of data sources and stakeholders', ' Participate in on-call rotations to monitor and resolve production issues during off-hours', ' Work through all stages of a data solution lifecycle, e.g., analyze/profile data, create conceptual, logical and physical data model designs, architect and design ETL, testing, reporting and analytics', 'The Selected Candidate Will']",Entry level,Full-time,Information Technology,Construction,2021-03-24 13:05:10
Data Engineer III,Walmart,"Dallas, TX",1 day ago,Be among the first 25 applicants,"['', ""Problem Formulation: Identifies possible options to address the business problems within one's discipline through analytics, big data analytics, and automation."", 'Data Modeling: Analyzes complex data elements, systems, data flows, dependencies, and relationships to contribute to conceptual, physical, and logical data models. Develops the Logical Data Model and Physical Data Models including data warehouse and data mart designs. Defines relational tables, primary and foreign keys, and stored procedures to create a data model structure. Evaluates existing data models and physical databases for variances and discrepancies. Develops efficient data flows. Analyzes data-related system integration challenges and proposes appropriate solutions.', 'Outlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications. ', 'Demonstrates up-to-date expertise and applies this to the development, execution, and improvement of action plans by providing expert advice and guidance to others in the application of information and best practices; supporting and aligning efforts to meet customer and business needs; and building commitment for perspectives and rationales.', 'Data Strategy: Understands, articulates, and applies principles of the defined strategy to routine business problems that involve a single function.', 'Code Development and Testing: Writes code to develop the required solution and application features by determining the appropriate programming language and leveraging business, technical, and data requirements. Creates test cases to review and validate the proposed solution design. Creates proofs of concept. Tests the code using the appropriate testing approach. Deploys software to production servers. Contributes code documentation, maintains playbooks, and provides timely progress updates.', 'Creates training documentation and trains end-users on data modeling. Oversees the tasks of less experienced programmers and stipulates system troubleshooting supports.', 'Data Governance: Supports the documentation of data governance processes. Supports the implementation of data governance practices.', 'Provides and supports the implementation of business solutions by building relationships and partnerships with key stakeholders; identifying business needs; determining and carrying out necessary processes and practices; monitoring progress and results; recognizing and capitalizing on improvement opportunities; and adapting to competing demands, organizational changes, and new responsibilities.', ""Position Summary... What You'll Do..."", 'Data Source Identification: Supports the understanding of the priority order of requirements and service level agreements. Helps identify the most suitable source for data that is fit for purpose. Performs initial data quality checks on extracted data.', ""Problem Formulation: Identifies possible options to address the business problems within one's discipline through analytics, big data analytics, and automation.Applied Business Acumen: Supports the development of business cases and recommendations. Owns delivery of project activity and tasks assigned by others. Supports process updates and changes. Solves business issues.Data Governance: Supports the documentation of data governance processes. Supports the implementation of data governance practices.Data Strategy: Understands, articulates, and applies principles of the defined strategy to routine business problems that involve a single function.Data Transformation and Integration: Extracts data from identified databases. Creates data pipelines and transform data to a structure that is relevant to the problem by selecting appropriate techniques. Develops knowledge of current data science and analytics trends.Data Source Identification: Supports the understanding of the priority order of requirements and service level agreements. Helps identify the most suitable source for data that is fit for purpose. Performs initial data quality checks on extracted data.Data Modeling: Analyzes complex data elements, systems, data flows, dependencies, and relationships to contribute to conceptual, physical, and logical data models. Develops the Logical Data Model and Physical Data Models including data warehouse and data mart designs. Defines relational tables, primary and foreign keys, and stored procedures to create a data model structure. Evaluates existing data models and physical databases for variances and discrepancies. Develops efficient data flows. Analyzes data-related system integration challenges and proposes appropriate solutions.Creates training documentation and trains end-users on data modeling. Oversees the tasks of less experienced programmers and stipulates system troubleshooting supports.Code Development and Testing: Writes code to develop the required solution and application features by determining the appropriate programming language and leveraging business, technical, and data requirements. Creates test cases to review and validate the proposed solution design. Creates proofs of concept. Tests the code using the appropriate testing approach. Deploys software to production servers. Contributes code documentation, maintains playbooks, and provides timely progress updates.Demonstrates up-to-date expertise and applies this to the development, execution, and improvement of action plans by providing expert advice and guidance to others in the application of information and best practices; supporting and aligning efforts to meet customer and business needs; and building commitment for perspectives and rationales.Provides and supports the implementation of business solutions by building relationships and partnerships with key stakeholders; identifying business needs; determining and carrying out necessary processes and practices; monitoring progress and results; recognizing and capitalizing on improvement opportunities; and adapting to competing demands, organizational changes, and new responsibilities.Models compliance with company policies and procedures and supports company mission, values, and standards of ethics and integrity by incorporating these into the development and implementation of business plans; using the Open Door Policy; and demonstrating and assisting others with how to apply these in executing business processes and practices."", 'Applied Business Acumen: Supports the development of business cases and recommendations. Owns delivery of project activity and tasks assigned by others. Supports process updates and changes. Solves business issues.', 'Data Transformation and Integration: Extracts data from identified databases. Creates data pipelines and transform data to a structure that is relevant to the problem by selecting appropriate techniques. Develops knowledge of current data science and analytics trends.', 'Models compliance with company policies and procedures and supports company mission, values, and standards of ethics and integrity by incorporating these into the development and implementation of business plans; using the Open Door Policy; and demonstrating and assisting others with how to apply these in executing business processes and practices.', 'Outlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications. ']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Amazon,"Seattle, WA",17 hours ago,Be among the first 25 applicants,"['', ' Ability to communicate clearly and concisely with technical and non-technical customers in order to understand ambiguous problems and articulate technical designs and solutions to complex problem ', 'Company', ' Experience with Object Oriented programming (e.g. Java, Scala, Python)', ' Experience using big data technologies (Hadoop, Hive, Hbase, Spark, EMR, etc.)', 'Preferred Qualifications', ' Experience with Machine Learning/deep learning development projects', ' High attention to detail and proven ability to manage multiple, competing priorities simultaneously', ' Love to get your hands dirty and solve challenging technical issues?', ' BS/MS in Computer Science or equivalent industry experience Domain knowledge of Distributed SOA Architecture, Relational DB knowledge, ElasticSearch, DynamoDB, and various AWS technologies 5+ years of experience as a Data Engineer, BI Engineer, Systems Analyst in a company with large, complex data sources 3+ years’ full-time experience working on a software design and development team Experience with Object Oriented programming (e.g. Java, Scala, Python) Experience using big data technologies (Hadoop, Hive, Hbase, Spark, EMR, etc.) Experience building complex software systems that have been successfully delivered to customers Experience with building high-performance, highly-available and scalable distributed systems Experience successfully mentoring junior DEs Experience with Machine Learning/deep learning development projects High attention to detail and proven ability to manage multiple, competing priorities simultaneously Passion for building great solutions which directly impact customers Experience working in a fast-paced environment where continuous innovation is desired Amazon is committed to a diverse and inclusive workplace', ' 3+ years’ full-time experience working on a software design and development team', ' Experience building complex software systems that have been successfully delivered to customers', ' 5+ years of experience as a Data Engineer, BI Engineer, Systems Analyst in a company with large, complex data sources', ' Are you excited about working directly to empower users? Love to get your hands dirty and solve challenging technical issues?', ' 3+ years of experience as a Data Engineer or in a similar role', ' 3+ years of experience as a Data Engineer or in a similar role Experience with data modeling, data warehousing, and building ETL pipelines Experience in SQL', ' Experience of having built and delivered multiple large scale, cross-functional projects ', 'Description', ' Experience with data modeling, data warehousing, and building ETL pipelines', ' Experience with building high-performance, highly-available and scalable distributed systems', ' Passion for building great solutions which directly impact customers', ' Experience in SQL', ' Are you excited about working directly to empower users?', ' Experience of distributed systems as it pertains to data storage and computing ', ' Experience with agile methodologies, coding standards, code reviews, source control management, build processes, testing, and operationsPreferred Qualifications BS/MS in Computer Science or equivalent industry experience Domain knowledge of Distributed SOA Architecture, Relational DB knowledge, ElasticSearch, DynamoDB, and various AWS technologies 5+ years of experience as a Data Engineer, BI Engineer, Systems Analyst in a company with large, complex data sources 3+ years’ full-time experience working on a software design and development team Experience with Object Oriented programming (e.g. Java, Scala, Python) Experience using big data technologies (Hadoop, Hive, Hbase, Spark, EMR, etc.) Experience building complex software systems that have been successfully delivered to customers Experience with building high-performance, highly-available and scalable distributed systems Experience successfully mentoring junior DEs Experience with Machine Learning/deep learning development projects High attention to detail and proven ability to manage multiple, competing priorities simultaneously Passion for building great solutions which directly impact customers Experience working in a fast-paced environment where continuous innovation is desired Amazon is committed to a diverse and inclusive workplaceAmazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.Company - Amazon.com Services LLCJob ID: A1490122', ' Willingness to dive deep, experiment rapidly and get things done ', ' Experience working in a fast-paced environment where continuous innovation is desired Amazon is committed to a diverse and inclusive workplace', 'Basic Qualifications', ' Domain knowledge of Distributed SOA Architecture, Relational DB knowledge, ElasticSearch, DynamoDB, and various AWS technologies', ' Experience successfully mentoring junior DEs', ' BS/MS in Computer Science or equivalent industry experience']",Not Applicable,Full-time,Strategy/Planning,Computer Software,2021-03-24 13:05:10
Data Engineer II,Microsoft,"Redmond, WA",2 days ago,27 applicants,"['', 'Build and maintain reports and dashboards that monitor KPIs, the identity attack ecosystem and other indicators that help us understand the threat landscape and the accuracy and performance of our detections.', 'Design, develop, and maintain data pipelines and back-end services for ML experimentation, investigations, reporting, data collection, and related functions.', 'Requirements', ""3+ years' experience in an Analytics or Data Engineer or Software Engineer role with focus on big data processing"", 'Experience with relational databases as well as working familiarity with a variety of big data sources in Scope, Spark, Scala and/or other big data systems.', ""3+ years' experience in an Analytics or Data Engineer or Software Engineer role with focus on big data processingExperience building and optimizing “big data” data workflows including ETL, business intelligence pipelines and data visualization tools such as Power BI."", 'Build and maintain the team’s grading pipeline for data labelling.', 'Demonstrated proficiency in writing complex highly optimized queries across diverse data sets and implementing data science applications in R and Python using cloud-based tools and notebooks (Jupyter, DataBricks, etc). ', 'Responsibilities', 'Help the team answer business questions with telemetry datCommunicate key insights and results with stakeholders and leadership. Interpret data/insights and uses storytelling and data visualization to recommend product decisions', 'Experience building and optimizing “big data” data workflows including ETL, business intelligence pipelines and data visualization tools such as Power BI.', 'Preferred Qualifications', 'Qualifications', 'Design, develop, and maintain data pipelines and back-end services for ML experimentation, investigations, reporting, data collection, and related functions.Build and maintain reports and dashboards that monitor KPIs, the identity attack ecosystem and other indicators that help us understand the threat landscape and the accuracy and performance of our detections.Build and maintain the team’s grading pipeline for data labelling.Help the team answer business questions with telemetry datCommunicate key insights and results with stakeholders and leadership. Interpret data/insights and uses storytelling and data visualization to recommend product decisions', 'Experience with relational databases as well as working familiarity with a variety of big data sources in Scope, Spark, Scala and/or other big data systems.Demonstrated proficiency in writing complex highly optimized queries across diverse data sets and implementing data science applications in R and Python using cloud-based tools and notebooks (Jupyter, DataBricks, etc). Excellent written and verbal communication skills', 'Excellent written and verbal communication skills']",Not Applicable,Full-time,Information Technology,Computer Hardware,2021-03-24 13:05:10
Data Engineer,Riskalyze,"Philadelphia, PA",2 weeks ago,38 applicants,"['', 'Consult with R&D Product Managers to ensure that new products and features are developed to feed data seamlessly into the data pipeline.', '3 weeks Vacation & 1 week of sick time per year + 11 paid holidays.', '401(k) Retirement savings with employer matching dollar-for-dollar, up to 4%.', 'Work with stakeholders to assist with data-related technical issues and support data infrastructure needs.', ' Medical, dental and vision with access to HSA or FSA depending on chosen medical plan. Available pet insurance. 401(k) Retirement savings with employer matching dollar-for-dollar, up to 4%. Annual bonus subject to company/individual performance. On-site financial planning with a registered financial advisor. 3 weeks Vacation & 1 week of sick time per year + 11 paid holidays. All hands team meetings every 6 weeks with catering. Fully stocked drink fridges. In office snacks 3x per week. ', 'Requirements', 'Strong communication, collaboration, and presentation skills.', 'Mentor other RevOps team members on data pipelines and architecture to create resilience in our resourcing and systems.', 'All hands team meetings every 6 weeks with catering.', 'Experience working independently on data pipeline and integration projects; owning and leading a function without a large team to fall back on; willingness to be entrepreneurial and scrappy in creating infrastructure from scratch. Strong systems thinker.', 'Medical, dental and vision with access to HSA or FSA depending on chosen medical plan.', 'Strong experience with ETL tools, databases, data warehousing solutions', 'Responsibilities', ' Experience working independently on data pipeline and integration projects; owning and leading a function without a large team to fall back on; willingness to be entrepreneurial and scrappy in creating infrastructure from scratch. Strong systems thinker. Strong experience with ETL tools, databases, data warehousing solutions 5+ years of experience working with at least half of the following technologies: SQL, cURL, JSON, Javascript, Kotlin, Apex and/or Python. Clear ability to rapidly extend your natural skills to encompass the other half. Elite at constructing data schemas and structuring datasets for ease of use by our customer-facing teams. Ability to distill down complex concepts and communicate them to non-technical teams. Strong communication, collaboration, and presentation skills. ', 'Build monitoring and alerting tools to notify multiple people on the Revenue Operations team when our core systems or products have issues producing data, our data pipelines have issues extracting and transforming data, or our customer-facing systems have issues receiving data.', 'Design and build automated and scalable data pipelines to extract that data from our core databases and products, transform that data into usable datasets for our customer-facing teams, and load that data into Salesforce, Domo and other tools.', ' Work with leaders across the company to develop a strong understanding of their needs for data sets in Salesforce, Domo and other tools for our customer-facing teams. Work with engineering teams in the R&D organization to understand the data schemas and structures. Design and build automated and scalable data pipelines to extract that data from our core databases and products, transform that data into usable datasets for our customer-facing teams, and load that data into Salesforce, Domo and other tools. Coordinate the validation of data quality by requiring both the R&D stakeholders who have knowledge of data structure, and the customer-facing teams who have knowledge of customer activities and business operations, to review the outputs of the data pipeline and ensure it is accurate and complete. Build monitoring and alerting tools to notify multiple people on the Revenue Operations team when our core systems or products have issues producing data, our data pipelines have issues extracting and transforming data, or our customer-facing systems have issues receiving data. Work with stakeholders to assist with data-related technical issues and support data infrastructure needs. Consult with R&D Product Managers to ensure that new products and features are developed to feed data seamlessly into the data pipeline. Mentor other RevOps team members on data pipelines and architecture to create resilience in our resourcing and systems. ', 'Elite at constructing data schemas and structuring datasets for ease of use by our customer-facing teams. Ability to distill down complex concepts and communicate them to non-technical teams.', 'Available pet insurance.', 'On-site financial planning with a registered financial advisor.', 'Benefits', 'Coordinate the validation of data quality by requiring both the R&D stakeholders who have knowledge of data structure, and the customer-facing teams who have knowledge of customer activities and business operations, to review the outputs of the data pipeline and ensure it is accurate and complete.', 'In office snacks 3x per week.', 'Work with leaders across the company to develop a strong understanding of their needs for data sets in Salesforce, Domo and other tools for our customer-facing teams. Work with engineering teams in the R&D organization to understand the data schemas and structures.', 'Fully stocked drink fridges.', 'Annual bonus subject to company/individual performance.', '5+ years of experience working with at least half of the following technologies: SQL, cURL, JSON, Javascript, Kotlin, Apex and/or Python. Clear ability to rapidly extend your natural skills to encompass the other half.']",Entry level,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Data Engineer,Capital One,"Plano, TX",1 day ago,59 applicants,"['', '2+ years of experience with NoSQL implementation (Mongo, Cassandra) ', 'Bachelor’s Degree At least 2 years of experience in application developmentAt least 1 years of experience in big data technologies (Cassandra, Accumulo, HBase, Spark, Hadoop, HDFS, AVRO, MongoDB, or Zookeeper)', ' slides 76-91', '1+ year experience working on streaming data applications (Spark Streaming, Kafka, Kinesis, and Flink', 'At least 2 years of experience in application development', 'Preferred Qualifications', 'Bachelor’s Degree ', 'Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment', ""Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative,inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who love to solve real problems and meet real customer needs. "", 'Capital One Data Engineer', '2+ years of experience developing Java based software solutions ', '2+ years of experience developing software solutions to solve complex business problems', ""Master's Degree 3+ years of experience in application development1+ year experience working on streaming data applications (Spark Streaming, Kafka, Kinesis, and Flink1+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)1+ years of experience with Ansible / Terraform2+ years of experience with Agile engineering practices 2+ years in-depth experience with the Hadoop stack (MapReduce, Pig, Hive, Hbase) 2+ years of experience with NoSQL implementation (Mongo, Cassandra) 2+ years of experience developing Java based software solutions 2+ years of experience in at least one scripting language (Python, Perl, JavaScript, Shell) 2+ years of experience developing software solutions to solve complex business problems2+ years of experience with UNIX/Linux including basic commands and shell scripting"", 'Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community', '2+ years of experience with Agile engineering practices ', 'At least 1 years of experience in big data technologies (Cassandra, Accumulo, HBase, Spark, Hadoop, HDFS, AVRO, MongoDB, or Zookeeper)', 'We are seeking Data Engineers who will be a part of a team that’s building new analytical and machine learning tools and frameworks to exploit advantages in the latest developments in cloud computing. As aCapital One Data Engineer, you’ll have the opportunity to be on the forefront of driving a major transformation within Capital One. Learn more about#lifeatcapitalone and our commitment todiversity & inclusion by jumping to slides 76-91 on our Corporate Social Responsibility Report. ', 'diversity & inclusion', 'inclusive,', '#lifeatcapitalone', 'Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Snowflake', 'Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies', 'At this time, Capital One will not sponsor a new applicant for employment authorization for this position.', 'What You’ll Do', '2+ years of experience in at least one scripting language (Python, Perl, JavaScript, Shell) ', ""Master's Degree "", 'Perform unit tests and conducting reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance', '2+ years in-depth experience with the Hadoop stack (MapReduce, Pig, Hive, Hbase) ', '2+ years of experience with UNIX/Linux including basic commands and shell scripting', 'Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems', 'Basic Qualifications', 'Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologiesWork with a team of developers with deep experience in machine learning, distributed microservices, and full stack systemsUtilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as SnowflakeShare your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering communityCollaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowermentPerform unit tests and conducting reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance', '3+ years of experience in application development', '1+ years of experience with Ansible / Terraform', '1+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)']",Entry level,Full-time,Information Technology,Banking,2021-03-24 13:05:10
Data Engineer - Data Services,American Express,"Phoenix, AZ",2 weeks ago,130 applicants,"['', 'Emerging Technologies', 'Millions of customers depend on our databases.', ' Agile Practices Emerging Technologies Programming Languages and Frameworks Programming/Software Development System/Platform Domain Knowledge ', 'Support initiatives that deliver workable end-to-end database infrastructure solutions', 'Programming Languages and Frameworks', 'Are you up for the challenge?', 'Contribute to the analysis of business, application, and technical infrastructure requirements.', 'Agile Practices', 'Minimum Qualifications', 'Build a strong infrastructure and a solid career.', 'Design, build, enhance and integrate the infrastructure required to support various database platforms and our business portfolio', 'Programming/Software Development', 'System/Platform Domain Knowledge', 'Here’s Just Some Of What You’ll Do', 'At the core of Infrastructure Engineering', ' Develop automation tools to improve the time to market and time to repair systems Contribute to the analysis of business, application, and technical infrastructure requirements. Design, build, enhance and integrate the infrastructure required to support various database platforms and our business portfolio Support initiatives that deliver workable end-to-end database infrastructure solutions ', 'Develop automation tools to improve the time to market and time to repair systems']",Associate,Full-time,Information Technology,Financial Services,2021-03-24 13:05:10
Data Engineer,Bloomberg LP,"New York, NY",3 weeks ago,91 applicants,"['Strong attention to detail and high degree of proven decision- making', 'Excellent interpersonal skills and ability to explain technical processes and solutions to business partners and management', 'Applied machine learning experience', 'What""s the role?', 'Development and design of management KPI dashboards', 'Does this sound like you?', 'Work closely with partners and their teams to understand their business needs and design solutionsPartner with stakeholders to design insightful analytics and key performance indicators that will promote efficiency/operational excellenceThink beyond specific tasks, understand the ultimate goal, and ask the right questions to drive value for the businessFind innovative ways to build business impactEffectively communicate project progress and success to partners in compelling, creative, and accessible waysAnalyze data in a reproducible environment using RStudio/Jupyter Notebook and have the technical capability to integrate different technologies (e.g. SQL and R or SQL and QlikSense) needed in the data analysis process', 'Think beyond specific tasks, understand the ultimate goal, and ask the right questions to drive value for the business', 'A minimum of 3 years of professional work experience in software development, data engineering, data science or information technology', ""We'll trust you to:"", ""You'll need to have:"", 'A self-starter attitude, creative problem solving skills, and experience with diverse data processes and systems', 'Effectively communicate project progress and success to partners in compelling, creative, and accessible ways', 'Partner with stakeholders to design insightful analytics and key performance indicators that will promote efficiency/operational excellence', 'Dashboard development experience in QlikSense', 'Hands-on knowledge of analytics, data modeling, and data visualization', 'Find innovative ways to build business impact', 'Analyze data in a reproducible environment using RStudio/Jupyter Notebook and have the technical capability to integrate different technologies (e.g. SQL and R or SQL and QlikSense) needed in the data analysis process', 'Full spectrum of project management skills: ideation, prioritization, communication, and delivery', 'Who We Are:', 'An undergraduate degree or higher in Computer Science, Mathematics, or data technology', 'An undergraduate degree or higher in Computer Science, Mathematics, or data technologyA minimum of 3 years of professional work experience in software development, data engineering, data science or information technologyA self-starter attitude, creative problem solving skills, and experience with diverse data processes and systemsApplied machine learning experienceHands-on knowledge of analytics, data modeling, and data visualizationStrong attention to detail and high degree of proven decision- makingFull spectrum of project management skills: ideation, prioritization, communication, and deliveryExcellent interpersonal skills and ability to explain technical processes and solutions to business partners and managementDashboard development experience in QlikSenseDevelopment and design of management KPI dashboards', 'Work closely with partners and their teams to understand their business needs and design solutions']",Not Applicable,Full-time,Information Technology,Financial Services,2021-03-24 13:05:10
Data Engineer,Porsche Cars North America,Atlanta Metropolitan Area,4 weeks ago,118 applicants,"['', 'Design, develop and maintain reliable automated data solutions based on the identification, collection and evaluation of business requirements. Including but not limited to data models, database objects, stored procedures and views', 'Developing new and enhancing existing data processing (Data Ingest, Data Transformation, Data Store, Data Management, Data Quality ) components', '5+ years professional data engineering experience focused on batch and real time data pipelines using Spark, Python, SQL, Java', 'Education:', 'Experience using JIRA and Agile Project Management software', 'Analyzes and profiles large and complex data sets to discover and/or validate data quality issues and business rules', 'We are hiring a Data Engineer for a great opportunity to work with an iconic car brand. This position will be responsible for migrating our local analytics platform/EDW to our new Snowflake database, as well as the ongoing maintenance and development Snowflake and AWS. This position will require a strong background in MPP databases, analytics, cloud architecture, and the tools to implement these solutions.\xa0Additionally, this position will be expected to become an expert on Porsche business processes as it relates to serving customers, AWS, and Snowflake', 'Exposure in Microsoft SSIS and SQL Server', 'Drive collaboration across a global, multicultural, multi-company team', 'Porsche is an equal opportunity employer and we take pride in our diversity. In order to provide equal employment and advancement opportunities to all individuals, employment decisions at Porsche will be based on merit, qualifications and abilities. Porsche does not discriminate in employment opportunities or practices on the basis of race, color, religion, sex, pregnancy, status as a parent, national origin, age, disability, family medical history, ancestry, medical condition, genetic information, sexual orientation, gender, gender identity, gender expression, marital status, familial status, registered domestic partner status, family and medical leave status, military status, criminal conviction history, or any other characteristic protected by federal, state or local law.\xa0', 'Responsibilities:', 'Conduct system monitoring across cloud environments', 'Qualifications:', 'Resolve technical and user issues', 'Automate installation, configuration, backup, monitoring and alerting processes in Snowflake', 'Implementation of RESTful API’s supporting system integrations', 'Facilitates the development of data-related policies, processes, procedures and standards', ""Education:\xa0Bachelor's Degree in Computer Science, Engineering, Data Analytics or comparable experience"", 'German language capability a plus', '\xa0', ""Actively researches philosophical and technological changes in data management and pursue an understanding of the organization's business plans and strategies. Evaluates the value of these trends to the organization and presents the information to management and other team members."", 'Critical thinking/problem solving', 'Position Objective:', 'Experience with code repository solutions', 'Fundamental experience with leveraging AWS for Analytics', '8+ Years of hands-on design and development experience in data space : data processing / data transformation using ETL tools, data warehouse (data modeling, programming), RDBMS', '5+ years professional data engineering experience focused on batch and real time data pipelines using Spark, Python, SQL, Java8+ Years of hands-on design and development experience in data space : data processing / data transformation using ETL tools, data warehouse (data modeling, programming), RDBMSExperience with creating API’s.Fundamental experience with leveraging AWS for AnalyticsExposure in Microsoft SSIS and SQL ServerWorking knowledge of MPP systems or Snowflake a plusExperience using JIRA and Agile Project Management softwareExperience with code repository solutionsAble to work in a global, multicultural environmentKnowledge of best practices and IT operations in an always-up, always-available servicesCritical thinking/problem solvingExperience in the Automotive and/or Financial Industries is a plusGerman language capability a plus', 'Knowledge of best practices and IT operations in an always-up, always-available services', 'Skills:', 'Working knowledge of MPP systems or Snowflake a plus', 'Experience in the Automotive and/or Financial Industries is a plus', ""Design, develop and maintain reliable automated data solutions based on the identification, collection and evaluation of business requirements. Including but not limited to data models, database objects, stored procedures and viewsDeveloping new and enhancing existing data processing (Data Ingest, Data Transformation, Data Store, Data Management, Data Quality ) componentsConduct system monitoring across cloud environmentsAnalyzes and profiles large and complex data sets to discover and/or validate data quality issues and business rulesResolve technical and user issuesAutomate installation, configuration, backup, monitoring and alerting processes in SnowflakeDrive collaboration across a global, multicultural, multi-company teamImplementation of RESTful API’s supporting system integrationsActively researches philosophical and technological changes in data management and pursue an understanding of the organization's business plans and strategies. Evaluates the value of these trends to the organization and presents the information to management and other team members.Facilitates the development of data-related policies, processes, procedures and standards"", 'Able to work in a global, multicultural environment', 'Experience with creating API’s.']",Mid-Senior level,Full-time,Information Technology,Automotive,2021-03-24 13:05:10
SQL Data Engineer,Tesla,"Fremont, CA",4 weeks ago,Over 200 applicants,"['', 'Provide timely and accurate estimates for newly proposed functionality enhancements', 'critical situation', 'Utilize technical and domain knowledge to develop and implement effective solutions; provide hands on mentoring to team members through all phases of the Systems Development Life Cycle (SDLC) using Agile practices.', 'Work in a time constrained environment to analyze, design, develop and deliver Enterprise Data Warehouse solutions for Tesla’s Finance and Accounting teamsWork on ETL SSIS, Business Intelligence & Reporting tools like SSRS, SSAS (Multidimensional and Tabular) and TableauWork with systems that handle sensitive data with strict SOX controls and change management processesDevelop collaborative relationships with key business sponsors and IT resources for the efficient resolution of work requests.Provide timely and accurate estimates for newly proposed functionality enhancementscritical situationCommunicate technical and business topics, as appropriate, in a 360-degree fashion, when required; communicate using written, verbal and/or presentation materials as necessary.Develop, enforce, and recommend enhancements to Applications in the area of standards, methodologies, compliance, and quality assurance practices; participate in design and code walkthroughs.Utilize technical and domain knowledge to develop and implement effective solutions; provide hands on mentoring to team members through all phases of the Systems Development Life Cycle (SDLC) using Agile practices.', 'Communicate technical and business topics, as appropriate, in a 360-degree fashion, when required; communicate using written, verbal and/or presentation materials as necessary.', 'Develop collaborative relationships with key business sponsors and IT resources for the efficient resolution of work requests.', 'Responsibilities', 'Minimum Qualifications:', 'Work with systems that handle sensitive data with strict SOX controls and change management processes', 'The Role', 'Work on ETL SSIS, Business Intelligence & Reporting tools like SSRS, SSAS (Multidimensional and Tabular) and Tableau', 'Develop, enforce, and recommend enhancements to Applications in the area of standards, methodologies, compliance, and quality assurance practices; participate in design and code walkthroughs.', 'Preferred Qualifications', 'Qualifications', 'Work in a time constrained environment to analyze, design, develop and deliver Enterprise Data Warehouse solutions for Tesla’s Finance and Accounting teams']",Entry level,Full-time,Information Technology,Automotive,2021-03-24 13:05:10
Data Engineer Senior (Remote),USAA,"Colorado Springs, CO",24 hours ago,Be among the first 25 applicants,"['', 'Geographical Differential: Geographic pay differential is additional pay provided to eligible employees working in locations where market pay levels are above the national average.Shift premium will be addressed on an individual-basis for applicable roles that are consistently scheduled for non-core hours.BenefitsAt USAA our employees enjoy best-in-class benefits to support their physical, financial, and emotional wellness. These benefits include comprehensive medical, dental and vision plans, 401(k), pension, life insurance, parental benefits, adoption assistance, paid time off program with paid holidays plus 16 paid volunteer hours, and various wellness programs. Additionally, our career path planning and continuing education assists employees with their professional goals.Please click on the link below for more details.USAA Total RewardsRelocation assistance is notavailable for this position.For Internal CandidatesMust complete 12 months in current position (from date of hire or date of placement), or must have manager’s approval prior to posting.Last day for internal candidates to apply to the opening is 3/28/21 by 11:59 pm CST time.', 'And 6 years of data management experience implementing data solutions demonstrating depth of technical understanding within a specific discipline(s)/technology(s)', 'Data Modeling experience', 'Implement efficient defect management, root cause analysis, and resolution processes.', 'to the opening is 3/28/21 by 11:59 pm CST time.', ""Bachelor's degree in related field of study OR 4 additional years of related experience beyond the minimum required."", 'Strong experience with SQL, Hadoop and ETL', 'Create proof of concepts and prototypes.', 'Ensures risks associated with business activities are effectively identified, measured, monitored, and controlled. ', 'Deep knowledge of a technology or product line.', 'Exposure to near real-time Analytic applications', 'For Internal Candidates', 'Breakdown business features into technical stories and approaches.', 'Mentor and coach junior engineers.', 'Shift premium', 'About USAA', 'Recent experience with Python and Spark', 'Preferred Requirements', 'Design and implement complex technical solutions.', 'Identifies and manages existing and emerging risks that stem from business activities and the job role.', 'Minimum Requirements', 'not', 'Last day for internal candidates to apply ', 'Compensation', 'Recent experience with Python and SparkStrong experience with SQL, Hadoop and ETLExposure to near real-time Analytic applicationsData Modeling experienceWorking knowledge of AWS and Snowflake', 'Design, build, manage and optimize data pipelines for data structures encompassing data transformation, data models, schemas, metadata, data quality, and workload management.', 'Benefits', 'Assist in setting technical direction for the team.', 'Identifies and manages existing and emerging risks that stem from business activities and the job role.Ensures risks associated with business activities are effectively identified, measured, monitored, and controlled. Follows written risk and compliance policies and procedures for business activities.Design and implement complex technical solutions.Design, build, manage and optimize data pipelines for data structures encompassing data transformation, data models, schemas, metadata, data quality, and workload management.Participate in daily standups and lead design reviews.Breakdown business features into technical stories and approaches.Analyze data and enable machine learning.Create proof of concepts and prototypes.Implement efficient defect management, root cause analysis, and resolution processes.Assist in setting technical direction for the team.Mentor and coach junior engineers.', 'USAA Total Rewards', 'Relocation', 'available', 'Follows written risk and compliance policies and procedures for business activities.', 'Working knowledge of AWS and Snowflake', 'Participate in daily standups and lead design reviews.', ""Bachelor's degree in related field of study OR 4 additional years of related experience beyond the minimum required.And 6 years of data management experience implementing data solutions demonstrating depth of technical understanding within a specific discipline(s)/technology(s)Deep knowledge of a technology or product line."", 'Analyze data and enable machine learning.']",Not Applicable,Full-time,Information Technology,Financial Services,2021-03-24 13:05:10
"Data Engineer, Users and Products",Google,"Mountain View, CA",2 days ago,45 applicants,"['', "" Bachelor's degree in Computer Science, related technical field or equivalent practical experience. Experience with one general purpose programming language (e.g., Java, C/C++, Python).  Experience in data processing using traditional and distributed systems (e.g., Hadoop, Spark, Dataflow, Airflow). Experience designing data models and data warehouses and using SQL and NoSQL database management systems. "", 'Excellent communication, organizational, and analytical skills.', 'Advanced degree in engineering or technical/scientific field of study. ', 'Experience in large scale distributed data processing.', 'Note: Disclosure as required by sb19-085 (8-5-20) of the minimum salary compensation for this role when being hired into our offices in Colorado.', 'Experience with Unix or GNU/Linux systems.', ""Bachelor's degree in Computer Science, related technical field or equivalent practical experience."", 'Create extract, transform, and load (ETLs) and reporting systems for new data using a variety of traditional as well as large-scale distributed data systems.', 'Experience designing data models and data warehouses and using SQL and NoSQL database management systems.', 'Design, develop and support data pipelines, warehouses and reporting systems to solve business operations, users and product problems.', 'About The Job', 'Additional Information', 'Collaborate and influence Users and Products stakeholders and support engineers to ensure our data infrastructure meets constantly evolving requirements.', 'Mountain View, CA, USA; Boulder, CO, USA', 'Experience with one general purpose programming language (e.g., Java, C/C++, Python). ', 'Responsibilities', 'Write and review technical documents, including design, development, and revision documents.', 'Experience in data processing using traditional and distributed systems (e.g., Hadoop, Spark, Dataflow, Airflow).', ' Design, develop and support data pipelines, warehouses and reporting systems to solve business operations, users and product problems. Create extract, transform, and load (ETLs) and reporting systems for new data using a variety of traditional as well as large-scale distributed data systems. Collaborate and influence Users and Products stakeholders and support engineers to ensure our data infrastructure meets constantly evolving requirements. Work closely with analysts to productionize various statistical and machine learning models using data processing pipelines. Write and review technical documents, including design, development, and revision documents. ', 'Experience designing data models and data warehouses and with non-relational data storage systems (NoSQL and distributed database management systems).', 'Work closely with analysts to productionize various statistical and machine learning models using data processing pipelines.', ' Advanced degree in engineering or technical/scientific field of study.  Experience designing data models and data warehouses and with non-relational data storage systems (NoSQL and distributed database management systems). Experience writing and maintaining ETLs which operate on a variety of structured and unstructured sources. Experience in large scale distributed data processing. Experience with Unix or GNU/Linux systems. Excellent communication, organizational, and analytical skills. ', 'Experience writing and maintaining ETLs which operate on a variety of structured and unstructured sources.']",Not Applicable,Full-time,Information Technology,Information Services,2021-03-24 13:05:10
Data Engineer,JPMorgan Chase & Co.,"Brooklyn, NY",4 weeks ago,Be among the first 25 applicants,"['', 'Experience with stream processing platforms such as Kafka or Dataflow', 'Proficiency in one or more modern programming languages', 'Develop strategy to provide proactive solutions and enable stakeholders to extract insights and value from data', 'Familiarity with data transformation and collection tools such as Pentaho, Informatica', 'Expert level skills in Python its standard library and its package', 'Design best practices for data processing, data modeling and warehouse development throughout our team and group', 'Familiarity with AWS ecosystem including S3, Glue, Redshift, Kinesis, EMR, EC2, SQS', 'Organization', 'Hands-on experience with data warehouse / data lake architectures based on Hadoop, Redshift or Snowflake', 'Build large-scale batch, ETL and real-time data pipelines using cloud and on-premises data technologies, such as Redshift, Python, Spark, PySpark, and Apache Kafka', 'Knowledge of industry-wide technology trends and best practices', 'Understand end to end data interactions and dependencies across complex data pipelines and data transformation and how they impact business decisions.', 'About Us', 'Responsibilities', 'Experience with container technologies such as Docker and Kubernetes', 'Familiarity of microservices stack based on AWS Lambdas. Elastic Search, Spring Boot, NodeJS', 'Advanced knowledge of application, data, and infrastructure architecture disciplines', 'Understanding of software skills such as business analysis, development, maintenance, and software improvement', 'Advanced level skills in SQL, data integration, data modeling and data architecture', 'Knowledge of data columnar and serialization formats such as JSON, XML, Parquet, Avro', 'Working proficiency in developmental toolsets', 'Hands-on experience building a data warehouse and data pipelines using Java, Python or Scala in a data intensive engineering role', 'BS/BA degree or equivalent experience', ' Build large-scale batch, ETL and real-time data pipelines using cloud and on-premises data technologies, such as Redshift, Python, Spark, PySpark, and Apache Kafka Design best practices for data processing, data modeling and warehouse development throughout our team and group Develop strategy to provide proactive solutions and enable stakeholders to extract insights and value from data Understand end to end data interactions and dependencies across complex data pipelines and data transformation and how they impact business decisions. ', ' BS/BA degree or equivalent experience Advanced knowledge of application, data, and infrastructure architecture disciplines Understanding of architecture and design across all systems Working proficiency in developmental toolsets Knowledge of industry-wide technology trends and best practices Ability to work in large, collaborative teams to achieve organizational goals Passionate about building an innovative culture Proficiency in one or more modern programming languages Understanding of software skills such as business analysis, development, maintenance, and software improvement Advanced level skills in SQL, data integration, data modeling and data architecture Expert level skills in Python its standard library and its package Hands-on experience building a data warehouse and data pipelines using Java, Python or Scala in a data intensive engineering role Hands-on experience with data warehouse / data lake architectures based on Hadoop, Redshift or Snowflake Experience with workflow orchestration tools such as Apache Airflow, Autosys Familiarity with data transformation and collection tools such as Pentaho, Informatica Experience with stream processing platforms such as Kafka or Dataflow Knowledge of data columnar and serialization formats such as JSON, XML, Parquet, Avro Experience with container technologies such as Docker and Kubernetes Experience with CI/CD systems e.g. Jenkins and automation / DevOps best practices Familiarity with AWS ecosystem including S3, Glue, Redshift, Kinesis, EMR, EC2, SQS Familiarity of microservices stack based on AWS Lambdas. Elastic Search, Spring Boot, NodeJS ', 'Passionate about building an innovative culture', 'Understanding of architecture and design across all systems', 'Experience with CI/CD systems e.g. Jenkins and automation / DevOps best practices', 'Experience with workflow orchestration tools such as Apache Airflow, Autosys', 'Ability to work in large, collaborative teams to achieve organizational goals', 'Job Description']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Virgin Pulse,"Providence, RI",1 day ago,Be among the first 25 applicants,"['', 'Security Competencies', ' A passion for learning technologies and applying them to the right projects will make you successful in this position. ', ' Columnar databases (Redshift, Snowflake, Firebolt, etc) ', ' Responsibilities ', ' Python programming language ', ' Enjoy actively experimenting with new technologies ', ' Python programming language  One or more data visualization / business intelligence tools (Tableau, MicroStrategy, etc.)  You are enthusiastic about working in a team and have great people skills  A passion for learning technologies and applying them to the right projects will make you successful in this position.  You understand the principles of agile software development  You can present technical concepts in a way that is easy for non-technical people to understand  You are tenacious in your investigation of issues and problems and are able to find the root cause of problems and propose solutions correct the root causes. ', ' Teach and train other team members ', ' Design and implement data models for applications, operations, or analytics ', ' In this role you will wear many hats but your skills will be especially essential in the following: ', ' Use Python to enhance and automate our existing capabilities ', ' You understand the principles of agile software development ', ' You are enthusiastic about working in a team and have great people skills ', ' Excellent verbal and written communication skills ', ' B.S./M.S. in Computer Science or equivalent technology experience ', 'Overview', ' Hands-on experience with some of the tech we use: Git, PostgreSQL, Python, Tableau ', ' Knowledge of OO programming and applications built on distributed service architecture ', ' Skills in scripting, data engineering or modeling, and business intelligence ', 'Our technology', ' Design and implement analytics reports ', ' Investigate and troubleshoot data reporting issues ', ' You can present technical concepts in a way that is easy for non-technical people to understand ', ' Have fun doing all of the above ', ' Perform QA tasks to verify the accuracy of reporting ', ' You are self-motivated, creative, and detail oriented ', 'Who are our employees? ', ' Why work here? ', ' B.S./M.S. in Computer Science or equivalent technology experience  5 or more years’ experience working directly with data and data warehouses  Hands-on experience with some of the tech we use: Git, PostgreSQL, Python, Tableau  You are self-motivated, creative, and detail oriented  Skills in scripting, data engineering or modeling, and business intelligence  Knowledge of OO programming and applications built on distributed service architecture  Enjoy actively experimenting with new technologies  Excellent verbal and written communication skills ', ' One or more data visualization / business intelligence tools (Tableau, MicroStrategy, etc.) ', ' Build files and reports that impact hundreds-of-thousands of people around the world ', 'Qualifications', 'We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to any protected class status.', ' 5 or more years’ experience working directly with data and data warehouses ', 'In a Typical Week You May', ' Use a wide variety of modern technologies, including ', ' Who is Virgin Pulse? ', ' What you bring to the team ', ' Build files and reports that impact hundreds-of-thousands of people around the world  Work on a product that changes people’s lives  Write and maintain advanced SQL queries for reporting extracts and assist more junior team members  Perform QA tasks to verify the accuracy of reporting  Investigate and troubleshoot data reporting issues  Design and implement data models for applications, operations, or analytics  Design and implement analytics reports  Use a wide variety of modern technologies, including ', ' You are tenacious in your investigation of issues and problems and are able to find the root cause of problems and propose solutions correct the root causes. ', ' Write and maintain advanced SQL queries for reporting extracts and assist more junior team members ', ' Commonly used AWS services (S3, Lambda, Redshift, EC2, etc) ', ' BI tools (Tableau, Domo, MicroStrategy) ', ' Work on a product that changes people’s lives ', 'Who You Are', ' Data streaming (Kafka, SQS/SNS queuing, etc)  Columnar databases (Redshift, Snowflake, Firebolt, etc)  Commonly used AWS services (S3, Lambda, Redshift, EC2, etc)  BI tools (Tableau, Domo, MicroStrategy) ', ' Data streaming (Kafka, SQS/SNS queuing, etc) ']",Entry level,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Data Engineer,"Docker, Inc","Washington, DC",4 weeks ago,55 applicants,"['', '2+ years of relevant industry experience', 'Creating production-ready ETL scripts with Python and SQL', 'Preferred Qualifications', 'Strong verbal and written communication skills', 'Data Engineer (Remote)', 'Maintain the integrity of data within our data pipeline and warehouse', 'Experience using data collection platforms such as Segment, RudderStack, Fivetran etc. ', 'Collaborate with product managers and analysts throughout the company to deliver reliable data that powers actionable insights', ' 2+ years of relevant industry experience Familiarity with data warehousing concepts including data model design and query optimization strategies Experience using and maintaining BI visualization tools such as (but not limited to) Looker, Tableau, or Power BI.  Creating production-ready ETL scripts with Python and SQL Experience with version control systems such as Github, Gitlab, Bitbucket etc.  Experience automating business and reporting processes Experience using data analysis and/or statistics to inform decisions  Strong verbal and written communication skills ', 'Experience with version control systems such as Github, Gitlab, Bitbucket etc. ', 'Ensure quality of data and completeness of event logging across Docker codebase', 'Experience using and maintaining BI visualization tools such as (but not limited to) Looker, Tableau, or Power BI. ', 'Integrate emerging methodology, technology, and version control practices that best fit the team. ', 'Champion a data-informed mindset within our culture', 'Responsibilities', 'Transform raw event logs into higher-order tables to make existing analysis easier and new analysis possible', 'Experience using data analysis and/or statistics to inform decisions ', 'Familiarity with at least one cloud ecosystem: AWS/Azure Infrastructure/Google Cloud', 'Creating automated reporting of weekly and monthly metrics and ROI for the executive management team and board', 'Integrate data from 3rd party services via ETL tools and custom pipelines', 'Proficiency in building data pipelines leveraging Docker and orchestration frameworks such as Jenkins/Airflow/Luigi', 'Qualifications', 'Experience automating business and reporting processes', 'Proficiency working with a Data Warehouse platform (e.g. Snowflake, Redshift, BigQuery, AWS Athena, or Spectrum)', 'At least 6 months of experience with Looker and LookML ', 'Experience designing and deploying high-performance systems with reliable monitoring and logging practices', 'Develop ETL jobs and tests to process, validate, transport, collate, aggregate, and distribute data', 'BS/MS in Computer Science, Math, Physics, or other technical fields', 'Design, build and automate business metrics into self-serve dashboards via Looker ', 'Experience of working in an agile environment and using tools such as JIRA/Asana/Trello ', 'Familiarity with data warehousing concepts including data model design and query optimization strategies', ' BS/MS in Computer Science, Math, Physics, or other technical fields At least 6 months of experience with Looker and LookML  Proficiency in building data pipelines leveraging Docker and orchestration frameworks such as Jenkins/Airflow/Luigi Experience designing and deploying high-performance systems with reliable monitoring and logging practices Proficiency working with a Data Warehouse platform (e.g. Snowflake, Redshift, BigQuery, AWS Athena, or Spectrum) Experience using data collection platforms such as Segment, RudderStack, Fivetran etc.  Familiarity with at least one cloud ecosystem: AWS/Azure Infrastructure/Google Cloud Experience of working in an agile environment and using tools such as JIRA/Asana/Trello  ', ' Implement, document, oversee and evolve the Snowflake and ETL infrastructure Maintain the integrity of data within our data pipeline and warehouse Ensure quality of data and completeness of event logging across Docker codebase Integrate data from 3rd party services via ETL tools and custom pipelines Develop ETL jobs and tests to process, validate, transport, collate, aggregate, and distribute data Transform raw event logs into higher-order tables to make existing analysis easier and new analysis possible Creating automated reporting of weekly and monthly metrics and ROI for the executive management team and board Integrate emerging methodology, technology, and version control practices that best fit the team.  Design, build and automate business metrics into self-serve dashboards via Looker  Collaborate with product managers and analysts throughout the company to deliver reliable data that powers actionable insights Champion a data-informed mindset within our culture ', 'Implement, document, oversee and evolve the Snowflake and ETL infrastructure']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer 3-2,PayPal,"Austin, TX",19 hours ago,Be among the first 25 applicants,"['', 'Experience with File Systems, server architectures, and distributed systems', 'Working experience in any MPP systems (Certified Teradata developer is a plus); Should have strong SQL programming skills', "" Be a Mentor for the junior members in the organizationJob Description:8+ years of experience in the IT industry, experience in Data Technology space is preferred.Knowledge of Teradata and BigQuery is essential.Working experience in any MPP systems (Certified Teradata developer is a plus); Should have strong SQL programming skillsExperience in Spark, HBase and HiveKnowledge of data warehousing conceptsKnowledge of Scheduling Tools is a plusExcellent written and oral communication skillsStrong analytical skills including the ability to define problems, collect data, establish facts, and draw valid conclusionsExpertise in database programming and performance tuning techniques; Teradata experience is mustFamiliar with data movement techniques and best practices to handle large volumes of dataExperience with data warehousing architecture and data modeling best practicesExperience with File Systems, server architectures, and distributed systemsStrong communication skills and willingness to take initiative to contribute beyond basic responsibilities.Working experience in an Agile methodology is highly preferred.We're a purpose-driven company whose beliefs are the foundation for how we conduct business every day. We hold ourselves to our One Team Behaviors which demand that we hold the highest ethical standards, to empower an open and diverse workplace, and strive to treat everyone who is touched by our business with dignity and respect. Our employees challenge the status quo, ask questions, and find solutions. We want to break down barriers to financial empowerment. Join us as we change the way the world defines financial freedom.PayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities.REQ ID R0068119"", ' Approaching the problem, taking into account all possibilities. ', ' Planning the execution of the project in an effective and efficient manner. ', 'Knowledge of Teradata and BigQuery is essential.', 'Job Description:', 'Experience with data warehousing architecture and data modeling best practices', '8+ years of experience in the IT industry, experience in Data Technology space is preferred.', 'Knowledge of data warehousing concepts', ' Ability to work in a fast paced environment ', 'Familiar with data movement techniques and best practices to handle large volumes of data', 'Experience in Spark, HBase and Hive', 'Knowledge of Scheduling Tools is a plus', ' Creativity and out of the box thinking is required. ', ' Participating and collaborating with cross functional teams in the organization to understand the business requirements and to deliver solutions that can scale. ', 'Strong analytical skills including the ability to define problems, collect data, establish facts, and draw valid conclusions', 'Expertise in database programming and performance tuning techniques; Teradata experience is must', '8+ years of experience in the IT industry, experience in Data Technology space is preferred.Knowledge of Teradata and BigQuery is essential.Working experience in any MPP systems (Certified Teradata developer is a plus); Should have strong SQL programming skillsExperience in Spark, HBase and HiveKnowledge of data warehousing conceptsKnowledge of Scheduling Tools is a plusExcellent written and oral communication skillsStrong analytical skills including the ability to define problems, collect data, establish facts, and draw valid conclusionsExpertise in database programming and performance tuning techniques; Teradata experience is mustFamiliar with data movement techniques and best practices to handle large volumes of dataExperience with data warehousing architecture and data modeling best practicesExperience with File Systems, server architectures, and distributed systemsStrong communication skills and willingness to take initiative to contribute beyond basic responsibilities.Working experience in an Agile methodology is highly preferred.', 'Excellent written and oral communication skills', ' Ability to deliver from coarse grained requirements ', ' Proactively anticipating problems and appropriately communicating to the team and management in a timely manner. ', ' Being flexible and being able to support all functions of product life cycle when required. ', 'Strong communication skills and willingness to take initiative to contribute beyond basic responsibilities.', 'Working experience in an Agile methodology is highly preferred.', 'Job Description Summary:']",Not Applicable,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Data Engineer,Ford Motor Company,"Dearborn, MI",5 days ago,40 applicants,"['', 'Implement data stores (Hadoop, relational DBs, noSQL DBs, etc.).', 'Participate in testing.', '3+ years of experience in DB Create/Modify (DB Create/Modify tables, DDL export/import, Data export/import)', 'Design data stores.', 'Perform Extract, Transform and Load (ETL) or variations of this activity', '3+ years of experience in Data Architecture (Data Design, Index Design, Referential Integrity)', 'Bachelor’s degree3+ years of experience in Data Architecture (Data Design, Index Design, Referential Integrity)3+ years of experience in DB Create/Modify (DB Create/Modify tables, DDL export/import, Data export/import)3+ years of experience in Performance Tuning (DB-side identification of issues, SQL tuning, DB tuning)3+ years of experience in Data Manipulation - Hadoop Hive/HDFS and SQL', 'Collaborate with multiple organizations (Business Customers, Product Owners, Data Tech, IT) to convert business goals into data storage solutions. Work in small, cross-functional teams and embrace lean and agile practices, software best practices, software quality scanning, automated testing, and CI/CD. Identify, document, communicate and design per requirements.Design data stores.Implement data stores (Hadoop, relational DBs, noSQL DBs, etc.).Perform Extract, Transform and Load (ETL) or variations of this activityParticipate in testing.Tune data stores (indexes, SQL queries) to improve performance.Assist with customer inquiries and incidents/problems.  Lead and support integration projects and discuss status with executives.Work within/across teams.', 'Strong communication skills – both verbal and written', 'Data Manipulation - Informatica Axon Data Governance (new in 2021)', 'DA/DBA – Security (FIM Groups, Ranger)', 'L2/L3 support experience', '3+ years of experience in Data Manipulation - Hadoop Hive/HDFS and SQL', 'Assist with customer inquiries and incidents/problems. ', 'Identify, document, communicate and design per requirements.', 'Collaborate with multiple organizations (Business Customers, Product Owners, Data Tech, IT) to convert business goals into data storage solutions. ', 'Tune data stores (indexes, SQL queries) to improve performance.', 'Ford Application Data knowledge', 'Data Manipulation – PII (PII Policy, PII Scanning)', 'Visualization (QlikView, QlikSense, Tableau)', ' Lead and support integration projects and discuss status with executives.', 'Bachelor’s degree', 'Our preferred requirements: ', 'Data Manipulation - S3, SQL Server, Sqoop, Teradata and Streaming data (Kafka, MQTT)', 'Data Manipulation - Informatica Enterprise Data Catalog, Informatica Enterprise Data Preparation and Informatica Secure At Source ', 'Work in small, cross-functional teams and embrace lean and agile practices, software best practices, software quality scanning, automated testing, and CI/CD. ', 'Data Engineer ', 'Data Manipulation – Alteryx, Attunity, Hadoop/Spark and Hadoop Execution (batch, remote, Ooozie, Livy)', 'The Minimum Requirements We Seek', 'What You’ll Be Able To Do', 'Strong communication skills – both verbal and writtenCritical thinking and decision makingDA/DBA: Data Steward – prior experience coordinating on-going landing of dataDA/DBA: Database ReplicationDA/DBA: DB Create/Modify (Physical design)DA/DBA – Security (FIM Groups, Ranger)Data Manipulation – Alteryx, Attunity, Hadoop/Spark and Hadoop Execution (batch, remote, Ooozie, Livy)Data Manipulation - Informatica Axon Data Governance (new in 2021)Data Manipulation - Informatica Big Data ManagementData Manipulation - Informatica Enterprise Data Catalog, Informatica Enterprise Data Preparation and Informatica Secure At Source Data Manipulation - Infosphere DataStage, NAS/SAN, NoSQL (Mongo), Oracle and PythonData Manipulation – PII (PII Policy, PII Scanning)Data Manipulation - S3, SQL Server, Sqoop, Teradata and Streaming data (Kafka, MQTT)Ford Application Data knowledgeL2/L3 support experienceVisualization (QlikView, QlikSense, Tableau)', 'Critical thinking and decision making', 'What You’ll Receive In Return', 'DA/DBA: DB Create/Modify (Physical design)', 'DA/DBA: Data Steward – prior experience coordinating on-going landing of data', 'Data Manipulation - Informatica Big Data Management', 'Work within/across teams.', 'Data Manipulation - Infosphere DataStage, NAS/SAN, NoSQL (Mongo), Oracle and Python', '3+ years of experience in Performance Tuning (DB-side identification of issues, SQL tuning, DB tuning)', 'DA/DBA: Database Replication']",Entry level,Full-time,Information Technology,Automotive,2021-03-24 13:05:10
Data Engineer,Verizon,"Irving, TX",4 weeks ago,89 applicants,"['', 'Experience knitting disperate data sources together', 'Four or more years of experience as a data engineer', 'Ability to travel occasionally', 'Master’s degree in Computer Science, Engineering, Statistics, IT, or related field.', 'Experience in data engineering, databases, and data warehouses.', 'Diversity and Inclusion at Verizon', 'Four or more years of experience building data pipelines', 'Experience as an open source Contributor.', 'What You’ll Be Doing...', ""You'll Need To Have"", 'Master’s degree in Computer Science, Engineering, Statistics, IT, or related field.Experience with Scala, Julia, R, Python or other machine learning programming languageExperience on Big Data platforms (i.e., Hadoop, Map/Reduce, Spark, HBase, CouchDB, Hive, etc.)Strong analytical and problem-solving skills.Experience working in a network operations center environment.Experience as an open source Contributor.', 'Explore suitable options and designs for specific analytical solutions.', 'diversity and inclusion', 'Work closely with Data Analysts to ensure data quality and availability for analytical modelling.', 'Bachelor’s degree or four or more years of work experience.Four or more years of experience as a data engineerFour or more years of experience finding, cleaning, and preparing data for use by Data ScientistsExperience knitting disperate data sources togetherFour or more years of experience building data pipelinesExperience using SQL (i.e., PL/SQL or T-SQL with RDBMSs like Teradata, MS SQL Server, Oracle, etc.)Experience in data engineering, databases, and data warehouses.Strong experience with data engineering in Python.Ability to travel occasionally', 'Strong analytical and problem-solving skills.', 'Four or more years of experience finding, cleaning, and preparing data for use by Data Scientists', 'What we’re looking for...', 'Support maintenance, bug fixes and, performance analysis along data pipeline.', 'Equal Employment Opportunity', 'When you join Verizon', 'Gather requirements, assess gaps, and build roadmaps and architectures to help the analytics driven organization achieve its goals.Work closely with Data Analysts to ensure data quality and availability for analytical modelling.Explore suitable options and designs for specific analytical solutions.Define extract, load, and transform (ELT) based on jointly defined requirements.Prepare, clean, and massage data for use in modeling and prototypesIdentify gaps and implement solutions for data security, quality, and automation of processes.Support maintenance, bug fixes and, performance analysis along data pipeline.', 'Experience using SQL (i.e., PL/SQL or T-SQL with RDBMSs like Teradata, MS SQL Server, Oracle, etc.)', 'Bachelor’s degree or four or more years of work experience.', 'Define extract, load, and transform (ELT) based on jointly defined requirements.', 'Experience with Scala, Julia, R, Python or other machine learning programming language', 'Prepare, clean, and massage data for use in modeling and prototypes', 'Identify gaps and implement solutions for data security, quality, and automation of processes.', 'Strong experience with data engineering in Python.', 'Experience on Big Data platforms (i.e., Hadoop, Map/Reduce, Spark, HBase, CouchDB, Hive, etc.)', 'Even Better If You Have', 'Gather requirements, assess gaps, and build roadmaps and architectures to help the analytics driven organization achieve its goals.', 'Experience working in a network operations center environment.']",Mid-Senior level,Full-time,Information Technology,Telecommunications,2021-03-24 13:05:10
Data Engineer,Eliassen Group,"Atlanta, GA",1 week ago,99 applicants,"['', 'Data Engineer\xa0', 'Data modeling for ETL/Data Warehousing\xa0\xa0', 'Are you a Data Engineer with cloud experience, looking for a new opportunity? If so, this may be an opportunity for you!', 'Required experience of the Data Engineer:', 'Salary: 125-135k plus Annual Bonus ', 'Benefits Offered: Medical, Dental, Vision, and 401k ', 'Azure or AWS Cloud experienceData modeling for ETL/Data Warehousing\xa0\xa0Professional experience with Java or PythonExperience building and managing Data Pipelines', 'Azure or AWS Cloud experience', '\xa0', 'Design, develop and maintain automated data solutions based on the identification, collection, and eval of business requirements.', 'Keywords: Azure, Big Data, Data Engineer, Spark,\xa0Data Warehouse, Python', 'Jerry King: jking@eliassen.com ', 'Atlanta, GA\xa0', 'Experience building and managing Data Pipelines', 'Design, develop and maintain automated data solutions based on the identification, collection, and eval of business requirements.Design, develop, and maintain automated data solutions data processing (Data Ingest, Data Store, Data Management)', 'Design, develop, and maintain automated data solutions data processing (Data Ingest, Data Store, Data Management)', 'Professional experience with Java or Python', 'For immediate consideration, please email your updated resume to Jerry King: jking@eliassen.com ', 'Responsibilities of the Data Engineer:']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Charles Schwab,"Phoenix, AZ",2 weeks ago,29 applicants,"['', ' 2+ years of experience working on agile teams delivering data solutions ', ' Analyzing the current technology environment to detect critical deficiencies, and recommend solutions for improvement ', ' Basic understanding of at least one IT Management frameworks such as ITIL or COBiT ', 'Your Opportunity ', ' Exceptional interpersonal skills, including teamwork, communication, and negotiation', ' Developing continuous integration and continuous deployment pipelines for data solutions that include automated unit & integration testing ', ' Designing, implementing, and maintaining data warehouses and near real-time data pipelines via the practical application of existing and new data engineering techniques ', ' Ability to quickly learn & become proficient with new technologies ', ' Ensuring consistency with published development, coding and testing standards ', 'What You Have', ' Experience writing automated unit, integration, and acceptance tests for data interfaces & data pipelines ', ' 1+ years of experience delivering solutions on public cloud platforms (Google Cloud preferred) ', ' 3+ years of experience designing, building, and supporting near real-time data pipelines and analytical solutions using Hadoop, Teradata, MS SQL Server, Talend, Informatica, and/or SSIS  2+ years of experience working on agile teams delivering data solutions  2+ years of experience building data pipelines and interfaces with object oriented languages (.Net, Java, Python)  1+ years of experience modeling star schema data warehouses using the Kimball dimensional modeling techniques  1+ years of experience delivering solutions on public cloud platforms (Google Cloud preferred)  Basic understanding of at least one IT Management frameworks such as ITIL or COBiT  Experience writing automated unit, integration, and acceptance tests for data interfaces & data pipelines  Ability to quickly learn & become proficient with new technologies  Exceptional interpersonal skills, including teamwork, communication, and negotiation', ' Collaborating directly with business and technology stakeholders to define future-state business capabilities & requirements, and translating those into transitional and target state data architectures  Analyzing the current technology environment to detect critical deficiencies, and recommend solutions for improvement  Designing, implementing, and maintaining data warehouses and near real-time data pipelines via the practical application of existing and new data engineering techniques  Developing continuous integration and continuous deployment pipelines for data solutions that include automated unit & integration testing  Mentoring, motivating, and supporting the team to achieve organizational objectives and goals  Advocating for agile practices to increase delivery throughput  Ensuring consistency with published development, coding and testing standards ', ' Mentoring, motivating, and supporting the team to achieve organizational objectives and goals ', 'What You Are Good At', ' 3+ years of experience designing, building, and supporting near real-time data pipelines and analytical solutions using Hadoop, Teradata, MS SQL Server, Talend, Informatica, and/or SSIS ', ' 1+ years of experience modeling star schema data warehouses using the Kimball dimensional modeling techniques ', ' 2+ years of experience building data pipelines and interfaces with object oriented languages (.Net, Java, Python) ', ' Collaborating directly with business and technology stakeholders to define future-state business capabilities & requirements, and translating those into transitional and target state data architectures ', ' Advocating for agile practices to increase delivery throughput ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Collabera Inc.,"McLean, VA",,N/A,"['', ""A bachelor's degree in Computer Sciences or related field, or equivalent work experience; master's degree preferred6+ years of experience in software development2+ years of experience leading small/medium teamsExperienced in building Real-time/Batch Ingestion and data processing pipelines built using Spark/DatabricksSkilled in coding in Python or JavaAble to contribute towards test automation and DevOps pipelinesStrong interpersonal skills, coupled with equally strong Team Building and CommunicationSense of urgency in daily work ethicStrong leadership, organizational and time management skills"", 'Present solutions to teams and stakeholders and document design decisions', 'Experienced in building Real-time/Batch Ingestion and data processing pipelines built using Spark/Databricks', ""A bachelor's degree in Computer Sciences or related field, or equivalent work experience; master's degree preferred"", 'Skilled in coding in Python or Java', 'Adopt Enterprise Architecture guidelines to store and manage Big Data.Develop data pipelines using Spark/DatabricksPresent solutions to teams and stakeholders and document design decisionsUtilize Enterprise Technology assets, when applicable.Work with Product owner to develop stories and user flows.Set up CI/CD Pipelines and maintain.Generally work is self-directed and not prescribed.Works with less structured, more complex issues.', 'This is Contract to Hire role with decent Salary plus benefits on conversion.Currently, not open for individuals who needs a sponsorship or has visa dependencies.', 'Utilize Enterprise Technology assets, when applicable.', 'In the role of Lead Software Engineer, Come and join our teams in building our Clinical Products and other applications. You will work independently and in a team as needed, in developing new features to build new products and enhance our applications as needed. Our platforms consume and generate a great volume of data.', '6+ years of experience in software development', 'Set up CI/CD Pipelines and maintain.', 'Sense of urgency in daily work ethic', 'Strong interpersonal skills, coupled with equally strong Team Building and Communication', 'Adopt Enterprise Architecture guidelines to store and manage Big Data.', 'Prerequisite', 'You will be responsible to', 'Work with Product owner to develop stories and user flows.', 'Works with less structured, more complex issues.', '2+ years of experience leading small/medium teams', 'Hiring Data Engineer for 100% Remote Role', 'This is Contract to Hire role with decent Salary plus benefits on conversion.', 'Currently, not open for individuals who needs a sponsorship or has visa dependencies.', 'Able to contribute towards test automation and DevOps pipelines', 'Develop data pipelines using Spark/Databricks', 'Must Have ', 'Description: ', 'Generally work is self-directed and not prescribed.', 'Strong leadership, organizational and time management skills']",Executive,Contract,Information Technology,Pharmaceuticals,2021-03-24 13:05:10
Data Engineer,Cypress HCM,United States,,N/A,"['', 'Work on and scale systems that can scale.', 'Experience with large-scale data and query optimization techniques.', 'Experience with ETL & data warehouse systems.', '3- 5 years of experience, ideally in a SaaS environment', 'Knowledge of cloud, distributed systems, and stream-processing systems.', 'Bachelor’s Degree in computer science3- 5 years of experience, ideally in a SaaS environmentExperience with large-scale data and query optimization techniques.Experience with ETL & data warehouse systems.Experience with AWS cloud services:EC2, RDS, Redshift, Aurora PostgresStrong in SQL, NoSQL and RDBMS.Knowledge in multiple scripting languages, Python preferredKnowledge of cloud, distributed systems, and stream-processing systems.', 'Implement and support systems that reliably and efficiently provide interactive query performance on large amounts of multi-modal data.Work on and scale systems that can scale.Create data tools for analytics and data scientist team members that assist them in building and optimizing our productTroubleshoot and improve the infrastructure required for optimal extractionCollect, parse, analyze, and visualize large sets of dataTurn data into insights.', 'Requirements', 'Implement and support systems that reliably and efficiently provide interactive query performance on large amounts of multi-modal data.', '\xa0', 'EC2, RDS, Redshift, Aurora Postgres', 'Responsibilities', 'Experience with AWS cloud services:', 'Strong in SQL, NoSQL and RDBMS.', 'Collect, parse, analyze, and visualize large sets of dataTurn data into insights.', 'Bachelor’s Degree in computer science', 'Troubleshoot and improve the infrastructure required for optimal extraction', 'Create data tools for analytics and data scientist team members that assist them in building and optimizing our product', 'Knowledge in multiple scripting languages, Python preferred']",Mid-Senior level,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Data Engineer (100% Remote),Dice,"Philadelphia, PA",3 days ago,Be among the first 25 applicants,"['', 'Experience', 'Preferred Educational Level']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,CVS Health,"New York, NY",7 days ago,61 applicants,"['', 'Proficient in SQL and experience in one of the databases', 'Required Qualifications', 'Experience with Hadoop and Hive is a plus', 'Business Overview', 'Collaborates with data scientists to integrate algorithms and models into automated processes.', 'Proficient in Python, Java, Scala, or C++; Experience in shell scripts', 'Experience in Spark are preferred but not required', 'Education', 'Design and implement scalable, configurable and self-learning marketing campaign platform', 'Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards.', 'Preferred Qualifications', 'Proficient in Python, Java, Scala, or C++; Experience in shell scriptsProficient in SQL and experience in one of the databasesExperience in Spark are preferred but not requiredExperience with Hadoop and Hive is a plus', 'Uses expertise, judgment and precedents to contribute to the resolution of moderately complex problems.', 'Job Description', 'Leads portions of initiatives of limited scope, with guidance and direction.', 'Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards.Collaborates with data scientists to integrate algorithms and models into automated processes.Design and implement scalable, configurable and self-learning marketing campaign platformUses expertise, judgment and precedents to contribute to the resolution of moderately complex problems.Leads portions of initiatives of limited scope, with guidance and direction.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,US Tech Solutions,"St Louis, MO",5 days ago,87 applicants,"['Title: Data Engineer - Python', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0Advanced experience with Python', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience and skills with project management and communication of strategy with both technical and non-technical audiences. Additional Info: Develop Business Intelligence Visualization SQL R Metrics Create Tools Manipulation Data Acquisition', ""•\xa0\xa0\xa0\xa0\xa0\xa0\xa0Bachelor's degree in Computer Science, Data Science, Ag/Life Sciences, or related field"", '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0Develop key metrics and data visualizations that will enable prescriptive operations and optimize pipeline processes and transparency through increased data visibility Required Skills/Experience:', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience working with multidisciplinary research and field teams', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0Understanding of Database systems and management of large data sets', 'Location: St. Louis, MO', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0In addition to strong technical skills, the successful candidate must excel in time management; and have a proven track record in problem solution leveraging complex data. • Ideal candidates will have a strong developmental desire to continually learn new technical skills to remain on the cutting edge of capabilities for data analysis and visualization.', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0Ability to construct a storyline, perform analyses, and create presentations to effectively communicate complex concepts and recommendations', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0Strong data analysis skills utilizing tools such as Spotfire, Tableau, Pipeline Pilot, or SQL; API consumption/development experience.', 'Desired Skills/Experience:', 'Responsibilities Include:', 'Job Description', 'Contract']",Mid-Senior level,Contract,Information Technology,Staffing and Recruiting,2021-03-24 13:05:10
Data Engineer (Remote),Automox,United States,2 weeks ago,83 applicants,"['', 'Bonus:', 'Experience with both relational SQL and NoSQL databases', 'Proficiency with and willingness to learn programming languages such as Golang, Python, Java, Scala', 'Salary:', 'Work on projects that are critical to Automox’s mission and have high visibility across the companyBuild, enable, and maintain high-quality, reliable data infrastructurePartner with data science, engineering, and product teams to deliver new capabilities to customersLeverage modern engineering practices and tools in a cloud-native SaaS environmentHave an opinion, test your ideas, work with talented yet humble people, have fun, get stuff done', 'TOTAL COMPENSATION', 'Build analytics tools as part of a data platform to enable data access, analytics workflows, and actionable insights into customer’s IT environments and the company’s operational efficiency', 'The mission of the Data Platform & Analytics (DPA) team is to ensure customers have the data and tools they need to make important and timely decisions. The role of the Data Engineer is to build the infrastructure that makes this possible.\xa0', 'LOCATION', 'Track record of creating and maintaining data pipelinesExpertise with deploying and maintaining distributed data processing frameworks, database technologies, and streaming/messaging systemsAbility to assemble large, complex data sets that satisfy the needs of both internal and external customersBuild analytics tools as part of a data platform to enable data access, analytics workflows, and actionable insights into customer’s IT environments and the company’s operational efficiencyFamiliarity with cloud environments and technologies (AWS preferred)Experience with both relational SQL and NoSQL databasesProficiency with and willingness to learn programming languages such as Golang, Python, Java, ScalaExperience partnering with business intelligence and analytics teams\xa0Comfortable working in a remote/distributed work environment requiring excellent verbal, written, and asynchronous communication skillsKnowledge of data science principles, statistics, or machine learning can be beneficial but should not be considered a required skill set', 'Build, enable, and maintain high-quality, reliable data infrastructure', 'Have an opinion, test your ideas, work with talented yet humble people, have fun, get stuff done', 'Benefits:\xa0', '-All employees are part of our Company bonus plan. Our bonus is a mix of company performance and individual contributions.', 'Bonus', 'KEY SKILLS AND ATTRIBUTES', 'We are committed to an inclusive and diverse Automox. Automox is an equal opportunity employer. We do not discriminate based on race, color, ethnicity, ancestry, national origin, religion, sex, gender, gender identity, gender expression, sexual orientation, age, disability, veteran status, genetic information, marital status, or any legally protected status.', 'Expertise with deploying and maintaining distributed data processing frameworks, database technologies, and streaming/messaging systems', 'Experience partnering with business intelligence and analytics teams\xa0', '-$105,000 - $150,000/year', ""-Our salary ranges are based on national averages and are determined based on the level of the position we are hiring for. The ranges are wide to leave room for variability in a candidate's skills, experience, and location all which impact where someone might come in on the range.\xa0"", 'Work on projects that are critical to Automox’s mission and have high visibility across the company', '-Time off: We have a flexible PTO policy with an additional 9 paid holidays.', 'OVERVIEW', 'Equity', 'Knowledge of data science principles, statistics, or machine learning can be beneficial but should not be considered a required skill set', 'THE FUN STUFF', 'WHY AUTOMOX\xa0\xa0', 'Ability to assemble large, complex data sets that satisfy the needs of both internal and external customers', 'We are on a mission to enable every IT Admin to automate the fundamental tasks that keep their corporation secure. This mission can only be accomplished with a culture embodies entrepreneurialism, accountability and providing our employees with the clear direction and freedom to do their best work. We don’t measure excellence based on how but on the what. Each employee has a value and contribution to the success of Automox. We look forward to working with you and seeing the success you will bring on our journey.\xa0\xa0', 'Comfortable working in a remote/distributed work environment requiring excellent verbal, written, and asynchronous communication skills', 'Salary', 'Automox aims to be an employer of choice and we know that means offering a comprehensive compensation package to support our employees. Our packages include base salary, bonus, equity, and benefits for all full time permanent employees.', 'Partner with data science, engineering, and product teams to deliver new capabilities to customers', '-Healthcare options through Cigna and Guardian including Medical, Dental, Vision, Basic Life insurance, Voluntary Life Insurance, Basic STD & LTD, HSA, FSA, 401(k) and more. Automox has a generous employer contribution towards all health plans with low premiums for all employees.\xa0\xa0', 'Track record of creating and maintaining data pipelines', 'Remote : USA the world is changing so are we, Automox has moved to a fully distributed company and is open to hiring across the US.\xa0', '-Perks: Monthly internet and wellness stipend, money to set up your home office, and no commute.', 'Founded in 2015,\xa0Automox is coming off its fourth quarter of record growth that has seen its platform become the most recommended solution in endpoint security and the preferred endpoint management solution for over 1,500 customers across 30 countries.\xa0With an increasing number of operating systems, servers, hardware, and applications that need to be maintained, updated, configured, and patched on a regular basis, IT ops teams are feeling fatigued and vulnerable. Automox is building a company and team to tackle this problem for millions of endpoints.\xa0', '-Stock options\xa0\xa0', 'Benefits', '-Parental Benefits: Adoption benefits, Parental leave', 'Equity:', 'Familiarity with cloud environments and technologies (AWS preferred)', 'Leverage modern engineering practices and tools in a cloud-native SaaS environment']",Associate,Full-time,Engineering,Computer & Network Security,2021-03-24 13:05:10
Data Engineer (100% remote!),Optello,"Chicago, IL",4 days ago,38 applicants,"['', ' Contributing to product development efforts with goals of reliable delivery, high quality (eg. bug free), technical excellence and continuous delivery', ' Practicing general software engineering principles (SOLID principles), best practices (code review), engineering patterns (e.g. design patterns), and Test Driven Development', 'Your Right to Work', ' Contributing to building new predictive and prescriptive analytics to compliment existing descriptive and diagnostic analytics', ' Collaborating with distributed team of engineers and product owners', ' Apache PIG', ' Relational databases', ' Azure App Functions', ' Spark', 'Please do NOT change the email subject line in any way. You must keep the JobID: linkedin : JG9-1619672 -- in the email subject line for your application to be considered.***', 'Email Your Resume In Word To', ' Enhancing the data pipeline and platform using architectural and design patterns such as data lake Implementing scalable data services using serverless Azure resources such as Data Factory, Synapse, Databricks, Azure Functions and traditional SQL Collaborating with data scientists to implement AI and machine learning solutions, and DevOps to automate workflows and processes Contributing to building new predictive and prescriptive analytics to compliment existing descriptive and diagnostic analytics Contributing to product development efforts with goals of reliable delivery, high quality (eg. bug free), technical excellence and continuous delivery Implementing technical design for feature enhancements using appropriate design patterns, data and object models Collaborating with distributed team of engineers and product owners Practicing general software engineering principles (SOLID principles), best practices (code review), engineering patterns (e.g. design patterns), and Test Driven Development', ' Prior experience in a similar role, ideally within the health tech space but one where analytics was the focal point of the technology Experience with the following languages and technologies: Relational databases SQL ETL (Azure Data Factory / SSIS, Databricks) Azure App Functions Azure Data lake Spark Apache PIG', ' Competitive compensation + benefits', ' Competitive compensation + benefits Generous annual bonus structure', ' Collaborating with data scientists to implement AI and machine learning solutions, and DevOps to automate workflows and processes', ' Experience with the following languages and technologies:', ' SQL', ' Generous annual bonus structure', 'Optello is proud to be an Equal Opportunity Employer', ' Azure Data lake', ' Implementing technical design for feature enhancements using appropriate design patterns, data and object models', ' Implementing scalable data services using serverless Azure resources such as Data Factory, Synapse, Databricks, Azure Functions and traditional SQL', ' Enhancing the data pipeline and platform using architectural and design patterns such as data lake', ' ETL (Azure Data Factory / SSIS, Databricks)', ' Prior experience in a similar role, ideally within the health tech space but one where analytics was the focal point of the technology']",Entry level,Full-time,Information Technology,Construction,2021-03-24 13:05:10
Data Engineer,Slalom,San Diego Metropolitan Area,2 weeks ago,143 applicants,"['', '·\xa0\xa0\xa0\xa0\xa0\xa0Data wrangling of heterogeneous data and explore and discover new insights', '·\xa0\xa0\xa0\xa0\xa0\xa0Proven experience with data warehousing, data ingestion, and data profiling', '·\xa0\xa0\xa0\xa0\xa0\xa0Familiarity with implementing analytics solutions with one or more Hadoop distributions (Cloudera, Hortonworks, MapR, HDInsight, EMR)', '·\xa0\xa0\xa0\xa0\xa0\xa0Understanding of agile project approaches and methodologies', 'Slalom is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law.', '·\xa0Familiarity or strong desire to learn quantitative analysis techniques (e.g., predictive modeling, machine learning, segmentation, optimization, clustering, regression)', '·\xa0\xa0\xa0\xa0\xa0\xa0Strong aptitude for learning new technologies and analytics techniques', '·\xa0\xa0\xa0\xa0\xa0\xa0Work as part of a team to develop Cloud Data and Analytics solutions', '·\xa0\xa0\xa0\xa0\xa0\xa0Proficient in Python and/or Java', '·\xa0\xa0\xa0\xa0\xa0\xa0Proficient in the Linux shell, including utilities such as SSH', '·\xa0\xa0\xa0\xa0\xa0\xa0Hands-on experience with leading commercial Cloud platforms, including AWS, Azure, and Google', 'Job Title: Data Engineer', '·\xa0\xa0\xa0\xa0\xa0\xa0Familiarity with streaming data ingestion', 'Responsibilities', '·\xa0\xa0\xa0\xa0\xa0\xa0Consulting experience', 'Qualifications', '·\xa0\xa0\xa0\xa0\xa0\xa03+ years of related work experience in Data Engineering or Data Warehousing', '·\xa0\xa0\xa0\xa0\xa0\xa0Proficient in a source code control system, such as Git', '·\xa0\xa0\xa0\xa0\xa0\xa0Gain hands-on experience with new data platforms and programming languages (e.g. Python, Hive, Spark)', '·\xa0\xa0\xa0\xa0\xa0\xa0Proficient in SQL', 'As a Data Engineer, you’ll work in small teams to deliver innovative solutions on Amazon Web Services, Azure, and Google Cloud using core cloud data warehouse tools, Hadoop, Spark, Event Stream platforms, and other Big Data related technologies. In addition to building the next generation of data platforms, you’ll be working with some of the most forward-thinking organizations in data and analytics.\xa0', '·\xa0\xa0\xa0\xa0\xa0\xa0Highly self-motivated and able to work independently as well as in a team environment', 'Job Title', 'Preferred Experience', '·\xa0\xa0\xa0\xa0\xa0\xa0Participate in development of cloud data warehouses and business intelligence solutions']",Mid-Senior level,Full-time,Information Technology,Management Consulting,2021-03-24 13:05:10
Data Engineer,Cognizant,"San Jose, CA",6 days ago,25 applicants,"['', 'Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.', 'Create and maintain optimal data pipeline architecture, Assemble large, sophisticated data sets that meet functional / non-functional business requirements.', 'Must have experience on designing and creating data ingestion and data computation jobs.', 'Experience leading projects and small team in the past is required.', 'Cognizant will not be able to provide sponsorship for this role. Candidates have the option of working remotely.', 'Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability etc.', 'Good understanding of data oriented projects for integration and analytics.', 'Work with partners including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.', 'Responsibilities', 'Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘Big data’ technologies.', ' 5 plus years of experience working on AWS, S3, Glue, EMR, Redshift, RDS. 4 plus years of experience working on Spark, Scala and Python. Experience leading projects and small team in the past is required. Experience on creating the frameworks towards building the data pipelines. Must have experience on designing and creating data ingestion and data computation jobs. Experience work with structured and unstructured data. Good understanding of data oriented projects for integration and analytics. Cognizant will not be able to provide sponsorship for this role. Candidates have the option of working remotely. ', 'Qualifications', '5 plus years of experience working on AWS, S3, Glue, EMR, Redshift, RDS.', 'Build analytics tools that utilize the data pipeline to deliver impactful insights into customer acquisition, operational efficiency and other key business performance metrics.', 'About Cognizant', 'Company Description', 'Experience work with structured and unstructured data.', ' For this role we are unable to sponsor visa now or in near future.', ' Create and maintain optimal data pipeline architecture, Assemble large, sophisticated data sets that meet functional / non-functional business requirements. Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability etc. Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘Big data’ technologies. Build analytics tools that utilize the data pipeline to deliver impactful insights into customer acquisition, operational efficiency and other key business performance metrics. Work with partners including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader. ', 'Experience on creating the frameworks towards building the data pipelines.', '4 plus years of experience working on Spark, Scala and Python.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Brooksource,Nashville Metropolitan Area,3 weeks ago,Over 200 applicants,"['A welcoming, team environment where you will be provided an opportunity to work with a Fortune 100 company.', 'Responsibilities:', '·\xa0\xa0\xa0\xa0\xa0Experience in prior Devops role is a plus', ""·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Bachelor’s Degree or Master's Degree in Data Science, Computer Science, Software/Computer Engineering, Mathematics, or related field"", 'Remote', 'What’s in it for you?', 'Data Engineer', 'Eight Eleven Group provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, national origin, age, sex, citizenship, disability, genetic information, gender, sexual orientation, gender identity, marital status, amnesty or status as a covered veteran in accordance with applicable federal, state, and local laws.', '·\xa0\xa0\xa0\xa0\xa0\xa0Project or Internship experience in previous Data Engineering role', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Standardize data, analyze tables within data warehouses, build APIs', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Contribute to batch pipelines, data modeling, and data warehousing solutions for consumers', '·\xa0\xa0\xa0\xa0\xa02-3 years of Python or Powershell scripting experience', '\xa0Minimum Qualifications', 'Contract to Hire', '\xa0']",Associate,Contract,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Huxley,"Chicago, IL",7 days ago,Over 200 applicants,"['SQL development experience', 'Must have Qualifications:', 'The ability to work with company with an award winning positive culture', 'Responsibilities:', 'Working in collaboration with other members from the data team implementing new ideas', 'Competitive compensation plus 15% bonus that is paid out bi-annually', ""What's in it for you?!"", 'Generous PTO and paid holidays', 'Experience building out data pipelines from scratch using PythonSQL development experienceExperience with cloud technologies like GCP is a plus', 'The ability to work with company with an award winning positive cultureCompetitive compensation plus 15% bonus that is paid out bi-annuallyGenerous PTO and paid holidays', 'Sthree US is acting as an Employment Agency in relation to this vacancy.', 'Experience with cloud technologies like GCP is a plus', ' ', 'Supporting the data quality program as well as operations, and technical documentations', 'Design and build out ETL Pipelines and tools', 'Experience building out data pipelines from scratch using Python', 'My client is looking to grow their Data team notorious for their Award Winning Culture. They are currently building out their analytical warehouse and real-time data that overlooks their ecommerce platform. Experience building out data pipelines and ETL tools is a must, which seems to align with your background!', 'Design and build out ETL Pipelines and toolsWorking in collaboration with other members from the data team implementing new ideasSupporting the data quality program as well as operations, and technical documentations']",Entry level,Full-time,Engineering,Computer Software,2021-03-24 13:05:10
Data Engineer,TA Digital ,"United, LA",4 weeks ago,Be among the first 25 applicants,"[' Provide high quality finished dashboards, reports, and workbooks', '*', ' Link disparate data sources to develop insight', ' 3-5 years in an analytics role (contingent labor roles are acceptable)', ' Data Analysis/Visualization with either Tableau or PBI', ' 3+ years of experience programming with SQL; Python and/or R is a plus but not required', ' SQL (Preferred ORACLE Experience)', ' Respond to daily requests for ad hoc reporting (2 + requests per day varying in complexity)', ' Understand key performance metrics and functional goals and ideate data solutions', ' Bachelor’s degree with a major in a quantitative discipline (computer science, engineering, math, etc.)', 'Requirements', ' Acquire user requirements to develop data source strategy, develop mock ups, and revise based on feedback', ' 3+ years of data visualization experience', 'Description', ' Understanding the need for operations and service reporting', 'About Us', ' Utilizing the data and develop the approach/finished product to enable business owners to make informed decisions', ' Data ETL Experience', ' Independent worker who can meet deadlines', 'U.S. Citizens and those authorized to work in the U.S are encouraged to apply. We are unable to sponsor at this time.**This candidate will be working with the Shared Solutions department at Client. One of their core objectives is to assess services being used within various departments across the enterprise. The Hiring Leader that this candidate will be reporting to, in particular will be need an Analyst to a more in depth look within Client’s call centers.This Data Engineer/Analyst will be responsible for the following tasks: Understanding the need for operations and service reporting Utilizing the data and develop the approach/finished product to enable business owners to make informed decisions Acquire user requirements to develop data source strategy, develop mock ups, and revise based on feedback Link disparate data sources to develop insight Provide high quality finished dashboards, reports, and workbooks Understand key performance metrics and functional goals and ideate data solutions Respond to daily requests for ad hoc reporting (2 + requests per day varying in complexity)RequirementsRequirements Bachelor’s degree with a major in a quantitative discipline (computer science, engineering, math, etc.) 3-5 years in an analytics role (contingent labor roles are acceptable) 3+ years of data visualization experience 3+ years of experience programming with SQL; Python and/or R is a plus but not required SQL (Preferred ORACLE Experience) Data ETL Experience Data Analysis/Visualization with either Tableau or PBI Demonstrable working knowledge of relational databases Independent worker who can meet deadlines Call center reporting and healthcare experience a plusBenefitsBase Salary, Benefits: Global Health, Dental, Vision, AD&D Insurance, Vacation / PTO / Holiday pay plus matching 401K, Tuition & Travel Expense Reimbursement as neededAbout UsTA Digital is the only global boutique agency that delivers the “best of both worlds” to clients seeking to achieve organizational success through digital transformation. Unlike smaller, regional agencies that lack the ability to scale or large organizations that succumb to a quantity-over-quality approach, we offer resource diversity while also providing meticulous attention to the details that enable strategic success.Over the past 20 years, TA Digital has positioned clients to achieve digital maturity by focusing on data, customer-centricity and exponential return on investment; by melding exceptional user experience and data-driven methodologies with artificial intelligence and machine learning, we enable digital transformations that intelligently build upon the strategies we set into motion. We are known as a global leader that assists marketing and technology executives in understanding the digital ecosystem while identifying cultural and operational gaps within their business - ultimately ushering organizations toward a more mature model and profitable digital landscape.Recognized in 2013, 2014, 2015, and 2019 Inc. 5000 list as one of the most successful technology companies in the United States, TA Digital is pleased also to share high-level strategic partnerships with world class digital experience platform companies like Adobe, SAP and Salesforce and possess global partnerships with industry leaders such as Sitecore, Episerver, Elastic Path, BigCommerce, AWS, Azure and Coveo.TA Digital has offices in US, Canada, UK and India. For more information, visit: www.tadigital.comEOE & OFCCP Compliant regardless of: Minority / Female / Veteran / Disabled / Sexual Orientation / Gender Identity / National Origin', ' Bachelor’s degree with a major in a quantitative discipline (computer science, engineering, math, etc.) 3-5 years in an analytics role (contingent labor roles are acceptable) 3+ years of data visualization experience 3+ years of experience programming with SQL; Python and/or R is a plus but not required SQL (Preferred ORACLE Experience) Data ETL Experience Data Analysis/Visualization with either Tableau or PBI Demonstrable working knowledge of relational databases Independent worker who can meet deadlines Call center reporting and healthcare experience a plus', ' Call center reporting and healthcare experience a plus', 'Benefits', ' Demonstrable working knowledge of relational databases', ' Understanding the need for operations and service reporting Utilizing the data and develop the approach/finished product to enable business owners to make informed decisions Acquire user requirements to develop data source strategy, develop mock ups, and revise based on feedback Link disparate data sources to develop insight Provide high quality finished dashboards, reports, and workbooks Understand key performance metrics and functional goals and ideate data solutions Respond to daily requests for ad hoc reporting (2 + requests per day varying in complexity)']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Advanced Data Engineer,Honeywell,"Atlanta, GA",2 weeks ago,Be among the first 25 applicants,"['', 'Experience working with remote and global teams and cross team collaboration', 'Experience in working with cloud-based deployments. Understanding of containers & container orchestration (Swarm or Kubernetes). ', 'Minimum of 4 years with development and deployment of complex big data ingestion jobs in Spark/Informatica BDM/Talend bringing prototypes to production on Hadoop/NoSQL/MPP platforms. ', 'Good understanding of branching, build, deployment, CI/CD methodologies such as Octopus and Bamboo', 'Experience in writing complex SQL statements', 'Experience with visualization software (Tableau, Spotfire, Qlikview, Angular js, D3.js)', 'Technology upgrade oversight', 'Minimum 2 years of experience in working with at least one NoSQL system (HBase, Cassandra, MongoDB etc.). In-depth knowledge of schema design to effectively tackle the requirement.', 'Experience with dimensional modeling, data warehousing and data mining', 'Consistently makes timely decisions even in the face of complexity, balancing systematic analysis with decisiveness', 'Experience working with in Agile Methodologies and Scrum ', ""Bachelor's degree in Computer Science, Engineering, Applied Mathematics or related field"", 'Location: ', 'Minimum 3 years of experience in developing and building applications to process very large amounts of data (structured and unstructured), including streaming real-time data (Spark, Scala, Kafka, Python, Spark streaming or other such tools).', 'Minimum of 4 years of hands on experience with Spark, Pig/Hive, etc. and automation of data flow using Informatica, Spark, NiFi or Airflow/Oozie.', 'Location: 715 Peachtree Street, N.E.,Atlanta,Georgia,30308,United States', 'Database performance management and API development', 'YOU MUST HAVE', 'Minimum of 6 years of data engineering experience', 'Hands on experience in Databricks, Cloudera, Hortonworks and/or Cloud (AWS EMR, Azure Data Lake Storage) based Hadoop distributions.', 'Category: ', 'Knowledge of software best practices, like Test-Driven Development (TDD) ', 'Hands on experience in Databricks, Cloudera, Hortonworks and/or Cloud (AWS EMR, Azure Data Lake Storage) based Hadoop distributions.Effective communication skills and succinct articulationExperience in writing complex SQL statementsExperience in working with cloud-based deployments. Understanding of containers & container orchestration (Swarm or Kubernetes). Experience in building advanced analytics solutions with data from enterprise systems like ERPs, CRMs, Marketing tools etc.Experience with dimensional modeling, data warehousing and data miningGood understanding of branching, build, deployment, CI/CD methodologies such as Octopus and BambooExperience working with in Agile Methodologies and Scrum Knowledge of software best practices, like Test-Driven Development (TDD) Database performance management and API developmentTechnology upgrade oversightExperience with visualization software (Tableau, Spotfire, Qlikview, Angular js, D3.js)Understanding of best-in-class model and data configuration and development processesExperience working with remote and global teams and cross team collaborationConsistently makes timely decisions even in the face of complexity, balancing systematic analysis with decisiveness', 'Experience in building advanced analytics solutions with data from enterprise systems like ERPs, CRMs, Marketing tools etc.', 'JOB ACTIVITIES', 'Category: Engineering', ""Bachelor's degree in Computer Science, Engineering, Applied Mathematics or related fieldMinimum of 6 years of data engineering experienceMinimum of 4 years with development and deployment of complex big data ingestion jobs in Spark/Informatica BDM/Talend bringing prototypes to production on Hadoop/NoSQL/MPP platforms. Minimum of 4 years of hands on experience with Spark, Pig/Hive, etc. and automation of data flow using Informatica, Spark, NiFi or Airflow/Oozie.Minimum 3 years of experience in developing and building applications to process very large amounts of data (structured and unstructured), including streaming real-time data (Spark, Scala, Kafka, Python, Spark streaming or other such tools).Minimum 2 years of experience in working with at least one NoSQL system (HBase, Cassandra, MongoDB etc.). In-depth knowledge of schema design to effectively tackle the requirement."", 'Understanding of best-in-class model and data configuration and development processes', 'Effective communication skills and succinct articulation', 'JOB ID: req260527Category: EngineeringLocation: 715 Peachtree Street, N.E.,Atlanta,Georgia,30308,United States', 'JOB ID: req260527', 'WE VALUE', 'JOB ID: ']",Associate,Full-time,Information Technology,Medical Devices,2021-03-24 13:05:10
Data Integration Engineer,Peloton Interactive,"Santa Clara, CA",7 days ago,Be among the first 25 applicants,"['', 'Job Responsibilities', 'Own the Boomi development process from requirements gathering to full implementation.', 'Peloton does not accept unsolicited agency resumes. Agencies should not forward resumes to our jobs alias, Peloton employees or any other organization location. Peloton is not responsible for any agency fees related to unsolicited resumes.', 'Lead the research, development & implementation of special projects, as needed.', 'Strong verbal and written communication skills', 'SQL language proficiency with at least one of the common database platforms such as SQL Server, Oracle, MySQL, and/or DB2 database management systems', 'You are a proactive problem-solver, even in areas of uncertainty and ambiguity.', 'Dell Boomi ', 'supply chain systems data integrations', 'Strong understanding of integration architecture options such as SoA and APIs', 'Experience with REST and SOAP web services', 'Boomi Developer/Architect certified', 'Collaborate with the development team to architect efficient and stable integrations.', 'You have excellent analytical and critical reasoning skills.', 'Monitor, troubleshoot, and resolve problems with integrations.', 'Ability to work well under pressure; manage tight deadlines and situations where conflicting priorities arise.', 'Minimum of 3 years experience working with integration platforms (Boomi, MuleSoft etc.).', ' You have the ability to analyze business problems and develop both long-term sustainable solutions and also quick tactical solutions. You can articulate complex concepts in a way that is understandable to non-technical stakeholders. You have excellent analytical and critical reasoning skills. You are a proactive problem-solver, even in areas of uncertainty and ambiguity. You possess strong collaboration skills and approach problems with positive intent while driving towards resolution. ', 'Bachelor’s degree', 'You can articulate complex concepts in a way that is understandable to non-technical stakeholders.', 'Strong analytical and critical thinking skills', 'Experience developing applications that utilize Boomi Integration', ' Bachelor’s degree Strong verbal and written communication skills Strong analytical and critical thinking skills Adapt and proactive at problem-solving and conflict resolution. Minimum of 3 years experience working with integration platforms (Boomi, MuleSoft etc.). Proven ability to implement at least 1 to 2 full integration projects involving on-premise and Cloud applications. Functional experience with ERP systems (i.e., NetSuite, SAP) Experience with REST and SOAP web services SQL language proficiency with at least one of the common database platforms such as SQL Server, Oracle, MySQL, and/or DB2 database management systems Ability to work well under pressure; manage tight deadlines and situations where conflicting priorities arise. ', 'Functional experience with ERP systems (i.e., NetSuite, SAP)', 'Provide guidance on the monitoring, troubleshooting, and resolution of data quality issues in collaboration with the appropriate stakeholders; and', 'About Peloton', ' Integration Engineer', 'Document and analyze current business processes and underlying systems/applications.', 'Basic Job Requirements', 'Adapt and proactive at problem-solving and conflict resolution.', 'You have the ability to analyze business problems and develop both long-term sustainable solutions and also quick tactical solutions.', 'Hands-on experience on Integration Platforms such as Boomi, MuleSoft; AnyPoint, Pentaho, and Informatica', ' Own the Boomi development process from requirements gathering to full implementation. Monitor, troubleshoot, and resolve problems with integrations. Collaborate with the development team to architect efficient and stable integrations. Document and analyze current business processes and underlying systems/applications. Lead the research, development & implementation of special projects, as needed. Provide guidance on the monitoring, troubleshooting, and resolution of data quality issues in collaboration with the appropriate stakeholders; and Communicate and collaborate effectively with technical peers and business users. ', 'You possess strong collaboration skills and approach problems with positive intent while driving towards resolution.', 'Experience with performance tuning optimization within Boomi', 'Proven ability to implement at least 1 to 2 full integration projects involving on-premise and Cloud applications.', 'Preferred Experience', ' Boomi Developer/Architect certified Experience developing applications that utilize Boomi Integration Strong understanding of integration architecture options such as SoA and APIs Experience with performance tuning optimization within Boomi Functional experience with ERP systems (i.e., NetSuite, SAP) Hands-on experience on Integration Platforms such as Boomi, MuleSoft; AnyPoint, Pentaho, and Informatica ', 'Who You Are', 'Communicate and collaborate effectively with technical peers and business users.']",Entry level,Full-time,Information Technology,"Health, Wellness and Fitness",2021-03-24 13:05:10
Data Engineer,Apex Systems,United States,1 week ago,113 applicants,"['', 'Apache AirflowAzure Data Factory (ADF)', '70% Data Platform Engineer and 30% Data OptimizatiionSenior level experience with DatabricksHands on experience with Azure data platform stackExperience with Spark designing and building end-to-end data pipelinesExperience working with Azure Delta Tables and ParquetsUnderstand ETL datawarehouse concepts and partitionsExperience with Data IngestionAbility to build pipelines to process structured and semi-structured data.Understand principles of data optimizationUnderstand the basic data engineering aspects and relational modeling', 'Understand the basic data engineering aspects and relational modeling', 'Experience with Data Ingestion', 'Understand principles of data optimization', 'Understand ETL datawarehouse concepts and partitions', 'Must Haves: Databricks, Spark and ETL', 'Ability to build pipelines to process structured and semi-structured data.', 'Hands on experience with Azure data platform stack', 'Azure Data Factory (ADF)', 'Experience with Spark designing and building end-to-end data pipelines', 'Experience working with Azure Delta Tables and Parquets', 'Requirements:', '70% Data Platform Engineer and 30% Data Optimizatiion', 'Apache Airflow', 'Senior level experience with Databricks', 'Additional preferred but not required:']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,"Starry, Inc.","New York, NY",5 days ago,39 applicants,"['', 'Contributor to open source software', 'Experience with batch ETL or backend bulk data operations', 'API and backend application development', 'Data engineering focused DevOps, observability, CI/CD, etc', 'Data warehouse and data lake architecture and design', 'Unit and integration testing data pipelines', 'Shell scripting, Git, jq, awk, and a variety of other Unix utilities', ' 100% employer paid low deductible health plan, dental plan, vision plan, AD&D and life insurance 401(k) retirement plan and stock options  12 weeks of 100% paid parental leave for new mothers and fathers after six months of continuous employment Professional development assistance after six months of employment Catered meals on a weekly basis for employees working in the office Casual dress, community clubs, annual fitness reimbursement, stocked kitchen and other perks and discounts ', 'Virtualization with Docker or an equivalent', '401(k) retirement plan and stock options ', ' 1+ years of data engineering experience or 3+ years of engineering experience Familiar with SQL, profile query performance, and design database schema Experience with batch ETL or backend bulk data operations Familiar with CI/CD, Docker or similar, and testing data intensive processes ', 'Workflow management tools like Airflow or Beam to manage ETL', 'Bonus points if...', 'Spark, DBT, and other data tools', '100% employer paid low deductible health plan, dental plan, vision plan, AD&D and life insurance', 'All Full Time Starry Employees Receive', 'Professional development assistance after six months of employment', 'Geographic information systems (GIS) experience', 'Orchestration experience with frameworks like Airflow', '1+ years of data engineering experience or 3+ years of engineering experience', ' An object-oriented programming language like Python or Java Shell scripting, Git, jq, awk, and a variety of other Unix utilities Various flavors of SQL, particularly those with GIS functionality Data warehouse and data lake architecture and design Workflow management tools like Airflow or Beam to manage ETL Stream processing platforms like Kafka or services like Kinesis Virtualization with Docker or an equivalent Spark, DBT, and other data tools Data engineering focused DevOps, observability, CI/CD, etc API and backend application development Unit and integration testing data pipelines ', 'We work hard, so we take care of each other and try to enjoy ourselves along the way. ', 'Catered meals on a weekly basis for employees working in the office', 'What You Will Be Working On', 'Streaming data experience with Kafka, Kinesis, RabbitMQ, etc', 'Qualifications', '12 weeks of 100% paid parental leave for new mothers and fathers after six months of continuous employment', 'Various flavors of SQL, particularly those with GIS functionality', 'About The Data Team', ' Streaming data experience with Kafka, Kinesis, RabbitMQ, etc Orchestration experience with frameworks like Airflow Container experience with frameworks like Docker Geographic information systems (GIS) experience Contributor to open source software ', 'Casual dress, community clubs, annual fitness reimbursement, stocked kitchen and other perks and discounts', 'Container experience with frameworks like Docker', 'About Starry', 'Stream processing platforms like Kafka or services like Kinesis', 'Familiar with CI/CD, Docker or similar, and testing data intensive processes', 'An object-oriented programming language like Python or Java', 'Familiar with SQL, profile query performance, and design database schema']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Robert Half,"Los Angeles, CA",,N/A,"['FOR immediate consideration please email Valerie Nielsen on LinkedIn with a copy of your resume or send over an email****Position: Data Engineer (Fully remote - ideally onsite a couple of days per week after covid)Salary: 115k-145k (benefits paid at 100% plus stock) (3 open roles - depending on level)Location: West LA, CAPlease note: this is a direct hire (not contract)Data EngineerA direct client of Robert Half Technology is looking for a well-rounded, Data Engineer to join their team on a direct hire basis in an effort to refine and grow their team. Our client is in the SaaS industry and the growth of their business is relying on the further development of their current team and massive growth during this time. orRequirementsMust Haves:Experience working with. Experience in R and JavaScript is a plus.cloud technologies: AWS (RDS, EC2, S3, Athena, Lambda, EMR, ECS), GCP (BigQuery, DataProc), Snowflake.Data platforms: PostgreSQL, MongoDB , data in different file formats (Parquet, JSON, CSV, Excel).With a deep understanding of data processing concepts and data modeling principles (the difference between OLTP, data warehouse, and data lake ). With advanced knowledge of SQL, Python, and Bash. Familiarity deploying solutions through docker or REST APIs.FOR immediate consideration please email Valerie Nielsen on LinkedIn with a copy of your resume****Robert Half Technology matches IT professionals with remote or on-site jobs on a temporary, project or full-time basis. From roles in software and applications to IT infrastructure and operations, we provide you unparalleled access to exciting career opportunities.Our experienced staffing professionals can promote you to employers and advocate on your behalf. We provide access to top jobs, competitive compensation and benefits, and free online training. For more opportunities, get the Robert Half app and receive instant notifications when our AI matches you with jobs.When you work with us, you’re working with the best. Robert Half has been recognized as one of FORTUNE’s “Most Admired Companies” every year since 1998 and was named to Forbes’ inaugural list of America’s Best Temporary Staffing Firms.Questions? Call your local office at 1.888.490.4429. All applicants applying for U.S. job openings must be authorized to work in the United States. Benefits are available to temporary professionals. Visit© 2020 Robert Half Technology. An Equal Opportunity Employer. M/F/Disability/Veterans. By clicking “Apply Now,” you’re agreeing to Robert Half’s Terms of Use (', 'Location: West LA, CA', '', 'FOR immediate consideration please email Valerie Nielsen on LinkedIn with a copy of your resume or send over an email****Position: Data Engineer (Fully remote - ideally onsite a couple of days per week after covid)Salary: 115k-145k (benefits paid at 100% plus stock) (3 open roles - depending on level)Location: West LA, CAPlease note: this is a direct hire (not contract)Data EngineerA direct client of Robert Half Technology is looking for a well-rounded, Data Engineer to join their team on a direct hire basis in an effort to refine and grow their team. Our client is in the SaaS industry and the growth of their business is relying on the further development of their current team and massive growth during this time. orRequirementsMust Haves:Experience working with. Experience in R and JavaScript is a plus.cloud technologies: AWS (RDS, EC2, S3, Athena, Lambda, EMR, ECS), GCP (BigQuery, DataProc), Snowflake.Data platforms: PostgreSQL, MongoDB , data in different file formats (Parquet, JSON, CSV, Excel).With a deep understanding of data processing concepts and data modeling principles (the difference between OLTP, data warehouse, and data lake ). With advanced knowledge of SQL, Python, and Bash. Familiarity deploying solutions through docker or REST APIs.', 'Description', 'Requirements', 'Position: Data Engineer (Fully remote - ideally onsite a couple of days per week after covid)', 'Salary: 115k-145k (benefits paid at 100% plus stock) (3 open roles - depending on level)', 'Data Engineer', 'FOR immediate consideration please email Valerie Nielsen on LinkedIn with a copy of your resume****Robert Half Technology matches IT professionals with remote or on-site jobs on a temporary, project or full-time basis. From roles in software and applications to IT infrastructure and operations, we provide you unparalleled access to exciting career opportunities.Our experienced staffing professionals can promote you to employers and advocate on your behalf. We provide access to top jobs, competitive compensation and benefits, and free online training. For more opportunities, get the Robert Half app and receive instant notifications when our AI matches you with jobs.When you work with us, you’re working with the best. Robert Half has been recognized as one of FORTUNE’s “Most Admired Companies” every year since 1998 and was named to Forbes’ inaugural list of America’s Best Temporary Staffing Firms.Questions? Call your local office at 1.888.490.4429. All applicants applying for U.S. job openings must be authorized to work in the United States. Benefits are available to temporary professionals. Visit© 2020 Robert Half Technology. An Equal Opportunity Employer. M/F/Disability/Veterans. By clicking “Apply Now,” you’re agreeing to Robert Half’s Terms of Use (', 'Must Haves:', 'Experience working with. Experience in R and JavaScript is a plus.cloud technologies: AWS (RDS, EC2, S3, Athena, Lambda, EMR, ECS), GCP (BigQuery, DataProc), Snowflake.Data platforms: PostgreSQL, MongoDB , data in different file formats (Parquet, JSON, CSV, Excel).With a deep understanding of data processing concepts and data modeling principles (the difference between OLTP, data warehouse, and data lake ). With advanced knowledge of SQL, Python, and Bash. Familiarity deploying solutions through docker or REST APIs.', 'With a deep understanding of data processing concepts and data modeling principles (the difference between OLTP, data warehouse, and data lake ). With advanced knowledge of SQL, Python, and Bash. Familiarity deploying solutions through docker or REST APIs.', 'Experience working with. Experience in R and JavaScript is a plus.', 'cloud technologies: AWS (RDS, EC2, S3, Athena, Lambda, EMR, ECS), GCP (BigQuery, DataProc), Snowflake.', 'Data platforms: PostgreSQL, MongoDB , data in different file formats (Parquet, JSON, CSV, Excel).']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Junior Data Engineer,Blackstone and Cullen,"Duluth, GA",5 days ago,52 applicants,"['', 'Qualifications For Data Engineer Middot']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,TuneCore,"New York, NY",2 weeks ago,Be among the first 25 applicants,"['', 'Matillion / Fivetran / MDM', 'Be comfortable working in a complex data environment involving in-depth data discovery, exploration and troubleshooting.', 'Comprehensive knowledge of data-modeling principles specifically with a focus on data analytics', 'Strong ETL proficiency using GUI-based tools or code-based patterns.', 'Have a thorough understanding of building data pipelines for RDBMS systems and various cloud-based data sources.', 'AWS', ""TuneCore is going through some amazing changes. In partnership with our parent company, Believe International, we are expanding rapidly with an emphasis on teamwork and career-growth. Our focus continues to lie in building a workplace that incorporates respect, fairness, transparency and growth in every step of our employees' journey and we intend to continue working tirelessly to create a culture of positivity, diversity and kindness in an enjoyable workplace. "", 'Experience with software engineering practicesMatillion / Fivetran / MDMAWSSnowflakeTableau', 'Be highly proficient in SQL & Python with an ability to work with large and highly complex datasets', 'Experience with software engineering practices', 'Excellent communication and interpersonal skills with the ability to liaise with multiple teams.', 'Candidates Should', 'TuneCore is seeking a seasoned Data Engineer who thrives in a dynamic and fast-paced environment and is able to work alongside our Software Engineering, Dev-Ops and Data Analytics teams to provide impactful solutions for all our data-driven business needs. ', 'Additional Information', 'Ensure adequate and thorough documentation of all existing and new processes.', 'Must have at least 5 years of demonstrable work experience as a Data Engineer or Data Architect in an enterprise environment.', 'Must have at least 5 years of demonstrable work experience as a Data Engineer or Data Architect in an enterprise environment.In-depth knowledge of RDBMS systems such as MySQL, MariaDB, PostgreSQL as well as cloud-based data warehousing solutions such as RedShift & Snowflake.Comprehensive knowledge of data-modeling principles specifically with a focus on data analyticsStrong ETL proficiency using GUI-based tools or code-based patterns.Excellent communication and interpersonal skills with the ability to liaise with multiple teams.', 'Be able to apply their skills in ingenious ways to solve new and existing problems.', 'Highly Desired Skills', 'Tableau', 'Be open to exploring and learning new technologies on the go.', 'Snowflake', 'Qualifications', 'Be highly proficient in SQL & Python with an ability to work with large and highly complex datasetsHave a thorough understanding of building data pipelines for RDBMS systems and various cloud-based data sources.Be comfortable working in a complex data environment involving in-depth data discovery, exploration and troubleshooting.Be able to apply their skills in ingenious ways to solve new and existing problems.Be open to exploring and learning new technologies on the go.Have the desire to lead and own the data engineering component of our new enterprise cloud-based data architecture.Be comfortable working in a cross-functional responsibility.Ensure adequate and thorough documentation of all existing and new processes.', 'Company Description', 'In-depth knowledge of RDBMS systems such as MySQL, MariaDB, PostgreSQL as well as cloud-based data warehousing solutions such as RedShift & Snowflake.', 'Job Description', 'Be comfortable working in a cross-functional responsibility.', 'Have the desire to lead and own the data engineering component of our new enterprise cloud-based data architecture.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Amazon Web Services (AWS),"Austin, TX",3 weeks ago,47 applicants,"['', ' Develop and support ETL pipelines with robust monitoring and alarming Develop data models that are optimized for business understand-ability and usability Develop and optimize data tables using best practices for partitioning, compression, parallelization, etc. Develop and maintain metadata, data catalog, and documentation for internal business customers Help internal business customers troubleshoot and optimize SQL and ETL solutions to solve reporting problems Work with internal business customers and partner technical teams to gather and document requirements for data publishing and consumption', ' Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy', ' Ability to adapt new solutions in a fast changing environment', ' Meets/exceeds Amazon’s leadership principles requirements for this role', 'Company', 'Preferred Qualifications', ' Extensive experience working with cloud services (AWS or MS Azure or GCS etc.) with a strong understanding of cloud databases (e.g. Redshift/Aurora/DynamoDB), compute engines (e.g. EMR/Glue), data streaming (e.g. Kinesis), storage (e.g. S3) etc.', ' Develop data models that are optimized for business understand-ability and usability', "" Bachelor's degree in Computer Science, Engineering, Mathematics, or a related technical discipline 5+ years of industry experience in Data Engineering, BI Engineer or related field with experience manipulating, processing, and extracting value from large datasets Ability to write high quality, maintainable, and robust code, often in SQL, Scala and Python. 5+ Years of Data Warehouse Experience with Oracle, Redshift, PostgreSQL, etc. Demonstrated strength in SQL, python/pyspark scripting, data modeling, ETL development, and data warehousing Extensive experience working with cloud services (AWS or MS Azure or GCS etc.) with a strong understanding of cloud databases (e.g. Redshift/Aurora/DynamoDB), compute engines (e.g. EMR/Glue), data streaming (e.g. Kinesis), storage (e.g. S3) etc. Experience/Exposure using big data technologies (Hadoop, Hive, Hbase, Spark, EMR, etc.)"", ' Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations', 'Key Responsibilities Include', ' Develop and support ETL pipelines with robust monitoring and alarming', ' Develop and maintain metadata, data catalog, and documentation for internal business customers', ' 5+ Years of Data Warehouse Experience with Oracle, Redshift, PostgreSQL, etc. Demonstrated strength in SQL, python/pyspark scripting, data modeling, ETL development, and data warehousing', ' Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets', 'Description', "" Bachelor's degree in Computer Science, Engineering, Mathematics, or a related technical discipline"", ' Strong analytical skills, 2+ years’ experience with Python and an interest in Machine Learning', ' Work with internal business customers and partner technical teams to gather and document requirements for data publishing and consumption', ' 5+ years of direct experience as a Data Engineer or related field in a company with large, complex data sources', ' Masters in computer science, mathematics, statistics, economics, or other quantitative fields', ' Experience working with AWS big data technologies (EMR, Redshift, S3, Glue, Athena, Kinesis and Lambda for serverless ETL)', ' Experience/Exposure using big data technologies (Hadoop, Hive, Hbase, Spark, EMR, etc.)', ' Meets/exceeds Amazon’s functional/technical depth and complexity for this role', ' Develop and optimize data tables using best practices for partitioning, compression, parallelization, etc.', 'Basic Qualifications', ' 5+ years of industry experience in Data Engineering, BI Engineer or related field with experience manipulating, processing, and extracting value from large datasets', ' Ability to write high quality, maintainable, and robust code, often in SQL, Scala and Python.', ' Help internal business customers troubleshoot and optimize SQL and ETL solutions to solve reporting problems', ' Masters in computer science, mathematics, statistics, economics, or other quantitative fields 5+ years of direct experience as a Data Engineer or related field in a company with large, complex data sources Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets Experience working with AWS big data technologies (EMR, Redshift, S3, Glue, Athena, Kinesis and Lambda for serverless ETL) Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations Strong analytical skills, 2+ years’ experience with Python and an interest in Machine Learning Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy Ability to adapt new solutions in a fast changing environment Meets/exceeds Amazon’s leadership principles requirements for this role Meets/exceeds Amazon’s functional/technical depth and complexity for this role']",Not Applicable,Full-time,Strategy/Planning,Computer Software,2021-03-24 13:05:10
Data Engineer,Avanade,"Dover, DE",4 weeks ago,Be among the first 25 applicants,"['', 'About Avanade', ' Implemented analytics systems for more than 550 clients ', ' 14-time winner of Microsoft Partner of the Year ', ' 14-time winner of Microsoft Partner of the Year  24,000+ certifications in Microsoft technology  90+ Microsoft partner awards  17 Gold Competencies  3,500 analytics professionals worldwide  1,000 data engineers  Implemented analytics systems for more than 550 clients  400 AI practitioners  300 cognitive service experts ', ' Assess client needs to build bespoke data design services ', 'About You', ' Do you enjoy making sure that information is accessible and easy to use? So do we. ', 'Your Skills And Business Experience Include', 'About The Role', ' Transforming business needs into technical solutions  Mapping data and analytics  Data profiling, cataloguing and mapping to enable the design and build of technical data flows  Use proven methods to solve business problems using Azure Data and Analytics services in combination with building data pipelines, data streams and system integration  Knowledge of multiple Azure data applications including Azure Databricks  Experience in preparing data for and building pipelines and architecture ', ' 90+ Microsoft partner awards ', ' 3,500 analytics professionals worldwide ', ' Build the building blocks for transforming enterprise data solutions ', ' Mapping data and analytics ', ' Use proven methods to solve business problems using Azure Data and Analytics services in combination with building data pipelines, data streams and system integration ', ' 400 AI practitioners ', ' Use your sound eye for business to translate business requirements into technical solutions ', ' 300 cognitive service experts ', ' Transforming business needs into technical solutions ', ' Give colleagues and clients the tools to find and use data for routine and non-routine analysis  Use your sound eye for business to translate business requirements into technical solutions  Analyze current business practices, processes and procedures to spot future opportunities  Assess client needs to build bespoke data design services  Build the building blocks for transforming enterprise data solutions  Design and build modern data pipelines, data streams, and data service Application Programming Interfaces (APIs)  Craft the architectures, data warehouses and databases that support access and Advanced Analytics, and bring them to life through modern visualization tools  Implement effective metrics and monitoring  Be comfortable to make your own decisions and guide your colleagues  Travel as needed to various client locations ', ' Design and build modern data pipelines, data streams, and data service Application Programming Interfaces (APIs) ', 'Why Avanade', ' 17 Gold Competencies ', 'Day-to-day, You Will', ' 1,000 data engineers ', ' Knowledge of multiple Azure data applications including Azure Databricks ', ' Be comfortable to make your own decisions and guide your colleagues ', ' 24,000+ certifications in Microsoft technology ', ' Travel as needed to various client locations ', ' Experience in preparing data for and building pipelines and architecture ', ' Implement effective metrics and monitoring ', ' Craft the architectures, data warehouses and databases that support access and Advanced Analytics, and bring them to life through modern visualization tools ', 'How We Support You', ' Analyze current business practices, processes and procedures to spot future opportunities ', ' Give colleagues and clients the tools to find and use data for routine and non-routine analysis ', ' Data profiling, cataloguing and mapping to enable the design and build of technical data flows ', 'Job Description']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,The Bachrach Group,New York City Metropolitan Area,2 weeks ago,116 applicants,"['', 'We are looking for a self-motivated Data Engineer to join a Data Management Team. You will be responsible for expanding and optimizing a Master Data Management Platform and ETL architecture, as well as optimizing data flow and collection for cross-functional teams. ', 'You must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products. The right candidate will be excited by the prospect of optimizing and/or re-designing a data architecture to support the next generation of products and data initiatives.', 'Strong Data Development skills in Oracle SQL and SSIS, ETL, Linux/UNIX, Data Warehousing, and Data Modeling', 'You have strong communication skills and an ability to synthesize information.', 'Experience with IBM Infosphere Master Data Management is a plus', 'Scripting in Unix or a Windows environment', 'Data Engineer', 'Strong Data Development skills in Oracle SQL and SSIS, ETL, Linux/UNIX, Data Warehousing, and Data ModelingDatabase engineering experience with Microsoft SQL Server.Database experience in Oracle (10g) and Microsoft SQL Server (2005 or 2008)Experience with ETL products like SSIS or Clover ETL.Scripting in Unix or a Windows environmentExperience with various structured data storage methods.You are solution-driven with an ability to understand the big picture.You have great analytical and problem-solving skills.You have strong communication skills and an ability to synthesize information.Experience with IBM Infosphere Master Data Management is a plusUS Citizens and Green Card Holders (Permanent Residents) can only be considered.We are UNABLE to provide sponsorship to anyone.', 'Experience with ETL products like SSIS or Clover ETL.', 'We are UNABLE to provide sponsorship to anyone.', 'Experience with various structured data storage methods.', 'US Citizens and Green Card Holders (Permanent Residents) can only be considered.', 'You have great analytical and problem-solving skills.', 'You are solution-driven with an ability to understand the big picture.', 'WHAT WE ARE LOOKING FOR:', 'Database engineering experience with Microsoft SQL Server.', 'Database experience in Oracle (10g) and Microsoft SQL Server (2005 or 2008)']",Associate,Full-time,Information Technology,Hospital & Health Care,2021-03-24 13:05:10
Data Engineer,Motion Recruitment,"Los Angeles, CA",21 hours ago,Be among the first 25 applicants,"['', 'SQL ', 'Database experience ', '401(k)', 'Medical Insurance & Health Savings Account (HSA)', 'You will receive the following benefits:', 'Pre-tax Commuter Benefit', ""Someone who's close to Analytics (BI reporting), not infrastructure"", 'Python', 'Competitive Salary: Up to $XXK/year, DOE', ' Big Data (Hive, Hadoop)  Snowflake ', 'Basic understanding of Database, Data Modeling', ' Competitive Salary: Up to $XXK/year, DOE You will receive the following benefits: Medical Insurance & Health Savings Account (HSA) 401(k) Paid Sick Time Leave Pre-tax Commuter Benefit ', 'Paid Sick Time Leave', 'Required Skills & Experience', "" SQL  Python Database experience  Basic understanding of Database, Data Modeling Someone who's close to Analytics (BI reporting), not infrastructure "", 'Plusses', 'Snowflake', 'Big Data (Hive, Hadoop) ', 'Job Description']",Entry level,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Data Engineer,Advantis Global,"Cupertino, CA",2 days ago,197 applicants,"['', ' using Spark, Kafka, Hadoop', ' Data visualization or web development', ' Pandas Data visualization or web development', 'About This Opportunity', ' Pandas', ' Experience with Web or REST API development in Python', ' Experience with workflow scheduling / orchestration such as Kubernetes or Airflow', ' Experience with Postgres database, familiar with SQL scripting', 'Opportunity For You', 'Key Success Factors', ' 2 years professional experience', 'Pluses', ' Experience in implementing data pipelines using python, familiar with Pandas and Numpy', ' Experience with query APIs', ' Extract Transform Load (ETL) experience', ' using JSON, ProtocolBuffers, or XML', ' Experience with Unix-based command line interface and Bash scripts', ' 2 years professional experience Experience in implementing data pipelines using python, familiar with Pandas and Numpy Experience with workflow scheduling / orchestration such as Kubernetes or Airflow Extract Transform Load (ETL) experience using Spark, Kafka, Hadoop Experience with query APIs using JSON, ProtocolBuffers, or XML Experience with Unix-based command line interface and Bash scripts Experience with Postgres database, familiar with SQL scripting Experience with Web or REST API development in Python']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Center Engineer,Cloudflare,"Austin, TX",18 hours ago,Be among the first 25 applicants,"['', ' Collaborating with internal teams (infrastructure, network engineering and SRE). Create documentation and manage remote contractors to complete datacenter installations and upgrades, including hardware manufacturers, datacenter and network providers, logistics partners and other service providers in support of our 120 and growing datacenter location  Build rack elevations, and work with remote contractors to rack and cable infrastructure globally Coordinate installation of cross-connects globally in support of physical network expansion Assist with the definition, documentation and implementation of consistent processes across all region Aggressively seek opportunities to introduce cutting-edge technology and automation solutions that are effective, efficient and scalable in order to improve our ability to deploy and maintain our global infrastructure Limited travel ', 'Must be proactive with proven ability to learn fast and execute on multiple tasks simultaneously', 'Ability to write scripts for internal tool', ' Minimum of 2 years of related data center or Linux systems administration experience Linux/Unix systems administration Basic configuration management tool experience like Saltstack, Chef, Puppet or Ansible Network hardware administration Familiarity with day-to-day tasks and projects common in Data Center Operations Ability to write scripts for internal tool Experience running and improving operational processes in a rapidly changing environment ', 'Basic configuration management tool experience like Saltstack, Chef, Puppet or Ansible', '1.1.1.1', 'Scripting or software development experience in Bash, Python or Go-lang', 'Strong verbal and written communication skills, problem-solving skills, attention to detail, and interpersonal skills', 'Bachelor’s degree; technical background in engineering, computer science, or MIS a plus', 'Familiarity with day-to-day tasks and projects common in Data Center Operations', 'Knowledge of the OSI-model and experience isolating network, hardware and software issues', ' Build rack elevations, and work with remote contractors to rack and cable infrastructure globally', 'Aggressively seek opportunities to introduce cutting-edge technology and automation solutions that are effective, efficient and scalable in order to improve our ability to deploy and maintain our global infrastructure', 'What Makes Cloudflare Special?', 'Ability to manage MS excel and Google spreadsheets', 'Linux/Unix systems administration', 'Previous experience installing / maintaining datacenter (and other IT) infrastructure and DCIM tools', 'Assist with the definition, documentation and implementation of consistent processes across all region', ' Multi-lingual; experience working with infrastructure in multiple countries Comfortable with remote “lights-out” and out-of-band access to data center resources Linux certifications Knowledge of the OSI-model and experience isolating network, hardware and software issues Configuration management systems such as Saltstack, Chef, Puppet or Ansible Scripting or software development experience in Bash, Python or Go-lang Familiarity with load balancing and reverse proxies such as Nginx, Varnish, HAProxy, Apache ', 'Must be a team player', ' Athenian Project ', 'Collaborating with internal teams (infrastructure, network engineering and SRE). Create documentation and manage remote contractors to complete datacenter installations and upgrades, including hardware manufacturers, datacenter and network providers, logistics partners and other service providers in support of our 120 and growing datacenter location', 'Coordinate installation of cross-connects globally in support of physical network expansion', 'About Us', 'Limited travel', 'Familiarity with load balancing and reverse proxies such as Nginx, Varnish, HAProxy, Apache', 'About The Department', 'Project Galileo', 'Network hardware administration', 'Multi-lingual; experience working with infrastructure in multiple countries', 'Minimum of 2 years of related data center or Linux systems administration experience', 'Comfortable with remote “lights-out” and out-of-band access to data center resources', 'Direct experience executing on datacenter / infrastructure projects with many moving parts', ' Bachelor’s degree; technical background in engineering, computer science, or MIS a plus Direct experience executing on datacenter / infrastructure projects with many moving parts Previous experience installing / maintaining datacenter (and other IT) infrastructure and DCIM tools Experience running and improving operational processes in a rapidly changing environment Strong verbal and written communication skills, problem-solving skills, attention to detail, and interpersonal skills Must be proactive with proven ability to learn fast and execute on multiple tasks simultaneously Ability to manage MS excel and Google spreadsheets Comfortable handling basic program management responsibilities (prioritization, planning, scheduling, status reporting) such as JIRA Experience managing remote contractors Must be a team player ', 'Experience managing remote contractors', 'Configuration management systems such as Saltstack, Chef, Puppet or Ansible', 'Bonus Points', 'Other Responsibilities May Include', 'Required Experience', 'Experience running and improving operational processes in a rapidly changing environment', 'Comfortable handling basic program management responsibilities (prioritization, planning, scheduling, status reporting) such as JIRA', 'Examples Of Desirable Skills, Knowledge And Experience', 'Path Forward Partnership', 'Linux certifications']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Piper Companies,"Raleigh, NC",20 hours ago,Be among the first 25 applicants,"['', 'Drive the vision for implementing strategies around Data Lake and Warehouse architecture', 'Qualifications For The Data Engineer Include', 'Salary Range: $135,000 - $145,000 (based on experience)Comprehensive benefits including medical, vision, dental, 401K, PTO and bonus', 'Experience working with Data Lake and Warehouse architecture', 'Introduce and maintain data platforms, data governance and data security capabilities and documentation including establishing roadmaps ', 'Develop and design models for complex analytical and data warehouse systems', 'Responsibilities For The Data Engineer Include', 'Salary Range: $135,000 - $145,000 (based on experience)', 'Experience working with Data Lake and Warehouse architectureTrack record of delivering high-quality features and services in a SaaS web applicationsHands-on experience and proficient knowledge of functional programing using languagesExperience working with Docker and Kubernetes', 'Security Clearance', 'Hands-on experience and proficient knowledge of functional programing using languages', 'Perform tasks related to data profiling, database design, data analysis, data quality, metadata management and support', 'Comprehensive benefits including medical, vision, dental, 401K, PTO and bonus', 'Drive the vision for implementing strategies around Data Lake and Warehouse architectureDevelop and design models for complex analytical and data warehouse systemsPerform tasks related to data profiling, database design, data analysis, data quality, metadata management and supportIntroduce and maintain data platforms, data governance and data security capabilities and documentation including establishing roadmaps ', 'Experience working with Docker and Kubernetes', 'Compensation For The Data Engineer Include', 'Job Category', 'Track record of delivering high-quality features and services in a SaaS web applications']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer II,Rivian,"Palo Alto, CA",1 day ago,Be among the first 25 applicants,"['', '2-5 years of hands-on IT experience in Data Warehouse, ETL & Reporting. Extensive experience of executing multiple data warehouse projects in variety of business domain', 'Design and develop secure, scalable, high-performance and reliable (cost effective) big data and analytics solutions that enable tableau dashboards and reports for cross-functional teams and executives.', 'Hands-on experience with Airflow, Alteryx and Tableau. (Certification is a plus)', 'Spread the data culture across the company by enabling best practices, standards, governed processes and relevant technologies in the Data platforms.', 'Role Summary', 'Establish technical partnerships with data scientists/engineers, and product owners/managers through brainstorming sessions, design reviews, and retrospectives.', 'Own the data management technical roadmap and execution aligning with peer platform organizations and product delivery, Ensure alignment with over-arching enterprise platform initiatives', ' Excellent communication and interpersonal skills, with the ability to break down complex technical problems into simple elegant solutions understandable by non-technical business partners. 2-5 years of hands-on IT experience in Data Warehouse, ETL & Reporting. Extensive experience of executing multiple data warehouse projects in variety of business domain 3+ years of experience in understanding variety of complex business use cases and modelling the data in the data warehouse. Very strong understanding of core data warehouse concepts. 3+ years of hands-on experience in productionizing and deploying Big Data platforms and applications, current experience working with: Relational/SQL, distributed columnar data stores/NoSQL databases. 3+ years of deep experience using S3, Python, SQL (Redshift) and Spark. Strong understanding of AWS Services like S3, Glue, Kinesis, Lambdas and Step functions. Strong experience in R or similar technologies with a background in data science a plus. Hands-on experience with Airflow, Alteryx and Tableau. (Certification is a plus) Familiarity with graph databases, timeseries databases, NiFi, Spark streaming, Kafka, Hive, Impala, Avro, and more is a plus. Strong understanding of Supply Chain and Financials business processes like Order Bookings, Invoicing/Billings, Forecasting, Demand Planning, Financial Planning. Purchasing, Payables, Manufacturing etc. ', 'Define templates and process for the analysis of data models, data flows, and integration patterns for structural deficiencies/soundness to build a robust and complete future state model', 'Lead by example to review code, look for code issues, provide meaningful and relevant feedback to engineers, stay up to date with system changes and latest technologies', 'Comfortably present and acquire consensus using multiple methods: Written diagrams and text, in-person meetings, in-person and remote presentations to both technical and business audiences', 'Excellent communication and interpersonal skills, with the ability to break down complex technical problems into simple elegant solutions understandable by non-technical business partners.', 'Strong understanding of Supply Chain and Financials business processes like Order Bookings, Invoicing/Billings, Forecasting, Demand Planning, Financial Planning. Purchasing, Payables, Manufacturing etc.', 'Responsibilities', ' Design and develop secure, scalable, high-performance and reliable (cost effective) big data and analytics solutions that enable tableau dashboards and reports for cross-functional teams and executives. Use Data & Analytics to answer business questions that leady to insights and actionable outcomes. Understand and deploy processes for: SDC, CI/CD, quality checks, error handling, error notifications, security, extensibility and maintainability of big data and analytics platform software and services Define templates and process for the analysis of data models, data flows, and integration patterns for structural deficiencies/soundness to build a robust and complete future state model Lead by example to review code, look for code issues, provide meaningful and relevant feedback to engineers, stay up to date with system changes and latest technologies Comfortably present and acquire consensus using multiple methods: Written diagrams and text, in-person meetings, in-person and remote presentations to both technical and business audiences Establish technical partnerships with data scientists/engineers, and product owners/managers through brainstorming sessions, design reviews, and retrospectives. Own the data management technical roadmap and execution aligning with peer platform organizations and product delivery, Ensure alignment with over-arching enterprise platform initiatives Spread the data culture across the company by enabling best practices, standards, governed processes and relevant technologies in the Data platforms. ', 'Qualifications', 'Strong understanding of AWS Services like S3, Glue, Kinesis, Lambdas and Step functions.', '3+ years of hands-on experience in productionizing and deploying Big Data platforms and applications, current experience working with: Relational/SQL, distributed columnar data stores/NoSQL databases.', 'Use Data & Analytics to answer business questions that leady to insights and actionable outcomes.', '3+ years of experience in understanding variety of complex business use cases and modelling the data in the data warehouse. Very strong understanding of core data warehouse concepts.', '3+ years of deep experience using S3, Python, SQL (Redshift) and Spark.', 'Familiarity with graph databases, timeseries databases, NiFi, Spark streaming, Kafka, Hive, Impala, Avro, and more is a plus.', 'Understand and deploy processes for: SDC, CI/CD, quality checks, error handling, error notifications, security, extensibility and maintainability of big data and analytics platform software and services', 'Strong experience in R or similar technologies with a background in data science a plus.']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer (Python),Regions Bank,"Lakewood, CO",3 days ago,Be among the first 25 applicants,"['', 'Experience with Python', 'The target information listed below is the 10th and 50th percent of the market range, based on the Birmingham, AL market and level of the position', 'Experience developing solutions for the financial services industry', 'Location Details', 'Experience building data solutions at scaleExperience designing and building relational data structures in multiple environmentsExperience with Airflow, Argo, Luigi, or similar orchestration toolExperience with DevOps principals and CI/CD.Experience with Docker and KubernetesExperience with No-SQL databases such as HBase, Cassandra, or MongoDBExperience with streaming technologies such as Kafka, Flink, or Spark StreamingExperience working with Hadoop ecosystem building Data Assets at an enterprise scaleProven record of accomplishment of delivering operational Data solutions including Report and Model Ready Data AssetsSignificant experience working with senior executives in the use of data, reporting and visualizations to support strategic and operational decision makingStrong ability to transform and integrate complex data from multiple sources into accessible, understandable, and usable data assets and frameworksStrong background in synthesizing data and analytics in a large (fortune 500), complex, and highly regulated environmentStrong technical background including database and business intelligence skills', 'Position Type', 'Builds data pipelines to collect and arrange data and manage data storage in Regions’ big data environment', 'Experience with Docker and Kubernetes', 'Primary Responsibilities', 'Supports any team members in the development of such information delivery and aid in the automation of data products', 'Provides consultation to all areas of the organization that plan to use data to make decisions', 'Experience working with Hadoop ecosystem building Data Assets at an enterprise scale', 'Prior banking or financial Services experience', 'Experience with streaming technologies such as Kafka, Flink, or Spark Streaming', 'Two (2) years of data-oriented experience including experience managing data and analytics resources', 'Partners with Regions Technology partners to Design, Build, and Maintain the data-based structures and systems in support of Data and Analytics and Data Product use cases', 'Compensation Details', 'Proven record of accomplishment of delivering operational Data solutions including Report and Model Ready Data Assets', 'Additional Locations:', 'Skills And Competencies', 'Requirements', 'Target Range To', 'Target Range From', 'Prior banking or financial Services experienceExperience developing solutions for the financial services industryBackground in Big Data Engineering and Advanced Data Analytics ', 'Background in Big Data Engineering and Advanced Data Analytics ', 'Strong ability to transform and integrate complex data from multiple sources into accessible, understandable, and usable data assets and frameworks', 'Strong background in synthesizing data and analytics in a large (fortune 500), complex, and highly regulated environment', 'Location:', 'Strong technical background including database and business intelligence skills', 'Strongly Preferred', 'Experience designing and building relational data structures in multiple environments', 'Builds robust, testable programs for moving, transforming, and loading data using big data tools such as Spark.', 'Experience with Airflow, Argo, Luigi, or similar orchestration tool', 'Experience with DevOps principals and CI/CD.', 'Experience with No-SQL databases such as HBase, Cassandra, or MongoDB', 'Experience building data solutions at scale', 'Significant experience working with senior executives in the use of data, reporting and visualizations to support strategic and operational decision making', ""Bachelor's degree in Business or a technical related fieldTwo (2) years of data-oriented experience including experience managing data and analytics resources"", 'Preferences', ""Bachelor's degree in Business or a technical related field"", 'Partners with Regions Technology partners to Design, Build, and Maintain the data-based structures and systems in support of Data and Analytics and Data Product use casesBuilds data pipelines to collect and arrange data and manage data storage in Regions’ big data environmentBuilds robust, testable programs for moving, transforming, and loading data using big data tools such as Spark.Ensures data is prepared, arranged and ready for each defined business use caseProvides consultation to all areas of the organization that plan to use data to make decisionsSupports any team members in the development of such information delivery and aid in the automation of data products', 'Job Description', 'Ensures data is prepared, arranged and ready for each defined business use case']",Not Applicable,Full-time,Information Technology,Banking,2021-03-24 13:05:10
Software Engineer - Data,Twitch,"San Francisco, CA",3 weeks ago,103 applicants,"['', 'You Will', ' 2+ years experience in Go or Python.', ' 3+ years of software development experience .', ' 401(k) , Maternity & Parental Leave ', 'Amazon Employee Discount', '1+ years working with distributed, highly available systems.', ' Develop new capabilities in our data warehouses and pipelines. Improve the reliability, flexibility , and scalability of our existing tools. Collaborate on our vision as we scale to our next petabyte. ', 'About The Role', 'Improve the reliability, flexibility , and scalability of our existing tools.', 'Flexible PTO', 'Collaborate on our vision as we scale to our next petabyte.', ' Commuter Benefits ', 'Breakfast, Lunch & Dinner Served Daily', 'Medical, Dental, Vision & Disability Insurance', 'About Us', ""You've worked with Amazon Web Services (AWS)."", ""You've made petabytes of data usable."", '  3+ years of software development experience .  2+ years experience in Go or Python. 1+ years working with distributed, highly available systems. ', 'Develop new capabilities in our data warehouses and pipelines.', 'You contribute to open source.', 'Bonus Points', ' Medical, Dental, Vision & Disability Insurance  401(k) , Maternity & Parental Leave  Flexible PTO  Commuter Benefits  Amazon Employee Discount Monthly Contribution & Discounts for Wellness Related Activities & Programs (e.g., gym memberships, off-site massages), Breakfast, Lunch & Dinner Served Daily Free Snacks & Beverages  ', 'Free Snacks & Beverages ', "" You've made petabytes of data usable. You've worked with Amazon Web Services (AWS). You contribute to open source. "", 'You Have:', 'Perks', 'Monthly Contribution & Discounts for Wellness Related Activities & Programs (e.g., gym memberships, off-site massages),']",Not Applicable,Full-time,Engineering,Internet,2021-03-24 13:05:10
Data Engineer,SECU Credit Union,"Linthicum, MD",4 days ago,Be among the first 25 applicants,"['', 'Demonstrated experience with self-directed work on large and complex problems.', 'Strong interpersonal and communication skills.', 'Implement proper data governance procedures with a strong focus on data cleanliness to support key decision making throughout the organization. ', 'Design, develop and maintain data structures and pipelines to facilitate the development of reports, visualizations, data sets and analytical models used by analysts, data scientists and other business stakeholders across the organization.Play an important role in the continued evolution of SECU’s modern data architecture. By collaborating with other data engineers, data scientists, analysts, technical consultants and other business users, you will ensure that data sets are of high integrity while in proper alignment with business needs.Implement proper data governance procedures with a strong focus on data cleanliness to support key decision making throughout the organization. Integrate automated data quality assurance mechanisms into orchestrated data pipelines using modern frameworks and performs root-cause analysis as needed to uncover data irregularities.Provide SQL, Python and other programming-specific technical guidance to other engineers and analysts to support continued learning and development throughout the organization. Support corporate research by designing and executing complex ad hoc queries as needed.Utilize version control systems such as Git as part of a CI/CD code pipeline, which includes data ingestion, transformation, orchestration, anomaly detection and metadata management.Develop a detailed understanding of the table structure of all critical databases and pipelines, serving as a liaison during key planning phases, which include technical and non-technical stakeholders. Be largely self-directed and will provide extensive communication and documentation in support of their work.Demonstrated experience with self-directed work on large and complex problems.', 'The SECU pledge Be relevant and significant, day in and day out, in the lives of our members, employees and the communities we serve in a highly ethical and fiscally responsible manner.', 'SECU is an Equal Opportunity Employer', 'Basic understanding of statistical techniques and concepts.', '** ', 'Generous paid leave programs', 'Market competitive payRobust 401(k) retirement savings programGenerous paid leave programsAnd more….SECU 2021 Benefits Guide', '4+ years of data-centric programming experience as a software/data engineer, including at least one year of experience implementing data pipelines using modern tools.', 'Experience with modern data warehouse frameworks, tools and architectures.', 'Bachelor’s degree or greater in any of the following or similar areas Data Science, Computer Science, Management Information Science, etc. Equivalent professional experience considered in lieu of a degree.', 'Innovative, creative and forward thinking; solutions-driven.', 'Occasional in-person meetings and commitments throughout the year are required**What You’ll Do.Design, develop and maintain data structures and pipelines to facilitate the development of reports, visualizations, data sets and analytical models used by analysts, data scientists and other business stakeholders across the organization.Play an important role in the continued evolution of SECU’s modern data architecture. By collaborating with other data engineers, data scientists, analysts, technical consultants and other business users, you will ensure that data sets are of high integrity while in proper alignment with business needs.Implement proper data governance procedures with a strong focus on data cleanliness to support key decision making throughout the organization. Integrate automated data quality assurance mechanisms into orchestrated data pipelines using modern frameworks and performs root-cause analysis as needed to uncover data irregularities.Provide SQL, Python and other programming-specific technical guidance to other engineers and analysts to support continued learning and development throughout the organization. Support corporate research by designing and executing complex ad hoc queries as needed.Utilize version control systems such as Git as part of a CI/CD code pipeline, which includes data ingestion, transformation, orchestration, anomaly detection and metadata management.Develop a detailed understanding of the table structure of all critical databases and pipelines, serving as a liaison during key planning phases, which include technical and non-technical stakeholders. Be largely self-directed and will provide extensive communication and documentation in support of their work.Demonstrated experience with self-directed work on large and complex problems.What We’re Looking For.Bachelor’s degree or greater in any of the following or similar areas Data Science, Computer Science, Management Information Science, etc. Equivalent professional experience considered in lieu of a degree.4+ years of data-centric programming experience as a software/data engineer, including at least one year of experience implementing data pipelines using modern tools.Software/data engineering experience with Python.Advanced SQL skills.Experience with modern data warehouse frameworks, tools and architectures.Basic understanding of statistical techniques and concepts.Innovative, creative and forward thinking; solutions-driven.Strong interpersonal and communication skills.Ability to work alone or in teams as appropriate.Experience working in a highly collaborative, team environment.Experience communicating among multiple stakeholders and balancing multiple projects. What You’ll Get.In addition to never being controlled by outside owners, one of the great perks of joining Team SECU is our total rewards package, which includesMarket competitive payRobust 401(k) retirement savings programGenerous paid leave programsAnd more….SECU 2021 Benefits GuideIf you’re interested in a challenging and rewarding career, then SECU is for you!We can’t wait to get to know you!SECU is an Equal Opportunity Employer', 'And more….SECU 2021 Benefits Guide', 'We can’t wait to get to know you!', 'Robust 401(k) retirement savings program', 'Design, develop and maintain data structures and pipelines to facilitate the development of reports, visualizations, data sets and analytical models used by analysts, data scientists and other business stakeholders across the organization.', 'Utilize version control systems such as Git as part of a CI/CD code pipeline, which includes data ingestion, transformation, orchestration, anomaly detection and metadata management.', 'Advanced SQL skills.', 'What You’ll Get.', 'Develop a detailed understanding of the table structure of all critical databases and pipelines, serving as a liaison during key planning phases, which include technical and non-technical stakeholders. ', 'Market competitive pay', 'If you’re interested in a challenging and rewarding career, then SECU is for you!', 'This position is remote for candidates living in Maryland or bordering/nearby states** Occasional in-person meetings and commitments throughout the year are required**What You’ll Do.Design, develop and maintain data structures and pipelines to facilitate the development of reports, visualizations, data sets and analytical models used by analysts, data scientists and other business stakeholders across the organization.Play an important role in the continued evolution of SECU’s modern data architecture. By collaborating with other data engineers, data scientists, analysts, technical consultants and other business users, you will ensure that data sets are of high integrity while in proper alignment with business needs.Implement proper data governance procedures with a strong focus on data cleanliness to support key decision making throughout the organization. Integrate automated data quality assurance mechanisms into orchestrated data pipelines using modern frameworks and performs root-cause analysis as needed to uncover data irregularities.Provide SQL, Python and other programming-specific technical guidance to other engineers and analysts to support continued learning and development throughout the organization. Support corporate research by designing and executing complex ad hoc queries as needed.Utilize version control systems such as Git as part of a CI/CD code pipeline, which includes data ingestion, transformation, orchestration, anomaly detection and metadata management.Develop a detailed understanding of the table structure of all critical databases and pipelines, serving as a liaison during key planning phases, which include technical and non-technical stakeholders. Be largely self-directed and will provide extensive communication and documentation in support of their work.Demonstrated experience with self-directed work on large and complex problems.What We’re Looking For.Bachelor’s degree or greater in any of the following or similar areas Data Science, Computer Science, Management Information Science, etc. Equivalent professional experience considered in lieu of a degree.4+ years of data-centric programming experience as a software/data engineer, including at least one year of experience implementing data pipelines using modern tools.Software/data engineering experience with Python.Advanced SQL skills.Experience with modern data warehouse frameworks, tools and architectures.Basic understanding of statistical techniques and concepts.Innovative, creative and forward thinking; solutions-driven.Strong interpersonal and communication skills.Ability to work alone or in teams as appropriate.Experience working in a highly collaborative, team environment.Experience communicating among multiple stakeholders and balancing multiple projects. What You’ll Get.In addition to never being controlled by outside owners, one of the great perks of joining Team SECU is our total rewards package, which includesMarket competitive payRobust 401(k) retirement savings programGenerous paid leave programsAnd more….SECU 2021 Benefits GuideIf you’re interested in a challenging and rewarding career, then SECU is for you!We can’t wait to get to know you!SECU is an Equal Opportunity Employer', 'This position is remote for candidates living in Maryland or bordering/nearby states** ', 'What We’re Looking For', 'Play an important role in the continued evolution of SECU’s modern data architecture. By collaborating with other data engineers, data scientists, analysts, technical consultants and other business users, you will ensure that data sets are of high integrity while in proper alignment with business needs.', 'Software/data engineering experience with Python.', 'This position is remote for candidates living in Maryland or bordering/nearby states', 'Integrate automated data quality assurance mechanisms into orchestrated data pipelines using modern frameworks and performs root-cause analysis as needed to uncover data irregularities.', 'Ability to work alone or in teams as appropriate.', 'Data Engineer', 'We are looking for innovative and dynamic professionals with a passion for exceptional service to join our SECU team as a', 'Experience working in a highly collaborative, team environment.', 'Occasional in-person meetings and commitments throughout the year are required', 'What You’ll Do.', 'Bachelor’s degree or greater in any of the following or similar areas Data Science, Computer Science, Management Information Science, etc. Equivalent professional experience considered in lieu of a degree.4+ years of data-centric programming experience as a software/data engineer, including at least one year of experience implementing data pipelines using modern tools.Software/data engineering experience with Python.Advanced SQL skills.Experience with modern data warehouse frameworks, tools and architectures.Basic understanding of statistical techniques and concepts.Innovative, creative and forward thinking; solutions-driven.Strong interpersonal and communication skills.Ability to work alone or in teams as appropriate.Experience working in a highly collaborative, team environment.Experience communicating among multiple stakeholders and balancing multiple projects. ', 'Provide SQL, Python and other programming-specific technical guidance to other engineers and analysts to support continued learning and development throughout the organization. Support corporate research by designing and executing complex ad hoc queries as needed.', 'Experience communicating among multiple stakeholders and balancing multiple projects. ', 'Be largely self-directed and will provide extensive communication and documentation in support of their work.', '**']",Entry level,Full-time,Information Technology,Banking,2021-03-24 13:05:10
Data Engineer,Northwestern Mutual,"Milwaukee, WI",4 weeks ago,Be among the first 25 applicants,"['', 'Familiarity with public cloud and technologies (AWS, Azure, Containerization)', 'Strong analytic skills related to working with data and unstructured datasets.', 'Applies engineering standard methodologies in order to analyze and develop solutions.', 'Build processes supporting data transformation, data structures, metadata, dependency and workload management.', 'Strong technical skills', 'Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.', 'Build and maintain efficient data pipeline architecture,', 'At Northwestern Mutual, we are strong, innovative and growing. We invest in our people. We care and make a positive difference. ', '3+ years professional experience required', 'Familiarity using continuous integration and deployment concepts', 'Experience supporting and working with cross functional teams in a dynamic environment.', 'At least 1 year experience with specific technical requirements/platforms (i.e. Java, JavaScript, .NET, Python, Informatica)', 'Can communicate clearly to the business', 'Knowledge, Skills, Abilities', ""Bachelor's Degree or equivalent experience"", 'Participates in setting team standards and best practices.', 'Build and maintain efficient data pipeline architecture,Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.Drive resolution of data-related technical issues and support data infrastructure needs.Work with enterprise data and analytics specialists to strive for greater functionality in our data systems.Applies engineering standard methodologies in order to analyze and develop solutions.Utilizes established monitoring and automation processes.Participates in setting team standards and best practices.', 'At Northwestern Mutual, we are strong, innovative and growing. We invest in our people. We care and make a positive difference.', 'Able to quickly learn and understand new business domain', 'Primary Duties & Responsibilities', 'This job is not covered by the existing Collective Bargaining Agreement.', 'Drive resolution of data-related technical issues and support data infrastructure needs.', 'Required Certifications', ' We are an equal opportunity/affirmative action employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, gender identity or expression, sexual orientation, national origin, disability, age or status as a protected veteran, or any other characteristic protected by law.', 'Experience performing root cause analysis on internal and external data and processes to answer specific business questions and find opportunities for improvement.', 'Strong project management and organizational skills.', 'Work with enterprise data and analytics specialists to strive for greater functionality in our data systems.', 'Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.', 'Grow your career with a best-in-class company that puts our client’s interests at the center of all we do. Get started now! ', 'Utilizes established monitoring and automation processes.', 'Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.', 'Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.', ""Bachelor's Degree or equivalent experience3+ years professional experience requiredAdvanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.Experience performing root cause analysis on internal and external data and processes to answer specific business questions and find opportunities for improvement.Strong analytic skills related to working with data and unstructured datasets.Build processes supporting data transformation, data structures, metadata, dependency and workload management.Strong project management and organizational skills.Experience supporting and working with cross functional teams in a dynamic environment.At least 1 year experience with specific technical requirements/platforms (i.e. Java, JavaScript, .NET, Python, Informatica)Familiarity with public cloud and technologies (AWS, Azure, Containerization)Familiarity using continuous integration and deployment conceptsStrong technical skillsAble to quickly learn and understand new business domainAble to mentor new team membersCan communicate clearly to the business"", 'Able to mentor new team members']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,"Cadence Bank, N.A.","Birmingham, AL",3 weeks ago,Be among the first 25 applicants,"['', 'Position Description', ' Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.', 'Maintains direction and focus through proactive planning and organized approaches to work resulting in target deadlines that are consistently met', '  3 - 5 years of experience as a Data Engineer or Sr. Data Analyst  1 - 3 years of experience manipulating, processing and extracting value from large disconnected datasets.  1 - 3 years of experience in developing, communicating and presenting concepts to varying audiences  1 - 3 years of experience participating in developing strategic plans to realize business objectives ', ' Perform root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.', ' Advanced, working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.', ' Demonstrated abilities in relationship management', ' Experience in leading process improvement initiatives', 'Required Education', '  Conduct regular meetings with internal and external stakeholders to ensure clarification and meet specific requirements in a timely manner.  Create and maintain optimal data pipeline architecture  Assemble and optimize large, complex data sets that meet functional / non-functional business requirements.  Perform root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.  Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.  Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.  Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.  Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.  Keep our data separated and secure across national boundaries through multiple data centers and regions.  Create data tools for analytics and business intelligence team members that assist them in building and optimizing our products into an innovative industry leader.  Work with data and analytics experts to strive for greater functionality in our data systems.  Gains understanding of customer needs and adapt product strategies to meet their expectations  Other duties as assigned or requested ', "" Bachelor's Degree - Computer Science, Statistics, Informatics, Information Systems or another quantitative field"", ' 5 - 7 years of experience as a Data Engineer', ' Create data tools for analytics and business intelligence team members that assist them in building and optimizing our products into an innovative industry leader.', 'Essential Responsibilities', ' Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.', 'Proven ability to quickly learn new applications, processes, and procedures', 'TRAVEL REQUIREMENT', ' Strong analytic skills related to working with unstructured datasets.', 'Professional image with ability to form good partner relationships across functions', ' 3 - 5 years of experience as a Data Engineer or Sr. Data Analyst', ' 3 - 5 years of experience manipulating, processing and extracting value from large disconnected datasets.', ' A successful history of manipulating, processing and extracting value from large disconnected datasets.', 'Minimum Experience', ' Conduct regular meetings with internal and external stakeholders to ensure clarification and meet specific requirements in a timely manner.', ' Keep our data separated and secure across national boundaries through multiple data centers and regions.', ' Work with data and analytics experts to strive for greater functionality in our data systems.', ' 1 - 3 years of experience in developing, communicating and presenting concepts to varying audiences', 'Demonstrates a meticulous attention to detail', ' Other duties as assigned or requested', ' Proven ability to quickly learn new applications, processes, and procedures Demonstrates a meticulous attention to detail Demonstrates the capacity to manage changing priorities and ambiguity while remaining calm and controlled Maintains direction and focus through proactive planning and organized approaches to work resulting in target deadlines that are consistently met Communicates a ""can do"" attitude and positive outlook, minimizing negative behaviors Professional image with ability to form good partner relationships across functions Demonstrates initiative, resourcefulness, and independence ', 'Equipment/Software', 'The above statements are intended to describe the general nature and level of the work being performed by people assigned to this work. This is not an exhaustive list of all duties and responsibilities. Cadence Management reserves the right to amend and change responsibilities to meet business and organizational needs as necessary.', ' Ability to motivate high performance, multi-discipline teams', ' 1 - 3 years of experience manipulating, processing and extracting value from large disconnected datasets.', ' 1 - 3 years of experience participating in developing strategic plans to realize business objectives', 'Demonstrates initiative, resourcefulness, and independence', '  Advanced, working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.  Strong analytic skills related to working with unstructured datasets.  A successful history of manipulating, processing and extracting value from large disconnected datasets.  Strong teamwork and interpersonal skills  Experience in leading process improvement initiatives  Ability to motivate high performance, multi-discipline teams  Demonstrated competency in project execution  Demonstrated abilities in relationship management ', ' Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.', ' Gains understanding of customer needs and adapt product strategies to meet their expectations', 'Demonstrates the capacity to manage changing priorities and ambiguity while remaining calm and controlled', 'Communicates a ""can do"" attitude and positive outlook, minimizing negative behaviors', ' Demonstrated competency in project execution', ' Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.', ""  Bachelor's Degree - Computer Science, Statistics, Informatics, Information Systems or another quantitative field "", '  5 - 7 years of experience as a Data Engineer  3 - 5 years of experience manipulating, processing and extracting value from large disconnected datasets.  1 - 3 years of experience in developing, communicating and presenting concepts to varying audiences  1 - 3 years of experience participating in developing strategic plans to realize business objectives  Industry organization participation / leadership (ISACA, EDM Council, IAPP, etc.) ', ' Assemble and optimize large, complex data sets that meet functional / non-functional business requirements.', ' Strong teamwork and interpersonal skills', ' Create and maintain optimal data pipeline architecture', 'Knowledge, Skills & Abilities', 'Behavioral Traits', 'Cadence Bank is an affirmative action/equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, religion, color, national origin, sex, age, status as a protected veteran, among other things, or status as a qualified individual with disability.', 'Preferred Experience', 'Position Summary', ' Industry organization participation / leadership (ISACA, EDM Council, IAPP, etc.)']",Not Applicable,Full-time,Information Technology,Banking,2021-03-24 13:05:10
Data Engineer,Accenture Federal Services,"St Louis, MO",2 weeks ago,26 applicants,"['', 'Minimum of 3 Years’\xa0experience with the following:', ""Here's what you need:\xa0"", 'The work:', 'Equal Employment Opportunity:\xa0All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.\xa0', ""Accenture\xa0Federal Services, helping our federal clients tackle their toughest challenges while unleashing their fullest potential…and then some. What makes our approach so unique? Operating from the nation’s capital, we bring together commercial innovation and leading-edge technologies to deliver an integrated and interactive experience that far exceeds expectations. How? Our passion meets purpose! Through our diverse culture and inclusive thinking, we embrace our employees' ideas taking them from concept to practical solutions. Not to mention, we sleep well at night knowing our work directly impacts and improves the way the world works. We keep our tech smarts sharp by providing abundant training and certification opportunities. Are you ready to learn and grow in a career, while making a difference?"", ""Bachelor's Degree"", 'Must be a U.S. Citizen; no Dual Citizenship', 'Accenture Federal Services is committed to providing veteran employment opportunities to our service men and women.\xa0', 'Experience working within a hybrid environment.', '\xa0\xa0', 'Experience Leading small teams of engineers, data analysts. Collaborating with data scientists, and data visualizers to bring powerful analytical solutions and insights to our clients', 'Experience with 2 of 3 - Java, Scala, and Python programming language.\xa0For example, utilizing Java to build data sorting algorithms and machine learning sequences.', 'Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.\xa0', 'viewers', 'Bonus Points:', 'You are:', 'stay tuned', 'Experience in the full software development lifecycle, utilizing emerging technologies and big data design principles in developing data pipelines, interfaces, and architecture to support big data and analytics initiativesExperience Leading small teams of engineers, data analysts. Collaborating with data scientists, and data visualizers to bring powerful analytical solutions and insights to our clients', 'Data Engineering or Big Data Technologies, or Data Transformation, and modelingCloud Technologies (Data Lake, Azure, Google, AWS etc.) or with open-source technologies (Spark, Kafka, Presto, Hive, Cassandra etc.) Will ensure that the data they extract is valid, assisting in documenting any requirements, and resolving ambiguities in the data.SQL and/or NOSQL databases to organize the collection, processing, and storing of data from different sources.Experience in architecting and building scalable data platformsMust be a U.S. Citizen; no Dual Citizenship', 'We are:', 'Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.\xa0', 'ratings', '\xa0', 'Professional Cloud Data Engineer certification in AWS, Azure or GCP', 'The Chip and Joanna\xa0Gaines\xa0at developing data pipelines. Like turning a\xa0fixer-upper\xa0house into the\xa0best on the block, you turn complicated datasets into useful,\xa0showstopping\xa0formats. You live in the cloud and open-source world and enjoy working with your team to bring all the pieces of the data story together, keeping\xa0ratings\xa0high and\xa0viewers\xa0wowed. What does a good time look like to you? Hanging out on the reddit big data feeds and chatting about the latest advances in machine learning and AI. And of course, you always\xa0stay tuned\xa0for what is coming next.', 'Experience in the full software development lifecycle, utilizing emerging technologies and big data design principles in developing data pipelines, interfaces, and architecture to support big data and analytics initiatives', 'Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture Federal Services.\xa0', 'Organization: Accenture Federal Services', 'Data Engineering or Big Data Technologies, or Data Transformation, and modeling', 'Location: St Louis, MO area', 'Must be conversant with cloud native CLIs for various data engineering related activities such as setting up compute and storage resources, IAM and ETL.', 'Cloud Technologies (Data Lake, Azure, Google, AWS etc.) or with open-source technologies (Spark, Kafka, Presto, Hive, Cassandra etc.) Will ensure that the data they extract is valid, assisting in documenting any requirements, and resolving ambiguities in the data.', 'Federal Data Engineer', 'showstopping\xa0', ""Professional Cloud Data Engineer certification in AWS, Azure or GCPExperience with 2 of 3 - Java, Scala, and Python programming language.\xa0For example, utilizing Java to build data sorting algorithms and machine learning sequences.Must be conversant with cloud native CLIs for various data engineering related activities such as setting up compute and storage resources, IAM and ETL.Experience working within a hybrid environment.Bachelor's Degree"", 'An active security clearance or the ability to obtain one may be required for this role.', 'Accenture Federal Services is an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.\xa0', 'SQL and/or NOSQL databases to organize the collection, processing, and storing of data from different sources.', 'We are:\xa0\xa0', 'Important Information', 'Experience in architecting and building scalable data platforms', 'fixer-upper', 'best on the block']",Associate,Full-time,Consulting,Information Technology and Services,2021-03-24 13:05:10
"Data Engineer, Analytics",Facebook,"Remote, OR",1 week ago,68 applicants,"['', 'Solve our most challenging data integration problems, utilizing optimal ETL patterns, frameworks, query techniques, sourcing from structured and unstructured data sources.', 'Optimize pipelines, dashboards, frameworks, and systems to facilitate easier development of data artifacts.', 'Knowledge and practical application of Python.', 'Create and contribute to frameworks that improve the efficacy of logging data, while working with data infrastructure to triage issues and resolve.', 'Conceptualize and own the data architecture for multiple large-scale projects, while evaluating design and operational cost-benefit tradeoffs within systems.', 'Conceptualize and own the data architecture for multiple large-scale projects, while evaluating design and operational cost-benefit tradeoffs within systems.Create and contribute to frameworks that improve the efficacy of logging data, while working with data infrastructure to triage issues and resolve.Collaborate with engineers, product managers, and data scientists to understand data needs, representing key data insights visually in a meaningful way.Define and manage SLA for all data sets in allocated areas of ownership.Determine and implement the security model based on privacy requirements, confirm safeguards are followed, address data quality issues, and evolve governance processes within allocated areas of ownership.Design, build, and launch collections of sophisticated data models and visualizations that support multiple use cases across different products or domains.Solve our most challenging data integration problems, utilizing optimal ETL patterns, frameworks, query techniques, sourcing from structured and unstructured data sources.Assist in owning existing processes running in production, optimizing complex code through advanced algorithmic concepts.Optimize pipelines, dashboards, frameworks, and systems to facilitate easier development of data artifacts.Influence product and cross-functional teams to identify data opportunities to drive impact.Mentor team members by giving/receiving actionable feedback.', 'Preferred Qualification', '5+ years experience in the data warehouse space.5+ years experience in custom ETL design, implementation and maintenance.5+ years experience with object-oriented programming languages.5+ years experience with schema design and dimensional data modeling.5+ years experience in writing SQL statements.Experience analyzing data to identify gaps and inconsistencies.Experience managing and communicating data warehouse plans to internal clients.', 'BS/BA in Technical Field, Computer Science or Mathematics.', 'Collaborate with engineers, product managers, and data scientists to understand data needs, representing key data insights visually in a meaningful way.', 'Determine and implement the security model based on privacy requirements, confirm safeguards are followed, address data quality issues, and evolve governance processes within allocated areas of ownership.', 'Design, build, and launch collections of sophisticated data models and visualizations that support multiple use cases across different products or domains.', 'Influence product and cross-functional teams to identify data opportunities to drive impact.', 'Minimum Qualification', '5+ years experience with schema design and dimensional data modeling.', 'BS/BA in Technical Field, Computer Science or Mathematics.Experience working with either a MapReduce or an MPP system.Knowledge and practical application of Python.Experience working autonomously in global teams.Experience influencing product decisions with data.', '5+ years experience with object-oriented programming languages.', 'Experience influencing product decisions with data.', '5+ years experience in the data warehouse space.', 'Mentor team members by giving/receiving actionable feedback.', 'Responsibilities', '5+ years experience in writing SQL statements.', 'Experience analyzing data to identify gaps and inconsistencies.', 'Experience working autonomously in global teams.', 'Define and manage SLA for all data sets in allocated areas of ownership.', 'Assist in owning existing processes running in production, optimizing complex code through advanced algorithmic concepts.', '5+ years experience in custom ETL design, implementation and maintenance.', 'Experience managing and communicating data warehouse plans to internal clients.', 'Experience working with either a MapReduce or an MPP system.']",Not Applicable,Full-time,Information Technology,Internet,2021-03-24 13:05:10
Data Engineer,Avenue Code,"Brazil, IN",3 weeks ago,33 applicants,"['', ' Curso Superior em Ciência da Computação ou afins Experiência com Python, Spark, SQL, Hadoop e Cloud Inglês avançado ', ' Fomos premiados como uma das melhores empresas para se trabalhar (prêmio GPTW – 2015, 2016 e 2020); Atuação com modernas tecnologias; Flexibilidade de horário; Desenvolvimento do inglês (língua oficial da empresa) ', 'Fomos premiados como uma das melhores empresas para se trabalhar (prêmio GPTW – 2015, 2016 e 2020);', 'More reasons to be an Avenue Coder?', 'Opportunities', 'Nice To Have', 'Required Qualifications', 'Trabalhar em time de Scrum global.', ' Experiência com Hive Experiência com Linux e Bash scripting Experiência com Azure Experiência com Jenkins e Gradle Experiência na criação de testes ', ' Propor pipelines de dados que atendam às demandas de um time de Cientista de Dados Criar pipelines de dados para viabilizar um sistema de previsão de demanda.  Trabalhar em time de Scrum global. Trabalhar com as mais novas tecnologias de Big Data ', 'Trabalhar com as mais novas tecnologias de Big Data', 'Criar pipelines de dados para viabilizar um sistema de previsão de demanda. ', 'Experiência na criação de testes', 'Propor pipelines de dados que atendam às demandas de um time de Cientista de Dados', 'Desenvolvimento do inglês (língua oficial da empresa)', 'Experiência com Hive', 'Inglês avançado', 'Experiência com Jenkins e Gradle', 'Experiência com Linux e Bash scripting', 'Atuação com modernas tecnologias;', 'Experiência com Python, Spark, SQL, Hadoop e Cloud', 'Flexibilidade de horário;', 'Curso Superior em Ciência da Computação ou afins', 'Experiência com Azure']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer - Remote - Denver,SecureTalent Partners,"Denver, CO",4 days ago,Be among the first 25 applicants,"['', 'Experience with ETL to data warehouse systems.', 'Has 2+ years of experience in SaaS development environments.', '3- 5 years of experience.', 'Experience with large-scale data and query optimization techniques.', 'Experience with AWS cloud services: EC2, RDS, Redshift, Aurora Postgres.', 'Troubleshoot and improve the infrastructure required for optimal extraction, transformation, and loading of data from a variety of data sources using SQL and AWS ‘big data’ technologies.', 'Has a Computer Science degree.Has 2+ years of experience in SaaS development environments.Is a ""student of the game"" and thrives on new challenges.Enjoys learning from teammates, and isn\'t afraid to teach others at the same time.Sees the glass half-full.This is a new industry space...your vision could make all the difference!Wants to make a lasting impact and lifelong connections, this is not just another paycheck', 'The ideal fit...', 'Knowledge in multiple scripting languages (e.g. Python).', 'Knowledge of cloud, distributed systems, and stream-processing systems.', 'Collect, parse, analyze, and visualize large sets of data and turn data into insights.', 'This is a new industry space...your vision could make all the difference!', 'Is a ""student of the game"" and thrives on new challenges.', 'Implement and support systems that reliably and efficiently provide interactive query performance on large amounts of multi-modal data.', 'Responsibilities', ""Enjoys learning from teammates, and isn't afraid to teach others at the same time."", 'Understanding of NoSQL and RDBMS.', 'Qualifications', 'Proficient in SQL', 'Wants to make a lasting impact and lifelong connections, this is not just another paycheck', 'Passionate about learning new technologies and solving hard problems in a fast-paced environment.', 'Implement and support systems that reliably and efficiently provide interactive query performance on large amounts of multi-modal data.Improve systems that handle scale.Troubleshoot and improve the infrastructure required for optimal extraction, transformation, and loading of data from a variety of data sources using SQL and AWS ‘big data’ technologies.Collect, parse, analyze, and visualize large sets of data and turn data into insights.Create data tools for analytics and data scientist team members that assist them in building and optimizing our product', 'Sees the glass half-full.', 'Create data tools for analytics and data scientist team members that assist them in building and optimizing our product', 'Has a Computer Science degree.', 'Improve systems that handle scale.', '3- 5 years of experience.Experience with large-scale data and query optimization techniques.Experience with ETL to data warehouse systems.Experience with AWS cloud services: EC2, RDS, Redshift, Aurora Postgres.Proficient in SQLUnderstanding of NoSQL and RDBMS.Knowledge in multiple scripting languages (e.g. Python).Knowledge of cloud, distributed systems, and stream-processing systems.Passionate about learning new technologies and solving hard problems in a fast-paced environment.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,TEKsystems,"Glen Allen, VA",1 week ago,40 applicants,"[' Able to work in diverse teams, under tight deadlines, to deliver quality work for customers ', 'Skills', "" Experience or knowledge of the insurance industrySkillsSQL, Python, Data AnalysisTop Skills Details SQL and/or Python  Experience working with structure or unstructured data to help pull insight - data analysis, manipulating data, data sets, transforming data  JavaScript – nice to haveAdditional Skills & QualificationsSkill Sets  Strong analytical and problem solving skills  Ability to contribute both independently and as part of a team  Strong listening, communication, interpersonal and presentation skills  Able to work in diverse teams, under tight deadlines, to deliver quality work for customers  Commitment to delivering excellent customer service  A drive to learn and master new technologies and techniquesExperience LevelEntry LevelAbout TEKsystemsWe're partners in transformation. We help clients activate ideas and solutions to take advantage of a new world of opportunity. We are a team of 80,000 strong, working with over 6,000 clients, including 80% of the Fortune 500, across North America, Europe and Asia. As an industry leader in Full-Stack Technology Services, Talent Services, and real-world application, we work with progressive leaders to drive change. That's the power of true partnership. TEKsystems is an Allegis Group company.The company is an equal opportunity employer and will consider all applications without regards to race, sex, age, color, religion, national origin, veteran status, disability, sexual orientation, gender identity, genetic information or any characteristic protected by law."", ' SQL and/or Python ', ' Create and maintain documentation including requirements, validation, and design ', ' Experience with business intelligence tools E.g. Qlik, Tableau, etc. ', ' Advanced SQL and/or Python working knowledge ', ' Act as a consultant to customers to pair their business problems with your expertise on what is possible with data ', 'About TEKsystems', ' Commitment to delivering excellent customer service ', ' Experience working with structure or unstructured data to help pull insight - data analysis, manipulating data, data sets, transforming data ', 'Experience Level', ' 3-5 years of experience in a related role ', ' Ability to contribute both independently and as part of a team ', 'Description', 'Additional Skills & Qualifications', ' Focus on the customer experience and the technology to foster trust in the data tools provided ', ' Experience working with structured and unstructured databases ', ' Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement ', ' Strong analytical and problem solving skills ', ' Work within a team to deliver data solutions from idea to production ', ' Provide information and data insights that support business operations and decisions ', ' A successful history of manipulating, processing and extracting value from large disconnected datasets Preferred: ', "" SQL and/or Python  Experience working with structure or unstructured data to help pull insight - data analysis, manipulating data, data sets, transforming data  JavaScript – nice to haveAdditional Skills & QualificationsSkill Sets  Strong analytical and problem solving skills  Ability to contribute both independently and as part of a team  Strong listening, communication, interpersonal and presentation skills  Able to work in diverse teams, under tight deadlines, to deliver quality work for customers  Commitment to delivering excellent customer service  A drive to learn and master new technologies and techniquesExperience LevelEntry LevelAbout TEKsystemsWe're partners in transformation. We help clients activate ideas and solutions to take advantage of a new world of opportunity. We are a team of 80,000 strong, working with over 6,000 clients, including 80% of the Fortune 500, across North America, Europe and Asia. As an industry leader in Full-Stack Technology Services, Talent Services, and real-world application, we work with progressive leaders to drive change. That's the power of true partnership. TEKsystems is an Allegis Group company.The company is an equal opportunity employer and will consider all applications without regards to race, sex, age, color, religion, national origin, veteran status, disability, sexual orientation, gender identity, genetic information or any characteristic protected by law."", ' Experience with full stack JavaScript development and frameworks ', "" A drive to learn and master new technologies and techniquesExperience LevelEntry LevelAbout TEKsystemsWe're partners in transformation. We help clients activate ideas and solutions to take advantage of a new world of opportunity. We are a team of 80,000 strong, working with over 6,000 clients, including 80% of the Fortune 500, across North America, Europe and Asia. As an industry leader in Full-Stack Technology Services, Talent Services, and real-world application, we work with progressive leaders to drive change. That's the power of true partnership. TEKsystems is an Allegis Group company.The company is an equal opportunity employer and will consider all applications without regards to race, sex, age, color, religion, national origin, veteran status, disability, sexual orientation, gender identity, genetic information or any characteristic protected by law."", 'Top Skills Details', ' Identify development and data needs in order to improve and streamline decision making Work Experience: Required: ', ' Develop and manage data tools and solutions that provide value directly to customers ', ' Strong listening, communication, interpersonal and presentation skills ', ' JavaScript – nice to have']",Entry level,Contract,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Jettison,Greater Chicago Area,6 days ago,135 applicants,"['', '5+ years of professional experience as a Data Engineer', 'A Bachelors, Masters, or PhD degree with an emphasis in Computer Science and Statistics/Math (e.g. Data Science, Physics, etc.)5+ years of professional experience as a Data EngineerExpert level skills in Python\xa0Working knowledge of DASK', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Proven success developing models in a low latency trading environment', 'Jettison is representing a company who is searching for a Data Engineer in the Chicago Area.\xa0', 'The Data Engineer will have the opportunity to work in our Chicago or New York office focusing on building out and supporting our research framework.\xa0', 'Develop and optimize existing tools, used by the quants', 'We are seeking a candidate that is searching for an opportunity to make a real impact, deliver value, and work within an experienced team.', 'Responsibilities:', 'Exellent written and verbal communication skills\xa0', 'Working knowledge of DASK', 'Collaborate with quant researchers, and core technologists to build and maintain the central research platform', 'Strong multi-tasking skillsExellent written and verbal communication skills\xa0', 'A Bachelors, Masters, or PhD degree with an emphasis in Computer Science and Statistics/Math (e.g. Data Science, Physics, etc.)', 'Collaborate with quant researchers, and core technologists to build and maintain the central research platformDevelop and optimize existing tools, used by the quantsOngoing testing and optimization of the research pipeline by collecting and analyzing data on the utilization and efficiency of the firms distributed systems', 'Strong multi-tasking skills', 'Expert level skills in Python\xa0', 'Required Skills & Abilities:', 'Ongoing testing and optimization of the research pipeline by collecting and analyzing data on the utilization and efficiency of the firms distributed systems']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Science Engineer,WorkBoard Inc.,"Austin, TX",7 days ago,Be among the first 25 applicants,"['', 'Leverage your mastery of business, communication, and technical skills to consult with experts in most successful enterprises in the business world. ', 'COMING IN', ' Flexible PTO & sick days Paid holidays Health insurance  401K with employer matching Quarterly All-Hands Meetings And much more! ', 'And much more!', ' Bring at least 5 years of combined experience in Business Intelligence, Data Science and Data Warehousing Software development expertise in Python. Expertise in SQL and experience with large relational database systems. Experience creating an Events framework to enable better data analytics Experience with Data Visualization standard methodologies Experience working with the Product team to define and create Product Success metrics and engagement drivers. BS in Computer Science, Engineering or related technical or equivalent experience Have worked at a fast growing SaaS organization where you’ve demonstrated personal accountability and willingness to go above and beyond the job description. You are genuine, warm, positive, empathetic, and engaging with a passion for technology, and customer success. ', ' Rich, deep data set to draw insights from and influence internal and product and engineering leadership Be part of the foundational team with great people, who have an entrepreneurial mindset and bring their absolute best every day. Leverage your mastery of business, communication, and technical skills to consult with experts in most successful enterprises in the business world.  Provide customer feedback and work with the engineering team to translate the feedback into product features.  ', 'Paid holidays', 'Understand and be a point of contact for our event logging pipeline and the business logic associated with the events', ""Within Three Months, You'll"", 'Become a certified OKR Coach and WorkBoard Expert!', 'Understand our user engagement and product analytics for 3 areas better than anyone', 'honest ~ ', 'Ability to have client-facing conversations on our published metrics and help them understand the product logic', 'Experience with Data Visualization standard methodologies', 'BS in Computer Science, Engineering or related technical or equivalent experience', 'Expertise in SQL and experience with large relational database systems.', 'THE TEAM', 'Understand our current Analytics events structure and framework', 'Be part of the foundational team with great people, who have an entrepreneurial mindset and bring their absolute best every day.', 'Experience creating an Events framework to enable better data analytics', 'Have action plans in place to achieve your Key Results!', 'WorkBoard', 'Rich, deep data set to draw insights from and influence internal and product and engineering leadership', 'Provide customer feedback and work with the engineering team to translate the feedback into product features. ', 'Software development expertise in Python.', 'Work with App Engineering, Product and Customer Success teams to roll out in-product data focused features', 'Maintain / Enhance our internal Dashboard in Python and Sigma', ' Fully understand the Schema for our entire Relational Database Understand and be a point of contact for our event logging pipeline and the business logic associated with the events Ability to have client-facing conversations on our published metrics and help them understand the product logic Maintain / Enhance our internal Dashboard in Python and Sigma Work with App Engineering, Product and Customer Success teams to roll out in-product data focused features Dig deep into data to find levers to increase adoption and success and work cross functionally to drive awareness of these levers ', 'Experience working with the Product team to define and create Product Success metrics and engagement drivers.', 'OUR VALUES - WE LIVE BY THE 4 Hs', 'happy ', '401K with employer matching', 'Fully understand the Schema of our core product tables', 'Fully understand the Schema for our entire Relational Database', 'Have a deep understanding of the problem space we are building the platform for, and how are various offerings are interconnected to provide a comprehensive business solution', 'THE OPPORTUNITY', 'Humble ', 'a Few Of Our Awesome Benefits', ' Have a deep understanding of the problem space we are building the platform for, and how are various offerings are interconnected to provide a comprehensive business solution Fully understand the Schema of our core product tables Understand our current Analytics events structure and framework Understand our user engagement and product analytics for 3 areas better than anyone ', 'Hungry ', 'You are genuine, warm, positive, empathetic, and engaging with a passion for technology, and customer success.', 'Demonstrate 100% understanding of WorkBoard’s underlying data structure and it’s mapping to our UI Objects', 'Dig deep into data to find levers to increase adoption and success and work cross functionally to drive awareness of these levers', 'Bring at least 5 years of combined experience in Business Intelligence, Data Science and Data Warehousing', 'THE WORKBOARD STORY', 'Flexible PTO & sick days', ""Within One Month, You'll"", 'Health insurance ', ' Become a certified OKR Coach and WorkBoard Expert! Demonstrate 100% understanding of WorkBoard’s underlying data structure and it’s mapping to our UI Objects Have action plans in place to achieve your Key Results! ', 'Have worked at a fast growing SaaS organization where you’ve demonstrated personal accountability and willingness to go above and beyond the job description.', ""Within Six Months, You'll"", 'Quarterly All-Hands Meetings']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Platform Engineer,Stash,"Remote, OR",3 days ago,Be among the first 25 applicants,"['', 'Flexible Remote ', 'Support existing platform', 'Awards & Recognition', 'Demonstrate a deep understanding of system design, data structures, and algorithms', 'Experience building large-scale data products and understand the tradeoffs made when building these features', 'MarCom Awards Double Gold & Platinum Winner (2018)', 'W3 Awards Winner for Best User Experience (2017)', 'Provide technical mentorship to the team', ' Experience in Machine Learning infrastructure Experience in Search Engines/Graph Databases ', 'Benefits & Perks', 'Experience in Search Engines/Graph Databases', 'Equity in Stash ', 'Team outings that do not involve trust falls...', 'Good understanding of SQL', ""Built in NYC's Best Places to work (2019)"", 'Tech Stack (evolving)', ""What You'll Do"", 'Contribute to the design/architecture/code some of these new initiatives including, but not limited to pluggable real time streaming platform, tooling around data governance, and build enhanced security around important datasets etc.', 'Optimize data access and consumption for our business and product colleagues', 'Strong understanding of data quality and governance', 'No recruiters, please.', ' Contribute to the design/architecture/code some of these new initiatives including, but not limited to pluggable real time streaming platform, tooling around data governance, and build enhanced security around important datasets etc. Collaborate with the team to build tools for data science/marketing/backend teams Design integration pipelines for new data sources and improve existing pipelines to perform efficiently at scale Support existing platform Provide technical mentorship to the team Leverage best practices in continuous integration and deployment to our cloud-based infrastructure Optimize data access and consumption for our business and product colleagues ', 'Experience in Machine Learning infrastructure', 'Flexible Vacation', 'Family-Friendly Medical, Dental, and Vision Insurance Plans', 'Leverage best practices in continuous integration and deployment to our cloud-based infrastructure', 'Proficiency in one the prominent languages such as Java/Python/Scala (scala preferred)', 'Gold Stars', 'Tearsheet Challenge Awards, Best Banking Card Product - Stock-Back® Card, 2020', 'Experience with AWS infrastructure - EMR, S3, Redshift ', 'Forbes Fintech 50 (2019 & 2020)', '401k', 'Design integration pipelines for new data sources and improve existing pipelines to perform efficiently at scale', ' 3+ years of building large-scale data products with distributed computing frameworks with a solid understanding of pipelines built with Hadoop MapReduce, Spark. Proficiency in one the prominent languages such as Java/Python/Scala (scala preferred) Good understanding of SQL Experience building large-scale data products and understand the tradeoffs made when building these features Demonstrate a deep understanding of system design, data structures, and algorithms Experience with AWS infrastructure - EMR, S3, Redshift  Strong understanding of data quality and governance ', '3+ years of building large-scale data products with distributed computing frameworks with a solid understanding of pipelines built with Hadoop MapReduce, Spark.', 'Learning & Development & Ergonomic Work Space Stipends', 'Commuter Benefits and Flexible Spending Account (FSA)', 'Employee referral bonuses', 'Collaborate with the team to build tools for data science/marketing/backend teams', 'LendIt Fintech Innovator of the Year (2019 & 2020)', 'Webby Award Winner for Best Mobile Sites & Apps in the Financial Services and Banking category (2017)', ' Equity in Stash  Flexible Vacation Flexible Remote  Family-Friendly Medical, Dental, and Vision Insurance Plans 401k Learning & Development & Ergonomic Work Space Stipends Commuter Benefits and Flexible Spending Account (FSA) Employee referral bonuses Team outings that do not involve trust falls... ', "" Forbes Fintech 50 (2019 & 2020) Best Digital Bank, Finovate Awards (2020) Tearsheet Challenge Awards, Best Banking Card Product - Stock-Back® Card, 2020 LendIt Fintech Innovator of the Year (2019 & 2020) Built in NYC's Best Places to work (2019) MarCom Awards Double Gold & Platinum Winner (2018) Webby Award Winner for Best Mobile Sites & Apps in the Financial Services and Banking category (2017) W3 Awards Winner for Best User Experience (2017) "", 'Who You Are', 'Best Digital Bank, Finovate Awards (2020)']",Entry level,Full-time,Engineering,Computer Software,2021-03-24 13:05:10
Data Engineer,Horizontal Talent,"Chicago, IL",1 week ago,Be among the first 25 applicants,[''],Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Junior Data Engineer (AWS),Invesco US,"Atlanta, GA",3 weeks ago,Be among the first 25 applicants,"['', 'Work across teams to deliver meaningful reference architectures that outline architecture principles and standard methodologies for technology advancement', 'Assist in crafting documents that ensure consistency in development across the online organization.', 'Your Role', '1-2 years of experience in building data pipelines in tools using Apache Airflow', 'The Department', 'Partner with the Business Analytics team to optimize the cloud / redshift environment to support data sciences capability', 'Employee stock purchase plan', 'Experience and Databases that include Oracle, MS Sql Server, Postgres, Aurora, Athena, Redshift Spectrum', 'Contribute to the build of the data engineering and feature engineering capabilities required to support customer centric analytics', 'Flexible time off and opportunities for a flexible work schedule', 'Communicate with various business areas, partner on the formulation of technical requirements for data ingestion, verification, scheduling, etc', 'Assist in the decision-making process related to the selection of software architecture solutions', 'Collaborate with business domain experts to make strategic recommendations on data management and advocate to improve analytical capability across the organization', 'Proficient in application/software architecture (Definition, Business Process Modeling, etc.)', 'Experience using GitHub, Bit Bucket, or other code repository solution', 'Experience building traditional data pipelines with Informatica', 'Maintains a broad understanding of implementation, integration, and inter-connectivity issues with emerging technologies to define data strategies', 'What’s in it for you?', 'Understanding of cloud architecture, networking, and security etc.', 'Understand the business and enable the full life cycle of development/ reporting/ integration projects: planning, design, develop, testing and rollout that confirms to Agile Standards', 'Gain adoption of architecture processes, standards and procedures', ' Flexible time off and opportunities for a flexible work schedule 401(K) matching of 100% up to the first 6% with additional supplemental contribution Health & wellbeing benefits Parental Leave benefits Employee stock purchase plan ', 'Knowledge of SQL and query optimization techniques', 'Experience working with structured, semi structured and Unstructured data sets', 'The experience you bring:', 'Understanding and experience working in an agile framework', 'Understanding of RESTful API design/build', '401(K) matching of 100% up to the first 6% with additional supplemental contribution', 'Strong experience with writing sophisticated programs, implementing architectures, and enabling automation in these environments', 'Consolidate, standardize, and control changes to capacity management data and metric definitions, ownership, accountability, and taxonomy to ensure alignment in understanding', 'Create and manage data, applications and technology architecture documentation and design artifacts', 'Exposure to machine learning services from Amazon Web Services (AWS) a plus', ' 1-2 years of experience in data modeling, data warehousing, and big data architectures 1-2 years of experience of Amazon cloud (AWS)- S3, EC2, RDS, Lambda, DMS, Glue, CloudWatch, SNS etc. 1-2 years of experience in building data pipelines in tools using Apache Airflow Knowledge of SQL and query optimization techniques Experience and Databases that include Oracle, MS Sql Server, Postgres, Aurora, Athena, Redshift Spectrum Experience with Columnar data stores - Redshift/Snowflake/Parquet Experience working with structured, semi structured and Unstructured data sets Proficient in application/software architecture (Definition, Business Process Modeling, etc.) Programming experience with Python or Scala Knowledge and experience of various data modeling techniques Knowledge and experience in distributed data processing using Spark or Hadoop Experience using GitHub, Bit Bucket, or other code repository solution Understanding of cloud architecture, networking, and security etc. Understanding and experience working in an agile framework Experience building traditional data pipelines with Informatica Exposure to machine learning services from Amazon Web Services (AWS) a plus Understanding of RESTful API design/build DevOps experience is a plus ', 'Experience with Columnar data stores - Redshift/Snowflake/Parquet', 'Knowledge and experience in distributed data processing using Spark or Hadoop', 'Parental Leave benefits', 'Help maintain the code and capability environment required to evolve data-driven, analytical capabilities with the end goal of understanding customer behavior and competitive dynamics', '1-2 years of experience in data modeling, data warehousing, and big data architectures', 'Programming experience with Python or Scala', 'Knowledge and experience of various data modeling techniques', 'DevOps experience is a plus', 'Responsible for Data availability/enablement for business reporting within the SLA', ' Build robust data pipelines using modern data engineering technology stack and Cloud architecture Manage application and data integration platforms using AWS Cloud Components Understand the business and enable the full life cycle of development/ reporting/ integration projects: planning, design, develop, testing and rollout that confirms to Agile Standards Responsible for Data availability/enablement for business reporting within the SLA Manage solution providers, define sourcing approach and manage the providers Create and manage data, applications and technology architecture documentation and design artifacts Work across teams to deliver meaningful reference architectures that outline architecture principles and standard methodologies for technology advancement Gain adoption of architecture processes, standards and procedures Partner with the Business Analytics team to optimize the cloud / redshift environment to support data sciences capability Help maintain the code and capability environment required to evolve data-driven, analytical capabilities with the end goal of understanding customer behavior and competitive dynamics Maintains a broad understanding of implementation, integration, and inter-connectivity issues with emerging technologies to define data strategies Assist in the decision-making process related to the selection of software architecture solutions Contribute to the build of the data engineering and feature engineering capabilities required to support customer centric analytics Assist in crafting documents that ensure consistency in development across the online organization. Strong experience with writing sophisticated programs, implementing architectures, and enabling automation in these environments Consolidate, standardize, and control changes to capacity management data and metric definitions, ownership, accountability, and taxonomy to ensure alignment in understanding Collaborate with business domain experts to make strategic recommendations on data management and advocate to improve analytical capability across the organization Communicate with various business areas, partner on the formulation of technical requirements for data ingestion, verification, scheduling, etc ', 'Manage solution providers, define sourcing approach and manage the providers', 'Build robust data pipelines using modern data engineering technology stack and Cloud architecture', '1-2 years of experience of Amazon cloud (AWS)- S3, EC2, RDS, Lambda, DMS, Glue, CloudWatch, SNS etc.', 'Manage application and data integration platforms using AWS Cloud Components', 'You will be responsible for:', 'Health & wellbeing benefits']",Not Applicable,Full-time,Information Technology,Investment Management,2021-03-24 13:05:10
Data Engineer,Yum! Brands,"Plano, TX",3 weeks ago,67 applicants,"['1+ years hands-on coding skills in languages like Scala, Python, SQL', 'Assisting and working closely with business stakeholders and data/research scientists by delivering data driven solutions to leverage company data to drive business outcomes, hence driving Pizza Hut growth.', 'Implementing and managing production support processes around data lifecycle, data quality, coding utilities, storage, reporting and other data integration points.', 'Dive into the data and perform Exploratory Data Analysis(EDA), statistical modelling and analytics using Python, SQL and Scala to perform Market Basket Analysis of order transactions, gather delivery process metrices and clean and control various master databases.', 'Creating and engineering data pipelines for end-to-end delivery of raw and trained data sets, to and from brand cloud environment using Big data frameworks like Apache Spark, Hadoop, and, Scala, databases like Snowflake. Develop and maintain global data models for delivery process optimization and fleet telematics, use cases and applications at Pizza Hut using Python, AWS EKS, AWS Cloudwatch and AWS S3 storage. Working closely with data scientists to scale, integrate and production data driven machine learning solutions. Dive into the data and perform Exploratory Data Analysis(EDA), statistical modelling and analytics using Python, SQL and Scala to perform Market Basket Analysis of order transactions, gather delivery process metrices and clean and control various master databases. Implementing and managing production support processes around data lifecycle, data quality, coding utilities, storage, reporting and other data integration points. Assisting and working closely with business stakeholders and data/research scientists by delivering data driven solutions to leverage company data to drive business outcomes, hence driving Pizza Hut growth. 1+ years of experience with tools like Kafka and Spark, Cloud Datawarehouse and Hadoop 1+ years hands-on coding skills in languages like Scala, Python, SQL Participate and lead in design sessions, demos and prototype sessions, testing and training workshops with business users and other IT associates', 'Key Responsibilities', '1+ years of experience with tools like Kafka and Spark, Cloud Datawarehouse and Hadoop', 'Creating and engineering data pipelines for end-to-end delivery of raw and trained data sets, to and from brand cloud environment using Big data frameworks like Apache Spark, Hadoop, and, Scala, databases like Snowflake.', 'Participate and lead in design sessions, demos and prototype sessions, testing and training workshops with business users and other IT associates', 'Develop and maintain global data models for delivery process optimization and fleet telematics, use cases and applications at Pizza Hut using Python, AWS EKS, AWS Cloudwatch and AWS S3 storage.', 'Working closely with data scientists to scale, integrate and production data driven machine learning solutions.']",Not Applicable,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,HAN Staffing,"Wayne, NJ",20 hours ago,Be among the first 25 applicants,"['', 'Deep Experience with both structured & unstructured data.', 'Experience developing API integrations/embedding of Reporting Tool visualizations (i.e. Tableau, Power BI, etc.)', 'Experience & Education', 'Healthcare Domain knowledge', 'Serve as a thought leader & expert on Analytics/BI tools', 'The following skills & experience are preferred but not required:', 'Experience in working with JIRA/Git/Bitbucket/JUNIT and other code management toolsets', 'Deep Experience with a variety of ETL/Data Solution tools such as AWS Data Pipeline, Azure Data Factory, AWS Redshift, Informatica, SSIS, etc.', ""Analyze customer's business needs and information requirements, define optimum system requirements to meet business needs, and be able to prioritize and manage new, in-line requirements"", ""Experience developing RESTful API's to enable data consumption"", 'Experience with Stream/Messaging/Workflow management a plus (i.e. Apache Airflow, Kinesis, Kafka, Spark, etc.)', 'Work closely with the deployment team to ensure all developed solutions are aligned to design documentation and customer requirements', ""Bachelor's degree Computer Science, Management Information Systems, or equivalent15+ years Deep Experience with RDBMS systems and data warehousing with an Expert Level understanding of BI/DW architecture, Cloud Solution Architecture (AWS and/or Azure), and Performance Standards/Tuning, with Certifications being a plus5+ years of experience in data modeling (logical and/or physical), ETL design, reporting designDeep Experience with a variety of ETL/Data Solution tools such as AWS Data Pipeline, Azure Data Factory, AWS Redshift, Informatica, SSIS, etc.Strong to Expert level SQL & Python skillsDeep Experience with both structured & unstructured data.Deep Experience with Relational (SQL Server, Oracle, MySQL, etc) & No-SQL databases (Cassandra, MongoDB, Hadoop, etc)Experience developing RESTful API's to enable data consumptionDemonstrated ability & strong interest in learning new technologiesExperience with relational and dimensional data modeling approachesExcellent written and verbal communication skills requiredThe following skills & experience are preferred but not required:At least 1-3 years Consulting or IS Consulting Experience Understanding of core Business Intelligence (BI)/data warehousing technology: RDBMS, Web, Client/ Server, and OLAP SkillsExperience with Stream/Messaging/Workflow management a plus (i.e. Apache Airflow, Kinesis, Kafka, Spark, etc.)Experience developing API integrations/embedding of Reporting Tool visualizations (i.e. Tableau, Power BI, etc.)Experience solving business problems by applying analytics techniques such as machine learning and statistical modeling using analytical tools and programming languagesExperience in working with JIRA/Git/Bitbucket/JUNIT and other code management toolsetsExperience in languages like: Scala, Python - any one is fineHealthcare Domain knowledge"", 'Strong to Expert level SQL & Python skills', 'Deep Experience with Relational (SQL Server, Oracle, MySQL, etc) & No-SQL databases (Cassandra, MongoDB, Hadoop, etc)', '5+ years of experience in data modeling (logical and/or physical), ETL design, reporting design', ""Develop Analytics/BI Solutions providing developed solutions as well as hand's-on guidance to the project team members"", ""Manage the development of Data Engineering Solutions including Data Lake / Data Warehouse / Business Intelligence solutionsDevelop Analytics/BI Solutions providing developed solutions as well as hand's-on guidance to the project team membersServe as a thought leader & expert on Analytics/BI toolsDevelop relational models, cubes and reports in a re-usable fashion aligned with the architectural directionWork closely with the deployment team to ensure all developed solutions are aligned to design documentation and customer requirementsMentor fellow Analytics Developers/Team Members/ClientsAnalyze customer's business needs and information requirements, define optimum system requirements to meet business needs, and be able to prioritize and manage new, in-line requirementsProvide hands-on problem-solving assistance with data integration and data profiling tools to developers"", 'At least 1-3 years Consulting or IS Consulting Experience Understanding of core Business Intelligence (BI)/data warehousing technology: RDBMS, Web, Client/ Server, and OLAP Skills', 'Experience with relational and dimensional data modeling approaches', ""Bachelor's degree Computer Science, Management Information Systems, or equivalent"", 'Experience in languages like: Scala, Python - any one is fine', 'Demonstrated ability & strong interest in learning new technologies', 'Manage the development of Data Engineering Solutions including Data Lake / Data Warehouse / Business Intelligence solutions', 'Provide hands-on problem-solving assistance with data integration and data profiling tools to developers', 'Experience solving business problems by applying analytics techniques such as machine learning and statistical modeling using analytical tools and programming languages', 'Develop relational models, cubes and reports in a re-usable fashion aligned with the architectural direction', 'Mentor fellow Analytics Developers/Team Members/Clients', 'Excellent written and verbal communication skills required', 'Job Description', '15+ years Deep Experience with RDBMS systems and data warehousing with an Expert Level understanding of BI/DW architecture, Cloud Solution Architecture (AWS and/or Azure), and Performance Standards/Tuning, with Certifications being a plus']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Tradeswell,"Baltimore, MD",4 weeks ago,38 applicants,"['', 'An passion for data, data engineering, and data science', 'Strong engineering background and understanding of Python, Spark, and Airflow, Data Lakes & Warehousing, AWS Step Functions', ' An passion for data, data engineering, and data science Strong engineering background and understanding of Python, Spark, and Airflow, Data Lakes & Warehousing, AWS Step Functions Experience managing managing infrastructure for data pipelines, ETL process, and data warehouses A desire to continually grow, learn, and iterate on the product and yourself ', 'Any unsolicited resumes/candidate profiles submitted through our website or to personal email accounts of employees of Tradeswell are considered property of Tradeswell and are not subject to payment of agency fees.', ' Integrate with 3rd party systems to ingest and normalize data for customers Design, build, and maintain business critical data infrastructure Collaborate with other engineers, product managers, and data scientists to solve problems for internal and external customers ', 'Integrate with 3rd party systems to ingest and normalize data for customers', 'A desire to continually grow, learn, and iterate on the product and yourself', 'Design, build, and maintain business critical data infrastructure', 'Experience managing managing infrastructure for data pipelines, ETL process, and data warehouses', 'Collaborate with other engineers, product managers, and data scientists to solve problems for internal and external customers']",Entry level,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Staff Data Engineer ,Johnson & Johnson,"Seattle, WA",6 days ago,43 applicants,"['', ' Experience working with AWS services (S3, EC2, Redshift)', ' Foster an open and honest culture of creativity and collaboration with intelligent risk-taking and respectful questioning', ' Develop and maintain scalable data pipelines to support continuing increases in data volume and complexity.', ' Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.', ' Strong software skills, proven by the successful products you have shipped', ' Bring new ideas and innovations to the data science', ' Extensive experience with and detailed understanding of Python', "" BS or Master's degree from an accredited four-year college in computer science or a related field or equivalent proven experience Extensive experience with and detailed understanding of Python Extensive experience working on ETL projects Strong experience working with both Mongo and SQL databases Strong software skills, proven by the successful products you have shipped Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc. Experience with git source control Experience working with AWS services (S3, EC2, Redshift) Experience working alongside data scientists and researchers Excellent written and spoken English Strong sense of responsibility for the happiness of customers and coworkers"", ' Experience with git source control', ' Experience working alongside data scientists and researchers', ' Implements processes and systems to monitor data quality, ensuring production data is always accurate and available for business processes that depend on it.', "" BS or Master's degree from an accredited four-year college in computer science or a related field or equivalent proven experience"", ' Bring a rigorous statistical, data-driven approach to all data science decisions', 'Responsibilities', ' Extensive experience working on ETL projects', 'Qualifications', ' Build data models to deliver insightful analytics while ensuring the highest standard in data integrity.', ' Works closely with a team of software engineers, machine learning engineers, product managers, and R&D researchers.', ' Develop and maintain scalable data pipelines to support continuing increases in data volume and complexity. Build data models to deliver insightful analytics while ensuring the highest standard in data integrity. Implements processes and systems to monitor data quality, ensuring production data is always accurate and available for business processes that depend on it. Works closely with a team of software engineers, machine learning engineers, product managers, and R&D researchers. Foster an open and honest culture of creativity and collaboration with intelligent risk-taking and respectful questioning Bring a rigorous statistical, data-driven approach to all data science decisions Bring new ideas and innovations to the data science', ' Excellent written and spoken English', ' Strong sense of responsibility for the happiness of customers and coworkers', 'Job Description', ' Strong experience working with both Mongo and SQL databases']",Not Applicable,Full-time,Information Technology,Hospital & Health Care,2021-03-24 13:05:10
Data Engineer,Modis,"Burbank, CA",1 week ago,56 applicants,"['', 'Work with application development team to deploy analytics data products through such ways as embedding analysis models into business applications and mobile solutions.', 'Minimum 5 years of data engineering, data modeling or data architecture experience with a focus on multidimensional data modeling for both structured and unstructured data.', 'Experience creating and maintaining automated data pipelines, data standards, and best practices to maintain integrity and security of the data; ensure adherence to developed standards.', 'Expert knowledge in areas of advanced data techniques including unstructured and spatial data.', 'If interested, please apply immediately or email your updated resume to dwijen.mehta@modis.com.\xa0\xa0', 'Minimum Qualifications:', 'Perform gap analysis as needed for purposes of maintaining, continuously enhancing data models and integrating KPI’s into the analytics platform as and when new KPI and business metrics are adapted by the organization.', 'Design conceptual, logical and physical data models, maintain data dictionary and capture metadata.', 'Proficient in leading business and internal team discussions to gather requirements, brainstorm and propose robust and scalable solutions; leverage business partner/unit input to enhance data models.', 'As a Data Engineer, you will design, develop, test and maintain efficient and sustainable data models to keep data accessible and ready for analysis. Working closely with Analytics Team, you will engage with business teams to understand requirements, design conceptual, logical and physical data models, and perform root-cause analysis and recommend solutions. ', 'Work closely with data experts to build and maintain KPI data dictionary, metadata, data standards, and ensure adherence to the Plans Analytics Method and data standards.', 'Expert knowledge of relational DBMS, specifically Oracle .', 'Competencies: ', 'Location: ', 'Location: Burbank, CA\xa0', ""At Modis, we use our insight, knowledge and global resources to make exceptional connections every day. With 60 branch offices located strategically throughout North America, we are positioned perfectly to deliver the industry's top talent to each of our clients. Clients choose Modis as their workforce partner to solve staffing challenges that range from locating hard-to-find niche talent to completing quick-fill demands."", 'Proficient in leading business and internal team discussions to gather requirements, brainstorm and propose robust and scalable solutions; leverage business partner/unit input to enhance data models.Expert knowledge of relational DBMS, specifically Oracle .Expert knowledge in areas of advanced data techniques including unstructured and spatial data.', 'Title: Data Engineer', 'Establish and maintain provenance, integrity and security of data used for self-service reporting, ad-hoc analysis or other levels of analysis.', 'Instills trust – follows through on commitments; is seen as direct and truthful; keeps confidences; practices what he/she preaches; shows consistency between words and actions.', 'Title: ', 'Experience in relevant technical languages and tools such as SQL, Python, NoSQL, Airflow, Quartz, ERWIN or equivalent.', '\xa0', 'Experience designing conceptual, logical and physical data models and maintaining data dictionary and capturing metadata.', 'Ensure developed data models are easy to use and efficient to access data thus enable transparency of data lineage to business teams and all stakeholders', 'Manages Ambiguity- Operating effectively, even when things are not certain, or the way forward is not clear.', 'Decision quality – Makes sound decisions, even in the absence of complete information; relies on a mixture of analysis, wisdom, experience, and judgment when making decisions; considers all relevant factors and uses appropriate decision-making criteria and principles; recognizes when a quick 80% solution will suffice.', 'Bachelor’s Degree in Computer Science, Information Systems, or other related field as well as equivalent work experience.', 'Customer focus - Gains insight into customer needs; identifies opportunities that benefit the customer; builds and delivers solutions that meet customer expectations; establishes and maintains effective customer relationships.Decision quality – Makes sound decisions, even in the absence of complete information; relies on a mixture of analysis, wisdom, experience, and judgment when making decisions; considers all relevant factors and uses appropriate decision-making criteria and principles; recognizes when a quick 80% solution will suffice.Communicates effectively – Is effective in a variety of communication settings: one-on-one, small and large groups, or among diverse styles and position levels; attentively listens to others; adjusts to fit the audience and the message; provides timely and helpful information to others across the organization; encourages the open expression of diverse ideas and opinions.Ensures accountability – follows through on commitments and makes sure others do the same; acts with a clear sense of ownership; takes personal responsibility for decisions, actions, and failures; establishes clear responsibilities and processes for monitoring work and measuring results; designs feedback loops into work.Instills trust – follows through on commitments; is seen as direct and truthful; keeps confidences; practices what he/she preaches; shows consistency between words and actions.Manages Ambiguity- Operating effectively, even when things are not certain, or the way forward is not clear.', 'Communicates effectively – Is effective in a variety of communication settings: one-on-one, small and large groups, or among diverse styles and position levels; attentively listens to others; adjusts to fit the audience and the message; provides timely and helpful information to others across the organization; encourages the open expression of diverse ideas and opinions.', 'Experience in developing and maintaining source to target mapping that includes extract requirements, derived field logic, domain values and data lineage.', 'Explore ways of modeling the Plans unstructured voice and text data into frameworks fit for analysis.', 'Preferred Qualifications:', 'Essential Job Functions: ', 'Knowledge Skills, & Abilities:', 'Utilize ETL tools and other data pipeline automation techniques to develop and maintain source to target mapping that includes extract requirements, derived field logic, domain values and data lineage.', 'Salary:\xa0', 'Salary:\xa0DOE', 'Engage business teams to understand requirements, document them and deliver robust and scalable solutions in the form of data models that can be leveraged for self-service analytics.', 'Engage business teams to understand requirements, document them and deliver robust and scalable solutions in the form of data models that can be leveraged for self-service analytics.Explore ways of modeling the Plans unstructured voice and text data into frameworks fit for analysis.Design conceptual, logical and physical data models, maintain data dictionary and capture metadata.Perform gap analysis as needed for purposes of maintaining, continuously enhancing data models and integrating KPI’s into the analytics platform as and when new KPI and business metrics are adapted by the organization.Work closely with data experts to build and maintain KPI data dictionary, metadata, data standards, and ensure adherence to the Plans Analytics Method and data standards.Work with application development team to deploy analytics data products through such ways as embedding analysis models into business applications and mobile solutions.Establish and maintain provenance, integrity and security of data used for self-service reporting, ad-hoc analysis or other levels of analysis.Utilize ETL tools and other data pipeline automation techniques to develop and maintain source to target mapping that includes extract requirements, derived field logic, domain values and data lineage.Ensure developed data models are easy to use and efficient to access data thus enable transparency of data lineage to business teams and all stakeholders', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Previous work experience in the Healthcare industry preferred.', 'Bachelor’s Degree in Computer Science, Information Systems, or other related field as well as equivalent work experience.Minimum 5 years of data engineering, data modeling or data architecture experience with a focus on multidimensional data modeling for both structured and unstructured data.Experience designing conceptual, logical and physical data models and maintaining data dictionary and capturing metadata.Experience creating and maintaining automated data pipelines, data standards, and best practices to maintain integrity and security of the data; ensure adherence to developed standards.Experience in developing and maintaining source to target mapping that includes extract requirements, derived field logic, domain values and data lineage.Experience in relevant technical languages and tools such as SQL, Python, NoSQL, Airflow, Quartz, ERWIN or equivalent.', 'This will be a hands on role utilizing Extract, Transfer & Load (ETL) tools to deliver source to target mappings, physical and logical data models and related scripts to automate and streamline data processes with the overarching goal of unlocking the value of data for the organization. This is a critical role to help build the foundation of a data-driven culture through implementation of a robust self-service data framework ', 'Ensures accountability – follows through on commitments and makes sure others do the same; acts with a clear sense of ownership; takes personal responsibility for decisions, actions, and failures; establishes clear responsibilities and processes for monitoring work and measuring results; designs feedback loops into work.', 'Customer focus - Gains insight into customer needs; identifies opportunities that benefit the customer; builds and delivers solutions that meet customer expectations; establishes and maintains effective customer relationships.']",Associate,Full-time,Information Technology,Entertainment,2021-03-24 13:05:10
Data Engineer,Nielsen,"Columbia, MD",4 weeks ago,58 applicants,"['', 'Develop and maintain the underlying infrastructure to support forecasting & statistical models, machine learning solutions, big data pipelines (from internal and external sources) used in a production environment', 'About Nielsen', 'Must be proficient with Python (and Spark/Scala) to develop sharable software with the appropriate technical documentation', 'Experience utilizing Apache Spark, Databricks & Airflow', 'Expertise with Tableau, or other data visualization software and techniques', 'Knowledge of survey sampling methodologies', 'Maintain and update comprehensive documentation on departmental procedures, checklists and metrics', 'Experience utilizing Gitlab, Git or similar to manage code development', 'Experience in leveraging CI/CD pipelines', 'Job Type:', 'Secondary Locations: ', 'Key tasks include – but are not limited to – data integration, data harmonization, automation, examining large volumes of data, identifying & implementing methodological, process & technology improvements', 'Undergraduate or graduate degree in mathematics, statistics, engineering, computer science, economics, business or fields that employ rigorous data analysis.', 'Preferred Experience', 'Work as an integral member of the Audio Data Science team in a time-critical production environment', 'Implement prevention and detection controls to ensure data integrity, as well as detect and address quality escapes', 'Knowledge of machine learning and data modeling techniques such as Time Series, Decision Trees, Random Forests, SVM, Neural Networks, Incremental Response Modeling, and Credit Scoring', 'What will I do?', 'Maintain and continuously improve the data infrastructure and analysis framework for the Audio Data Science team', 'Research and develop new technology solutions to meet the needs of upcoming big data projects', 'Expertise in querying large datasets with SQL and of working with Oracle, Netezza, Data Warehouse and Data Lake data structures', ' Maintain and continuously improve the data infrastructure and analysis framework for the Audio Data Science team Research and develop new technology solutions to meet the needs of upcoming big data projects Transition the data science tech infrastructure away from legacy systems Work with cross-functional teams to implement and validate enhanced audience measurement methodologies Build and refine data queries from large relational databases/data warehouses/data lakes for various analyses and/or requests Utilize tools such as Python, Tableau, AWS, Databricks etc. to independently develop, test and implement high quality custom, modular code to perform complex data analysis, visualizations, and answer client queries Maintain and update comprehensive documentation on departmental procedures, checklists and metrics Implement prevention and detection controls to ensure data integrity, as well as detect and address quality escapes Work closely with internal customers and IT personnel to improve current processes and engineer new methods, frameworks and data pipelines Work as an integral member of the Audio Data Science team in a time-critical production environment Key tasks include – but are not limited to – data integration, data harmonization, automation, examining large volumes of data, identifying & implementing methodological, process & technology improvements Develop and maintain the underlying infrastructure to support forecasting & statistical models, machine learning solutions, big data pipelines (from internal and external sources) used in a production environment ', 'Strong ability to proactively gather information, work independently as well as within an multi disciplinary team', 'Experience utilizing cloud computing platforms such as AWS, Azure, etc', 'Data Engineer - 80661', 'Primary Location', 'Experience in containerization such as Docker and/or Kubernetes', ' Is this for me? ', 'Work closely with internal customers and IT personnel to improve current processes and engineer new methods, frameworks and data pipelines', ' Undergraduate or graduate degree in mathematics, statistics, engineering, computer science, economics, business or fields that employ rigorous data analysis. Must be proficient with Python (and Spark/Scala) to develop sharable software with the appropriate technical documentation Experience utilizing Gitlab, Git or similar to manage code development Experience utilizing Apache Spark, Databricks & Airflow Expertise with Tableau, or other data visualization software and techniques Experience in containerization such as Docker and/or Kubernetes Expertise in querying large datasets with SQL and of working with Oracle, Netezza, Data Warehouse and Data Lake data structures Experience in leveraging CI/CD pipelines Experience utilizing cloud computing platforms such as AWS, Azure, etc Strong ability to proactively gather information, work independently as well as within an multi disciplinary team ', 'Travel:', ' Knowledge of machine learning and data modeling techniques such as Time Series, Decision Trees, Random Forests, SVM, Neural Networks, Incremental Response Modeling, and Credit Scoring Knowledge of survey sampling methodologies Knowledge of statistical tests and procedures such as ANOVA, Chi-squared, Correlation, Regression, etc ', 'Knowledge of statistical tests and procedures such as ANOVA, Chi-squared, Correlation, Regression, etc', 'Transition the data science tech infrastructure away from legacy systems', 'Utilize tools such as Python, Tableau, AWS, Databricks etc. to independently develop, test and implement high quality custom, modular code to perform complex data analysis, visualizations, and answer client queries', 'Build and refine data queries from large relational databases/data warehouses/data lakes for various analyses and/or requests', 'Work with cross-functional teams to implement and validate enhanced audience measurement methodologies']",Not Applicable,Full-time,Information Technology,Market Research,2021-03-24 13:05:10
Data Engineer,Labcorp,"Tampa, FL",1 week ago,Be among the first 25 applicants,"['', ' Document data workflow diagrams for major services.', ' T-SQL, dynamic SQL, scripting, reporting services and Integration Services, flat file automation (pull, post).', ' Exceptional skills in relational database systems - MS SQL Server and Azure SQL Server.', '  5+ years working in information systems development roles or proven technical skills coupled with a degree in Computer Science, Information Systems or similar field.   3+ years working in a data engineering capacity.  Exceptional skills in relational database systems - MS SQL Server and Azure SQL Server.  T-SQL, dynamic SQL, scripting, reporting services and Integration Services, flat file automation (pull, post).  Highly proficient with SQL Server Integration Services, SQL Server Agent automation.  Ability to identify and diagnose ETL and database related issues, perform root cause analysis, and recommend corrective actions.  Python experience in the context of data orchestration, automation and analysis.  Data visualization design and development capability in Tableau or other tools/languages.  Ability to work with complex business requirements in developing SQL stored procedures.  Solid understanding of flat file formats and file format conversion.  Understanding of EDI file formats.  Solid communication skills for documenting complex processes and oral communication skills in communicating with technical and non-technical employees.  Data workflow mapping in Visio or other similar tool. ', '  Design and develop data report extracts, processes and comprehensive systems to support the needs of the business and functionality of web applications.  Build automated data ingestion and ETL processes utilizing SSIS, T-SQL, SQL agent jobs, Python and Azure Cloud Services.   Develop and maintain complex matching logic systems and stored procedures for merging inbound patient results data from flat files or staging tables to existing patient profile records.   Continuously fine tune and optimize data integration systems to promote stability and eliminate exceptions requiring human intervention.  Work with business and client liaisons to create standardized approaches to data sharing and integration.  Quickly identify and resolve production issues related to data matching, ingestion merge or reporting export.  Document data workflow diagrams for major services.  Build and manage monitoring systems and tests for production data quality.  Build data systems and processes to enable easy troubleshooting and exception reconciliation by the Data Ops team.  Develop procedures to enhance reporting and query capabilities to improve efficiency and accuracy.  Design and build data visualizations, interactive dashboards and report extracts to support the internal business and clients.  Collaborate with application development team on database architecture and processes to support various LabCorp Employer Services internal and client facing applications. ', ' Develop and maintain complex matching logic systems and stored procedures for merging inbound patient results data from flat files or staging tables to existing patient profile records. ', 'Department', ' 3+ years working in a data engineering capacity.', ' Build data systems and processes to enable easy troubleshooting and exception reconciliation by the Data Ops team.', ' Ability to work with complex business requirements in developing SQL stored procedures.', ' Experience in the healthcare field. ', ' 5+ years working in information systems development roles or proven technical skills coupled with a degree in Computer Science, Information Systems or similar field. ', ' Solid communication skills for documenting complex processes and oral communication skills in communicating with technical and non-technical employees.', ' Experience with Azure Cloud service offerings such as: Azure Data Factory, Azure Databricks, Azure Functions, Azure Synapse.', 'Role Description', ' Build automated data ingestion and ETL processes utilizing SSIS, T-SQL, SQL agent jobs, Python and Azure Cloud Services. ', 'Requirements', ' Experience with Agile/Scrum methodologies and working as part of a cross-functional delivery team.', ' Python experience in the context of data orchestration, automation and analysis.', ' Design and build data visualizations, interactive dashboards and report extracts to support the internal business and clients.', ' Highly proficient with SQL Server Integration Services, SQL Server Agent automation.', ' Build and manage monitoring systems and tests for production data quality.', ' Understanding of EDI file formats.', 'What You Will Be Doing', ' Design and develop data report extracts, processes and comprehensive systems to support the needs of the business and functionality of web applications.', ' Quickly identify and resolve production issues related to data matching, ingestion merge or reporting export.', 'Location:', ' Develop procedures to enhance reporting and query capabilities to improve efficiency and accuracy.', ' Ability to identify and diagnose ETL and database related issues, perform root cause analysis, and recommend corrective actions.', 'Strongly Preferred', '  Experience with Agile/Scrum methodologies and working as part of a cross-functional delivery team.  Experience with Azure Cloud service offerings such as: Azure Data Factory, Azure Databricks, Azure Functions, Azure Synapse.  Experience in the healthcare field.  ', ' Data visualization design and development capability in Tableau or other tools/languages.', ' Collaborate with application development team on database architecture and processes to support various LabCorp Employer Services internal and client facing applications.', 'Your Background and Experience ', ' Continuously fine tune and optimize data integration systems to promote stability and eliminate exceptions requiring human intervention.', 'Job Title', ' Solid understanding of flat file formats and file format conversion.', ' Work with business and client liaisons to create standardized approaches to data sharing and integration.', ' Data workflow mapping in Visio or other similar tool.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer II,Thermo Fisher Scientific,"Harrisburg, PA",1 week ago,Be among the first 25 applicants,"['', 'Work with global and local teams to develop best practices and make recommendations for best impact.', 'Will likely include recommendations and initiatives for additional third-party work, connectivity to other manufacturing, lab, and business systems, and establishing relevant tools, guidelines, and systems when needed.', 'Good organization is required as this roll will be contributing to complex projects with many deliverables across multiple manufacturing sites.', 'A strong problem solver with solid analysis skills.', 'Strong communication skills. This role will be required to communicate across several roles and manufacturing sites across the network.', 'Education', 'Experience', 'Work with global and local teams to develop best practices and make recommendations for best impact.Will likely include recommendations and initiatives for additional third-party work, connectivity to other manufacturing, lab, and business systems, and establishing relevant tools, guidelines, and systems when needed.Work closely with other global functions and sites on initiatives or efforts related to data modeling and Discoverant hierarchy development (ex: Global Process Validation, Automation, Quality, Continuous Process Verification leaders, Operational Excellence, others).', '1 -3 years of data engineering experience is required.Experience in SQL, Oracle, Object-Oriented coding, or HTML development.Working knowledge of basic statistics including control charts, process capability, and statistical process control.Understanding of the software development life cycle and GAMP 5 software validation practices.Basic understanding of pharmaceutical processing. The PSG network manufactures compressed tablets, capsules, soft gels, steriles, biologics, API, and viral vectors.High energy with strong self-motivation. Must be able to work independently with guidance from leadership and mentorship from senior team members.Good organization is required as this roll will be contributing to complex projects with many deliverables across multiple manufacturing sites.Strong communication skills. This role will be required to communicate across several roles and manufacturing sites across the network.A strong problem solver with solid analysis skills.Ability to travel up to 25% annually.', 'Understand and use knowledge of systems and technologies in the manufacturing network that are directly and indirectly related to Discoverant to ensure the global Discoverant implementation is effective in its function and is leveraged for best impact.', 'Collaborate with technical staff on the Discoverant implementation roll-out at PSG’s global manufacturing sites as well as projects focused on advanced process modeling and visualizations.Work with global manufacturing sites to define data needs of the organization.Understand and use knowledge of systems and technologies in the manufacturing network that are directly and indirectly related to Discoverant to ensure the global Discoverant implementation is effective in its function and is leveraged for best impact.', 'Experience in SQL, Oracle, Object-Oriented coding, or HTML development.', 'High energy with strong self-motivation. Must be able to work independently with guidance from leadership and mentorship from senior team members.', 'Working knowledge of basic statistics including control charts, process capability, and statistical process control.', 'What will you do?', 'Ability to travel up to 25% annually.', 'Collaborate with technical staff on the Discoverant implementation roll-out at PSG’s global manufacturing sites as well as projects focused on advanced process modeling and visualizations.', '1 -3 years of data engineering experience is required.', 'Basic understanding of pharmaceutical processing. The PSG network manufactures compressed tablets, capsules, soft gels, steriles, biologics, API, and viral vectors.', 'How will you get there?', 'Understanding of the software development life cycle and GAMP 5 software validation practices.', 'Work with global manufacturing sites to define data needs of the organization.', 'Work closely with other global functions and sites on initiatives or efforts related to data modeling and Discoverant hierarchy development (ex: Global Process Validation, Automation, Quality, Continuous Process Verification leaders, Operational Excellence, others).']",Not Applicable,Full-time,Information Technology,Pharmaceuticals,2021-03-24 13:05:10
Data Engineer,State Farm ®,"Richardson, TX",6 days ago,44 applicants,"['', 'Provide data pipeline solutions for the development, implementation, execution, validation, monitoring, and improvement of data science solutions', 'Understand the basics of machine learning models in order to optimize data science solutions', 'If A Recent College Graduate', 'What You Can Expect', 'Critical Thinking', 'Teamwork', 'We Are Looking for Candidates With', 'Expertise in data engineering practices including the identification, acquisition, cleansing, profiling, and ETL (extracting, transformation, and loading) of data used in data science discovery and deployment solutions', 'Technical/Functional Expertise', 'SFARM', ' Transcript (unofficial copy accepted)Office Location: Bloomington, Atlanta, or Dallas will be considered.', 'Identify critical and emerging technologies, techniques, tools, data sources, and platforms in the data engineering field, including cloud-based solutions, that will support and extend quantitative analytic deployment solutions', 'Adaptability ', ' Resume', 'Overview', 'State Farm Careers!', 'Conform with State Farm strategic analytic data related policies, environments, and direction', 'A Day In The Life Of A Data Engineer', ""The following must be attached to candidate's application at the time of submission -please attach both of the following as one complete document under the resume section:"", "" Resume Transcript (unofficial copy accepted)Office Location: Bloomington, Atlanta, or Dallas will be considered.Applicants are required to be eligible to lawfully work in the U.S. immediately; employer will not sponsor applicants for U.S. work authorization (e.g. H-1B visa) for this opportunity**QualificationsWe Are Looking for Candidates WithExperience and strong knowledge in the Property & Casualty (P&C) data concepts, usage and design, ability to work collaboratively and adapt to change quickly.Expertise in data engineering practices including the identification, acquisition, cleansing, profiling, and ETL (extracting, transformation, and loading) of data used in data science discovery and deployment solutionsExpertise in providing pipeline solutions for the development, implementation, execution, validation, monitoring, and improvement of data science solutionsExpertise in providing reusable data pipeline solutionsUnderstanding of the basics of machine learning models in order to optimize data science solutionsAn understanding of critical and emerging technologies, techniques, tools, data sources, and platforms in the data engineering field, including cloud-based solutions, that will support and extend quantitative analytic deployment solutionsAn ability to develop and maintain an effective network of both scientific and business contacts and knowledge to obtain relevant information and intelligence around the market and emergent opportunitiesCompetenciesTechnical/Functional ExpertiseCritical ThinkingTeamworkAdaptability What You Can ExpectNext Steps: Competitive candidates may be invited to participate in pre-employment testing and/or the interview process. This is where the excitement begins!What’s In It For YouCompetitive Benefits, Pay and Bonus Potential: Who doesn’t want money, right? Volunteer opportunities: Get involved and give back to the community! Continuing Education Support: We support opportunities for you to learn and grow!A Learning Culture: Mentoring, Professional Designations, Employee Development, and more!We embrace Diversity and Inclusion: We are one team and it is simply the right thing to do!Incumbency PeriodThere is a 24-month incumbency period (beginning on the employee's effective date) for any lateral job movements and a 12-month incumbency period for any promotional opportunities, which must be met before the employee may post for other State Farm positions. The incumbency period does not affect the at-will relationship between State Farm and the employee and does not create an employment contract, nor contractual rights.Last Date to Apply: 3/31/2021Learn more about our benefits at State Farm Careers!We are not just offering a job but a meaningful career! We’re here to help life go right®Come join our passionate team!SFARM"", 'may', 'Expertise in providing reusable data pipeline solutions', 'Demonstrate up-to-date expertise in data engineering practices and provides solutions for the identification, acquisition, cleansing, profiling, and ETL (extracting, transformation, and loading) of data used in data science discovery and deployment solutions', 'An ability to develop and maintain an effective network of both scientific and business contacts and knowledge to obtain relevant information and intelligence around the market and emergent opportunities', 'Demonstrate up-to-date expertise in data engineering practices and provides solutions for the identification, acquisition, cleansing, profiling, and ETL (extracting, transformation, and loading) of data used in data science discovery and deployment solutionsProvide data pipeline solutions for the development, implementation, execution, validation, monitoring, and improvement of data science solutionsDevelop intelligent data management and pipelines solutions for reusabilityEstablish business domain knowledge for existing State Farm data sources and investigates, recommends, and initiates acquisition of new data resources from internal and external open and vendor data sources for model building, training, and deploymentConform with State Farm strategic analytic data related policies, environments, and directionUnderstand the basics of machine learning models in order to optimize data science solutionsIdentify critical and emerging technologies, techniques, tools, data sources, and platforms in the data engineering field, including cloud-based solutions, that will support and extend quantitative analytic deployment solutionsDevelop and maintain an effective network of both scientific and business contacts/knowledge obtaining relevant information and intelligence around the market and emergent opportunities', 'Technical/Functional ExpertiseCritical ThinkingTeamworkAdaptability ', 'An understanding of critical and emerging technologies, techniques, tools, data sources, and platforms in the data engineering field, including cloud-based solutions, that will support and extend quantitative analytic deployment solutions', ""Applicants are required to be eligible to lawfully work in the U.S. immediately; employer will not sponsor applicants for U.S. work authorization (e.g. H-1B visa) for this opportunity**QualificationsWe Are Looking for Candidates WithExperience and strong knowledge in the Property & Casualty (P&C) data concepts, usage and design, ability to work collaboratively and adapt to change quickly.Expertise in data engineering practices including the identification, acquisition, cleansing, profiling, and ETL (extracting, transformation, and loading) of data used in data science discovery and deployment solutionsExpertise in providing pipeline solutions for the development, implementation, execution, validation, monitoring, and improvement of data science solutionsExpertise in providing reusable data pipeline solutionsUnderstanding of the basics of machine learning models in order to optimize data science solutionsAn understanding of critical and emerging technologies, techniques, tools, data sources, and platforms in the data engineering field, including cloud-based solutions, that will support and extend quantitative analytic deployment solutionsAn ability to develop and maintain an effective network of both scientific and business contacts and knowledge to obtain relevant information and intelligence around the market and emergent opportunitiesCompetenciesTechnical/Functional ExpertiseCritical ThinkingTeamworkAdaptability What You Can ExpectNext Steps: Competitive candidates may be invited to participate in pre-employment testing and/or the interview process. This is where the excitement begins!What’s In It For YouCompetitive Benefits, Pay and Bonus Potential: Who doesn’t want money, right? Volunteer opportunities: Get involved and give back to the community! Continuing Education Support: We support opportunities for you to learn and grow!A Learning Culture: Mentoring, Professional Designations, Employee Development, and more!We embrace Diversity and Inclusion: We are one team and it is simply the right thing to do!Incumbency PeriodThere is a 24-month incumbency period (beginning on the employee's effective date) for any lateral job movements and a 12-month incumbency period for any promotional opportunities, which must be met before the employee may post for other State Farm positions. The incumbency period does not affect the at-will relationship between State Farm and the employee and does not create an employment contract, nor contractual rights.Last Date to Apply: 3/31/2021Learn more about our benefits at State Farm Careers!We are not just offering a job but a meaningful career! We’re here to help life go right®Come join our passionate team!SFARM"", 'Last Date to Apply:', 'Come join our passionate team!', 'Competitive Benefits, Pay and Bonus Potential: Who doesn’t want money, right? ', 'Responsibilities', 'Develop and maintain an effective network of both scientific and business contacts/knowledge obtaining relevant information and intelligence around the market and emergent opportunities', 'Expertise in providing pipeline solutions for the development, implementation, execution, validation, monitoring, and improvement of data science solutions', 'Understanding of the basics of machine learning models in order to optimize data science solutions', 'Qualifications', 'Continuing Education Support: We support opportunities for you to learn and grow!', 'One Company…Many Careers!', 'Office Location', 'We embrace Diversity and Inclusion: We are one team and it is simply the right thing to do!', 'Develop intelligent data management and pipelines solutions for reusability', 'Experience and strong knowledge in the Property & Casualty (P&C) data concepts, usage and design, ability to work collaboratively and adapt to change quickly.', 'A Learning Culture: Mentoring, Professional Designations, Employee Development, and more!', 'Experience and strong knowledge in the Property & Casualty (P&C) data concepts, usage and design, ability to work collaboratively and adapt to change quickly.Expertise in data engineering practices including the identification, acquisition, cleansing, profiling, and ETL (extracting, transformation, and loading) of data used in data science discovery and deployment solutionsExpertise in providing pipeline solutions for the development, implementation, execution, validation, monitoring, and improvement of data science solutionsExpertise in providing reusable data pipeline solutionsUnderstanding of the basics of machine learning models in order to optimize data science solutionsAn understanding of critical and emerging technologies, techniques, tools, data sources, and platforms in the data engineering field, including cloud-based solutions, that will support and extend quantitative analytic deployment solutionsAn ability to develop and maintain an effective network of both scientific and business contacts and knowledge to obtain relevant information and intelligence around the market and emergent opportunities', 'Competitive Benefits, Pay and Bonus Potential: Who doesn’t want money, right? Volunteer opportunities: Get involved and give back to the community! Continuing Education Support: We support opportunities for you to learn and grow!A Learning Culture: Mentoring, Professional Designations, Employee Development, and more!We embrace Diversity and Inclusion: We are one team and it is simply the right thing to do!', 'Establish business domain knowledge for existing State Farm data sources and investigates, recommends, and initiates acquisition of new data resources from internal and external open and vendor data sources for model building, training, and deployment', 'We are not just offering a job but a meaningful career! We’re here to help life go right®', 'Incumbency Period', 'What’s In It For You', 'Volunteer opportunities: Get involved and give back to the community! ', 'Competencies']",Not Applicable,Contract,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer (Remote),BuildZoom,"Myrtle Point, OR",1 week ago,Be among the first 25 applicants,"['', 'Exceptional written/verbal communication and emotional intelligence.', 'Nice to have experience: Go, Airflow, Elasticsearch, Redis, AWS Lambda, AWS Aurora, Snowflake', 'Values', 'Collaboration. Our team is stronger than the sum of its parts.', 'Scrappy and Tenacious. Win in the face of seemingly insurmountable obstacles.', 'Drive. A passionate desire to improve the construction and remodeling industry.Self Improvement. A dedication to personal growth, achievement, and self-actualization.Collaboration. Our team is stronger than the sum of its parts.Quantitative. Our business decisions are made with data.Scrappy and Tenacious. Win in the face of seemingly insurmountable obstacles.', 'Broad interests to influence data and business strategy.', 'By submitting your interest in this job, you agree to receive text notifications with additional steps to complete your job application. You will receive up to 6 messages from the number ""63879"". Message & data rates may apply. Please refer to our privacy policy for more information.', 'Drive. A passionate desire to improve the construction and remodeling industry.', '3+ years of hands-on software engineering experience in a professional team-based environment.', 'This position can be located in Phoenix, AZ or San Francisco, CA. ', 'Must have professional experience: Python, SQL, Data Modeling', 'Requirements', 'Exceptional written/verbal communication and emotional intelligence.3+ years of hands-on software engineering experience in a professional team-based environment.Broad interests to influence data and business strategy.Must have professional experience: Python, SQL, Data ModelingNice to have experience: Go, Airflow, Elasticsearch, Redis, AWS Lambda, AWS Aurora, Snowflake', 'Self Improvement. A dedication to personal growth, achievement, and self-actualization.', 'Quantitative. Our business decisions are made with data.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Doyensys Inc,"Plano, TX",3 days ago,Be among the first 25 applicants,[''],Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer - Tableau,Rackspace Technology,"Grapevine, TX",23 hours ago,Be among the first 25 applicants,"['', 'Excellent technical architecture skills, design and deliver innovative Proof of Concepts for customers.', '8+ Years of experience in Tableau development and Tableau Administration 2+ years of experience in delivering AWS Data Solutions.Experienced in DW/BI, ETL and Data related technologies and toolsExperienced working with any public cloud platform like AWS, GCP or AzureExperience with configuring Tableau with different Identity stores and have knowledge of Integrating with AD for SAML.A solid understanding of SQL, rational databases, and normalization.Proficiency in use of query and reporting analysis tools.DW/BI, ETL and Data related technologies and toolsGood conceptual knowledge and working experience on report/dashboard creation, creating report specifications, integration test planning & testing, unit test planning & testing, UAT & implementation support', 'Tableau certification along with any Cloud certifications such as AWS Solutions Architect, Developer, GCP Professional Data Engineer or Microsoft Data / AI certifications', '2+ years of experience in delivering AWS Data Solutions.', 'Provide support and expertise to the business community to assist with better utilization of Tableau.Understand business requirements, conduct analysis, and recommend solution optionsShould be good with writing SQL queries and Stored proceduresWorking closely with the business, this role will be responsible for designing, building and maintaining Tableau WorkbooksDevelop reports, dashboards, scorecardsGather business requirements, elicit technical requirements, prepare report specificationsManaging Tableau ServerTwisting SQL queries for improving performances', 'Expert in Tableau administration/architecture', '8+ Years of experience in Tableau development and Tableau Administration ', 'Skilled Tableau Developer with experience in implementing best practice in analytics and visualization, promote agile delivery, and help expose data', 'Mentor and train other architects/Engineer within the wider Rackspace community on Tableau and other tools like Looker', 'Drive the engagements with customers from the architectural pillar, from design to delivery, create runbooks etc.', 'About Rackspace Technology', 'More on Rackspace Technology', 'Development and deployment of Tableau, implement and maintain test strategies and plans', 'Job Description Summary', 'A solid understanding of SQL, rational databases, and normalization.', 'Extensive experience in developing, maintaining and managing Tableau driven dashboards & analytics', 'Experience with Data level security, maintaining users, groups and role in Tableau', 'Should be able to install and configure Multi Node Tableau Architecture in Linux and Window Environments', 'Gather business requirements, elicit technical requirements, prepare report specifications', 'Understand business requirements, conduct analysis, and recommend solution options', 'Managing Tableau Server', 'Twisting SQL queries for improving performances', 'DW/BI, ETL and Data related technologies and tools', 'Experienced in DW/BI, ETL and Data related technologies and tools', 'Extensive experience in developing, maintaining and managing Tableau driven dashboards & analyticsExpert in Tableau administration/architectureSkilled Tableau Developer with experience in implementing best practice in analytics and visualization, promote agile delivery, and help expose dataExperience with Data level security, maintaining users, groups and role in TableauDevelopment and deployment of Tableau, implement and maintain test strategies and plansDrive the engagements with customers from the architectural pillar, from design to delivery, create runbooks etc.Should be able to install and configure Multi Node Tableau Architecture in Linux and Window EnvironmentsPerforming and documenting data analysis, data validation, and data mapping/design.Work with customers and the wider Rackspace organizations on re-thinking and re-designing IT Data landscapes using cloud-native technologiesExcellent technical architecture skills, design and deliver innovative Proof of Concepts for customers.Mentor and train other architects/Engineer within the wider Rackspace community on Tableau and other tools like Looker', 'Qualifications & Experience', 'Work with customers and the wider Rackspace organizations on re-thinking and re-designing IT Data landscapes using cloud-native technologies', 'Should be good with writing SQL queries and Stored procedures', 'Experienced working with any public cloud platform like AWS, GCP or Azure', 'Cloud Certifications', 'Experience with configuring Tableau with different Identity stores and have knowledge of Integrating with AD for SAML.', 'Performing and documenting data analysis, data validation, and data mapping/design.', 'Develop reports, dashboards, scorecards', 'Good conceptual knowledge and working experience on report/dashboard creation, creating report specifications, integration test planning & testing, unit test planning & testing, UAT & implementation support', 'What You’ll Be Doing', 'Proficiency in use of query and reporting analysis tools.', 'Working closely with the business, this role will be responsible for designing, building and maintaining Tableau Workbooks', 'Provide support and expertise to the business community to assist with better utilization of Tableau.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
"Data Engineer, Power BI",Softchoice,"Malvern, PA",2 weeks ago,Be among the first 25 applicants,"['', ' Delivering excellence: Developing, testing and managing Data cloud native and 3rd party tooling and automation technologies, delivered through code, templates and scripts to ensure top-quality client experiences, delivered with consistency and efficiency.  Leading by collaboration: Working with software developers, application architects, QA engineers, and operations teams to deliver on – and exceed – client expectations. You will also team up with SME’s, practice leads and vendors to fine tune our strategy and overcome obstacles to adoption at every step. Evangelizing the New: You aren’t just deeply familiar with cutting-edge concepts and technologies. You know how to tailor your insights to customers, occasionally working onsite with customers as a consultant to explore their needs to provide a compelling vision forward. Identify business use cases, gather requirements to satisfy use case, analyze data sources to solve business case. Use a variety of data analysis and organizational tools to uncover insights. ', 'Some reasons why our employees love working here:\u202f ', ' Softchoice has been recognized as a Best Workplace in Canada by the Great Place to Work Institute for 15 consecutive years.  Softchoice has been certified as a Great Place to Work in the United States for several years.  We offer meaningful work that drives professional development.  Our team members have 2 paid volunteer days per year to give back to a cause of their choice.  We offer an opportunity to build a career in the technology industry.  We have raised over $3 Million through our team member run charity Softchoice Cares.  You will have the opportunity to take an ownership position here at Softchoice.\u202f\u202f  ', 'Foundational cloud IaaS and PaaS technologies', 'Delivering excellence: Developing, testing and managing Data cloud native and 3rd party tooling and automation technologies, delivered through code, templates and scripts to ensure top-quality client experiences, delivered with consistency and efficiency. ', 'Use a variety of data analysis and organizational tools to uncover insights.', ' Solution design and data migration Security and Compliance Reporting Tools: Power BI, SSRS, etc. Working knowledge of TSQL, PLSQL and DAX Foundational cloud IaaS and PaaS technologies ', 'GIT', 'Bonus points for having multiple related certifications for Azure, AWS or GCP.', ' Great communication and consultative skills, with the ability to occasionally work onsite with customers and be able to present technically advanced topics to large groups of people with confidence and impact. End to end ability to chart a strategy and identify the solution, while also being able to deliver it and support the client through adoption and maturity Experience working in professional services and/or software engineering for clients in a provider to client relationship. Bonus if experience in a consulting firm. ', ' interview & employment accommodatio', 'Power BI Embedded', 'Azure Cosmos DB', ' Expert-level understanding as a Power BI Developer and for data analysis. Good understanding about DevOps culture, methodologies, coding and automation. Proven experience driving results with relevant technologies and methods, strong familiarity, and knowledge of cloud platforms such as Azure. Excellent communication skills, ability to work in an agile environment. At least 3 years of relevant experience designing and implementing Power BI desktop and service. An appreciation of report design and the user experience. Working knowledge of SQL Server, Azure SQL Database and Amazon RDS. ', 'Experience working in professional services and/or software engineering for clients in a provider to client relationship. Bonus if experience in a consulting firm.', 'Evangelizing the New: You aren’t just deeply familiar with cutting-edge concepts and technologies. You know how to tailor your insights to customers, occasionally working onsite with customers as a consultant to explore their needs to provide a compelling vision forward.', 'Prior to commencing employment: ', 'Why you’ll love Softchoice: ', 'Working knowledge of SQL Server, Azure SQL Database and Amazon RDS.', 'An appreciation of report design and the user experience.', 'At least 3 years of relevant experience designing and implementing Power BI desktop and service.', 'Apache Spark', 'Azure Synapse', 'Softchoice has been certified as a Great Place to Work in the United States for several years. ', 'Security and Compliance', 'End to end ability to chart a strategy and identify the solution, while also being able to deliver it and support the client through adoption and maturity', 'You will have the opportunity to take an ownership position here at Softchoice.\u202f\u202f ', 'Experience in the following areas are an asset: Power BI Report Server Power BI Embedded Integration Services: Azure Data Factory Azure Cosmos DB Azure Synapse Oracle Database tuning and optimization Apache Spark GIT  ', 'Reporting Tools: Power BI, SSRS, etc.', 'Diversity, Inclusion & Belonging', ' Experience in designing and implementing BI solutions with expertise in the following areas: Solution design and data migration Security and Compliance Reporting Tools: Power BI, SSRS, etc. Working knowledge of TSQL, PLSQL and DAX Foundational cloud IaaS and PaaS technologies   Experience in the following areas are an asset: Power BI Report Server Power BI Embedded Integration Services: Azure Data Factory Azure Cosmos DB Azure Synapse Oracle Database tuning and optimization Apache Spark GIT   Knowledge of AWS data services (i.e. Redshift, S3, RDS) Bonus points for having multiple related certifications for Azure, AWS or GCP. ', 'Great communication and consultative skills, with the ability to occasionally work onsite with customers and be able to present technically advanced topics to large groups of people with confidence and impact.', 'Our team members have 2 paid volunteer days per year to give back to a cause of their choice. ', 'Leading by collaboration: Working with software developers, application architects, QA engineers, and operations teams to deliver on – and exceed – client expectations. You will also team up with SME’s, practice leads and vendors to fine tune our strategy and overcome obstacles to adoption at every step.', 'Inclusion & Equal opportunity employment: ', 'Require an accommodation? We are ready to help: ', 'The impact you will have: ', 'Solution design and data migration', 'Knowledge of AWS data services (i.e. Redshift, S3, RDS)', 'We offer an opportunity to build a career in the technology industry. ', 'Identify business use cases, gather requirements to satisfy use case, analyze data sources to solve business case.', 'Integration Services: Azure Data Factory', 'We have raised over $3 Million through our team member run charity Softchoice Cares. ', 'Power BI Report Server', ""What you'll bring to the table:"", 'Excellent communication skills, ability to work in an agile environment.', 'Our commitment to your experience: ', 'Expert-level understanding as a Power BI Developer and for data analysis.', 'Working knowledge of TSQL, PLSQL and DAX', 'Proven experience driving results with relevant technologies and methods, strong familiarity, and knowledge of cloud platforms such as Azure.', 'Oracle', ""What you'll do:"", ' Power BI Report Server Power BI Embedded Integration Services: Azure Data Factory Azure Cosmos DB Azure Synapse Oracle Database tuning and optimization Apache Spark GIT ', 'Database tuning and optimization', 'Good understanding about DevOps culture, methodologies, coding and automation.', 'We offer meaningful work that drives professional development. ', 'Softchoice has been recognized as a Best Workplace in Canada by the Great Place to Work Institute for 15 consecutive years. ', 'Experience in designing and implementing BI solutions with expertise in the following areas: Solution design and data migration Security and Compliance Reporting Tools: Power BI, SSRS, etc. Working knowledge of TSQL, PLSQL and DAX Foundational cloud IaaS and PaaS technologies  ']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,CapTech Consulting,"Reston, VA",2 weeks ago,Be among the first 25 applicants,"['', 'Competitive salary with performance-based bonus opportunities', 'Single and Family Health Insurance plans, including Dental coverage', 'Communicate with all levels of stakeholders as appropriate, including executives, data modelers, application developers, business users, and customers', 'CapTech is an equal opportunity employer committed to fostering a culture of equality, inclusion and fairness — each foundational to our core values. We strive to create a diverse environment where each employee is encouraged to bring their unique ideas, backgrounds and experiences to the workplace.', 'Development experience building ETL graphs using the Ab Initio GDE, EME and Co-Operating system', 'Experience tuning SQL queries to ensure performance and reliability', 'Strong SQL development skills', 'Minimum of 4-5 years experience designing, developing, and testing software aligned with defined requirements', ""Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experienceDevelopment experience building ETL graphs using the Ab Initio GDE, EME and Co-Operating systemStrong SQL development skillsDevelopment experience with at least two different programming languages (Python, Java, Scala, etc.)Development experience with Unix tools and shell scriptsDevelopment experience with at least two different database platforms (Teradata, Oracle, MySQL, MS SQL, etc.)Minimum of 4-5 years experience designing, developing, and testing software aligned with defined requirementsExperience tuning SQL queries to ensure performance and reliabilitySoftware engineering best-practices, including version control (Git, TFS, JIRA, etc.) and test-driven development"", 'Some of our technologies might include: Python, Cassandra, Spark, Java, Scala, Informatica, SQL Server, SSIS, Oracle, Kafka.', 'Development experience with Unix tools and shell scripts', 'Present programming documentation and design to team members and convey complex information in a clear and concise manner.', 'Training and Certification opportunities eligible for expense reimbursement', 'Development experience with at least two different programming languages (Python, Java, Scala, etc.)', 'Matching 401(k)', 'Specific Responsibilities For The Data Engineer, Analytics Position Include', 'Team building and social activities', 'Development experience with at least two different database platforms (Teradata, Oracle, MySQL, MS SQL, etc.)', 'Software engineering best-practices, including version control (Git, TFS, JIRA, etc.) and test-driven development', 'Extract data from multiple sources, integrate disparate data into a common data model, and integrate data into a target database, application, or file using efficient programming processes.', 'Short-Term and Long-Term disability', 'Collaborate with Quality Assurance resources to debug code and ensure the timely delivery of products.', 'Write and refine code to ensure performance and reliability of data extraction and processing.', 'Design, develop, document, and test advanced data systems that bring together data from disparate sources, making it available to data scientists, analysts, and other users using scripting and/or programming languages (Python, Java, Scala, etc)', 'Qualifications', 'Participate in requirements gathering sessions with business and technical staff to distill technical requirements from business requests.', 'Design, develop, document, and test advanced data systems that bring together data from disparate sources, making it available to data scientists, analysts, and other users using scripting and/or programming languages (Python, Java, Scala, etc)Evaluate structured and unstructured datasets utilizing statistics, data mining, and predictive analytics to gain additional business insightsDesign, develop, and implement data processing pipelines at scalePresent programming documentation and design to team members and convey complex information in a clear and concise manner.Extract data from multiple sources, integrate disparate data into a common data model, and integrate data into a target database, application, or file using efficient programming processes.Write and refine code to ensure performance and reliability of data extraction and processing.Communicate with all levels of stakeholders as appropriate, including executives, data modelers, application developers, business users, and customersParticipate in requirements gathering sessions with business and technical staff to distill technical requirements from business requests.Partner with clients to fully understand business philosophy and IT Strategy; recommend process improvements to increase efficiency and reliability in ETL development.Collaborate with Quality Assurance resources to debug code and ensure the timely delivery of products.Some of our technologies might include: Python, Cassandra, Spark, Java, Scala, Informatica, SQL Server, SSIS, Oracle, Kafka.', 'Design, develop, and implement data processing pipelines at scale', 'Partner with clients to fully understand business philosophy and IT Strategy; recommend process improvements to increase efficiency and reliability in ETL development.', 'Company Description', 'Evaluate structured and unstructured datasets utilizing statistics, data mining, and predictive analytics to gain additional business insights', 'Job Description', 'Mentor program to help you develop your career', ""Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience"", 'Competitive salary with performance-based bonus opportunitiesSingle and Family Health Insurance plans, including Dental coverageShort-Term and Long-Term disabilityMatching 401(k)Competitive Paid Time OffTraining and Certification opportunities eligible for expense reimbursementTeam building and social activitiesMentor program to help you develop your career', 'Competitive Paid Time Off']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer - Defense Analytics,McKinsey & Company,"Washington, DC",4 weeks ago,51 applicants,"['', 'Comfortable using the terminal and Linux/UNIX-like systems', 'SECRET or TS/SCI clearance is a plus', 'Development experience using Hadoop (HDFS, Hive, HBase, Impala) and/or Spark ', 'Ability to take creative approaches to analysis and syndicate complex ideas clearly', 'Practitioner in analytics with a strong grasp of ETL, reporting tools, data governance, data warehousing, dimensional modeling and working with structured and unstructured data', ""Who You'll Work With"", 'Database development on either relational databases (e.g., PostgreSQL, SQL Server) or NoSQL on distributed platforms (e.g. HBase, Cassandra)', ""What You'll Do"", 'Strong understanding of lambda architectures and stream processing', 'Experience working on a collaborative Agile product team ', 'Experience in defense/national security is a plus', 'Experience with developing solutions on cloud computing services (e.g. Azure, AWS, GCP) ', 'Familiarity with Kubernetes and Docker containers', '3-5 years of experience in a data engineering and/or architecture role', 'Qualifications', 'Ability to perform quantitative and statistical techniques to identify trends, patterns and correlations ', 'Intermediate skills with SQL as well as 1 programming language (Scala, Java, C++, Python, R)', 'Bachelor’s or master’s degree in a technical or quantitative field (mathematics, statistics, CS, engineering, etc.) 3-5 years of experience in a data engineering and/or architecture roleAbility to perform quantitative and statistical techniques to identify trends, patterns and correlations Ability to take creative approaches to analysis and syndicate complex ideas clearlyPractitioner in analytics with a strong grasp of ETL, reporting tools, data governance, data warehousing, dimensional modeling and working with structured and unstructured dataDevelopment experience using Hadoop (HDFS, Hive, HBase, Impala) and/or Spark Database development on either relational databases (e.g., PostgreSQL, SQL Server) or NoSQL on distributed platforms (e.g. HBase, Cassandra)Comfortable using the terminal and Linux/UNIX-like systemsIntermediate skills with SQL as well as 1 programming language (Scala, Java, C++, Python, R)Strong understanding of lambda architectures and stream processingExperience with developing solutions on cloud computing services (e.g. Azure, AWS, GCP) Familiarity with Kubernetes and Docker containersExposure to BI tools such as Tableau, Power BI, Looker, R ShinyExperience working on a collaborative Agile product team Experience in defense/national security is a plusSECRET or TS/SCI clearance is a plus', 'Exposure to BI tools such as Tableau, Power BI, Looker, R Shiny', 'Bachelor’s or master’s degree in a technical or quantitative field (mathematics, statistics, CS, engineering, etc.) ']",Not Applicable,Full-time,Analyst,Defense & Space,2021-03-24 13:05:10
Data Engineer,PRI Global,"St Louis, MO",1 week ago,94 applicants,"['', '100% remote', '2+ years of experience in Computer Engineering, Software Development, System Administration, Linux Administration, DevOps and Site Reliability Engineering. ', '   3+ years of experience in Computer Engineering, Software Development', ""Bachelor's degree in Computer Science, Computer Engineering, Software Engineering, related field or equivalent experience. Master’s degree in Computer Science or Computer Engineering preferred. 2+ years of experience in Computer Engineering, Software Development, System Administration, Linux Administration, DevOps and Site Reliability Engineering. Experience with the following programs/platforms is preferred: Streaming Frameworks - Apache Spark, Apache Storm, Kafka K-Streams, Apache Samza, Apache Flink, Apache Apex, Apache Nifi and Apache Pulsar; Storage Technologies - MongoDB, Oracle, Teradata, Hadoop, DocumentDB, SQL Server, Volt DB, Apache Ignite, CockroachDB, Amazon Aurora and Redis; Linux – RedHat; Continuous Integration and Deployment – CloudBees, Jenkins, Ansible Tower, Puppet and Chef."", '  SQL (Oracle, MSSQL, MySQL, etc) and NoSQL (Mongo)', 'Day to Day Responsibilities of this Position and Description of Project:\xa0Project involves defining required data types, identifying and tracing data between source and destination systems for a regulatory related initiative.\xa0Designing architecture of a given platform, documenting data flow, performing complex data analysis, and ETL.', '   Hands-on experience with ETL tools & automation', 'Education/Experience:', '   Strong understanding of data modeling, algorithms, and data transformation techniques', ' ', ""Bachelor's degree in Computer Science, Computer Engineering, Software Engineering, related field or equivalent experience. Master’s degree in Computer Science or Computer Engineering preferred. "", '   Scripting languages such as Python', 'Experience with the following programs/platforms is preferred: Streaming Frameworks - Apache Spark, Apache Storm, Kafka K-Streams, Apache Samza, Apache Flink, Apache Apex, Apache Nifi and Apache Pulsar; Storage Technologies - MongoDB, Oracle, Teradata, Hadoop, DocumentDB, SQL Server, Volt DB, Apache Ignite, CockroachDB, Amazon Aurora and Redis; Linux – RedHat; Continuous Integration and Deployment – CloudBees, Jenkins, Ansible Tower, Puppet and Chef.']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer (Remote),Latitude Inc,United States,5 days ago,Be among the first 25 applicants,"['', 'Build algorithms and prototypes.', 'Prepare data for prescriptive and predictive modeling.', 'Explore ways to enhance data quality and reliability.', 'Extensive experience with NoSQL databases.', 'Improving data quality and efficiency.', 'Essential Duties And Responsibilities', 'Conduct complex data analysis and report on results.', 'Knowledge of programming languages (e.g. Go, Java, Python)', 'Model events, notification and alerts.', 'Degree in Computer Science, IT, or similar field; a master’s is a plus', '5 years of previous experience as a data engineer or in a similar role', 'Desired Skills And Experience', 'Analyze and interpret trends and patterns related to the data.', 'Strong numerical and analytical skills', 'Transform raw data to actional data from various sources.', 'Develop analytical tools and programs.', 'Data engineering certification is a plus', 'Identify opportunities for data acquisition.', 'Analyzing raw data.', 'Collaborate with data scientists and architects on several projects.', 'Developing and maintaining datasets.', '5 years of previous experience as a data engineer or in a similar roleTechnical expertise with data models, data mining, and segmentation techniquesKnowledge of programming languages (e.g. Go, Java, Python)Extensive experience with NoSQL databases.Strong numerical and analytical skillsDegree in Computer Science, IT, or similar field; a master’s is a plusData engineering certification is a plus', 'Model queries and build templates for querying large data sets.', 'Technical expertise with data models, data mining, and segmentation techniques', 'Job Description', 'Build optimal data systems and pipelines.', 'Analyzing raw data.Developing and maintaining datasets.Improving data quality and efficiency.Model queries and build templates for querying large data sets.Model events, notification and alerts.Build optimal data systems and pipelines.Transform raw data to actional data from various sources.Analyze and interpret trends and patterns related to the data.Conduct complex data analysis and report on results.Prepare data for prescriptive and predictive modeling.Build algorithms and prototypes.Explore ways to enhance data quality and reliability.Identify opportunities for data acquisition.Develop analytical tools and programs.Collaborate with data scientists and architects on several projects.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
"Data Engineer, Data Warehouse",SoFi,"Helena, MT",3 days ago,Be among the first 25 applicants,"['', 'Strong business communication skills that can break down technical problems into business language for non-technical personnel', 'Competitive salary packages and bonusesComprehensive medical, dental, vision and life insurance benefitsGenerous vacation and holidaysPaid parental leave for eligible employees401(k) and education on retirement planningTuition reimbursement on approved programsMonthly contribution up to $200 to help you pay off your student loansGreat health & well-being benefits including: telehealth parental support, subsidized gym programEmployer paid lunch program (except for remote employees)Fully stocked kitchen with snacks and drinks (When we’re back in the office of course)', 'Comprehensive medical, dental, vision and life insurance benefits', 'Generous vacation and holidays', 'Data modelingBuild and maintain data structures and ETL/ELT data pipelinesProvision, optimize and maintain data feeds to external systemsWrite code to validate data quality and clean existing dataHelp analytics team, upstream engineering teams, as well as non-technical business users in understanding the Data WarehouseBe part of an on call support rotation to support the Data Warehouse and it’s automated processesCreating technical documentation ', 'Fully stocked kitchen with snacks and drinks (When we’re back in the office of course)', 'Experience using kafka', 'Monthly contribution up to $200 to help you pay off your student loans', 'What You’ll Need', 'Understands database architecture', 'Tuition reimbursement on approved programs', 'Great health & well-being benefits including: telehealth parental support, subsidized gym program', 'Nice To Have', 'Data modeling', 'Experience using cloud data technologies such as Redshift, Snowflake, or GCP', 'Experience writing SQL against several different database platforms', 'Experience in building data feeds and business reports', 'Experience using AWS data technologies such as (S3, Glue, Kinesis, Lambda, etc.)', 'Write code to validate data quality and clean existing data', 'Creating technical documentation ', 'Ability to work in a fast-paced environment, meet deadlines, and prioritize a workload', 'Paid parental leave for eligible employees', 'Position at SoFi', 'Description', 'Provision, optimize and maintain data feeds to external systems', 'Working knowledge of some AWS data technologies', 'Experience using business intelligence reporting tools (Tableau, Looker, etc.)', 'Responsibilities', 'Why You’ll Love Working Here', 'Competitive salary packages and bonuses', 'Understanding of the software development lifecycle process', '401(k) and education on retirement planning', 'Experience writing SQL against several different database platformsExperience creating data pipelines using Python scriptingExperience using cloud data technologies such as Redshift, Snowflake, or GCPExperience using AWS data technologies such as (S3, Glue, Kinesis, Lambda, etc.)Experience in dockerExperience using kafkaExperience in building data feeds and business reports', 'Employer paid lunch program (except for remote employees)', '3+ years working experience working with automated scripting, data modeling, and data architectureProficient in writing and optimizing SQL scripts Understands database architectureWorking experience in the Python language with an emphasis on dataWorking knowledge of some AWS data technologiesUnderstanding of the software development lifecycle processSkills and experience in finding, investigating, and resolving data quality issuesAbility to work in a fast-paced environment, meet deadlines, and prioritize a workloadAbility to bring new ideas and promote process improvementStrong business communication skills that can break down technical problems into business language for non-technical personnel', 'Build and maintain data structures and ETL/ELT data pipelines', 'Be part of an on call support rotation to support the Data Warehouse and it’s automated processes', 'Working experience in the Python language with an emphasis on data', 'Help analytics team, upstream engineering teams, as well as non-technical business users in understanding the Data Warehouse', '3+ years working experience working with automated scripting, data modeling, and data architecture', 'Experience in docker', 'The Role', 'Skills and experience in finding, investigating, and resolving data quality issues', 'Ability to bring new ideas and promote process improvement', 'Proficient in writing and optimizing SQL scripts ', 'Experience creating data pipelines using Python scripting']",Not Applicable,Full-time,Information Technology,Financial Services,2021-03-24 13:05:10
Data Engineer - 100% Remote,Conexess Group,"Nashville, TN",5 days ago,Be among the first 25 applicants,"['', 'Direct experience of building data piplines using Azure Data Factory and Apache Spark (preferably Databricks).', 'You Will Have', 'Experience working in a Dev/Ops environment with tools such as Microsoft Visual Studio Team Services, Chef, Puppet or Terraform', 'Experience using geospatial frameworks on Apache Spark and associated design and development patterns', 'Experience with Apache Kafka / Nifi for use with streaming data / event-based data', 'Experience with Open Source non-relational / NoSQL data repositories (incl. MongoDB, Cassandra, Neo4J)', ' Strong knowledge of Data Management principles Experience in building ETL / data warehouse transformation processes Direct experience of building data piplines using Azure Data Factory and Apache Spark (preferably Databricks). Experience using geospatial frameworks on Apache Spark and associated design and development patterns Microsoft Azure Big Data Architecture certification. Hands on experience designing and delivering solutions using the Azure Data Analytics platform (Cortana Intelligence Platform) including Azure Storage, Azure SQL Data Warehouse, Azure Data Lake, Azure Cosmos DB, Azure Stream Analytics Experience with Apache Kafka / Nifi for use with streaming data / event-based data Experience with other Open Source big data products Hadoop (incl. Hive, Pig, Impala) Experience with Open Source non-relational / NoSQL data repositories (incl. MongoDB, Cassandra, Neo4J) Experience working with structured and unstructured data including imaging & geospatial data. Experience working in a Dev/Ops environment with tools such as Microsoft Visual Studio Team Services, Chef, Puppet or Terraform ', 'Microsoft Azure Big Data Architecture certification.', 'Experience in building ETL / data warehouse transformation processes', 'Experience working with structured and unstructured data including imaging & geospatial data.', 'Hands on experience designing and delivering solutions using the Azure Data Analytics platform (Cortana Intelligence Platform) including Azure Storage, Azure SQL Data Warehouse, Azure Data Lake, Azure Cosmos DB, Azure Stream Analytics', 'Strong knowledge of Data Management principles', 'Experience with other Open Source big data products Hadoop (incl. Hive, Pig, Impala)']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Arcadia,"New York, NY",7 days ago,Be among the first 25 applicants,"['', 'Experience with Amazon Web Services (AWS) or other cloud infrastructure platforms', 'Extensive experience in managing data pipelines, schemas, and storage for multiple systems for multiple teams', 'Strong communication skills', 'Manage data pipelines from disparate sources, standardizing and feeding them into our centralized data warehouse', 'Experience in one or more of the following languages: Python, Java, Ruby, Javascript', 'Experience in predictive modeling and statistical analysis', ' Manage data pipelines from disparate sources, standardizing and feeding them into our centralized data warehouse Work with both the Engineering and Analytics & Data Science teams to optimize data flow and queries for large data sets to improve scalability Sync data across internal and external systems, such as marketing and sales automation tools, to enable key stakeholders to build best-in-class experiences Support ongoing efforts to establish and enforce best practices on data quality, use, and security across the company ', 'Undergraduate and/or graduate degree in math, statistics, engineering, computer science, or related technical field', 'Support ongoing efforts to establish and enforce best practices on data quality, use, and security across the company', 'Sync data across internal and external systems, such as marketing and sales automation tools, to enable key stakeholders to build best-in-class experiences', 'Advanced knowledge of algorithms, data structures, and relational algebra', 'Join a mashup of energy enthusiasts and creative tech wizards who are taking the fight to climate change. Disrupt and reimagine the energy experience using modern technologies.', 'Work with both the Engineering and Analytics & Data Science teams to optimize data flow and queries for large data sets to improve scalability', 'Paid Time Off (holidays, vacation, professional development, volunteer, parental leave)', 'Professional development opportunities', 'What You’ll Do', 'A supportive engineering culture that values diversity, empathy, teamwork, trust, and efficiency', 'A chance to decarbonize and disrupt the energy sector', 'Market-based compensation (salary + equity)', 'Experience in the energy sector', 'What We’re Looking For', 'Healthcare, dental, vision, 401(k) and commuter benefits', ' 3+ years combined programming and/or DevOps experience Significant experience with and a strong understanding of languages/tools relevant to engineering & data teams’ work Experience in one or more of the following languages: Python, Java, Ruby, Javascript Advanced knowledge of algorithms, data structures, and relational algebra Database management experience with PostgreSQL, RDS, or Redshift Data extraction experience with a strong understanding of thread-based and event-based paradigms Extensive experience in managing data pipelines, schemas, and storage for multiple systems for multiple teams Strong communication skills ', ' Market-based compensation (salary + equity) Healthcare, dental, vision, 401(k) and commuter benefits Paid Time Off (holidays, vacation, professional development, volunteer, parental leave) A supportive engineering culture that values diversity, empathy, teamwork, trust, and efficiency Professional development opportunities All-company lunches Free clean energy A chance to decarbonize and disrupt the energy sector ', 'Eliminating carbon footprints, eliminating carbon copies.', 'Data extraction experience with a strong understanding of thread-based and event-based paradigms', 'Significant experience with and a strong understanding of languages/tools relevant to engineering & data teams’ work', 'Benefits', 'Free clean energy', 'Experience with enterprise database interfaces and messaging APIs', 'a 100% clean energy future.', 'Experience with entity resolution at scale', 'Must-haves', 'What will help you succeed:', '3+ years combined programming and/or DevOps experience', 'Database management experience with PostgreSQL, RDS, or Redshift', 'All-company lunches', 'Nice-to-haves', ' Undergraduate and/or graduate degree in math, statistics, engineering, computer science, or related technical field Experience in predictive modeling and statistical analysis Experience with enterprise database interfaces and messaging APIs Experience with Amazon Web Services (AWS) or other cloud infrastructure platforms Experience with entity resolution at scale Experience in the energy sector ']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer,Daugherty Business Solutions,"Columbus, Ohio Metropolitan Area",,N/A,"['', 'Daugherty is hiring experienced Data Engineers to join our Columbus-based team.\xa0The ideal employee is a problem solver with the ability to utilize insights, creativity and perspective to drive business success for our clients.', 'Data & Analytics', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Exposure to stream-processing and messaging, such as Storm, Spark-Streaming, Kafka and MQ.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Intermediate level of SQL programming and query performance tuning techniques for data integration and consumption using design for optimum performance against large data assets within an OLTP, OLAP and MPP architecture.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Work with team members and functional leads to understand existing data requirements and validation rules to support moving existing data warehouse workloads into a distributed data platform.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Recommend ways to improve data reliability, efficiency and quality.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Revenue sharing and a 401(k) retirement savings plan.', 'If you require accommodations or assistance to complete the online application process, please inform any recruiter you are working with (or send an email to careers@daugherty.com) and identify the type of accommodation or assistance you are requesting. Do not include any medical or health information in this email. The recruiting team will respond to your email promptly.', 'From Ron Daugherty, founder and CEO of Daugherty Business Solutions:\xa0“I am excited to announce that Daugherty is expanding to Columbus, OH.\xa0This expansion is a direct result of the hard work of our consultants during this pandemic.\xa0Daugherty teammates continue to find ways to add even more value for our clients and to increase the demand for our consulting expertise.\xa0Columbus is a growing metro area with a strong base of talent, excellent universities, and interesting perspective clients.\xa0That’s why I’m proud to add Columbus to our existing development centers across the country.”', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Maintain and manage Hadoop clusters in development and production environments.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Some experience with programming Languages, such as Scala, Java, R and Python.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Contribute to the creation and maintenance of optimal data pipeline architectures.', 'As a Data Engineer you will have the opportunity to:', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Employ a variety of languages and tools to marry systems together.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Familiarity with high performance data libraries including Spark, NumPy and TensorFlow.', 'When you are a Daugherty employee, your job doesn’t end when a contract is up. You stay on as an indispensable member of the team with career growth opportunities tailored to your interests and talents. We want you to be eager to take on a new challenge. We are always 100% honest about what to expect, because we don’t want Daugherty to be just another job; we want Daugherty to be your dream job.\xa0', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Interest in Hadoop family languages including Pig and Hive.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Create custom software components (e.g. specialized UDFs) and analytics applications.', 'Daugherty Business Solutions is an inclusive Equal Employment Opportunity employer that considers applicants without regard to gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law.', 'We are looking for motivated people with:', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Assemble large, complex data sets that meet functional/non-functional business requirements.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Implement & automate high-performance algorithms, prototypes and predictive models.', '\xa0', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Proven ability to pick up new languages and technologies quickly.', 'Due to COVID-19, most of our employees are working remotely. We’ve implemented a virtual hiring process and continue to interview candidates by phone or video and are onboarding new hires remotely. We value the safety of each member of our community because we know we’re all in this together.', 'In addition to existing Software Development Centers across the organization; in Minneapolis, Atlanta, Dallas, Chicago, NY and the St. Louis Headquarters, the team in Columbus will strategically support Daugherty’s growth strategy and commitment to delivering high quality software fast and effectively for its Fortune 500 clients.\xa0', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Interest working with AWS technologies such as Redshift, RDS, S3, EMR, ADP, Hive, Kinesis,', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Understanding of DevOps and CI/CD toolset, such as Jenkins, GitLab CI, Buildbot, Drone and Bamboo.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience building data pipelines to connect analytics stacks, client data visualization tools and external data sources.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Interest in Python, R, sh/bash and JVM-based languages including Scala and Java.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Life, disability and long-term care insurance.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Robust career development and training.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Excellent health, dental and vision insurance.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Knowledge of cloud and distributed systems principles, including load balancing, networks, scaling, and in-memory versus disk.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Collaborate and work closely with team to build data platforms.', 'We offer members of Team Daugherty:', 'Daugherty Business Solutions, a leading advisory services and technology consulting firm will be expanding its operations to open a new, world-class Software Development Center in Columbus, Ohio to support its rapid growth and to engage the region’s diverse talent pool and thriving business community.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Little to no travel.', 'SNS/SQS and QuickSight.', 'Interested? Apply today to take your career to the next level!']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,The Hartford,"Hartford, CT",2 weeks ago,Be among the first 25 applicants,"['', ' Design and develop high quality, scalable software modules for next generation analytics solution suite  Prototype high impact innovations, catering to changing business needs, by learning and leveraging new technologies (AWS Cloud, Big Data, Snowflake).  Integrate with Data Quality Services to ensure Quality data is Published to consumers.  Possesses functional knowledge and skills reflective of a competent practitioner with the ability to deliver on work of varying technical complexity  Consults with functional management in the analysis of short and long-range business requirements and recommends innovations which anticipate the future impact of changing business needs  Works closely with client management to identify and specify the complex business requirements and processes for diverse development platforms, computing environments (e.g., Cloud, host based, distributed systems, client server), software, hardware, technologies and tools  Coordinate activities with cross-functional IT unit stakeholders (e.g., database, operations, telecommunications, technical support, etc.)  Researches and evaluates alternative solutions and recommends the most efficient and cost effective solution for the systems design  Work within a self-organized scrum development team regarding all design and implementation ', ' Bachelor degree with at least 2 years of applicable work experience or equivalent', ' Data warehouse applications knowledge in insurance domain ', 'Understanding of current and emerging IT products, services, processes and methodologies.', 'Role Expectations', 'Ability to work as part of and with high performing teams.', ' Experience in ETL / Data Integration Technologies ', ' Work within a self-organized scrum development team regarding all design and implementation ', ' Consults with functional management in the analysis of short and long-range business requirements and recommends innovations which anticipate the future impact of changing business needs ', ' Researches and evaluates alternative solutions and recommends the most efficient and cost effective solution for the systems design ', 'Ability to develop and maintain systems according to a defined set of standards.', 'Analytical approach with a strong ability to uncover and resolve problems by delivering innovative approaches and solutions.', ' Bachelor degree with at least 2 years of applicable work experience or equivalent Experience in ETL / Data Integration Technologies  Knowledge of Oracle Exadata , Unix/Linux Shell scripting, Autosys, Version Control Tools  Data warehouse applications knowledge in insurance domain  Experienced in Agile Scrum and Kanban Methodologies Understanding of current and emerging IT products, services, processes and methodologies.Analytical approach with a strong ability to uncover and resolve problems by delivering innovative approaches and solutions.Ability to develop and maintain systems according to a defined set of standards.Ability to set and manage own priorities effectively in a dynamic organization.Ability to work as part of and with high performing teams.', 'Qualifications', 'Ability to set and manage own priorities effectively in a dynamic organization.', ' Design and develop high quality, scalable software modules for next generation analytics solution suite ', 'Ready to grow your career leveraging the latest DATA technologies?', ' Integrate with Data Quality Services to ensure Quality data is Published to consumers. ', ' Coordinate activities with cross-functional IT unit stakeholders (e.g., database, operations, telecommunications, technical support, etc.) ', ' Prototype high impact innovations, catering to changing business needs, by learning and leveraging new technologies (AWS Cloud, Big Data, Snowflake). ', ' Experienced in Agile Scrum and Kanban Methodologies ', ' Knowledge of Oracle Exadata , Unix/Linux Shell scripting, Autosys, Version Control Tools ', ' Possesses functional knowledge and skills reflective of a competent practitioner with the ability to deliver on work of varying technical complexity ', ' Works closely with client management to identify and specify the complex business requirements and processes for diverse development platforms, computing environments (e.g., Cloud, host based, distributed systems, client server), software, hardware, technologies and tools ']",Entry level,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Data Engineer Sr,PNC,"Strongsville, OH",2 days ago,Be among the first 25 applicants,"['', 'Customer Focused - Knowledgeable of the values and practices that align customer needs and satisfaction as primary considerations in all business decisions and able to leverage that information in creating customized customer solutions.', ""Customer Focused - Knowledgeable of the values and practices that align customer needs and satisfaction as primary considerations in all business decisions and able to leverage that information in creating customized customer solutions.Managing Risk - Assessing and effectively managing all of the risks associated with their business objectives and activities to ensure they adhere to and support PNC's Enterprise Risk Management Framework."", 'Leads in developing, supporting and implementing data solutions for multiple applications in order to meet business objectives and user requirements. Leverages technical knowledge and industry experience to design, build and maintain technology solutions.', 'Oversees the development and implementation of data solutions for multiple applications to ensure its scalability, availability and maintainability.', 'Disability Accommodations Statement', 'Equal Employment Opportunity (EEO)', 'Education', 'Leads in designing and building data service infrastructure on multiple data platforms, according the workflow.', 'California Residents ', 'Managing Risk', 'Leads data requirement analysis and the data preparation process development for targeted data solutions.', 'Leads in developing, supporting and implementing data solutions for multiple applications in order to meet business objectives and user requirements. Leverages technical knowledge and industry experience to design, build and maintain technology solutions.Leads data requirement analysis and the data preparation process development for targeted data solutions.Leads in designing and building data service infrastructure on multiple data platforms, according the workflow.Oversees the development and implementation of data solutions for multiple applications to ensure its scalability, availability and maintainability.Consults on data migration and transformation to ensure the accuracy and security of data solutions.', 'Work Experience', 'Consults on data migration and transformation to ensure the accuracy and security of data solutions.', 'Position Overview', ""Managing Risk - Assessing and effectively managing all of the risks associated with their business objectives and activities to ensure they adhere to and support PNC's Enterprise Risk Management Framework."", 'Job Description', 'Customer Focused', 'Competencies']",Not Applicable,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Amgen,"Tampa, FL",4 weeks ago,94 applicants,"['', 'Collaborate with Data Architects, Business SME’s, and Data Scientists to design and develop end-to-end data pipeline to meet fast paced business need across geographic regions', 'Experience with software DevOps CI/CD tools, such Git, Jenkins', 'Experience with ETL tool, for example Informatica PowerCenter', 'Experience with Tableau Dashboard and Tableau Server', 'Preferred Qualifications', 'Able to explore new tools, technologies that will help to improve ETL platform performance', 'Hands on development experience with Informatica Power Center, MDM, Data Integration Hub', 'Participate in sprint planning meetings and provide estimations on technical implementation; Collaborate and communicate effectively with the product teams', 'Be a key team member assisting in design and development of the data pipeline for Global Data and Analytics team', 'Experience with Pharmaceutical industry, commercial operations ', 'Be a key team member assisting in design and development of the data pipeline for Global Data and Analytics teamCollaborate with Data Architects, Business SME’s, and Data Scientists to design and develop end-to-end data pipeline to meet fast paced business need across geographic regionsServe as system admin to manage AWS and Databricks platform;Adhere to best practices for coding, testing and designing reusable code/componentAble to explore new tools, technologies that will help to improve ETL platform performanceParticipate in sprint planning meetings and provide estimations on technical implementation; Collaborate and communicate effectively with the product teams', 'Adhere to best practices for coding, testing and designing reusable code/component', 'Experience on AWS, familiar with EC2, S3, Redshift/Spectrum, Glue, Athena, RDS, Lambda, and API gateway', 'Responsibilities', 'Serve as system admin to manage AWS and Databricks platform;', 'Experience with software development (Java, Python preferred), end-to-end system designExperience with data modeling for both OLAP and OLTP databases, hands-on experience with SQL, preferred Oracle, PostgreSQL, and Hive SQLExperience with ETL tool, for example Informatica PowerCenterAbility to learn quickly, be organized and detail oriented Hands on development experience with Informatica Power Center, MDM, Data Integration HubExperience with software DevOps CI/CD tools, such Git, JenkinsExperience on AWS, familiar with EC2, S3, Redshift/Spectrum, Glue, Athena, RDS, Lambda, and API gatewayExperience with Apache Airflow and Apache SparkExperience with Tableau Dashboard and Tableau ServerExperience with Pharmaceutical industry, commercial operations ', 'Experience with Apache Airflow and Apache Spark', 'Ability to learn quickly, be organized and detail oriented ', 'Basic Qualifications', 'Experience with data modeling for both OLAP and OLTP databases, hands-on experience with SQL, preferred Oracle, PostgreSQL, and Hive SQL', 'Experience with software development (Java, Python preferred), end-to-end system design']",Not Applicable,Full-time,Information Technology,Biotechnology,2021-03-24 13:05:10
Data Engineer,Storable,"North Carolina, United States",4 weeks ago,102 applicants,"['', 'We leverage our platform in combination with our over 25 years of storage industry expertise to help our thousands of storage customers achieve their tenant experience and operational efficiency objectives every single day.', 'Computer science degree or equivalent experience3+\xa0 years experience in software development, data engineering, BI development, and / or data architectureExperience with Python, SQL, Airflow, AWS, RESTful APIs, and Tableau or other data visualization toolsConsistent track record of leading successful delivery for a large-scale project or being a key contributor on multiple projectsConsistent track record of positively influencing project direction and contributions to cross-functional and/or cross-organizational collaborations', 'Generous health coverage for you and your family, including short- and long-term disability coverage, 401(k) and HSA matching, company-provided life insurance, and more.', 'What you’ll do everyday:', 'Work with data and analytics experts to strive for greater functionality in our data systems', '3+\xa0 years experience in software development, data engineering, BI development, and / or data architecture', 'Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using various SQL and ‘big data’ technologies', 'All applicants must be currently authorized to work in the United States on a full-time basis.', 'The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.', 'Do More, Be More – Company rewards and recognition that add up and can be redeemed for once-in-a-lifetime Bucketlist experiences!', 'Storable is looking for a Data Engineer to join our Data Architecture team in Austin. This team member will be responsible for expanding and optimizing our cloud native big data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing distributed systems as well as building them from the ground up.\xa0', 'Storable is committed to providing equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, Storable will provide reasonable accommodations for qualified individuals with disabilities.', 'At Storable, we believe storage operators should have one partner they can trust to help you get the results they need for their business. That’s why we’ve built the industry’s first fully integrated platform that offers facility management software, facility websites, marketing programs and services, payments, and deeply integrated tenant insurance capabilities all in one solution.\xa0', 'Must\xa0be located on:\xa0TX, KS, NC, MO, CO, PA, IL, IN', 'Experience with Python, SQL, Airflow, AWS, RESTful APIs, and Tableau or other data visualization tools', 'About Us:', 'Location:\xa0Remote', 'Benefits and Perks:', 'Fun company events, including Halloween costume contests, ugly sweater competitions, baseball game outings, ice cream socials, food trucks, and more.', 'Generous health coverage for you and your family, including short- and long-term disability coverage, 401(k) and HSA matching, company-provided life insurance, and more.Engage remotely! Since mid-March we transitioned to a fully remote environment and we run regular contests, trivia games and remote happy hours!Instant-Access & Flexible vacation. We trust you, so we have a ‘take what you need’ vacation policy. No waiting to use it or need for accruals.Do More, Be More – Company rewards and recognition that add up and can be redeemed for once-in-a-lifetime Bucketlist experiences!Get active in the community by joining one of our many quarterly offsite volunteer and community service events.Fun company events, including Halloween costume contests, ugly sweater competitions, baseball game outings, ice cream socials, food trucks, and more.', 'Consistent track record of leading successful delivery for a large-scale project or being a key contributor on multiple projects', 'Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics', 'Instant-Access & Flexible vacation. We trust you, so we have a ‘take what you need’ vacation policy. No waiting to use it or need for accruals.', 'Create and maintain optimal data pipeline architecture', 'Computer science degree or equivalent experience', 'Consistent track record of positively influencing project direction and contributions to cross-functional and/or cross-organizational collaborations', 'Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.', 'Must', 'Data Engineer', 'Work closely with stakeholders including the Executive, Product, Data and Design teams to design and deliver products and functionality to address analytical and functional data needs', 'Get active in the community by joining one of our many quarterly offsite volunteer and community service events.', 'What you need to bring to the table:', 'The Data Engineer will support our software developers, database architects, data analysts and data scientists on stakeholder initiatives and ensure delivered architecture is consistent and supportable throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products.\xa0', 'Create data tools for analytics and data scientist and business operations team members that assist them in building and optimizing our product', 'Assemble large, complex data sets that meet both functional and non-functional requirements', 'Create and maintain optimal data pipeline architectureAssemble large, complex data sets that meet both functional and non-functional requirementsIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using various SQL and ‘big data’ technologiesBuild analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metricsWork closely with stakeholders including the Executive, Product, Data and Design teams to design and deliver products and functionality to address analytical and functional data needsCreate data tools for analytics and data scientist and business operations team members that assist them in building and optimizing our productWork with data and analytics experts to strive for greater functionality in our data systems', 'Engage remotely! Since mid-March we transitioned to a fully remote environment and we run regular contests, trivia games and remote happy hours!']",Mid-Senior level,Full-time,Information Technology,Internet,2021-03-24 13:05:10
"Data Engineer, CRM",TikTok,"Mountain View, CA",1 day ago,Be among the first 25 applicants,"['', 'Responsibilities:', '2. 3+ years of experience with Data Warehouse. ', ""TikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too.  "", 'The goal of the Customer Growth team is to build a stable, flexible and intelligent CRM platform, improve commercialization efficiency and client satisfaction. We are seeking self-motivated software engineers to develop an excellent platform for our clients and sales all over the world.  ', 'TikTok is committed to providing reasonable accommodations during our recruitment process. If you need assistance or an accommodation, please reach out to us at usrc@tiktok.com.', 'Qualifications:', 'Responsibilities: ', '4. Proficiency in multiple systems languages (Java, Python, Go etc). ', '1. Follow up the excellent solutions in the industry and implement them in our system. ', 'TikTok is the leading destination for short-form mobile video. Our mission is to inspire creativity and bring joy. TikTok has global offices including Los Angeles, Mountain View, New York, London, Paris, Berlin, Dubai, Mumbai, Singapore, Jakarta, Seoul and Tokyo.  ', '2. Responsible for SAAS CRM system operation, maintenance and problem solving. ', '6. Knowledge of Ad Sales automated processes and best practices in a CRM platform.  ', ""3. Understand the user's personalized demands and select reasonable technical solutions to solve problems quickly."", '3. Experience with Spark, Hive, Hadoop, SQL, Kafka, Parquet, HDFS, or HBase. ', ""Leveraging your knowledge of CRM system architecture, you'll work hands on in a fast paced environment to engineer solutions and actionable recommendations to develop TikTok's proprietary CRM into an enterprise grade, market leading platform. You'll work alongside a global team of product managers located in TikTok's major markets to build innovative sales, marketing and analytics tools to increase user adoption and satisfaction.  "", ""1. Bachelor's degree or above in Computer Science or related fields. "", '5. Knowledge of standard CRM functionality across Lead to Compensation pipeline. ']",Mid-Senior level,Full-time,Engineering,Internet,2021-03-24 13:05:10
Data Engineer,IBM,"Austin, TX",2 weeks ago,Be among the first 25 applicants,"['', 'software engineering', 'Key Responsibilities', '3+ years design & implementation experience with distributed applications', 'DevOps', 'Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores. ', 'data process ', 'About Business Unit', 'Develop code using Python, Scala, R languages', 'Experience in software engineering with object-oriented design, coding and testing patterns on large-scale data infrastructures', 'Preferred Technical And Professional Expertise', 'Experience with stream-processing systems: Storm, Spark-Streaming, etc.Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.', 'Demonstrated knowledge of software development tools and methodologies', 'Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical requirements and support their data infrastructure needs.', 'Experience with big data tools: Hadoop, Spark, Kafka, etc.', 'stream', 'Use DevOps best practices such as continuous integration, continuous delivery in the production implementation. ', 'Familiar with big data solutions with experience on Hadoop based technologies such as MapReduce, Hive MongoDB or Cassandra.', 'Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical requirements and support their data infrastructure needs.Provide the ability to work within agile development methodology and collaborate effectively with multi-disciplinary teamsBuild modern enterprise solutions which support scaling, development, test and data quality evaluation big data solutions based on the requirements. Understand data architecture, build large-scale data processing systems supporting data transformation, data structures, metadata, dependency and workload management. and optimizes data flow. Have experience on big data process including collecting, parsing, manipulating, managing, analyzing and visualizing large sets of data to turn information into insights using multiple platforms. Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores. Have expertise in data persistence solutions, experience with the latest (NoSQL) database technologies, and experience with building complex SQL queries using various (NoSQL or RDBMS) databases such as MongoDB or DB2Experience in software engineering with object-oriented design, coding and testing patterns on large-scale data infrastructuresUse DevOps best practices such as continuous integration, continuous delivery in the production implementation. ', 'Experience with relational SQL and NoSQL databases, including Postgres and Cassandra', 'Experience with big data tools: Hadoop, Spark, Kafka, etc.Familiar with big data solutions with experience on Hadoop based technologies such as MapReduce, Hive MongoDB or Cassandra.Experience with stream-processing systems: Storm, Spark-Streaming, etc.Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.Knowledge of cloud technologies such as Kubernetes, Cloud Foundry, PaaS, and IaaS (SoftLayer)', 'Develop code using Python, Scala, R languagesExperience with relational SQL and NoSQL databases, including Postgres and Cassandra3+ years design & implementation experience with distributed applications3+ years of working experience in database architectures and data pipeline developmentDemonstrated knowledge of software development tools and methodologiesComputer Science with software engineering and Math background desired', 'Build modern enterprise solutions which support scaling, development, test and data quality evaluation big data solutions based on the requirements. ', 'About IBM', 'Provide the ability to work within agile development methodology and collaborate effectively with multi-disciplinary teams', 'Knowledge of cloud technologies such as Kubernetes, Cloud Foundry, PaaS, and IaaS (SoftLayer)', 'Have expertise in data persistence solutions, experience with the latest (NoSQL) database technologies, and experience with building complex SQL queries using various (NoSQL or RDBMS) databases such as MongoDB or DB2', 'persistence', 'Have experience on big data process including collecting, parsing, manipulating, managing, analyzing and visualizing large sets of data to turn information into insights using multiple platforms. ', 'Understand data architecture, build large-scale data processing systems supporting data transformation, data structures, metadata, dependency and workload management. and optimizes data flow. ', '3+ years of working experience in database architectures and data pipeline development', 'Computer Science with software engineering and Math background desired']",Not Applicable,Full-time,Sales,Computer Hardware,2021-03-24 13:05:10
Data Engineer,Hearst Autos,"Detroit, MI",1 week ago,Be among the first 25 applicants,"['', 'Work with GCP Big Data products including BigQuery, Pub/Sub, Cloud Composer, Dataflow, Dataproc, Dataprep, Data Studio, Bigtable, Cloud Storage, etc.', 'You apply software engineering best practices to the data engineering process.', 'Implement new and support existing data pipelines (ETL, ELT) and data wrangling procedures using Python and SQL.', 'You are passionate about working with diverse and large datasets.', 'You have experience with batch and stream data processing.', 'You have at least 3-5 years experience as a data engineer and are experienced in the Google data services stack.', 'Potentially work in AWS cloud services.', 'Research, analyze, and recommend technical approaches for solving difficult and challenging development and integration problems.', 'Work with GCP Big Data products including BigQuery, Pub/Sub, Cloud Composer, Dataflow, Dataproc, Dataprep, Data Studio, Bigtable, Cloud Storage, etc.Implement different types of storage (filesystem, relational, NoSQL) and work with various kinds of data (structured, unstructured, metrics, log files, etc.)Implement new and support existing data pipelines (ETL, ELT) and data wrangling procedures using Python and SQL.Tune application and query performance using performance profiling tools and SQLFamiliarity with agile software development practices and drive to ship quickly.Research, analyze, and recommend technical approaches for solving difficult and challenging development and integration problems.Potentially work in AWS cloud services.', 'Tune application and query performance using performance profiling tools and SQL', 'What You’ll Do', 'Familiarity with agile software development practices and drive to ship quickly.', 'Your Impact', 'You have at least 3-5 years experience as a data engineer and are experienced in the Google data services stack.You are passionate about working with diverse and large datasets.You apply software engineering best practices to the data engineering process.You have a desire to learn and continuously improve.You have experience with batch and stream data processing.', 'You have a desire to learn and continuously improve.', 'Why Hearst Magazines?', 'Job Description', 'Implement different types of storage (filesystem, relational, NoSQL) and work with various kinds of data (structured, unstructured, metrics, log files, etc.)', 'Who You Are']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer,"ITCO Solutions, Inc.","Carlsbad, CA",4 weeks ago,60 applicants,"['', '2+ years of professional development experience and at least one year of creating object-oriented JavaScript from scratch', '2 years of professional experience in a team environment (no 1-2 person teams)', ' At least one year of professional experience with react and read 3+ years of overall experience Currently working on a mob X but anyone can pick it up Want work horses on the team Individuals who are young and hungry to learn, take direction and contribute 2+ years of professional development experience and at least one year of creating object-oriented JavaScript from scratch Professional experience working in a team environment Love candidates who look at languages as tools, and development as the end goal Computer science degree is a major plus ', ' 2+ years of professional React experience building state driven applications (Redux or MobX preferred) A development style that puts a high value on testing, ideally using React Testing Library and/or Cypress A thorough understanding of full stack web development using a modern JavaScript framework, APIs, and microservices (backend experience preferred, Ruby on Rails a big plus). 4 Year Computer Science Degree 2 years of professional experience in a team environment (no 1-2 person teams) Focus on React, and Redux Developers that test their own code ', 'Focus on React, and Redux', 'Currently working on a mob X but anyone can pick it up', 'Professional experience working in a team environment', 'A thorough understanding of full stack web development using a modern JavaScript framework, APIs, and microservices (backend experience preferred, Ruby on Rails a big plus).', 'Reduce complexity', 'Ideal Candidate', 'ITCO Overview', ' Professional experience working for one person Only react experience from code camps No testing experience ', 'At least one year of professional experience with react and read', 'No testing experience', 'Computer science degree is a major plus', 'SmartSoft ', '3+ years of overall experience', 'Hard-dollar costs savings by as much as 30% for enterprise software licensing', 'Provide an assessment of your existing licensing', 'Location: ', '2+ years of professional React experience building state driven applications (Redux or MobX preferred)', 'Individuals who are young and hungry to learn, take direction and contribute', 'Professional experience working for one person', 'Not Ideal', 'Only react experience from code camps', 'Infrastructure, Virtualization, Cloud, and Managed services', ' Hard-dollar costs savings by as much as 30% for enterprise software licensing Simplify the management and maintenance of software purchases Provide an assessment of your existing licensing', 'Want work horses on the team', 'Consulting Services ', '4 Year Computer Science Degree', 'Other Testing Tools', 'Turn-Key project delivery', 'Simplify the management and maintenance of software purchases', 'Jasmine, Mocha, Chai,', 'A development style that puts a high value on testing, ideally using React Testing Library and/or Cypress', ' Jasmine, Mocha, Chai, ', 'Love candidates who look at languages as tools, and development as the end goal', 'Workforce Solutions ', 'Developers that test their own code', 'Job Description', ' Turn-Key project delivery Reduce complexity Infrastructure, Virtualization, Cloud, and Managed services ', 'Required Skills']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,ID.me,"McLean, VA",7 days ago,34 applicants,"['Overview', '', 'Qualifications', 'Responsibilities']",Associate,Full-time,Engineering,Computer Software,2021-03-24 13:05:10
Data Engineer (m/f/d),Springer Nature Technology and Publishing Solutions,"Springer, NM",2 weeks ago,Be among the first 25 applicants,"['What You Will Do', ' Visit our website at ', 'Strong analytical, organizational and problem-solving skills', 'Ability to adapt and drive change', 'Who We Are', ' Excited by the prospect of joining us? ', 'Develop best practices and blueprints that can be used across projects', 'Visit the Springer Nature Editorial and Publishing website at', 'Excellent command (business fluency) of English and German', ' Your contact person for the location Heidelberg is Ms Birgit Kolb, HR Director Heidelberg. ', ' Project management skills Experience with Google Cloud Platform technologies like Big Query, Cloud Composer or Data Flow Experience with data and process analysis tools like the Celonis Execution Management System Knowledge of scientific publishing processes ', ' University degree in the area of computer science, engineering or a comparable education Solution-oriented problem-solver, proficient and experienced with SQL and Python Experience with different DB systems (e.g. MS SQL server, Postgres, Vertica) Experience with data pipelines and ETL processes, ideally for larger volumes of data Strong analytical, organizational and problem-solving skills Ability to adapt and drive change Excellent command (business fluency) of English and German ', 'Youare as much into working with data, processes and software solutions as you are into working with people. You would describe yourself as a get-it-done and out-of-the-box person who appreciates the cultural diversity of a global company. You have already gained professional experience in international projects, you value self-dependency and reliability as much as we do, and you would be ready to get trained on the job.', 'University degree in the area of computer science, engineering or a comparable education', 'Springer Nature opens the doors to discovery for researchers, educators, clinicians and other professionals. Every day, around the globe, our imprints, books, journals, platforms and technology solutions reach millions of people. For over 175 years our brands and imprints have been a trusted source of knowledge to these communities and today, more than ever, we see it as our responsibility to ensure that fundamental knowledge can be found, verified, understood and used by our communities – enabling them to improve outcomes, make progress, and benefit the generations that follow. ', 'Experience with data and process analysis tools like the Celonis Execution Management System', 'Solution-oriented problem-solver, proficient and experienced with SQL and Python', 'i', 'Experience with different DB systems (e.g. MS SQL server, Postgres, Vertica)', 'Experience with Google Cloud Platform technologies like Big Query, Cloud Composer or Data Flow', ' Maintain and expand existing reporting systems, both proprietary an third-party Develop best practices and blueprints that can be used across projects ', 'Knowledge of scientific publishing processes', 'Experience with data pipelines and ETL processes, ideally for larger volumes of data', 'Project management skills', 'Maintain and expand existing reporting systems, both proprietary an third-party', 'Monitoritor the data pipelines, ensure their reliability and help troubleshoot any issues.', 'What We Expect', 'We take our work seriously but not ourselves, placing more importance on collaboration and relationship building than on solo heroics. Our mission is to perform together to lay the foundation for analyzing today’s processes to make tomorrow’s publishing more efficient.', 'This position can be filled at the following locations: Heidelberg, Dordrecht', ' Analyze, normalize and transform data from various in-house systems Implement data pipelines and data models for process mining Monitoritor the data pipelines, ensure their reliability and help troubleshoot any issues. ', 'Analyze, normalize and transform data from various in-house systems', 'What We Would Appreciate', 'Implement data pipelines and data models for process mining', 'You will join a new team working at the vibrant pulse of Springer Nature’s publishing workflows processing millions of articles. We combine data from a variety of systems to mine process flows and facilitate granular end-to-end analysis. We are an enabler for continuous improvement and transformation monitoring.', 'Who You Are', ' We are looking forward to your online application using our online application system (SuccessFactors) stating your first possible date of joining and your salary expectations. ']",Entry level,Full-time,Information Technology,Online Media,2021-03-24 13:05:10
Data Engineer,Jerry.ai,"Palo Alto, CA",3 weeks ago,178 applicants,"['', 'Develop tools supporting self-service data pipeline management (ETL)', 'Boston', ' Start-up energy working with a brilliant and passionate team Exponential growth (5 straight quarters of 50-100%+ quarter over quarter growth) Flat structure and access to senior leadership for continuous mentorship Meritocracy - we promote based on performance, not tenure Rockstar teammates. You will be working with a strong team with prior work experience at Amazon, Microsoft, NVIDIA, Alibaba, etc. ', 'Expertise in Python for developing and maintaining data pipeline code.', 'Rockstar teammates. You will be working with a strong team with prior work experience at Amazon, Microsoft, NVIDIA, Alibaba, etc.', 'Locations ', ' Owner of the core company data pipeline, responsible for scaling up data processing flow to meet the rapid data growth Consistently evolve data model & data schema based on business and engineering needs Implement systems tracking data quality and consistency Develop tools supporting self-service data pipeline management (ETL) SQL and MapReduce job tuning to improve data processing performance ', 'Flat structure and access to senior leadership for continuous mentorship', 'Meritocracy - we promote based on performance, not tenure', 'personal concierge for your car and home', 'About The Role', 'Owner of the core company data pipeline, responsible for scaling up data processing flow to meet the rapid data growth', 'Proficient in SQL, specially with Postgres dialect.', 'We’d love to hear from you if you like ', ' 2+ years of data engineering experience within a rigorous engineering environment Proficient in SQL, specially with Postgres dialect. Expertise in Python for developing and maintaining data pipeline code. Experience with Apache Spark and PySpark library (experience with AWS extension of PySpark is a plus). Experience with BI software (preferably Metabase or Tableau). Experience with Hadoop (or similar) Ecosystem. Experience with deploying and maintaining data infrastructure in the cloud (experience with AWS preferred). Comfortable working directly with data analytics to bridge business requirements with data engineering ', '2+ years of data engineering experience within a rigorous engineering environment', 'Implement systems tracking data quality and consistency', 'Comfortable working directly with data analytics to bridge business requirements with data engineering', 'Requirements', 'SQL and MapReduce job tuning to improve data processing performance', 'Experience with Hadoop (or similar) Ecosystem.', 'Toronto', 'Responsibilities', 'Palo Alto', 'About Jerry.ai', 'Experience with Apache Spark and PySpark library (experience with AWS extension of PySpark is a plus).', 'Experience with deploying and maintaining data infrastructure in the cloud (experience with AWS preferred).', 'Start-up energy working with a brilliant and passionate team', 'Consistently evolve data model & data schema based on business and engineering needs', 'Exponential growth (5 straight quarters of 50-100%+ quarter over quarter growth)', 'Experience with BI software (preferably Metabase or Tableau).', ' Toronto Boston Palo Alto']",Associate,Full-time,Engineering,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer,Michael Page,"New York, NY",3 weeks ago,Be among the first 25 applicants,"['', 'Competitive compensation', 'Has experience providing technical leadership and mentoring other engineers for best practices on data engineering', 'Growth opportunity', 'Growth OpportunityFull benefitsPaid sick daysPaid vacation daysFree products', 'The Successful Applicant', 'Contact: Gamou Ngom', 'Paid vacation days', 'Evaluate and integrate open source and vendor tools for various parts of data infrastructure', ""MPI does not discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity or expression, national origin, age, disability, veteran status, marital status, or based on an individual's status in any group or class protected by applicable federal, state or local law. MPI encourages applications from minorities, women, the disabled, protected veterans and all other qualified applicants."", 'Excellent communication skills. Ability to effectively communicate with both technical and non-technical audiences.', 'About Our Client', 'Growth Opportunity', ""Participate in code reviews, standups, and planning sessions, while listening to feedback and commenting on others' approaches"", ""Create data tools for data science and BI teams to assist them in building and optimizing our product into an innovative industry leaderEvaluate and integrate open source and vendor tools for various parts of data infrastructureParticipate in code reviews, standups, and planning sessions, while listening to feedback and commenting on others' approachesWork with stakeholders including the Executive, Product, Data Science and BI teams to assist with data-related technical issues and support their data infrastructure needsBuild scalable data pipelines and API integrations to support continuing increase in data volume and complexityDesign and build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources"", 'Description', 'Solid programming foundations and proficiency with data related languages such as Python/Spark/R.', 'Build scalable data pipelines and API integrations to support continuing increase in data volume and complexity', 'Work with stakeholders including the Executive, Product, Data Science and BI teams to assist with data-related technical issues and support their data infrastructure needs', ""What's On Offer"", 'Strong understanding of relational databases and SQL.Solid programming foundations and proficiency with data related languages such as Python/Spark/R.Excellent communication skills. Ability to effectively communicate with both technical and non-technical audiences.Has proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategyHas experience providing technical leadership and mentoring other engineers for best practices on data engineering', 'Design and build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources', 'Free products', 'Full benefits', 'Has proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy', 'Data Engineer', 'Quote job ref: 1514898', 'Strong understanding of relational databases and SQL.', 'Job Description', 'Create data tools for data science and BI teams to assist them in building and optimizing our product into an innovative industry leader', 'Paid sick days']",Entry level,Full-time,Information Technology,Consumer Goods,2021-03-24 13:05:10
Data Science Engineer (New Grad),Salesforce,"San Francisco, CA",2 days ago,Over 200 applicants,"['', 'What You Will Do', 'Salesforce.com', 'MS in computer science, software engineering or a related field. MUST have graduated in December 2020 or graduating in spring 2021.', 'You are solutions-focused and look for ways of improving current processes, models and status quo', 'Job Details', 'Collaborative: We love to brainstorm together!', 'Salesfore.com', 'You have strong communication skills and prefer to chart your own course to get the job done', 'Familiarity with managing source code using Git', 'You have completed coursework and have a good understanding of data structures, software engineering principles, database management systems and algorithms.', 'Accommodations ', 'Accommodations  - ', 'You are capable of writing solid code in Python, Pyspark or Java.', 'Salesforce.org', 'You are a passionate, curious, and effective self-starter', 'Team Overview', 'To get the best candidate experience, please consider applying for a maximum of 3 roles within 12 months to ensure you are not duplicating efforts.', 'You are familiar with data manipulation and ML libraries (like numpy, pandas, Sklearn etc...), orchestration tools (airflow, cron etc...), containerization tools (like Docker, AWS ECS), Cloud platforms.', 'MS in computer science, software engineering or a related field. MUST have graduated in December 2020 or graduating in spring 2021.You have completed coursework and have a good understanding of data structures, software engineering principles, database management systems and algorithms.You are capable of writing solid code in Python, Pyspark or Java.You are familiar with data manipulation and ML libraries (like numpy, pandas, Sklearn etc...), orchestration tools (airflow, cron etc...), containerization tools (like Docker, AWS ECS), Cloud platforms.Familiarity with distributed storage and distributed computing paradigms (Hadoop, Spark etc...)Familiarity with managing source code using GitYou have strong communication skills and prefer to chart your own course to get the job doneYou are solutions-focused and look for ways of improving current processes, models and status quoYou are an independent and impact-driven problem solver and a fast, efficient learnerYou are a passionate, curious, and effective self-starterCollaborative: We love to brainstorm together!', 'Posting Statement', 'Familiarity with distributed storage and distributed computing paradigms (Hadoop, Spark etc...)', 'Job Category', 'MUST have graduated in December 2020 or graduating in spring 2021.', 'Who You Are', 'You are an independent and impact-driven problem solver and a fast, efficient learner']",Not Applicable,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
"Data Engineer, IT Data Warehousing",American Airlines,"Phoenix, AZ",3 weeks ago,Be among the first 25 applicants,"['', 'CI/CD: GitHub, SubVersion, Jenkins, UrbanCode Deploy', 'CI/CD:', ""Master's degree in Computer Science, Computer Engineering, Technology, Information Systems (CIS/MIS), Engineering or related technical discipline, or equivalent experience/trainingAirline Industry experience2+ years of experience leading technical team members"", 'Skills, Licenses & Certifications', 'Experience with NoSQL databases like Cassandra (Preferred)', 'Big Data platform:', 'Researches and implements new technologies to enhance current processes, security, and performance', 'Databases:', ""All you'll need for success"", ""Bachelor's degree in Computer Science, Computer Engineering, Technology, Information Systems (CIS/MIS), Engineering or related technical discipline, or equivalent experience/training5 years of Software Development experience with expertise in data analytics, warehousing and engineering3 years of experience in Agile methods and mindset as an approach to software development"", 'Big Data platform: Hadoop, HDFS, Solr', 'Data Ingestion/Wrangling:', 'Big Data platform: Hadoop, HDFS, SolrData Ingestion/Wrangling: Kafka, Trifacta, Alteryx, Nifi, Big IntegrateScripting: Python, Spark, Unix, SQLData Warehousing: DataStage, InformaticaDatabases: Teradata, Cassandra, MongoDB, Oracle, SQL ServerCI/CD: GitHub, SubVersion, Jenkins, UrbanCode DeployBI: Cognos, Tableau, PowerBICloud: Azure, AWS, IBM, Google', 'Intro', 'Collaborates with leaders, business analysts, project managers, IT architects, technical leads and other developers, along with internal customers, to understand needs and develop solutions according to business requirementsCreates and maintains jobs to extract, transform and load data using ingestion tools and techniquesDesigns, develops and reviews the integration of analytics solutions and collaborates with application architects and development teamsBe a key contributor to the data governance, privacy and cataloging processesPartners with Data Scientists to deliver critical business initiatives using Artificial Intelligence, Machine Learning tools, etc. Troubleshoots and debugs complex issues; identifies and implements solutionsCollaborates with Architects to introduce and integrate analytics/data tools into the organizationLeads development of coding standards and enforce best practices and security guidelinesResearches and implements new technologies to enhance current processes, security, and performanceBe a part of a DevOps team that completely owns and supports the application 24/7Mentors other team members to make our team crossfunctionalParticipates in organization wide analytics as needed', 'Be a part of a DevOps team that completely owns and supports the application 24/7', '2+ years of experience leading technical team members', 'Travel Perks: Ready to explore the world? You, your family and your friends can reach 365 destinations on more than 6,800 daily flights across our global network.', ""What You'll Do"", 'Collaborates with Architects to introduce and integrate analytics/data tools into the organization', ""Master's degree in Computer Science, Computer Engineering, Technology, Information Systems (CIS/MIS), Engineering or related technical discipline, or equivalent experience/training"", 'Data Warehousing:', 'Scripting: Python, Spark, Unix, SQL', 'Creates and maintains jobs to extract, transform and load data using ingestion tools and techniques', 'Cloud: Azure, AWS, IBM, Google', 'Collaborates with leaders, business analysts, project managers, IT architects, technical leads and other developers, along with internal customers, to understand needs and develop solutions according to business requirements', 'Combination of experience in any of the following tools/platforms required:Big Data platform: Hadoop, HDFS, SolrData Ingestion/Wrangling: Kafka, Trifacta, Alteryx, Nifi, Big IntegrateScripting: Python, Spark, Unix, SQLData Warehousing: DataStage, InformaticaDatabases: Teradata, Cassandra, MongoDB, Oracle, SQL ServerCI/CD: GitHub, SubVersion, Jenkins, UrbanCode DeployBI: Cognos, Tableau, PowerBICloud: Azure, AWS, IBM, GoogleExperience with DevOps Toolchain methodologies, including Test Driven Development (TDD), Continuous Integration, and Continuous DeploymentDemonstrated initiative, flexibility, and ability to adapt to changing priorities and work environmentsAzure Cloud computing experience leveraging native Azure products like Azure Data Factory, Azure Databricks, Event Hubs, Azure Data Lake, Blob storage (Preferred)Experience with NoSQL databases like Cassandra (Preferred)', ' Responsible for leveraging cutting edge technology to solve business problems at American Airlines by participating in all phases of the development process. The developer will be a part of a cross functional team building data services and solutions for IT and Business partners.', 'Combination of experience in any of the following tools/platforms required:Big Data platform: Hadoop, HDFS, SolrData Ingestion/Wrangling: Kafka, Trifacta, Alteryx, Nifi, Big IntegrateScripting: Python, Spark, Unix, SQLData Warehousing: DataStage, InformaticaDatabases: Teradata, Cassandra, MongoDB, Oracle, SQL ServerCI/CD: GitHub, SubVersion, Jenkins, UrbanCode DeployBI: Cognos, Tableau, PowerBICloud: Azure, AWS, IBM, Google', 'Designs, develops and reviews the integration of analytics solutions and collaborates with application architects and development teams', 'Additional Benefits: Other great benefits include our Employee Assistance Program, pet insurance and discounts on hotels, cars, cruises and more', 'BI:', 'Cloud:', ""What You'll Get"", 'Demonstrated initiative, flexibility, and ability to adapt to changing priorities and work environments', 'Preferred Qualifications- Education & Prior Job Experience', 'Mentors other team members to make our team crossfunctional', 'Participates in organization wide analytics as needed', 'Health Benefits: On day one, you’ll have access to your health, dental, prescription and vision benefits to help you stay well. And that’s just the start, we also offer virtual doctor visits, flexible spending accounts and more. ', 'Be a key contributor to the data governance, privacy and cataloging processes', 'Scripting:', 'Minimum Qualifications- Education & Prior Job Experience', 'Troubleshoots and debugs complex issues; identifies and implements solutions', '3 years of experience in Agile methods and mindset as an approach to software development', ' This job is a member of the Information Technology Team within the Information Technology Division. Responsible for leveraging cutting edge technology to solve business problems at American Airlines by participating in all phases of the development process. The developer will be a part of a cross functional team building data services and solutions for IT and Business partners.', 'Leads development of coding standards and enforce best practices and security guidelines', '5 years of Software Development experience with expertise in data analytics, warehousing and engineering', 'Databases: Teradata, Cassandra, MongoDB, Oracle, SQL Server', ""Why you'll love this job"", ""Bachelor's degree in Computer Science, Computer Engineering, Technology, Information Systems (CIS/MIS), Engineering or related technical discipline, or equivalent experience/training"", ' This job is a member of the Information Technology Team within the Information Technology Division.', 'Data Warehousing: DataStage, Informatica', 'Experience with DevOps Toolchain methodologies, including Test Driven Development (TDD), Continuous Integration, and Continuous Deployment', 'Azure Cloud computing experience leveraging native Azure products like Azure Data Factory, Azure Databricks, Event Hubs, Azure Data Lake, Blob storage (Preferred)', 'Wellness Programs: We want you to be the best version of yourself - that’s why our wellness programs provide you with all the right tools, resources and support you need.', 'Airline Industry experience', '401(k) Program: Available upon hire and, depending on the workgroup, employer contributions to your 401(k) program are available after one year.', 'BI: Cognos, Tableau, PowerBI', 'Feel Free to be yourself at American', 'Partners with Data Scientists to deliver critical business initiatives using Artificial Intelligence, Machine Learning tools, etc. ', 'Data Ingestion/Wrangling: Kafka, Trifacta, Alteryx, Nifi, Big Integrate', 'Travel Perks: Ready to explore the world? You, your family and your friends can reach 365 destinations on more than 6,800 daily flights across our global network.Health Benefits: On day one, you’ll have access to your health, dental, prescription and vision benefits to help you stay well. And that’s just the start, we also offer virtual doctor visits, flexible spending accounts and more. Wellness Programs: We want you to be the best version of yourself - that’s why our wellness programs provide you with all the right tools, resources and support you need.401(k) Program: Available upon hire and, depending on the workgroup, employer contributions to your 401(k) program are available after one year.Additional Benefits: Other great benefits include our Employee Assistance Program, pet insurance and discounts on hotels, cars, cruises and more']",Not Applicable,Full-time,Information Technology,Airlines/Aviation,2021-03-24 13:05:10
Data Engineer,Tata Consultancy Services,"Charlotte, NC",1 week ago,Be among the first 25 applicants,[''],Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer - Enterprise Data Analytics Platform,GSK,"Philadelphia, PA",5 days ago,Be among the first 25 applicants,"['', 'Leverage reusable code modules to solve problems across the team and organization', ""This is a job description to aide in the job posting but does not include all job evaluation details.If you require an accommodation or other assistance to apply for a job at GSK, please contact the GSK Service Centre at 1-877-694-7547 (US Toll Free) or +1 801 567 5155 (outside US).GSK is an Equal Opportunity Employer and, in the US, we adhere to Affirmative Action principles. This ensures that all qualified applicants will receive equal consideration for employment without regard to race, color, national origin, religion, sex, pregnancy, marital status, sexual orientation, gender identity/expression, age, disability, genetic information, military service, covered/protected veteran status or any other federal, state or local protected class.Important notice to Employment businesses/ AgenciesGSK does not accept referrals from employment businesses and/or employment agencies in respect of the vacancies posted on this site. All employment businesses/agencies are required to contact GSK's commercial and general procurement/human resources department to obtain prior written authorization before referring any candidates to GSK. The obtaining of prior written authorization is a condition precedent to any agreement (verbal or written) between the employment business/ agency and GSK. In the absence of such written authorization being obtained any actions undertaken by the employment business/agency shall be deemed to have been performed without the consent or contractual agreement of GSK. GSK shall therefore not be liable for any fees arising from such actions or any fees arising from any referrals by employment businesses/agencies in respect of the vacancies posted on this site.Please note that if you are a US Licensed Healthcare Professional or Healthcare Professional as defined by the laws of the state issuing your license, GSK may be required to capture and report expenses GSK incurs, on your behalf, in the event you are afforded an interview for employment. This capture of applicable transfers of value is necessary to ensure GSK’s compliance to all federal and state US Transparency requirements. For more information, please visit GSK’s Transparency Reporting For the Record site."", 'Bachelor’s Degree', 'Understanding of IAM and authorization (RBAC and ABAC)', ""These include Patient focus, Transparency, Respect, Integrity along with Courage, Accountability, Development, and Teamwork.Operating at pace and agile decision-making – using evidence and applying judgement to balance pace, rigour and risk.Committed to delivering high quality results, overcoming challenges, focusing on what matters, execution.Continuously looking for opportunities to learn, build skills and share learning.Sustaining energy and well-being.Building strong relationships and collaboration, honest and open conversations.Budgeting and cost-consciousnessThis is a job description to aide in the job posting but does not include all job evaluation details.If you require an accommodation or other assistance to apply for a job at GSK, please contact the GSK Service Centre at 1-877-694-7547 (US Toll Free) or +1 801 567 5155 (outside US).GSK is an Equal Opportunity Employer and, in the US, we adhere to Affirmative Action principles. This ensures that all qualified applicants will receive equal consideration for employment without regard to race, color, national origin, religion, sex, pregnancy, marital status, sexual orientation, gender identity/expression, age, disability, genetic information, military service, covered/protected veteran status or any other federal, state or local protected class.Important notice to Employment businesses/ AgenciesGSK does not accept referrals from employment businesses and/or employment agencies in respect of the vacancies posted on this site. All employment businesses/agencies are required to contact GSK's commercial and general procurement/human resources department to obtain prior written authorization before referring any candidates to GSK. The obtaining of prior written authorization is a condition precedent to any agreement (verbal or written) between the employment business/ agency and GSK. In the absence of such written authorization being obtained any actions undertaken by the employment business/agency shall be deemed to have been performed without the consent or contractual agreement of GSK. GSK shall therefore not be liable for any fees arising from such actions or any fees arising from any referrals by employment businesses/agencies in respect of the vacancies posted on this site.Please note that if you are a US Licensed Healthcare Professional or Healthcare Professional as defined by the laws of the state issuing your license, GSK may be required to capture and report expenses GSK incurs, on your behalf, in the event you are afforded an interview for employment. This capture of applicable transfers of value is necessary to ensure GSK’s compliance to all federal and state US Transparency requirements. For more information, please visit GSK’s Transparency Reporting For the Record site."", 'Continuously looking for opportunities to learn, build skills and share learning.', 'Site Name:', 'Why you?', 'Understanding of container technologies (Docker/Kubernetes), streaming technologies (Kafka), and automated testing patterns and tools a plus', 'Preferred Qualifications', '1+ years experience with UNIX/Linux including basic commands and shell scripting', '1+ years experience in at least one scripting language (Python, Perl, JavaScript, Shell)', 'Experience in industry, i.e. code committer, published, white paper, etc.', 'Experience building and operating in a regulated environment (GxP, Sarbanes-Oxley)', 'Are you looking to expand your expertise as a Data Engineer within a global environment that allows you to keep pace with the speed of change?\u202f At GSK, we are transforming how new technologies are used to improve performance across the organization.', 'Develop sustainable data driven solutions with new data technologies to meet the needs of our organization and business customersBuild robust end-to-end systems with an eye on the long-term maintenance and support of the applicationLeverage reusable code modules to solve problems across the team and organizationHandle multiple functions / roles for the projects / Agile teamsWork with established standards across the team and organizationUnderstand complex multi-tier, multi-platform systemsContribute to building a framework of a significant complexityWork with internal team of data engineers (both full-time associates and/or third-party resources)', 'Contribute to building a framework of a significant complexity', 'Build robust end-to-end systems with an eye on the long-term maintenance and support of the application', 'Work with established standards across the team and organization', '1 year of experience in big data technologies (Cassandra, HBase, Spark, Hadoop, HDFS, AVRO, MongoDB, Zookeeper)', 'Bachelor’s Degree1-2 years coding or at least 1-2-year(s) experience data warehousing or in unstructured data environments1 year of experience in Azure cloud technologies and/or other cloud data platform like AWS, MapR, Cloudera, Google Cloud1 year of experience in big data technologies (Cassandra, HBase, Spark, Hadoop, HDFS, AVRO, MongoDB, Zookeeper)', 'Develop sustainable data driven solutions with new data technologies to meet the needs of our organization and business customers', '2+ years experience with NoSQL implementation (Mongo, Cassandra, etc. a plus)', '1+ years experience developing software solutions to solve complex business problems', 'Why GSK?', '1+ years experience designing, developing, and implementing ETL', 'Understand complex multi-tier, multi-platform systems', '1+ years experience with Relational Database Systems and SQL', 'Committed to delivering high quality results, overcoming challenges, focusing on what matters, execution.', 'Sustaining energy and well-being.', 'This role will provide YOU the opportunity to lead key activities to progress YOUR career, these responsibilities include some of the following…', 'Posted Date:', 'If you have the following characteristics, it would be a plus:', ""Master's Degree in fields such as Computer Science, Computer Engineering, Data Science, related discipline or military experienceAgile experience is preferredUnderstanding of data ingestion and preparation processes.\u202f\u202f Experience in semantic data store (Azure SQL DW/Snowflake, Cosmos DB), PowerBI, Azure Databricks, Data Ingestion/Acquisition/Prep (Azure ADF, Talend) are desirable.Experience building and operating in a regulated environment (GxP, Sarbanes-Oxley)Experience with API and Microservices technologies/architectureUnderstanding of IAM and authorization (RBAC and ABAC)Understanding of container technologies (Docker/Kubernetes), streaming technologies (Kafka), and automated testing patterns and tools a plus2+ years experience with NoSQL implementation (Mongo, Cassandra, etc. a plus)1+ years experience in at least one scripting language (Python, Perl, JavaScript, Shell)1+ years experience developing software solutions to solve complex business problems1+ years experience with Relational Database Systems and SQL1+ years experience designing, developing, and implementing ETL1+ years experience with UNIX/Linux including basic commands and shell scriptingExperience in industry, i.e. code committer, published, white paper, etc."", 'Building strong relationships and collaboration, honest and open conversations.', 'Operating at pace and agile decision-making – using evidence and applying judgement to balance pace, rigour and risk.', 'Handle multiple functions / roles for the projects / Agile teams', 'Understanding of data ingestion and preparation processes.\u202f\u202f Experience in semantic data store (Azure SQL DW/Snowflake, Cosmos DB), PowerBI, Azure Databricks, Data Ingestion/Acquisition/Prep (Azure ADF, Talend) are desirable.', 'Budgeting and cost-consciousness', 'We are looking for professionals with these required skills to achieve our goals:', 'Work with internal team of data engineers (both full-time associates and/or third-party resources)', '1 year of experience in Azure cloud technologies and/or other cloud data platform like AWS, MapR, Cloudera, Google Cloud', ""Master's Degree in fields such as Computer Science, Computer Engineering, Data Science, related discipline or military experience"", '1-2 years coding or at least 1-2-year(s) experience data warehousing or in unstructured data environments', 'Experience with API and Microservices technologies/architecture', 'Basic Qualifications', 'Our values and expectations are at the heart of everything we do and form an important part of our culture.', 'Operating at pace and agile decision-making – using evidence and applying judgement to balance pace, rigour and risk.Committed to delivering high quality results, overcoming challenges, focusing on what matters, execution.Continuously looking for opportunities to learn, build skills and share learning.Sustaining energy and well-being.Building strong relationships and collaboration, honest and open conversations.Budgeting and cost-consciousness', 'Agile experience is preferred', 'As GSK Focuses On Our Values And Expectations And a Culture Of Innovation, Performance, And Trust, The Successful Candidate Will Demonstrate The Following Capabilities']",Not Applicable,Full-time,Strategy/Planning,Pharmaceuticals,2021-03-24 13:05:10
Data Engineer (Data Insights and Governance),Snap Inc.,"Los Angeles, CA",6 days ago,71 applicants,"['', ' Hands on experience with Google BigQuery ', ' Drive adoption of the data sets you’ve produced ', ' BS/BA degree in Computer Science, Math, Physics, or a related field, or equivalent years of experience in a relevant field  3+ year experience in SQL or similar languages  3+ years development experience in at least one object-oriented or scripting language (Python, Java, Scala, etc) ', ' Work closely with stakeholders in engineering, finance, sales, marketing, strategy, and governance to make high quality datasets available to consumers in a timely manner ', ' 3+ years development experience in at least one object-oriented or scripting language (Python, Java, Scala, etc) ', 'Minimum Qualifications', ' Experience in version control systems such as Git ', ' Experience in building data pipelines to serve reporting needs ', 'Preferred Qualifications', ' Experience owning all or part of a team roadmap ', ' Ability to prioritize requests from multiple stakeholders in disparate domains ', ' BS/BA degree in Computer Science, Math, Physics, or a related field, or equivalent years of experience in a relevant field ', ' Experience in building data pipelines to serve reporting needs  Experience owning all or part of a team roadmap  Ability to prioritize requests from multiple stakeholders in disparate domains  Ability to effectively communicate complex projects to non-technical stakeholders ', ' Become familiar with our data consumption portals and their capabilities ', ' Work closely with stakeholders in engineering, finance, sales, marketing, strategy, and governance to make high quality datasets available to consumers in a timely manner  Develop data pipelines adhering with privacy and governance principles  Become familiar with our data consumption portals and their capabilities  Build expertise and ownership of data quality for supported domains  Build tooling and implement systems to overcome limitations of the data consumption portals when appropriate  Drive adoption of the data sets you’ve produced ', ' Experience with Airflow ', ' Experience leading a small team of data or software engineers ', ' Ability to effectively communicate complex projects to non-technical stakeholders ', ' Data architecture and warehousing experience ', 'What You’ll Do', ' Develop data pipelines adhering with privacy and governance principles ', ' Build tooling and implement systems to overcome limitations of the data consumption portals when appropriate ', ' Experience in ETL / Data application development ', ' accommodations-ext@snap.com ', ' Build expertise and ownership of data quality for supported domains ', 'Knowledge, Skills & Abilities', ' Hands on experience with Google BigQuery  Experience in version control systems such as Git  Data architecture and warehousing experience  Experience leading a small team of data or software engineers  Experience with Airflow  Experience in ETL / Data application development ', ' 3+ year experience in SQL or similar languages ']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer,Accenture,"Florham Park, NJ",7 days ago,Be among the first 25 applicants,"['', 'Understand and used Object Orientated design techniques', 'Data & Analytics Transformation (current state assessment, strategy development, value case, roadmap, and blueprint)', 'Experience using TDD and unit testing as part of normal software development using packages such as Postman, Jest, JUnit, PyUnit, Swagger, etc.', 'Create a value chain to help address the challenges of acquiring data, evaluating its value, distilling & analyzing ', 'Data architecture (Understanding logical ways of organizing and analyzing data and how this affects building databases, APIs, and UIs)', 'Experience in data migration from on-prem to cloud. Multi-Cloud experience - AWS/Azure/Google a plus', 'Process Automation, Machine Learning, and Artificial Intelligence practices (knowledge of how advancing digital tools and techniques are applied in enterprise data and analytics strategies and roadmaps)', 'Basic Qualifications:', 'Business Translator (identifying business problem, initiative, analytics intervention, data science management, data science interpretation, storytelling)', 'Work in an agile CI/CD environment, (Jenkins/Ansible experience a plus)', 'Minimum of 3 years of combined data, analytics and strategic consulting experience', 'Minimum of two years of experience in one or more of the following areas: ', 'Database design and performance optimization with multiple databases (relational data stores - RDS, Aurora; NoSQL data stores - DynamoDB; data warehouses -Teradata, Redshift, snowflake; graph databases)', 'Experience with containerization platforms a plus, such as Docker, Kubernetes ', 'For now, all Accenture business travel, international and domestic, is currently restricted to client-essential sales/delivery activity only.Please note: The safety and well-being of our people continues to be the top priority, and our decisions around travel are informed by government COVID-19 response directives, recommendations from leading health authorities and guidance from a number of infectious disease experts.', 'Infrastructure as Code (Terraform or similar technology)', 'Minimum of 3 years of hands-on experience on one or more of these technologies -Python, Scala, Spark, PySpark', 'Lead data modeling activities to capture and model data requirements, business rules, and logical and physical models', '2 years of Experience writing REST or GraphQL APIs', 'A Bachelor’s Degree or equivalent work experience (12 years) or an Associate’s Degree with 6 years of work experience', 'Experience working with large data sets in distributed data environments ', 'Preferred Qualifications:']",Mid-Senior level,Full-time,Strategy/Planning,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Averity,Miami-Fort Lauderdale Area,,N/A,"['', 'Full Medical, Dental and Vision Benefits', 'What Skills Do You Need?', 'Spark/Databrick', 'AWS- S3, EMR, Lambda', '$135,000 - $155,00035% BonusFull Medical, Dental and Vision Benefits', 'Very strong Data Warehousing concepts', '$135,000 - $155,000', 'This is an opportunity where you can come in and work with some brilliant engineers. Most importantly we offer stability a ton of room for growth within our organization. Plus we are located in Miami Florida!', 'Python ETL developmentSpark/DatabrickAWS- S3, EMR, LambdaVery strong Data Warehousing conceptsDeep dive into the code and can understand beyond a basic scopeYou bring high energy and outgoing personality', '35% Bonus', 'What’s in it for you?', 'Compensation?', '\xa0', 'Deep dive into the code and can understand beyond a basic scope', 'Python ETL development', 'You bring high energy and outgoing personality', ""As a Data Engineer you will be building out existing ETL pipelines and building new ones from scratch. You will be responsible for the data warehousing and creating AWS architecture and functions. Most importantly you have a passion for data and the ability to dive deep into code. You don't just understand the concepts of the tools but have a firm understanding of how to use them.\xa0"", 'What is the Job?', ""We are a Canadian-American multinational fast food holding company formed in 2014. We are one of the\xa0largest operator's of fast food restaurants in the world and\xa0a household name worldwide worth over $40 Billion. We own and operate thousands of restaurants in hundreds of countries, and are now growing our e-commerce / direct to consumer brand. We are looking to hire 4-6 Engineers from Mid-Level to senior level."", 'Who Are We?']",Mid-Senior level,Full-time,Engineering,Computer Software,2021-03-24 13:05:10
Data Engineer,DPR Construction,"Richmond, VA",1 week ago,Be among the first 25 applicants,"['', 'Secure the movement of sensitive information in a manner consistent with company policy and management expectations', ' to success in this role', 'Excellent knowledge of EL and ELT, Datawarehousing, and cloud-based tools', 'Identify, design, and implement internal process improvements, automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.', 'Growth and Development – Know or learn what is needed to deliver results and successfully compete', 'Define and lead API integration strategies and for the enterprise', 'Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.', 'Experience in scripting languages like Batch, Shell in Unix environment', 'Ability to understand, consume and use API’s, JSON, Webservices for Data pipelines.', 'Ability to work effectively with others who are in remote locations and varying time zones', 'Assemble large, complex data sets that meet functional / non-functional business requirements.', 'Ability to adapt quickly to change & deep curiosity to learn new tools and technologies and apply themAbility to work with and collaborate across the team and work effectively with others to identify the impact on the company’s business processes, other applications, network, etc.Strong analytical and problem-solving abilities.Seek and Embrace Change – Continuously improve work processes rather than accepting the status quoGrowth and Development – Know or learn what is needed to deliver results and successfully competeAbility to work effectively with others who are in remote locations and varying time zonesResourceful creative approach to problem-solving is expectedStrong communication skills, with the ability to work both independently and in project teamsMotivation to continually learn and take on added responsibilities while maintaining a positive attitude', ' DPR has been nationally recognized for its strong company culture, based on a well-defined purpose “We Exist to Build Great Things,” and four core values: integrity, enjoyment, uniqueness and ever forward. A flat, title-less organization that empowers people at all levels to make decisions, DPR ranked on FORTUNE’s “100 Best Companies to Work For” list for five consecutive years. For more information, visit http://www.dpr.com .', 'Control integration quality and develop ways to detect and correct anomalies with data exchange', 'Ability to work with and collaborate across the team and work effectively with others to identify the impact on the company’s business processes, other applications, network, etc.', 'Motivation to continually learn and take on added responsibilities while maintaining a positive attitude', 'Create and maintain optimal data pipeline architectureAssemble large, complex data sets that meet functional / non-functional business requirements.Enable data access, data processing, and data products by architecting, maintaining, scaling, monitoring and securing Data Warehouse, EL & ETL system, and data pipelines and BI systemsIdentify, design, and implement internal process improvements, automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.Keep our data separated and secureCreate data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.Work with data and analytics experts to strive for greater functionality in our data systems.Define and lead API integration strategies and for the enterpriseImplement enterprise integrations that result in a scalable, flexible, and highly available solutions that perform well under high trafficSecure the movement of sensitive information in a manner consistent with company policy and management expectationsControl integration quality and develop ways to detect and correct anomalies with data exchange', 'Seek and Embrace Change – Continuously improve work processes rather than accepting the status quo', 'Strong with SQL development knowledge for Relational Databases', 'Experience in Data Mapping, XML/JSON, and web service', 'Strong communication skills, with the ability to work both independently and in project teams', 'Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.', 'Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.', 'Business and Technical Analysis skills', 'Experience in Software development.', 'Solid understanding of database engineering and design (Relational, De-normalized, Data Lakes, etc.)Knowledge of AWS and Azure platformsExperience in Software development.Ability to understand, consume and use API’s, JSON, Webservices for Data pipelines.Excellent knowledge of EL and ELT, Datawarehousing, and cloud-based toolsStrong with SQL development knowledge for Relational DatabasesBusiness and Technical Analysis skillsExperience in scripting languages like Batch, Shell in Unix environmentExperience with integration of data from multiple data sources like API’s, JSON and any other databases, Flat-files, Spreadsheets.Experience in Data Mapping, XML/JSON, and web service', 'Responsibilities', 'Ability to adapt quickly to change & deep curiosity to learn new tools and technologies and apply them', 'Implement enterprise integrations that result in a scalable, flexible, and highly available solutions that perform well under high traffic', 'Enable data access, data processing, and data products by architecting, maintaining, scaling, monitoring and securing Data Warehouse, EL & ETL system, and data pipelines and BI systems', 'Create and maintain optimal data pipeline architecture', 'Qualifications', 'Key', 'Solid understanding of database engineering and design (Relational, De-normalized, Data Lakes, etc.)', 'Work with data and analytics experts to strive for greater functionality in our data systems.', 'Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.', 'Keep our data separated and secure', 'Resourceful creative approach to problem-solving is expected', 'Experience with integration of data from multiple data sources like API’s, JSON and any other databases, Flat-files, Spreadsheets.', 'Job Description', 'Knowledge of AWS and Azure platforms', 'Position Summary', 'Strong analytical and problem-solving abilities.']",Entry level,Full-time,Information Technology,Construction,2021-03-24 13:05:10
Data Engineer (No C2C OR H1B),DISYS,"New York, United States",,N/A,"['', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Healthcare Experience', 'Ability to:', 'POSITION PURPOSE:', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Organization and planning', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Act as a subject matter expert in healthcare data', 'Must Have', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Data Vault 2.0', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Data Mart design, architecture, and construction', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Apply logical data structures to business processes.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Knowledge of JavaScript, Ajax, Web parts, XML (XSD/XSLT), and ASP.NET', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0MDX and T-SQL Languages', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Sound decision making and problem solving.', 'QUALIFICATIONS: (Minimum qualifications\xa0required\xa0for the job)\xa0', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience:\xa0Five years in requirements gathering, design, development, and support of business intelligence (BI) solutions', 'Minimum qualifications\xa0', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Education:\xa0\xa0Bachelor’s Degree in Information Technology/Computer Science, or Computer Engineering; Master’s degree or emphasis on Data Management preferred.\xa0', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0TFS\xa0', 'Experience:\xa0', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0VB.Net', 'Technology Stack Knowledge:', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Microsoft Certification (MCP, MCSE)', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Java', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Requirements gathering in a BI setting.', 'Experience with data transformations and building enterprise data warehouses are the key requirements, as well as MS SQL (and SSIS) proficiency.', 'for the job)', 'QUALIFICATIONS: (', '\xa0', '\ufeffSkills:', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0High level of initiative', 'Knowledge, Skills and Abilities (KSAs):', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0WhereScape', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Knowledge of data warehouse methodologies', 'DISYS is looking for a 100% REMOTE Data Engineer for one of our direct clients based in Boise, Idaho.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Model data structures to optimize reporting and represent appropriate business rules.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience with data visualization solutions', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0GitHub source code management', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Utilize critical thinking skills.', 'Education:\xa0\xa0', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience supporting and designing ETL processes.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Project management', 'DISYS is interested in every candidate but at this time our client is not sponsoring H1B and will not be engaged in C2C.', 'required\xa0', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Talend MDM', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Self-directed while acting as an enabling, collaborative team member.\xa0', 'Assist the existing Health Data Services design, development, implementation, documentation, and support of Business Intelligence solutions.\xa0\xa0Provide technical leadership and consulting for business users and IS professionals on the design, development, and utilization of BCI’s analytical tools, technologies, and processes.\xa0', 'Preferred Qualifications:', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Strong written and verbal communication', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Tableau visualization', 'Knowledge of:', '\xa0There are “nice to haves” with healthcare/PHI experience, Kimball model experience and tools like Wherescape,', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer - Remote - 125K,Jefferson Frank,"Fairfax, VT",1 week ago,Be among the first 25 applicants,"['', 'Skills', ' 135 employees', ' HQ in Fairfax, VA. They are remote now but want the candidates in-office once they return', ' Up to 10% bonus for each role, to be paid out quarterly', ' 4+ yrs exp AWS experience required (Redshift required) Extensive Python exp Experience creating Data Pipelines & Data Lakes Data experience in Hadoop, Spark, HIVE Kubernetes/Kubernetes tools like Helm (Docker experience is OK) SQL, (Postgres, MySQL) APIs exp a plus', ' Never stopped hiring during COVID-19, very stable', ' Up to 10% bonus for each role, to be paid out quarterly 80% health, dental & vision coverage for individual AND families 401k - 100% match up to 6%. Fully vested after first year Unlimited sick leave, 3 weeks PTO', 'About My Client', ' SQL, (Postgres, MySQL)', ' 135 employees HQ in Fairfax, VA. They are remote now but want the candidates in-office once they return They are a health technology company that uses evidence-based data to solve health challenges. They come up with research-related products that can be utilized in the health care field employees', ' Data experience in Hadoop, Spark, HIVE', ' Unlimited sick leave, 3 weeks PTO', ' Kubernetes/Kubernetes tools like Helm (Docker experience is OK)', ' Solid mission, VERY strong funding Never stopped hiring during COVID-19, very stable Really strong benefits', ' 4+ yrs exp', ' APIs exp a plus', ' 401k - 100% match up to 6%. Fully vested after first year', ' AWS experience required (Redshift required)', ' They are a health technology company that uses evidence-based data to solve health challenges. They come up with research-related products that can be utilized in the health care field employees', ' Solid mission, VERY strong funding', ' Really strong benefits', ' Experience creating Data Pipelines & Data Lakes', ' 80% health, dental & vision coverage for individual AND families', 'Benefits', ' Extensive Python exp']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Fluence,San Francisco Bay Area,2 weeks ago,154 applicants,"['', 'Kotlin\xa0\xa0Python\xa0\xa0React,\xa0Redux\xa0\xa0Postgres\xa0\xa0Cassandra\xa0Redis\xa0\xa0Kubernetes\xa0AWS\xa0', 'Passionate about learning, and tackling new and exciting technical\xa0challenges\xa0\xa0', 'React,\xa0Redux\xa0\xa0', 'Enjoys pair\xa0programming\xa0\xa0', 'Fluence, a Siemens and AES company, is the global market leader in energy storage technology solutions and services, combining the agility of a technology company with the expertise, vision and financial backing of two well-established and respected industry giants. Building on the pioneering work of AES Energy Storage and Siemens energy storage, our goal is to create a more sustainable future by transforming the way we power our world. Providing design, delivery and integration, Fluence offers proven energy storage technology solutions that address the diverse needs and challenges of customers in a rapidly transforming energy landscape. ', 'Leading ', '\xa05+ years of professional software experience\xa0\xa0Proven ability to meet deadlines and deliver solutions quickly at high\xa0quality\xa0\xa0Expert knowledge of database languages, data flow architectures and modeling techniques\xa0\xa0Passionate about learning, and tackling new and exciting technical\xa0challenges\xa0\xa0Strong computer science fundamentals, including knowledge of data structures and\xa0algorithms\xa0\xa0Organized and detail-oriented, able to work well under deadlines in a changing environment and perform multiple tasks effectively and\xa0concurrently\xa0\xa0Experience with AWS or other cloud-based development\xa0\xa0Enjoys pair\xa0programming\xa0\xa0Previous experience working at a\xa0startup\xa0\xa0', '\xa05+ years of professional software experience\xa0\xa0', 'Strong computer science fundamentals, including knowledge of data structures and\xa0algorithms\xa0\xa0', 'Previous experience working at a\xa0startup\xa0\xa0', 'ABOUT FLUENCE', 'Build, test, scale, and refine\xa0data ingestion, warehousing and analytical components across our energy market\xa0platforms', 'Proven ability to meet deadlines and deliver solutions quickly at high\xa0quality\xa0\xa0', 'Experience with AWS or other cloud-based development\xa0\xa0', 'Fluence currently has more than 2.4 gigawatts of projects in operation or awarded across 24 countries and territories worldwide. We topped the Navigant Research utility-scale energy storage leaderboard in 2018 and were named one of Fast Company’s Most Innovative Companies in 2019. In 2020, our sixth-generation Tech Stack won Commercial Technology of the Year at the 22nd annual S&P Global Platts Global Energy Awards.', 'AWS\xa0', 'Be an owner of what you build, managing the entire product\xa0lifecycle\xa0\xa0', 'Fluence IS AN EQUAL OPPORTUNITY EMPLOYER and fully subscribes to the principles of Equal Employment Opportunity to ensure that all applicants and employees are considered for hire, promotion, and job status without regard to race, color, religion, sex, national origin, age, disability, veteran status, sexual orientation, marital or familial status.', 'GET IN TOUCH', 'Please send your resume and cover letter to careers@fluenceenergy.com.', 'Location: San Francisco, CA, or continental US', 'Working on transforming a fundamental part of our society is exciting and fulfilling. It requires creativity, diversity of ideas and backgrounds, and building trust to effect change and move with speed. We respect our coworkers and customers. We listen to what others have to say, and we are inclusive. Technologies you will likely use in the role:', 'Cassandra\xa0', '\xa0', 'Do others come to you for your subject matter expertise? Are you excited by the challenge of working in a start-up atmosphere with a purpose? ', 'Organized and detail-oriented, able to work well under deadlines in a changing environment and perform multiple tasks effectively and\xa0concurrently\xa0\xa0', 'Kotlin\xa0\xa0', 'Fun', 'Fluence is defined by its unwavering commitment to safety, quality, and integrity. We take personal ownership in what we do, developing trust in our relationships with internal and external stakeholders. We firmly believe in having honest, forthcoming, and fair communications. You will work within the growing Software Development team, along-side data scientists, product managers, and subject matter experts to shape the evolution of our company and the future of the electricity grid.\xa0In this role you will: ', 'Postgres\xa0\xa0', 'Data Engineer ', 'Tackle a wide variety of challenges throughout the stack and contribute to all parts of our code\xa0base\xa0\xa0', 'Redis\xa0\xa0', 'Kubernetes\xa0', 'Be a team player in a small, flat-structured, highly collaborative\xa0environment\xa0\xa0', 'Expert knowledge of database languages, data flow architectures and modeling techniques\xa0\xa0', 'Python\xa0\xa0', 'Here at Fluence, we strive to continuously improve, be intellectually curious and be adaptive to our customers and employee’s needs. Collaboration is key, both in our partnerships with our customers, and with each other. Fluence prioritizes the most critical efforts that allow for the greatest impact. To be successful in this role you have:', 'Build, test, scale, and refine\xa0data ingestion, warehousing and analytical components across our energy market\xa0platformsBe an owner of what you build, managing the entire product\xa0lifecycle\xa0\xa0Tackle a wide variety of challenges throughout the stack and contribute to all parts of our code\xa0base\xa0\xa0Be a team player in a small, flat-structured, highly collaborative\xa0environment\xa0\xa0', 'Responsible', 'This\xa0Data\xa0engineer\xa0will be a part of the new Fluence Digital business unit, formed following Fluence’s acquisition of San Francisco-based start-up AMS. Fluence Digital’s technology uses artificial intelligence, advanced price forecasting, portfolio optimization and market bidding to ensure energy storage and flexible generation assets are responding optimally to price signals sent by the market.\xa0', 'Agile']",Mid-Senior level,Full-time,Information Technology,Renewables & Environment,2021-03-24 13:05:10
Data Engineer,Radancy,"Atlanta, GA",1 week ago,Be among the first 25 applicants,"['', ' Build and maintain ETL pipelines utilizing Python that connect 1st and 3rd party data  Work with Cloud Computing Platforms (GCP/AWS), Luigi, Kafka and other open-source technologies  Conduct data modeling, schema design, and SQL development  Ingest and aggregate data from both internal and external data sources to build our world class datasets  Develop and lead the testing and fixing of new or enhanced solutions for data products and reports, including automating ETL testing  Collaborate with Product Owner and domain experts to recognize and help adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation  Assist with the development and review of technical and end user documentation including ETL workflows, research, and data analysis  Work with Product team to define data collection and engineering frameworks  Build monitoring dashboards and automate data quality testing  Responsible for daily integrity checks, performing deployments and releases  Own meaningful parts of our service, have an impact, grow with the company ', ' Build monitoring dashboards and automate data quality testing ', ' Exposure to front end development: HTML, JavaScript, jQuery, Angular or similar libraries ', ' Familiarity with C#, .Net, Kafka, Docker ', ' Exposure / familiarity with Google Cloud Platform / BigQuery / Amazon Redshift ', ' Collaborate with Product Owner and domain experts to recognize and help adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation ', ' Enthusiastic about working with and exploring new data sets ', ' The team has extensive experience in ETL development, works with large scale data in real time, and cross collaborates with other engineering teams across the organization. ', 'The Team', 'Overview', ' Work with Product team to define data collection and engineering frameworks ', 'Flexible Location (Remote Hubs): ', ' Build and maintain ETL pipelines utilizing Python that connect 1st and 3rd party data ', 'About The Job', 'Desired Technical Qualifications', ' Responsible for daily integrity checks, performing deployments and releases ', ' Work with Cloud Computing Platforms (GCP/AWS), Luigi, Kafka and other open-source technologies ', ' Product / reporting suite experience ', ' Assist with the development and review of technical and end user documentation including ETL workflows, research, and data analysis ', ' AdTech experience preferred ', ' Radancy Data Engineering works on data services across product organizations within Radancy, and supports building a customer facing data visualization product. The Data Engineering team supports an enterprise grade recruitment platform focusing on talent acquisition and job opportunity exploration.  The team has extensive experience in ETL development, works with large scale data in real time, and cross collaborates with other engineering teams across the organization. ', ' Bachelors or Masters degree in Computer Science or other related field ', ' Conduct data modeling, schema design, and SQL development ', ' Ingest and aggregate data from both internal and external data sources to build our world class datasets ', ' Detail oriented and strong communicator ', ' 2+ years of Python, SQL, and ETL development  Bachelors or Masters degree in Computer Science or other related field  Product / reporting suite experience  Familiarity with C#, .Net, Kafka, Docker  Exposure to front end development: HTML, JavaScript, jQuery, Angular or similar libraries  Exposure / familiarity with Google Cloud Platform / BigQuery / Amazon Redshift  AdTech experience preferred  Enthusiastic about working with and exploring new data sets  Detail oriented and strong communicator ', ' Radancy', ' Radancy Data Engineering works on data services across product organizations within Radancy, and supports building a customer facing data visualization product. The Data Engineering team supports an enterprise grade recruitment platform focusing on talent acquisition and job opportunity exploration. ', ' Own meaningful parts of our service, have an impact, grow with the company ', ' 2+ years of Python, SQL, and ETL development ', ' Develop and lead the testing and fixing of new or enhanced solutions for data products and reports, including automating ETL testing ']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
"Data Engineer (Python, ETL)",Maania Consultancy Services,"Reston, VA",5 days ago,Be among the first 25 applicants,"['', ' Exploratory data analysis (EDA), ETL and data processing using Python and Pandas', 'Required Skills:', ' Virtualization systems (e.g., Docker).', 'Desired Skills:', ' Developing and Maintaining PostgreSQL databases', ' Experience with data', ' Experience with tactical data communications.', ' Experience with machine learning', 'Eligible for Security Clearance', ' Developing and operating data processing systems']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer Intern - Data Science,LinkedIn,"Sunnyvale, CA",3 weeks ago,Over 200 applicants,[''],Internship,Internship,Engineering,Internet,2021-03-24 13:05:10
Remote Data Engineer Lead ,Vaco,"Phoenix, AZ",2 weeks ago,57 applicants,"['', 'Command in big data tech stack - Apache Kafka, Hadoop', 'Collaborate with Applications and Security Teams', 'Strong communication and analytical and problem-solving skills', ' ', 'Experience working in Azure environment', 'Monitor application performance and operations', '6+ years of experience in Data Engineering, Data WarehousingHands on experience building big data pipelines using Python, Apache Spark, Apache AirflowCommand in big data tech stack - Apache Kafka, HadoopExperience working in Azure environmentExperience with Azure DevOps is requiredExperience with delta lake is a plusExperience with SQL Server is a plusStrong communication and analytical and problem-solving skills', '6+ years of experience in Data Engineering, Data Warehousing', 'Experience with delta lake is a plus', 'Work with Security/Infrastructure Architects and Data Engineers to implement security policies', 'We are seeking a Senior Data Engineer to help build data infrastructure and data pipelines that power business data. Ideal candidate is expected to operate within a distributed, agile, cross-functional environment. This is an opportunity to create an enterprise-wide impact by providing normalized data to all stakeholders and downstream systems. Candidate is responsible for the ETL/ELT processes, following architecture guidelines, reliability, accuracy, monitoring, and infrastructure surrounding internal data processing.', 'Experience with SQL Server is a plus', ""Design implement and champion the tooling required for data ingestion, transformation and orchestration.Collaborate with Applications and Security TeamsWork with Data Analysts, Business SME's in data modelling effortsWork with Security/Infrastructure Architects and Data Engineers to implement security policiesCollaborate with Application teams to ingest change data streamsPerform code review and debug in case of process failures or data discrepanciesMonitor application performance and operations"", 'Perform code review and debug in case of process failures or data discrepancies', 'Collaborate with Application teams to ingest change data streams', 'Design implement and champion the tooling required for data ingestion, transformation and orchestration.', 'Experience with Azure DevOps is required', 'Hands on experience building big data pipelines using Python, Apache Spark, Apache Airflow', ""Work with Data Analysts, Business SME's in data modelling efforts""]",Associate,Contract,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Senior Data Engineer (Remote),Glassdoor,"Austin, TX",4 weeks ago,Be among the first 25 applicants,"['', 'Background in Scrum/Agile development methodologies.', ' Work with purpose – join us in creating transparency for job seekers everywhere 100% company paid medical/dental/vision/life coverage, with 80% dependent coverage Long Term Incentive Plan  401(k) Plan with a Company Match to prepare for your future Generous paid holidays and open paid time off  ', 'Experience working with Tableau, Looker or other data visualization software.', 'Employee feedback transparency: ', 'Experience building customer facing products, machine learning pipelines or data products.', '5+ years of hands-on experience with developing data warehouse solutions and data products.', 'Capable of delivering on multiple competing priorities with little supervision.', 'Our Commitments', 'Nice To Have', 'Diversity & Inclusion transparency: We are committed to building a company that is more diverse and representative of society at large. Glassdoor externally publishes our Diversity & Inclusion report and information about our employee population to hold ourselves accountable to our commitment. We also provide programs and resources to create a greater sense of belonging for our employees.', 'Long Term Incentive Plan ', 'Pay transparency: We believe that with more salary transparency, you hold the information to ensure fair pay now and in the future as your career changes and grows. Pay bands and our compensation philosophy are shared publicly to ensure that everyone is paid fairly. Our annual Pay Gap Study found no gender or race/ethnicity pay gap at Glassdoor.', 'Experience with scripting languages: Perl, Shell, etc.', 'Participate in rotational on-call support.', 'Design and develop big data applications using a variety of different technologies.', ""Bachelor's Degree in computer science or equivalent experience."", '2+ years of hands-on experience developing a distributed data processing platform with Hadoop, Hive, Spark, Airflow, Kafka, etc.', 'Exposure to test driven development and automated testing frameworks.', 'Develop logical and physical data models for big data platforms.', 'Learn our business domain and technology infrastructure quickly.', 'Company performance transparency: ', 'Ability to program in multiple programming/scripting languages: Python, Java, JS, Scala, etc', ""Employee feedback transparency: We believe in providing you with greater insight into what it is really like to work with us. In addition to our Glassdoor Reviews, we publicly publish our employee feedback survey responses to ensure everyone has a complete picture of what it's like to work here."", 'Be passionate about or have contributed to open sourced engineering projects in the past.', 'Pay transparency:', 'Why Glassdoor?', 'Responsibilities', 'Document, share your knowledge freely and proactively with others in the team.', 'Generous paid holidays and open paid time off ', 'A values-based culture: Our values are Transparency, Innovation, Good People and Grit. We look for talented, passionate people who embody these values in how they show up to work every day.', 'Practice working with, processing, and managing large data sets (multi TB/PB scale).', ""Company performance transparency: We believe you should know how your work contributes to moving the company forward. That's why we share detailed business performance updates in our monthly All Hands and deeper financial results every quarter. For more insight into the performance of our parent company, you can explore the financial results of Recruit Holdings, and its HR Technology segment in particular, each quarter. Operating transparently at Glassdoor is fundamental as we advocate for transparency in the broader workforce. The ultimate goal is not just to change how we operate at Glassdoor, but for every employer to follow our lead!"", '2+ years of experience using Python as a programming language.', 'Excellent verbal and written communication skills.', 'Familiarity with AWS or GCS technologies.', ' Design and develop big data applications using a variety of different technologies. Develop logical and physical data models for big data platforms. Automate workflows using Apache Airflow. Write data pipelines using Apache Hive, Apache Spark. Create solutions on AWS using services such as Lambda, API Gateway, Kinesis etc. Participate in rotational on-call support. Provide ongoing maintenance and enhancements to existing systems. Learn our business domain and technology infrastructure quickly. Document, share your knowledge freely and proactively with others in the team. ', 'Key Qualifications', '2-3 years of hands-on experience in modeling and designing schema for data lakes or for RDBMS platforms.', '100% company paid medical/dental/vision/life coverage, with 80% dependent coverage', 'Create solutions on AWS using services such as Lambda, API Gateway, Kinesis etc.', 'Work with purpose – join us in creating transparency for job seekers everywhere', "" 5+ years of hands-on experience with developing data warehouse solutions and data products. 2+ years of hands-on experience developing a distributed data processing platform with Hadoop, Hive, Spark, Airflow, Kafka, etc. 2-3 years of hands-on experience in modeling and designing schema for data lakes or for RDBMS platforms. 2+ years of experience using Python as a programming language. Experience with scripting languages: Perl, Shell, etc. Practice working with, processing, and managing large data sets (multi TB/PB scale). Exposure to test driven development and automated testing frameworks. Background in Scrum/Agile development methodologies. Capable of delivering on multiple competing priorities with little supervision. Excellent verbal and written communication skills. Bachelor's Degree in computer science or equivalent experience. "", "" A values-based culture: Our values are Transparency, Innovation, Good People and Grit. We look for talented, passionate people who embody these values in how they show up to work every day. Pay transparency: We believe that with more salary transparency, you hold the information to ensure fair pay now and in the future as your career changes and grows. Pay bands and our compensation philosophy are shared publicly to ensure that everyone is paid fairly. Our annual Pay Gap Study found no gender or race/ethnicity pay gap at Glassdoor. Diversity & Inclusion transparency: We are committed to building a company that is more diverse and representative of society at large. Glassdoor externally publishes our Diversity & Inclusion report and information about our employee population to hold ourselves accountable to our commitment. We also provide programs and resources to create a greater sense of belonging for our employees. Employee feedback transparency: We believe in providing you with greater insight into what it is really like to work with us. In addition to our Glassdoor Reviews, we publicly publish our employee feedback survey responses to ensure everyone has a complete picture of what it's like to work here. Company performance transparency: We believe you should know how your work contributes to moving the company forward. That's why we share detailed business performance updates in our monthly All Hands and deeper financial results every quarter. For more insight into the performance of our parent company, you can explore the financial results of Recruit Holdings, and its HR Technology segment in particular, each quarter. Operating transparently at Glassdoor is fundamental as we advocate for transparency in the broader workforce. The ultimate goal is not just to change how we operate at Glassdoor, but for every employer to follow our lead! "", 'A values-based culture: ', 'Diversity & Inclusion transparency: ', ' Experience building customer facing products, machine learning pipelines or data products. Experience working with Tableau, Looker or other data visualization software. Familiarity with AWS or GCS technologies. Ability to program in multiple programming/scripting languages: Python, Java, JS, Scala, etc Be passionate about or have contributed to open sourced engineering projects in the past. ', 'Senior Data Engineer', '401(k) Plan with a Company Match to prepare for your future', 'Automate workflows using Apache Airflow.', 'Provide ongoing maintenance and enhancements to existing systems.', 'Write data pipelines using Apache Hive, Apache Spark.']",Associate,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Data Engineer Intermediate (work from home Mid-Atlantic US resident),Geisinger,"Harrisburg, PA",1 week ago,Be among the first 25 applicants,"['', 'Involves high-level participation in the design and management of a computational infrastructure for different purposes including applied medical research.', 'Builds data ingestion pipelines for the Big Data Hadoop environment.Programming for a Big Data distributed computing environment using Java, Scala or similar object oriented programming languages.Programming data processing and integration algorithms on the Apache Hadoop, HBase stack.Coordinates projects and responsible for timely and accurate execution.Collaborates and participates in the design and implementation of various projects.Involves high-level participation in the design and management of a computational infrastructure for different purposes including applied medical research.Collaborates with other technology team members, clinicians and researchers on projects requiring data and analytic services.Works closely with data architects to define and execute an enterprise data architecture for complex healthcare data flows.Writes code for parallel computing.Develops new programs and responsible for moving existing code to high performance distributed systems code.Responsible to document all changes completed on the system within designated timeframes.Responsible for following department coding/programming guidelines to produce efficient routines.Provides preliminary code review, testing, debugging, and general testing instructions.', 'Our Purpose & Values', 'Coordinates projects and responsible for timely and accurate execution.', 'EXCELLENCE', 'KINDNESS', 'Writes code for parallel computing.', 'INNOVATION', 'Responsible to document all changes completed on the system within designated timeframes.', 'LEARNING', 'Develops new programs and responsible for moving existing code to high performance distributed systems code.', 'Job Summary', 'Collaborates with other technology team members, clinicians and researchers on projects requiring data and analytic services.', 'Responsible for following department coding/programming guidelines to produce efficient routines.', 'Education', 'SAFETY', 'Experience', 'About Geisinger', 'Builds data ingestion pipelines for the Big Data Hadoop environment.', 'Programming data processing and integration algorithms on the Apache Hadoop, HBase stack.', 'Provides preliminary code review, testing, debugging, and general testing instructions.', 'Programming for a Big Data distributed computing environment using Java, Scala or similar object oriented programming languages.', 'Job Duties', 'Collaborates and participates in the design and implementation of various projects.', 'Works closely with data architects to define and execute an enterprise data architecture for complex healthcare data flows.']",Entry level,Full-time,Information Technology,Nonprofit Organization Management,2021-03-24 13:05:10
Data Engineer - Princeton,Ergo,"Princeton, NJ",2 weeks ago,30 applicants,"['', ' Expert SQL skills including experience with distributed and spatial queries', ' Build data pipelines for the automation of data processes', ' Drive data architecture design decisions considering future growth', ' Execute on projects to provide relevant datasets in support of business initiatives', ' Experience with data visualization tools such as Power BI', ' Bachelor’s degree (MS preferred) in Computer Science, Statistics, Math or equivalent combination of education and experience  1-3 years of experience with statistical modeling, data modeling, data intake, and data-curation procedures  Experience in insurance industry (preferred experience working with insurance auto/mobility data)  Expert SQL skills including experience with distributed and spatial queries  1+ years of hands-on R or Python experience is highly preferred  Experience in data pipeline development using Databricks and Azure Data Factory  Experience with data visualization tools such as Power BI  Drive and dedication, as well as creativity and hands-on attitude  Curiosity in searching for new solutions outside of traditional approaches  Demonstrated ability to experiment with and learn new technologies  Strong oral and written communication and interpersonal skills  Excellent analytical, problem solving and organizational skills.', ' Experience in data pipeline development using Databricks and Azure Data Factory', ' Bachelor’s degree (MS preferred) in Computer Science, Statistics, Math or equivalent combination of education and experience', ' Independently perform the extraction, cleaning, transformation and loading of various data file formats, databases and cloud/web sources', 'Key Responsibilities Of This Position Include', ' Develop expert knowledge of data and analytics infrastructure within Munich Re', ' Demonstrated ability to experiment with and learn new technologies', 'Qualifications / Job Requirements', ' Execute on projects to provide relevant datasets in support of business initiatives  Independently perform the extraction, cleaning, transformation and loading of various data file formats, databases and cloud/web sources  Work with various complex databases containing data from multiple sources, at various levels of granularity to deliver high-quality data  Build data pipelines for the automation of data processes  Drive data architecture design decisions considering future growth  Develop expert knowledge of data and analytics infrastructure within Munich Re', ' Curiosity in searching for new solutions outside of traditional approaches', ' Excellent analytical, problem solving and organizational skills.', ' 1-3 years of experience with statistical modeling, data modeling, data intake, and data-curation procedures', ' Work with various complex databases containing data from multiple sources, at various levels of granularity to deliver high-quality data', ' Strong oral and written communication and interpersonal skills', ' 1+ years of hands-on R or Python experience is highly preferred', ' Drive and dedication, as well as creativity and hands-on attitude', ' Experience in insurance industry (preferred experience working with insurance auto/mobility data)', 'Job Description']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Princess Polly,"West Hollywood, CA",3 weeks ago,59 applicants,"['', ' Individual & Team Based Leadership Development Programs', ' Qualifications ', ' Expert SQL Skills (required) ', ' Positive Company Culture that Celebrates both Personal & Company Milestones', ' Flexible working arrangements Amazing Employee Discount Program (40%) Company Sponsored Medical (HMO & PPO Options), Dental, & Vision Plans Company Paid Life, Short Term Disability, Long Term Disability, & Employee', ' Responsibilities ', ' 401(k) Program (100% Match Up to 5% of Pay)', ' Company Paid Life, Short Term Disability, Long Term Disability, & Employee', ' Amazing Employee Discount Program (40%)', ' Flexible working arrangements', ' 401(k) Program (100% Match Up to 5% of Pay) Individual & Team Based Leadership Development Programs Positive Company Culture that Celebrates both Personal & Company Milestones 15 Vacation Days + 10 Sick Days + 10 Holidays', ' Experience Using Dbt (strongly Preferred) ', ' Company Sponsored Medical (HMO & PPO Options), Dental, & Vision Plans', ' 15 Vacation Days + 10 Sick Days + 10 Holidays']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer,Bounteous,"Denver, CO",4 weeks ago,35 applicants,"['', 'College degree in Computer Science, Data Science, Analytics or related field', 'Strong understanding of customer data platforms', 'Be a platform expert in leading CDP solutions like AEP, TreasureData, RedPoint or similar', 'Must be legally eligible to work in Canada.', 'Proficient in at least one programming language such as Python, Java, Scala', 'Work with Consultants and Data Scientists to identify and write the necessary queries needed for segmentation, reporting, analysis, and ML models', 'Preferred Qualifications', 'Develop a deep expertise in our client’s data infrastructure and partner with the respective teams ', 'Build the unified customer profile ', '5+ years of experience building data pipelines and extracting, transforming, and loading marketing and customer data ', 'Proficient in code version control systems like Git', 'Be a platform expert in leading CDP solutions like AEP, TreasureData, RedPoint or similarDevelop a deep expertise in our client’s data infrastructure and partner with the respective teams Work with the Solution Architect and extract, transform and load marketing and customer data into the platform in an automated and scalable mannerBuild the unified customer profile Work with Consultants and Data Scientists to identify and write the necessary queries needed for segmentation, reporting, analysis, and ML models', 'Experience working with data warehouse solutions like Amazon Redshift, Google BigQuery, Snowflake, or similar', 'Experience working with cloud technologies such as AWS, Google Cloud, Azure, or similar ', 'Data Engineer ', 'Strong SQL skills ', 'For Employment Opportunities Based In Canada', 'Role And Responsibilities', 'College degree in Computer Science, Data Science, Analytics or related field5+ years of experience building data pipelines and extracting, transforming, and loading marketing and customer data Strong SQL skills Proficient in at least one programming language such as Python, Java, ScalaExperience working with data warehouse solutions like Amazon Redshift, Google BigQuery, Snowflake, or similarExperience working with cloud technologies such as AWS, Google Cloud, Azure, or similar Proficient in code version control systems like GitStrong understanding of customer data platformsExposure to Spark, Hadoop, and other big data technologies is a plus', 'Exposure to Spark, Hadoop, and other big data technologies is a plus', 'Bounteous is an equal opportunity employer. We embrace diversity and are committed to creating an inclusive workplace. In accordance with the Ontario Human Rights Code and Accessibility for Ontarians with Disabilities Act, 2005, accommodation will be provided at any point throughout the hiring process, provided the candidate makes their accommodation needs known to Bounteous. We welcome applications from all qualified candidates. ', 'Work with the Solution Architect and extract, transform and load marketing and customer data into the platform in an automated and scalable manner']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer,Parsons Corporation,"Springfield, VA",5 days ago,Be among the first 25 applicants,"['', 'Design the logical data relationships and query structures of new databases considering factors such as access methods, access frequency, storage media, data volatility, query requirements, and operating environments to support insider threat detection application software and data analysis.Develop logical data models translated into workable physical database schema and structures in the database development process to support insider threat detection application software and data analysis.Design enterprise database strategies for functions such as backup, recovery and migration or to correct extremely complex operational and performance. Implement new database designs using DataVault 2.0 methodology.Evaluate data sets and engineer solutions for integrating into the Data WarehouseSustain existing Data Warehouse and application databasesPerform daily database administration functions such as developing queries and reports based on customer requirements, modifying or developing database views, and managing backup and recovery operations.Develop, sustain and preserve user manuals and instructions that guide customers in executing data access functions such as running queries and reports.Participate in multi-functional teams and manage work through JIRA.', 'Sustain existing Data Warehouse and application databases', 'Implement new database designs using DataVault 2.0 methodology.', 'Possess a minimum of seven (7) years of experience in Application Software and Extract Transform Load (ETL) and/or Data, Database and/or Data Warehouse implementations', 'Minimum Clearance Required To Start', 'Evaluate data sets and engineer solutions for integrating into the Data Warehouse', 'Duties Include', 'Participate in multi-functional teams and manage work through JIRA.', 'Possess a minimum of three (3) years of experience in implementing, managing, and maintaining data warehouses', 'Required', 'Design enterprise database strategies for functions such as backup, recovery and migration or to correct extremely complex operational and performance. ', 'Perform daily database administration functions such as developing queries and reports based on customer requirements, modifying or developing database views, and managing backup and recovery operations.', 'Develop logical data models translated into workable physical database schema and structures in the database development process to support insider threat detection application software and data analysis.', 'Possess experience building and scheduling data integration scripts that automat ingest of data from multiple sources (Oracle, SQL, Postgres).', 'Possess experience with SQL and SQL Plus ', 'Bachelors or equivalent experience in related field', 'Design the logical data relationships and query structures of new databases considering factors such as access methods, access frequency, storage media, data volatility, query requirements, and operating environments to support insider threat detection application software and data analysis.', 'Bachelors or equivalent experience in related fieldPossess a minimum of seven (7) years of experience in Application Software and Extract Transform Load (ETL) and/or Data, Database and/or Data Warehouse implementationsPossess a minimum of three (3) years of experience in implementing, managing, and maintaining data warehousesPossess experience with SQL and SQL Plus Possess experience building and scheduling data integration scripts that automat ingest of data from multiple sources (Oracle, SQL, Postgres).Possess experience using data Extract Transform Load (ETL) applications such as IBM DataStage and WhereScape.', 'Possess experience using data Extract Transform Load (ETL) applications such as IBM DataStage and WhereScape.', 'Develop, sustain and preserve user manuals and instructions that guide customers in executing data access functions such as running queries and reports.', 'Job Description']",Not Applicable,Full-time,Information Technology,Construction,2021-03-24 13:05:10
Data Engineer,East West Bank,"Pasadena, CA",4 weeks ago,Be among the first 25 applicants,"['', ' Anticipate changes in the internal and external environment and adapt the testing program accordingly ', ' Must be team-oriented with experience working on interdepartmental team projects. ', ' Build reports in SSRS, Power BI, Tableau, Excel and other visualization tools. ', ' Bachelor’s degree required in Computer Science, Information Technology, Management Information Systems or Business Management.  Master’s degree in Mathematics, Statistics, Computer Science, Data Science or relevant.  Strong computing skills with at least one of the following: C#, Java, Python, R, T-SQL with a strong interest to learn.  Quantitative, analytical, process oriented and troubleshooting skills  Proficiency with Excel.  Analytical and problem solving skills including troubleshooting.  Able to work under pressure while managing competing demands and tight deadlines.  Well organized with meticulous attention to detail.  Can-do attitude, self-motivated and strong work ethic.  Self-driven to identify areas of improvement.  Must be team-oriented with experience working on interdepartmental team projects.  Extract and collect large data sets from various sources and formats  Interpret and analyze results from data extractions to identifying patterns and trends  Evaluate internal controls and identify deficiencies through the testing of data source, systems, and processes  Define new data collection and analysis processes  Create reports, processes and tools to monitor key risk indicators for business units across the organization.  Test and validate that key assumptions, data sources, and procedures utilized in measuring and monitoring risk and internal controls can be relied upon on an ongoing basis; and, in the case of transaction testing, to assess that controls are working as intended.  Test adherence with Bank’s policies and controls, as well as regulatory requirements  Maintain an understanding of business operations, operational risks and regulatory requirements  Anticipate changes in the internal and external environment and adapt the testing program accordingly  Assuring the integrity of data, including data extraction, storage, manipulation, processing and analysis  Provides recommendations to streamline tasks and create a more efficient working environment  Report results back to management and relevant team members  Product knowledge in ', ' Master’s degree in Mathematics, Statistics, Computer Science, Data Science or relevant. ', ' Test adherence with Bank’s policies and controls, as well as regulatory requirements ', ' Product knowledge in ', ' Deposits ', ' Build high-performance algorithms, predictive models, and prototypes. ', ' Bachelor’s degree required in Computer Science, Information Technology, Management Information Systems or Business Management. ', 'Introduction ', ' Analytical and problem solving skills including troubleshooting. ', ' Collect and document business requirements. Maintain functional and technical artifacts including design documents, data mappings, architecture, data models, and dictionaries. ', ' Collaborate with team members and business units to Deliver Data-Oriented solutions to the enterprise. ', ' Able to work under pressure while managing competing demands and tight deadlines. ', 'Overview', ' Loans ', ' Extract and collect large data sets from various sources and formats ', ' Report results back to management and relevant team members ', ' Define new data collection and analysis processes ', ' Proficiency with Excel. ', ' Strong computing skills with at least one of the following: C#, Java, Python, R, T-SQL with a strong interest to learn. ', ' Provides recommendations to streamline tasks and create a more efficient working environment ', 'Responsibilities', ' Translate, cleanse and normalize large datasets. ', 'Qualifications', ' Can-do attitude, self-motivated and strong work ethic. ', ' Assuring the integrity of data, including data extraction, storage, manipulation, processing and analysis ', ' Banking operations', ' Loans  Deposits  Banking operations', ' Create reports, processes and tools to monitor key risk indicators for business units across the organization. ', ' Test and validate that key assumptions, data sources, and procedures utilized in measuring and monitoring risk and internal controls can be relied upon on an ongoing basis; and, in the case of transaction testing, to assess that controls are working as intended. ', ' Quantitative, analytical, process oriented and troubleshooting skills ', ' Interpret and analyze results from data extractions to identifying patterns and trends ', ' Evaluate internal controls and identify deficiencies through the testing of data source, systems, and processes ', ' Maintain an understanding of business operations, operational risks and regulatory requirements ', ' Well organized with meticulous attention to detail. ', ' Design, implement, deploy and maintain Data solutions. These solutions are written in T-SQL, Python, C#, Java and R. ', ' Collaborate with team members and business units to Deliver Data-Oriented solutions to the enterprise.  Design, implement, deploy and maintain Data solutions. These solutions are written in T-SQL, Python, C#, Java and R.  Build reports in SSRS, Power BI, Tableau, Excel and other visualization tools.  Build high-performance algorithms, predictive models, and prototypes.  Translate, cleanse and normalize large datasets.  Collect and document business requirements. Maintain functional and technical artifacts including design documents, data mappings, architecture, data models, and dictionaries. ', ' Self-driven to identify areas of improvement. ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Lorien,"Seattle, WA",3 weeks ago,Over 200 applicants,"['', 'U.S. Citizens, Green Card Holders, and those', 'consideration without regard to race, color, religion, gender, national origin,', 'Experience with performance tuning activities at database and ETL level', '* Consolidating Charity tables in Redshift', 'Coding proficiency in at least one modern programming language (Python, Ruby, Java, etc)', 'Knowledge of distributed systems as it pertains to data storage and computing', ' ', '• Business intelligence experience', 'We plan to have weekly check-ins for the following projects. Each project will have its own backward looking timelines:', 'Hands on SQL knowledge and experience in relational database concepts.', '* Individual user access for Redshift', 'Lorien', '* Accounting jobs migration to Andes tables', 'Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations.', '* Smart alarms for detecting Data anomaly', 'Prior Data Engineering tool experience (such as Datanet, Andes etc)', 'Required Skills: ', 'Experienced in at least one massively parallel processing data technology such as AWS Redshift, Teradata, Netezza, Spark or Hadoop based big data solution.', 'age, disability, veteran status, or any other factor determined to be unlawful', 'SERVICES TO BE PERFORMED', 'authorized to work in the U.S for any employer will be considered.', '9-month contract with likely extensions / possible conversion to FTE ', '\xa0', '4-5+ years of experience', 'Preferred Skills', 'Health, Dental, Vision and 401k benefits are available to choose from', '* Roadmap for Attribution Model', '4-5+ years of experienceHands on SQL knowledge and experience in relational database concepts.Coding proficiency in at least one modern programming language (Python, Ruby, Java, etc)Experienced in at least one massively parallel processing data technology such as AWS Redshift, Teradata, Netezza, Spark or Hadoop based big data solution.Robust experience with data modeling, data warehousing, and building ETL pipelines using big data technologies (Hadoop, Hive, Hbase, Spark, EMR, etc.)Experience with performance tuning activities at database and ETL levelKnowledge of orchestrations and automation tools (Airflow, Oozie, etc.)Knowledge of distributed systems as it pertains to data storage and computingProven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy.', 'Robust experience with data modeling, data warehousing, and building ETL pipelines using big data technologies (Hadoop, Hive, Hbase, Spark, EMR, etc.)', '• Large tech company experience (Facebook, Microsoft, Google, IBM, etc.)', 'Knowledge of orchestrations and automation tools (Airflow, Oozie, etc.)', 'Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy.', 'Disqualifiers / red flags:', '* Salesforce: Enable in EU', '* Decouple Associate & Paladin cluster (Phase-II) ($23K in potential annual cost saving for Associate once both phases complete)', 'Experience working with AWS big data technologies (EMR, Redshift, S3)', 'Best vs. Average', 'Lorien is an Equal Opportunity Employer - All qualified applicants will receive', 'Strong SQL and Python Scripting background.Experience working with AWS big data technologies (EMR, Redshift, S3)Prior Data Engineering tool experience (such as Datanet, Andes etc)Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations.', '* Publish Andes datasets for cross-platform data sharing', '• Someone that will need a lot of guidance / cannot work independently ', 'Strong SQL and Python Scripting background.', '* Migration of DJS jobs to Datanet', 'under applicable law.\xa0']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Quantitative Data Engineer,JCW,"New York, NY",4 weeks ago,Over 200 applicants,"['Margarita Romero recruits for Risk and Analytics professionals across the financial services sector and would be happy to provide further details about this or other positions. You can reach her at (646) 564 – 3594 or at margarita.romero@jcwresourcing.com.', ' 5+ years experience managing data for quant trading purposes   ', '', 'Python is required, SQL and C++ are a plus. ', 'Python is required, SQL and C++ are a plus. Degree in Computer Science, Financial Engineering, Statistics, or related fields.  5+ years experience managing data for quant trading purposes   ', 'JCW is currently working on a search for a Junior/ Senior Data Engineer. The qualified candidate will join a multi-strategy investment manager here in NYC. ', 'Requirements ', 'Details ', 'Degree in Computer Science, Financial Engineering, Statistics, or related fields. ', 'The Data Engineer will be working alongside the Data Analytics and Strategy team. The qualified candidate will be a  hands-on programmer and will be knowledgeable in data modeling. ', 'Requirements', ' ', 'Details']",Associate,Full-time,Analyst,Financial Services,2021-03-24 13:05:10
Data Engineer,Koddi,"United, LA",4 weeks ago,Be among the first 25 applicants,"['', 'You have extensive experience working within distributed architectures and building horizontally scalable infrastructure', 'Recommend and implement improvements to data processes and warehouses that improve supportability, usability, and scalability', 'Experience with Extract, Transform, and Load (ETL)', 'Your tech stack includes Scala, Java, GoLang, and/or Python', 'What skills or experience will you bring?', 'High level of experience with SQL, with ability to design and optimize objects', 'You have experience working with Spark and AWS technologies', '4+ years of experience as a data or software engineer', 'You’ve got experience working with Relational Databases such as MS SQL Server, Postgres, MySQL', ""Bachelor's or Master's degree in CS or other technical, science, or math fields"", 'Optimize and refactor existing codeImprove the efficiency, scalability, and reliability of applications', 'Strong communication and teamwork skills', 'Design, deliver and implement data engineering solutions for optimized downstream impact, with end-to-end input and oversight of data engineering projects', 'Work within robust data systems and develop custom solutions while consulting with external customersDesign, deliver and implement data engineering solutions for optimized downstream impact, with end-to-end input and oversight of data engineering projectsRecommend and implement improvements to data processes and warehouses that improve supportability, usability, and scalabilityOptimize and refactor existing codeImprove the efficiency, scalability, and reliability of applications', 'Work within robust data systems and develop custom solutions while consulting with external customers', 'Willingness to learn and utilize emerging technologies', 'What will you do?', ""Bachelor's or Master's degree in CS or other technical, science, or math fields4+ years of experience as a data or software engineerYou have experience working with Spark and AWS technologiesYou’ve got experience working with Relational Databases such as MS SQL Server, Postgres, MySQLYour tech stack includes Scala, Java, GoLang, and/or PythonYou have extensive experience working within distributed architectures and building horizontally scalable infrastructureHigh level of experience with SQL, with ability to design and optimize objectsExperience with Extract, Transform, and Load (ETL)Strong communication and teamwork skillsWillingness to learn and utilize emerging technologies""]",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer (New Grad),Orchard,"New York, NY",2 weeks ago,Over 200 applicants,"['', 'Orchard is radically simplifying the way people buy and sell their homes. For the average American, the home purchase and sale process takes months, creates anxiety, and is filled with uncertainty and hassle. Orchard has reimagined the end-to-end experience of buying and selling, from innovative home search tools to find the perfect home to the ability to buy a new home before selling your current one. Orchard customers manage the entire experience through a personalized online dashboard, while also getting the support of best-in-class Orchard real estate agents.\xa0', 'Work with the following technologies: Python3, PostgreSQL, Docker, AWS, Airflow', 'Orchard’s engineering culture is centered around product empathy and autonomous teams with high feature ownership. Engineers take ownership of feature development end to end. This means we partner with Product Managers and Designers to solve ambiguous business problems and have a high degree of collaborative input before writing code. We strive to keep common infrastructure and dependencies simple (or only as complex as necessary), to keep the coordination costs of infrastructure deployments low, and support lean & nimble product engineering teams.\xa0', 'Equity participation', 'Prior internship experience working with a team to plan, prioritize, build, and deploy code', 'Integrating with third-party data sources (MLS and county tax data are primary sources, among others) to support our home transaction platform and Data Science initiatives', 'About Orchard', 'Building and maintaining ETL pipelines to support business intelligence', ""We're proud to be recognized by Glassdoor, Inc. Magazine, Fast Company and Forbes on their lists of best places to work. We also have a 4.9 Glassdoor rating! Orchard is building the first one-stop-shop in real estate and we’re bringing together the most innovative professionals across real estate, business, marketing, technology and design. We also have some pretty great perks:"", 'Proficiency in SQL and Python. Familiarity with Postgres or Airflow is a plus.', 'Prioritization & value orientation: we operate with agile principles and balance long-term planning with re-evaluating priorities based on new data and assumptions', ""Proficiency in SQL and Python. Familiarity with Postgres or Airflow is a plus.Prior internship experience working with a team to plan, prioritize, build, and deploy codeCurrently have, or are in the process of attaining, BS or MS in Computer Science or related fieldA results-oriented attitude with attention to detail: we expect engineers to own projects from planning to production deployment (and production operations!)Low ego with appetite for feedback: we value humility and sharing + receiving feedback for growth and continuous improvementBusiness empathy & clear communication: we work with product, design & business stakeholders collaboratively from early in the design process. We look for engineers to frame technical problems in the context of business value and be able to communicate with cross-functional teamsPrioritization & value orientation: we operate with agile principles and balance long-term planning with re-evaluating priorities based on new data and assumptionsProblem solving & ownership: we look for a willingness to take on problems in a growing organization where there's not yet a defined solution"", 'We’d Love to Hear From You if You Have', ""We're currently working from home until it's safe for employees to return to the office. We anticipate returning later this year and are excited to welcome people back to our offices and see one another in person. Until then, your interviews will all happen virtually. If there is anything we can do to make your process easier, don't hesitate to let us know!\xa0"", 'Building and maintaining model training and validation pipelines for our automated valuation model (AVM)', ""Problem solving & ownership: we look for a willingness to take on problems in a growing organization where there's not yet a defined solution"", 'Integrating with third-party data sources (MLS and county tax data are primary sources, among others) to support our home transaction platform and Data Science initiativesBuilding and maintaining ETL pipelines to support business intelligenceBuilding and maintaining model training and validation pipelines for our automated valuation model (AVM)Deploying machine learning models (our AVM) to production so that analysts can use them to value the homes we make offers onWork with the following technologies: Python3, PostgreSQL, Docker, AWS, Airflow', 'Flexible PTO', 'Low ego with appetite for feedback: we value humility and sharing + receiving feedback for growth and continuous improvement', 'Equity participationFlexible PTOUp to 18 weeks of paid family leaveEmployee discount on Orchard’s services', '\xa0', 'Why Orchard', '\ufeffOrchard is proud to be an equal opportunity employer. We provide employment opportunities without regard to age, race, color, ancestry, national origin, religion, disability, sex, gender identity or expression, sexual orientation, veteran status, or any other protected status in accordance with applicable law.', 'The team values engineers who prioritize results and testing, and we’re looking for engineers who are excited to explore our data and come up with novel ways to use it.', 'This is a full-time role based out of our New York City office.', 'Deploying machine learning models (our AVM) to production so that analysts can use them to value the homes we make offers on', 'About the Role', 'Up to 18 weeks of paid family leave', 'Headquartered in New York City and with offices throughout Texas, Colorado, Georgia, North Carolina, and Virginia, Orchard has over 300 employees and growing.\xa0We have raised over $130 million in equity financing from top-tier investors including Revolution, Firstmark, Accomplice, Navitas and Juxtapose. Our investors have also backed the likes of Pinterest, AirBnb, Shopify and Sweetgreen. Orchard is proud to be recognized as part of Glassdoor’s Best Places to Work.', 'We are interested in all qualified candidates who are eligible to work in the United States. However, we are not able to sponsor visas at this time.', 'A results-oriented attitude with attention to detail: we expect engineers to own projects from planning to production deployment (and production operations!)', 'This is an exciting opportunity to join a high-growth team at the ground floor, and play an instrumental role in making the home buying and selling experience frictionless for our customers.', 'Employee discount on Orchard’s services', 'Currently have, or are in the process of attaining, BS or MS in Computer Science or related field', 'What You’ll Do Here', 'Business empathy & clear communication: we work with product, design & business stakeholders collaboratively from early in the design process. We look for engineers to frame technical problems in the context of business value and be able to communicate with cross-functional teams', 'As part of the larger engineering organization, Orchard’s Data Team performs a function that is at the core of our business: we are responsible for building and maintaining the technical infrastructure to ingest the data sources, and deploy the models that drive our decision-making and software. Part of this role will be working directly with the Data Science team in order to make their models production-ready.']",Entry level,Full-time,Information Technology,Real Estate,2021-03-24 13:05:10
Data Engineer,StockX,"Detroit, MI",2 weeks ago,Be among the first 25 applicants,"['', ' Masters in Computer Science, Physics, Mathematics, Statistics or other Engineering disciplines Experience with data visualization tools such as Tableau, Looker, PowerBI Experience with Hadoop implementation ', 'Build and support a big data platform on the cloud', 'Experience with data visualization tools such as Tableau, Looker, PowerBI', 'Experience with Database Architecture/Schema design', 'Define and implement automation of jobs and testing', '7+ years’ experience in data warehouse / data lake technical architecture', 'Experience providing technical leadership and mentoring other engineers for best practices on data engineering', 'Ability to work independently with business partners and management to understand their needs and exceed expectations in delivering tools/solutions', 'We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. This job description is intended to convey information essential to understanding the scope of the job and the general nature and level of work performed by job holders within this job. However, this job description is not intended to be an exhaustive list of qualifications, skills, efforts, duties, responsibilities or working conditions associated with the position. StockX reserves the right to amend this job description at any time.', ""3+ years' experience with AWS or engineering in other cloud environments"", 'Nice To Have', 'Automation of end to end data pipeline with metadata, data quality checks and audit ', 'Support mission critical applications and near real time data needs from the data platform', 'Design and build mission critical data pipelines with a highly scalable distributed architecture - including data ingestion (streaming, events and batch), data integration, data curation', 'Capture and publish metadata and new data to subscribed users', 'Motivate, coach, and serve as a role model and mentor for other development team associates/members that leverage the platform ', 'Work collaboratively with business analysts, product managers, data scientists as well as business partners and actively participate in design thinking session', "" 7+ years’ experience in data warehouse / data lake technical architecture Minimum 3 years of Big Data and Big Data tools in one or more of the following: Kafka, MapReduce, Spark or Python, Hadoop 3+ years' experience with AWS or engineering in other cloud environments Experience with Database Architecture/Schema design Strong familiarity with batch processing and workflow tools such as AirFlow, NiFi Ability to work independently with business partners and management to understand their needs and exceed expectations in delivering tools/solutions Strong interpersonal, verbal and written communication skills and ability to present complex technical/analytical concepts to executive audience Strong business mindset with customer obsession; ability to collaborate with business partners to identify needs and opportunities for improved data management and delivery Experience providing technical leadership and mentoring other engineers for best practices on data engineering BS/BA degree in Computer Science, Physics, Mathematics, Statistics or other Engineering disciplines "", 'Strong business mindset with customer obsession; ability to collaborate with business partners to identify needs and opportunities for improved data management and delivery', 'Responsibilities', 'Qualifications', 'Minimum 3 years of Big Data and Big Data tools in one or more of the following: Kafka, MapReduce, Spark or Python, Hadoop', 'Participate in design and code reviews', 'Help continually improve ongoing reporting and analysis processes, simplifying self-service support for business stakeholders', 'Build and support reusable framework to ingest, integration and provision data', 'Strong interpersonal, verbal and written communication skills and ability to present complex technical/analytical concepts to executive audience', 'Optimize the data pipeline to support ML workloads and use cases', 'BS/BA degree in Computer Science, Physics, Mathematics, Statistics or other Engineering disciplines', 'Experience with Hadoop implementation', 'Strong familiarity with batch processing and workflow tools such as AirFlow, NiFi', ' Design and build mission critical data pipelines with a highly scalable distributed architecture - including data ingestion (streaming, events and batch), data integration, data curation Help continually improve ongoing reporting and analysis processes, simplifying self-service support for business stakeholders Build and support reusable framework to ingest, integration and provision data Automation of end to end data pipeline with metadata, data quality checks and audit  Build and support a big data platform on the cloud Define and implement automation of jobs and testing Optimize the data pipeline to support ML workloads and use cases Support mission critical applications and near real time data needs from the data platform Capture and publish metadata and new data to subscribed users Work collaboratively with business analysts, product managers, data scientists as well as business partners and actively participate in design thinking session Participate in design and code reviews Motivate, coach, and serve as a role model and mentor for other development team associates/members that leverage the platform  ', 'Masters in Computer Science, Physics, Mathematics, Statistics or other Engineering disciplines']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Amply Media,"Kansas City, MO",3 weeks ago,42 applicants,"['', 'Team Player:\xa0Demonstrates a strong ability to support department staff members and managers; able to establish collaborative relationships with peers.', 'Creativity:\xa0Research the latest trends to design competitive ad units.', 'Experience with developing, executing and maintaining complex SQL queries against multiple databases is required.Experience using Snowflake, Postgres, or equivalentExperience with visualization and reporting tools, such as Tableau, PowerBI and Looker.Desire to work with a team of people solving complex problems and sharing knowledge freelyPassion for data management and data technologies', 'Company Overview', 'Amply Media is more than just one of the world’s largest mobile content providers. Our audience of over 100 million plus subscribers look forward to our content each day because we give our users exciting, engaging, and powerful mobile content, all focused on the web’s most popular categories.', 'No dress code policy! Wear your flip flops and shorts in the summer', 'Adaptability:\xa0Capable of adjusting to changing priorities in a fast-paced environment.', '\xa0\xa0', 'Experience with developing, executing and maintaining complex SQL queries against multiple databases is required.', 'Passion for data management and data technologies', 'Prioritization:\xa0', 'Passion for Technology:', 'Time flies here. Every day, our ‘ROI Junkies’ are committed to finding innovative ways to grow our highly-engaged audience and connect these users with the top-tier advertisers who can’t reach them anywhere else. From generating fresh content, to creating amazing new creatives, to the audience engagement algorithms, our full-stack optimization process seeks to leave no stone unturned. This is a team that must react and move quickly in a fast-paced environment. Our independent teams work on 3 to 5 new “bets” at a time and must juggle these priorities each day.', 'Resourceful:\xa0Looks for ways to achieve goals with available resources.', 'Create new reports based on client requirements using metric and dimension based reporting data models.', 'We offer a fun, work hard – play hard cultureWe have shuffleboard, virtual reality, a retro gaming console and a beer fridgeNo dress code policy! Wear your flip flops and shorts in the summerComplimentary snacks and beverages as well as catered lunchesLocated on the Country Club Plaza- next to world class shopping and restaurants\xa0Fun social events and perks! - check out our Instagram to see moreThis is an opportunity to be with an industry leading company that continues to experience tremendous growth\xa0Great Place to Work Certified and named Ingram’s Best Companies To Work For in Kansas City', 'This is an opportunity to be with an industry leading company that continues to experience tremendous growth\xa0', 'Proactively improve reporting workflows and automate manual processes throughout the business for higher efficiency, robustness, and speed.', 'Communication (written and oral):', 'Prioritization:\xa0Able to arrange projects in order of importance relative to each other.', 'Interact face-to-face with business stakeholders, develop cooperative relationships, and acquire domain knowledge of the business.', 'Communication (written and oral):\xa0Able to successfully communicate product recommendations based on design knowledge', 'The successful candidate will also demonstrate the following abilities:\xa0', 'Support existing reporting infrastructure and troubleshoot issues related to dataCreate new reports based on client requirements using metric and dimension based reporting data models.Proactively improve reporting workflows and automate manual processes throughout the business for higher efficiency, robustness, and speed.Support the development of new products and services via ingestion, processing, and formatting data for reporting and analytics.Interact face-to-face with business stakeholders, develop cooperative relationships, and acquire domain knowledge of the business.Design and implement effective database models to store and retrieve company data', '\xa0', 'Amply Media\xa0lives at the junction where the science of cutting-edge technology meets with audience engagement to create a new and powerful force that fosters real, long-term relationships with users. We pride ourselves on leading the industry in mobile engagement and monetization. We’ve gathered a “one-for-all” minded, world-class team of innovative developers, marketing ninjas, imaginative designers and content developers whose zeal for what they do is slightly north of fanatical. Seriously… obsessive.\xa0\xa0', 'Located on the Country Club Plaza- next to world class shopping and restaurants\xa0', 'We offer a fun, work hard – play hard culture', 'We have shuffleboard, virtual reality, a retro gaming console and a beer fridge', 'Responsibilities', 'Fun social events and perks! - check out our Instagram to see more', 'Experience with visualization and reporting tools, such as Tableau, PowerBI and Looker.', 'Passion for Technology:\xa0Excitement for new technology, bleeding edge applications, and a positive attitude towards solving real world challenges.', 'Resourceful:', 'Creativity:', 'Team Player:', 'Qualifications', 'Experience using Snowflake, Postgres, or equivalent', 'Why Amply Media?', 'Support existing reporting infrastructure and troubleshoot issues related to data', 'Desire to work with a team of people solving complex problems and sharing knowledge freely', 'Adaptability:', 'Great Place to Work Certified and named Ingram’s Best Companies To Work For in Kansas City', 'Design and implement effective database models to store and retrieve company data', 'Complimentary snacks and beverages as well as catered lunches', 'Adaptability:\xa0Capable of adjusting to changing priorities in a fast-paced environment.Prioritization:\xa0Able to arrange projects in order of importance relative to each other.Passion for Technology:\xa0Excitement for new technology, bleeding edge applications, and a positive attitude towards solving real world challenges.Creativity:\xa0Research the latest trends to design competitive ad units.Resourceful:\xa0Looks for ways to achieve goals with available resources.Team Player:\xa0Demonstrates a strong ability to support department staff members and managers; able to establish collaborative relationships with peers.Communication (written and oral):\xa0Able to successfully communicate product recommendations based on design knowledge', 'Support the development of new products and services via ingestion, processing, and formatting data for reporting and analytics.']",Mid-Senior level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer (Remote),KAR Global,"Chicago, IL",4 weeks ago,Be among the first 25 applicants,"['', 'About Our Team', 'Bachelor’s degree in Business, Computer Science, Management Information Systems or equvalent (required).5+ years experience Postgres or Oracle SQL development including functions, stored procedures, and indexing or equivalent (required).Experience in production data management in high availability product delivery ODS / RDBMS or equivalent (required).Experience planning and designing maintainable data schemas (required).Experience with Python, Docker, and data warehouse environments (preferred).Experience using Github / Jenkins (CI/CD) / Artifactory / PyPy or comparable delivery stacks (preferred).Experience with Postgres, Elastic Search, AWS EMR, and AWS ECS (preferred).Experience with AWS Redshift, MPP, or Dynamo DB (preferred).Experience with Informatica (preferred)Experience with Kinesis/Kafka (preferred).Experience working with large enterprise data lakes / Snowflake (preferred).Experience with Informatica MDM Hub configurations - Data modeling & Data Mappings (Landing, staging and Base Objects), Data validation, Match and Merge rules, Active VOS, SIF Framework, and MDM User Exits (preferred)Experience working with master data management in automotive or business to business customer domains (preferred)', 'Employee stock purchase program', 'Experience in production data management in high availability product delivery ODS / RDBMS or equivalent (required).', ' And we’re an auction company powering the world’s most advanced and integrated mobile, digital and physical auction marketplaces. ', 'Competitive compensation', 'Location', 'Paid holidays and generous paid time off', ' Collaborates with source systems data stewards, Data owners and technical personnel for data governance and resolves any data quality or technical issues related to data ingestion. ', 'Experience with AWS Redshift, MPP, or Dynamo DB (preferred).', 'Design, build, and delivery of enterprise data services solutions for MDM on the DaaS Platform.', 'Who We Are', 'Experience planning and designing maintainable data schemas (required).', ' We’re a technology company delivering next generation tools to accelerate and simplify remarketing. ', 'What You Need To Be Successful', 'Responsibilities Include', 'Experience with Informatica (preferred)', 'Experience working with master data management in automotive or business to business customer domains (preferred)', 'Experience working with large enterprise data lakes / Snowflake (preferred).', '401(k) with employer match', 'Contribute to the Data Engineering and MDM team delivery framework including building of re-usable code, implementing industry best practices, and maintain a common delivery framework.', 'Experience with Kinesis/Kafka (preferred).', 'Learning and development resources', 'Experience with Informatica MDM Hub configurations - Data modeling & Data Mappings (Landing, staging and Base Objects), Data validation, Match and Merge rules, Active VOS, SIF Framework, and MDM User Exits (preferred)', 'What You Will Be Doing', 'Bachelor’s degree in Business, Computer Science, Management Information Systems or equvalent (required).', 'Paid parental leave', '5+ years experience Postgres or Oracle SQL development including functions, stored procedures, and indexing or equivalent (required).', 'Work with product, data science, analytics, and engineering teams to learn project data needs and define project scope.Design, build, and delivery of enterprise data services solutions for MDM on the DaaS Platform.Building and delivery of Python/Docker feed framework data pipeline jobs and services.Contribute to the Data Engineering and MDM team delivery framework including building of re-usable code, implementing industry best practices, and maintain a common delivery framework.Monitoring, maintenance, documentation, and incident resolution of scheduled production data jobs supporting internal and external customers data needs. Collaborates with source systems data stewards, Data owners and technical personnel for data governance and resolves any data quality or technical issues related to data ingestion. ', 'Insurance coverage that includes medical, dental, vision and life insurance', 'Work with product, data science, analytics, and engineering teams to learn project data needs and define project scope.', 'Building and delivery of Python/Docker feed framework data pipeline jobs and services.', 'Monitoring, maintenance, documentation, and incident resolution of scheduled production data jobs supporting internal and external customers data needs.', 'Experience using Github / Jenkins (CI/CD) / Artifactory / PyPy or comparable delivery stacks (preferred).', ' We’re an analytics company leveraging data to inform and empower our customers with clear, actionable insights. ', 'Wellness program', 'About Our Candidate', ' We’re a technology company delivering next generation tools to accelerate and simplify remarketing.  We’re an analytics company leveraging data to inform and empower our customers with clear, actionable insights.  And we’re an auction company powering the world’s most advanced and integrated mobile, digital and physical auction marketplaces. ', 'Flexible spending account', 'Experience with Python, Docker, and data warehouse environments (preferred).', 'Competitive compensationInsurance coverage that includes medical, dental, vision and life insuranceFlexible spending accountWellness program401(k) with employer matchEmployee stock purchase programPaid holidays and generous paid time offPaid parental leaveLearning and development resources', 'Experience with Postgres, Elastic Search, AWS EMR, and AWS ECS (preferred).']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer – Webscraping,Balyasny Asset Management L.P.,"Chicago, IL",,N/A,"['', 'Minimum Qualifications', 'Role Overview', 'Preferred Qualifications', 'Experience containerizing workloads with Docker (Kubernetes a plus)', 'Data Intelligence Group (DIG) ', 'Experience with AWS', 'Excellent verbal and written communication skills', 'Collaborate with analysts to understand and anticipate requirements', 'Linux experience (Windows experience a plus)', 'The Data Intelligence Group (DIG) is a key part of BAM’s continued growth. Year over year, the knowledge needed to leverage data plays an increasingly important role in the firm’s core business.\xa0The analysis, services, software, and operational expertise that DIG provides are part of BAM’s competitive advantage.', 'Author tests to validate data availability and integrity', '\xa0', 'Familiarity with scraping and common scraping tools (Selenium, scrapy, Fiddler, Postman, xpath)', 'We are looking for a creative and meticulous developer to join our Webscraping team.\xa0The data we provide drives investment decisions across the firm and we work hard to make sure it’s timely and accurate.\xa0The optimal candidate will be strongly self-motivated with the ability to work and solve problems independently.\xa0In your role, you will:', 'Collaborate with analysts to understand and anticipate requirementsDesign, implement, and maintain webscrapes for a wide variety of alternative datasetsAuthor tests to validate data availability and integrityMaintain alerting systems to ensure smooth day-to-day operationsInvestigate and defuse time-sensitive data incidents', 'Bachelors/Masters degree in Computer Science or a related field1-3 years web development experience (Python/SQL/HTML/CSS/HTTP)Linux experience (Windows experience a plus)Excellent verbal and written communication skills', 'Investigate and defuse time-sensitive data incidents', 'Maintain alerting systems to ensure smooth day-to-day operations', 'Experience with build automation (Jenkins, TeamCity)', 'Aptitude for designing infrastructure, data products, and tools for Data Scientists', 'Bachelors/Masters degree in Computer Science or a related field', '1-3 years web development experience (Python/SQL/HTML/CSS/HTTP)', 'Aptitude for designing infrastructure, data products, and tools for Data ScientistsFamiliarity with scraping and common scraping tools (Selenium, scrapy, Fiddler, Postman, xpath)Experience containerizing workloads with Docker (Kubernetes a plus)Experience with build automation (Jenkins, TeamCity)Experience with AWS', 'Design, implement, and maintain webscrapes for a wide variety of alternative datasets']",Associate,Full-time,Information Technology,Investment Management,2021-03-24 13:05:10
Data Engineer,Kforce Inc,"Sandy, UT",1 week ago,Be among the first 25 applicants,"['', ' Experience with scheduling ETL jobs using a scheduling tool ', ' Create technical specifications documents and design process diagrams', ' Experience in data streaming services', ' Experience working with unstructured and semi structured data sets', ' Experience in data classification & handling around data types included in HIPAA, PCI, CCPA, and other types', "" Bachelor's degree in Information Systems or related discipline preferred (or equivalent work experience)"", ' Industry related certifications a plus', ' Troubleshoot and optimize ETL code; Interpret ETL logs, perform data validation, dissect code, understand the benefits and drawbacks of parallelism, apply best practices using change data capture, expressions, scoping of variables, commonly used transforms, event handlers and logging providers, understand and optimize the surrogate key generation and inconsistent data type handling', ' Ability to work independently and as part of a team', ' Ability to read and write effective, modular, dynamic (parameterized) and robust code, establish and follow already established code standards, and ETL framework ', 'Requirements', ""  Bachelor's degree in Information Systems or related discipline preferred (or equivalent work experience)  Industry related certifications a plus  Minimum of 3 years of experience with ETL tool (SSIS, Informatica, etc.)   Experience implementing ETL for Data Warehouse and Business Intelligence solutions   Experience in RDBMS design and development, and performance tuning  Deep familiarity with database technologies and cloud infrastructure  Experience in data classification & handling around data types included in HIPAA, PCI, CCPA, and other types  Experience with scheduling ETL jobs using a scheduling tool   Experience in data streaming services  Ability to read and write effective, modular, dynamic (parameterized) and robust code, establish and follow already established code standards, and ETL framework   Strong analytical and problem solving skills   Ability to work independently and as part of a team  Comfortable with remote, hybrid work environments  Experience writing ETL jobs from Snowflake, Teradata, SQL Server, MySQL, Postgres, CSV files, etc.  Experience implementing solutions in cloud infrastructure. AWS, Azure, GCP, etc.  Experience working with unstructured and semi structured data sets  Experience with Reporting tools. PowerBI or Tableau preferred "", '  Solidify and extend existing ETL Processes and Framework   Design ETL jobs and reusable components to implement specified business requirements   Troubleshoot and optimize ETL code; Interpret ETL logs, perform data validation, dissect code, understand the benefits and drawbacks of parallelism, apply best practices using change data capture, expressions, scoping of variables, commonly used transforms, event handlers and logging providers, understand and optimize the surrogate key generation and inconsistent data type handling  Create technical specifications documents and design process diagrams  Develop functional specifications for data acquisition, transformation and load processes  Develop scripts for data file processing and process integration tasks ', ' Develop functional specifications for data acquisition, transformation and load processes', ' Solidify and extend existing ETL Processes and Framework ', ' Experience in RDBMS design and development, and performance tuning', 'Responsibilities', ' Strong analytical and problem solving skills ', ' Experience with Reporting tools. PowerBI or Tableau preferred', ' Comfortable with remote, hybrid work environments', ' Deep familiarity with database technologies and cloud infrastructure', ' Develop scripts for data file processing and process integration tasks', ' Experience writing ETL jobs from Snowflake, Teradata, SQL Server, MySQL, Postgres, CSV files, etc.', ' Experience implementing ETL for Data Warehouse and Business Intelligence solutions ', ' Minimum of 3 years of experience with ETL tool (SSIS, Informatica, etc.) ', ' Experience implementing solutions in cloud infrastructure. AWS, Azure, GCP, etc.', ' Design ETL jobs and reusable components to implement specified business requirements ']",Associate,Full-time,Information Technology,Media Production,2021-03-24 13:05:10
Data Engineer,Obsidian Security,"San Mateo, CA",1 week ago,52 applicants,"['', 'Participate in the engineering life-cycle at Obsidian, including designing distributed systems, writing production code, conducting code reviews and working alongside our infrastructure and reliability teams.', 'BS/MS Computer Science or a highly quantitative discipline (MS Degree preferred)Ability with the following programming languages: Scala and PythonExtensive experience with the following software packages: SQL, NoSQL, Apache products (Spark, Hive, Airflow, Hadoop, etc.) and graph databasesFamiliarity with DevOps and AWS (S3, EMR, EC2 are a must)Successfully delivered and maintained a major cloud service with a large number of end users.Cybersecurity experience is a plus', 'You’re an engineer who is experienced designing, implementing and supporting new data processing, data sets and systems to support various advanced analytics needs from multiple sources.You enjoy solving large-scale problems and are comfortable doing incremental quality work while building brand new systems to enable future quality improvements', 'Work in a big data ecosystem to develop and maintain an analytics pipeline for acquisition, storage, and processing data types of interest to feed real-time artificial intelligent system behaviors.', 'Collaborate with the existing engineering team, data scientists, analysts and the business to understand requirements and data needs and definitions.', 'Familiarity with DevOps and AWS (S3, EMR, EC2 are a must)', 'Cybersecurity experience is a plus', 'Ability with the following programming languages: Scala and Python', 'Support and build well-engineered data systems to support analytical needs for cloud based systems (AWS) and deliver data via APIs', 'Extensive experience with the following software packages: SQL, NoSQL, Apache products (Spark, Hive, Airflow, Hadoop, etc.) and graph databases', 'Successfully delivered and maintained a major cloud service with a large number of end users.', 'You enjoy solving large-scale problems and are comfortable doing incremental quality work while building brand new systems to enable future quality improvements', 'Develop or implement tools to support analyst-driven machine learning analyses.', 'BS/MS Computer Science or a highly quantitative discipline (MS Degree preferred)', 'You’re an engineer who is experienced designing, implementing and supporting new data processing, data sets and systems to support various advanced analytics needs from multiple sources.', 'Participate in the engineering life-cycle at Obsidian, including designing distributed systems, writing production code, conducting code reviews and working alongside our infrastructure and reliability teams.Work in a big data ecosystem to develop and maintain an analytics pipeline for acquisition, storage, and processing data types of interest to feed real-time artificial intelligent system behaviors.Develop or implement tools to support analyst-driven machine learning analyses.Support and build well-engineered data systems to support analytical needs for cloud based systems (AWS) and deliver data via APIsCollaborate with the existing engineering team, data scientists, analysts and the business to understand requirements and data needs and definitions.']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,AirDNA,"Denver, CO",3 weeks ago,45 applicants,"['', 'Passionate about providing well documented and user friendly data sources', 'Cluster management', '3+ years of data engineering experience', 'Meaningful equity upside—we are a small team with no outside investors and want every hire to share in our long term financial success', 'Familiarity with Scala (or similar JVM language)', 'Build, maintain, and monitor data pipelinesCluster managementHelp design and build out the next generation of our data platformCraft and optimize data pipelines using SparkImplement data and application integrations with key industry partnersWork closely with our Data Science team to develop new products and scale out algorithmsMaintain and migrate legacy data systems', 'Help design and build out the next generation of our data platform', 'Strong experience with Spark', 'Experience building composable and stable ETL systems', 'We have a great LoDo office space, a short walk from the Union Station light-rail stop; enjoy coffee, tea, beer, snacks, bike parking and great food and drink nearby', 'Craft and optimize data pipelines using Spark', 'About AirDNA ', '3+ years of data engineering experienceStrong experience with SparkFamiliarity with Scala (or similar JVM language)Experience building composable and stable ETL systemsPassionate about providing well documented and user friendly data sourcesDemonstrated successful experience working in a hands-on, fast-paced, creative, entrepreneurial environmentBS or MS in Computer Science/Engineering preferred', "" Here's What You'll Get To Do "", 'Implement data and application integrations with key industry partners', 'Competitive cash compensation and benefits, the salary range for this position is $110,000 - $140,000Meaningful equity upside—we are a small team with no outside investors and want every hire to share in our long term financial successWe have a great LoDo office space, a short walk from the Union Station light-rail stop; enjoy coffee, tea, beer, snacks, bike parking and great food and drink nearby', 'Demonstrated successful experience working in a hands-on, fast-paced, creative, entrepreneurial environment', 'Competitive cash compensation and benefits, the salary range for this position is $110,000 - $140,000', 'BS or MS in Computer Science/Engineering preferred', 'Maintain and migrate legacy data systems', 'Work closely with our Data Science team to develop new products and scale out algorithms', 'Build, maintain, and monitor data pipelines']",Entry level,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Data Engineer,ConsultNet,"Rockville, MD",5 days ago,Be among the first 25 applicants,"['Familiarity of columnar storage formats ORC, Parquet and various compression techniques', 'Write complex SQL queries', 'Provide tech expertise and leadership for analysis, design, and modeling work', 'Strong and creative analytical and problem-solving skills', 'Job Responsibilities', 'Set clear expectations with engineering team and peers', 'Ensure security and compliance requirements are met', 'Strong communication skills', 'Be delivery-focused', 'Unit testing with Junit or Scalatest', 'SPARKHadoop, Hive, HbaseREST APIsWrite complex SQL queriesPerformance tuning and optimizationFamiliarity of columnar storage formats ORC, Parquet and various compression techniquesUnit testing with Junit or ScalatestGit/MavenCode ReviewsAgileTesting strategy and implementationAWS Cloud experience is a plus.', 'Believes in Scrum/Agile, and has deep experience delivering software when working on teams that use Scrum/Agile methodology', 'Code Reviews', 'Provide timely and regular updates to management', 'Testing strategy and implementation', 'Establish and implement ETL and big data processing best practices to design, test, implement, and support mission critical applications', 'Performance tuning and optimization', '3+ years of experience required, including some commercial/non-government workHands on experience designing, developing, implementing, testing, and deploying large scale ETL/Data Analytics/Java/J2EE projectsBelieves in Scrum/Agile, and has deep experience delivering software when working on teams that use Scrum/Agile methodologyStrong and creative analytical and problem-solving skillsStrong communication skillsEnable long-term implementation, facilitate product vision and strategy.', 'Hadoop, Hive, Hbase', 'Ship great products! ', 'Experience & Qualifications', 'SPARK', 'Able to grasp complex business requirements', 'Data Engineer ', 'THE ROLE', 'Own the outcome', 'Able to grasp complex business requirementsProvide tech expertise and leadership for analysis, design, and modeling workDesign, develop, implement, test, deploy, and support big data processing software in the AWS CloudEstablish and implement ETL and big data processing best practices to design, test, implement, and support mission critical applicationsSet clear expectations with engineering team and peersProvide timely and regular updates to managementEnsure security and compliance requirements are metBe delivery-focusedBe quality-focusedOwn the outcomeShip great products! ', 'Be quality-focused', 'Git/Maven', 'Hands on experience designing, developing, implementing, testing, and deploying large scale ETL/Data Analytics/Java/J2EE projects', 'ConsultNet is an Equal Opportunity Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, ancestry, citizenship, disability, age, military or veteran status, and other characteristics protected under federal, state and local law.', 'THE PLATFORM', 'AWS Cloud experience is a plus.', 'Enable long-term implementation, facilitate product vision and strategy.', 'Design, develop, implement, test, deploy, and support big data processing software in the AWS Cloud', ""Our client, one of the largest Amazon Web Services (AWS) partner for data services, is looking for top talent to join their elite team of technologists to build and contribute to large-scale, innovative projects. Our client wants candidates who are eager to learn and grow with them as they work to move their market monitoring applications to modern big data platforms in the cloud where up to 60 billion market events are being processed each day.If you are a fast learner, have a sharp mathematical and technical mind and are interested in working for one of the biggest data users in the country in a fast-paced, open source environment, our client has an immediate need for a Data Engineer to coordinate changes to computer databases, test, and implement the database applying knowledge of database management systems.THE PLATFORMOur client's Data Platform is responsible for the collection, management and dissemination of data in their institution. It's an enterprise capability that services multiple business lines across the entire organization. The platform collects and processes billions of records (up to 100 billion) each day in support of their regulatory mission to protect investors and promote market integrity. They are embarking upon a large, multi-year, enterprise-wide initiative to re-platform major pieces of our existing offering, replacing legacy functionality with a set of modern, highly customer-centric products and capabilities.The ideal candidate will provide technical leadership and implementation to a team focused on delivering one or more of those modern, customer-centric products and capabilities.THE ROLEData Engineer Are you passionate about technology? Do you love programming? Do you sometimes pat yourself on the back after writing a particularly elegant function? Are you an aficionado of the art of software design? Do you expect the best from yourself and those around you?Our client is looking for passionate developers who want to make a difference. If you are a rock star developer, a strong communicator, and a self-starter who can grab hold of a challenge and dive deep with it, then please join our client's delivery-focused team.Job Responsibilities"", '3+ years of experience required, including some commercial/non-government work', 'Required Technical Skills & Knowledge for Lead Data engineer:', 'REST APIs', 'Agile']",Entry level,Contract,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,VanderHouwen,"Austin, TX",1 week ago,55 applicants,"['', 'Nice To Have', 'Description', 'Data Engineer', 'Data Engineer Qualifications', 'Data Engineer Responsibilities', 'Must Have Proficient Knowledge In', 'About VanderHouwen']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Harnham,"New York, United States",1 week ago,91 applicants,"['', '$80-90/hr', '6-month w Extension', 'A client in the Apparel industry is looking for a Data Engineer, in this role, you will be helping manage all the engineering pieces around Data (e.g. feature pipelines, deployment, monitoring, etc)', 'Pyspark, bigdata hadoop, databricks', 'Remote', '5 years of relevant professional experienceBatch streaming is a must-have, streaming is a nice to havePyspark, bigdata hadoop, databricksExperience in using CI/CD pipelineExperience leveraging open sources big data processing frameworks, such as Apache Spark, Hadoop and streaming technologies such as Kafka', 'Data Engineer ', '5 years of relevant professional experience', 'Batch streaming is a must-have, streaming is a nice to have', 'Experience in using CI/CD pipeline', 'Experience leveraging open sources big data processing frameworks, such as Apache Spark, Hadoop and streaming technologies such as Kafka']",Mid-Senior level,Contract,Engineering,Staffing and Recruiting,2021-03-24 13:05:10
Data Engineer,HelloFresh,"York, IL",1 week ago,34 applicants,"['', 'Company sponsored outings & Employee Resource Groups', 'Implement ETLs monitoring automation', 'Snacks, cold brew on tap & monthly catered lunches', 'Able to develop an in-depth understanding of HelloFresh’s core product and architecture, and act as an ambassador for state of the art software solutions and industry best practices', 'Come see what’s cookin’ at HelloFresh!', 'Design and implement end-to-end data products and marketing automation flows: from data ingestions for data science modeling to creation of automated pipelines to external software (Salesforce, etc.)', 'Proficient in Python (with knowledge of OOP) and SQL (DDL, DML, CTEs, query optimization, ...). ', 'Past experience working with Apache Spark required', 'Data cleaning/enrichment: keeping data clean and consistent with production systems (e.g. bug fixes, backfills …)', ' Competitive Salary & 401k company match that vests immediately upon participation Generous parental leave of 16 weeks & PTO policy $0 monthly premium and other flexible health plans effective first day of employment 75% discount on your subscription to HelloFresh (as well as other product initiatives) Snacks, cold brew on tap & monthly catered lunches Company sponsored outings & Employee Resource Groups Collaborative, dynamic work environment within a fast-paced, mission-driven company ', 'Competitive Salary & 401k company match that vests immediately upon participation', 'BSc in a STEM discipline', 'You are ...', 'You will do ...', '$0 monthly premium and other flexible health plans effective first day of employment', 'Generous parental leave of 16 weeks & PTO policy', 'At a minimum, you have ...', 'The ability to design, implement and deliver maintainable and high-quality code using best practices (e.g. git/github, secrets, configurations, yaml/json)', "" Design and deploy cloud-based Data infrastructure (AWS, Databricks) Implement ETLs monitoring automation Help design, update and extend HelloFresh's data model (create new schemas, fact tables, mat views, joins, etc.) Data cleaning/enrichment: keeping data clean and consistent with production systems (e.g. bug fixes, backfills …) Design and implement end-to-end data products and marketing automation flows: from data ingestions for data science modeling to creation of automated pipelines to external software (Salesforce, etc.) Data Transformations: implement the logic of the data pipeline (aggregations, projections, selections, etc …) "", ' BSc in a STEM discipline 2+ years’ data engineering experience is required Proficient in Python (with knowledge of OOP) and SQL (DDL, DML, CTEs, query optimization, ...).  Past experience working with Apache Spark required Experience with end-to-end testing and general DevOps practices for data pipelines The ability to design, implement and deliver maintainable and high-quality code using best practices (e.g. git/github, secrets, configurations, yaml/json) Knowledge of data structures (DataFrames, RDDs, Dataclasses) and data formats (CSV, JSON, Parquet, Avro, ORC) Experience with software design patterns, and building highly scalable solutions preferred Experience with job orchestration tools like Airflow, Luigi or similar preferred ', 'An active, solution-oriented member of autonomous, cross-functional agile teams collaborating with Product Owners, Data Scientists, and Business Intelligence teams', '2+ years’ data engineering experience is required', 'Experience with software design patterns, and building highly scalable solutions preferred', ""Help design, update and extend HelloFresh's data model (create new schemas, fact tables, mat views, joins, etc.)"", '75% discount on your subscription to HelloFresh (as well as other product initiatives)', 'Knowledge of data structures (DataFrames, RDDs, Dataclasses) and data formats (CSV, JSON, Parquet, Avro, ORC)', ' An active, solution-oriented member of autonomous, cross-functional agile teams collaborating with Product Owners, Data Scientists, and Business Intelligence teams Able to develop an in-depth understanding of HelloFresh’s core product and architecture, and act as an ambassador for state of the art software solutions and industry best practices ', 'Experience with end-to-end testing and general DevOps practices for data pipelines', 'Collaborative, dynamic work environment within a fast-paced, mission-driven company', 'Design and deploy cloud-based Data infrastructure (AWS, Databricks)', 'Experience with job orchestration tools like Airflow, Luigi or similar preferred', 'Data Transformations: implement the logic of the data pipeline (aggregations, projections, selections, etc …)', 'You’ll get …', 'Job Description']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer,Hired Recruiters,"New York, NY",1 week ago,Be among the first 25 applicants,"['', 'Experience operating a production solution which supports the business', ' Help lead the team to build our data infrastructure and work with emerging technologies such as Snowflake and associated cloud services Design, implement, and operate stable, scalable, low cost solutions to flow data from production systems into the data warehouse and into end-user facing BI applications Implement processes and systems to monitor data quality, ensuring production data is always accurate and available Running machine learning experiments using best-in-class ML platforms Automate & optimize everything Leverage best in industry practices to build the next generation data ecosystem to collect, move, store and analyze data ', 'Leverage best in industry practices to build the next generation data ecosystem to collect, move, store and analyze data', 'You have worked with a variety of cloud and data solutions, such as: AWS / Azure / Google Cloud, SnowFlake, SnowPlow, Kafka, Segment.io, DataBricks, Trifecta, Hadoop, Spark, Airflow', 'What You’ll Do', 'Design, implement, and operate stable, scalable, low cost solutions to flow data from production systems into the data warehouse and into end-user facing BI applications', 'You have 5+ years of experience in the field of data engineering or related engineering experience', 'Help lead the team to build our data infrastructure and work with emerging technologies such as Snowflake and associated cloud services', 'Running machine learning experiments using best-in-class ML platforms', ' You have 5+ years of experience in the field of data engineering or related engineering experience You have recent experience building large-scale production data solutions You are familiar with data driven marketing and integrating into marketing automation solutions. You have a command of various programming languages to collect and manipulate data such as Python, R, and SQL. Experience operating a production solution which supports the business You have strong solution architecture skills and a passion for building data solutions that power the future business. You have worked with a variety of cloud and data solutions, such as: AWS / Azure / Google Cloud, SnowFlake, SnowPlow, Kafka, Segment.io, DataBricks, Trifecta, Hadoop, Spark, Airflow We are looking for people that will thrive in the collaborative environment while also being strong individual contributors with a proactive mindset. ', 'You have strong solution architecture skills and a passion for building data solutions that power the future business.', 'You have a command of various programming languages to collect and manipulate data such as Python, R, and SQL.', 'Automate & optimize everything', 'You are familiar with data driven marketing and integrating into marketing automation solutions.', 'Implement processes and systems to monitor data quality, ensuring production data is always accurate and available', 'We are looking for people that will thrive in the collaborative environment while also being strong individual contributors with a proactive mindset.', 'Who You Are', 'You have recent experience building large-scale production data solutions']",Mid-Senior level,Full-time,Engineering,Computer Software,2021-03-24 13:05:10
Data Engineer,TargetCW,"Seattle, WA",3 weeks ago,Be among the first 25 applicants,"[' Good to have: experience with Tableau ', ' Experience with building data warehouses and dimensional modeling ', ' Improve data discovery and literacy: Create documentation on data relationships and dependencies ', ' Estimated Duration: ', ' Strong proficiency in using one of the script languages, such as Python ', '$70-80/hr', ' Maintain a high level of trust in these data sets: keep existing data sources fresh against changing requirements, definitions, and data quality issues. ', ' Estimated Duration:  9 months with a possibility of extending up to 18 months total ', ' 3+ years of experience in data engineering ', 'Overview', ""MUST BE US CITIZEN, NO C2C** We are the world's leading live streaming platform for gamers and the things we love. We make it possible to watch, play and chat with millions of other fans from around the world. We are looking for a Data Engineer to join our team ASAP! If you have 3+ years of experience in Data Engineering along with Python and SQL, we want to speak to you! For this role, we’re looking for a data engineer to join the Creator Analytics team: Responsibilities Build ETL pipelines between our Salesforce instance and our data warehouse  Maintain a high level of trust in these data sets: keep existing data sources fresh against changing requirements, definitions, and data quality issues.  Improve reliability and performance: Conduct unit, integration, and system tests on our data sources in order to validate data against source systems, and continuously optimize performance in order to improve query speed and reduce cost.  Improve data discovery and literacy: Create documentation on data relationships and dependencies Requirements 3+ years of experience in data engineering  Strong proficiency in using one of the script languages, such as Python  Strong proficiency in SQL  Familiarity with AWS services such as Redshift and S3  Experience using cloud services to automate the data pipelines  Experience with building data warehouses and dimensional modeling  Experience with best practices for development including query optimization, version control, code reviews, and documentation  Experience with Salesforce backends  Fluency with a Git/GitHub version control workflow  Good to have: experience with Tableau  Estimated Duration:  9 months with a possibility of extending up to 18 months total PLEASE SUBMIT YOUR RESUME TO BE CONSIDERED!#4"", 'Data Engineer (W2 ONLY)', ' Fluency with a Git/GitHub version control workflow ', 'Requirements', ' Experience using cloud services to automate the data pipelines ', 'Remote', ' Strong proficiency in SQL ', ' Experience with best practices for development including query optimization, version control, code reviews, and documentation ', ' Familiarity with AWS services such as Redshift and S3 ', ' Experience with Salesforce backends ', ' 3+ years of experience in data engineering  Strong proficiency in using one of the script languages, such as Python  Strong proficiency in SQL  Familiarity with AWS services such as Redshift and S3  Experience using cloud services to automate the data pipelines  Experience with building data warehouses and dimensional modeling  Experience with best practices for development including query optimization, version control, code reviews, and documentation  Experience with Salesforce backends  Fluency with a Git/GitHub version control workflow  Good to have: experience with Tableau ', ' Build ETL pipelines between our Salesforce instance and our data warehouse ', 'Full Time', "" We are the world's leading live streaming platform for gamers and the things we love. We make it possible to watch, play and chat with millions of other fans from around the world. We are looking for a Data Engineer to join our team ASAP! If you have 3+ years of experience in Data Engineering along with Python and SQL, we want to speak to you! "", ' Improve reliability and performance: Conduct unit, integration, and system tests on our data sources in order to validate data against source systems, and continuously optimize performance in order to improve query speed and reduce cost. ', ' Build ETL pipelines between our Salesforce instance and our data warehouse  Maintain a high level of trust in these data sets: keep existing data sources fresh against changing requirements, definitions, and data quality issues.  Improve reliability and performance: Conduct unit, integration, and system tests on our data sources in order to validate data against source systems, and continuously optimize performance in order to improve query speed and reduce cost.  Improve data discovery and literacy: Create documentation on data relationships and dependencies ']",Entry level,Contract,Engineering,Computer Software,2021-03-24 13:05:10
Data Engineer,EyeCare Partners,"Ballwin, MO",1 week ago,Be among the first 25 applicants,"['', 'Position Summary:', ""Bachelor's degree in Computer Science, Information Systems or equivalent + 3 years related experience3+ years of hands-on-experience in the design, development, and implementation of data solutionsAdvanced SQL knowledge with strong query writing, stored procedures skillsExperience with Snowflake development and supportExperience with object-oriented/object function scripting languages: Python, Java, Scala, etc.Experience with AWS cloud services: EC2, EMR, RDS, DMSExperience with relational databases such as SQL Server and object relational databases such as PostgreSQLExperience with stream-processing systems: Storm, Spark-Streaming, etc.Experience with data analysis, ETL, and workflow automationExperience working with multiple ETL/ELT tools and cloud based data hubsDemonstrated problem solvingDemonstrated ability to think and work with a proactive mindsetA self-motivated personality with a passion for working in a fast-paced environment"", 'Coordinate the build and maintenance of data pipelines by third party service providers', 'Present solutions and options to leadership, project teams and other stakeholders adapting style to both technical and non-technical audiences', '3+ years of hands-on-experience in the design, development, and implementation of data solutions', 'Educate organization on available and emerging tool sets', 'Proactively monitor and resolve on-going production issues', 'Requirements:', 'Experience with data analysis, ETL, and workflow automation', ""Bachelor's degree in Computer Science, Information Systems or equivalent + 3 years related experience"", 'Work with data and business analysts to deploy and support a robust data cataloging strategy', 'Essential Responsibilities:', 'Analyze data elements from various systems, data flow, dependencies, relationships and assist in designing conceptual physical and logical data models', 'Enabling and executing data migrations across systems (e.g. SQL server to Snowflake or other cloud data platforms)', 'Experience with AWS cloud services: EC2, EMR, RDS, DMS', 'Experience with relational databases such as SQL Server and object relational databases such as PostgreSQL', 'Work with various business and technical stakeholders and assist with data-related technical needs and issues', 'Ensures teams adhere to documented design and development patterns and standards', 'Data Engineer – Information Technology', 'Demonstrated problem solving', 'This role will support our software engineers, data architects, data analysts and data scientists on various enterprise data initiatives and will ensure SLA based data delivery. The ideal candidate will be excited by the opportunity to design & build our company’s data architecture to support our next generation of data and analytics solutions.', 'Experience with Snowflake development and support', 'Work with data and analytics teams and drive greater value from our data and analytics investments', 'Work closely with various technical teams to ensure consistency, quality of solutions and knowledge sharing across the enterprise', 'Design, build and maintain data pipelines from various source systems into SnowflakeAnalyze data elements from various systems, data flow, dependencies, relationships and assist in designing conceptual physical and logical data modelsDesign, build and maintain complex data sets designed to meet various business needs in the areas of reporting, advanced analytics and ad-hoc analysisCoordinate the build and maintenance of data pipelines by third party service providersEnabling and executing data migrations across systems (e.g. SQL server to Snowflake or other cloud data platforms)Development and implementation of scripts for datahub maintenance, monitoring, performance tuningWork with data and business analysts to deploy and support a robust data quality platformWork with data and business analysts to deploy and support a robust data cataloging strategyWork with various business and technical stakeholders and assist with data-related technical needs and issuesWork with data and analytics teams and drive greater value from our data and analytics investmentsWork closely with cross-functional teams to understand and transform business requirements into scalable and manageable solutionsPresent solutions and options to leadership, project teams and other stakeholders adapting style to both technical and non-technical audiencesEnsures teams adhere to documented design and development patterns and standardsProactively monitor and resolve on-going production issuesWork closely with various technical teams to ensure consistency, quality of solutions and knowledge sharing across the enterpriseEducate organization on available and emerging tool setsEnsure adherence to the approach of self-service data solutions and enable other teams with analytics solutions delivery via ‘Data as a Service’ model', 'Experience working with multiple ETL/ELT tools and cloud based data hubs', 'Ensure adherence to the approach of self-service data solutions and enable other teams with analytics solutions delivery via ‘Data as a Service’ model', 'Development and implementation of scripts for datahub maintenance, monitoring, performance tuning', 'Advanced SQL knowledge with strong query writing, stored procedures skills', 'Design, build and maintain complex data sets designed to meet various business needs in the areas of reporting, advanced analytics and ad-hoc analysis', 'Experience with object-oriented/object function scripting languages: Python, Java, Scala, etc.', 'Work with data and business analysts to deploy and support a robust data quality platform', 'Experience with stream-processing systems: Storm, Spark-Streaming, etc.', 'Design, build and maintain data pipelines from various source systems into Snowflake', 'Demonstrated ability to think and work with a proactive mindset', 'A self-motivated personality with a passion for working in a fast-paced environment', 'Work closely with cross-functional teams to understand and transform business requirements into scalable and manageable solutions', 'Position Summary:\xa0We are seeking an experienced professional who will serve as the Data Engineer in our Data & Analytics that is deploying some of the modern data platforms and analytics tools. The Data Engineer will be responsible and accountable for expanding and optimizing our data and data pipeline architecture, as well as optimizing data collection. ']",Associate,Full-time,Information Technology,Medical Practice,2021-03-24 13:05:10
Data Engineer (PENA),Panasonic North America,"Sparks, NV",6 days ago,Be among the first 25 applicants,"['', 'Supplemental Information', 'Experience building and optimizing data models, data structures and meta-dataHands-on experience with scripting and programming languages like Python, R, GoLang, Julia, Java, C++Hands-on experience with relational NoSQL and SQL databases like Oracle, Postgres, MySQL, CassandraHands-on experience with software and tools like Kafka, Spark, Hadoop, Tableau, JMPHands-on experience with workflow management and pipeline tools like Airflow, Luigi and AzkabanExcellent analytical and problem-solving & communication skillsAbility to identify and analyze data patterns and quality discrepanciesExperience leading projects and coordinating with cross-functional teamsExperience documenting metrics and interpreting them as indicators of data quality.', 'Data engineering certification (e.g IBM Certified Data Engineer) is a plus', 'Identifying, designing and implementing internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes', 'Knowledge of Windows and Linux in Desktop and Server environment is a plus', 'Preferred Qualifications', 'Education And Training', 'Hands-on experience with relational NoSQL and SQL databases like Oracle, Postgres, MySQL, Cassandra', 'Build data systems and pipelines', 'Degree in Computer Science, Information Systems, Statistics or similar field; Master’s is a plus', 'Experience building and optimizing data models, data structures and meta-data', 'Roles And Responsibilities', 'Experience documenting metrics and interpreting them as indicators of data quality.', 'Prepare data for prescriptive and predictive modeling', 'Job Summary', 'Identifying, designing and implementing internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processesExplore and build algorithms and design and test prototypes to enhance data quality, efficiency and reliabilityCollaborate with data scientists and other engineering and operations teams on several projectsDevelop, construct, test and maintain database infrastructure and architecturesPrepare data for prescriptive and predictive modelingEvaluate business needs and objectives and identify opportunities for data acquisitionAnalyze and organize raw data from different sources to meet business requirementsBuild data systems and pipelinesWorking with engineering, operations and executive teams and assisting them with data-related technical issues and infrastructure needs', '4+ years as a Data Engineer or an equivalent combination of education and experience.', 'Good understanding of various models and phases of SDLCExperience working in an Agile environmentHands-on experience with Docker and Kubernetes is a plusKnowledge of Windows and Linux in Desktop and Server environment is a plus', 'Develop, construct, test and maintain database infrastructure and architectures', 'Experience working in an Agile environment', 'Evaluate business needs and objectives and identify opportunities for data acquisition', 'Ability to identify and analyze data patterns and quality discrepancies', 'About Us', 'Hands-on experience with scripting and programming languages like Python, R, GoLang, Julia, Java, C++', 'Analyze and organize raw data from different sources to meet business requirements', 'Good understanding of various models and phases of SDLC', 'Personal Protective Equipment (PPE) Requirements', 'Degree in Computer Science, Information Systems, Statistics or similar field; Master’s is a plusData engineering certification (e.g IBM Certified Data Engineer) is a plus4+ years as a Data Engineer or an equivalent combination of education and experience.', 'Excellent analytical and problem-solving & communication skills', 'Working with engineering, operations and executive teams and assisting them with data-related technical issues and infrastructure needs', 'Collaborate with data scientists and other engineering and operations teams on several projects', 'Explore and build algorithms and design and test prototypes to enhance data quality, efficiency and reliability', 'Hands-on experience with software and tools like Kafka, Spark, Hadoop, Tableau, JMP', 'Essential Qualifications', 'Hands-on experience with workflow management and pipeline tools like Airflow, Luigi and Azkaban', 'Experience leading projects and coordinating with cross-functional teams', 'Hands-on experience with Docker and Kubernetes is a plus']",Entry level,Full-time,Information Technology,Construction,2021-03-24 13:05:10
Data Engineer - Marketing Data Eng team,Disney Streaming Services,"New York, NY",1 week ago,28 applicants,"['', 'Familiarity with binary data serialization formats such as Parquet, Avro, and ThriftExperience deploying data notebook and analytic environments such as Jupyter and DatabricksFamiliarity with deploying and running AWS-based data solutionsExperience with graph-based data workflows using Apache AirflowKnowledge of marketing technology stacks', 'Knowledge of marketing technology stacks', 'Loading and querying cloud-hosted databases such as Snowflake/Redshift', 'Preferred Qualifications', '2-3 years of experience developing in object oriented PythonFamiliar with big-data solutions using technologies such as Databricks, EMR, S3, and SparkLoading and querying cloud-hosted databases such as Snowflake/RedshiftBuilding data pipelines using Kinesis, Kafka, Spark, or Flink', 'Building data pipelines using Kinesis, Kafka, Spark, or Flink', 'Job Summary', 'Experience deploying data notebook and analytic environments such as Jupyter and Databricks', 'Work in an Agile environment that focuses on collaboration and teamwork', 'Familiarity with binary data serialization formats such as Parquet, Avro, and Thrift', 'Build and deploy streaming and batch data pipelines in support of the Disney+ marketing ecosystem to enable data activation and segmentation capabilities across paid media and lifecycle engagement marketing.', 'Familiarity with deploying and running AWS-based data solutions', 'Responsibilities', 'Build integrations with paid media platforms to optimize marketing spend through data and analytics', 'Experience with graph-based data workflows using Apache Airflow', '2-3 years of experience developing in object oriented Python', 'Familiar with big-data solutions using technologies such as Databricks, EMR, S3, and Spark', 'Basic Qualifications', 'Integrate with a variety of data providers ranging from upstream data services to 3rd party data solutions.', 'Collaborate with product, marketing, and analytics teams to build data-forward solutions to subscriber growth and retention', 'Develop unit/integration tests and data validations to ensure the quality of code and data', 'Build and deploy streaming and batch data pipelines in support of the Disney+ marketing ecosystem to enable data activation and segmentation capabilities across paid media and lifecycle engagement marketing.Integrate with a variety of data providers ranging from upstream data services to 3rd party data solutions.Collaborate with product, marketing, and analytics teams to build data-forward solutions to subscriber growth and retentionBuild integrations with paid media platforms to optimize marketing spend through data and analyticsDevelop unit/integration tests and data validations to ensure the quality of code and dataWork in an Agile environment that focuses on collaboration and teamwork']",Mid-Senior level,Full-time,Quality Assurance,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer,RemoteHub,"Denver, CO",2 weeks ago,Be among the first 25 applicants,"['', ' Data Studio Job', ' SQL Azure, Synapse (SQL Datawarehouse)', 'Solid Familiarity On The Following Tools', ' Azure Data Lake, Blob Storage', ' Develop and unit test assigned features to meet product requirements.', ' Expert knowledge of SQL and of relational database systems and concepts.', ' Azure Data Factory', ' Understand business processes, logical data models and relational database implementations for data analysis.', ' Experience designing and building scalable and robust data pipelines to enable data-driven decisions for the business.', ' PowerBI', ' Develop, implement and tune ETL processes.', ' Ensure documentation of all project artefacts are accurate and current.', ' Develop and implement solutions for data quality validation and continuous improvement.', ' Drive our data platform and help evolve our technology stack and development best practices', ' Databricks', ' Gathering technical requirement from customer and enable the right team to develop and implement it. Experience designing and building scalable and robust data pipelines to enable data-driven decisions for the business. Understand business processes, logical data models and relational database implementations for data analysis. Build data expertise and implement own data quality test cases for required areas. Expert knowledge of SQL and of relational database systems and concepts. Expert knowledge in Azure Data Factory, Databricks Script, Scope Scripts, Cosmos etc. Demonstrated strength in data modeling, ETL development, and data warehousing Develop, implement and tune ETL processes. Experience analyzing data to discover opportunities and address gaps. Develop and maintain data pipelines including solutions for data collection, management, and usage. Develop and implement solutions for data quality validation and continuous improvement. Drive our data platform and help evolve our technology stack and development best practices Develop and unit test assigned features to meet product requirements. Working knowledge of data quality approaches and techniques. Programming language experience (C#, Python, Scala Spark.) is a plus. Build visualizations in PowerBI to help derive meaningful insights from data. Maintain and enhance our data and computation platform up and running. Accountable for ensuring that project tasks are accomplished to schedule at acceptable level of quality; or that appropriate contingencies are implemented. Ensure documentation of all project artefacts are accurate and current. Ensure all projects are delivered successfully and according to Engagement statement of work process, methodology, and quality standards.', ' Accountable for ensuring that project tasks are accomplished to schedule at acceptable level of quality; or that appropriate contingencies are implemented.', ' Experience analyzing data to discover opportunities and address gaps.', ' Build data expertise and implement own data quality test cases for required areas.', ' Build visualizations in PowerBI to help derive meaningful insights from data.', ' Ensure all projects are delivered successfully and according to Engagement statement of work process, methodology, and quality standards.', ' Gathering technical requirement from customer and enable the right team to develop and implement it.', ' Data Quality management. Test Cases etc.', ' Working knowledge of data quality approaches and techniques.', ' SQL Azure, Synapse (SQL Datawarehouse) Azure Data Factory Kusto Scope Scripts, Cosmos PowerBI Azure Data Lake, Blob Storage Databricks Data Quality management. Test Cases etc. Data Studio Job', ' Demonstrated strength in data modeling, ETL development, and data warehousing', ' Expert knowledge in Azure Data Factory, Databricks Script, Scope Scripts, Cosmos etc.', ' Kusto', ' Apply for this Data Engineer position', ' Maintain and enhance our data and computation platform up and running.', ' Scope Scripts, Cosmos', ' Develop and maintain data pipelines including solutions for data collection, management, and usage.', ' Programming language experience (C#, Python, Scala Spark.) is a plus.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Junior Data Engineer (AWS),Invesco Ltd.,"Atlanta, GA",3 weeks ago,29 applicants,[''],Associate,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Senior Data Engineer,Varo Bank,"San Francisco, CA",20 hours ago,Be among the first 25 applicants,"['', ""Previous Experiences That'll Help You Be Great"", 'CULTURAL ALIGNMENT', 'Conduct code reviews in accordance with team processes and standards', 'Experience with other AWS components like Cloudwatch, EKS, KMS, Lamdas, S3 is a strong plus', ""We're Looking For Someone Who Will"", 'Provide technical leadership in the area of data systems development including data ingestion, data curation, data storage, high-throughput data processing and analytics', ""Bachelor's degree in Computer Science, MIS, Engineering or related field, or relevant work experience5-7 years of data modeling, data pipelines, data lake, data warehouse experience.5+ years programming experience in Python (will also consider Java, Kotlin, C, C++)5+ years’ experience working within the AWS Big Data/Hadoop Ecosystem (EMR, Glue and Athena)Experience with other AWS components like Cloudwatch, EKS, KMS, Lamdas, S3 is a strong plusExperience with downstream consumption patterns (reports, dashboards) is a plusExperience in Hadoop, HDFS, Hive, Presto, REST/SOAP API, Spark2, Airflow is a plus"", 'Participate in developing and enforcing data security & access control policies', 'Stay Curious:', 'Develop and maintain the data strategy for Varo in terms of capabilities and control mechanisms that support company responsibilities as a regulated national bank', 'Take Ownership:', 'OUR CORE VALUES', 'Make it Better:', 'Beware of fraudulent job postings!', '5+ years’ experience working within the AWS Big Data/Hadoop Ecosystem (EMR, Glue and Athena)', ""Bachelor's degree in Computer Science, MIS, Engineering or related field, or relevant work experience"", 'About Varo', 'Work with business partners on requirements, clarification and results', '5-7 years of data modeling, data pipelines, data lake, data warehouse experience.', 'About The Data Engineering Role', 'Develop effective controls for a resilient data ingestion process', 'Support application data integration design and build efforts, including real-time capabilities', 'Experience in Hadoop, HDFS, Hive, Presto, REST/SOAP API, Spark2, Airflow is a plus', 'Work with AWS big data technologies including EMR, Glue, S3, EKS, Lambda, Athena, RDS', 'Design, build and maintain Varo’s data pipelines to process data into and out of Varo’s Data Lake', 'Design, build and maintain Varo’s data pipelines to process data into and out of Varo’s Data LakeWork with AWS big data technologies including EMR, Glue, S3, EKS, Lambda, Athena, RDSDevelop and maintain the data strategy for Varo in terms of capabilities and control mechanisms that support company responsibilities as a regulated national bankProvide technical leadership in the area of data systems development including data ingestion, data curation, data storage, high-throughput data processing and analyticsWork with business partners on requirements, clarification and resultsParticipate in developing and enforcing data security & access control policiesDevelop effective controls for a resilient data ingestion processSupport application data integration design and build efforts, including real-time capabilitiesConduct code reviews in accordance with team processes and standards', 'Learn More About Varo By Following Us', '5+ years programming experience in Python (will also consider Java, Kotlin, C, C++)', 'Experience with downstream consumption patterns (reports, dashboards) is a plus', 'Customers First:', 'Respect:']",Associate,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Senior Data Engineer,Cerner Corporation,"Kansas City, MO",21 hours ago,Be among the first 25 applicants,"['', ' 1 year of data engineering and cloud technology work experience including data analysis, data ingestion, data modeling and/or machine learning ', ' Experience working with big data and big data technologies ', ' Perform other responsibilities as assigned', ' Experience with Java, Python and Scala ', ' Develop and document the strategy to maintain and support the platform, pipelines and tooling ', ' Bachelor’s degree in Computer Science, Computer Engineering or Information Systems or Software Engineering or related field, or equivalent relevant work experience ', ' Participate in architecture discussions, design sessions and code reviews to review for functional correctness, architectural maintainability and performance ', ' Expectations : ', ' Bachelor’s degree in Computer Science, Computer Engineering or Information Systems or Software Engineering or related field, or equivalent relevant work experience  5 years of Software engineering work experience  1 year of data engineering and cloud technology work experience including data analysis, data ingestion, data modeling and/or machine learning ', ' Collaborate with strategists and data scientists to influence requirements and design prototypes ', ' Must be currently residing in or willing to relocate to the Kansas City metro area ', ' Participate in architecture discussions, design sessions and code reviews to review for functional correctness, architectural maintainability and performance  Collaborate with strategists and data scientists to influence requirements and design prototypes  Use big data tools to design batch and streaming feature pipelines and pipelines to populate a data lake  Create clear, well-constructed strategic technical designs for large or complex scope projects with assistance  Estimate work effort for current and future projects of large or complex scope  Develop and document the strategy to maintain and support the platform, pipelines and tooling  Share knowledge and skills across the team through mentorship, coaching, technical talks and blogs ', ' Use big data tools to design batch and streaming feature pipelines and pipelines to populate a data lake ', 'Qualifications', ' Basic Qualifications : ', ' Experience with Java, Python and Scala  Experience working with big data and big data technologies ', ' Must be currently residing in or willing to relocate to the Kansas City metro area  Willing to work additional or irregular hours as needed and allowed by local regulations  Work in accordance with corporate and organizational security policies and procedures, understand personal role in safeguarding corporate and client assets, and take appropriate action to prevent and report any compromises of security within scope of position  Perform other responsibilities as assigned', ' Create clear, well-constructed strategic technical designs for large or complex scope projects with assistance ', ' Estimate work effort for current and future projects of large or complex scope ', ' 5 years of Software engineering work experience ', ' Work in accordance with corporate and organizational security policies and procedures, understand personal role in safeguarding corporate and client assets, and take appropriate action to prevent and report any compromises of security within scope of position ', ' Preferred Qualifications : ', ' Share knowledge and skills across the team through mentorship, coaching, technical talks and blogs ', ' Willing to work additional or irregular hours as needed and allowed by local regulations ']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Senior Data Engineer,The Athletic,"United, LA",4 weeks ago,Be among the first 25 applicants,"['', ""Minimum 4 years data engineering experience.Bachelor's degree in CS or relevant discipline.Experience with Python, SQL, Docker, and cloud environment - AWS, GCP or Azure.Experience with Redshift, Kubernetes, and Spark desired.Experience in orchestrating/scheduling data pipelines.Proficient in source code control systems.Able to communicate results, outcomes and issues to different audiences.The current team is largely based in San Francisco, CA, but this role can be based in a 100% remote capacity within the United States or Canada."", 'Build tools and data marts to enable analytics.', ""Bachelor's degree in CS or relevant discipline."", 'Investigate, debug, and fix user-reported production issues.', ' About The Role ', ' Responsibilities ', 'Able to communicate results, outcomes and issues to different audiences.', 'Identify and fix issues and reduce tech debt.', 'Experience in orchestrating/scheduling data pipelines.', 'The current team is largely based in San Francisco, CA, but this role can be based in a 100% remote capacity within the United States or Canada.', 'Work with Data Science, and Product teams to build and deploy features backed by machine learning and modern data toolsets.', 'Architect and build data pipelines to optimize for performance, data quality, scalability, ease of future development, and cost.', 'Work with Data Science, and Product teams to build and deploy features backed by machine learning and modern data toolsets.Architect and build data pipelines to optimize for performance, data quality, scalability, ease of future development, and cost.Build tools and data marts to enable analytics.Identify and fix issues and reduce tech debt.Investigate, debug, and fix user-reported production issues.', 'Experience with Redshift, Kubernetes, and Spark desired.', 'Experience with Python, SQL, Docker, and cloud environment - AWS, GCP or Azure.', 'Minimum 4 years data engineering experience.', 'About Us ', 'Proficient in source code control systems.']",Associate,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer - Washington,Mobomo,"Washington, DC",2 weeks ago,Be among the first 25 applicants,"['', 'Support the creation of a cloud based data management Infrastructure through the development of data lakes and a data warehouse. You will be involved in all aspects of the modeling process, including data discovery, data quality, and logical/physical modeling. The analyst should have experience with particular Informatica tools (EDC, IDQ, and Axon) in an Azure Cloud environment, as well as other Azure based tools such as Polybase and Synapse. ', 'Must be able to communicate technical requirements across multiple organizations clearly and concisely', 'Active Secret clearance required', 'Prior experience with federal government agencies ', 'Must have at least three years of experience in the following:Data Warehouse Tools Data Audit and Profiling Data Warehouse ETL Testing Informatica Axon Informatica Data Quality Informatica Enterprise Data CatalogActive Secret clearance requiredPrior experience with federal government agencies ', 'About Mobomo', 'Data Warehouse ETL Testing ', 'Data Audit and Profiling ', 'Informatica Axon ', 'Responsibilities', 'Data Warehouse Tools ', 'Informatica Data Quality ', 'Data Warehouse Tools Data Audit and Profiling Data Warehouse ETL Testing Informatica Axon Informatica Data Quality Informatica Enterprise Data Catalog', 'Must have at least three years of experience in the following:Data Warehouse Tools Data Audit and Profiling Data Warehouse ETL Testing Informatica Axon Informatica Data Quality Informatica Enterprise Data Catalog', 'Requirements', 'Informatica Enterprise Data Catalog', 'Soft Skills']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer,DISH Network,"Englewood, CO",2 weeks ago,Be among the first 25 applicants,"['', ' 3+ years of experience working with distributed data technologies (e.g. Spark, Kafka, Flink etc) for building efficient, large-scale ‘big data’ pipelines; ', ' Work with product vendors to identify and manage open product issues ', ' Minimum of 5 years of experience delivering data solutions on a variety of data warehousing, big data and cloud data platforms. ', ' Experience supporting and working with cross-functional teams in a dynamic environment ', ' Excellent written/verbal communication skills. ', ' Lead teams in design, development and delivery of data solutions in wireless space using cloud data platforms ', 'Technical Requirements', ' Experience with change data capture tools (CDC) preferred such as Attunity/goldengate ', ' Experience with building streaming ingestion pipleline using Kafka streams, Apache Flink, or others ', ' Experience with Cloud Computing platforms like Amazon AWS, Google Cloud etc. ', ' Strong Software Engineering experience with proficiency in at least one of the following programming languages: Spark, Python, Scala or equivalent ', ' Work with product owners and technical leads to lead technical discussions and resolve technical issues ', ' Experience with building data ingestion pipelines both real time and batch using best practices ', ""What You'll Do"", ' Strong interpersonal, analytical, problem-solving, influencing, prioritization, decision- making and conflict resolution skills ', ' Apply best practices of data integration for data quality and automation ', ' Solve complex data integration problems ', ' Minimum of 5 years of experience delivering data solutions on a variety of data warehousing, big data and cloud data platforms.  3+ years of experience working with distributed data technologies (e.g. Spark, Kafka, Flink etc) for building efficient, large-scale ‘big data’ pipelines;  Strong Software Engineering experience with proficiency in at least one of the following programming languages: Spark, Python, Scala or equivalent  Experience with building data ingestion pipelines both real time and batch using best practices  Experience with building streaming ingestion pipleline using Kafka streams, Apache Flink, or others  Experience with Cloud Computing platforms like Amazon AWS, Google Cloud etc.  Experience supporting and working with cross-functional teams in a dynamic environment  Experience with relational SQL and NoSQL databases, including Postgres, and Mongodb.  Experience with change data capture tools (CDC) preferred such as Attunity/goldengate  Experience with scheduling tools preferrable Control-M,Airflow or AWS Step functions.  Strong interpersonal, analytical, problem-solving, influencing, prioritization, decision- making and conflict resolution skills  Excellent written/verbal communication skills. ', ' Manage day-to-day development activities for new data solutions and troubleshooting existing implementations. ', ' Develop and maintaining code for data ingestion and curation using databricks ', ' Experience with relational SQL and NoSQL databases, including Postgres, and Mongodb. ', ' Experience with scheduling tools preferrable Control-M,Airflow or AWS Step functions. ', ' Lead teams in design, development and delivery of data solutions in wireless space using cloud data platforms  Manage day-to-day development activities for new data solutions and troubleshooting existing implementations.  Work with product owners and technical leads to lead technical discussions and resolve technical issues  Apply best practices of data integration for data quality and automation  Work with product vendors to identify and manage open product issues  Solve complex data integration problems  Develop and maintaining code for data ingestion and curation using databricks  Work with business analysts to understand business requirements and use cases', ' Work with business analysts to understand business requirements and use cases']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Global Technical Talent,"San Francisco, CA",6 days ago,Be among the first 25 applicants,"['Help to define architecture based on technology and business needsWrite well-crafted, well-tested, readable, maintainable codeParticipate in code reviews to ensure code quality and distribute knowledgeCan help lead large projects from idea to positive executionAct on feedback to learn and growUnblock, support and effectively communicate across teams to achieve resultsAbility to operate independently including designing, implementing and testing.', 'Help to define architecture based on technology and business needs', 'Write well-crafted, well-tested, readable, maintainable code', 'Must have 9+ years of software engineering industry experienceExperience with object-oriented programming language - Python preferredExperience with distributed systemsExperience working with databases, relational or NoSQLAbility to thrive in a startup environmentAbility to write thorough, scalable and clear design documentationDesigns, builds and improves a set of team owned components', 'Unblock, support and effectively communicate across teams to achieve results', 'Ability to write thorough, scalable and clear design documentation', '\xa0Experience & Skills:', 'Job ', 'Must have 9+ years of software engineering industry experience', 'Can help lead large projects from idea to positive execution', 'Responsibilities:', 'Ability to operate independently including designing, implementing and testing.', 'Experience with distributed systems', 'Designs, builds and improves a set of team owned components', '\xa0', 'Participate in code reviews to ensure code quality and distribute knowledge', 'Experience & Skills:', 'Experience working with databases, relational or NoSQL', 'Ability to thrive in a startup environment', 'Data Engineer', 'Experience with object-oriented programming language - Python preferred', 'Job Responsibilities:', 'Act on feedback to learn and grow']",Entry level,Contract,Information Technology,Insurance,2021-03-24 13:05:10
Data Engineer/Analyst,"ALTA IT Services, LLC","Baltimore, MD",1 week ago,Be among the first 25 applicants,"['', 'Expert in designing complex and semantically rich data structures.', 'Location', 'Position Description', 'Worked with OLTP/data warehouse modeling, and play key role in multiple projects and act as knowledge expert (Technical, Functional).', 'Extensively used ETL methodologies for supporting data extraction, transformations and loading processing, in a corporate-wide-ETL Solution.', 'Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.', 'Ability to understand complex business processes to derive conceptual and logical data models.', 'Education', ' Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements Able to work independently with BA and PM to gather requirements, perform data discovery, come up with data model, work with data engineers to build data pipeline, and work with developers to answer their questions. Extensively used ETL methodologies for supporting data extraction, transformations and loading processing, in a corporate-wide-ETL Solution. Expertise in Conceptual Data Modeling, Logical Data Modeling, Physical Data Modeling, Enterprise Data Warehouse Design, DataMart Design, Metadata, Data Quality, Master Data Management and Master Data/Data Governance using Erwin Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics. Ability to understand complex business processes to derive conceptual and logical data models. Lead complex discussions and engagements that may involve multiple project teams from client. Experience designing relational data models, analytical data models and non-relational data models for OLTP and advanced analytic applications. Expert in designing complex and semantically rich data structures. Worked with OLTP/data warehouse modeling, and play key role in multiple projects and act as knowledge expert (Technical, Functional). Ability to optimize and performance tune SQL queries Good data analysis, problem solving and SQL skills.', 'Good data analysis, problem solving and SQL skills.', 'Expertise in Conceptual Data Modeling, Logical Data Modeling, Physical Data Modeling, Enterprise Data Warehouse Design, DataMart Design, Metadata, Data Quality, Master Data Management and Master Data/Data Governance using Erwin', 'Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements', 'Lead complex discussions and engagements that may involve multiple project teams from client.', 'Ability to optimize and performance tune SQL queries', 'Able to work independently with BA and PM to gather requirements, perform data discovery, come up with data model, work with data engineers to build data pipeline, and work with developers to answer their questions.', 'Skills Requirements', 'Experience designing relational data models, analytical data models and non-relational data models for OLTP and advanced analytic applications.', 'Clearance']",Entry level,Contract,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,The Select Group,"Charlotte, NC",7 days ago,Be among the first 25 applicants,"['', 'Experience with end-to -end solutions', 'DATA ENGINEER- REMOTE', 'Experience leading and supporting a product team will be vital for the success of this individual.', ' Able to ingest and digress data from Hadoop ', 'Knowledge and or experience developing POC (Proof of concepts) Able to ingest and digress data from Hadoop  ', 'Experience using API Interfaces', '80% of the their time will be coding and doing hands on development within the Hadoop ecosystem using Spark or Scala', ' Tools include: Spark, Hive, Oozie workflows (ability to create and modify these workflows), Horton Works experience ', 'Daily Responsibilities', ' 5 years of experience in Data Engineering or hadoop development Tools include: Spark, Hive, Oozie workflows (ability to create and modify these workflows), Horton Works experience   In depth knowledge of developing code in Spark (1.6 and 2.0) or Scala (these are preferred but experience in R and Python is okay too) for machine learning Experience using API Interfaces Knowledge and or experience developing POC (Proof of concepts) Able to ingest and digress data from Hadoop   Experience with end-to -end solutions ', 'Able to ingest and digress data from Hadoop', '5 years of experience in Data Engineering or hadoop development Tools include: Spark, Hive, Oozie workflows (ability to create and modify these workflows), Horton Works experience  ', 'In depth knowledge of developing code in Spark (1.6 and 2.0) or Scala (these are preferred but experience in R and Python is okay too) for machine learning', '15% of their time will be facilitating the direction of development for the team', '5% of their time will be communicating with product owner and stakeholders .', 'This Tech Lead/ Sr Data Engineer will be responsible for working with a team supporting a critical product.', ' This Tech Lead/ Sr Data Engineer will be responsible for working with a team supporting a critical product. 80% of the their time will be coding and doing hands on development within the Hadoop ecosystem using Spark or Scala 15% of their time will be facilitating the direction of development for the team 5% of their time will be communicating with product owner and stakeholders . Experience leading and supporting a product team will be vital for the success of this individual.', 'Tools include: Spark, Hive, Oozie workflows (ability to create and modify these workflows), Horton Works experience', 'Must-have Skills']",Not Applicable,Full-time,Information Technology,Computer Networking,2021-03-24 13:05:10
Data Engineer,Keyrus,New York City Metropolitan Area,5 days ago,28 applicants,"['', 'Any understanding of IT processes and systems (like Alteryx server) is highly preferred', 'At least 3+ years working with ETL related technologies such as Alteryx, SSIS, Talend, ODI, and etc.Strong knowledge of data manipulation, data blending, and wrangling, with experience in data querying (if you can write complex queries and nested queries, that’d be great!)Strong knowledge of relational databases such as MSSQL and Oracle\xa0Any understanding of IT processes and systems (like Alteryx server) is highly preferredMinimum of a Bachelor’s degree in Computer Science, Information Systems, IT, or a related field', 'Strong knowledge of data manipulation, data blending, and wrangling, with experience in data querying (if you can write complex queries and nested queries, that’d be great!)', 'Although the role will be utilizing Alteryx, we do not require Alteryx experience. We are looking for candidates who have strong backend/database backgrounds (i.e. SQL Developers, Database Analysts, ETL Developers) and are passionate about data and are excited to learn new technologies!', 'At least 3+ years working with ETL related technologies such as Alteryx, SSIS, Talend, ODI, and etc.', 'We are looking for a Data Engineer to work with our client, who is one of the largest food and beverage retailers in the U.S.\xa0The role will be responsible for continuing to develop the analytics infrastructure for the company’s various business teams from finance to supply chain, as they revamp and migrate their legacy systems to Alteryx. As there are no dedicated business analysts on this team, this role will wear multiple hats and work closely with business stakeholders to gather requirements and translate business logic into technical specifications for development.', '**This is a full time role with our client (not Keyrus). We are unfortunately unable to work with candidates who now or in the future will require work sponsorship.**', 'Minimum of a Bachelor’s degree in Computer Science, Information Systems, IT, or a related field', '\ufeff', 'Strong knowledge of relational databases such as MSSQL and Oracle\xa0', 'Requirements:']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Big Data Engineer,ClientSolv Inc.,"Englewood, CO",3 days ago,Be among the first 25 applicants,"['', 'This 6 month contract with option to hire role will be onsite in Englewood, CO. There are not telecommuting options available for this role.', 'AWS knowledge is a nice to have skill set', 'Core Big Data development hands on experience using Spark or Map ReduceJava, Scala ,PythonData EngineeringAWS knowledge is a nice to have skill set', 'Strong interpersonal, analytical, problem-solving, influencing, prioritization, decision- making and conflict resolution skills', 'Top skills', 'Additional Skills', 'Strong Software Engineering experience with proficiency in at least one of the following programming languages: Spark, Python, Scala or equivalent', 'Experience with building streaming ingestion pipleline using Kafka streams, Apache Flink, or others', ' Contribute to our cloud strategy based on prior experience', 'Top skills (Technical):', ' Independently work with all stakeholders across the organization to deliver enhanced functionality', 'Strong experience in writing SQL and PLSQL ', 'Job Duties And Responsibilities', 'Experience with Cloud Computing platforms like Amazon AWS, Google Cloud etc.', 'Experience with building data ingestion pipelines both real time and batch using best practices', 'Experience with scheduling tools preferrable Control-M,Airflow or AWS Step functions.', 'Minimum of 5 years of experience delivering data solutions on a variety of data warehousing, big data and cloud data platforms.', 'Core Big Data development hands on experience using Spark or Map Reduce', 'Minimum of 5 years of experience delivering data solutions on a variety of data warehousing, big data and cloud data platforms.3+ years of experience working with distributed data technologies (e.g. Spark, Kafka, Flink etc) for building efficient, large-scale ‘big data’ pipelines;Strong Software Engineering experience with proficiency in at least one of the following programming languages: Spark, Python, Scala or equivalentExperience with building data ingestion pipelines both real time and batch using best practicesExperience with building streaming ingestion pipleline using Kafka streams, Apache Flink, or othersStrong experience in writing SQL and PLSQL Experience with Cloud Computing platforms like Amazon AWS, Google Cloud etc.Experience supporting and working with cross-functional teams in a dynamic environmentExperience with relational SQL and NoSQL databases, including Postgres, and Mongodb.Experience with change data capture tools (CDC) preferred such as Attunity/goldengateExperience with scheduling tools preferrable Control-M,Airflow or AWS Step functions.Strong interpersonal, analytical, problem-solving, influencing, prioritization, decision- making and conflict resolution skillsExcellent written/verbal communication skills.', ' Optimizing data and data pipeline architecture', ' Understand the latest technologies in a rapidly innovative marketplace', 'Data Engineering', 'Excellent written/verbal communication skills.', 'Java, Scala ,Python', 'Qualifications', 'Experience with relational SQL and NoSQL databases, including Postgres, and Mongodb.', 'Experience with change data capture tools (CDC) preferred such as Attunity/goldengate', 'Experience supporting and working with cross-functional teams in a dynamic environment', 'Company Description', ' Evangelist for data engineering function leveraging Bigdata framework', '3+ years of experience working with distributed data technologies (e.g. Spark, Kafka, Flink etc) for building efficient, large-scale ‘big data’ pipelines;', 'Job Description', ' Support software engineers and data scientists']",Mid-Senior level,Contract,Engineering,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,"NBCUniversal Media, LLC","Burbank, CA",2 weeks ago,Be among the first 25 applicants,"['', ' Experience with Snowflake, Amazon Web Services, or related cloud platforms a plus  Understanding of big data technology stacks (Hive / Spark etc) is a plus ', ' Direct experience with data modeling, ETL/ELT development principles, and data warehousing concepts ', ' Direct experience working with sources like Nielsen, Adobe Analytics, comScore, and other media/entertainment industry datasets a plus ', 'Notices', "" Bachelor's degree in Computer Science, Data Science, Statistics, Informatics, Information Systems or related field. "", ' Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using cloud-native data engineering principles ', ' Implement the appropriate design patterns while optimizing performance, cost, security, and scale and end user experience ', 'Multiple Locations', "" Action-oriented – You're constantly figuring out new problems and are regularly showing results with a positive attitude, always displaying ethical behavior, integrity, and building trust Strong understanding of Agile principles and best practices  You’ve dealt with ambiguity and can make quality decisions in a dynamic, fast-paced environment "", ' Strong understanding of Agile principles and best practices ', 'Country', ' Knowledge of data management fundamentals and data storage principles ', ' Experience with Snowflake, Amazon Web Services, or related cloud platforms a plus ', ' Media-focused – Strong knowledge/passion for media including broadcast TV, digital, and film Direct experience working with sources like Nielsen, Adobe Analytics, comScore, and other media/entertainment industry datasets a plus ', ' 5+ years of experience in a data engineering role ', ' Analytical – You have experience in delivering data analytics solutions that promote data discovery Experience with Snowflake, Amazon Web Services, or related cloud platforms a plus  Understanding of big data technology stacks (Hive / Spark etc) is a plus ', ' Understanding of big data technology stacks (Hive / Spark etc) is a plus ', ' Build and manage relationships with supporting IT teams in order to effectively deliver work products to production ', "" Analytical – You have experience in delivering data analytics solutions that promote data discovery Experience with Snowflake, Amazon Web Services, or related cloud platforms a plus  Understanding of big data technology stacks (Hive / Spark etc) is a plus  Media-focused – Strong knowledge/passion for media including broadcast TV, digital, and film Direct experience working with sources like Nielsen, Adobe Analytics, comScore, and other media/entertainment industry datasets a plus  Communicator – You have excellent verbal and written skills with the ability to communicate ideas effectively across all levels of the organization, both technical and non-technical  Action-oriented – You're constantly figuring out new problems and are regularly showing results with a positive attitude, always displaying ethical behavior, integrity, and building trust Strong understanding of Agile principles and best practices  You’ve dealt with ambiguity and can make quality decisions in a dynamic, fast-paced environment "", 'City', ' You’ve dealt with ambiguity and can make quality decisions in a dynamic, fast-paced environment ', 'About Us', ' Strong understanding of Agile principles and best practices  You’ve dealt with ambiguity and can make quality decisions in a dynamic, fast-paced environment ', 'Responsibilities', ' Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. ', ' Communicator – You have excellent verbal and written skills with the ability to communicate ideas effectively across all levels of the organization, both technical and non-technical ', 'Qualifications/Requirements', 'Qualifications', 'State/Province', ' General understanding of cloud data engineering design patterns and use cases ', ' Collaborate with business leaders, engineers, and product managers to understand data needs.  Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using cloud-native data engineering principles  Design, build, and scale data pipelines across a variety of source systems and streams (internal, third-party, as well as cloud-based), distributed/elastic environments, and downstream applications and/or self-service solutions  Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.  Implement the appropriate design patterns while optimizing performance, cost, security, and scale and end user experience  Participate in development sprints, demos, and retrospectives, as well as release and deployment  Build and manage relationships with supporting IT teams in order to effectively deliver work products to production ', ' Collaborate with business leaders, engineers, and product managers to understand data needs. ', ' Desired Characteristics: ', ' Demonstratable experience in Airflow, Luigi or similar orchestration engines ', ' Participate in development sprints, demos, and retrospectives, as well as release and deployment ', 'Sub-Business', "" 5+ years of experience in a data engineering role  Direct experience with data modeling, ETL/ELT development principles, and data warehousing concepts  Knowledge of data management fundamentals and data storage principles  Experience in building data pipelines using Python/SQL or similar programming languages  Demonstratable experience in Airflow, Luigi or similar orchestration engines  General understanding of cloud data engineering design patterns and use cases  Bachelor's degree in Computer Science, Data Science, Statistics, Informatics, Information Systems or related field. "", ' Experience in building data pipelines using Python/SQL or similar programming languages ', ' Design, build, and scale data pipelines across a variety of source systems and streams (internal, third-party, as well as cloud-based), distributed/elastic environments, and downstream applications and/or self-service solutions ', 'Career Level']",Not Applicable,Full-time,Information Technology,Broadcast Media,2021-03-24 13:05:10
Data Engineer - Analytics Platform,Equifax,"Alpharetta, GA",3 weeks ago,Be among the first 25 applicants,"['', 'Comply with Equifax data security, compliance and governance rules at all times', 'Work experience in regulatory and data compliant environments and Credit Industry Domain knowledge is preferred', 'Grow at your own pace through online courses at Learning @ Equifax.', 'Ownership', 'Decide-Execute-Ship', 'Accountability', 'We offer excellent compensation packages with market competitive pay, comprehensive healthcare packages, 401k matching, schedule flexibility, work from home opportunities, paid time off, and organizational growth potential.Grow at your own pace through online courses at Learning @ Equifax.', 'At Equifax, we believe knowledge drives progress. As a global data, analytics and technology company, we play an essential role in the global economy by helping employers, employees, financial institutions and government agencies make critical decisions with greater confidence.\xa0', 'If this sounds like somewhere you want to work, don’t delay, apply today - we’re looking for you!', 'Hands on experience in Cloud technologies and Google Data Cloud tools, BigTable and BigQuery and other Big data technologies like Hadoop', 'Trust', 'Bravery', 'We offer excellent compensation packages with market competitive pay, comprehensive healthcare packages, 401k matching, schedule flexibility, work from home opportunities, paid time off, and organizational growth potential.', 'BS/ MS in Computer Science, Management Information Systems, or a related field5+ years in data engineering, data wrangling or related roles.\xa0\xa0Strong skills in SQL, Python, SPARK, Google Data Prep, Google Data Flow, Java or related skillsLiterate in programming languages used for statistical modeling and analysis including machine learning (like SAS, Python, R, SPARK, H2O), data warehousing solutions, Data API\xa0 and building data pipelines, as well as possess a strong foundation in software engineeringExpertise in developing a data schema and data modelsYou possess excellent written and verbal communication skills with the ability to communicate with team members at various levels, including business leaders.', 'Expertise in developing a data schema and data models', 'What You’ll Do ', 'Design and develop Ignite Internal data quality certification and monitoring\xa0', 'BS/ MS in Computer Science, Management Information Systems, or a related field', 'Experience in Data Fabric data pipelines, Data Catalog and Collibra for on-boarding and purposing data sources is a huge plus', 'We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.', 'Equifax is transforming core data supply chains to the Equifax Data Fabric, and analytics platform to Ignite Internal built on Google Cloud based technologies.\xa0 Data Fabric will transform all how Equifax manages and monetizes data globally and Ignite Internal is where all the R&D work will happen. This exciting new role will be on the cutting edge of this transformation helping with adoption of Ignite Internal and sunsetting Cambrian legacy Hadoop platform.', 'Identify, design, and implement internal process improvements, build new capabilities and automation to drives innovation and business value', 'You possess excellent written and verbal communication skills with the ability to communicate with team members at various levels, including business leaders.', 'Success Attributes of an Equifax employee; does this describe you?', 'Qualifications ', 'Strong skills in SQL, Python, SPARK, Google Data Prep, Google Data Flow, Java or related skills', 'Literate in programming languages used for statistical modeling and analysis including machine learning (like SAS, Python, R, SPARK, H2O), data warehousing solutions, Data API\xa0 and building data pipelines, as well as possess a strong foundation in software engineering', 'Primary Location:', 'Think and act differently', 'Schedule:', 'Function:', 'Strong analytical skills and attention to detail and accuracy', 'Curiosity', 'Collaboration', 'Configure, code and test data preparation complex plug-ins in a variety of Google Cloud tools including\xa0 Python, JAVA, Google Data Flow', 'The Career Data Engineer will lead and perform the technical activities, of all complexities, ranging from data analysis, design, testing, implementation of new tools and technologies and support users within D&A to complete Ignite Internal migration efforts and continued enhancement of the platform.\xa0 In addition to technical and programming activities, the role requires deep knowledge of Equifax Data, Data\xa0 Fabric, Google Cloud, Equifax Security and Data Protection policies and the business acumen needed to collaborate in a fast-paced\xa0 innovative environment. \xa0 The Data Engineer will work closely with the Innovation Data Scientists, Data Stewards, Data Analyst, Product\xa0 and Technology teams.\xa0\xa0', 'Lead technical research, support broad base user rollout of tools, capabilities and data across US for 200+ hands on users and implement patterns for Global teams\xa0Identify, design, and implement internal process improvements, build new capabilities and automation to drives innovation and business valueDrive Ignite Internal platform monitoring, cost reduction initiatives and best practices on cloud\xa0Design and develop Ignite Internal data quality certification and monitoring\xa0Champion project migration from Cambrian on-prem Hadoop platform to Ignite Internal GCPConfigure, code and test data preparation complex plug-ins in a variety of Google Cloud tools including\xa0 Python, JAVA, Google Data FlowComply with Equifax data security, compliance and governance rules at all times', 'Champion project migration from Cambrian on-prem Hadoop platform to Ignite Internal GCP', 'AccountabilityBraveryCuriosityCollaborationThink and act differentlyTrustOwnershipDecide-Execute-Ship', 'Lead technical research, support broad base user rollout of tools, capabilities and data across US for 200+ hands on users and implement patterns for Global teams\xa0', 'The Perks of being an Equifax Employee?', 'Drive Ignite Internal platform monitoring, cost reduction initiatives and best practices on cloud\xa0', 'We work to help create seamless and positive experiences during life’s pivotal moments: applying for jobs or a mortgage, financing an education or buying a car. Our impact is real and to accomplish our goals we focus on nurturing our people for career advancement and their learning and development, supporting our next generation of leaders, maintaining an inclusive and diverse work environment, and regularly engaging and recognizing our employees. Regardless of location or role, the individual and collective work of our employees makes a difference and we are looking for talented team players to join us as we help people live their financial best.', 'Hands on experience in Cloud technologies and Google Data Cloud tools, BigTable and BigQuery and other Big data technologies like HadoopStrong analytical skills and attention to detail and accuracyExcellent communication and collaboration skills and ability to work independently as well as in teamsExperience in Data Fabric data pipelines, Data Catalog and Collibra for on-boarding and purposing data sources is a huge plusWork experience in regulatory and data compliant environments and Credit Industry Domain knowledge is preferred', 'Who is Equifax?\xa0', 'Extra Points for any of the Following', 'Excellent communication and collaboration skills and ability to work independently as well as in teams', '5+ years in data engineering, data wrangling or related roles.\xa0\xa0']",Mid-Senior level,Full-time,Analyst,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,KeHE Distributors,"Naperville, IL",7 days ago,Be among the first 25 applicants,"['', 'Experience implementing AWS architecture using Serverless Framework', 'Drive innovation and efficiency through new approaches', 'Develop, construct, test and maintain optimal data pipeline/ETL architecturesWork closely within the team to prepare data for predictive and prescriptive modelingOptimize AWS data delivery infrastructure for greater scalabilityUtilize SQL as well as big data tools and frameworks to optimize data acquisition and preparation from enterprise data lake and data warehouseWork with Enterprise Cloud Architecture teams to strive for greater functionality in our data systemsDevelop architecture required to return data to data warehouse for front-end product utilizationCurate data models in the data warehouse to be used by front-end advanced analytics designersProvide production level code reviews for the teamHelp design, maintain and implement quality assurance and testing approachesDeploy scripts and architectures to production via Jenkins', 'Desire to stay up to date with current technologies and best practices for data management and data science', 'Experience implementing AWS architecture using Serverless FrameworkUnderstanding of C programming languageExperience utilizing big data tools such as PySpark, Scala or others', 'Optimize AWS data delivery infrastructure for greater scalability', 'Primary Responsibilities', 'Minimum Requirements, Qualifications, Additional Skills, Aptitude', 'Work closely within the team to prepare data for predictive and prescriptive modeling', 'Overview', 'Ability to work in a team environment that promotes collaboration', 'Curate data models in the data warehouse to be used by front-end advanced analytics designers', 'Bachelor’s Degree in Computer Science, Mathematics, Engineering, Management Information Systems or related field', 'Provide production level code reviews for the team', 'Deploy scripts and architectures to production via Jenkins', 'Help design, maintain and implement quality assurance and testing approaches', 'Develop architecture required to return data to data warehouse for front-end product utilization', '1-3 years of experience building data pipelines within the AWS ecosystem', 'Experience utilizing big data tools such as PySpark, Scala or others', 'Develop, construct, test and maintain optimal data pipeline/ETL architectures', 'Proficient programing experience using Python, R or similar language with experience building production level code', 'Utilize SQL as well as big data tools and frameworks to optimize data acquisition and preparation from enterprise data lake and data warehouse', ' Requisition ID ', 'Advanced SQL and data design concepts', ' Preferred Experience and Abilities: ', 'Sound good?', 'Proficient working with Jenkins and deploying to production via Jenkin’s jobs', 'Bachelor’s Degree in Computer Science, Mathematics, Engineering, Management Information Systems or related field1-3 years of experience building data pipelines within the AWS ecosystem1-3 years of experience designing and implementing data warehouse solutionsAdvanced SQL and data design conceptsProficient programing experience using Python, R or similar language with experience building production level codeProficient working with Jenkins and deploying to production via Jenkin’s jobsDesire to stay up to date with current technologies and best practices for data management and data scienceDrive innovation and efficiency through new approachesAbility to work in a team environment that promotes collaboration', 'Essential Functions', 'Work with Enterprise Cloud Architecture teams to strive for greater functionality in our data systems', 'Understanding of C programming language', '1-3 years of experience designing and implementing data warehouse solutions']",Entry level,Full-time,Information Technology,Food & Beverages,2021-03-24 13:05:10
Lead Data Engineer,Insight Global,"Portland, Oregon Metropolitan Area",,N/A,"['Enterprise-level experience working with large consumer data sets', '5+ yeas of Data Engineering experience utilizing SQL, Python, Spark, and AWS (EMR)', '', 'Apache Nifi', 'Supply chain analytics experience', 'Day-to-Day\xa0', 'Plusses', 'The Lead Data Engineer will be responsible for assisting a team of about 10 technical resources with technical monitoring and troubleshooting. The Lead will have a small programming responsibility, but the majority of their role will be centered around leading meetings, assisting engineers to get unblocked, expert code reviews, solution proposals, and creating technical user stories. Additional responsibilities include communicating with the supply chain business units and ensuring project timelines are maintained.', 'Desired Skills and Experience\xa0', '2+ years of technical lead experience within a Data Engineering team', 'Experience with data visualization tools like Tableau', '\xa0']",Mid-Senior level,Contract,Information Technology,Staffing and Recruiting,2021-03-24 13:05:10
Principal Data Engineer,bp,"Chicago, IL",24 hours ago,Be among the first 25 applicants,"['Experience in retail and / or supply chain data', 'Role SynopsisThe Principal Data Engineer for Digital Customers & Markets leads data engineering across our customer facing businesses as well as Trading & Shipping, Gas & Low Carbon and Regions, Cities & Solutions. The Principal Data Engineer will drive our big data & analytics technical data strategy and combine their technical expertise with strong business acumen and data domain knowledge to create value for bp. The individual continues to be hands-on to define and build data products, for example writing and reviewing code, architecting distributed data systems and providing actionable, pragmatic insights in technical design reviews. Data Engineering & Data Management is a discipline at bp and the individual will line manage a number of individuals as well as 3rd party contractors.Key Accountabilities ', 'Leads, grows and develops a team of data engineers that writes, deploys and maintains software to build, integrate, manage, maintain, and quality-assure data at bp.Creates positive engagement and drives an inclusive work environment with team and stakeholders through the quality of interactions and collaboration across multiple business entities.Effectively works with cross-disciplinary collaborators and stakeholders across multiple business entities.Architects and designs reliable and scalable data infrastructure.Advocates for and ensures their team adheres to software engineering best practices (e.g. technical design, technical design review, unit testing, monitoring & alerting, checking in code, code review, documentation),Responsible for deploying secure and well-tested software that meets privacy and compliance requirements.Responsible for service reliability and following site-reliability engineering best practices: on-call rotations for services they oversee, responsible for defining and maintaining SLAs.Actively contributes to improve developer velocity.', 'Advocates for and ensures their team adheres to software engineering best practices (e.g. technical design, technical design review, unit testing, monitoring & alerting, checking in code, code review, documentation),', 'Strong stakeholder management and ability to lead large organizations through influence', 'Deep and hands-on experience (typically 12+ years) designing, planning, productionizing, maintaining and documenting reliable and scalable data infrastructure and data products in complex environments', 'Experience (typically 6+ years) leading, growing and developing a data engineering team of around 30-150 people', 'BS degree in computer science or related field', 'Experience in retail and / or supply chain dataExperience in AWS and / or Azure native data platforms', 'Effectively works with cross-disciplinary collaborators and stakeholders across multiple business entities.', 'Creates positive engagement and drives an inclusive work environment with team and stakeholders through the quality of interactions and collaboration across multiple business entities.', 'Leads, grows and develops a team of data engineers that writes, deploys and maintains software to build, integrate, manage, maintain, and quality-assure data at bp.', 'Responsible for service reliability and following site-reliability engineering best practices: on-call rotations for services they oversee, responsible for defining and maintaining SLAs.', 'Experience in AWS and / or Azure native data platforms', 'Continuous learning and improvement mindset', 'Experience designing and implementing large-scale distributed systems', 'Development experience in one or more object-oriented programming languages (e.g. Python, Go, Java, C++)', 'Actively contributes to improve developer velocity.', 'Responsible for deploying secure and well-tested software that meets privacy and compliance requirements.', 'Essential Experience and Job Requirements', 'Deep knowledge and hands-on experience in technologies across all data lifecycle stages', 'Experience (typically 6+ years) leading, growing and developing a data engineering team of around 30-150 peopleDeep and hands-on experience (typically 12+ years) designing, planning, productionizing, maintaining and documenting reliable and scalable data infrastructure and data products in complex environmentsDevelopment experience in one or more object-oriented programming languages (e.g. Python, Go, Java, C++)Experience designing and implementing large-scale distributed systemsDeep knowledge and hands-on experience in technologies across all data lifecycle stagesStrong stakeholder management and ability to lead large organizations through influenceContinuous learning and improvement mindsetBS degree in computer science or related field', 'Architects and designs reliable and scalable data infrastructure.']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Lead Data Engineer,The Knot Worldwide,"Austin, TX",2 weeks ago,54 applicants,"['', 'ABOUT THE ROLE AND OUR TEAM:', 'Document built processes and data content and publish needed information to our data dictionary', 'SUCCESSFUL LEAD DATA ENGINEERS HAVE:', 'A Bachelor’s Degree in Computer Science, Engineering, Information Systems, or a related field is preferred. An advanced degree or professional licensing is a plus.', 'Experience developing advanced data pipelines and streaming channels', 'Experience with Big Data Solutions such as Snowflake, Redshift, Google BigQuery and or Hadoop', 'Experience with at least one Business Intelligence tool such as Qlik and or Metabase', 'You Love Our Users. You keep our global community at the center of everything you do.', 'DESIRED SKILLS/EXPERIENCE:', 'Analyze, propose and seek to establish data permissions and profile segmentation aligning to standards such as Personas', 'Proven and demonstrated ability to work under pressure', 'Experience with advanced Ralph Kimball dimensional modelling architecture', 'WHAT WE DO MATTERS:', 'Support project efforts that seek to reduce technical debt, example: (Multiple data stores that are redundant)', 'Ability to troubleshoot complex integration issues', 'Experience with ETL / ELT platforms such as Matillion, Dell Boomi, Luigi, Informatica, Talend, and or SSIS', 'Participate in maintaining quality and governance standards ', 'Programming experience with Ruby, Python, Scala, Shell Scripting', ' You Dream Big. You iterate and experiment to drive innovation. You Love Our Users. You keep our global community at the center of everything you do. You Do the Right Thing. You strengthen your team through respect, fairness, and inclusion. You Hustle Every Day. You favor urgency and own your outcomes.  You Win Together. People are at the heart of our success and you play as a team. ', 'Excellent communication skills, both verbal and written', 'Minimum of 7 years experience with SQL and advanced relational, operational, object-oriented and data warehouse development', 'Introduce a consistent change management process that aligns with our standards; examples are: new field, object and data creation', 'Conduct research and make recommendations on data requirements, products and data services', 'Collaborate with Data Engineering leadership on data standardization and integration best practices', 'Present ideas, expectations and information in a concise well-organized way', 'Working knowledge of data movement, monitoring and management technologies ', 'Demonstrate self-confidence, energy and enthusiasm', 'Ability to ramp up quickly and fully exploit platform advanced feature sets', 'Manage time well, whilst correctly prioritizing tasks', 'Resolve Ad-hoc data questions from the organization in a timely and accurate manner', ' A Bachelor’s Degree in Computer Science, Engineering, Information Systems, or a related field is preferred. An advanced degree or professional licensing is a plus. Programming experience and a demonstrated interest in data trends, data analysis, quality, movement, measurement, profiling, reporting and analytics Proven and demonstrated ability to work under pressure Working knowledge of data movement, monitoring and management technologies  Minimum of 7 years experience with SQL and advanced relational, operational, object-oriented and data warehouse development Hands-on experience with at least one of the following databases (MYSQL, PostgreSQL, Snowflake, MSSQL, Redshift, MongoDB, Couchbase)  Minimum of 5 years experience actively working with AWS services (S3, Lambda, SNS, EC2, Load Balancers, Routing tables, EMR, VDI / PCI networking) Excellent communication skills, both verbal and written Demonstrate self-confidence, energy and enthusiasm Present ideas, expectations and information in a concise well-organized way Conduct research and make recommendations on data requirements, products and data services Demonstrated ability to act in a lead role Ability to ramp up in a short time on new technologies Ability to troubleshoot complex integration issues Manage time well, whilst correctly prioritizing tasks ', 'Ability to ramp up in a short time on new technologies', 'You Win Together. People are at the heart of our success and you play as a team.', 'Extract, load and transform data from The Knot Worldwide and other sources to our DataWarehouse for the consumption of our end users.', 'Architect and develop advanced solutions for integrating between disparate systems which include cloud-based sources, API and AWS Services', 'You Dream Big. You iterate and experiment to drive innovation.', 'You Do the Right Thing. You strengthen your team through respect, fairness, and inclusion.', 'Continuous improvement on current ETL processes to ensure accuracy, timeliness and scalability as data volumes grow.', 'WHAT WE LOVE ABOUT YOU:', 'Programming experience and a demonstrated interest in data trends, data analysis, quality, movement, measurement, profiling, reporting and analytics', ' Extract, load and transform data from The Knot Worldwide and other sources to our DataWarehouse for the consumption of our end users. Architect and develop advanced solutions for integrating between disparate systems which include cloud-based sources, API and AWS Services Maintaining current ETL processes and resolving daily ETL job failures as they arise Resolve Ad-hoc data questions from the organization in a timely and accurate manner Continuous improvement on current ETL processes to ensure accuracy, timeliness and scalability as data volumes grow. Identify through documented or current issues, data structural problems that increase inconsistencies and reduce quality and performance within the enterprise set of systems and the related data stores  Bring awareness and communicate standards across the data landscape Introduce a consistent change management process that aligns with our standards; examples are: new field, object and data creation Support project efforts that seek to reduce technical debt, example: (Multiple data stores that are redundant) Analyze, propose and seek to establish data permissions and profile segmentation aligning to standards such as Personas Document built processes and data content and publish needed information to our data dictionary Provide guidance to team members with respect to best practices Participate in maintaining quality and governance standards  Collaborate with Data Engineering leadership on data standardization and integration best practices ', 'Maintaining current ETL processes and resolving daily ETL job failures as they arise', 'RESPONSIBILITIES: ', 'Bring awareness and communicate standards across the data landscape', 'Hands-on experience with at least one of the following databases (MYSQL, PostgreSQL, Snowflake, MSSQL, Redshift, MongoDB, Couchbase) ', 'Provide guidance to team members with respect to best practices', 'You Hustle Every Day. You favor urgency and own your outcomes. ', 'Minimum of 5 years experience actively working with AWS services (S3, Lambda, SNS, EC2, Load Balancers, Routing tables, EMR, VDI / PCI networking)', 'Demonstrated ability to act in a lead role', 'Experience in database OLTP schema design and architecture models', 'WHAT YOU LOVE ABOUT US:', ' Experience with advanced Ralph Kimball dimensional modelling architecture Experience in database OLTP schema design and architecture models Experience developing advanced data pipelines and streaming channels Experience with at least one Business Intelligence tool such as Qlik and or Metabase Experience with Big Data Solutions such as Snowflake, Redshift, Google BigQuery and or Hadoop Experience with ETL / ELT platforms such as Matillion, Dell Boomi, Luigi, Informatica, Talend, and or SSIS Programming experience with Ruby, Python, Scala, Shell Scripting Ability to ramp up quickly and fully exploit platform advanced feature sets ', 'Identify through documented or current issues, data structural problems that increase inconsistencies and reduce quality and performance within the enterprise set of systems and the related data stores ']",Associate,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Cloud Data Engineer,Bradford & Galt,Greater St. Louis,2 days ago,Be among the first 25 applicants,"['', 'Perform data exploration in support of new projects and to troubleshoot issues', 'EXPERIENCE BASE/BACKGROUND', 'Create real-time data streaming pipelines using GCP App Engine and Dataflow', '3+ years hands on SQL and relational database experience ', 'Develop optimal extract, load and transform pipelines from a wide variety of data sources using Python and SparkUtilize GCP ‘big data’ technologies like Cloud Dataproc and BigQuerySchedule and orchestrate solutions using GCP Cloud ComposerCreate real-time data streaming pipelines using GCP App Engine and DataflowPerform data exploration in support of new projects and to troubleshoot issuesWork closely with data analysts to facilitate data acquisition and enable efficient analysisAdapt existing batch and real time data pipelines to accommodate new business requirementsDevelop and deploy code with exception handling, logging and monitoringEnsure data quality throughout the entire data processing pipelinePerform unit testing, integration testing, and assist with user acceptance testing Be proactive in clarifying requirements when needed to ensure accurate results', 'Ability to conduct code reviews and provide constructive feedback to peer team members', 'Perform unit testing, integration testing, and assist with user acceptance testing ', 'Schedule and orchestrate solutions using GCP Cloud Composer', 'Develop and deploy code with exception handling, logging and monitoring', 'Work closely with data analysts to facilitate data acquisition and enable efficient analysis', '\xa0', 'Excellent Communication Skills\xa0', 'Adapt existing batch and real time data pipelines to accommodate new business requirements', 'Bachelor’s Degree in computer science or equivalent experience required', 'Bradford & Galt is an equal opportunity employer. We will not discriminate, and will take affirmative action measures to ensure against discrimination in employment, recruitment, advertisements for employment, compensation, termination, and other conditions of employment, against any employee or job applicant on the basis of race, color, gender, national origin, age, religion, creed, disability, veteran’s status, or sexual orientation.', 'Responsibilities (Other duties may be assigned)', '2+ years hands on experience in a public cloud development environment (GCP preferred)', 'Proficient understanding of distributed computing principles', 'Responsibilities', '3+ years hands on experience with Python', 'Ensure data quality throughout the entire data processing pipeline', 'Develop optimal extract, load and transform pipelines from a wide variety of data sources using Python and Spark', 'Bachelor’s Degree in computer science or equivalent experience required3+ years hands on experience with Python3+ years hands on SQL and relational database experience 2+ years hands on experience in a public cloud development environment (GCP preferred)Experience setting standards for design, documentation, testing, and code qualityProficient understanding of distributed computing principlesExperience with messaging systems, such as Kafka and GCP PubSubAbility to conduct code reviews and provide constructive feedback to peer team membersExcellent Communication Skills\xa0', 'Experience setting standards for design, documentation, testing, and code quality', 'Utilize GCP ‘big data’ technologies like Cloud Dataproc and BigQuery', 'Be proactive in clarifying requirements when needed to ensure accurate results', 'Experience with messaging systems, such as Kafka and GCP PubSub', '*Mid (2-4 years of experience) is the level of experience accepted for this opening*']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
"Junior Data Engineer - Austin, TX",Deloitte,"Austin, TX",2 weeks ago,36 applicants,"['', 'The team', 'Prior professional services or federal consulting experience', 'High-performing team player ', 'Ability to thrive in a fast-paced work environment with multiple stakeholders', 'Creativity and innovation – desire to learn and apply new technologies, products, and libraries ', 'Recruiter tips', 'Familiarity with microservice architectures ', '2+ years of experience with SQL queries and JSON objects', ' Implement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to generate insights leveraging cloud-based platforms Leverage automation, cognitive and science-based techniques to manage data, predict scenarios and prescribe actions Drive operational efficiency by maintaining their data ecosystems, sourcing analytics expertise and providing As-a-Service offerings for continuous insights and improvements ', 'Deloitte’s culture', '2+ years of experience with data modeling, data warehousing, and building ETL pipelines', '2+ years of experience with extract, transform, and load (ETL) methods and tools', 'Corporate citizenship', 'Interest in event streaming architectures, such as Apache Kafka ', ' Familiarity with microservice architectures  Interest in event streaming architectures, such as Apache Kafka  Prior professional services or federal consulting experience Knowledge of data mining, machine learning, data visualization and statistical modeling Ability to thrive in a fast-paced work environment with multiple stakeholders Creativity and innovation – desire to learn and apply new technologies, products, and libraries  High-performing team player  ', 'Qualifications', 'Ability to Obtain and Maintain a Government Security Clearance', '1+ years of experience with both SQL and NoSQL databases, including PostgreSQL and MongoDB', 'Junior Data Engineer – Austin, TX', 'Knowledge of data mining, machine learning, data visualization and statistical modeling', "" Ability to Obtain and Maintain a Government Security Clearance 2+ years of experience with extract, transform, and load (ETL) methods and tools 2+ years of experience with data modeling, data warehousing, and building ETL pipelines 2+ years of experience with SQL queries and JSON objects 1+ years of experience with both SQL and NoSQL databases, including PostgreSQL and MongoDB Bachelor's degree in Computer Science, Engineering, Mathematics or other business-related field Must be legally authorized to work in the United States without the need for employer sponsorship, now or at any time in the future. "", 'Implement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to generate insights leveraging cloud-based platforms', 'Benefits', 'Leverage automation, cognitive and science-based techniques to manage data, predict scenarios and prescribe actions', ""Bachelor's degree in Computer Science, Engineering, Mathematics or other business-related field"", 'Work you’ll do', 'How You’ll Grow', 'Must be legally authorized to work in the United States without the need for employer sponsorship, now or at any time in the future.', 'Drive operational efficiency by maintaining their data ecosystems, sourcing analytics expertise and providing As-a-Service offerings for continuous insights and improvements']",Not Applicable,Full-time,Management,Accounting,2021-03-24 13:05:10
Data Engineer (DE - Client Data Engineering) - Data Design & Cura,Goldman Sachs,"Dallas, TX",1 week ago,38 applicants,"['', ' Deep understanding of multidimensionality of data, data curation and data quality, such as traceability, security, performance latency and correctness across supply and demand processes', ' RESPONSIBILITIES AND QUALIFICATIONS ', 'About Goldman Sachs', 'Preferred Qualifications', ' In-depth knowledge of relational and columnar SQL databases, including database design', ' Excellent communications skills and the ability to work with subject matter expert to extract critical business concepts', ' Strong analytical and problem solving skills', ' A Bachelor’s degree (Masters preferred) in a computational field (Computer Science, Applied Mathematics, Engineering, or in a related quantitative discipline)', ' Engage with data consumers and producers in order to design appropriate models to suit all needs', ' Extensive knowledge and proven experience applying domain driven design to build complex business applications', ' 5+ years of relevant work experience in a team-focused environment A Bachelor’s degree (Masters preferred) in a computational field (Computer Science, Applied Mathematics, Engineering, or in a related quantitative discipline) Working knowledge of more than one programming language (Python, Java, C++, C#, etc.) Extensive knowledge and proven experience applying domain driven design to build complex business applications Deep understanding of multidimensionality of data, data curation and data quality, such as traceability, security, performance latency and correctness across supply and demand processes In-depth knowledge of relational and columnar SQL databases, including database design General knowledge of business processes, data flows and the quantitative models that generate or consume data Excellent communications skills and the ability to work with subject matter expert to extract critical business concepts Independent thinker, willing to engage, challenge or learn Ability to stay commercially focused and to always push for quantifiable commercial impact Strong work ethic, a sense of ownership and urgency Strong analytical and problem solving skills Ability to collaborate effectively across global teams and communicate complex ideas in a simple manner', ' Ability to collaborate effectively across global teams and communicate complex ideas in a simple manner', ' Experience with the Hadoop eco-system (HDFS, Spark)', ' Independent thinker, willing to engage, challenge or learn', ' Working knowledge of more than one programming language (Python, Java, C++, C#, etc.)', ' Strong work ethic, a sense of ownership and urgency', 'How You Will Fulfill Your Potential', ' Ability to stay commercially focused and to always push for quantifiable commercial impact', ' Evaluate, select and acquire new internal & external data sets that contribute to business decision making', ' Engineer streaming data processing pipelines', 'Skills And Experience We Are Looking For', ' Financial Services industry experience Experience with the Hadoop eco-system (HDFS, Spark)', ' Deploy modern data management tools to curate our most important data sets, models and processes, while identifying areas for process automation and further efficiencies Evaluate, select and acquire new internal & external data sets that contribute to business decision making Engineer streaming data processing pipelines Drive adoption of Cloud technology for data processing and warehousing Engage with data consumers and producers in order to design appropriate models to suit all needs', ' Financial Services industry experience', ' Drive adoption of Cloud technology for data processing and warehousing', ' 5+ years of relevant work experience in a team-focused environment', ' General knowledge of business processes, data flows and the quantitative models that generate or consume data', ' Deploy modern data management tools to curate our most important data sets, models and processes, while identifying areas for process automation and further efficiencies']",Executive,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,GoExpedi,"Houston, TX",7 days ago,Be among the first 25 applicants,"['', 'Medical, Dental, and Vision Plan401K PlanPaid Leave (vacation, sick time, and company holidays)Casual dress environmentProfessional growth & career longevity', 'Strong knowledge of DevOps, in particular, using the Repo and pushing and pulling source code with version control', 'Accountable for the delivery of the data strategy surrounding business intelligence and predictive analytics projects that integrate data into a centralized data environment', ' Responsibilties ', 'Strong knowledge of data architectures and systems – SQL and others for ETL purposes', ' Benefits ', 'Minimum 3-4 years of relevant experience in data analytics as a data engineer, data analyst, data scientist (or applied scientist role)', 'Utilize your knowledge of modern data driven methods and domain understanding to support the creation of new products, services and insights using predictive models', 'Think independently and work as part of a cross-functional team to achieve common goals', 'Experience in using Machine Learning, AI and other data science technologies', 'Use DevOps Repository to build, manage and optimize data pipelines and deploy these pipelines into production', 'Work independently on data modeling/engineering tasks and are eager to learn', 'Paid Leave (vacation, sick time, and company holidays)', 'Knowledge of object-oriented programming and building Rest APIs for data analysis', 'BS degree required; MS preferred in Data Engineering, Data Science Computer Science, Engineering, or another relevant field', 'Develop and design presentations, reports, and other deliverables and summarize findings to key stakeholders and executive leadership, communicating with both technical and non-technical team members.', 'Use your experience and are hands-on in Python, Database Systems (such as SQL) and Azure cloud tools to contribute towards solving data analytics problems', 'Advanced experience with any of the following: Looker, Tableau, Domo, Sisense, or Power BI', 'Solid and demonstrable experience with programming languages such as Python (used numpy, pandas, etc.) within platforms such as Spydr, Anaconda, and Jupyter Notebook', 'Perform data wrangling and processing from multiple databases (structured or un-structured data) while having a keen eye for data quality', ' Requirements ', 'Medical, Dental, and Vision Plan', 'Professional growth & career longevity', '401K Plan', 'Write independent source code, validate and test for model quality improvement', 'Utilize your knowledge of modern data driven methods and domain understanding to support the creation of new products, services and insights using predictive modelsWrite independent source code, validate and test for model quality improvementUse your experience and are hands-on in Python, Database Systems (such as SQL) and Azure cloud tools to contribute towards solving data analytics problemsPerform data wrangling and processing from multiple databases (structured or un-structured data) while having a keen eye for data qualityUse DevOps Repository to build, manage and optimize data pipelines and deploy these pipelines into productionThink independently and work as part of a cross-functional team to achieve common goalsWork independently on data modeling/engineering tasks and are eager to learnStay current in the field of advanced analytics and take initiative to apply new technologiesDevelop and design presentations, reports, and other deliverables and summarize findings to key stakeholders and executive leadership, communicating with both technical and non-technical team members.Accountable for the delivery of the data strategy surrounding business intelligence and predictive analytics projects that integrate data into a centralized data environment', 'BS degree required; MS preferred in Data Engineering, Data Science Computer Science, Engineering, or another relevant fieldMinimum 3-4 years of relevant experience in data analytics as a data engineer, data analyst, data scientist (or applied scientist role)Experience in using Machine Learning, AI and other data science technologiesSolid and demonstrable experience with programming languages such as Python (used numpy, pandas, etc.) within platforms such as Spydr, Anaconda, and Jupyter NotebookStrong knowledge of data architectures and systems – SQL and others for ETL purposesStrong knowledge of DevOps, in particular, using the Repo and pushing and pulling source code with version controlAbility to work with MS Azure cloud tools experience with using Web APIs and JSONKnowledge of object-oriented programming and building Rest APIs for data analysisAdvanced experience with any of the following: Looker, Tableau, Domo, Sisense, or Power BI', 'Casual dress environment', 'Ability to work with MS Azure cloud tools experience with using Web APIs and JSON', 'Stay current in the field of advanced analytics and take initiative to apply new technologies']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,ADT,"Kansas City, KS",6 days ago,Be among the first 25 applicants,"['', 'MS in Computer Science or related quantitative discipline', 'Company Overview', 'Minimum Qualifications', 'Mentor and develop data users across CXO organization. ', 'Preferred Qualifications', 'Responsible for data quality and ensuring data is as clean as possible for consumption activities. ', 'Act as a change leader to introduce and implement increased capabilities to advance data usage. ', 'Reduce production support time. ', 'Partner across IT and business groups to identify integrated solution opportunities. ', 'Partner with data and business users to develop a rich pipeline of value-added work. ', 'Recommend technology and process improvements across the data environment. ', '2+ years demonstrated experience with Python, R', 'Passionate about building innovative data solutions', '5+ years demonstrated experience with ETL technologies and building data pipelines. (i.e. DataFlow, DataFusion, SSIS, Alteryx)', '5+ years demonstrated experience with SQL Server (SSIS, SSAS, SSRS)', '7+ years demonstrated experience with ETL and data pipeline frameworks', 'Experience leveraging GCP for data warehousing and analytic consumption', ' Responsible for building end to end data pipelines that provide raw, cleansed and transformed layers for data consumption. Responsible for data quality and ensuring data is as clean as possible for consumption activities.  Design best practices for data processing, data modeling and warehouse development. Develop data strategies and designs to provide proactive solutions and enable stakeholders to extract insights and value from data. Understand end to end data interactions and dependencies across complex data pipelines and data transformation and how they impact business decisions. Create rapid prototypes to test concepts, stakeholder buy-in and conceptual direction.  Responsible for data warehouse design, maintenance and understanding usage. Act as a change leader to introduce and implement increased capabilities to advance data usage.  Partner with data and business users to develop a rich pipeline of value-added work.  Oversee user acceptance testing for data releases to ensure high value output.  Recommend technology and process improvements across the data environment.  Reduce production support time.  Partner across IT and business groups to identify integrated solution opportunities.  Mentor and develop data users across CXO organization.  Develop deep functional understanding of ADT Customer Experience Operations organization including Customer Care, Business Intelligence, Advanced Analytics, Retention, Sales, Marketing, Technology and Field Operations.  ', 'ADT LLC is an Equal Employment Opportunity (EEO) employer. We are committed to having a diverse and inclusive workforce and do our best to foster a culture and environment where every employee feels valued. Our goal is to serve our customers and help save lives. We can achieve this goal when we have the best talent working in an environment where employees feel included and recognized. Visit us online at jobs.adt.com to learn more.', 'Responsible for data warehouse design, maintenance and understanding usage.', 'BS in Computer Science, Information Systems, or related quantitative discipline ', 'Responsible for building end to end data pipelines that provide raw, cleansed and transformed layers for data consumption.', 'Experience with data libraries (i.e. Data Catalog)', 'Develop deep functional understanding of ADT Customer Experience Operations organization including Customer Care, Business Intelligence, Advanced Analytics, Retention, Sales, Marketing, Technology and Field Operations. ', 'Understand end to end data interactions and dependencies across complex data pipelines and data transformation and how they impact business decisions.', 'Position Responsibilities', 'Oversee user acceptance testing for data releases to ensure high value output. ', 'Experience with Cloud Transformations', 'Design best practices for data processing, data modeling and warehouse development.', 'Experience coordinating and negotiating across IT teams', ' MS in Computer Science or related quantitative discipline 7+ years demonstrated experience with ETL and data pipeline frameworks Experience with Cloud Transformations Experience leveraging GCP for data warehousing and analytic consumption ', '3+ years demonstrated experience with Oracle schema management', '5+ years demonstrated experience with cloud data warehouses (i.e. GCP BigQuery, CloudTable)', '5 years working experience with Agile development methodologies', 'Develop data strategies and designs to provide proactive solutions and enable stakeholders to extract insights and value from data.', 'Create rapid prototypes to test concepts, stakeholder buy-in and conceptual direction. ', 'Experience working with various Code Repository and Planning tools (i.e. DevOps, Gi, Bitbucket)', 'Experience preparing data for ML activities. (i.e. AutoML, H2O.ai, Alteryx, DataFlow, DataFusion)', 'Position Summary', ' BS in Computer Science, Information Systems, or related quantitative discipline  5+ years demonstrated experience with cloud data warehouses (i.e. GCP BigQuery, CloudTable) 5+ years demonstrated experience with ETL technologies and building data pipelines. (i.e. DataFlow, DataFusion, SSIS, Alteryx) 5+ years demonstrated experience with SQL Server (SSIS, SSAS, SSRS) 5+ years demonstrated experience building semantic layers to simplify reporting and analytics 3+ years demonstrated experience with Oracle schema management 2+ years demonstrated experience with Python, R Experience preparing data for ML activities. (i.e. AutoML, H2O.ai, Alteryx, DataFlow, DataFusion) Experience with data libraries (i.e. Data Catalog) 5 years working experience with Agile development methodologies Experience coordinating and negotiating across IT teams Experience working with various Code Repository and Planning tools (i.e. DevOps, Gi, Bitbucket) Passionate about building innovative data solutions ', '5+ years demonstrated experience building semantic layers to simplify reporting and analytics']",Entry level,Full-time,Information Technology,Construction,2021-03-24 13:05:10
Data Engineer- Intelligent Forecasting,DICK'S Sporting Goods,"Pittsburgh, PA",2 weeks ago,Be among the first 25 applicants,[''],Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer,Bayside Solutions,"Seattle, WA",1 week ago,34 applicants,"['', 'Description:', '.', '5+ years of experience in big data ecosystem. Example technologies include batch and stream processing (e.g. Spark, Hive, Flink, Beam), analytical engines (e.g. Presto, Druid), search platform (e.g. Solr/Lucene), tooling (e.g. Airflow, Jupyter, Superset, Tableau), and storage format (e.g. Iceberg)Excellent verbal and written communication skills, able to collaborate cross-functionally with data science, machine learning, data platform and analytics teamsCustomer-focused mindset, with emphasis on user experience and satisfactionSuperb problem-solving skills, and able to thrive in a fast-paced and dynamic environmentHands-on in designing, building, scaling, and troubleshooting solutions to big data problemsMust be self-driven, and able to provide advice and support to users to properly integrate with our data platformProgramming experience in Java, Python, Scala, or similar languagesPassionate about latest big data technologies, open source community presence is a big plusExperience with AWS, Kubernetes, Infrastructure-as-code, and data privacy & compliance is a big plus', 'Data Engineer\xa0', ' Education & Experience:', 'Hands-on in designing, building, scaling, and troubleshooting solutions to big data problems', 'Excellent verbal and written communication skills, able to collaborate cross-functionally with data science, machine learning, data platform and analytics teams', 'Experience with AWS, Kubernetes, Infrastructure-as-code, and data privacy & compliance is a big plus', 'Duration:', 'Customer-focused mindset, with emphasis on user experience and satisfaction', '5+ years of experience in big data ecosystem. Example technologies include batch and stream processing (e.g. Spark, Hive, Flink, Beam), analytical engines (e.g. Presto, Druid), search platform (e.g. Solr/Lucene), tooling (e.g. Airflow, Jupyter, Superset, Tableau), and storage format (e.g. Iceberg)', 'Passionate about latest big data technologies, open source community presence is a big plus', 'Programming experience in Java, Python, Scala, or similar languages', 'Superb problem-solving skills, and able to thrive in a fast-paced and dynamic environment', 'Our Company Bio:', 'Must be self-driven, and able to provide advice and support to users to properly integrate with our data platform', 'Key Qualifications:', 'Seattle, WA', 'www.baysidesolutions.com']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Idaho Central Credit Union,"Chubbuck, ID",3 weeks ago,Be among the first 25 applicants,"['', 'Self-motivated with the ability to prioritize,meetdeadlines,andmanagechanging', 'Other duties as assigned.', '2+ years’ experience working in cloud computing with Azure experience required', 'Do you enjoy working with other people and finding solutions when everyone else only see problems? Are you compelled to initiate action and remain proactive in getting things done? Do you take pride in the work you accomplish? Does success motivate you to want to do more and be better? Can you handle multiple projects at the same time and smile? ', 'Source-code management tools such as GitHub', 'Work closely with team members and BI Analysts to turn data into critical information and knowledge that can be used to make sound business decisions.', 'Are you compelled to initiate action and remain proactive in getting things done? ', 'Do you enjoy working with other people and finding solutions when everyone else only see problems? ', 'Ability to work with other department supervisors.', 'SQL development', 'Advocate of CI/CD methodologies and agile ways of working.', 'Build conceptual and logical data models.', 'Ability to work with and communicate with all Credit Union personnel in the various departments.', 'Design and development focus include but is not limited to dimensional data model design, ETL/ELT frameworks and processing, meta-data management, operational data integration, master data management and data quality, data auditing and profiling, and business intelligence reporting solutions.', 'Responsible for the full life cycle development, implementation, support, architecture and tuning of the Enterprise Data Warehouse, including Data Marts, Data Lake and data pipelines.', 'Support the integration of enterprise application databases and real time processing into the data warehouse.', 'Can you handle multiple projects at the same time and smile? ', 'Experience with business intelligence visualization tools and business intelligence tools, such as, Microsoft Power BI', 'Identify, design, and implement internal process improvements including automating manual processes, optimize data delivery and designing cloud infrastructure for greater scalability, etc..', 'Ensure all data sources are accurate, congruent, reliable, and secure.', 'Technical skill-sets needed include but not limited to SQL, ELT/ETL development, configuring and maintaining data warehousing solutions, data modeling, data lake technologies, data architecture, and BI reporting development.', 'Responsible for architecting, configuring, analyzing, and maintaining all data structures used for BI analytics and reporting.', 'Technologies involved but not limited to Azure Data Lake, Azure Data Factory, Azure Data-bricks, Azure Synapse, SQL Server, Power BI, Oracle database, AI/ML', 'Do you take pride in the work you accomplish? ', 'Write technical policies, procedures, and documentation for the systems including data flow diagrams, architecture diagrams, etc.', 'Contribute to a team responsible for the design, development and implementation of mission critical business intelligence reports and applications to enterprise scale.', 'Does success motivate you to want to do more and be better? ', 'Microsoft SQL Server, Oracle, and other relational and NoSQL databases.SQL developmentExperience with business intelligence visualization tools and business intelligence tools, such as, Microsoft Power BIData warehousing, Data Lake, data modeling, data pipelinesELT/ETL development.Knowing programming languages such as Java, R, Python is a plus but is not required.Source-code management tools such as GitHub2+ years’ experience working in cloud computing with Azure experience requiredKnowledge of data processing, hardware platforms, and enterprise software Technical experience with enterprise systems, databases, and user support.Advocate of CI/CD methodologies and agile ways of working.Ability to maintain confidentiality of Credit Union and member records at all times.Self-motivated with the ability to prioritize,meetdeadlines,andmanagechangingWillingness to work occasionally outside of normal business hours.Excellent English oral and written communication skills.Ability to work with other department supervisors.Strong analytical and problem-solving Ability to maintain confidentiality of Credit Union and member records at all times.Ability to work with and communicate with all Credit Union personnel in the various departments.', 'Work closely with team members and BI Analysts to turn data into critical information and knowledge that can be used to make sound business decisions.Responsible for architecting, configuring, analyzing, and maintaining all data structures used for BI analytics and reporting.Ensure all data sources are accurate, congruent, reliable, and secure.Responsible for the full life cycle development, implementation, support, architecture and tuning of the Enterprise Data Warehouse, including Data Marts, Data Lake and data pipelines.Define and build data integration processes to be used across the organization.Build conceptual and logical data models.Identify, design, and implement internal process improvements including automating manual processes, optimize data delivery and designing cloud infrastructure for greater scalability, etc..Ensure all data accesses, along with data at rest or in transit is secure and follows the best in class data governance standards.Support the integration of enterprise application databases and real time processing into the data warehouse.Technical skill-sets needed include but not limited to SQL, ELT/ETL development, configuring and maintaining data warehousing solutions, data modeling, data lake technologies, data architecture, and BI reporting development.Technologies involved but not limited to Azure Data Lake, Azure Data Factory, Azure Data-bricks, Azure Synapse, SQL Server, Power BI, Oracle database, AI/MLThis person will work closely with external professional services and third-party vendors to have a full knowledge of systems and help establish road-map for future enhancements.Write technical policies, procedures, and documentation for the systems including data flow diagrams, architecture diagrams, etc.Contribute to a team responsible for the design, development and implementation of mission critical business intelligence reports and applications to enterprise scale.Design and development focus include but is not limited to dimensional data model design, ETL/ELT frameworks and processing, meta-data management, operational data integration, master data management and data quality, data auditing and profiling, and business intelligence reporting solutions.Other duties as assigned.', 'Microsoft SQL Server, Oracle, and other relational and NoSQL databases.', 'Excellent English oral and written communication skills.', 'Ensure all data accesses, along with data at rest or in transit is secure and follows the best in class data governance standards.', 'Knowledge of data processing, hardware platforms, and enterprise software Technical experience with enterprise systems, databases, and user support.', 'Willingness to work occasionally outside of normal business hours.', 'This person will work closely with external professional services and third-party vendors to have a full knowledge of systems and help establish road-map for future enhancements.', 'Data warehousing, Data Lake, data modeling, data pipelines', 'ELT/ETL development.', 'Experience In', 'Knowing programming languages such as Java, R, Python is a plus but is not required.', 'Strong analytical and problem-solving Ability to maintain confidentiality of Credit Union and member records at all times.', 'Ability to maintain confidentiality of Credit Union and member records at all times.', 'Define and build data integration processes to be used across the organization.']",Entry level,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Data Engineer,WellSky,"Overland Park, KS",2 days ago,Be among the first 25 applicants,"['', 'Bonus Points: Go to the top of the stack if you have any Qlik certification! ', 'You will join a team of top engineers creating interactive reports and dashboards with stunning visualizations. ', 'Familiarity with ETL/ELT processes, data pipelines, data lakes, etc. ', 'Demonstrated ability to work with cross-functional teams in a fast paced, agile environment', ' Do you have what it takes? ', 'Author of code/scripts to power visualizations for complex scenarios ', 'Preferred Experience: \u202f ', 'About WellSky', ' Bonus Points: Go to the top of the stack if you have any Qlik certification!  ', ' A day in the life! ', 'Advanced SQL knowledge working with a variety of databases', 'You will be responsible for the following:\u202f ', 'Your hard work will touch the lives of real people and families navigating life and death issues with the support of our solutions. ', 'Extensive experience in designing and developing BI dashboards, using modern tools (Qlik, Tableau, Sisense, etc.) ', ' At least 3 years direct, hands on experience with Qlik Products (Sense, View, Data Catalyst) Extensive experience in designing and developing BI dashboards, using modern tools (Qlik, Tableau, Sisense, etc.)  Advanced SQL knowledge working with a variety of databases Strong command of SQL from a BI dashboard, reporting context ', 'We seek to build purpose-driven teams where comradery and compassion are coupled with a dogged pursuit of excellence. ', 'At least 3 years direct, hands on experience with Qlik Products (Sense, View, Data Catalyst)', 'Strong command of SQL from a BI dashboard, reporting context', 'Preferred Certifications', 'Bonus Points', 'C# and/or .Net Core API development, or other OO type programming language ', 'Required Experience', ' Do you stand above the rest? ', 'You will have direct impact on the users of our solutions, mainly doctors, nurses, and others on the front lines of healthcare and community services. ', 'Meet WellSky', ' You will join a team of top engineers creating interactive reports and dashboards with stunning visualizations.  You will have direct impact on the users of our solutions, mainly doctors, nurses, and others on the front lines of healthcare and community services.  Your hard work will touch the lives of real people and families navigating life and death issues with the support of our solutions.  We seek to build purpose-driven teams where comradery and compassion are coupled with a dogged pursuit of excellence.  ', ' C# and/or .Net Core API development, or other OO type programming language  Familiarity with ETL/ELT processes, data pipelines, data lakes, etc.  Author of code/scripts to power visualizations for complex scenarios  Demonstrated ability to work with cross-functional teams in a fast paced, agile environment ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Node.Digital,"Tysons Corner, VA",4 days ago,Be among the first 25 applicants,"['', ' Must be a U.S. citizen with the ability to obtain and maintain a Federal Government clearance.', 'Company Overview', ' Develop extensibility programming within the Costpoint software platform', ' Bachelors degree required. Degree in Information Technology preferred. Degree can be substituted with equivalent professional experience.', ' Experience with data warehousing - Kimball method preferred', ' 5 - 8 years working with Microsoft SQL, writing custom code to support business processes, data management, and reporting.', ' Develop and manage system integrations across all corporate systems', ' Familiar with standard accounting concepts and methods', ' Experience with database design ERD, working/temp tables, tablespaces, partitions, indexes, dynamic object sizing, statistics.', ' Experience with programming Web Services technologies - REST, SOAP, Java, and Costpoint Integration Console Experience with database programming - SQL Server scripting preferred and developing triggers, stored procedures, indexes, functions, tables, views, linked servers, scripts, and queries. Experience with database design ERD, working/temp tables, tablespaces, partitions, indexes, dynamic object sizing, statistics. Experience with systems/data integration software and products Streamsets preferred Experience with working on project teams to implement software upgrades, process improvements, and software implementations Understanding of the underlying Costpoint and T&E databases Experience with data warehousing - Kimball method preferred Ability to analyze problems to provide the best solution. Good communication skills, specifically: listening to customer needs, restating those needs, and documenting your work Strong time management skills and ability to coordinate and prioritize tasks with little supervision. Able to adapt to changing work efforts, manage impact of shifting priorities, and resolve technical emergencies as they may occur.', ' Experience with programming Web Services technologies - REST, SOAP, Java, and Costpoint Integration Console', 'Requirements', ' Strong time management skills and ability to coordinate and prioritize tasks with little supervision.', ' Experience with the following systems; Streamsets, Databasics, iCIMS, ServiceNow, Tableau, and Jira', ' 5 years programming experience in Java, C#, or similar language', ' Creating and supporting data integrations with Deltek systems 5 - 8 years using and administering Deltek products such as Costpoint 7.1, T&E 9/10 Experience with the following systems; Streamsets, Databasics, iCIMS, ServiceNow, Tableau, and Jira Familiar with standard accounting concepts and methods', ' Experience with systems/data integration software and products Streamsets preferred', 'Security Requirements', 'Responsibilities', ' 5 - 8 years using and administering Deltek products such as Costpoint 7.1, T&E 9/10', ' Experience with database programming - SQL Server scripting preferred and developing triggers, stored procedures, indexes, functions, tables, views, linked servers, scripts, and queries.', 'Qualifications', ' Manage the relationships between financial systems and the dependencies within each application as well as the integrations with LMI systems outside of the Deltek suite of products.', ' Provide backup administration support for Deltek Costpoint 7, Time & Expense 9, and Cognos 10.2 Business Intelligence to include application updates and backups', ' Assist (as necessary) in gathering and documenting user and process requirements.', ' Ability to analyze problems to provide the best solution.', ' Good communication skills, specifically: listening to customer needs, restating those needs, and documenting your work', ' Support the execution of training scripts and training plans.', ' Understanding of the underlying Costpoint and T&E databases', ' 5 - 8 years creating and supporting data integrations with ERP and other corporate business systems', ' Creating and supporting data integrations with Deltek systems', ' Provide guidance to IT project managers on levels of effort, as well as recommendations for adherence to requirements, project plans, timelines, and deliverables.', ' Conduct technical demonstrations for employees to help influence adoption and use of Costpoint and other Deltek products.', ' Experience with working on project teams to implement software upgrades, process improvements, and software implementations', ' Provide technical support to system users and functional subject matter experts; contribute to developing and leading subject matter expert teams across various business units and matrixed service lines', ' Bachelors degree required. Degree in Information Technology preferred. Degree can be substituted with equivalent professional experience. 5 - 8 years creating and supporting data integrations with ERP and other corporate business systems 5 - 8 years working with Microsoft SQL, writing custom code to support business processes, data management, and reporting. 5 years programming experience in Java, C#, or similar language', ' Manage the relationships between financial systems and the dependencies within each application as well as the integrations with LMI systems outside of the Deltek suite of products. Develop and manage system integrations across all corporate systems Develop extensibility programming within the Costpoint software platform Provide technical support to system users and functional subject matter experts; contribute to developing and leading subject matter expert teams across various business units and matrixed service lines Provide backup administration support for Deltek Costpoint 7, Time & Expense 9, and Cognos 10.2 Business Intelligence to include application updates and backups Provide guidance to IT project managers on levels of effort, as well as recommendations for adherence to requirements, project plans, timelines, and deliverables. Conduct technical demonstrations for employees to help influence adoption and use of Costpoint and other Deltek products. Assist (as necessary) in gathering and documenting user and process requirements. Support the execution of training scripts and training plans.', 'Preferred Experience', ' Able to adapt to changing work efforts, manage impact of shifting priorities, and resolve technical emergencies as they may occur.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
"Software Engineer, Data",Knock,"Seattle, WA",3 days ago,64 applicants,"['', ' Build data pipelines and aggregate data. Design data schemas and optimize internal data warehouses, augmenting data from multiple sources. Design, build, and maintain REST APIs to serve data to customers Cross-functionally collaborate with our Data Science and Machine Learning teams.  Understand the data that powers our applications, and be able to propose appropriate data models for new features. Build new ETL jobs from scratch, as well as maintain existing jobs. Be committed to good engineering practice of testing, logging, alerting and deployment processes. Monitor and troubleshoot operational or data issues in the data pipelines. Drive architectural plans and implementation for future data storage, reporting, and analytic solutions. ', 'Your experience in SQL, MySQL, or Postgres will be valuable here to identify slow queries and debugging', 'Drive architectural plans and implementation for future data storage, reporting, and analytic solutions.', ' Knock is a 100% remote, work from home culture and has been since our inception in 2015  100% employee covered medical, dental, & vision premiums  Unlimited PTO (2 weeks mandatory) + flexible work schedules  Paid parental leave  $1,000 each year for education, training, and professional development  Salary + Equity. We offer equitable compensation based market data, accomplishments, experience, & your location ', 'All of our teams have a strong customer-first mindset and data-driven approach to our work, and that should be your approach as well', 'Knock is a 100% remote, work from home culture and has been since our inception in 2015 ', 'Salary + Equity. We offer equitable compensation based market data, accomplishments, experience, & your location', '100% employee covered medical, dental, & vision premiums ', 'Design data schemas and optimize internal data warehouses, augmenting data from multiple sources.', 'Please no recruitment firm or agency inquiries, you will not receive a reply from us.', 'Have proven success working 100% remote in prior positions & are experienced working with a distributed, national team. ', 'PDF is our preferred format for resumes and any other attachments. Thank you!', 'Understand the data that powers our applications, and be able to propose appropriate data models for new features.', 'Have experience building data pipelines and utilizing programming tools to do so. Here Knock we use Apache Spark, Scala, Rust, Go, Python, and Rest APIs - but you can learn these technologies on the job', ' Have experience building data pipelines and utilizing programming tools to do so. Here Knock we use Apache Spark, Scala, Rust, Go, Python, and Rest APIs - but you can learn these technologies on the job You should be versed in developing APIs to serve data produced by ETL jobs You have a desire to work at a rapidly growing startup and make it a success, and are comfortable learning new technologies and tools Your experience in SQL, MySQL, or Postgres will be valuable here to identify slow queries and debugging All of our teams have a strong customer-first mindset and data-driven approach to our work, and that should be your approach as well Believe in creating diverse, equitable, and inclusive practices and programs that will further Knock’s commitment to making an impact, learning, putting people first, being open, and courageous. Have proven success working 100% remote in prior positions & are experienced working with a distributed, national team.  We encourage you to apply even if you don’t have every listed requirement.  ', 'Monitor and troubleshoot operational or data issues in the data pipelines.', 'We encourage you to apply even if you don’t have every listed requirement. ', '$1,000 each year for education, training, and professional development ', 'Cross-functionally collaborate with our Data Science and Machine Learning teams. ', 'Build data pipelines and aggregate data.', 'This position is in the continental United States.', 'Build new ETL jobs from scratch, as well as maintain existing jobs.', 'Paid parental leave ', 'Be committed to good engineering practice of testing, logging, alerting and deployment processes.', 'Benefits, Perks, & Enjoying Life', 'You should be versed in developing APIs to serve data produced by ETL jobs', 'Design, build, and maintain REST APIs to serve data to customers', 'Believe in creating diverse, equitable, and inclusive practices and programs that will further Knock’s commitment to making an impact, learning, putting people first, being open, and courageous.', 'Unlimited PTO (2 weeks mandatory) + flexible work schedules ', 'You have a desire to work at a rapidly growing startup and make it a success, and are comfortable learning new technologies and tools']",Mid-Senior level,Full-time,Engineering,Internet,2021-03-24 13:05:10
Data Engineer,Louisiana Economic Development,"McLean, VA",2 weeks ago,Be among the first 25 applicants,[],Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,"Spneedel Technologies, Inc.","McLean, VA",6 days ago,Be among the first 25 applicants,[],Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer Analyst / Developer,Travelers,"Hartford, CT",2 weeks ago,40 applicants,"['', 'College Degree in STEM related field', ""Education:College Degree in STEM related fieldTechnical Knowledge:Knowledge of data and visualization tools (examples but not limited to): Visualization Platforms: MicroStrategy, Cognos, QlikView, etc.- Programming languages: SQL, SAS, R, Python, etc.- Database Platforms: SQL Server (SSIS, SSRS), Teradata, Hadoop, AWS, etc.Experience:Prior relevant experience.Job Specific Technical Skills & CompetenciesCommunication Skills:Ability to communicate thoughts/ideas clearly.Demonstrates basic active and effective communication skills with team members - including active listening and effective written and verbal communication skills.Effectively contributes and communicates with the immediate team.Problem Solving & Decision Making:Able to recognize and analyze basic issues and develop timely, practical, cost effective solutions with supervisory assistance.Able to identify cause and effect relationships in assigned systems and applications.Relationship Management:Works with other technical areas to achieve project/department or division goals.Ability to foster relationships with peers to achieve objectives. Practices objectivity and openness to others' views.Planning and Project Management:Ability to manage time and competing priorities.Developing the ability to accurately evaluate and estimate new tasks.Developing a planning skill set including providing management with accurate and timely status information.Employment PracticesTravelers is an equal opportunity employer. We value the unique abilities and talents each individual brings to our organization and recognize that we benefit in numerous ways from our differences.If you have questions regarding the physical requirements of this role, please send us an email so we may assist you.Travelers reserves the right to fill this position at a level above or below the level included in this posting."", 'Performs analysis of moderately complex data sources (Ops, Customer, Digital) to determine value and use. Recommends data to include in reporting and analytical projects.', 'Education:', 'Minimum Qualifications', 'Ability to communicate thoughts/ideas clearly.', 'Effectively contributes and communicates with the immediate team.', 'Education, Work Experience, & Knowledge', 'Technical Knowledge:', ' Visualization Platforms: MicroStrategy, Cognos, QlikView, etc.- Programming languages: SQL, SAS, R, Python, etc.- Database Platforms: SQL Server (SSIS, SSRS), Teradata, Hadoop, AWS, etc.', 'Applies moderate data derivations and assists with moderate business transformation rules and data requirements. ', 'Identify issues and recommend solutions.', 'Learning and partner with others in providing training to users.', 'Demonstrates basic active and effective communication skills with team members - including active listening and effective written and verbal communication skills.', 'Data Analysis, Acquisition, Preparation, and Exploration:', 'Able to identify cause and effect relationships in assigned systems and applications.', 'Job Description Summary', 'Experience:', ""Ability to foster relationships with peers to achieve objectives. Practices objectivity and openness to others' views."", 'Data Culture:', 'Company Summary', 'Operationalizes and automates moderately complex data products independently (CCDP, SDP, Teradata)', 'Ability to manage time and competing priorities.', 'Knowledge of data and visualization tools (examples but not limited to):', 'Perform basic analysis with guidance of others.', 'Employment Practices', 'Prior relevant experience.', 'Under guidance operationalize and automate well defined simple data products.', 'Understand core data management competencies - data governance, data security, data quality.', 'Problem Solving & Decision Making:', ""Communication Skills:Ability to communicate thoughts/ideas clearly.Demonstrates basic active and effective communication skills with team members - including active listening and effective written and verbal communication skills.Effectively contributes and communicates with the immediate team.Problem Solving & Decision Making:Able to recognize and analyze basic issues and develop timely, practical, cost effective solutions with supervisory assistance.Able to identify cause and effect relationships in assigned systems and applications.Relationship Management:Works with other technical areas to achieve project/department or division goals.Ability to foster relationships with peers to achieve objectives. Practices objectivity and openness to others' views.Planning and Project Management:Ability to manage time and competing priorities.Developing the ability to accurately evaluate and estimate new tasks.Developing a planning skill set including providing management with accurate and timely status information."", 'Able to recognize and analyze basic issues and develop timely, practical, cost effective solutions with supervisory assistance.', 'Works with other technical areas to achieve project/department or division goals.', 'Target Openings', 'Begin to develop basic insurance and business intelligence and analytics knowledge through daily work assignments.', 'Assist with Data Acquisition, Prep and Exploration following well defined criteria and steps.', 'Communication Skills:', 'Relationship Management:', 'Developing the ability to accurately evaluate and estimate new tasks.', 'Data Solutions & Analytic Implementations:', 'Job Specific Technical Skills & Competencies', 'Data Analysis, Acquisition, Preparation, and Exploration:Applies moderate data derivations and assists with moderate business transformation rules and data requirements. Performs analysis of moderately complex data sources (Ops, Customer, Digital) to determine value and use. Recommends data to include in reporting and analytical projects.Assist with Data Acquisition, Prep and Exploration following well defined criteria and steps.Apply data derivations, business transformation rules, and data requirements.Perform basic analysis with guidance of others.Identify issues and recommend solutions.Data Solutions & Analytic Implementations:Under guidance operationalize and automate well defined simple data products.Learning and partner with others in providing training to users.Operationalizes and automates moderately complex data products independently (CCDP, SDP, Teradata)Data Culture:Begin to develop basic insurance and business intelligence and analytics knowledge through daily work assignments.Learn Travelers BI&A standards, processes, and environmental landscape.Understand core data management competencies - data governance, data security, data quality.', 'College degree or equivalent training with data tools, techniques, and manipulation experience required.', 'Planning and Project Management:', 'Apply data derivations, business transformation rules, and data requirements.', 'Learn Travelers BI&A standards, processes, and environmental landscape.', 'Developing a planning skill set including providing management with accurate and timely status information.', 'Primary Job Duties & Responsibilities']",Entry level,Full-time,Information Technology,Law Practice,2021-03-24 13:05:10
Data Engineer,Blueprint Technologies,"Dallas, TX",6 days ago,27 applicants,"['', 'Experience with Pyspark or Scala is a plus (Databricks or Spark)\u202f\u202f ', 'At least 3-years of experience as a software development or data engineer ', 'FLSA - Job Classification:', 'Data Engineer', 'What will I be doing?', 'Qualifications:', ' At least 3-years of experience as a software development or data engineer  At least 2-years of experience with SQL, Python and/or other data collection tools & reporting  Advanced knowledge and skills with AWS is required  Experience with Pyspark or Scala is a plus (Databricks or Spark)\u202f\u202f  Excellent collaboration skills to work on a team as well as independently (be self-reliant and resourceful) Excellent organization skills and able to multi-task and detailed oriented  Excellent verbal and written communication skills (must be able to write clear and concise emails for any audience, etc.  ', 'At least 2-years of experience with SQL, Python and/or other data collection tools & reporting ', 'Who is Blueprint?', 'Why Blueprint?', 'What does Blueprint do?', 'Location:', 'Excellent organization skills and able to multi-task and detailed oriented ', 'Excellent verbal and written communication skills (must be able to write clear and concise emails for any audience, etc. ', 'Excellent collaboration skills to work on a team as well as independently (be self-reliant and resourceful)', 'Advanced knowledge and skills with AWS is required ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer (Google Cloud Platform),"Metasys Technologies, Inc.","California, United States",1 day ago,47 applicants,"['', 'Java advanced', 'Apache Beam beginner level ok', 'Duration: 6+ Months\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0', 'SQL advanced', 'Python intermediate', 'Hadoop ecosystem intermediate', 'Experience building data pipelines in Dataflow and DatafusionApache Beam beginner level okHadoop ecosystem intermediate', 'Required Skills: ', '\xa0', 'Oracle intermediate', 'Good to Have Skills: ', 'Experience building data pipelines in Dataflow and Datafusion', 'Opening: Sr . Data Engineer (Google Cloud Platform )', 'Data import and export experienceSQL advancedJava advancedPython intermediateBig Query proficiencyOracle intermediateSQL Server intermediate', 'Data import and export experience', 'Duration: 6+ Months', 'Location: Mountain View, CA', 'Big Query proficiency', 'SQL Server intermediate']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer/Analyst - FHIR,Alignment Healthcare,"Orange County, CA",5 days ago,62 applicants,"['·\xa0\xa0\xa0\xa0\xa0\xa0Clear understanding & working knowledge of HIPAA protocols.', '·\xa0\xa0\xa0\xa0\xa0\xa0Strong HL7 FHIR v4 Knowledge', '·\xa0\xa0\xa0\xa0\xa0\xa0To perform this job successfully, an individual must be able to perform each essential duty satisfactorily.\xa0The requirements listed below are representative of the knowledge, skill, and/or ability required.\xa0Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.', 'Work Environment', '·\xa0\xa0\xa0\xa0\xa0\xa0Strong Data Modeling experience', '·\xa0\xa0\xa0\xa0\xa0\xa0Interview and hire necessary personnel.', 'Alignment Healthcare is a data and technology driven healthcare company focused partnering with health systems, health plans and provider groups to provide care delivery that is preventive, convenient, coordinated, and that results in improved clinical outcomes for seniors.\xa0', '·\xa0\xa0\xa0\xa0\xa0\xa0Understand and examine the current Healthcare data received from different applications/vendors & build the data modeling/mapping for converting the raw Healthcare data into FHIR format.', '·\xa0\xa0\xa0\xa0\xa0\xa0Strong Healthcare Payer and clinical data Experience', '·\xa0\xa0\xa0\xa0\xa0\xa0Strong appreciation of timeliness and the ability to multitask and accomplish assigned tasks within project timelines.', 'Minimum Experience:', 'Education/Licensure:', 'Alignment Healthcare was founded with a mission to revolutionize health care with a serving heart culture. Through its unique integrated care delivery models, deep physician partnerships and use of proprietary technologies, Alignment is committed to transforming health care one person at a time.', 'We are experiencing rapid growth (backed by top private equity firms), our Data Services and BI team is looking for the best and brightest leaders. Data drives the way we make decisions. We love our customers and understanding them better makes it possible to provide the best clinical outcome and care experience.', '·\xa0\xa0\xa0\xa0\xa0\xa0Extensive hands-on an experience with .Net, C#, Azure Data Factory, APIs and other cloud-based data services.', '·\xa0\xa0\xa0\xa0\xa0\xa0BS in Computer Science, IT or equivalent and/or equivalent experience.', '·\xa0\xa0\xa0\xa0\xa0\xa0Strong SQL Skills', '·\xa0\xa0\xa0\xa0\xa0\xa0Ability to effectively present information and respond to questions from groups of managers and customers.', '·\xa0\xa0\xa0\xa0\xa0\xa05+ years of experience in healthcare industry with proven understanding of data terminology.', '·\xa0\xa0\xa0\xa0\xa0\xa0Microsoft Azure Certification is a\xa0plus', '·\xa0\xa0\xa0\xa0\xa0\xa0Familiarity with Interoperability and Patient Access Final Law', '·\xa0\xa0\xa0\xa0\xa0\xa0Experience in implementing data modeling based on FHIR standards in Healthcare Domain in Azure Cloud', '·\xa0\xa0\xa0\xa0\xa0\xa0Conducting in-depth data analysis and perform data model design and data mapping activates, using spreadsheets, Visio or more specialized BI and Analytical tools', '·\xa0\xa0\xa0\xa0\xa0\xa0Creates and maintains positive, cohesive work environment.', '\xa0', 'Position Summary:\xa0', 'We are currently seeking a Data Engineer-FHIR Lead. FHIR® (Fast Health Interoperability Resources) is an HL7 standard which brings RESTful APIs and a common format for hundreds of clinical data models to healthcare.\xa0This position will play a key role in building FHIR platform operating on a cloud-based data platform and its pipelines using REST API’s, Microsoft Azure cloud services, machine learning and Big Data technologies.', '·\xa0\xa0\xa0\xa0\xa0\xa0Map data from a variety of complex data sources using a proprietary mapping tool into a FHIR model.', 'Other:', 'By becoming a part of the Alignment Healthcare team, you will provide members with the quality of care they truly need and deserve. We believe that great work comes from people who are inspired to be their best. We have built a team of talented and experienced people who are passionate about transforming the lives of the seniors we serve. In this fast-growing company, you will find ample room for growth and innovation alongside the Alignment community.', '·\xa0\xa0\xa0\xa0\xa0\xa0Demonstrated advanced analytical and reporting skills.', '·\xa0\xa0\xa0\xa0\xa0\xa0Effective negotiation skills.', '·\xa0\xa0\xa0\xa0\xa0\xa05+ years of experience with EDI/FHIR/HL7 platforms & technologies and knowledge of Healthcare Industry standards and requirements.', '·\xa0\xa0\xa0\xa0\xa0\xa0Strong knowledge in HL7 standard data formats and FHIR® formatted XML and JSON objects and need to know how to read and write functionality works based on the GET/PUT/POST/DELETE functions used in web-based APIs.\xa0', '·\xa0\xa0\xa0\xa0\xa0\xa0Coordinate with internal and external stakeholders to convert the Healthcare data into canonical FHIR format & make the data available in downstream usage.', 'Minimum Requirements:', '·\xa0\xa0\xa0\xa0\xa0\xa0The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job.\xa0Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.', '(May include but are not limited to)', '·\xa0\xa0\xa0\xa0\xa0\xa0Excellent human relations and verbal/written communication skills.', '·\xa0\xa0\xa0\xa0\xa0\xa0Collaborates with technical resources to perform root cause analysis and complete remediation of data quality issues', '·\xa0\xa0\xa0\xa0\xa0\xa0Participate in project meetings, providing input to project plans and providing status updates.', '·\xa0\xa0\xa0\xa0\xa0\xa0Experience with Healthcare data such as patient level data, claims data and provider data.', '·\xa0\xa0\xa0\xa0\xa0\xa0Analyze and Validate Data to ensure the appropriate data modeling, vocabulary, terminology, and code-set standard for representing and harmonizing Healthcare data', '·\xa0\xa0\xa0\xa0\xa0\xa0FHIR certification is a plus', '·\xa0\xa0\xa0\xa0\xa0\xa0Define the strategy, vision, goals, objectives and target key results for the development of HIE and interoperability digital services in Azure.', '·\xa0\xa0\xa0\xa0\xa0\xa0The liaison between Business and the Programming Teams.', '·\xa0\xa0\xa0\xa0\xa0\xa0Determines approaches to interoperability of Clinical/Healthcare data, including mapping of clinical content to data standards to support use case requirements.', '·\xa0\xa0\xa0\xa0\xa0\xa0Works closely with the Data Engineering & BI leadership to set goals and objectives.', 'General Duties/Responsibilities:', '·\xa0\xa0\xa0\xa0\xa0\xa05+ years database experience with MS SQL Server.', '·\xa0\xa0\xa0\xa0\xa0\xa0Interact with different business units translate their data needs into requirements.']",Mid-Senior level,Full-time,Information Technology,Hospital & Health Care,2021-03-24 13:05:10
Data Engineer - Big Data Platforms,"Lowe's Companies, Inc.","Charlotte, NC",2 weeks ago,Be among the first 25 applicants,"['', 'Responsibilities of Hadoop admin include – deploying a Hadoop cluster, maintaining a Hadoop cluster, adding and removing nodes using cluster monitoring tools like Ganglia Nagios or Cloudera Manager, configuring the Name Node high availability and keeping a track of all the running Hadoop jobs.Implementing, managing and administering the overall Hadoop infrastructure.Work closely with the data engineer, network, BI and application teams to make sure that all the big data applications are highly available and performing as expected.Hadoop admin is responsible for capacity planning and estimating the requirements for lowering or increasing the capacity of the Hadoop cluster.Setup/migrate job between on-prem to cloud and other way. Should have knowledge of cloud networking and IAM', 'About Lowe’s In The Community', '2-3 years of experience in Hadoop, NO-SQL, RDBMS or any Cloud Bigdata components, Teradata, MicroStrategy ', 'Expertise in Python, SQL, Scripting, Teradata, Hadoop utilities like Sqoop, Hive, Pig, Map Reduce, Spark, Ambari, Ranger, Kafka or equivalent Cloud Bigdata components', 'Develops, configures, or modifies integrated business and/or enterprise application solutions within various computing environments by designing and coding component-based applications using various programming languages', 'Key Responsibilities', 'Minimum Qualifications', 'Setup/migrate job between on-prem to cloud and other way. Should have knowledge of cloud networking and IAM', 'Understands Computer Science and/or Computer Engineering fundamentals; knows software architecture and readily applies this to Platform solutions', 'Translates business requirements and specifications into logical program designs, modules, stable application systems, and data solutions with occasional guidance from senior colleagues; partners with Product Team to understand business needs and functional specificationsDevelops, configures, or modifies integrated business and/or enterprise application solutions within various computing environments by designing and coding component-based applications using various programming languagesConducts the implementation and maintenance of complex business and enterprise data solutions to ensure successful deployment of released applicationsSupports systems integration testing (SIT) and user acceptance testing (UAT), provides insight into defining test plans, and ensures quality software deploymentParticipates in the end-to-end product lifecycle by applying and sharing an in-depth understanding of company and industry methodologies, policies, standards, and controlsUnderstands Computer Science and/or Computer Engineering fundamentals; knows software architecture and readily applies this to Platform solutionsAutomates and simplifies team development, test, and operations processes; develops conceptual, logical and physical architectures consisting of one or more viewpoints (business, application, data, and infrastructure) required for business solution deliverySolves difficult technical problems; solutions are testable, maintainable, and efficient ', 'Preferred Qualifications', 'Platform Engineering Qualifications', '2 years of IT experience developing and implementing business systems within an organization', 'Participates in the end-to-end product lifecycle by applying and sharing an in-depth understanding of company and industry methodologies, policies, standards, and controls', 'Translates business requirements and specifications into logical program designs, modules, stable application systems, and data solutions with occasional guidance from senior colleagues; partners with Product Team to understand business needs and functional specifications', 'Job Summary', 'Solves difficult technical problems; solutions are testable, maintainable, and efficient ', 'Experience with application and integration middleware', '2-3 years of experience in Hadoop, NO-SQL, RDBMS or any Cloud Bigdata components, Teradata, MicroStrategy Expertise in Python, SQL, Scripting, Teradata, Hadoop utilities like Sqoop, Hive, Pig, Map Reduce, Spark, Ambari, Ranger, Kafka or equivalent Cloud Bigdata components', 'Platform Engineering Responsibilities', '2 years of experience leading teams, with or without direct reports', 'About Lowe’s', '2 years of experience working with an IT Infrastructure Library (ITIL) framework', '2 years of experience in Data, BI or Platform Engineering, Data Warehousing/ETL, or Software Engineering', 'Hadoop admin is responsible for capacity planning and estimating the requirements for lowering or increasing the capacity of the Hadoop cluster.', 'Supports systems integration testing (SIT) and user acceptance testing (UAT), provides insight into defining test plans, and ensures quality software deployment', '4 years of experience with technical documentation in a software development environment', 'Conducts the implementation and maintenance of complex business and enterprise data solutions to ensure successful deployment of released applications', '4 years of experience working with defect or incident tracking software', 'Experience with database technologies', ""Bachelor\\'s degree in Engineering, Computer Science, CIS, or related field (or equivalent work experience in a related field)2 years of experience in Data, BI or Platform Engineering, Data Warehousing/ETL, or Software Engineering1 year of experience working on project(s) involving the implementation of solutions applying development life cycles (SDLC)"", ""Bachelor\\'s degree in Engineering, Computer Science, CIS, or related field (or equivalent work experience in a related field)"", 'Work closely with the data engineer, network, BI and application teams to make sure that all the big data applications are highly available and performing as expected.', ""Master\\'s degree in Computer Science, CIS, or related field"", 'Responsibilities of Hadoop admin include – deploying a Hadoop cluster, maintaining a Hadoop cluster, adding and removing nodes using cluster monitoring tools like Ganglia Nagios or Cloudera Manager, configuring the Name Node high availability and keeping a track of all the running Hadoop jobs.', 'Implementing, managing and administering the overall Hadoop infrastructure.', '1 year of experience working on project(s) involving the implementation of solutions applying development life cycles (SDLC)', ""Master\\'s degree in Computer Science, CIS, or related field2 years of IT experience developing and implementing business systems within an organization4 years of experience working with defect or incident tracking software4 years of experience with technical documentation in a software development environment2 years of experience working with an IT Infrastructure Library (ITIL) framework2 years of experience leading teams, with or without direct reportsExperience with application and integration middlewareExperience with database technologies"", 'Automates and simplifies team development, test, and operations processes; develops conceptual, logical and physical architectures consisting of one or more viewpoints (business, application, data, and infrastructure) required for business solution delivery']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer-Azure,Nesco Resource,"Austin, TX",2 weeks ago,27 applicants,"['Advanced knowledge with SQL Server Database - including writing advanced SQL scripts, profiling and optimization.', ""This role can sit remote from anywhere in the U.S. but the person must be willing to relocate to Austin if/when the office re-opens. Client will discuss relocation reimbursement options if necessary in that case.  There's a 15% bonus paid out annually based on team & individual performance. VP wants someone who has been through a Migration to Azure from a legacy system (SSIS to Azure is fine as well). "", 'Enjoy working in Agile as part of a scrum team', 'Goals for the team:', 'This is a permanent job, reporting to the VP of Data Engineering.', 'Strong experience with NoSQL database, including Postgres', 'KNOWLEDGE/SKILLS: ', 'Bachelors/Masters in Computer Science, MIS/Information Management, Engineering or related fieldStrong experience with NoSQL database, including PostgresPrior experience\xa0working with Azure Cloud Services: Data Factory, SQL database, Functions, Data Lake, Databricks, Logic Apps, and Azure Automation.Fluent with the following scripting languages: Python, Scala, and C#Advanced knowledge with SQL Server Database - including writing advanced SQL scripts, profiling and optimization.Working knowledge of BI Tools: MS Integration Services, Reporting Services, and Analysis Services, and Power BIPrior experience working with other Big Data tools such as Spark, Snowflake, and KafkaPrefer someone with experience working in Financial Services, Wealth Mgmt. Industry', ' ', 'We are looking for a passionate data engineer to join our growing Data and Analytics team on our journey in modernizing our platform to the next-generation cloud platform. You will be responsible for expanding, optimizing, and improving overall data quality and set up next-generation data orchestration using modern cloud tools and technologies. ', 'Prior experience working with other Big Data tools such as Spark, Snowflake, and Kafka', 'Work with and support\xa0an Application Engineering team, DBA, Infrastructure, and Project Management Office.', 'Design/develop data pipelines to extract data from multiple data sources using Azure, Snowflake Cloud, and other cloud-native technologies', ' - Completely decommission their Data Center by EOY ', 'Prefer someone with experience working in Financial Services, Wealth Mgmt. Industry', 'KNOWLEDGE/SKILLS:', ' Duties/Responsibilities: ', '\xa0', 'Description', 'Bachelors/Masters in Computer Science, MIS/Information Management, Engineering or related field', ' - Migrate data processes using ETL to the new platform in the cloud which includes integrating all the applications on the existing platform ', 'Fluent with the following scripting languages: Python, Scala, and C#', 'Working knowledge of BI Tools: MS Integration Services, Reporting Services, and Analysis Services, and Power BI', 'Design/develop data pipelines to extract data from multiple data sources using Azure, Snowflake Cloud, and other cloud-native technologiesBuild out a data model to gain actionable insights from data, operational efficiency, and other key business performance metrics.Design, manage, & monitor inbound and outbound data processesEnjoy working in Agile as part of a scrum teamAutomate the data testing processes and integrate them with monitoring systemsWork with and support\xa0an Application Engineering team, DBA, Infrastructure, and Project Management Office.Analyze existing legacy systems and data sets to help Business Analysts define the functional/non-functional requirements.Meet with the business users, assist with data-related technical issues, and support their data infrastructure needs.', 'Design, manage, & monitor inbound and outbound data processes', 'Goals for the team: ', 'Duties/Responsibilities:', 'Meet with the business users, assist with data-related technical issues, and support their data infrastructure needs.', 'Automate the data testing processes and integrate them with monitoring systems', 'Build out a data model to gain actionable insights from data, operational efficiency, and other key business performance metrics.', 'Description: ', 'Prior experience\xa0working with Azure Cloud Services: Data Factory, SQL database, Functions, Data Lake, Databricks, Logic Apps, and Azure Automation.', 'Analyze existing legacy systems and data sets to help Business Analysts define the functional/non-functional requirements.']",Mid-Senior level,Full-time,Information Technology,Financial Services,2021-03-24 13:05:10
Data Engineer,Evolytics,"Kansas City, MO",6 days ago,Be among the first 25 applicants,"['', ' Working knowledge in any of the following database systems will be a plus: NoSQL, Mongo DB, Couch DB', ' Named Top Analytics Agency by the Digital Analytics Association in 2018 and again in 2020', ' Advanced SQL', ' Legally authorized to work in the United States without company sponsorship now or in the future', ' Creating and preparing databases and tables for predictive modeling and data science applications', 'Other Experience That Is Helpful, But Not Required', ' Build and maintain database architecture, including fine tuning and optimizing queries, data pipelines, and automation workflows.', ' Opportunity to work alongside amazingly fun people who are passionate about delivering awesome in everything they do.', ' Proficiency in spreadsheet and presentation technologies such as Excel, PowerPoint, or Google Docs', ' Awesome team building events like a day at the Royals game or mini-golf with margaritas', ' Create, prepare, and maintain databases and tables to power reports, dashboards, predictive models, and downstream analysis.', ' Voted Coolest Office Space (Small Business) by the Kansas City Business Journal', ' Design and build data infrastructure that enables actionable insights used to optimize digital marketing performance such as online advertising, social media marketing, websites, and mobile experiences.', ' Knowledge of business intelligence methodologies and tools', ' Foodweekly lunches, daily snacks, fruits, beverages, unlimited coffee', ' Great Compensation Package with Paid Time Off, Performance Bonuses and IRA Matching Contributions', ' Validate data to determine and document any gaps between available data and requirements for reporting outputs and downstream analysis.', ' Honored for Best Places to Work in 2020 by the Kansas City Business Journal', ""What You'll Be Doing"", ' Developing and implementing data transformation via ETL processes and data pipelines', ' Working knowledge in any of the following database systems will be a plus: NoSQL, Mongo DB, Couch DB Working with clickstream web analytics tools such as Adobe Analytics (Omniture SiteCatalyst), Google Analytics, or working knowledge of the field of web analytics Knowledge of commonly-used digital metrics, analytic concepts, and online marketing channel best practices Working with data analysis tools such as SAS, Tableau, Google Data Studio, or Power BI Knowledge of business intelligence methodologies and tools Proficiency in spreadsheet and presentation technologies such as Excel, PowerPoint, or Google Docs Creating and preparing databases and tables for predictive modeling and data science applications', ' Working in big data solutions such as Hadoop, Hive, or Spark', ' Collaboration-oriented office space with plenty of room for working sessions or potlucks', ' Working with a leading analytics or relational database system, such as Redshift, Vertica, BigQuery, PostgreSQL, or MySQL', ' Develop processes and procedures for ingesting data from disparate sources.', ' Competitive Benefits Package including Health, Dental, Vision, and Life Insurance', ' Working knowledge in at least one of the following scripting languages: Python, Bash, Java, Scala, R, Perl, Node.js', ' Relaxed work environment: casual dress code, pool/ping pong table, treadmill desks Collaboration-oriented office space with plenty of room for working sessions or potlucks Awesome team building events like a day at the Royals game or mini-golf with margaritas Foodweekly lunches, daily snacks, fruits, beverages, unlimited coffee Learning opportunities: company-provided training, conferences and super-smart co-workers', ' Relaxed work environment: casual dress code, pool/ping pong table, treadmill desks', 'About Evolytics', ' Using change release processes and tools such as git', ' Recognized as a Great Place to Work in 2020', ' Working with clickstream web analytics tools such as Adobe Analytics (Omniture SiteCatalyst), Google Analytics, or working knowledge of the field of web analytics', ' Create and maintain customized SQL queries to build reporting data structures.', ' Fortunes Best Small and Medium Workplaces in 2020: #25 in US', ' Plan, create, and fine-tune data pipelines and automation workflows.', 'Benefits', ' Fortunes Best Small and Medium Workplaces in 2020: #25 in US Honored for Best Places to Work in 2020 by the Kansas City Business Journal Recognized as a Great Place to Work in 2020 Voted Coolest Office Space (Small Business) by the Kansas City Business Journal Named Top Analytics Agency by the Digital Analytics Association in 2018 and again in 2020', ' Develop and implement data models necessary to build analytic solutions as defined by stakeholder requirements.', ' Manage multiple client requests and detailed project activities at any one time to ensure accurate, timely and efficient reporting and analysis deliverables.', 'Perks', ' Linux command line', ' Competitive Benefits Package including Health, Dental, Vision, and Life Insurance Great Compensation Package with Paid Time Off, Performance Bonuses and IRA Matching Contributions Opportunity to work alongside amazingly fun people who are passionate about delivering awesome in everything they do.', ' Working with data analysis tools such as SAS, Tableau, Google Data Studio, or Power BI', ' Learning opportunities: company-provided training, conferences and super-smart co-workers', ' Working knowledge in at least one of the following scripting languages: Python, Bash, Java, Scala, R, Perl, Node.js Linux command line Advanced SQL Working with a leading analytics or relational database system, such as Redshift, Vertica, BigQuery, PostgreSQL, or MySQL Developing cloud-based data solutions on Snowflake, AWS, Azure, or Google Cloud Working in big data solutions such as Hadoop, Hive, or Spark Using change release processes and tools such as git Developing and implementing data transformation via ETL processes and data pipelines Legally authorized to work in the United States without company sponsorship now or in the future Minimum of a Bachelors degree in Computer Science, Information Systems, Business, Marketing, or a related discipline', ' Minimum of a Bachelors degree in Computer Science, Information Systems, Business, Marketing, or a related discipline', ' Develop processes and procedures for ingesting data from disparate sources. Build and maintain database architecture, including fine tuning and optimizing queries, data pipelines, and automation workflows. Create and maintain customized SQL queries to build reporting data structures. Develop and implement data models necessary to build analytic solutions as defined by stakeholder requirements. Validate data to determine and document any gaps between available data and requirements for reporting outputs and downstream analysis. Manage multiple client requests and detailed project activities at any one time to ensure accurate, timely and efficient reporting and analysis deliverables.', ' Developing cloud-based data solutions on Snowflake, AWS, Azure, or Google Cloud', ' Create, prepare, and maintain databases and tables to power reports, dashboards, predictive models, and downstream analysis. Plan, create, and fine-tune data pipelines and automation workflows. Design and build data infrastructure that enables actionable insights used to optimize digital marketing performance such as online advertising, social media marketing, websites, and mobile experiences.', ' Knowledge of commonly-used digital metrics, analytic concepts, and online marketing channel best practices']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
"Jr. Data Engineer,  Business Intelligence",Essence,New York City Metropolitan Area,1 week ago,126 applicants,"['', 'Desirable', 'Support the development strong working relationships with third-party data providers that we rely on for access to necessary data', 'Analytically minded, enabling you to understand and overcome technically complex challenges, and to tell compelling stories with data', 'Experience using or building reports with business intelligence software, ideally Google DataStudio', 'Delivers best results when working in a team environment, and an ability to partner effectively with people of varying degrees of technical capability', 'Support other members of the team responsible for “last mile” transformation and visualization of data within Google Data Studio reports and dashboards', 'Provide hands-on support to users of reportion solutions, helping the wider team triage and respond to user queries in a timely manner', 'Work experience within a marketing organization, preferably at a media agency or related company (e.g. publisher, ad tech, client marketing org)', 'Assist the development of technical solutions, in line with specifications,\xa0 that collect, store and transform disparate data sourcesAssist the development and maintenance of automated jobs that ensure required data is made available in an efficient and scalable way as possibleAssist the development and maintenance of data quality checks and procedures, helping account teams overcome data issues before they impact the quality of the reporting solutionSupport the translation of user requirements and business needs into technical specificationsBecome a proficient user of Google Cloud Platform and associated technologies, ensuring your work incorporates industry best practiceMonitor automated jobs, troubleshooting data issues as-and-when they ariseSupport other members of the team responsible for “last mile” transformation and visualization of data within Google Data Studio reports and dashboardsProvide hands-on support to users of reportion solutions, helping the wider team triage and respond to user queries in a timely mannerAttend internal stakeholder meetings, presenting your solutions and providing updates on your work.Support the development strong working relationships with third-party data providers that we rely on for access to necessary data', 'Support the translation of user requirements and business needs into technical specifications', 'Previous experience working with data and technologyExperience with digital marketing platforms and the data they generate, in particular Google Marketing Platform, Facebook, Twitter etc.\xa0\xa0An understanding of how data is tracked and exchanged in the process of digital advertising (e.g. role of ad servers and other third-party tech vendors)Experience using or building reports with business intelligence software, ideally Google DataStudioWork experience within a marketing organization, preferably at a media agency or related company (e.g. publisher, ad tech, client marketing org)', 'Eagerness to learn and become a better programmerSome experience with programming and/or statistical languages (e.g. SQL, Python)Analytically minded, enabling you to understand and overcome technically complex challenges, and to tell compelling stories with dataStrong organizational skills and attention to detail, including the ability to manage multiple tasks in a fairly autonomous wayStrong spoken and written communication skills, ensuring your thoughts and needs are heard and understoodAn ability to demonstrate a passion for the digital marketing ecosystem, and an understanding of the role that data plays within itDelivers best results when working in a team environment, and an ability to partner effectively with people of varying degrees of technical capability', 'Experience with digital marketing platforms and the data they generate, in particular Google Marketing Platform, Facebook, Twitter etc.\xa0\xa0', 'Required', ':', 'Monitor automated jobs, troubleshooting data issues as-and-when they arise', 'Eagerness to learn and become a better programmer', 'Assist the development and maintenance of data quality checks and procedures, helping account teams overcome data issues before they impact the quality of the reporting solution', 'An ability to demonstrate a passion for the digital marketing ecosystem, and an understanding of the role that data plays within it', 'Strong spoken and written communication skills, ensuring your thoughts and needs are heard and understood', 'Attend internal stakeholder meetings, presenting your solutions and providing updates on your work.', 'Strong organizational skills and attention to detail, including the ability to manage multiple tasks in a fairly autonomous way', 'About Essence', 'As a Junior Data Engineer your primary responsibility will be to support a Senior Data Engineer to create and maintain underlying data infrastructure that provides the wider team with the data they need to provide timely, accurate and meaningful deliverables & reporting.\xa0 In doing so you will gain a foundational understanding of cloud technology and key data engineering skills and knowledge to help you build a career in this fast evolving, and in demand, industry.', 'A bit about yourself:', 'This role forms part of a globally distributed business intelligence team, whose objective\xa0 is to ensure that one of our most important global accounts have access to the right data and insights in order to inform their marketing decisions.\xa0\xa0', 'Previous experience working with data and technology', 'What you can expect from Essence', 'Assist the development of technical solutions, in line with specifications,\xa0 that collect, store and transform disparate data sources', 'Some experience with programming and/or statistical languages (e.g. SQL, Python)', 'Become a proficient user of Google Cloud Platform and associated technologies, ensuring your work incorporates industry best practice', 'An understanding of how data is tracked and exchanged in the process of digital advertising (e.g. role of ad servers and other third-party tech vendors)', 'The team’s primary responsibility is to maintain a global Google Cloud based reporting solution, which automates the collection and transformation of disparate marketing data into a single source of truth.\xa0 Not only does that mean running and maintaining the solution that already exists, but also continually improving it to incorporate new data sources, and to derive new insights, to support ever-evolving business demands.', 'Assist the development and maintenance of automated jobs that ensure required data is made available in an efficient and scalable way as possible', 'The Role:', 'Some of the things we’d like you to do:', 'Visit essenceglobal.com for more information and follow us on Twitter at @essenceglobal.', 'Essence’s mission is to make advertising more valuable to the world.\xa0 We do this by employing the world’s very best talent to solve some of the toughest challenges of today’s digital marketing landscape.\xa0 It’s important that we hire people whose values reflect those of our own: genuine, results-focused, daring and insightful.\xa0 As an Essence employee, we promise you a workplace that invests in your career, cares for you and is fun and engaging.\xa0 We believe these factors create a workplace where you can be yourself and do amazing work.', ""Essence, part of GroupM, is a global data and measurement-driven media agency whose mission is to make brands more valuable to the world. Clients include Google, Flipkart, NBCUniversal, L'Oréal and the Financial Times. The agency is more than 2,000 people strong, manages $4.5B in annualized media spend, and deploys campaigns in 121 markets via 22 offices in APAC, EMEA and the Americas.\xa0""]",Associate,Full-time,Engineering,Marketing and Advertising,2021-03-24 13:05:10
SQL Data Engineer,Sequoia Consulting Group,"Tempe, AZ",2 weeks ago,Be among the first 25 applicants,"['', 'Ability to succeed in a dynamic, Agile environment Strong prioritization and time-management skills Dedication to team goals that include support of live 24/7 production systems A consummate collaborator, able to establish good relationships with technical, product, and business owners A champion of quality, able to QA and vouch for the integrity of the report output Maintaining business partner engagement and setting expectations Assessing current processes and recommending changes as needed Documenting and communicating technical specifications to ensure that proper and optimized techniques, queries, data standards, and final outputs are understood and incorporated into data and analytics processes Participate in business analysis activities to gather required reporting and dashboard requirements Translate business requirements into specifications that will be used to implement the required user-friendly environments, reports and dashboards, built from potentially multiple data sources ', 'Assess fitness-for-purpose of existing data model and processes ', 'Design and implement effective database solutions and models to store and retrieve company data ', 'Working familiarity with Salesforce (SFDC) ', 'A consummate collaborator, able to establish good relationships with technical, product, and business owners ', '3+ years’ experience in data modeling and architecting, ETL, data engineering, or BI fields with concentration on data transformations and data modeling ', 'Assess database implementation procedures to ensure they follow internal and external regulations ', 'What Does the Role Entail?', 'Lead all aspects of the migration of data from legacy systems to new solutions ', 'Monitor the system performance by performing regular tests, fixing, and integrating new features ', 'Useful Skills And Experience', 'Recommend solutions to improve new and existing database systems ', 'Maintaining business partner engagement and setting expectations ', 'Ability to succeed in a dynamic, Agile environment ', 'Assessing current processes and recommending changes as needed ', 'Assess fitness-for-purpose of existing data model and processes Design conceptual and logical data models and flowcharts Design and implement effective database solutions and models to store and retrieve company data Development of reporting solutions to meet the operational and executive needs of the platform Examine and identify database structural necessities by evaluating client operations, applications, and programming Optimize new and current database systems Assess database implementation procedures to ensure they follow internal and external regulations Install and organize information systems to guarantee company functionality Prepare accurate database design and architecture reports for management and executive teams Lead all aspects of the migration of data from legacy systems to new solutions Monitor the system performance by performing regular tests, fixing, and integrating new features Recommend solutions to improve new and existing database systems ', 'Extensive hands-on experience working with SQL and Python for the purposes of data modeling and ETL ', 'Design conceptual and logical data models and flowcharts ', 'Understanding of various data extraction and transformation techniques ', ""What You'll Do"", 'Development of reporting solutions to meet the operational and executive needs of the platform ', 'Sequoia’s Culture – Our most important asset', 'Knowledge of medical, dental, LTD/STD/life insurance concepts and data Understanding of various data extraction and transformation techniques Working familiarity with Salesforce (SFDC) Knowledge of Mulesoft is a bonus Familiar with data visualization standard methodologies\u202f ', 'Soft Skills', 'SQL', 'Advanced working knowledge and ability to write complex SQL queries in MySQL, Snowflake, and Salesforce (SFDC) environments ', 'Compensation & Benefits', 'A champion of quality, able to QA and vouch for the integrity of the report output ', 'Translate business requirements into specifications that will be used to implement the required user-friendly environments, reports and dashboards, built from potentially multiple data sources ', 'Knowledge of medical, dental, LTD/STD/life insurance concepts and data ', 'Examine and identify database structural necessities by evaluating client operations, applications, and programming ', 'Prepare accurate database design and architecture reports for management and executive teams ', 'Optimize new and current database systems ', 'Participate in business analysis activities to gather required reporting and dashboard requirements ', 'Required Skills & Experience', 'Data Engineer ', 'Knowledge of Mulesoft is a bonus ', 'Dedication to team goals that include support of live 24/7 production systems ', 'Strong prioritization and time-management skills ', 'Familiar with data visualization standard methodologies\u202f ', 'Install and organize information systems to guarantee company functionality ', 'Advanced working knowledge and ability to write complex SQL queries in MySQL, Snowflake, and Salesforce (SFDC) environments Extensive hands-on experience working with SQL and Python for the purposes of data modeling and ETL Strong familiarity with Kimball, OLAP, and EDW data design methodologies, especially for healthcare and benefits datasets 3+ years’ experience in data modeling and architecting, ETL, data engineering, or BI fields with concentration on data transformations and data modeling ', 'Strong familiarity with Kimball, OLAP, and EDW data design methodologies, especially for healthcare and benefits datasets ', 'Documenting and communicating technical specifications to ensure that proper and optimized techniques, queries, data standards, and final outputs are understood and incorporated into data and analytics processes ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
AWS Data lake Engineer,NLB Services,"Plano, TX",1 day ago,Be among the first 25 applicants,"['Location: Plano, TX', 'Required Skills:', 'Role: AWS Data Lake Engineer (6 openings)', 'Need 2 to 8 years experienced candidates', 'Familiar with development tools such as Jenkins, Jira, Git etc. ;', 'Only USC/GC', 'Splunk', 'Familiar with development tools such as Jenkins, Jira, Git etc. ;Rest API with MVC architecture;Unit test case modules (junit, pytest, mockito);Splunk', 'Unit test case modules (junit, pytest, mockito);', 'Rest API with MVC architecture;', 'Little bit of Java knowledge', 'Look for AWS, Data Lake, SQL, MVC, Rest API skills', 'No client interview', '\xa0']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Remote Data Engineer (Python / ETL / AWS / Graph),Jobot,"Philadelphia, PA",21 hours ago,Be among the first 25 applicants,"['', 'Exp. extracting, processing, storing, and querying of petabyte-scale datasets', 'Full benefits Medical, Dental, Vision', 'Build & operate automated ETL pipelines that process terabytes of text data nightlyDevelop service frontends around our various backend datastores (AWS Aurora MySQL, Elasticsearch, S3)Perform technical analyses and requirements specification with our product team on data service integrationsHelp customers bring their data to the platform', 'Competitive base salary based on exp. ', 'Remote Data Engineer (Python / ETL / AWS) needed for a growing cloud based R&D Biotech company!', 'Work Remote', 'Python 3+ or Java programming, both would be preferred', 'A Bit About Us', 'Help customers bring their data to the platform', 'Familiarity with building and using containers', 'Job Details', 'Familiarity with event-based microservices', 'Prior work with text and natural-language processing', 'Bonus', 'Day-to-day experience using AWS technologies such as Lambda, ECS Fargate, SQS, & SNS', 'Exp. building and operating cloud-native data pipelines', 'Knowledge of Graph databases and Graph Theory', 'Python 3+ or Java programming, both would be preferredDay-to-day experience using AWS technologies such as Lambda, ECS Fargate, SQS, & SNSExp. building and operating cloud-native data pipelinesExp. extracting, processing, storing, and querying of petabyte-scale datasetsFamiliarity with building and using containersFamiliarity with event-based microservices', 'Prior experience with Elasticsearch (custom development and/or administration)', 'Competitive base salary based on exp. Full benefits Medical, Dental, Vision401 (K) with generous company matchGenerous vacation, sick, PTO, and holidaysWork RemoteBonus', '401 (K) with generous company match', 'Prior experience with Elasticsearch (custom development and/or administration)Prior work with text and natural-language processingKnowledge of Graph databases and Graph Theory', 'Develop service frontends around our various backend datastores (AWS Aurora MySQL, Elasticsearch, S3)', 'Build & operate automated ETL pipelines that process terabytes of text data nightly', 'Perform technical analyses and requirements specification with our product team on data service integrations', 'Why join us?', 'Generous vacation, sick, PTO, and holidays']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Platform Engineer - Experimentation,Stitch Fix,"San Francisco, CA",2 weeks ago,Be among the first 25 applicants,"['', 'We are challenged, developed and have meaningful impact', 'We Get Excited About Candidates Who Have…', ' Build and own large aspects to the experimentation platform, with a focus on scalable, resilient infrastructure Help us curate a consistent set of interfaces Be involved in the day-to-day operations of the team, including maintaining and improving our current platform and supporting full-stack data scientists Work with business partners to identify new opportunities for experimentation support ', ' Help shape how experimentation is done at Stitch Fix Be an owner of a highly visible and high-traffic platform Have the opportunity to work on all aspects of the experimentation system, from services to backend data stores to analytics and logging Share the responsibility of directing the team’s investment in impactful directions Contribute to a culture of technical collaboration and scalable resilient systems Work with bright and kind colleagues who are passionate about their craft ', 'You will be proud to say that you work for Stitch Fix and will know that the work you do brings joy to our clients every day', 'A rigorous focus on simple solutions - more complex is not always better', 'About Stitch Fix', 'Help shape how experimentation is done at Stitch Fix', 'We are a successful, fast-growing company at the forefront of tech and fashion, redefining retail for the next generation', 'A desire to question the status quo and promote innovative solutions to challenging problems', 'You’re Excited About This Opportunity Because You Will...', 'Be involved in the day-to-day operations of the team, including maintaining and improving our current platform and supporting full-stack data scientists', 'About The Role', 'Be an owner of a highly visible and high-traffic platform', 'Help us curate a consistent set of interfaces', 'Experience working autonomously and taking ownership of projects', 'We take what we do seriously. We don’t take ourselves seriously', ""Please Review Stitch Fix's Recruiting Privacy Policy Here"", 'We love solving problems, thinking creatively and trying new things', 'We are committed to our clients and connected through our vision of “Transforming the way people find what they love”', 'We are a technologically and data-driven business', 'A robust focus on business impact', 'The ability to effectively communicate with technical and non-technical business partners alike, and who are interested in building strong cross-functional relationships', 'In This Role You Can Expect To', 'We believe in autonomy & taking initiative', 'Work with business partners to identify new opportunities for experimentation support', 'Build and own large aspects to the experimentation platform, with a focus on scalable, resilient infrastructure', 'Exceptional coding and design skills', 'Contribute to a culture of technical collaboration and scalable resilient systems', 'About The Team', 'YOU’LL LOVE WORKING AT STITCH FIX BECAUSE…', 'Have the opportunity to work on all aspects of the experimentation system, from services to backend data stores to analytics and logging', 'We have a smart, experienced leadership team that wants to do it right & is open to new ideas', 'We are a group of bright, kind and goal oriented people. You can be your authentic self here, and are empowered to encourage others to do the same!', 'Share the responsibility of directing the team’s investment in impactful directions', 'Work with bright and kind colleagues who are passionate about their craft', ' We are a group of bright, kind and goal oriented people. You can be your authentic self here, and are empowered to encourage others to do the same! We are a successful, fast-growing company at the forefront of tech and fashion, redefining retail for the next generation We are a technologically and data-driven business We are committed to our clients and connected through our vision of “Transforming the way people find what they love” We love solving problems, thinking creatively and trying new things We believe in autonomy & taking initiative We are challenged, developed and have meaningful impact We take what we do seriously. We don’t take ourselves seriously We have a smart, experienced leadership team that wants to do it right & is open to new ideas We offer competitive compensation packages and comprehensive health benefits You will be proud to say that you work for Stitch Fix and will know that the work you do brings joy to our clients every day ', 'Strong experience building out scalable distributed and production systems', 'We offer competitive compensation packages and comprehensive health benefits', ' Experience working autonomously and taking ownership of projects Strong experience building out scalable distributed and production systems A rigorous focus on simple solutions - more complex is not always better Exceptional coding and design skills A robust focus on business impact The ability to effectively communicate with technical and non-technical business partners alike, and who are interested in building strong cross-functional relationships A desire to question the status quo and promote innovative solutions to challenging problems ']",Mid-Senior level,Full-time,Research,Apparel & Fashion,2021-03-24 13:05:10
Data Engineer III,Expedia Group,"Austin, TX",4 days ago,Be among the first 25 applicants,"['', ""Bachelor’s + 7 years or Master's + 5 years in Computer Science or Engineering or related experience .Experience building data pipelines with data from event streams, on distributed data systems (AWS/Hadoop)Batch and/or stream processing experience using Spark, K-Streams, KafkaExperience building low-latency data product APIsProfessional development experience in Java/Python/Scala"", 'Professional development experience in Java/Python/Scala', 'Electronic, adjustable stand-up desk', 'Frequent company update talks with our leadership team', 'Technologies We Use', 'Backend development building applications from concept to completion', 'Employee Stock Purchase Program', 'You will commit to vigilantly rewriting, refactoring, and perfecting code', 'Dedicated to delivering tested and optimized high performance code for a distributed SOA environment', 'Competitive health and insurance benefits', ""Bachelor’s + 7 years or Master's + 5 years in Computer Science or Engineering or related experience ."", 'Scale our public API’s to give other partners the ability to leverage new Expedia Group services', 'Simplify our core property status workflow to enhance both our travelers’ & suppliers’ experience', 'Benefits & Perks', 'Competitive health and insurance benefitsCompetitive salaryAnnual target bonus or commissionParental leave for up to 20 weeks (dependent on eligibility)Paid vacation and sick timeEmployee Stock Purchase ProgramFree snacks and beveragesFrequent company update talks with our leadership teamFree listing on Vrbo.comElectronic, adjustable stand-up deskDiscounted Metro & Rail passCasual dress', 'Develop fast, scalable, highly available, and reliable property status software that will control the enabled status for all of Expedia’s properties', 'Java 8, Python, Scala, Spark, K-Streams, Hadoop, Elasticsearch, Jetty, and Linux', 'Parental leave for up to 20 weeks (dependent on eligibility)', 'Create and maintain quality software using premier tools: Git, Splunk, Datadog, New Relic, etc.', 'You will develop property status funnel features that will drive our business through real-time feedback loops', ""What You'll Do"", 'Annual target bonus or commission', 'Experience building data pipelines with data from event streams, on distributed data systems (AWS/Hadoop)', 'About Vrbo', 'You will scale our services to tens of thousands of requests per second', 'Competitive salary', 'Batch and/or stream processing experience using Spark, K-Streams, Kafka', 'Paid vacation and sick time', 'Responsibilities', 'Scale our private API’s to allow enhanced Expedia UI experiences', 'Opportunities to showcase your work on our tech blog and internal & external conferences', 'Participate in resolution of production issues and lead efforts toward solutions', 'Casual dress', 'Use real-time data to understand performance and ensure system scalability', 'Free snacks and beverages', 'Dockerize our apps and services for cloud deployment', 'Develop quality scalable, tested and reliable applications using industry standard methodologies', 'Backend development building applications from concept to completionYou will commit to vigilantly rewriting, refactoring, and perfecting codeDedicated to delivering tested and optimized high performance code for a distributed SOA environmentDevelop quality scalable, tested and reliable applications using industry standard methodologiesWork in an agile environment with product management and operationsCreate and maintain quality software using premier tools: Git, Splunk, Datadog, New Relic, etc.Participate in resolution of production issues and lead efforts toward solutionsOpportunities to showcase your work on our tech blog and internal & external conferences', 'Experience building low-latency data product APIs', 'Free listing on Vrbo.com', 'Why Join Us', 'Who You Are', 'Develop fast, scalable, highly available, and reliable property status software that will control the enabled status for all of Expedia’s propertiesYou will scale our services to tens of thousands of requests per secondUse real-time data to understand performance and ensure system scalabilityDockerize our apps and services for cloud deploymentYou will develop property status funnel features that will drive our business through real-time feedback loopsScale our public API’s to give other partners the ability to leverage new Expedia Group servicesScale our private API’s to allow enhanced Expedia UI experiencesSimplify our core property status workflow to enhance both our travelers’ & suppliers’ experience', 'Work in an agile environment with product management and operations', 'Discounted Metro & Rail pass']",Mid-Senior level,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Data Engineer (Remote) - Last Call Media,Last Call Media,"Remote, OR",,N/A,"['', 'Able to empathize, which helps us understand the needs of the client, the customer, the product, and the team at LCM. ', 'Experienced with communicating directly with clients', 'Provide guidance on the collection of web analytics data (eg: Implementation advice for GTM).', 'Technical Architecture experience.', 'Infrastructure as code tools, especially Terraform.', 'General comfort with Linux environments.', 'What You’ll Bring', 'Extracting data from Google Analytics/BigQuery, JSON files, and various web APIs.', 'A proven track record of building robust web data pipelines. 2+ years of similar experienceAn obvious drive to grow and learn from the highly-skilled team around you.Experience and/or a desire to work remotely.Ability to work efficiently, sometimes under tight deadlines', 'General comfort with Linux environments.Familiarity with DevOps principles, such as GitOps and infrastructure as code.Technical Architecture experience.Javascript experience.', 'Comfortable asking for help', 'Highly communicative', 'Work with team members to optimize and extend an existing data warehouse.', 'Experience and/or a desire to work remotely.', 'Last Call Media', 'Extracting data from Google Analytics/BigQuery, JSON files, and various web APIs.Utilizing Google Analytics/Tag Manager to collect web analytics data.Writing ETLs, particularly in Python for Apache Airflow or AWS Step functions.Building and maintaining AWS infrastructure, including ECS containers and Lambda functions.Building and optimizing data warehouses, especially Postgres databases.Infrastructure as code tools, especially Terraform.', 'Familiarity with DevOps principles, such as GitOps and infrastructure as code.', 'Application Process', 'Able to work independently ', 'Able to empathize, which helps us understand the needs of the client, the customer, the product, and the team at LCM. Highly communicativeAble to work independently Comfortable asking for helpExperienced with communicating directly with clientsEager and motivated to learn new conceptsA team player in a collaborative environment A fast learner', 'It’d Be Nice If You Also Had', 'Writing ETLs, particularly in Python for Apache Airflow or AWS Step functions.', 'An obvious drive to grow and learn from the highly-skilled team around you.', 'Provide guidance on feasibility and advisability of upcoming data collection projects.', 'What You’ll Do', 'Javascript experience.', 'A team player in a collaborative environment ', 'Building and maintaining AWS infrastructure, including ECS containers and Lambda functions.', 'A fast learner', 'Eager and motivated to learn new concepts', 'comprehensive benefits', 'Work with team members to optimize and extend an existing data warehouse.Gather new data collection requirements and implement or change ETL processes.Optimize existing ETL processes.Provide guidance on feasibility and advisability of upcoming data collection projects.Provide guidance on the collection of web analytics data (eg: Implementation advice for GTM).', 'All Of Us At LCM Pride Ourselves On Being', 'Optimize existing ETL processes.', 'A proven track record of building robust web data pipelines. ', 'Gather new data collection requirements and implement or change ETL processes.', 'The Role', 'Ability to work efficiently, sometimes under tight deadlines', '2+ years of similar experience', 'Building and optimizing data warehouses, especially Postgres databases.', 'Utilizing Google Analytics/Tag Manager to collect web analytics data.']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
System Engineer,EPITEC,"Thousand Oaks, CA",,N/A,"['Must be able to lift/move/do minor component replacement on full-size desktop computer systems.Experience with automation is a plus, as well as experience with computer programming, systems, and/or robotics. Previous intern experience is a plus.', 'REQUIRED EXPERIENCE FOR SYSTEMS ENGINEER:', 'POSITION:', ""Epitec represents the world’s top companies and works to fill their open jobs with the world’s best talent. That’s led to Epitec servicing an impressive list of Fortune 100 companies. We've also won many awards, including one of Crain’s Detroit Business “Cool Places to Work,” and 101 Best & Brightest – local, national and elite winner. And that’s just the beginning, as we work to innovate the way the world thinks about employment."", 'Must have sufficient interpersonal skills to be able to communicate with research end-users & remote site support staff.', 'Remote but MUST be okay to report to site in Thousand Oaks, CA from time to time.A local candidate is ideal.Position involves support of senior staff in the delivery of projects, particularly for OS reimaging/updating of researcher linux workstations.Must have sufficient interpersonal skills to be able to communicate with research end-users & remote site support staff.', 'Thousand Oaks, CA', 'We started Epitec with a single focus, “Placing People First.” Knowing every good endeavor begins with listening and understanding, we’ve set about challenging every part of the employment process. Bringing the proper connections together for the perfect fit.', 'JOB SUMMARY FOR SYSTEMS ENGINEER:', 'Remote but MUST be okay to report to site in Thousand Oaks, CA from time to time.', 'Must be able to lift/move/do minor component replacement on full-size desktop computer systems.', 'JOB TYPE:', 'What is the result?', 'Employ basic engineering and interpersonal skills and practices to gather individual user requirements and integrate into customized plan for individual system reimaging.Perform field evaluations of existing systems and execute instructions per plan utilizing troubleshooting and analytical skills to resolve peculiarities of each individual system configuration.Work on site as needed for initial steps of reimaging USTO systems, work remotely for other locations via on-site staff for the hands-on portion of reimaging task.', 'Experience with automation is a plus, as well as experience with computer programming, systems, and/or robotics. Previous intern experience is a plus.', '\xa0REQUIRED EXPERIENCE FOR SYSTEMS ENGINEER:', 'A local candidate is ideal.', 'SKILLS AND QUALIFICATIONS FOR SYSTEMS ENGINEER:', '\xa0', 'RESPONSIBILITIES FOR SYSTEMS ENGINEER:', 'LOCATION:', 'How is Epitec different?', 'Why should you choose Epitec?', 'W2-Contract', 'Perform field evaluations of existing systems and execute instructions per plan utilizing troubleshooting and analytical skills to resolve peculiarities of each individual system configuration.', 'Work on site as needed for initial steps of reimaging USTO systems, work remotely for other locations via on-site staff for the hands-on portion of reimaging task.', 'Epitec gets to know our prospective employees, using these insights to locate the perfect placement for you. We are there, every step of the way. Providing a best-in-class compensation package combined with the opportunity to grow financially and personally through your work.', 'Employ basic engineering and interpersonal skills and practices to gather individual user requirements and integrate into customized plan for individual system reimaging.', '\xa0SKILLS AND QUALIFICATIONS FOR SYSTEMS ENGINEER:', 'Systems Engineer', 'Previous Linux experience required with basic linux systems administrator experience sufficient to install and configure OS.', 'Epitec', 'Position involves support of senior staff in the delivery of projects, particularly for OS reimaging/updating of researcher linux workstations.', '\xa0RESPONSIBILITIES FOR SYSTEMS ENGINEER:']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Apple,"Cupertino, CA",23 hours ago,Be among the first 25 applicants,"['', 'Description', 'Education & Experience', 'Summary', 'Key Qualifications']",Not Applicable,Full-time,Information Technology,Consumer Electronics,2021-03-24 13:05:10
Data Engineer,Mozilla,"Richmond, VA",17 hours ago,Be among the first 25 applicants,"['', 'You will work with other data engineers to design and maintain scalable data models and ETL pipelines.', 'We Have Multiple Openings For The Following', 'Proficiency with one or more of the programming languages used by our teams (SQL, Python, Java, Rust).', 'Ability to work collaboratively with a distributed team.', 'Commitment to diversity, equity, inclusion, and belonging', ' Strong CS fundamentals: data structures, algorithms, etc. Proficiency with one or more of the programming languages used by our teams (SQL, Python, Java, Rust). Ability to work collaboratively with a distributed team. Ability to write and speak English well. ', 'You have experience with data systems: Databases, message queues, batch and stream processing', 'You will help design, build, and improve the infrastructure for ingesting, storing, and transforming data at a scale of tens of terabytes per day.', 'Specific Skills/Experience', 'You have experience with highly scalable distributed systems hosted on cloud providers (e.g. Google Cloud Platform)', 'You have a working knowledge of web development technologies: HTML, Javascript, CSS, HTTP.', 'General Professional Requirements', 'You will help design and build systems to monitor and analyze data from Mozilla’s products. See https://telemetry.mozilla.org', 'Ability to write and speak English well.', ' You have experience with data systems: Databases, message queues, batch and stream processing You have experience with highly scalable distributed systems hosted on cloud providers (e.g. Google Cloud Platform) You have a working knowledge of web development technologies: HTML, Javascript, CSS, HTTP. ', 'About Mozilla', 'You will work with data scientists to answer questions and guide product decisions.', 'Strong CS fundamentals: data structures, algorithms, etc.', 'The Role', ' You will help design, build, and improve the infrastructure for ingesting, storing, and transforming data at a scale of tens of terabytes per day. You will help design and build systems to monitor and analyze data from Mozilla’s products. See https://telemetry.mozilla.org You will work with other data engineers to design and maintain scalable data models and ETL pipelines. You will work with data scientists to answer questions and guide product decisions. ']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer III,Walmart,"Dallas, TX",2 days ago,Be among the first 25 applicants,"['', 'Work with business customers, product managers and engineers to design feature-based solutions and implement them in an agile fashion.', 'Minimum Qualifications...', 'Demonstrates expertise in writing complex, highly optimized queries across large data sets', 'Being human-led is our true disruption.', 'The above information has been designed to indicate the general nature and level of work performed in the role.', 'Design, develop and build database to power Big Data analytical systems.Design data integration pipeline architecture and ensure successful creation of the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using Spark, SQL, HQL and other technologies.Build robust and scalable applications using SQL, Scala/Python and Spark.Create real time data streaming and processing using Kafka and/or Spark streaming.Work on creating data ingestion processes to maintain Global Data lake on Google cloud or AzureEngage with architects and senior technical leads to create and enhance complex software components.Design, configure and implement systems that can scale to process terabytes of data between heterogeneous systems on premise and cloud.Work with business customers, product managers and engineers to design feature-based solutions and implement them in an agile fashion.Develop proof-of-concept prototype with fast iteration and experimentation.Develop and maintain design documentation, test cases, performance and monitoring and performance evaluation using Git, Crontab, Putty, Jenkins, Maven, Confluence, ETL, Automic, Zookeeper, Cluster ManagerPerform continuous integration and deployment using Jenkins and Git', ' It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications required of employees assigned to this job. The full Job Description can be made available as part of the hiring process.', 'Build robust and scalable applications using SQL, Scala/Python and Spark.', 'Develop and maintain design documentation, test cases, performance and monitoring and performance evaluation using Git, Crontab, Putty, Jenkins, Maven, Confluence, ETL, Automic, Zookeeper, Cluster Manager', 'You’ll Make An Impact By', 'Develop proof-of-concept prototype with fast iteration and experimentation.', 'Retail experience and knowledge of commercial data is a huge plus', 'Engage with architects and senior technical leads to create and enhance complex software components.', 'About Global Tech', '3+ years of experience with 1+ years of Big data development experienceExperience in HDFS, Hive, Hive UDF’s, MapReduce, Druid, Spark, Python, Hue, Shell Scripting, Unix.Demonstrates expertise in writing complex, highly optimized queries across large data setsRetail experience and knowledge of commercial data is a huge plusExperience with BI Tool Tableau or Looker is a plus', 'Outlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications. ', '3+ years of experience with 1+ years of Big data development experience', 'Outlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications. ', 'Create real time data streaming and processing using Kafka and/or Spark streaming.', 'Perform continuous integration and deployment using Jenkins and Git', 'Experience in HDFS, Hive, Hive UDF’s, MapReduce, Druid, Spark, Python, Hue, Shell Scripting, Unix.', 'Design, develop and build database to power Big Data analytical systems.', ""Position Summary... What You'll Do..."", 'Work on creating data ingestion processes to maintain Global Data lake on Google cloud or Azure', 'Design, configure and implement systems that can scale to process terabytes of data between heterogeneous systems on premise and cloud.', 'Experience with BI Tool Tableau or Looker is a plus', 'Design data integration pipeline architecture and ensure successful creation of the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using Spark, SQL, HQL and other technologies.']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Amazon,"Seattle, WA",17 hours ago,Be among the first 25 applicants,"['', ' Willingness to dive deep, experiment rapidly and get things done', 'Company', ' Experience with Object Oriented programming (e.g. Java, Scala, Python)', ' Experience with agile methodologies, coding standards, code reviews, source control management, build processes, testing, and operations', ' BS Computer Science or equivalent industry experience', ' Experience using big data technologies (Hadoop, Hive, Hbase, Spark, EMR, etc.)', ' 1+ years of experience as a Data Engineer or in a similar role Experience with data modeling, data warehousing, and building ETL pipelines Experience in SQL', 'Preferred Qualifications', ' Love to get your hands dirty and solve challenging technical issues?', ' Are you excited about working directly to empower users? Love to get your hands dirty and solve challenging technical issues?', ' 5+ years of experience as a Data Engineer, BI Engineer, Systems Analyst in a company with large, complex data sources', ' Experience of having built and delivered multiple large scale, cross-functional projects Willingness to dive deep, experiment rapidly and get things done Experience with agile methodologies, coding standards, code reviews, source control management, build processes, testing, and operations', 'Description', ' Experience with data modeling, data warehousing, and building ETL pipelines', ' Experience with building high-performance, highly-available and scalable distributed systems', ' Passion for building great solutions which directly impact customers', ' BS Computer Science or equivalent industry experience Domain knowledge of Distributed SOA Architecture, Relational DB knowledge, ElasticSearch, DynamoDB, and various AWS technologies 5+ years of experience as a Data Engineer, BI Engineer, Systems Analyst in a company with large, complex data sources Experience with Object Oriented programming (e.g. Java, Scala, Python) Experience using big data technologies (Hadoop, Hive, Hbase, Spark, EMR, etc.) Experience with building high-performance, highly-available and scalable distributed systems Passion for building great solutions which directly impact customers Experience working in a fast-paced environment where continuous innovation is desired Amazon is committed to a diverse and inclusive workplace', ' Experience in SQL', ' Are you excited about working directly to empower users?', ' Experience of having built and delivered multiple large scale, cross-functional projects', ' 1+ years of experience as a Data Engineer or in a similar role', ' Experience working in a fast-paced environment where continuous innovation is desired Amazon is committed to a diverse and inclusive workplace', 'Basic Qualifications', ' Domain knowledge of Distributed SOA Architecture, Relational DB knowledge, ElasticSearch, DynamoDB, and various AWS technologies']",Not Applicable,Full-time,Strategy/Planning,Computer Software,2021-03-24 13:05:10
Data Engineer,Lockheed Martin,"Littleton, CO",4 weeks ago,Be among the first 25 applicants,"['Description:', '', 'Work across the Space Data Governance team and SAP team leadership to recommend and develop data strategies to align vision and strategy roadmaps', 'Desired Skills', 'Lockheed Martin is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.', 'BASIC QUALIFICATIONS:', 'Minimum 8-10 years of related experience within a corporate environment required; experience must include successful track record with data modeling, ETL processing, data architecture, understanding of reporting platforms, and machine learning and predictive analytic solutions', 'Experience leading practices and SAP S/4 solution capabilities for the in-scope business analytics, including finance, procurement, supply chain management, manufacturing, quality management, logistics and order to cash', 'Provide input to business cases and drive business value into existing processes by defining and delivering data driven process improvements', 'Strong experience working with HANA studio, HANA calculation views/ CDS and BOBJ (WEBI, Lumira) Demonstrated success working in cross-functional teams with both business and technical resources', 'Collaborate with internal customers (functional/subject matter experts and/or end users) across various business units to build knowledge regarding their specific business area, unique processes, and operational data needed to support business objectives', 'Work across the Space Data Governance team and SAP team leadership to recommend and develop data strategies to align vision and strategy roadmapsWork with Space data and SAP teams as well as the corporate Data Analytics COE to support data mining and analysis of best practicesCollaborate with internal customers (functional/subject matter experts and/or end users) across various business units to build knowledge regarding their specific business area, unique processes, and operational data needed to support business objectivesProvide input to business cases and drive business value into existing processes by defining and delivering data driven process improvements', 'Job.Qualifications', 'Work with Space data and SAP teams as well as the corporate Data Analytics COE to support data mining and analysis of best practices', 'Knowledge of information access and delivery methods, analytics applications and tool portfolio, data sourcing and integration methods such as OLAP / ROLAP / HOLAP and an understanding of data warehousing/mining, ETL, EII, data cleansing, and architecture', 'Your Responsibilities Include', 'Minimum 8-10 years of related experience within a corporate environment required; experience must include successful track record with data modeling, ETL processing, data architecture, understanding of reporting platforms, and machine learning and predictive analytic solutionsExperience leading practices and SAP S/4 solution capabilities for the in-scope business analytics, including finance, procurement, supply chain management, manufacturing, quality management, logistics and order to cashStrong experience working with HANA studio, HANA calculation views/ CDS and BOBJ (WEBI, Lumira) Demonstrated success working in cross-functional teams with both business and technical resourcesAbility to adapt to change quickly (high flexibility)Excellent written and oral communication skillsExperience using Tableau, Brainspace, or other data visualization tools', 'Experience using Tableau, Brainspace, or other data visualization tools', 'Experience Level', ""Bachelor's degree in Information Technology, Data Engineering, Data Science or a related field, requiredBroad experience and expertise working with various technologies such as Python, SQL, R, SAS, Apache Spark, Google AI, TensorFlow, and other frameworks and libraries for AIKnowledge of information access and delivery methods, analytics applications and tool portfolio, data sourcing and integration methods such as OLAP / ROLAP / HOLAP and an understanding of data warehousing/mining, ETL, EII, data cleansing, and architecture"", 'Broad experience and expertise working with various technologies such as Python, SQL, R, SAS, Apache Spark, Google AI, TensorFlow, and other frameworks and libraries for AI', 'Excellent written and oral communication skills', 'Ability to adapt to change quickly (high flexibility)', 'Basic Qualifications', ""Bachelor's degree in Information Technology, Data Engineering, Data Science or a related field, required""]",Entry level,Full-time,Information Technology,Construction,2021-03-24 13:05:10
Data Engineer,Capital One,"New York, NY",1 day ago,75 applicants,"['', '2+ years of experience with NoSQL implementation (Mongo, Cassandra) ', 'Bachelor’s Degree At least 2 years of experience in application developmentAt least 1 years of experience in big data technologies (Cassandra, Accumulo, HBase, Spark, Hadoop, HDFS, AVRO, MongoDB, or Zookeeper)', ' slides 76-91', '1+ year experience working on streaming data applications (Spark Streaming, Kafka, Kinesis, and Flink', 'At least 2 years of experience in application development', 'Preferred Qualifications', 'Bachelor’s Degree ', 'Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment', 'Capital One Data Engineer', '2+ years of experience developing Java based software solutions ', '2+ years of experience developing software solutions to solve complex business problems', ""Master's Degree 3+ years of experience in application development1+ year experience working on streaming data applications (Spark Streaming, Kafka, Kinesis, and Flink1+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)1+ years of experience with Ansible / Terraform2+ years of experience with Agile engineering practices 2+ years in-depth experience with the Hadoop stack (MapReduce, Pig, Hive, Hbase) 2+ years of experience with NoSQL implementation (Mongo, Cassandra) 2+ years of experience developing Java based software solutions 2+ years of experience in at least one scripting language (Python, Perl, JavaScript, Shell) 2+ years of experience developing software solutions to solve complex business problems2+ years of experience with UNIX/Linux including basic commands and shell scripting"", 'Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community', '2+ years of experience with Agile engineering practices ', 'At least 1 years of experience in big data technologies (Cassandra, Accumulo, HBase, Spark, Hadoop, HDFS, AVRO, MongoDB, or Zookeeper)', 'diversity & inclusion', 'inclusive,', '#lifeatcapitalone', 'Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Snowflake', 'Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies', 'What You’ll Do', '2+ years of experience in at least one scripting language (Python, Perl, JavaScript, Shell) ', ""Master's Degree "", 'Perform unit tests and conducting reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance', '2+ years in-depth experience with the Hadoop stack (MapReduce, Pig, Hive, Hbase) ', '2+ years of experience with UNIX/Linux including basic commands and shell scripting', 'Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems', 'Basic Qualifications', 'Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologiesWork with a team of developers with deep experience in machine learning, distributed microservices, and full stack systemsUtilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as SnowflakeShare your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering communityCollaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowermentPerform unit tests and conducting reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance', '3+ years of experience in application development', '1+ years of experience with Ansible / Terraform', '1+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)']",Entry level,Full-time,Information Technology,Banking,2021-03-24 13:05:10
Data Engineer III - Entry Level,USAA,"San Antonio, TX",6 days ago,30 applicants,"['', 'Collaborate with senior engineers and assist in the implementation of technical solutions.', 'Certification from an approved technical field of study,', 'to the opening is 3/23/21 by 11:59 pm CST time.', 'Preferred', 'Design, write, test and deploy data pipeline code.', 'Geographical Differential: Geographic pay differential is additional pay provided to eligible employees working in locations where market pay levels are above the national average.Shift premium will be addressed on an individual-basis for applicable roles that are consistently scheduled for non-core hours.BenefitsAt USAA our employees enjoy best-in-class benefits to support their physical, financial, and emotional wellness. These benefits include comprehensive medical, dental and vision plans, 401(k), pension, life insurance, parental benefits, adoption assistance, paid time off program with paid holidays plus 16 paid volunteer hours, and various wellness programs. Additionally, our career path planning and continuing education assists employees with their professional goals.Please click on the link below for more details.USAA Total RewardsRelocation assistance is notavailable for this position.For Internal CandidatesMust complete 12 months in current position (from date of hire or date of placement), or must have manager’s approval prior to posting.Last day for internal candidates to apply to the opening is 3/23/21 by 11:59 pm CST time.', 'Computer Science Degree Preferred', 'Computer Science Degree PreferredETL/ELT development in the Bank Data environment. Experience programming with Python', '4 additional years of related experience beyond the minimum required.0 to 2 years of data management experience implementing data solutions or coursework in applicable discipline', 'Ensures risks associated with business activities are effectively identified, measured, monitored, and controlled. ', 'ETL/ELT development in the Bank Data environment. ', 'Experience programming with Python', 'Participate in design and code review sessions.', 'For Internal Candidates', 'Shift premium', 'About USAA', 'Identifies and manages existing and emerging risks that stem from business activities and the job role.', 'Minimum Requirements', '0 to 2 years of data management experience implementing data solutions or coursework in applicable discipline', ""Bachelor's degree in related field of study,"", 'Identifies and manages existing and emerging risks that stem from business activities and the job role.Ensures risks associated with business activities are effectively identified, measured, monitored, and controlled. Follows written risk and compliance policies and procedures for business activities.Collaborate with senior engineers and assist in the implementation of technical solutions.Design, write, test and deploy data pipeline code.Participate in design and code review sessions.', 'not', 'Last day for internal candidates to apply ', 'Compensation', 'Benefits', 'USAA Total Rewards', 'available', 'Relocation', 'Follows written risk and compliance policies and procedures for business activities.', '4 additional years of related experience beyond the minimum required.']",Not Applicable,Full-time,Information Technology,Financial Services,2021-03-24 13:05:10
Data Engineer,JPMorgan Chase & Co.,"Newark, DE",4 weeks ago,Be among the first 25 applicants,"['', 'Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.', 'Experience supporting and working with cross-functional teams in a dynamic environment.', 'Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.', 'Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.', 'Build processes supporting data transformation, data structures, metadata, dependency and workload management.', 'Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.', 'Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.', 'Organization', 'Strong analytic skills related to working with unstructured datasets.', ' BS/BA degree or equivalent experience Advanced knowledge of application, data, and infrastructure architecture disciplines Understanding of software skills such as business analysis, development, maintenance, and software improvement Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies. Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics. Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Keep our data separated and secure across national boundaries through multiple data centers and AWS regions. Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader. Work with data and analytics experts to strive for greater functionality in our data systems. Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases. Experience building and optimizing ‘big data’ data pipelines, architectures and data sets. Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. Strong analytic skills related to working with unstructured datasets. Build processes supporting data transformation, data structures, metadata, dependency and workload management. A successful history of manipulating, processing and extracting value from large disconnected datasets. Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores. Strong project management and organizational skills. Experience supporting and working with cross-functional teams in a dynamic environment. ', 'Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.', 'Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.', 'Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.', 'Experience with big data tools: Hadoop, Spark, Kafka, etc.', 'About Us', ' Experience with big data tools: Hadoop, Spark, Kafka, etc. Experience with relational SQL and NoSQL databases, including Postgres and Cassandra. Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc. Experience with AWS cloud services: EC2, EMR, RDS, Redshift Experience with stream-processing systems: Storm, Spark-Streaming, etc. Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc. ', 'Advanced knowledge of application, data, and infrastructure architecture disciplines', 'Understanding of software skills such as business analysis, development, maintenance, and software improvement', 'Strong project management and organizational skills.', 'Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.', 'Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.', 'Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.', 'A successful history of manipulating, processing and extracting value from large disconnected datasets.', 'BS/BA degree or equivalent experience', 'Work with data and analytics experts to strive for greater functionality in our data systems.', 'Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.', 'Experience with stream-processing systems: Storm, Spark-Streaming, etc.', 'Experience with AWS cloud services: EC2, EMR, RDS, Redshift', 'Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.', 'They Should Also Have Experience Using The Following Software/tools', 'Job Description']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Riskalyze,"Atlanta, GA",3 weeks ago,152 applicants,"['', 'Consult with R&D Product Managers to ensure that new products and features are developed to feed data seamlessly into the data pipeline.', 'Work with leaders across the company to develop a strong understanding of their needs for data sets in Salesforce, Domo and other tools for our customer-facing teams. Work with engineering teams in the R&D organization to understand the data schemas and structures.Design and build automated and scalable data pipelines to extract that data from our core databases and products, transform that data into usable datasets for our customer-facing teams, and load that data into Salesforce, Domo and other tools.Coordinate the validation of data quality by requiring both the R&D stakeholders who have knowledge of data structure, and the customer-facing teams who have knowledge of customer activities and business operations, to review the outputs of the data pipeline and ensure it is accurate and complete.Build monitoring and alerting tools to notify multiple people on the Revenue Operations team when our core systems or products have issues producing data, our data pipelines have issues extracting and transforming data, or our customer-facing systems have issues receiving data.Work with stakeholders to assist with data-related technical issues and support data infrastructure needs.Consult with R&D Product Managers to ensure that new products and features are developed to feed data seamlessly into the data pipeline.Mentor other RevOps team members on data pipelines and architecture to create resilience in our resourcing and systems.', 'We encourage people from underrepresented groups to apply!', 'Annual bonus subject to company/individual performance', 'Work with stakeholders to assist with data-related technical issues and support data infrastructure needs.', 'All hands team meetings every 6 weeks with catering', 'Board game nights and movie outings', 'Experience working independently on data pipeline and integration projects; owning and leading a function without a large team to fall back on; willingness to be entrepreneurial and scrappy in creating infrastructure from scratch. Strong systems thinker.Strong experience with ETL tools, databases, data warehousing solutions5+ years of experience working with at least half of the following technologies: SQL, cURL, JSON, Javascript, Kotlin, Apex and/or Python. Clear ability to rapidly extend your natural skills to encompass the other half.Elite at constructing data schemas and structuring datasets for ease of use by our customer-facing teams. Ability to distill down complex concepts and communicate them to non-technical teams.Strong communication, collaboration, and presentation skills.', 'Wellness/Gym Benefit: Atlanta employees have access to a subsidized membership at RPM in the Bank of America Plaza.', 'Commuter Benefit (Atlanta, GA.): Riskalyze will cover the cost for Atlanta employees MARTA pass or parking pass.', 'Medical, dental and vision with access to HSA or FSA depending on chosen medical plan', 'Available pet insurance', 'Requirements', '3 weeks Vacation & 1 week sick time per year + 11 paid holidays.', 'Our Data Engineer will align to our Revenue Operations department & will be responsible for fulfilling the business need for actionable data insights and intelligence by owning the extraction, transformation and loading of data from our core products into our customer-facing systems.', 'Strong communication, collaboration, and presentation skills.', 'Mentor other RevOps team members on data pipelines and architecture to create resilience in our resourcing and systems.', 'Experience working independently on data pipeline and integration projects; owning and leading a function without a large team to fall back on; willingness to be entrepreneurial and scrappy in creating infrastructure from scratch. Strong systems thinker.', 'Strong experience with ETL tools, databases, data warehousing solutions', '401(k) Retirement savings with employer matching dollar-for-dollar, up to 4%', 'Responsibilities', 'Build monitoring and alerting tools to notify multiple people on the Revenue Operations team when our core systems or products have issues producing data, our data pipelines have issues extracting and transforming data, or our customer-facing systems have issues receiving data.', 'In office snacks 3x per week', 'At Riskalyze, we’re on a mission to empower the world to invest fearlessly. Investing feels broken for the average investor, and subjective risk semantics like “aggressive” and “moderately conservative” don’t help. We believe that when advisors align the world’s investments with each investor’s Risk Number®, anyone can invest fearlessly.', 'Design and build automated and scalable data pipelines to extract that data from our core databases and products, transform that data into usable datasets for our customer-facing teams, and load that data into Salesforce, Domo and other tools.', 'Elite at constructing data schemas and structuring datasets for ease of use by our customer-facing teams. Ability to distill down complex concepts and communicate them to non-technical teams.', 'Benefits', 'Coordinate the validation of data quality by requiring both the R&D stakeholders who have knowledge of data structure, and the customer-facing teams who have knowledge of customer activities and business operations, to review the outputs of the data pipeline and ensure it is accurate and complete.', 'Work with leaders across the company to develop a strong understanding of their needs for data sets in Salesforce, Domo and other tools for our customer-facing teams. Work with engineering teams in the R&D organization to understand the data schemas and structures.', 'We are located at brand new office space at the Bank of America Plaza in Mid-Town Atlanta.Commuter Benefit (Atlanta, GA.): Riskalyze will cover the cost for Atlanta employees MARTA pass or parking pass.Medical, dental and vision with access to HSA or FSA depending on chosen medical planAvailable pet insurance401(k) Retirement savings with employer matching dollar-for-dollar, up to 4%Annual bonus subject to company/individual performanceFree financial advisory services are offered to Riskalyzers wanting expert guidance on how to handle their money.3 weeks Vacation & 1 week sick time per year + 11 paid holidays.Wellness/Gym Benefit: Atlanta employees have access to a subsidized membership at RPM in the Bank of America Plaza.All hands team meetings every 6 weeks with cateringFully stocked drink fridgesIn office snacks 3x per weekBoard game nights and movie outings', 'We are located at brand new office space at the Bank of America Plaza in Mid-Town Atlanta.', 'Fully stocked drink fridges', 'Riskalyze is an equal opportunity employer.', 'Free financial advisory services are offered to Riskalyzers wanting expert guidance on how to handle their money.', '5+ years of experience working with at least half of the following technologies: SQL, cURL, JSON, Javascript, Kotlin, Apex and/or Python. Clear ability to rapidly extend your natural skills to encompass the other half.']",Mid-Senior level,Full-time,Engineering,Computer Software,2021-03-24 13:05:10
Sr Data Engineer,PayPal,"Austin, TX",19 hours ago,Be among the first 25 applicants,"['', 'Ability to understand and discuss technical concepts, manage tradeoffs, and generate and evaluate new opportunities with internal and external partners.', 'Key Responsibilities', 'Minimum Qualifications', 'Create and manage Data Integrations between Google Cloud Datawarehouse and tools such as Essbase, SAP, etc.', 'Design and build data foundation, data pipelines and ETL that will enable the Finance team to perform insightful analysis and create data visualizations to track key business metricsPartner with data, engineering and business teams to translate business problems into technical specs and work with developers in India to build out scalable, sustainable architectureOwn table design and architecture, transformation logic and efficient query development to support growing needs of the organizationAutomate analyses and data pipelines to optimize infrastructure performanceCreate and manage Data Integrations between Google Cloud Datawarehouse and tools such as Essbase, SAP, etc.Develop testing and monitoring across the transformation layer to ensure data quality from sources and all models downstreamBuild out documentation that supports code maintainability and ultimately a Data Dictionary that makes data accessible to the organization', 'Bachelor’s degree in Computer Science, Engineering, Physics, Math, etc. or equivalent background in rigorous technical problem solving.', 'Job Description:', 'Ability to manage multiple projects and priorities', '5+ years of experience in a data engineering role; experience with Finance datasets a big plusExperience developing data solutions on cloud platforms; Google Cloud certification is highly desirableExperience with ETL tools and proficiency in SQL, Python and/or RBachelor’s degree in Computer Science, Engineering, Physics, Math, etc. or equivalent background in rigorous technical problem solving.Ability to manage multiple projects and prioritiesAbility to understand and discuss technical concepts, manage tradeoffs, and generate and evaluate new opportunities with internal and external partners.', 'Design and build data foundation, data pipelines and ETL that will enable the Finance team to perform insightful analysis and create data visualizations to track key business metrics', 'Develop testing and monitoring across the transformation layer to ensure data quality from sources and all models downstream', 'Automate analyses and data pipelines to optimize infrastructure performance', 'Partner with data, engineering and business teams to translate business problems into technical specs and work with developers in India to build out scalable, sustainable architecture', 'Experience with ETL tools and proficiency in SQL, Python and/or R', 'Build out documentation that supports code maintainability and ultimately a Data Dictionary that makes data accessible to the organization', 'Own table design and architecture, transformation logic and efficient query development to support growing needs of the organization', '5+ years of experience in a data engineering role; experience with Finance datasets a big plus', 'Job Description Summary:', 'Experience developing data solutions on cloud platforms; Google Cloud certification is highly desirable']",Not Applicable,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Data Engineer,Virgin Pulse,"Minneapolis, MN",1 day ago,Be among the first 25 applicants,"['', 'Security Competencies', ' A passion for learning technologies and applying them to the right projects will make you successful in this position. ', ' Columnar databases (Redshift, Snowflake, Firebolt, etc) ', ' Responsibilities ', ' Python programming language ', ' Enjoy actively experimenting with new technologies ', ' Python programming language  One or more data visualization / business intelligence tools (Tableau, MicroStrategy, etc.)  You are enthusiastic about working in a team and have great people skills  A passion for learning technologies and applying them to the right projects will make you successful in this position.  You understand the principles of agile software development  You can present technical concepts in a way that is easy for non-technical people to understand  You are tenacious in your investigation of issues and problems and are able to find the root cause of problems and propose solutions correct the root causes. ', ' Teach and train other team members ', ' Design and implement data models for applications, operations, or analytics ', ' In this role you will wear many hats but your skills will be especially essential in the following: ', ' Use Python to enhance and automate our existing capabilities ', ' You understand the principles of agile software development ', ' You are enthusiastic about working in a team and have great people skills ', ' Excellent verbal and written communication skills ', ' B.S./M.S. in Computer Science or equivalent technology experience ', 'Overview', ' Hands-on experience with some of the tech we use: Git, PostgreSQL, Python, Tableau ', ' Knowledge of OO programming and applications built on distributed service architecture ', ' Skills in scripting, data engineering or modeling, and business intelligence ', 'Our technology', ' Design and implement analytics reports ', ' Investigate and troubleshoot data reporting issues ', ' You can present technical concepts in a way that is easy for non-technical people to understand ', ' Have fun doing all of the above ', ' Perform QA tasks to verify the accuracy of reporting ', ' You are self-motivated, creative, and detail oriented ', 'Who are our employees? ', ' Why work here? ', ' B.S./M.S. in Computer Science or equivalent technology experience  5 or more years’ experience working directly with data and data warehouses  Hands-on experience with some of the tech we use: Git, PostgreSQL, Python, Tableau  You are self-motivated, creative, and detail oriented  Skills in scripting, data engineering or modeling, and business intelligence  Knowledge of OO programming and applications built on distributed service architecture  Enjoy actively experimenting with new technologies  Excellent verbal and written communication skills ', ' One or more data visualization / business intelligence tools (Tableau, MicroStrategy, etc.) ', ' Build files and reports that impact hundreds-of-thousands of people around the world ', 'Qualifications', 'We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to any protected class status.', ' 5 or more years’ experience working directly with data and data warehouses ', 'In a Typical Week You May', ' Use a wide variety of modern technologies, including ', ' Who is Virgin Pulse? ', ' What you bring to the team ', ' Build files and reports that impact hundreds-of-thousands of people around the world  Work on a product that changes people’s lives  Write and maintain advanced SQL queries for reporting extracts and assist more junior team members  Perform QA tasks to verify the accuracy of reporting  Investigate and troubleshoot data reporting issues  Design and implement data models for applications, operations, or analytics  Design and implement analytics reports  Use a wide variety of modern technologies, including ', ' You are tenacious in your investigation of issues and problems and are able to find the root cause of problems and propose solutions correct the root causes. ', ' Write and maintain advanced SQL queries for reporting extracts and assist more junior team members ', ' Commonly used AWS services (S3, Lambda, Redshift, EC2, etc) ', ' BI tools (Tableau, Domo, MicroStrategy) ', ' Work on a product that changes people’s lives ', 'Who You Are', ' Data streaming (Kafka, SQS/SNS queuing, etc)  Columnar databases (Redshift, Snowflake, Firebolt, etc)  Commonly used AWS services (S3, Lambda, Redshift, EC2, etc)  BI tools (Tableau, Domo, MicroStrategy) ', ' Data streaming (Kafka, SQS/SNS queuing, etc) ']",Entry level,Full-time,Information Technology,Computer Software,2021-03-24 13:05:10
Data Engineer,Verizon,"Charlotte, NC",4 weeks ago,63 applicants,"['', 'Experience knitting disperate data sources together', 'Four or more years of experience as a data engineer', 'Ability to travel occasionally', 'Master’s degree in Computer Science, Engineering, Statistics, IT, or related field.', 'Experience in data engineering, databases, and data warehouses.', 'Diversity and Inclusion at Verizon', 'Four or more years of experience building data pipelines', 'Experience as an open source Contributor.', 'What You’ll Be Doing...', ""You'll Need To Have"", 'Master’s degree in Computer Science, Engineering, Statistics, IT, or related field.Experience with Scala, Julia, R, Python or other machine learning programming languageExperience on Big Data platforms (i.e., Hadoop, Map/Reduce, Spark, HBase, CouchDB, Hive, etc.)Strong analytical and problem-solving skills.Experience working in a network operations center environment.Experience as an open source Contributor.', 'Explore suitable options and designs for specific analytical solutions.', 'diversity and inclusion', 'Work closely with Data Analysts to ensure data quality and availability for analytical modelling.', 'Bachelor’s degree or four or more years of work experience.Four or more years of experience as a data engineerFour or more years of experience finding, cleaning, and preparing data for use by Data ScientistsExperience knitting disperate data sources togetherFour or more years of experience building data pipelinesExperience using SQL (i.e., PL/SQL or T-SQL with RDBMSs like Teradata, MS SQL Server, Oracle, etc.)Experience in data engineering, databases, and data warehouses.Strong experience with data engineering in Python.Ability to travel occasionally', 'Strong analytical and problem-solving skills.', 'Four or more years of experience finding, cleaning, and preparing data for use by Data Scientists', 'What we’re looking for...', 'Support maintenance, bug fixes and, performance analysis along data pipeline.', 'Equal Employment Opportunity', 'When you join Verizon', 'Gather requirements, assess gaps, and build roadmaps and architectures to help the analytics driven organization achieve its goals.Work closely with Data Analysts to ensure data quality and availability for analytical modelling.Explore suitable options and designs for specific analytical solutions.Define extract, load, and transform (ELT) based on jointly defined requirements.Prepare, clean, and massage data for use in modeling and prototypesIdentify gaps and implement solutions for data security, quality, and automation of processes.Support maintenance, bug fixes and, performance analysis along data pipeline.', 'Experience using SQL (i.e., PL/SQL or T-SQL with RDBMSs like Teradata, MS SQL Server, Oracle, etc.)', 'Bachelor’s degree or four or more years of work experience.', 'Define extract, load, and transform (ELT) based on jointly defined requirements.', 'Experience with Scala, Julia, R, Python or other machine learning programming language', 'Prepare, clean, and massage data for use in modeling and prototypes', 'Identify gaps and implement solutions for data security, quality, and automation of processes.', 'Strong experience with data engineering in Python.', 'Experience on Big Data platforms (i.e., Hadoop, Map/Reduce, Spark, HBase, CouchDB, Hive, etc.)', 'Even Better If You Have', 'Gather requirements, assess gaps, and build roadmaps and architectures to help the analytics driven organization achieve its goals.', 'Experience working in a network operations center environment.']",Mid-Senior level,Full-time,Information Technology,Telecommunications,2021-03-24 13:05:10
Data Platform Engineer,Bloomberg LP,"New York, NY",2 days ago,Be among the first 25 applicants,"['We""d love to see:', ' Experience working with structured and unstructured data, especially CSV, JSON, Parquet', 'We""ll trust you to:', ' Experience with data processing systems using data processing frameworks such as Apache Spark, Apache Flink, Dask', ' 3+ years experience in software development using Python, Scala, or Go  Experience working with Kubernetes, especially extending Kubernetes using admission webhooks, controllers, and other building services that interact directly with the Kubernetes API Built systems in event-driven or streaming architectures using systems such as Kafka/Kinesis, RabbitMQ, NATS, and AWS SNS/SQS Experience with building, managing, and deploying systems used for data processing Architected, designed, developed, and operationalized scalable, resilient, and reliable services Software engineering fundamentals, including using participating in Agile, knowledge and use distributed version control systems such as Git/Mercurial, code reviews, emphasis on quality, and commitment to documentation', ' Design, build, and maintain the control plane for Second Measure""s platform for processing data built with Kubernetes, Argo Workflows, Presto, and Apache Spark Build tools and abstractions for using the platform that empower data scientists and data engineers with the autonomy to deliver value to our clients', ' Experience working with structured and unstructured data, especially CSV, JSON, Parquet Argo Workflows orchestration system experience Experience with data processing systems using data processing frameworks such as Apache Spark, Apache Flink, Dask Distributed query experience including Presto/Trino, AWS Redshift', ' Software engineering fundamentals, including using participating in Agile, knowledge and use distributed version control systems such as Git/Mercurial, code reviews, emphasis on quality, and commitment to documentation', ' Architected, designed, developed, and operationalized scalable, resilient, and reliable services', ' Distributed query experience including Presto/Trino, AWS Redshift', 'You""ll need to have:', ' Experience working with Kubernetes, especially extending Kubernetes using admission webhooks, controllers, and other building services that interact directly with the Kubernetes API', ' Experience with building, managing, and deploying systems used for data processing', ' Argo Workflows orchestration system experience', ' Built systems in event-driven or streaming architectures using systems such as Kafka/Kinesis, RabbitMQ, NATS, and AWS SNS/SQS', ' 3+ years experience in software development using Python, Scala, or Go ', ' Design, build, and maintain the control plane for Second Measure""s platform for processing data built with Kubernetes, Argo Workflows, Presto, and Apache Spark', ' Build tools and abstractions for using the platform that empower data scientists and data engineers with the autonomy to deliver value to our clients']",Not Applicable,Full-time,Engineering,Financial Services,2021-03-24 13:05:10
Data Engineer,Collabera Inc.,"Houston, TX",,N/A,"['', ""A bachelor's degree in Computer Sciences or related field, or equivalent work experience; master's degree preferred6+ years of experience in software development2+ years of experience leading small/medium teamsExperienced in building Real-time/Batch Ingestion and data processing pipelines built using Spark/DatabricksSkilled in coding in Python or JavaAble to contribute towards test automation and DevOps pipelinesStrong interpersonal skills, coupled with equally strong Team Building and CommunicationSense of urgency in daily work ethicStrong leadership, organizational and time management skills"", 'Present solutions to teams and stakeholders and document design decisions', 'Experienced in building Real-time/Batch Ingestion and data processing pipelines built using Spark/Databricks', ""A bachelor's degree in Computer Sciences or related field, or equivalent work experience; master's degree preferred"", 'Skilled in coding in Python or Java', 'Adopt Enterprise Architecture guidelines to store and manage Big Data.Develop data pipelines using Spark/DatabricksPresent solutions to teams and stakeholders and document design decisionsUtilize Enterprise Technology assets, when applicable.Work with Product owner to develop stories and user flows.Set up CI/CD Pipelines and maintain.Generally work is self-directed and not prescribed.Works with less structured, more complex issues.', 'This is Contract to Hire role with decent Salary plus benefits on conversion.Currently, not open for individuals who needs a sponsorship or has visa dependencies.', 'Utilize Enterprise Technology assets, when applicable.', 'In the role of Lead Software Engineer, Come and join our teams in building our Clinical Products and other applications. You will work independently and in a team as needed, in developing new features to build new products and enhance our applications as needed. Our platforms consume and generate a great volume of data.', '6+ years of experience in software development', 'Contract to Hire', 'Set up CI/CD Pipelines and maintain.', 'Sense of urgency in daily work ethic', 'Strong interpersonal skills, coupled with equally strong Team Building and Communication', 'Adopt Enterprise Architecture guidelines to store and manage Big Data.', 'Prerequisite', 'You will be responsible to', 'Work with Product owner to develop stories and user flows.', 'Works with less structured, more complex issues.', '2+ years of experience leading small/medium teams', 'Hiring Data Engineer for 100% Remote Role', 'This is Contract to Hire role with decent Salary plus benefits on conversion.', 'Currently, not open for individuals who needs a sponsorship or has visa dependencies.', 'Able to contribute towards test automation and DevOps pipelines', 'Develop data pipelines using Spark/Databricks', 'Must Have ', 'Description: ', 'Generally work is self-directed and not prescribed.', 'Strong leadership, organizational and time management skills']",Executive,Contract,Information Technology,Pharmaceuticals,2021-03-24 13:05:10
Data Engineer,Eliassen Group,"Mooresville, NC",,N/A,"['', 'This position can offer a very strong compensation with benefits and PTO if needed.\xa0We cannot work through a 3rd party employer for this role.\xa0', 'Bachelors Degree in Computer Science, Engineering or related field preferredExpertise in Java, Scala, PythonExpertise in SQLExpertise in ScriptingExpertise in TeradataExpertise in Hadoop (Sqoop, Hive, Pig, Map Reduce)Expertise in Spark (Spark Streaming, MLib)Expertise in Kafka or Cloud Bigdata components', 'Expertise in Java, Scala, Python', 'Related Terms: Java, Scala, Python, SQL, Scripting, Teradata, Hadoop, Sqoop, Hive, Pig, Map Reduce, Spark, Spark Streaming, MLib, Kafka, Cloud, Bigdata, BI, Data Warehousing, Platform Engineering, ETL, IT Infrastructure Library framework', 'A Fortune 500 client in the Charlotte market is seeking a Data Engineer. This Data Engineer will design, write, develop and implement innovative enterprise data integration solutions.', 'Expertise in Kafka or Cloud Bigdata components', 'This Data Engineer will have experience with Data, BI, Platform Engineering, Data Warehousing, ETL, and/or Software Engineering.\xa0The ideal Data Engineer will also have experience working with defect or incident tracking software, technical documentation, and IT Infrastructure Library framework.', 'Expertise in Teradata', 'Please send over an updated resume to Jdonahue@eliassen.com', 'Expertise in SQL', 'Expertise in Scripting', 'Expertise in Hadoop (Sqoop, Hive, Pig, Map Reduce)', 'Expertise in Spark (Spark Streaming, MLib)', 'Requirements', 'Bachelors Degree in Computer Science, Engineering or related field preferred']",Mid-Senior level,Full-time,Information Technology,Staffing and Recruiting,2021-03-24 13:05:10
Data Engineer,CVS Health,"New York, NY",7 days ago,58 applicants,"['', 'Required Qualifications', 'Business Overview', 'Education', 'Preferred Qualifications', 'Job Description']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,"Docker, Inc","Boston, MA",4 weeks ago,80 applicants,"['', '2+ years of relevant industry experience', 'Creating production-ready ETL scripts with Python and SQL', 'Preferred Qualifications', 'Strong verbal and written communication skills', 'Data Engineer (Remote)', 'Maintain the integrity of data within our data pipeline and warehouse', 'Experience using data collection platforms such as Segment, RudderStack, Fivetran etc. ', 'Collaborate with product managers and analysts throughout the company to deliver reliable data that powers actionable insights', ' 2+ years of relevant industry experience Familiarity with data warehousing concepts including data model design and query optimization strategies Experience using and maintaining BI visualization tools such as (but not limited to) Looker, Tableau, or Power BI.  Creating production-ready ETL scripts with Python and SQL Experience with version control systems such as Github, Gitlab, Bitbucket etc.  Experience automating business and reporting processes Experience using data analysis and/or statistics to inform decisions  Strong verbal and written communication skills ', 'Experience with version control systems such as Github, Gitlab, Bitbucket etc. ', 'Ensure quality of data and completeness of event logging across Docker codebase', 'Experience using and maintaining BI visualization tools such as (but not limited to) Looker, Tableau, or Power BI. ', 'Integrate emerging methodology, technology, and version control practices that best fit the team. ', 'Champion a data-informed mindset within our culture', 'Responsibilities', 'Transform raw event logs into higher-order tables to make existing analysis easier and new analysis possible', 'Experience using data analysis and/or statistics to inform decisions ', 'Familiarity with at least one cloud ecosystem: AWS/Azure Infrastructure/Google Cloud', 'Creating automated reporting of weekly and monthly metrics and ROI for the executive management team and board', 'Integrate data from 3rd party services via ETL tools and custom pipelines', 'Proficiency in building data pipelines leveraging Docker and orchestration frameworks such as Jenkins/Airflow/Luigi', 'Qualifications', 'Experience automating business and reporting processes', 'Proficiency working with a Data Warehouse platform (e.g. Snowflake, Redshift, BigQuery, AWS Athena, or Spectrum)', 'At least 6 months of experience with Looker and LookML ', 'Experience designing and deploying high-performance systems with reliable monitoring and logging practices', 'Develop ETL jobs and tests to process, validate, transport, collate, aggregate, and distribute data', 'BS/MS in Computer Science, Math, Physics, or other technical fields', 'Design, build and automate business metrics into self-serve dashboards via Looker ', 'Experience of working in an agile environment and using tools such as JIRA/Asana/Trello ', 'Familiarity with data warehousing concepts including data model design and query optimization strategies', ' BS/MS in Computer Science, Math, Physics, or other technical fields At least 6 months of experience with Looker and LookML  Proficiency in building data pipelines leveraging Docker and orchestration frameworks such as Jenkins/Airflow/Luigi Experience designing and deploying high-performance systems with reliable monitoring and logging practices Proficiency working with a Data Warehouse platform (e.g. Snowflake, Redshift, BigQuery, AWS Athena, or Spectrum) Experience using data collection platforms such as Segment, RudderStack, Fivetran etc.  Familiarity with at least one cloud ecosystem: AWS/Azure Infrastructure/Google Cloud Experience of working in an agile environment and using tools such as JIRA/Asana/Trello  ', ' Implement, document, oversee and evolve the Snowflake and ETL infrastructure Maintain the integrity of data within our data pipeline and warehouse Ensure quality of data and completeness of event logging across Docker codebase Integrate data from 3rd party services via ETL tools and custom pipelines Develop ETL jobs and tests to process, validate, transport, collate, aggregate, and distribute data Transform raw event logs into higher-order tables to make existing analysis easier and new analysis possible Creating automated reporting of weekly and monthly metrics and ROI for the executive management team and board Integrate emerging methodology, technology, and version control practices that best fit the team.  Design, build and automate business metrics into self-serve dashboards via Looker  Collaborate with product managers and analysts throughout the company to deliver reliable data that powers actionable insights Champion a data-informed mindset within our culture ', 'Implement, document, oversee and evolve the Snowflake and ETL infrastructure']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Integration Engineer,Peloton Interactive,"Plano, TX",2 weeks ago,34 applicants,"['', 'Job Responsibilities', 'Own the Boomi development process from requirements gathering to full implementation.', 'Peloton does not accept unsolicited agency resumes. Agencies should not forward resumes to our jobs alias, Peloton employees or any other organization location. Peloton is not responsible for any agency fees related to unsolicited resumes.', 'Lead the research, development & implementation of special projects, as needed.', 'Strong verbal and written communication skills', 'SQL language proficiency with at least one of the common database platforms such as SQL Server, Oracle, MySQL, and/or DB2 database management systems', 'You are a proactive problem-solver, even in areas of uncertainty and ambiguity.', 'Dell Boomi ', 'supply chain systems data integrations', 'Strong understanding of integration architecture options such as SoA and APIs', 'Experience with REST and SOAP web services', 'Boomi Developer/Architect certified', 'Collaborate with the development team to architect efficient and stable integrations.', 'You have excellent analytical and critical reasoning skills.', 'Monitor, troubleshoot, and resolve problems with integrations.', 'Ability to work well under pressure; manage tight deadlines and situations where conflicting priorities arise.', 'Minimum of 3 years experience working with integration platforms (Boomi, MuleSoft etc.).', ' You have the ability to analyze business problems and develop both long-term sustainable solutions and also quick tactical solutions. You can articulate complex concepts in a way that is understandable to non-technical stakeholders. You have excellent analytical and critical reasoning skills. You are a proactive problem-solver, even in areas of uncertainty and ambiguity. You possess strong collaboration skills and approach problems with positive intent while driving towards resolution. ', 'Bachelor’s degree', 'You can articulate complex concepts in a way that is understandable to non-technical stakeholders.', 'Strong analytical and critical thinking skills', 'Experience developing applications that utilize Boomi Integration', ' Bachelor’s degree Strong verbal and written communication skills Strong analytical and critical thinking skills Adapt and proactive at problem-solving and conflict resolution. Minimum of 3 years experience working with integration platforms (Boomi, MuleSoft etc.). Proven ability to implement at least 1 to 2 full integration projects involving on-premise and Cloud applications. Functional experience with ERP systems (i.e., NetSuite, SAP) Experience with REST and SOAP web services SQL language proficiency with at least one of the common database platforms such as SQL Server, Oracle, MySQL, and/or DB2 database management systems Ability to work well under pressure; manage tight deadlines and situations where conflicting priorities arise. ', 'Functional experience with ERP systems (i.e., NetSuite, SAP)', 'Provide guidance on the monitoring, troubleshooting, and resolution of data quality issues in collaboration with the appropriate stakeholders; and', 'About Peloton', ' Integration Engineer', 'Document and analyze current business processes and underlying systems/applications.', 'Basic Job Requirements', 'Adapt and proactive at problem-solving and conflict resolution.', 'You have the ability to analyze business problems and develop both long-term sustainable solutions and also quick tactical solutions.', 'Hands-on experience on Integration Platforms such as Boomi, MuleSoft; AnyPoint, Pentaho, and Informatica', ' Own the Boomi development process from requirements gathering to full implementation. Monitor, troubleshoot, and resolve problems with integrations. Collaborate with the development team to architect efficient and stable integrations. Document and analyze current business processes and underlying systems/applications. Lead the research, development & implementation of special projects, as needed. Provide guidance on the monitoring, troubleshooting, and resolution of data quality issues in collaboration with the appropriate stakeholders; and Communicate and collaborate effectively with technical peers and business users. ', 'You possess strong collaboration skills and approach problems with positive intent while driving towards resolution.', 'Experience with performance tuning optimization within Boomi', 'Proven ability to implement at least 1 to 2 full integration projects involving on-premise and Cloud applications.', 'Preferred Experience', ' Boomi Developer/Architect certified Experience developing applications that utilize Boomi Integration Strong understanding of integration architecture options such as SoA and APIs Experience with performance tuning optimization within Boomi Functional experience with ERP systems (i.e., NetSuite, SAP) Hands-on experience on Integration Platforms such as Boomi, MuleSoft; AnyPoint, Pentaho, and Informatica ', 'Who You Are', 'Communicate and collaborate effectively with technical peers and business users.']",Entry level,Full-time,Information Technology,"Health, Wellness and Fitness",2021-03-24 13:05:10
Data Engineer,TuneCore,"Brooklyn, NY",2 weeks ago,25 applicants,"['', 'Matillion / Fivetran / MDM', 'Be comfortable working in a complex data environment involving in-depth data discovery, exploration and troubleshooting.', 'Comprehensive knowledge of data-modeling principles specifically with a focus on data analytics', 'Strong ETL proficiency using GUI-based tools or code-based patterns.', 'Have a thorough understanding of building data pipelines for RDBMS systems and various cloud-based data sources.', 'AWS', ""TuneCore is going through some amazing changes. In partnership with our parent company, Believe International, we are expanding rapidly with an emphasis on teamwork and career-growth. Our focus continues to lie in building a workplace that incorporates respect, fairness, transparency and growth in every step of our employees' journey and we intend to continue working tirelessly to create a culture of positivity, diversity and kindness in an enjoyable workplace. "", 'Experience with software engineering practicesMatillion / Fivetran / MDMAWSSnowflakeTableau', 'Be highly proficient in SQL & Python with an ability to work with large and highly complex datasets', 'Experience with software engineering practices', 'Excellent communication and interpersonal skills with the ability to liaise with multiple teams.', 'Candidates Should', 'TuneCore is seeking a seasoned Data Engineer who thrives in a dynamic and fast-paced environment and is able to work alongside our Software Engineering, Dev-Ops and Data Analytics teams to provide impactful solutions for all our data-driven business needs. ', 'Ensure adequate and thorough documentation of all existing and new processes.', 'Must have at least 5 years of demonstrable work experience as a Data Engineer or Data Architect in an enterprise environment.', 'Must have at least 5 years of demonstrable work experience as a Data Engineer or Data Architect in an enterprise environment.In-depth knowledge of RDBMS systems such as MySQL, MariaDB, PostgreSQL as well as cloud-based data warehousing solutions such as RedShift & Snowflake.Comprehensive knowledge of data-modeling principles specifically with a focus on data analyticsStrong ETL proficiency using GUI-based tools or code-based patterns.Excellent communication and interpersonal skills with the ability to liaise with multiple teams.', 'Be able to apply their skills in ingenious ways to solve new and existing problems.', 'Highly Desired Skills', 'Tableau', 'Be open to exploring and learning new technologies on the go.', 'Snowflake', 'Qualifications', 'Be highly proficient in SQL & Python with an ability to work with large and highly complex datasetsHave a thorough understanding of building data pipelines for RDBMS systems and various cloud-based data sources.Be comfortable working in a complex data environment involving in-depth data discovery, exploration and troubleshooting.Be able to apply their skills in ingenious ways to solve new and existing problems.Be open to exploring and learning new technologies on the go.Have the desire to lead and own the data engineering component of our new enterprise cloud-based data architecture.Be comfortable working in a cross-functional responsibility.Ensure adequate and thorough documentation of all existing and new processes.', 'Company Description', 'In-depth knowledge of RDBMS systems such as MySQL, MariaDB, PostgreSQL as well as cloud-based data warehousing solutions such as RedShift & Snowflake.', 'Job Description', 'Be comfortable working in a cross-functional responsibility.', 'Have the desire to lead and own the data engineering component of our new enterprise cloud-based data architecture.']",Mid-Senior level,Full-time,Information Technology,Marketing and Advertising,2021-03-24 13:05:10
Data Engineer (Remote),Automox,"Atlanta, GA",5 days ago,Be among the first 25 applicants,"['', ' Overview ', 'Experience partnering with business intelligence and analytics teams ', 'Experience with both relational SQL and NoSQL databases', 'Proficiency with and willingness to learn programming languages such as Golang, Python, Java, Scala', 'Work on projects that are critical to Automox’s mission and have high visibility across the companyBuild, enable, and maintain high-quality, reliable data infrastructurePartner with data science, engineering, and product teams to deliver new capabilities to customersLeverage modern engineering practices and tools in a cloud-native SaaS environmentHave an opinion, test your ideas, work with talented yet humble people, have fun, get stuff done', 'Build analytics tools as part of a data platform to enable data access, analytics workflows, and actionable insights into customer’s IT environments and the company’s operational efficiency', 'Stock options', 'Build, enable, and maintain high-quality, reliable data infrastructure', 'Parental Benefits: Adoption benefits, Parental leave', ""Our salary ranges are based on national averages and are determined based on the level of the position we are hiring for. The ranges are wide to leave room for variability in a candidate's skills, experience, and location all which impact where someone might come in on the range."", ' Benefits ', 'Have an opinion, test your ideas, work with talented yet humble people, have fun, get stuff done', 'Expertise with deploying and maintaining distributed data processing frameworks, database technologies, and streaming/messaging systems', 'Work on projects that are critical to Automox’s mission and have high visibility across the company', 'Knowledge of data science principles, statistics, or machine learning can be beneficial but should not be considered a required skill set', 'All employees are part of our Company bonus plan. Our bonus is a mix of company performance and individual contributions.', ' Equity ', ' Salary ', ' Bonus ', 'Ability to assemble large, complex data sets that satisfy the needs of both internal and external customers', 'Comfortable working in a remote/distributed work environment requiring excellent verbal, written, and asynchronous communication skills', 'Time off: We have a flexible PTO policy with an additional 9 paid holidays.', 'Perks: Monthly internet and wellness stipend, money to set up your home office, and no commute.', 'Partner with data science, engineering, and product teams to deliver new capabilities to customers', '$105,000 - $150,000/year', 'Track record of creating and maintaining data pipelines', 'Track record of creating and maintaining data pipelinesExpertise with deploying and maintaining distributed data processing frameworks, database technologies, and streaming/messaging systemsAbility to assemble large, complex data sets that satisfy the needs of both internal and external customersBuild analytics tools as part of a data platform to enable data access, analytics workflows, and actionable insights into customer’s IT environments and the company’s operational efficiencyFamiliarity with cloud environments and technologies (AWS preferred)Experience with both relational SQL and NoSQL databasesProficiency with and willingness to learn programming languages such as Golang, Python, Java, ScalaExperience partnering with business intelligence and analytics teams Comfortable working in a remote/distributed work environment requiring excellent verbal, written, and asynchronous communication skillsKnowledge of data science principles, statistics, or machine learning can be beneficial but should not be considered a required skill set', 'Healthcare options through Cigna and Guardian including Medical, Dental, Vision, Basic Life insurance, Voluntary Life Insurance, Basic STD & LTD, HSA, FSA, 401(k) and more. Automox has a generous employer contribution towards all health plans with low premiums for all employees.', 'Familiarity with cloud environments and technologies (AWS preferred)', 'Leverage modern engineering practices and tools in a cloud-native SaaS environment']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Brooksource,Greater Minneapolis-St. Paul Area,2 days ago,27 applicants,"['', 'If you feel that this describes you, keep reading and apply!', 'Participate in architectural evolution of data engineering patterns, frameworks, systems, and platforms including defining best practices and standards for managing data collections and integration.', ""Design and develop ETL/ELT solutions on Azure Databricks, Delta Lake and Spark to support Digital MBO's."", 'Experience designing and building data service APIs.', 'Mentor other data engineers and provide significant technical direction by teaching other data engineers how to leverage cloud data platforms.', 'Consume data from a variety of sources (RDBMS, APIs, FTPs and other cloud storage) & formats (Excel, CSV, XML, JSON, Parquet, Unstructured)', 'This is a unique, high visibility opportunity for someone who wants to have business impact, dive deep into large scale data pipeline and work closely with cross functional team. This position would be fully remote, and report directly to the Director of Data Science.', 'Responsibilities:', 'Design and build data service APIs.', 'Eight Eleven Group provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, national origin, age, sex, citizenship, disability, genetic information, gender, sexual orientation, gender identity, marital status, amnesty or status as a covered veteran in accordance with applicable federal, state, and local laws.\xa0', 'Experience writing advanced / complex SQL with performance tuning and optimization.', 'Identify ways to improve data reliability, data integrity, system efficiency and quality.', 'Work with data scientists to deploy machine learning models to real-time analytics systems.', 'As a Data Engineer, you will work cross-functionally with data scientists, data analysts, product managers, and stakeholders to understand business needs and develop, maintain and optimize the data sets, data models and large-scale data pipelines. You’ll primarily leverage the Azure Databricks Spark cloud stack used for data science models and visualizations. You will partner with various teams to drive best practices and set standards for data engineering patterns and optimization. You are a key influencer in data engineering strategy.', 'Create and maintain the technical architecture of the Enterprise Delta Lake to consolidate data from many systems into a single source for machine learning and reporting analytics.', '5+ years of experience optimizing and managing modern large-scale data pipelines ETL/ELT processing to support data integration for analytics, machine learning features and predictive modelling.Expertise within Azure Databricks, Delta Lake and SparkExperience writing advanced / complex SQL with performance tuning and optimization.Experience designing and building data service APIs.Interest in and proven ability to mentor and coach other data engineers.', 'Required Qualifications:', 'Interest in and proven ability to mentor and coach other data engineers.', ""Design and develop ETL/ELT solutions on Azure Databricks, Delta Lake and Spark to support Digital MBO's.Develop, implement, and deploy large scale data pipelines powering machine learning algorithms, insights generation, business intelligence dashboards, reporting and new data products.Create and maintain the technical architecture of the Enterprise Delta Lake to consolidate data from many systems into a single source for machine learning and reporting analytics.Consume data from a variety of sources (RDBMS, APIs, FTPs and other cloud storage) & formats (Excel, CSV, XML, JSON, Parquet, Unstructured)Identify ways to improve data reliability, data integrity, system efficiency and quality.Participate in architectural evolution of data engineering patterns, frameworks, systems, and platforms including defining best practices and standards for managing data collections and integration.Work with data scientists to deploy machine learning models to real-time analytics systems.Design and build data service APIs.Mentor other data engineers and provide significant technical direction by teaching other data engineers how to leverage cloud data platforms."", 'Expertise within Azure Databricks, Delta Lake and Spark', 'Develop, implement, and deploy large scale data pipelines powering machine learning algorithms, insights generation, business intelligence dashboards, reporting and new data products.', '5+ years of experience optimizing and managing modern large-scale data pipelines ETL/ELT processing to support data integration for analytics, machine learning features and predictive modelling.', 'Do you love big data and the challenges it entails? Do you want to create applications that people love?']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Apex Systems,"Cincinnati, OH",2 weeks ago,31 applicants,"['', '\ufeffEssential Responsibilities:\xa0', 'Ability to advise on technology decisionsEmotional Intelligence, ability to influence up and out and the ability to work independentlyMust be a team player with a strong desire to winHighly organized and efficient; able to balance competing priorities and execute accordinglyStrong Customer mindset and ability to understand commercial and product strategy.Strong analytical and strong problem-solving skills, effectively evaluates information/data to make decisionsAnticipates obstacles and develops plans to resolveContinuous improvement oriented – actively generates process improvements; champions and drives change initiativesStrong oral and written communication skills.', 'Develop communication, training and change management deliverables required to increase adoption.\xa0', 'Continuous improvement oriented – actively generates process improvements; champions and drives change initiatives', 'Emotional Intelligence, ability to influence up and out and the ability to work independently', 'Must be a team player with a strong desire to win', 'Desired Characteristics', 'Desired Characteristics:\xa0', 'Continuously evaluate current system capabilities, create/update standards to ensure optimal system performance; seek opportunities to optimize the SAP reporting/analytics platform for space utilization, query optimization, SLT optimization.', 'Strong oral and written communication skills.', 'Anticipates obstacles and develops plans to resolve', 'Collaborate with OASIS functional teams to understand the functional information required to deliver reports/analytics', 'Collaborate with BASIS, Security and Infrastructure teams, as needed, to achieve Reporting & Analytic priorities.', '\ufeffEssential Responsibilities', 'Work closely with the team’s technical product architects/owners and business customers to understand the reporting and analytic needs and recommend technical solutions to meet them.', 'Experience with SAP Business Objects technology stack', 'Strong analytical and strong problem-solving skills, effectively evaluates information/data to make decisions', 'At least 6 years of experience in data and analytics.', 'Strong Customer mindset and ability to understand commercial and product strategy.', 'Apex Systems is looking for an Analytics Engineer\xa0for one of our large fortune 500 clients in Cincinnati, OH. If you are interested in any Analytics Engineer opportunities, please email Phillip Medley at pmedley@apexsystems.com.\xa0\xa0\xa0', '\xa0', 'Serve as the technical owner and point of contact for the overall system architecture, including architecture of data models, reports and applications.', 'Promote and accelerate self-service reporting capability', 'Knowledge of SAP structures, object dependencies and tables', 'Experience with SQL and Spotfire development', 'Provide technical input and oversight, as needed in related technologies: data integrations, Data Modeling, HANA sidecar, SAP security models.', 'Establish standards for development, security and deployments', 'Manage the platform roadmap to ensure continued platform performance/stability.', 'Highly organized and efficient; able to balance competing priorities and execute accordingly', 'Contribute technical expertise to the Reporting & Analytics strategy, including creation of conceptual and logical data models ensuring that uniformly recognized and accepted data definitions are developed and applied throughout the Solution Architecture.', 'Eligibility Requirements:\xa0', ""Bachelor's Degree in Computer Science or “STEM” Majors (Science, Technology, Engineering and Math)"", 'Eligibility Requirements', 'Ability to advise on technology decisions', ""Bachelor's Degree in Computer Science or “STEM” Majors (Science, Technology, Engineering and Math)At least 6 years of experience in data and analytics.Knowledge of SAP structures, object dependencies and tablesExperience with SAP HANA Sidecar and SLT Technology stackExperience with SAP Business Objects technology stackExperience with data modeling o help implement data dictionary type definitionsExperience with SQL and Spotfire development"", 'Experience with SAP HANA Sidecar and SLT Technology stack', 'Work closely with the team’s technical product architects/owners and business customers to understand the reporting and analytic needs and recommend technical solutions to meet them.Serve as the technical owner and point of contact for the overall system architecture, including architecture of data models, reports and applications.Continuously evaluate current system capabilities, create/update standards to ensure optimal system performance; seek opportunities to optimize the SAP reporting/analytics platform for space utilization, query optimization, SLT optimization.Manage the platform roadmap to ensure continued platform performance/stability.Contribute technical expertise to the Reporting & Analytics strategy, including creation of conceptual and logical data models ensuring that uniformly recognized and accepted data definitions are developed and applied throughout the Solution Architecture.Provide technical input and oversight, as needed in related technologies: data integrations, Data Modeling, HANA sidecar, SAP security models.Collaborate with BASIS, Security and Infrastructure teams, as needed, to achieve Reporting & Analytic priorities.Establish standards for development, security and deploymentsPromote and accelerate self-service reporting capabilityCollaborate with OASIS functional teams to understand the functional information required to deliver reports/analyticsDevelop communication, training and change management deliverables required to increase adoption.\xa0', 'Experience with data modeling o help implement data dictionary type definitions', ""Wouldn't be able to work C2C at this time.  ""]",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Data Engineer,Slalom,"San Jose, CA",3 days ago,101 applicants,"['', ' You embrace a continuous learner mentality. ', ' Experience working with SQL on relational databases ', ' We choose to imagine things made better, and then set out on a journey to realize what’s possible. ', ' You’re a smart, collaborative person who is excited about technology and driven to get things done. ', ' Understanding of agile project approaches and methodologies ', ' Understanding of basic testing types ', ' Self-starter with the ability to work independently or as part of a project team ', ' We’ll never trade the upside of wonder for the comfort of the familiar or the safety of convention. ', ' Strong analytical problem-solving ability ', ' We are engineers, makers, planners, architects, and designers.  We choose to imagine things made better, and then set out on a journey to realize what’s possible.  We’ll never trade the upside of wonder for the comfort of the familiar or the safety of convention. ', ' You have passion for data! ', ' We are engineers, makers, planners, architects, and designers. ', ' Deep programming skills in one of modern object-oriented data pipelining languages like Python or Java ', ' Experience with workflow orchestration platforms such as Airflow, Glue and Dataflow ', ' Experience with code management tools (e.g. Git, SVN) and DevOps tools (e.g. Docker, Bamboo, Jenkins) ', ' Understanding of different types of storage (filesystem, relation, MPP, NoSQL) and working with various kinds of data (structured, unstructured, metrics, logs, etc.) ', ' You have passion for data!  You’re a smart, collaborative person who is excited about technology and driven to get things done.  You’re not afraid to be bring your authentic self to work.  You embrace a continuous learner mentality. ', 'Qualifications', ' Strong aptitude for learning new technologies and analytics techniques ', ' Understanding of data architecture concepts such as data modeling, metadata, workflow management, ETL/ELT, real-time streaming), data quality ', ' Familiarity or strong desire to learn quantitative analysis techniques (e.g., predictive modeling, machine learning, segmentation, optimization, clustering, regression) ', 'Why do we work here?', ' Capability to conduct performance analysis, troubleshooting and remediation ', ' You’re not afraid to be bring your authentic self to work. ', 'What technologies will you be using?', 'Data Engineer', ' Deep programming skills in one of modern object-oriented data pipelining languages like Python or Java  Experience with code management tools (e.g. Git, SVN) and DevOps tools (e.g. Docker, Bamboo, Jenkins)  Experience with workflow orchestration platforms such as Airflow, Glue and Dataflow  Understanding of different types of storage (filesystem, relation, MPP, NoSQL) and working with various kinds of data (structured, unstructured, metrics, logs, etc.)  Understanding of data architecture concepts such as data modeling, metadata, workflow management, ETL/ELT, real-time streaming), data quality  Familiarity or strong desire to learn quantitative analysis techniques (e.g., predictive modeling, machine learning, segmentation, optimization, clustering, regression)  Understanding of agile project approaches and methodologies  Understanding of basic testing types  Experience working with SQL on relational databases  Strong aptitude for learning new technologies and analytics techniques  Strong analytical problem-solving ability  Self-starter with the ability to work independently or as part of a project team  Capability to conduct performance analysis, troubleshooting and remediation ', 'Who are you?', 'Who are we?']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
Big Data Senior Engineer,American Express,"New York, NY",2 days ago,Be among the first 25 applicants,"['', 'Experience managing design, implementation and architecture governance within a large corporate environment.', '4+ years of experience in software engineering and data related services.', 'Minimum Qualifications', 'Set The Agenda: Define What Winning Looks Like, Put Enterprise Thinking First, Lead with an External Perspective Bring Others With You: Build the Best Team, Seek & Provide Coaching Feedback, Make Collaboration EssentialDo It The Right Way: Communicate Frequently, Candidly & Clearly, Make Decisions Quickly & Effectively, Live the Blue Box Values, Great Leadership Demands Courage\u2002\u2002', 'BS degree or higher in computer science or related discipline4+ years of experience on an agile development teamStrong debugging skillsBig Data Platform experienceDeep understanding of data model design.Strong working knowledge of hive, spark, shell scripting, java.Experience with developing an Organized Data Layer, familiarity with EnterpriseSalesforce ODL preferred.Experience with Cornerstone frameworks and process, event engine, ingestion, use case setup and managementWorking knowledge of Blaze ODL frameworks and processes, Job Execution Framework, MemSQL Bulk LoaderLearn new technologies and drive opportunities for adoption', 'Knowledge Of Platforms', '4+ years practical agile experience. ', 'American Express is an equal opportunity employer and makes employment decisions without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, disability status, age, or any other status protected by law.', 'Experience coding solutions to satisfy user stories provided by product owners', 'Enterprise Cloud PlatformBig DataBlaze', 'Strong working knowledge of hive, spark, shell scripting, java.', 'Strong debugging skills', 'Big Data Platform experience', 'Working knowledge of Blaze ODL frameworks and processes, Job Execution Framework, MemSQL Bulk Loader', 'Bring Others With You: Build the Best Team, Seek & Provide Coaching Feedback, Make Collaboration Essential', 'Do It The Right Way: Communicate Frequently, Candidly & Clearly, Make Decisions Quickly & Effectively, Live the Blue Box Values, Great Leadership Demands Courage\u2002\u2002', 'Bachelor’s degree in related field or equivalent years of experience. Preferred degree in Computer Science or Mathematics.', 'Enterprise Leadership Behaviors', 'Experience with developing an Organized Data Layer, familiarity with EnterpriseSalesforce ODL preferred.', 'Past Experience', 'Functional Skills/Capabilities', 'Deep understanding of data model design.', 'Blaze', 'Experience coding solutions to satisfy user stories provided by product ownersAble to colaborate and communicate with other engineers and leadership', 'You Lead the Way. We’ve Got Your Back.', 'Learn new technologies and drive opportunities for adoption', 'Big Data', 'Enterprise Cloud Platform', 'BS degree or higher in computer science or related discipline', '2+ years experience leading a geographically dispersed team of data engineers on a enterprise class platform', 'Set The Agenda: Define What Winning Looks Like, Put Enterprise Thinking First, Lead with an External Perspective ', '4+ years of experience in software engineering and data related services.4+ years practical agile experience. Experience managing design, implementation and architecture governance within a large corporate environment.2+ years experience leading a geographically dispersed team of data engineers on a enterprise class platform', 'Experience with Cornerstone frameworks and process, event engine, ingestion, use case setup and management', 'Able to colaborate and communicate with other engineers and leadership', '4+ years of experience on an agile development team', 'Academic Background']",Mid-Senior level,Full-time,Information Technology,Financial Services,2021-03-24 13:05:10
Advanced Data Engineer,Honeywell,"Atlanta, GA",5 days ago,43 applicants,"['', 'Experience working with remote and global teams and cross team collaboration', 'Good understanding of branching, build, deployment, CI/CD methodologies such as Octopus and Bamboo', 'Experience with visualization software (Tableau, Spotfire, Qlikview, Angular js, D3.js)', 'Experience in writing complex SQL statements', 'Experience working with in Agile Methodologies and Scrum', 'Technology upgrade oversight', 'Hands on experience in Databricks, Cloudera, Hortonworks and/or Cloud (AWS EMR, Azure Data Lake Storage) based Hadoop distributions.Effective communication skills and succinct articulationExperience in writing complex SQL statementsExperience in working with cloud-based deployments. Understanding of containers & container orchestration (Swarm or Kubernetes).\xa0Experience in building advanced analytics solutions with data from enterprise systems like ERPs, CRMs, Marketing tools etc.Experience with dimensional modeling, data warehousing and data miningGood understanding of branching, build, deployment, CI/CD methodologies such as Octopus and BambooExperience working with in Agile Methodologies and ScrumKnowledge of software best practices, like Test-Driven Development (TDD)Database performance management and API developmentTechnology upgrade oversightExperience with visualization software (Tableau, Spotfire, Qlikview, Angular js, D3.js)Understanding of best-in-class model and data configuration and development processesExperience working with remote and global teams and cross team collaborationConsistently makes timely decisions even in the face of complexity, balancing systematic analysis with decisiveness', 'Minimum 2 years of experience in working with at least one NoSQL system (HBase, Cassandra, MongoDB etc.). In-depth knowledge of schema design to effectively tackle the requirement.', 'Experience with dimensional modeling, data warehousing and data mining', 'Consistently makes timely decisions even in the face of complexity, balancing systematic analysis with decisiveness', 'Join a company that is transforming from a traditional industrial company to a contemporary digital industrial business, harnessing the power of cloud, big data, analytics, Internet of Things, and design thinking. You will lead change that brings value to our customers, partners, and shareholders through the creation of innovative software and data-driven products and services. You will work with customers to identify their high value business questions and work through their data to search for answers. You will be responsible for working within Honeywell to identify opportunities for new growth and efficiency based on data analysis.\xa0', ""Bachelor's degree in Computer Science, Engineering, Applied Mathematics or related field"", 'Honeywell is charging into the Industrial IoT revolution with the establishment of Honeywell Connected Enterprise (HCE), building on our heritage of invention and deep, on-the-ground industry expertise. HCE is the leading industrial disruptor, building and connecting software solutions to streamline and centralize the assets, people and processes that help our customers make smarter, more accurate business decisions. Moving at the speed of software, we are creating, innovating and delivering solutions fast, challenging the way things have always been done, piloting new ways for all of us to work, and expecting our successes to set new standards for our customers and for Honeywell.', 'Experience in working with cloud-based deployments. Understanding of containers & container orchestration (Swarm or Kubernetes).\xa0', 'Minimum 3 years of experience in developing and building applications to process very large amounts of data (structured and unstructured), including streaming real-time data (Spark, Scala, Kafka, Python, Spark streaming or other such tools).', 'Minimum of 4 years of hands on experience with Spark, Pig/Hive, etc. and automation of data flow using Informatica, Spark, NiFi or Airflow/Oozie.', '\xa0', 'Database performance management and API development', 'YOU MUST HAVE', ""Bachelor's degree in Computer Science, Engineering, Applied Mathematics or related fieldMinimum of 6 years of data engineering experienceMinimum of 4 years with development and deployment of complex big data ingestion jobs in Spark/Informatica BDM/Talend bringing prototypes to production on Hadoop/NoSQL/MPP platforms.Minimum of 4 years of hands on experience with Spark, Pig/Hive, etc. and automation of data flow using Informatica, Spark, NiFi or Airflow/Oozie.Minimum 3 years of experience in developing and building applications to process very large amounts of data (structured and unstructured), including streaming real-time data (Spark, Scala, Kafka, Python, Spark streaming or other such tools).Minimum 2 years of experience in working with at least one NoSQL system (HBase, Cassandra, MongoDB etc.). In-depth knowledge of schema design to effectively tackle the requirement."", 'Minimum of 6 years of data engineering experience', 'Hands on experience in Databricks, Cloudera, Hortonworks and/or Cloud (AWS EMR, Azure Data Lake Storage) based Hadoop distributions.', 'Knowledge of software best practices, like Test-Driven Development (TDD)', 'Experience in building advanced analytics solutions with data from enterprise systems like ERPs, CRMs, Marketing tools etc.', 'You will identify and implement process improvements – and you don’t like to the same thing twice so you will automate it if you can. You are always keeping an eye on scalability, optimization, and process.\xa0\xa0You have worked with Big Data before, IoT data, SQL, Azure, AWS, and a bunch of other acronyms.\xa0', 'JOB ACTIVITIES', 'As an Advanced Data Engineer, you will be part of a team that delivers contemporary analytics solutions for all Honeywell business groups and functions. You will build strong relationships with leadership to effectively deliver contemporary data analytics solutions and contribute directly to business success. You will develop solutions on various Database systems viz. Databricks, Hive, Hadoop, PostgreSQL, etc.\xa0', 'Understanding of best-in-class model and data configuration and development processes', 'You will work on a team including scrum masters, product owners, data architects, data engineers, data scientists and DevOps. You and your team collaborate to build products from the idea phase through launch and beyond. The software you write makes it to production in couple of sprints. Your team will be working on creating a new platform using your experience of APIs, microservices, and platform development.', 'Effective communication skills and succinct articulation', 'Minimum of 4 years with development and deployment of complex big data ingestion jobs in Spark/Informatica BDM/Talend bringing prototypes to production on Hadoop/NoSQL/MPP platforms.', 'WE VALUE']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-24 13:05:10
