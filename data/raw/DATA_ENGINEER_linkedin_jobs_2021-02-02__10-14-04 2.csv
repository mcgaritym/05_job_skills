job_title,company,location,date_posted,applicants,job_text,seniority_level,employment_type,job_function,industries,date_scraped
Data Engineer,Yum! Brands,"Plano, TX",6 hours ago,Be among the first 25 applicants,"['Develop and maintain global data models for delivery process optimization and fleet telematics, use cases and applications at Pizza Hut using Python, AWS EKS, AWS Cloudwatch and AWS S3 storage.', 'Creating and engineering data pipelines for end-to-end delivery of raw and trained data sets, to and from brand cloud environment using Big data frameworks like Apache Spark, Hadoop, and, Scala, databases like Snowflake. Develop and maintain global data models for delivery process optimization and fleet telematics, use cases and applications at Pizza Hut using Python, AWS EKS, AWS Cloudwatch and AWS S3 storage. Working closely with data scientists to scale, integrate and production data driven machine learning solutions. Dive into the data and perform Exploratory Data Analysis(EDA), statistical modelling and analytics using Python, SQL and Scala to perform Market Basket Analysis of order transactions, gather delivery process metrices and clean and control various master databases. Implementing and managing production support processes around data lifecycle, data quality, coding utilities, storage, reporting and other data integration points. Assisting and working closely with business stakeholders and data/research scientists by delivering data driven solutions to leverage company data to drive business outcomes, hence driving Pizza Hut growth. 1+ years of experience with tools like Kafka and Spark, Cloud Datawarehouse and Hadoop 1+ years hands-on coding skills in languages like Scala, Python, SQL Participate and lead in design sessions, demos and prototype sessions, testing and training workshops with business users and other IT associates', 'Dive into the data and perform Exploratory Data Analysis(EDA), statistical modelling and analytics using Python, SQL and Scala to perform Market Basket Analysis of order transactions, gather delivery process metrices and clean and control various master databases.', 'Creating and engineering data pipelines for end-to-end delivery of raw and trained data sets, to and from brand cloud environment using Big data frameworks like Apache Spark, Hadoop, and, Scala, databases like Snowflake.', 'Key Responsibilities', 'Assisting and working closely with business stakeholders and data/research scientists by delivering data driven solutions to leverage company data to drive business outcomes, hence driving Pizza Hut growth.', '1+ years of experience with tools like Kafka and Spark, Cloud Datawarehouse and Hadoop', 'Working closely with data scientists to scale, integrate and production data driven machine learning solutions.', '1+ years hands-on coding skills in languages like Scala, Python, SQL', 'Implementing and managing production support processes around data lifecycle, data quality, coding utilities, storage, reporting and other data integration points.', 'Participate and lead in design sessions, demos and prototype sessions, testing and training workshops with business users and other IT associates']",Not Applicable,Full-time,Information Technology,Information Technology and Services,2021-02-02 10:10:22
Data Science Engineer,Fearless,"Baltimore, MD",9 hours ago,Be among the first 25 applicants,"['', ""So, what's next?"", 'Understand, monitor QA, translate and collaborate with teams to ensure ongoing data quality.', 'Understands how to ETL new data sources along with mining for insights', 'What you should know:', 'This position has the flexibility to support some remote work / telecommuting.', ""Why we're excited about you:"", 'Effective organizational and time management skills with the ability to work independently, as well as with remote teams, under strict project deadlines', ""Business Interview - At this point, you've made it to the final frontier! The Business Interview is when you'll meet with Fearless leadership to dot the i's, cross the t's, and determine whether or not we'll be moving forward with the hiring process. "", '3 weeks accrued PTO + 1 week sick leave + 10 federal holidays + your birthday off ', 'Experience working with at least one data analysis tool like Hadoop, Apache Spark, or cloud-provider equivalents', 'Support teams in running growth programs and A/B tests through analyzing results and communicating findings and insights.', ""Technical Interview - This is where we get into the nitty gritty of the project. During the Technical Interview, you'll be interviewed by our Passion Coaches and/or the team's Project Lead to make sure your skills align with the project requirements."", 'Strong understanding of Python with the ability to quickly pick up new libraries. Optional: react, cypher', 'Safe Harbor 401(k) plan with employer contributions', '100% coverage of the employee-only premium for HSA, HMO, or PPO plan and Employee Wellness Plan', ' This position is located in our Baltimore Headquarters. This position will be 100% remote until after COVID where it is expected to be in person.  This position has the flexibility to support some remote work / telecommuting. ', 'Strongly Proficient in statistics, data processing, or data annotation', ""What you'll be doing:"", 'Strong analytical and problem-solving skills with attention to detail', 'Support regular ad-hoc data and analysis needs to better understand customer behaviors. ', 'Family-friendly workplace ', '[Free parking in downtown Baltimore / public transit coverage]', 'Partner with various stakeholders and teams of stakeholders to understand business problems, help define them into KPIs and then deliver insightful analysis in reports or visualizations', 'Why Fearless?', 'Flexible schedule', ' Develop custom data models, algorithms, and predictive models to perform multifaceted analysis Strong understanding of Python with the ability to quickly pick up new libraries. Optional: react, cypher Build and maintain predictive models and machine learning algorithms from the ground up to solve real-world business challenges Understands how to ETL new data sources along with mining for insights Familiarity with Machine Learning and Deep Learning Tools Strongly Proficient in statistics, data processing, or data annotation Experience with various types of databases such as relational, key-value, document, graph Experience with OLAP data storage technologies like data lakes, and data warehouses Experience working with at least one data analysis tool like Hadoop, Apache Spark, or cloud-provider equivalents Understand, monitor QA, translate and collaborate with teams to ensure ongoing data quality. Stitch, calibrate, and optimize sparse and noisy data across various data sources Partner with various stakeholders and teams of stakeholders to understand business problems, help define them into KPIs and then deliver insightful analysis in reports or visualizations ', 'About Fearless', 'Compensation:', 'Experience with various types of databases such as relational, key-value, document, graph', ""Cultural Interview - We're a people-first company, so we always start off by getting to know more about you, how you work, what your career goals are, and what you're passionate about. This is your opportunity to ask questions and get a feel for Fearless, so don't be shy!"", "" Cultural Interview - We're a people-first company, so we always start off by getting to know more about you, how you work, what your career goals are, and what you're passionate about. This is your opportunity to ask questions and get a feel for Fearless, so don't be shy! Technical Interview - This is where we get into the nitty gritty of the project. During the Technical Interview, you'll be interviewed by our Passion Coaches and/or the team's Project Lead to make sure your skills align with the project requirements. Business Interview - At this point, you've made it to the final frontier! The Business Interview is when you'll meet with Fearless leadership to dot the i's, cross the t's, and determine whether or not we'll be moving forward with the hiring process.  "", 'Develop custom data models, algorithms, and predictive models to perform multifaceted analysis', ' Flexible schedule Family-friendly workplace  3 weeks accrued PTO + 1 week sick leave + 10 federal holidays + your birthday off  100% coverage of the employee-only premium for HSA, HMO, or PPO plan and Employee Wellness Plan Tech, education / training, and snack allowances  [Free parking in downtown Baltimore / public transit coverage] Safe Harbor 401(k) plan with employer contributions ', 'This position is located in our Baltimore Headquarters. This position will be 100% remote until after COVID where it is expected to be in person. ', ' Strong analytical and problem-solving skills with attention to detail Understand, monitor QA, translate and collaborate with teams to ensure ongoing data quality. View data through the lens of what questions to ask, what assumptions to make, what algorithms to use and how to get the biggest impact from it Support regular ad-hoc data and analysis needs to better understand customer behaviors.  Support teams in running growth programs and A/B tests through analyzing results and communicating findings and insights. Effective organizational and time management skills with the ability to work independently, as well as with remote teams, under strict project deadlines ', 'Experience with OLAP data storage technologies like data lakes, and data warehouses', 'View data through the lens of what questions to ask, what assumptions to make, what algorithms to use and how to get the biggest impact from it', 'Tech, education / training, and snack allowances ', 'Build and maintain predictive models and machine learning algorithms from the ground up to solve real-world business challenges', 'Familiarity with Machine Learning and Deep Learning Tools', 'Stitch, calibrate, and optimize sparse and noisy data across various data sources']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-02-02 10:10:22
Data Engineer,Goldman Sachs,"Dallas, TX",4 hours ago,Be among the first 25 applicants,"['', 'The Team', ' Project Management and organizational skills ', ' GS.com/careers ', ' Strong technical ability, willingness to learn and evolve your skills with advances in technology ', ' Strong sense of ownership and responsibility for the overall product and the success of the business, not just the infrastructure under your direct control. ', ' BS. or higher in Computer Science (or equivalent work experience) ', 'Responsibilities And Qualifications', ' Experience in database performance tuning & optimizing complex SQL queries ', ' Expertise in database and data warehousing concepts (e.g. star schema, entitlement implementations, SQL v/s NOSQL modeling, milestoning, indexing, partitioning) ', ' Experience with analytics databases such as Snowflake ', ' Engineer streaming data processing pipelines ', 'About Goldman Sachs', ' Participate in various technical and architectural discussions both within the team and across the organization.  Manage the full lifecycle of software components, from requirements through design, testing, development, release and demise  Deploy modern data management tools to curate our most important data sets, models and processes, while identifying areas for process automation and further efficiencies  Evaluate, select and acquire new internal & external data sets that contribute to business decision making  Engineer streaming data processing pipelines  Drive adoption of Cloud technology for data processing and warehousing, and providing scalable and performant solutions  Engage with data consumers and producers to design appropriate models to suit all needs  Provide guidance and mentorship to fellow engineers. ', ' Manage the full lifecycle of software components, from requirements through design, testing, development, release and demise ', ' Engage with data consumers and producers to design appropriate models to suit all needs ', ' Proficiency in designing, developing, and testing software in one or more of Python, Java, Groovy, or golang; open to using and learning multiple languages ', ' Excellent communication skills including the ability to liaise with both technical and non-technical teams ', ' Deploy modern data management tools to curate our most important data sets, models and processes, while identifying areas for process automation and further efficiencies ', ' Evaluate, select and acquire new internal & external data sets that contribute to business decision making ', ' BS. or higher in Computer Science (or equivalent work experience)  Minimum 6 years of relevant Big Data experience using modern processing frameworks (Hadoop, Spark, Airflow, Flink) and programming languages (Java/Scala/Python)  Expertise in database and data warehousing concepts (e.g. star schema, entitlement implementations, SQL v/s NOSQL modeling, milestoning, indexing, partitioning)  Strong Problem Solving skills, self-directed and self-motivated  Excellent communication skills including the ability to liaise with both technical and non-technical teams  Experience with data strategy, data governance and standards, data architecture principles, and metadata management best practices  Experience in database performance tuning & optimizing complex SQL queries  Strong technical ability, willingness to learn and evolve your skills with advances in technology  Proficiency in designing, developing, and testing software in one or more of Python, Java, Groovy, or golang; open to using and learning multiple languages  Strong sense of ownership and responsibility for the overall product and the success of the business, not just the infrastructure under your direct control.  Ability to understand and effectively debug both new and existing solutions. ', 'The Role', ' https:// www.goldmansachs.com/careers/footer/disability-statement.html ', ' Experience Amazon AWS, Microsoft 365 and Salesforce ', ' Provide guidance and mentorship to fellow engineers. ', ' Financial Services or Fintech background  Working knowledge of UML or BPM  MongoDB, Cassandra experience  Experience with analytics databases such as Snowflake  Experience Amazon AWS, Microsoft 365 and Salesforce  Project Management and organizational skills  Technical leadership experience of 2+ years ', ' Working knowledge of UML or BPM ', ' MongoDB, Cassandra experience ', 'Basic Qualifications', 'Preferred Qualifications', ' Drive adoption of Cloud technology for data processing and warehousing, and providing scalable and performant solutions ', 'GOLDMAN SACHS ENGINEERING CULTURE', ' Technical leadership experience of 2+ years ', ' Financial Services or Fintech background ', ' Participate in various technical and architectural discussions both within the team and across the organization. ', ' Strong Problem Solving skills, self-directed and self-motivated ', ' Experience with data strategy, data governance and standards, data architecture principles, and metadata management best practices ', ' Minimum 6 years of relevant Big Data experience using modern processing frameworks (Hadoop, Spark, Airflow, Flink) and programming languages (Java/Scala/Python) ', ' Ability to understand and effectively debug both new and existing solutions. ']",Executive,Full-time,Information Technology,Information Technology and Services,2021-02-02 10:10:22
Data Engineer,Charles Schwab,"Westlake, TX",16 hours ago,Be among the first 25 applicants,"['', 'Collaborating directly with business and technology stakeholders to define future-state business capabilities & requirements, and translating those into transitional and target state data architectures', '2+ years of experience using object oriented languages (.Net, Spring Boot and Spring DI frameworks preferred) to deliver data interfaces using Pivotal Cloud Foundry', 'Analyzing the current technology environment to detect critical deficiencies, and recommend solutions for improvement', '2+ years of experience working on agile teams delivering data solutions', '1+ years of experience delivering solutions on public cloud platforms (Google Cloud preferred)', 'What You Are Good At', '4+ years of experience designing, building, and supporting near real-time data pipelines and analytical solutions using Hadoop, Teradata, MS SQL Server, Talend, Informatica, and/or SSIS2+ years of experience working on agile teams delivering data solutions2+ years of experience using object oriented languages (.Net, Spring Boot and Spring DI frameworks preferred) to deliver data interfaces using Pivotal Cloud Foundry1+ years of experience designing data warehouses using the Kimball dimensional modeling techniques1+ years of experience delivering solutions on public cloud platforms (Google Cloud preferred)Basic understanding of at least one IT Management frameworks such as ITIL or COBiTExperience writing automated unit, integration, and acceptance tests for data interfaces & data pipelinesAbility to quickly learn & become proficient with new technologiesExceptional interpersonal skills, including teamwork, communication, and negotiation', 'What You Have', 'Designing, implementing, and maintaining data warehouses and near real-time data pipelines via the practical application of existing and new data engineering techniques', 'Collaborating directly with business and technology stakeholders to define future-state business capabilities & requirements, and translating those into transitional and target state data architecturesAnalyzing the current technology environment to detect critical deficiencies, and recommend solutions for improvementDesigning, implementing, and maintaining data warehouses and near real-time data pipelines via the practical application of existing and new data engineering techniquesDeveloping continuous integration and continuous deployment pipelines for data solutions that include automated unit & integration testingMentoring, motivating, and supporting the team to achieve organizational objectives and goalsAdvocating for agile practices to increase delivery throughputEnsuring consistency with published development, coding and testing standards', 'Ensuring consistency with published development, coding and testing standards', 'Experience writing automated unit, integration, and acceptance tests for data interfaces & data pipelines', 'Options', 'Developing continuous integration and continuous deployment pipelines for data solutions that include automated unit & integration testing', 'Mentoring, motivating, and supporting the team to achieve organizational objectives and goals', 'Exceptional interpersonal skills, including teamwork, communication, and negotiation', 'Why Schwab? ', 'Ability to quickly learn & become proficient with new technologies', '4+ years of experience designing, building, and supporting near real-time data pipelines and analytical solutions using Hadoop, Teradata, MS SQL Server, Talend, Informatica, and/or SSIS', '1+ years of experience designing data warehouses using the Kimball dimensional modeling techniques', 'Advocating for agile practices to increase delivery throughput', 'Basic understanding of at least one IT Management frameworks such as ITIL or COBiT', 'Your Opportunity ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-02-02 10:10:22
Data Engineer II,Premera Blue Cross,"Mountlake Terrace, WA",11 hours ago,Be among the first 25 applicants,"['', 'Collaborate with data scientists and other analysts to further understand business problems', 'Free parking', 'Ability to communicate information and ideas verbally and in writing so others will understand', 'Bachelor’s degree in computer science, computer engineering, or similar area or 4 years applicable experience', 'Knowledge of Tableau, SAS, R, and other analytic tools', 'Perform thorough peer design and code reviews', 'Data Engineer II', 'Life and disability insurance', 'Develop code to complete effective solutions using applicable technology', 'Join Our Team: Do Meaningful Work and Improve People’s Lives ', 'Ability to use Extract Transform Load (ETL) tools (SSIS, Data Stage, Cask)', 'Retirement programs (401K employer match and pension plan)', 'Bachelor’s degree in computer science, computer engineering, or similar area or 4 years applicable experience2 years’ experience in data integration, design, and managementKnowledge of software development lifecycle, relational database theory, and skills to utilize one or more programming languagesFamiliarity with healthcare specific regulatory requirements for data managementExperience providing data integration services within healthcare organizationsKnowledge of Tableau, SAS, R, and other analytic toolsStrong data processing programming skills across SQL-based and Hadoop-based technologiesAbility to communicate information and ideas verbally and in writing so others will understandKnowledge of Agile and Scrum project methodologies utilizing TFS, Jira/ConfluenceAbility to use Extract Transform Load (ETL) tools (SSIS, Data Stage, Cask)Ability to use Kimball methodology for dimensional data modeling, 3rd Normal form DW', 'Troubleshoot issues as they arise and solve problems both independently and collaboratively', 'Ability to use Kimball methodology for dimensional data modeling, 3rd Normal form DW', 'Strong data processing programming skills across SQL-based and Hadoop-based technologies', 'Equal Employment Opportunity/affirmative Action', 'Familiarity with healthcare specific regulatory requirements for data management', 'Use developing data ETL experience to develop data pipelines to support data product automation', 'Generous Paid Time Off to reenergize', 'Knowledge of Agile and Scrum project methodologies utilizing TFS, Jira/Confluence', 'Work closely with the business to create understanding of the needs, pace, and direction for our business partners', 'Translate needs into requirements and specifications, while maintaining contact with the customers throughout project completion', 'Medical, vision and dental coverageLife and disability insuranceRetirement programs (401K employer match and pension plan)Wellness incentives, onsite services, a discount program and moreTuition assistance for undergraduate and graduate degreesGenerous Paid Time Off to reenergizeFree parking', 'Solve business and data science problems using data centric programming and scripting skills to create data models and pipelinesWork closely with the business to create understanding of the needs, pace, and direction for our business partnersTranslate needs into requirements and specifications, while maintaining contact with the customers throughout project completionTroubleshoot issues as they arise and solve problems both independently and collaborativelyCollaborate with data scientists and other analysts to further understand business problemsDevelop code to complete effective solutions using applicable technologyPerform thorough peer design and code reviewsUse developing data ETL experience to develop data pipelines to support data product automation', 'Knowledge of software development lifecycle, relational database theory, and skills to utilize one or more programming languages', 'Experience providing data integration services within healthcare organizations', 'Wellness incentives, onsite services, a discount program and more', 'What You Will Do', '2 years’ experience in data integration, design, and management', 'Medical, vision and dental coverage', 'Tuition assistance for undergraduate and graduate degrees', 'What we offer', 'What You Will Bring', 'Solve business and data science problems using data centric programming and scripting skills to create data models and pipelines']",Not Applicable,Full-time,Information Technology,Insurance,2021-02-02 10:10:22
Data Engineer,Logitech,"San Francisco, CA",22 hours ago,29 applicants,"[' “All qualified applicants will receive consideration for employment without regard to race, sex, age, color, religion, sexual orientation, gender identity, national origin, protected veteran status, or on the basis of disability.”If you require an accommodation to complete any part of the application process, or are limited in the ability or unable to access or use this online application process and need an alternative method for applying, you may contact us at 510-713- 4866 for assistance.', 'Logitech is the sweet spot for people who are passionate about products, making a mark, and having fun doing it. As a company, we’re small and flexible enough for every person to take initiative and make things happen. But we’re big enough in our portfolio, and reach, for those actions to have a global impact. That’s a pretty sweet spot to be in and we’re always striving to keep it that way.', 'Description', 'Fluency with SQL. You are familiar with advanced SQL techniques such as window functions and user-defined functions.', 'Humility & Empathy', 'A self-driven ownership mindset. You are someone who is naturally curious and excels at finding solutions to ambiguous problems. You leave no stone unturned and enjoy getting to a root understanding of every issue.', 'We are Streamlabs, a software-oriented branch of Logitech based in San Francisco and Vancouver. We make the leading set of tools and software allowing live streamers to engage with viewers, monetize their broadcasts, and grow their channels. ', 'Position at Streamlabs', '3+ years of experience working as a Data Engineer, Data Scientist, or Data Ops', 'Expertise in one or more scripting languages such as Python or R', 'Experience working with NoSQL databases such as MongoDB', 'Key Qualifications', 'For consideration, you must bring the following minimum skills and behaviors to our team:', 'If you are passionate about empowering creators, working with driven people, operating with a high amount of autonomy, and seeing the results of your work quickly impact the business - this might be a good fit for you.', 'The Role', '3+ years of experience working as a Data Engineer, Data Scientist, or Data OpsExperience building out a data pipeline / data warehouseFluency with SQL. You are familiar with advanced SQL techniques such as window functions and user-defined functions.Experience working with NoSQL databases such as MongoDBExpertise in one or more scripting languages such as Python or RA self-driven ownership mindset. You are someone who is naturally curious and excels at finding solutions to ambiguous problems. You leave no stone unturned and enjoy getting to a root understanding of every issue.A strong product sense, and a good intuition about creators and what makes them tick.', 'Lay the foundation for data-based decision making at Streamlabs', 'Experience building out a data pipeline / data warehouse', 'A strong product sense, and a good intuition about creators and what makes them tick.', 'Experience building out data infrastructure from scratch', 'Speed', 'Experience leading or managing a data teamExperience working in a fast-growing startup environmentExperience building out data infrastructure from scratch', 'Help leverage our data to build models to understand and predict conversions, engagement, churn, etc.', 'If you require an accommodation to complete any part of the application process, or are limited in the ability or unable to access or use this online application process and need an alternative method for applying, you may contact us at 510-713- 4866 for assistance.', 'Ownership', 'Build a data pipeline to collect, normalize, and warehouse raw data from our many applications and millions of users', 'Work with a team of highly talented individuals to understand and support the data needs of our business', 'Experience leading or managing a data team', 'Present your findings to the company with clear and effective communication', 'We’re looking for a talented Data Engineer to help define what data means at Streamlabs. As our first Data Engineer, you will be instrumental in building out our data infrastructure and strategy, which will play a pivotal role in scaling our business.', 'Your Contribution', 'Build a data pipeline to collect, normalize, and warehouse raw data from our many applications and millions of usersLay the foundation for data-based decision making at StreamlabsWork with a team of highly talented individuals to understand and support the data needs of our businessHelp leverage our data to build models to understand and predict conversions, engagement, churn, etc.Present your findings to the company with clear and effective communicationWear many different hats and contribute to your team with both technical and non-technical skills', 'without regard to race, sex, age, color, religion, sexual orientation, gender identity, national origin, protected veteran status, or on the basis of disability.”', 'Wear many different hats and contribute to your team with both technical and non-technical skills', 'Experience working in a fast-growing startup environment', 'SpeedHumility & EmpathyOwnership', 'At Streamlabs, these values are most important to our culture:', 'In Addition, Preferable Skills And Behaviors Include', 'You will be responsible for working with our product teams to build a data pipeline that collects data from across our many applications and millions of users. You will also be responsible for assisting in analyzing this data and delivering key insights to the team that will drive our product and business decisions.', 'All qualified applicants will receive consideration for employment']",Not Applicable,Full-time,Design,Consumer Electronics,2021-02-02 10:10:22
Data Engineer,Practifi,"Chicago, IL",7 hours ago,Be among the first 25 applicants,"['', 'What you’ll be doing:', 'Casual dress code', 'This is a permanent, full-time position based in Practifi’s Chicago, Illinois office.', 'Work with the Onboarding Manager to provide regular status updates to internal and external stakeholders.\xa0\xa0\xa0', '2+ years experience transforming data in scripting languages (such as Python) or database systems or SQL.', '401k match', 'Practifi’s success depends on a strong commitment to diversity, equity and inclusion. We encourage applicants from all backgrounds. Practifi is an equal opportunity employer. Practifi does not discriminate against any applicant for employment due to age, color, sex, disability, national origin, race, religion, veteran status, or any other protected class.', 'Leverage your existing system and process analysis skills to produce data strategies that create a seamless and uninterrupted experience for our clients.\xa0', 'Desire to work in a consulting environment with active client engagement.', 'Data Engineer', ""3 'Practifi' days per year"", '2+ years previous experience working in a similar data- focused role.', 'Collaborating with clients to successfully map source data to the Practifi data model.\xa0', '\xa0', 'Provide key insights based on data analytics to aid the client in drawing conclusions as to the business value and impact of their data and best approaches for migrating and transforming data to support new system architectures.', 'Reporting to\xa0 the Onboarding Manager and working closely with the Sales, Professional Services and Client Success teams to understand client and data requirements needed to support successful and smooth solution implementations.\xa0', 'We are looking for a Data Engineer to join our Chicago team to support our massive client growth. This role is primarily focused on analysing, transforming and migrating CRM-based data from a diverse range of source systems into the Salesforce.com platform underpinning Practifi.\xa0 This is a client facing role within an internal consulting group so if you’re wanting to extend your reach from pure technical to include client engagement, this is the perfect opportunity.', 'Reporting to\xa0 the Onboarding Manager and working closely with the Sales, Professional Services and Client Success teams to understand client and data requirements needed to support successful and smooth solution implementations.\xa0Leverage your existing system and process analysis skills to produce data strategies that create a seamless and uninterrupted experience for our clients.\xa0Collaborating with clients to successfully map source data to the Practifi data model.\xa0Leverage the Salesforce ecosystem and industry leading tools to extract, transform and load data.\xa0Perform data profiling tasks to collect statistics, trends, impacts, and summaries that lead to a structured and successful implementation.Provide key insights based on data analytics to aid the client in drawing conclusions as to the business value and impact of their data and best approaches for migrating and transforming data to support new system architectures.Work with the Onboarding Manager to provide regular status updates to internal and external stakeholders.\xa0\xa0\xa0', 'Perform data profiling tasks to collect statistics, trends, impacts, and summaries that lead to a structured and successful implementation.', 'Leverage the Salesforce ecosystem and industry leading tools to extract, transform and load data.\xa0', 'Adept in the use of ETL products & technologies.', ""20 days paid vacation per year3 'Practifi' days per year401k matchHealth benefitsCasual dress codeOpportunity to collaborate with the team in Sydney, Australia"", 'Experience working with APEX Data Loader a plus.', 'About you:', 'As a Data Engineer at Practifi, your primary responsibilities will include data strategy, data profiling & mapping, data migration, and continuous client communication.\xa0', 'Previous experience in the Financial Advice/ RIA industry a plus.', 'Opportunity to collaborate with the team in Sydney, Australia', '2+ years previous experience working in a similar data- focused role.2+ years experience transforming data in scripting languages (such as Python) or database systems or SQL.Adept in the use of ETL products & technologies.Some experience working on the Salesforce platform is ideal.Experience working with APEX Data Loader a plus.Previous experience in the Financial Advice/ RIA industry a plus.Desire to work in a consulting environment with active client engagement.', '20 days paid vacation per year', 'Practifi is a fun, and hugely dynamic environment with an awesome culture, incredible benefits and a fast pace. If you’re ready to push hard into the next big phase of your data career talk to us!', 'Health benefits', 'We are a global SaaS WealthTech scale-up with offices in Chicago, USA and Sydney, Australia, powering growing financial advice firms around the world. Practifi is secure, reliable and massively scalable.', 'Benefits:', 'Some experience working on the Salesforce platform is ideal.']",Entry level,Full-time,Information Technology,Financial Services,2021-02-02 10:10:22
Junior Data Engineer,The Topps Company,"Orlando, FL",19 hours ago,Be among the first 25 applicants,"['', ' Building, maintaining and optimizing data pipelines written in Python and Scala using Spark ', ' Code proficiency in at least one of the following languages: Scala, Python ', ' High interest in Big Data processing ', 'Qualifications', ' Assisting the data science and analytics teams to produce key data insights and visualizations. ', ' BS/BA in either Computer Science, Statistics, Mathematics or related field required ', ' Recognize and adopt best practices in reporting and analysis this includes data integrity, testing, maintainability, validation and documentation ', ' 1-4 years of experience in a Data/Software Engineering role ', 'Overview', ' BS/BA in either Computer Science, Statistics, Mathematics or related field required  Masters in Computer Science, Statistics, Mathematics, Data Science or related field a plus ', ' Intellectual curiosity. Passion for learning and exploring ', ' Cataloging all data sources ', ' Building, maintaining and optimizing data pipelines written in Python and Scala using Spark  Working with engineering groups, as well as product and strategy groups to create and gather data requirements  Integrating Data Warehouse with BI tools such as Looker  Recognize and adopt best practices in reporting and analysis this includes data integrity, testing, maintainability, validation and documentation  Be a resource and advocate for data within the division by teaching and empowering others to use data  Cataloging all data sources  Collecting data from various sources such as Postgres DBs, Kinesis Firehoses, Internal and External APIs, etc.  Assisting the data science and analytics teams to produce key data insights and visualizations. ', ' Familiarity with Mongo/Postgres is a plus ', ' Familiarity with Looker (or other similar visualization tools) is highly valued ', ' Team player ', ' 1-4 years of experience in a Data/Software Engineering role  Code proficiency in at least one of the following languages: Scala, Python  High interest in Big Data processing  Experience on AWS is a preferred  Experience with Test Driven Development is a plus  Familiarity with Spark/Databricks is highly valued  Familiarity with Looker (or other similar visualization tools) is highly valued  Familiarity with Mongo/Postgres is a plus  Interest in building machine learning systems  Analytical/mathematical mindset  Good communication skills  Intellectual curiosity. Passion for learning and exploring  Team player ', ' Familiarity with Spark/Databricks is highly valued ', ' Good communication skills ', 'Options', ' Responsibilities include: ', 'Education', ' Integrating Data Warehouse with BI tools such as Looker ', ' Be a resource and advocate for data within the division by teaching and empowering others to use data ', 'Responsibilities', ' Requirements include: ', ' Collecting data from various sources such as Postgres DBs, Kinesis Firehoses, Internal and External APIs, etc. ', ' Working with engineering groups, as well as product and strategy groups to create and gather data requirements ', ' Masters in Computer Science, Statistics, Mathematics, Data Science or related field a plus ', ' Analytical/mathematical mindset ', ' Interest in building machine learning systems ', ' Experience with Test Driven Development is a plus ', ' Experience on AWS is a preferred ']",Associate,Full-time,Information Technology,Sports,2021-02-02 10:10:22
Data Engineer,Airtime,"Los Angeles, CA",10 hours ago,Be among the first 25 applicants,"['', 'Experience with Amazon REDSHIFT, MongoDB, Amazon Aurora, Looker etc.', ""Airtime's Mission:"", 'Experience with REST APIs', 'Employ an array of technological languages and tools to connect data systems together', ""What you'll do:"", ""What you'll bring to the table:"", 'Identify, design, and implement internal process improvements by automating manual processes; optimizing data storage, delivery, and visualization; etc.', ' At least 3 years of experience working in data and analytics Experience with Amazon REDSHIFT, MongoDB, Amazon Aurora, Looker etc. Proficiency using object-oriented scripting languages: Python, PHP, Java, etc. Familiarity with front-end technologies and data visualization: HTML5, JavaScript, d3.js, etc. Experience designing and querying relational SQL databases: Postgres & MySQL Proficient at web server administration (Apache, Node) with expertise with Ubuntu Linux and command line interfaces. Experience with REST APIs Proficient in code versioning tools: Git, etc. In-depth understanding of the software development lifecycle', 'In this role: ', 'Test, debug, and audit custom scripts to ensure data accuracy', 'Experience designing and querying relational SQL databases: Postgres & MySQL', 'Proficient at web server administration (Apache, Node) with expertise with Ubuntu Linux and command line interfaces.', 'Familiarity with front-end technologies and data visualization: HTML5, JavaScript, d3.js, etc.', 'In-depth understanding of the software development lifecycle', 'Proficient in code versioning tools: Git, etc.', 'Build key working relationships with The Data Science Team and The Airtime IT Team to ensure alignment and support of all projects and systems', 'Create custom software components and analytic tools that utilize the data pipeline to provide actionable insights into key performance metrics to assist with growing and optimization of the Airtime application', 'Ensure that all systems meet the business/company requirements as well as industry best practices', ' Create and maintain an optimal data pipeline architecture within the Airtime data infrastructure Identify, design, and implement internal process improvements by automating manual processes; optimizing data storage, delivery, and visualization; etc. Employ an array of technological languages and tools to connect data systems together Build key working relationships with The Data Science Team and The Airtime IT Team to ensure alignment and support of all projects and systems Test, debug, and audit custom scripts to ensure data accuracy Ensure that all systems meet the business/company requirements as well as industry best practices Create custom software components and analytic tools that utilize the data pipeline to provide actionable insights into key performance metrics to assist with growing and optimization of the Airtime application ', 'Create and maintain an optimal data pipeline architecture within the Airtime data infrastructure', 'At least 3 years of experience working in data and analytics', 'Proficiency using object-oriented scripting languages: Python, PHP, Java, etc.']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-02-02 10:10:22
Data Engineer,Pie Insurance,"Denver, CO",23 hours ago,Be among the first 25 applicants,"['', 'Strong experience in ETL/ELT platforms is strongly preferred.', '3-5 years working in data as an engineer. Building data solutions for a company who uses data as a primary part of their business.Experience in data warehouse and/or data analytics. Qualified candidates may also come from a strong database skill-set involved in analytics architecture.Strong experience in writing complex SQL queries.Strong experience in ETL/ELT platforms is strongly preferred.Exposure to one major SQL RDBMS or analytics database. (Snowflake, Redshift, MySQL, Postgres, Oracle, SQL Server, etc.)Big Data and Business Intelligence exposure would help in the success of this role.', 'Experience in data warehouse and/or data analytics. Qualified candidates may also come from a strong database skill-set involved in analytics architecture.', 'Exposure to one major SQL RDBMS or analytics database. (Snowflake, Redshift, MySQL, Postgres, Oracle, SQL Server, etc.)', '3-5 years working in data as an engineer. Building data solutions for a company who uses data as a primary part of their business.', 'Our Achievements', 'Big Data and Business Intelligence exposure would help in the success of this role.', ""How You'll Do It"", 'Pie Insurance is an equal opportunity employer. We do not discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity, marital status, age, disability, national or ethnic origin, military service status, citizenship, or other protected characteristic.', 'Strong experience in writing complex SQL queries.', 'Why Pie? ', 'The Right Stuff']",Entry level,Full-time,Information Technology,Computer Software,2021-02-02 10:10:22
Data Engineer,Podsights,United States,,N/A,"['', 'Ideally, you will have 2+ years of experience as a software or data engineer. Our current stack includes Python, Google Cloud (Dataflow, Airflow, Spanner, BigQuery, etc.), Redis, Go, Node, and potentially whatever you bring to the table.', '$110k - $150k', 'Our mission is simple, we are looking to grow the podcast industry. Far too many brands try a podcast advertising campaign and churn. Or worse: they don’t even try to enter the market. By providing a platform for brands to optimize results, we encourage investment in podcast advertising, and by proxy to publishers.', 'We want curious engineers. You take things apart and see how they work (the real test is if you can put them together too!).', 'Our team consists of members of Embedly (acquired by Medium), Horizon Media, Claritas, NPM, and How Stuff Works. You can learn more about us here:\xa0https://podsights.com/team', 'At Podsights, we look for ambition and curiosity. We love engineers that aren’t solely interested in the craft of developing software. Want to start a company of your own one day? Yes, we want to talk. Are you interested in the business side of software? Tell us more.', 'Remote within the U.S.\xa0--\xa0$110k - $150k', 'If the above interests you, drop us a note. We’re less worried about a resume (LinkedIn is fine), but more importantly, tell us about your ideal next role.', ""Podsights is a small, distributed organization seeking an engineer to join our growing team! Here's a little about us, a little about what we believe, and what we are looking for."", 'The podcasting industry has seen a lot of growth in the past few years, and we believe it has more room to grow. Beyond growth, podcasting presents some interesting problems: its decentralized, RSS-based nature, and the various ways people can listen. Joining Podsights is an excellent opportunity to be hands-on with an evolving medium.', 'Your role will be more of a generalist, but with a focus on the data engineering side or in other words a jack of all data trades. On a given day, you may work on our data ingestion pipeline, the ETL system, or handling a client ask. The benefit of joining a small team is you will have a tremendous impact on the company’s direction, and to some extent, the industry.', 'We believe that where you went to school has little bearing on your performance as a software engineer. Where you live isn’t a proxy for talent. Your current employer provides low signal about your future potential.', 'Podsights is an attribution platform for podcast advertising. We likely work with your favorite publisher and handle over 4 billion events a month.']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-02-02 10:10:22
Data Engineer,recruitAbility,United States,22 hours ago,27 applicants,"['', 'Design and develop data pipelines to extract data from a wide variety of data sources using Azure, Snowflake Cloud, and cloud-native technologies.Build a data model to get actionable insights from data, operational efficiency, and other key business performance metrics.Design and manage inbound and outbound data processes and monitoring. Work with the data provider to bring in new feed into our data eco-system.Enjoy working in Agile as part of a scrum team and deliver high-quality products incrementally in an interactive manner.Write Test Driven Development based code to meet overall data quality standards as defined by the users.Automate the data testing processes and integrate them with monitoring systems.You are a team player who enjoys working with and supporting an Application Engineering team, DBA, Infrastructure, and Project Management Office.Analyze existing systems (including legacy) and data sets to help Business Analysts define the functional and non-functional requirements.Meet with the business users, assist with data-related technical issues, and support their data infrastructure needs', 'Full health, vision, dental. 401(k) plans along with a host of voluntary plans such as car insurance, legal services, and more.', 'Bachelors/Masters in Computer Science, MIS/Information Management, Engineering or related field', 'Preference for background in Financial Services, ideally in the Wealth Management/Independent Broker-Dealer/RIA industry', 'Award-winning, a stable leader in their market space and still growing', 'Primary Requirements for the Data Engineer', 'Excellent salary and comprehensive benefits package for this full-time positionA world-class team of professionals, casual work environment, and rich cultureChallenging projects now and on the Technology Roadmap going out several yearsCareer path, training support, and opportunities for advancement withinAward-winning, a stable leader in their market space and still growingA solid compensation plan includes comprehensive benefits and a bonus planFull health, vision, dental. 401(k) plans along with a host of voluntary plans such as car insurance, legal services, and more.A brand new state of the art building in Southwest Austin with a basketball court, volleyball court, baseball field, walking trails, unlimited coffee, tea, and sparkling water', 'Write Test Driven Development based code to meet overall data quality standards as defined by the users.', 'Background in working with Azure Cloud Services: Data Factory, SQL database, Functions, Data Lake, Databricks, Logic Apps, and Azure Automation.', 'Excellent salary and comprehensive benefits package for this full-time position', 'Experience with other Big Data tools such as Spark, Snowflake, and Kafka', 'Analyze existing systems (including legacy) and data sets to help Business Analysts define the functional and non-functional requirements.', 'A solid compensation plan includes comprehensive benefits and a bonus plan', 'Strong experience with NoSQL database, including PostgresBackground in working with Azure Cloud Services: Data Factory, SQL database, Functions, Data Lake, Databricks, Logic Apps, and Azure Automation.Fluent in object-oriented and functional script language: Python, Scala, and C#.Advanced working knowledge of SQL Server database - writing advanced SQL script, profiling, and optimization.Working knowledge of Business Intelligence tools: Microsoft Integration Services, Reporting Services, and Analysis Services, as well as PowerBI.Experience with other Big Data tools such as Spark, Snowflake, and KafkaPreference for background in Financial Services, ideally in the Wealth Management/Independent Broker-Dealer/RIA industryBachelors/Masters in Computer Science, MIS/Information Management, Engineering or related field', 'Design and develop data pipelines to extract data from a wide variety of data sources using Azure, Snowflake Cloud, and cloud-native technologies.', 'Fluent in object-oriented and functional script language: Python, Scala, and C#.', 'Automate the data testing processes and integrate them with monitoring systems.', 'Challenging projects now and on the Technology Roadmap going out several years', 'What’s in it for you as a Data Engineer with a growing company?', 'Design and manage inbound and outbound data processes and monitoring. Work with the data provider to bring in new feed into our data eco-system.', 'Strong experience with NoSQL database, including Postgres', 'Career path, training support, and opportunities for advancement within', 'A brand new state of the art building in Southwest Austin with a basketball court, volleyball court, baseball field, walking trails, unlimited coffee, tea, and sparkling water', 'Primary Responsibilities of the Data Engineer', 'Meet with the business users, assist with data-related technical issues, and support their data infrastructure needs', 'Our client, headquartered in the “Silicon Hills” of Austin, Texas, offers an experience as unique as the city in which it operates. The firm supports more than 1,700 independent financial advisors in delivering comprehensive securities and investment advisory services to their clients. With a culture rich in reinvention and advisor advocacy, they have developed integrated business management technology that, combined with its personalized consulting services, offers exceptional scale and efficiency', 'Build a data model to get actionable insights from data, operational efficiency, and other key business performance metrics.', 'Advanced working knowledge of SQL Server database - writing advanced SQL script, profiling, and optimization.', 'A world-class team of professionals, casual work environment, and rich culture', 'Working knowledge of Business Intelligence tools: Microsoft Integration Services, Reporting Services, and Analysis Services, as well as PowerBI.', 'Enjoy working in Agile as part of a scrum team and deliver high-quality products incrementally in an interactive manner.', 'You are a team player who enjoys working with and supporting an Application Engineering team, DBA, Infrastructure, and Project Management Office.', 'They are looking for a passionate Data Engineer to join our growing Data and Analytics team. Join this exciting journey in modernizing their legacy solutions to the next-generation cloud platform. You will be responsible for working in a cross-functional team to expand, optimize, and improve overall data quality and set up next-generation data orchestration using modern cloud tools and technologies. As a Cloud Data Engineer, you will be designing and building secure and resilient architectures, with the goal of providing actionable insights to our Advisors for them to optimize their business.']",Mid-Senior level,Full-time,Engineering,Financial Services,2021-02-02 10:10:22
Data Applications Engineer,CarGurus,"Cambridge, MA",10 hours ago,Be among the first 25 applicants,"['', 'Support our efforts to develop and implement a single source of truth for our metrics.', 'Familiarity with Salesforce and Zuora data a big plus, but not required.', 'Who You Are', 'What You’ll Do', 'CarGurus Culture', ' Become a data subject matter expert, providing excellent customer service as an internal consultant to analysts, engineers, and executives. Work with stakeholders across all levels of the organization to translate business analysis requirements into logical data models and transformations using our analytics platform (Snowflake, dbt, Looker). Support our efforts to develop and implement a single source of truth for our metrics. Administer and support Looker and Snowflake. ', ' 2-5 years experience in data modeling, data engineering, business intelligence, or other quantitative field. Team player who thrives in a collaborative environment with strong interpersonal skills. Creative thinker, with an interest in solving business problems with data. Passionate about creating production grade data models and data quality, supporting what you build. Experience in SQL with ability to design and validate complex queries. Familiar with Looker, Tableau, or other business intelligence and data visualization tools. Familiar with Redshift, Snowflake, BigQuery, or other OLAP data warehouses. Familiarity with Salesforce and Zuora data a big plus, but not required. ', 'Creative thinker, with an interest in solving business problems with data.', 'Data Applications Engineer', '2-5 years experience in data modeling, data engineering, business intelligence, or other quantitative field.', 'Become a data subject matter expert, providing excellent customer service as an internal consultant to analysts, engineers, and executives.', 'Work with stakeholders across all levels of the organization to translate business analysis requirements into logical data models and transformations using our analytics platform (Snowflake, dbt, Looker).', 'Administer and support Looker and Snowflake.', 'Team player who thrives in a collaborative environment with strong interpersonal skills.', 'Experience in SQL with ability to design and validate complex queries.', 'Passionate about creating production grade data models and data quality, supporting what you build.', 'Familiar with Looker, Tableau, or other business intelligence and data visualization tools.', 'Familiar with Redshift, Snowflake, BigQuery, or other OLAP data warehouses.']",Mid-Senior level,Full-time,Engineering,Information Technology and Services,2021-02-02 10:10:22
BI / Data Engineer,Smoothstack,"Washington, DC",9 hours ago,Be among the first 25 applicants,"['', 'Passion for Information technology and desire to gain in-depth knowledge', 'ETL & BI Tools (Informatica / Abinitio / Tableau / Dremio, etc)', 'Machine Learning overview', 'Spark Programming using Python', 'Hands-on experience or bachelor’s degree or minor in any of the following: Computer Science, Engineering, Statistics, Mathematics or Economics or equivalent training or course work experience', 'Basic knowledge of Java, Python or any programming language', 'Excitement and eagerness to learn new technology', '401 K', ' Basic Qualifications ', ' Pay Package - $50,000 K + plusRelocation assistance providedHealth/Medical Benefits401 KIndustry CertificationsMentoring ProgramOpportunity to work with one of our many large-scale government entities', 'Relocation assistance provided', 'SQL - in depth / NoSQL DB Overview', 'Data Warehousing ConceptsSQL - in depth / NoSQL DB OverviewETL & BI Tools (Informatica / Abinitio / Tableau / Dremio, etc)Big data conceptsMachine Learning overviewCloud Essentials – AWSServerless Data LakesProgramming Language - PythonAWS Data Analytics Services (EMR / SQS / SNS /Lambda / S3 / Redshift / Redshift Spectrum / Kinesis / DMS / DynamoDB / Glue/ Sage Maker / RDS/ Quicksight, etc)Spark Programming using PythonAny prior exposure or experience with the above mentioned skills is preferred. ', 'Any prior exposure or experience with the above mentioned skills is preferred. ', 'AWS Data Analytics Services (EMR / SQS / SNS /Lambda / S3 / Redshift / Redshift Spectrum / Kinesis / DMS / DynamoDB / Glue/ Sage Maker / RDS/ Quicksight, etc)', 'Opportunity to work with one of our many large-scale government entities', 'Mentoring Program', ' Pay Package - $50,000 K + plus', 'Serverless Data Lakes', 'Health/Medical Benefits', 'Big data concepts', 'We have a continued investment in motivated, passionate individuals like you who strive to move forward in their careers through apprenticeship programs like Smoothstack. Please be aware we have modified our training so that we can continue to offer our apprenticeship program remotely until the CDC deems it appropriate to work in shared office spaces. All future programs are continuing as scheduled.', 'Interest in Data engineering, Data analytics, Business Intelligence, Data Science', 'Data Warehousing Concepts', 'Excitement and eagerness to learn new technologyPassion for Information technology and desire to gain in-depth knowledgeBasic knowledge of Java, Python or any programming languageHands-on experience or bachelor’s degree or minor in any of the following: Computer Science, Engineering, Statistics, Mathematics or Economics or equivalent training or course work experienceInterest in Data engineering, Data analytics, Business Intelligence, Data ScienceUS Citizenship or Green Card is required to apply', 'Cloud Essentials – AWS', 'Industry Certifications', 'Programming Language - Python', 'Ability & Interest to learn the below skills set ', 'US Citizenship or Green Card is required to apply']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-02-02 10:10:22
Data Engineer,First Republic Bank,"San Francisco, CA",11 hours ago,Be among the first 25 applicants,"['This job description is not intended to be all-inclusive. Employee may perform other related duties as assigned to meet the ongoing needs of the organization. The Company is an equal opportunity employer. In this regard, the Company makes reasonable accommodations for qualified applicants and employees with disabilities in order to enable them to perform all essential job functions, unless doing so creates an undue hardship.', 'As a Data Engineer in Regulatory and Corporate technology you will be responsible for designing data model working with data architects. Involved in\xa0\xa0 data pipeline\xa0\xa0 development leveraging various methodology. Help modernize the current technology stack to be more cloud native with higher focus around data quality and security.', 'Background in data science, analytics, or data mining.', 'Perform detailed analysis to troubleshoot and resolve identified issues and maintain data integrity', 'In Technology, we support First Republic’s employees and clients through the acquisition, integration and management of the Bank’s information technology systems and services. We drive innovation and explore emerging technologies so our people can be productive and focus on what matters most - providing extraordinary service. ', 'Experience in DevSecOps and automation using CICD tools and process', 'Own your work and your career - apply now', 'Bachelors or Master degree in information technology, computer science or data scienceStrong skills in python and knowledge of various frameworks like pandas, pyspark.Experience in building cloud native data lakes, pipelines and stream processingExperience with cloud services preferably AWS and\xa0 Snowflake.Background in data science, analytics, or data mining.Familiar with Data Virtualization concepts ideally with Denodo/Composite experience.Familiar with Data Virtualization concepts ideally with Denodo/Composite experience.Experience in DevSecOps and automation using CICD tools and processProven history of learning and implementing new technology in fast moving environment.Hands-On experience with \xa0data pipeline design and development.Experience with both SQL and NoSQL as well as their relevant data modeling patternsDemonstrated experience working in large-scale data environments which included real-time and batch processing requirements.Familiar with Data Virtualization concepts ideally with Denodo/Composite experience.', 'Experience with both SQL and NoSQL as well as their relevant data modeling patterns', 'Hands-On experience with \xa0data pipeline design and development.', 'Must be able to review and analyze data reports and manuals; must be computer proficient.Must be able to communicate effectively via telephone and in person.', 'Familiar with Data Virtualization concepts ideally with Denodo/Composite experience.', 'Apply data science skills to model data for quality verification', '\xa0', 'Incredible teams doing exceptional work, every day', 'Data Engineer:', 'Experience in building cloud native data lakes, pipelines and stream processing', 'First Republic is subject to federal laws that restrict the employment of individuals with certain types of criminal histories, including FDIA Section 19 and FINRA. To the extent not inconsistent with our obligations under those federal laws and regulations, First Republic will consider qualified candidates with criminal histories in a manner consistent with the Los Angeles and San Francisco ban-the-box laws.', 'Design, develop and maintain various data model for regulatory and corporate domainDevelop Pipelines, ensuring the best practice are implemented for data governance, data quality, data lineage and data cleansing.Apply data science skills to model data for quality verificationPerform detailed analysis to troubleshoot and resolve identified issues and maintain data integrityResponsible for driving and managing data source integration between various vendor systems. ', 'Must be able to review and analyze data reports and manuals; must be computer proficient.', 'Strong skills in python and knowledge of various frameworks like pandas, pyspark.', 'At First Republic, we care about our people. Founded in 1985, we offer extraordinary client service in private banking, private business banking and private wealth management. We believe that personal connections are everything and our success is driven by the relationships we form with our colleagues and clients. You’ll always feel empowered and valued here.\xa0 ', 'Bachelors or Master degree in information technology, computer science or data science', 'Responsible for driving and managing data source integration between various vendor systems. ', 'Proven history of learning and implementing new technology in fast moving environment.', 'Must be able to communicate effectively via telephone and in person.', 'Experience with cloud services preferably AWS and\xa0 Snowflake.', 'What you’ll do as a Data Engineer:', 'Demonstrated experience working in large-scale data environments which included real-time and batch processing requirements.', 'You could be a great fit if you have:', 'Job demands:', 'Are you willing to take initiative and make decisions? Are you willing to go the extra mile because you love what you do and how you can contribute as a team? Do you want the freedom to grow and the opportunity to take charge of your own career? If so, then come join us.', 'Design, develop and maintain various data model for regulatory and corporate domain', 'We want hard working team players. You’ll have the independence to learn, lead and drive change. A culture of extraordinary service, empowerment and stability&mdash;that’s the First Republic way.', 'Develop Pipelines, ensuring the best practice are implemented for data governance, data quality, data lineage and data cleansing.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-02-02 10:10:22
Data Engineer,Guitar Center,"Westlake Village, CA",20 hours ago,Be among the first 25 applicants,"['', ' Lead or participate in business meetings to capture requirements, provide status updates and document the requirements', 'gig', ' Provide support and enhancement to our existing EDW environment, working with the business and IT leads to interpret new project requirement into an agile software development process.', ' Subject matter expert for data warehouse team who can drive data platform/platform activities and manage large scale data management projects. ', ' BI technologies such as Domo, Looker, and Tableau', ' Cloud Data Warehouse platforms preferably Snowflake or Redshift', ' Deliver high quality data engineering components/services that are agile, robust, scalable, and reusable.', ' Expertise with at some programming languages -- Python, Scala or similar languages', ' Hackathon mindset and willing to take new challenges while coaching/delegating junior engineers.', "" Bachelor's Degree (or 4 years of equivalent work experience) preferably in Computer Science, MIS or similar."", ' Collaborate and communicate effectively with peers at onsite, offshore and other team members to deliver strong and on-time data products. ', 'this', ' Experience with AWS Analytics and Data Pipeline Services ', ""  Bachelor's Degree (or 4 years of equivalent work experience) preferably in Computer Science, MIS or similar.  Minimum of 6 years of experience in building Data platform - Design and Develop data model, build data pipelines to integrate data from many sources, performance tuning and support all parts of the data platform  3 year of work experience in the following areas:  Cloud Data Warehouse platforms preferably Snowflake or Redshift  BI technologies such as Domo, Looker, and Tableau  AWS cloud, Azure, or Google cloud architecture and technology   Experience with AWS Analytics and Data Pipeline Services   Must have expert level SQL programming knowledge and experience.   Expertise with at some programming languages -- Python, Scala or similar languages  Experience in Web Services, API integration, Data exchanges with third parties is preferred   Experience in Elastic Stack, Kibana is a plus  Self-starter who can work independently and own and derive. "", ' Ability to translate requirements for BI and Reporting to Database design and reporting design', ' Derive and own data platform assessment, selection and implementation process.', 'Responsibilities will include, but will not be limited to the following:', ' Capable to understand business challenges and single handedly design executive level dashboards/visualizations that can provide meaningful insights for business and technology leaders.', ' Ability to develop and understand data pipelines and modern ways of automating data pipeline using cloud based and on premise technologies', ' Experience in Elastic Stack, Kibana is a plus', '  Subject matter expert for data warehouse team who can drive data platform/platform activities and manage large scale data management projects.   Derive and own data platform assessment, selection and implementation process.  Capable to understand business challenges and single handedly design executive level dashboards/visualizations that can provide meaningful insights for business and technology leaders.  Deliver high quality data engineering components/services that are agile, robust, scalable, and reusable.  Hackathon mindset and willing to take new challenges while coaching/delegating junior engineers.  Past experience and capability to migrate data platforms to cloud (AWS, Assure, Oracle preferred)  Ability to develop and understand data pipelines and modern ways of automating data pipeline using cloud based and on premise technologies  Ability to translate requirements for BI and Reporting to Database design and reporting design  Provide support and enhancement to our existing EDW environment, working with the business and IT leads to interpret new project requirement into an agile software development process.  Collaborate and communicate effectively with peers at onsite, offshore and other team members to deliver strong and on-time data products.   Lead or participate in business meetings to capture requirements, provide status updates and document the requirements  Additional duties as assigned. ', '  Cloud Data Warehouse platforms preferably Snowflake or Redshift  BI technologies such as Domo, Looker, and Tableau  AWS cloud, Azure, or Google cloud architecture and technology ', 'us?', 'and', 'apply?', ' Self-starter who can work independently and own and derive.', 'Love', 'join', 'to', ' AWS cloud, Azure, or Google cloud architecture and technology', ""To Join Our Band, You'll Need The Following Experience"", 'About Guitar Center', 'want', ' 3 year of work experience in the following areas:  Cloud Data Warehouse platforms preferably Snowflake or Redshift  BI technologies such as Domo, Looker, and Tableau  AWS cloud, Azure, or Google cloud architecture and technology ', ' Past experience and capability to migrate data platforms to cloud (AWS, Assure, Oracle preferred)', ' Must have expert level SQL programming knowledge and experience. ', ' Minimum of 6 years of experience in building Data platform - Design and Develop data model, build data pipelines to integrate data from many sources, performance tuning and support all parts of the data platform', ' Additional duties as assigned.', ' Experience in Web Services, API integration, Data exchanges with third parties is preferred ', 'Why']",Not Applicable,Full-time,Information Technology,Retail,2021-02-02 10:10:22
Data Engineer,Gravie,"Minneapolis, MN",17 hours ago,Be among the first 25 applicants,"['', ' Experience building and optimizing data pipelines, architectures and data sets.', ' Bachelor’s Degree, an undergraduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field 5+ years of experience in a Data Engineer role. Advanced working SQL knowledge and experience working with relational databases, query authoring as well as working familiarity with a variety of databases. Experience building and optimizing data pipelines, architectures and data sets. Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. Strong analytic skills related to working with unstructured datasets. Build processes supporting data transformation, data structures, metadata, dependency and workload management. A successful history of manipulating, processing and extracting value from large disconnected datasets. Working knowledge of message queuing, stream processing, and highly scalable data stores. Strong project management and organizational skills. Experience supporting and working with cross-functional teams in a dynamic environment.', ' Assemble large, complex data sets that meet functional / non-functional business requirements.', 'You Will', ' Bachelor’s Degree, an undergraduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field', ' Strong analytic skills related to working with unstructured datasets.', ' Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.', ' Working knowledge of message queuing, stream processing, and highly scalable data stores.', ' Create and maintain optimal data pipeline architecture', ' Implements processes and systems to monitor data quality, ensuring production data is accurate and available for key stakeholders and business processes that depend on it.', ' Strong project management and organizational skills.', ' A successful history of manipulating, processing and extracting value from large disconnected datasets.', 'Data Engineer ', ' Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.', ' Experience supporting and working with cross-functional teams in a dynamic environment.', ' Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.', ' Create and maintain optimal data pipeline architecture Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. Assemble large, complex data sets that meet functional / non-functional business requirements. Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS technologies. Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics. Implements processes and systems to monitor data quality, ensuring production data is accurate and available for key stakeholders and business processes that depend on it. Work with stakeholders including the Executive, Product, and Data Scienceteams to assist with data-related technical issues and support their data infrastructure needs. Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader. Work with data and analytics experts to strive for greater functionality in our data systems. Demonstrate the Gravie competencies of authenticity, creativity, curiosity and outcome orientation.', ' Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS technologies.', ' Work with stakeholders including the Executive, Product, and Data Scienceteams to assist with data-related technical issues and support their data infrastructure needs.', ' Build processes supporting data transformation, data structures, metadata, dependency and workload management.', ' Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.', ' Work with data and analytics experts to strive for greater functionality in our data systems.', ' 5+ years of experience in a Data Engineer role.', 'You Bring', ' Demonstrate the Gravie competencies of authenticity, creativity, curiosity and outcome orientation.', ' Advanced working SQL knowledge and experience working with relational databases, query authoring as well as working familiarity with a variety of databases.']",Not Applicable,Full-time,Information Technology,Computer Software,2021-02-02 10:10:22
Data Engineer,Farm Credit Services of America,"Omaha, NE",11 hours ago,Be among the first 25 applicants,"['', 'Recommend data life-cycle management approaches and strategies and implement recommendations as appropriate. Create, monitor, and manage metadata to create visibility of data origin, lineage and downstream data usage. ', '5+ years Data Engineering, Software Development or Infrastructure Design ', 'Project leadership involving cross-functional teams and outside consultants or vendors experience. ', 'Implement data quality and privacy standards, create and maintain data quality & privacy monitoring processes, and publish data quality and privacy metrics. ', 'Qualifications', '5+ years Data Engineering, Software Development or Infrastructure Design Project leadership involving cross-functional teams and outside consultants or vendors experience. Experience with Data Lake ecosystems, Metadata Management, Data Modeling, Data Cataloging, Data Quality Workflow and Remediation tools, and Master Data Management is preferred. Bachelor’s degree in Computer Science, Management Information Systems, Data Governance, Data Management or comparable experience in a related field required.', 'Identify differences between logical and physical data models and between disparate physical models. ', 'Position Description', 'Experience with Data Lake ecosystems, Metadata Management, Data Modeling, Data Cataloging, Data Quality Workflow and Remediation tools, and Master Data Management is preferred. ', 'Responsibilities', 'Support the end-to-end metadata process for creating, enhancing, and managing metadata schemas, models and structured data within a technology repository system. Ensure metadata is complete and current by regularly monitoring the schemas. ', 'Help lead training activities such as classes and workshops on Data Management tools and processes. ', 'Data Engineer', 'Provide production support for the Data Management team services and tools. ', 'Serve as a resource for data management implementations on other technology teams and collaborate with data domain owners, business owners, and leaders. ', 'Bachelor’s degree in Computer Science, Management Information Systems, Data Governance, Data Management or comparable experience in a related field required.', 'Identify differences between logical and physical data models and between disparate physical models. Support the end-to-end metadata process for creating, enhancing, and managing metadata schemas, models and structured data within a technology repository system. Ensure metadata is complete and current by regularly monitoring the schemas. Recommend data life-cycle management approaches and strategies and implement recommendations as appropriate. Create, monitor, and manage metadata to create visibility of data origin, lineage and downstream data usage. Implement data quality and privacy standards, create and maintain data quality & privacy monitoring processes, and publish data quality and privacy metrics. Serve as a resource for data management implementations on other technology teams and collaborate with data domain owners, business owners, and leaders. Provide production support for the Data Management team services and tools. Help lead training activities such as classes and workshops on Data Management tools and processes. ']",Entry level,Full-time,Information Technology,Banking,2021-02-02 10:10:22
Data Engineer,North Highland,"Portland, OR",2 hours ago,Be among the first 25 applicants,"['', ' We started as three leaders and a kitchen table. ', 'What You Will Need', ' MAKE CHANGE HAPPEN. ', '  Looking to build foundation so feeling comfortable ingesting data from different master sources,   AWS – S3, Snowflake, Airflow, PySpark, Python, Redshift   Knowledge of industry standards on data pipelines   Experience ingesting data in the past is preferred   Engineering skillset and understanding on how to break down the strategy and solutions with the product owner to design an implementation.   An Agile background is important.   The ability to collaborate and work with team members and business partners is important.   Demonstrated success in a matrix environment   5+ years of data engineering experience  ', ' Participates in quality assurance and develops test application code in client server environment. ', ' Experience ingesting data in the past is preferred ', ' Engineering skillset and understanding on how to break down the strategy and solutions with the product owner to design an implementation. ', ' Adapts business requirements, developed by modeling/development staff and systems engineers, and develops the data, database specifications, and table and element attributes for an application. ', ' Assists in the design of user interface and business application prototypes. ', ' Looking to build foundation so feeling comfortable ingesting data from different master sources, ', ' Evaluates and installs database management systems. ', 'LEAVE YOUR MARK ON A BETTER WORLD. ', "" At more experienced levels, helps to develop an understanding of client's original data and storage mechanisms. "", ' Determines how tables relate to each other and how fields interact within the tables for a relational model. ', ' Knowledge of industry standards on data pipelines ', ' 5+ years of data engineering experience ', ' to apply ', ' Provides expertise in devising, negotiating and defending the tables and fields provided in the database. ', ' Codes complex programs and derives logical processes on technical platforms. ', ' Determines appropriateness of data for storage and optimum storage organization. ', ' The ability to collaborate and work with team members and business partners is important. ', ' Exciting work you will do:… ', ' An Agile background is important. ', ' HERE ', ""  Establishes database management systems, standards, guidelines and quality assurance for database deliverables, such as conceptual design, logical database, capacity planning, external data interface specification, data loading plan, data maintenance plan and security policy.   Documents and communicates database design.   Evaluates and installs database management systems.   Codes complex programs and derives logical processes on technical platforms.   Builds windows, screens and reports.   Assists in the design of user interface and business application prototypes.   Participates in quality assurance and develops test application code in client server environment.   Provides expertise in devising, negotiating and defending the tables and fields provided in the database.   Adapts business requirements, developed by modeling/development staff and systems engineers, and develops the data, database specifications, and table and element attributes for an application.   At more experienced levels, helps to develop an understanding of client's original data and storage mechanisms.   Determines appropriateness of data for storage and optimum storage organization.   Determines how tables relate to each other and how fields interact within the tables for a relational model.  "", ' …Click ', ' Demonstrated success in a matrix environment ', ' Establishes database management systems, standards, guidelines and quality assurance for database deliverables, such as conceptual design, logical database, capacity planning, external data interface specification, data loading plan, data maintenance plan and security policy. ', ' COLLABORATE WITH AMAZING PEOPLE. ', ' Why North Highland? ', ' AWS – S3, Snowflake, Airflow, PySpark, Python, Redshift ', ' Builds windows, screens and reports. ', ' At North Highland, you’re never a number. ', ' Documents and communicates database design. ']",Entry level,Contract,Information Technology,Information Technology and Services,2021-02-02 10:10:22
Data Engineer,Base-2 Solutions,"Washington, DC",1 hour ago,Be among the first 25 applicants,[],Entry level,Full-time,Information Technology,Information Technology and Services,2021-02-02 10:10:22
Data Engineer,Sketchy,United States,22 hours ago,Be among the first 25 applicants,"['', 'Generous PTO package with floating holidays', 'We are looking for a Data Engineer who is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross-functional teams. You will play an integral role in helping us become more data-aware as a company and enabling data insights across our teams.', 'Fun team events (Monthly and virtual for now)', 'Able to get into the weeds and propose and implement solutions without hand holding', 'Great Benefits including 99 % Coverage of Medical, dental, vision. Up to 4% match on 401k and more!', 'What you bring to the role:', 'SketchyGroup LLC is an Equal Opportunity Employer. SketchyGroup LLC encourages women and minorities to apply and does not and will not discriminate on the basis of race, religion, color, sex, age, sexual orientation, marital status, national origin, disability or any other basis prohibited by applicable law.', 'Experience with relational SQL and NoSQL databases', '\ufeff', 'The Job: Data Engineer', 'Create and maintain optimal data pipeline architecture', 'Competitive compensation plan', 'Experience with data pipeline and workflow management tools', 'Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.', 'Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources', 'Competitive compensation planInnovative, high growth and collaborative culture.Generous PTO package with floating holidaysFun team events (Monthly and virtual for now)Great Benefits including 99 % Coverage of Medical, dental, vision. Up to 4% match on 401k and more!', 'Keep our data separated and secure across national boundaries through multiple data centers', 'Sketchy is a TCG (The Chernin Group) portfolio company (joining other companies such as Headspace, Surfline Food52 and Crunchyroll) and a Reach Capital portfolio company (joining other start-up companies that bring a playfulness to learning)', 'Experience with object-oriented/object function scripting languages', 'About Sketchy:', 'Create and maintain optimal data pipeline architectureIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery re-designing infrastructure for greater scalability and usabilityBuild analytics tools that utilize the data pipeline to provide actionable insightsBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sourcesKeep our data separated and secure across national boundaries through multiple data centers', 'Sketchy is an online visual learning platform that helps students effortlessly learn and recall information through a blend of art, story, spaced repetition and memory palace techniques. Sketchy was born when four medical students began creating sketched stories to distinguish and memorize similarly named viruses, as they realized that the same learning methodologies can be used across a variety of subjects.', '3+ years experience in Data Engineering', 'Must have experience in building reports, dashboards, and/or data modeling layers in Looker, Tableau or Mode', 'Build analytics tools that utilize the data pipeline to provide actionable insights', 'Self-starter who is excited to be part of a growing startup company', 'Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery re-designing infrastructure for greater scalability and usability', 'What you will do:', 'Must have at least an undergraduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field', 'Strong analytic skills related to working with unstructured datasets', 'Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores', 'Location:\xa0Remote |California | Hawaii | Illinois | New York | Colorado | Massachusetts | Washington', 'Experience with GCP services', '3+ years experience in Data EngineeringMust have at least an undergraduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative fieldAdvanced working SQL knowledge and experience working with relational databases, query authoring (SQL)Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data storesExperience building and optimizing ‘big data’ data pipelines, architectures and data sets.Strong analytic skills related to working with unstructured datasetsMust have experience in building reports, dashboards, and/or data modeling layers in Looker, Tableau or ModeExperience with relational SQL and NoSQL databasesExperience with data pipeline and workflow management toolsExperience with GCP servicesExperience with object-oriented/object function scripting languagesSelf-starter who is excited to be part of a growing startup companyAble to get into the weeds and propose and implement solutions without hand holding', 'Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL)', 'Innovative, high growth and collaborative culture.', 'Location:\xa0', 'What We Offer', 'Since its inception in 2013, Sketchy has become the premiere learning destination for Medical School students around the world, currently serving over 30,000 active users (or a third of the total 89,000 medical students in the United States) and an alumni base of 100,000+ students. Sketchy is creating the most engaging and effective educational service for students of higher education everywhere by combining visual storytelling with interactive learning tools that together dramatically enhance recall and knowledge acquisition.']",Entry level,Full-time,Information Technology,Higher Education,2021-02-02 10:10:22
Data Engineer,Infutor Data Solutions,"Oakbrook Terrace, IL",6 hours ago,Be among the first 25 applicants,"['Position Summary: In this role you will be responsible for expanding and optimizing our data and data pipeline architecture. The Data Engineer will support our developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects.', 'Create data tools for data team members that assist them in building and optimizing our product into an innovative industry leader.', 'Peer Recognition Awards', 'Ownership: Performance Unit Grants401k with Company MatchMedical & Prescription Drug, Dental, and Vision Insurance (monthly premiums 100% paid for employee only coverage!)Provided at No Cost to Employees: Group Term Life Insurance, Long-Term Disability, and Accidental Death & Personal Loss InsuranceFlexible Spending Accounts for Health Care, Dependent Care, and Commuter BenefitsIdentity Protection InsuranceVoluntary Term Life Insurance & Group Universal Life InsuranceAccident & Critical Illness InsuranceEmployee Referral Bonus ProgramPaid Time Off for Vacation, Illness, & Maternity/Paternity LeavePeer Recognition AwardsCorporate-sponsored Activities & Events Year Round', ""Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS big data' technologies."", 'Git and GitLab or GitHub, (branching, merging, etc.)', 'Experience working under agile/scrum and the major pieces of that framework (epics, stories, sprint planning and retros),', ""Crain's Best Places to Work CompanyCrain's Best-Paying CompanyBuiltIn Best Places to WorkInc. 5000 Fastest Growing Companies in the USAdWeek Top 20 Fastest Growing Solution ProvidersMartech Breakthrough Award Winner for Best Contact Database Solution"", 'Employee Referral Bonus Program', 'Qualifications: ', 'Assemble large, complex data sets that meet functional / non-functional business requirements.', 'Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.', 'Work with data and analytics experts to strive for greater functionality in our data systems.', 'Interacting with other parts of the business to evangelize and implement platform(s) and product(s) emanating from the team and also to collaborate with other technical teams', 'Company Overview ', 'Paid Time Off for Vacation, Illness, & Maternity/Paternity Leave', 'Intermediate/Advanced knowledge of Python and the environments it executes in.', 'Benefits: ', 'Experience configuring and operating Jenkins', 'Responsibilities:', 'Voluntary Term Life Insurance & Group Universal Life Insurance', ""Interacting with other parts of the business to evangelize and implement platform(s) and product(s) emanating from the team and also to collaborate with other technical teamsWorking with architect to maintain code base for ETL pipelines and large batch processing and streaming systems (Git/GitLab for source control, Unit/Integration Testing, code reviews)Comfortable writing stories and associated acceptance criteria for agile/scrum workflowAssemble large, complex data sets that meet functional / non-functional business requirements.Create data tools for data team members that assist them in building and optimizing our product into an innovative industry leader.Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS big data' technologies.Keep our data separated and secure across multiple data centers and AWS regions.Work with data and analytics experts to strive for greater functionality in our data systems.Experience with and knowledge of large batch data processing"", 'Understanding of relational database modeling (MySQL, SQL Server) and experience using SQL', 'Data Engineer ', ""Infutor Data Solutions is the expert in Consumer Identity Management and Identity Resolution, offering award-winning solutions to a variety of brands and martech firms. These world-class data solutions enable businesses to most accurately identify key consumer information, powering them to make critical, strategic business decisions. We do this through access to the most comprehensive and accurate data, provided through powerful and flexible technology, giving businesses extensive insight into the core of their business. Infutor's data solutions drive essential business functions in identity verification, fraud and risk, marketing, analytics, multi-media, and compliance applications. "", 'Corporate-sponsored Activities & Events Year Round', 'Provided at No Cost to Employees: Group Term Life Insurance, Long-Term Disability, and Accidental Death & Personal Loss Insurance', 'Martech Breakthrough Award Winner for Best Contact Database Solution', 'Medical & Prescription Drug, Dental, and Vision Insurance (monthly premiums 100% paid for employee only coverage!)', 'Accident & Critical Illness Insurance', ' ', '401k with Company Match', 'Experience with and knowledge of large batch data processing', 'Identity Protection Insurance', 'Our company of rapidly-growing data, technology and analytics leaders is looking for a Data Engineer to join our team that has recently been recognized as: ', 'BuiltIn Best Places to Work', ""1-2 years experience developing and deploying solutions on AWS and familiarity with AWS' ecosystem of services"", 'Flexible Spending Accounts for Health Care, Dependent Care, and Commuter Benefits', 'PM19', 'BASH scripting, basics of LINUX operating system, including some light administration.', 'Keep our data separated and secure across multiple data centers and AWS regions.', ""Crain's Best Places to Work Company"", 'Comfortable writing stories and associated acceptance criteria for agile/scrum workflow', 'Ownership: Performance Unit Grants', 'Working with architect to maintain code base for ETL pipelines and large batch processing and streaming systems (Git/GitLab for source control, Unit/Integration Testing, code reviews)', ""Experience working under agile/scrum and the major pieces of that framework (epics, stories, sprint planning and retros),Intermediate/Advanced knowledge of Python and the environments it executes in.Understanding of relational database modeling (MySQL, SQL Server) and experience using SQLBASH scripting, basics of LINUX operating system, including some light administration.Git and GitLab or GitHub, (branching, merging, etc.)Experience configuring and operating Jenkins1-2 years experience developing and deploying solutions on AWS and familiarity with AWS' ecosystem of services"", ""Crain's Best-Paying Company"", 'AdWeek Top 20 Fastest Growing Solution Providers', 'Inc. 5000 Fastest Growing Companies in the US']",Entry level,Full-time,Information Technology,Computer Software,2021-02-02 10:10:22
Data Engineer,Integrity Marketing Group LLC,"Farmington, UT",6 hours ago,Be among the first 25 applicants,"['Strong understanding of API development and functionality.', 'Here at TA we welcome and embrace diversity. All qualified applicants will receive consideration for employment without regard to race, sex, color, religion, national origin, protected veteran status, and on the basis of disability.', 'Work with data and analytics experts to strive for greater functionality in our data systems', 'Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.', 'Other duties assigned ', 'Experience with AWS cloud services: EC2, RDS.', 'Interface with Clients in order to learn and understand business requirements.  ', 'Overview', 'Data Engineer', 'Strong analytic skills related to working with unstructured datasets.', 'Experience with using Python in an object-oriented/object function way.', 'Experience with REST API calls and best practices.', 'Build processes supporting data transformation, data structures, metadata, dependency and workload management.', 'Working MySQL knowledge and experience working with relational databases, query authoring as well as working familiarity with a variety of databases.Working Python language knowledge and experience with core programming basics including:Objects, Definitions, Variables Loops and if-statements.Strong understanding of API development and functionality.Minimum of 2 years of experience coding REST services and APIs using PythonExperience building new database tables and stored procedures with MySQL or similar database architecture.Ability to document architecture, structure and implementation details for your projects.Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations.Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.Strong analytic skills related to working with unstructured datasets.Build processes supporting data transformation, data structures, metadata, dependency and workload management.A successful history of manipulating, processing and extracting value from large disconnected datasets.Experience supporting and working with cross-functional teams in a dynamic environment.We are looking for a candidate with 3+ years of experience in a data or analytics role, who has attained a Bachelor’s degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using some combination of the following software/tools:Experience with relational MySQL databases.Experience with using Python in an object-oriented/object function way.Experience with AWS cloud services: EC2, RDS.Experience with REST API calls and best practices.Other duties assigned ', 'Objects, Definitions, Variables Loops and if-statements.', 'Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.', 'Experience supporting and working with cross-functional teams in a dynamic environment.', 'Minimum of 2 years of experience coding REST services and APIs using Python', 'We are looking for a Data Engineer to join our growing team of data management experts. The hire will be responsible for expanding and optimizing our data architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is a leader who takes accountability for their work and enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing and fine-tuning our data architecture to support our next generation of products and data initiatives.', 'Use Client information to assemble large, complex data sets that meet functional business requirements.', 'Architect API and integration strategy across different applications, including Salesforce Marketing Cloud.', 'Architect API and integration strategy across different applications, including Salesforce Marketing Cloud.Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using SQL and AWS big data technologies.Interface with Clients in order to learn and understand business requirements.  Use Client information to assemble large, complex data sets that meet functional business requirements.Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using MySQL and Python technologies.Access data through API’s, FTP file drops and email attachments in order to stage data for reporting and analysis.Work with data and analytics experts to strive for greater functionality in our data systems', 'Experience with relational MySQL databases.Experience with using Python in an object-oriented/object function way.Experience with AWS cloud services: EC2, RDS.Experience with REST API calls and best practices.Other duties assigned ', 'Experience building new database tables and stored procedures with MySQL or similar database architecture.', 'Access data through API’s, FTP file drops and email attachments in order to stage data for reporting and analysis.', 'Ability to document architecture, structure and implementation details for your projects.', 'Experience with relational MySQL databases.', 'A successful history of manipulating, processing and extracting value from large disconnected datasets.', 'Basic Skills Required', 'Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using MySQL and Python technologies.', 'Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using SQL and AWS big data technologies.', 'We are looking for a candidate with 3+ years of experience in a data or analytics role, who has attained a Bachelor’s degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using some combination of the following software/tools:Experience with relational MySQL databases.Experience with using Python in an object-oriented/object function way.Experience with AWS cloud services: EC2, RDS.Experience with REST API calls and best practices.Other duties assigned ', 'Essential job functions', 'Working Python language knowledge and experience with core programming basics including:Objects, Definitions, Variables Loops and if-statements.', 'Working MySQL knowledge and experience working with relational databases, query authoring as well as working familiarity with a variety of databases.', 'Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations.']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-02-02 10:10:22
"Data Engineer/Senior Data Engineer, IT Applications",American Airlines,"Fort Worth, TX",9 hours ago,Be among the first 25 applicants,"['', 'Develop POC’s when necessary', ""All you'll need for success"", 'Design job/jobstream flows via scheduling tool, ensuring proper dependencies within enterprise schedules', 'Develop, code, test, and implement data solutions according to business requirements', 'Experience in Big Data development including Python, Spark, Scala, Parquet', ""What You'll Get"", 'Experience in Cloud; IBM or Azure', 'Travel Perks: Ready to explore the world? You, your family and your friends can reach 365 destinations on more than 6,800 daily flights across our global network.', '5 years of hands on experience in data warehouse development including framework and application solutions leveraging DataStage and TeradataExperience in Big Data development including Python, Spark, Scala, ParquetExperience in Cloud; IBM or Azure', 'Contribute to continuous improvement of On Premise Data warehouse and BigData applications and the Cloud applications', ""Bachelor's degree in Computer Science, Computer Engineering, Technology, Information Systems (CIS/MIS), Engineering or related technical discipline, or equivalent experience/training"", 'Source code management in Git or Subversion', 'Feel Free to be yourself at American', 'Preferred Qualifications- Education & Prior Job Experience', 'Strong problem-solving ability with a positive ""can-do"" attitude', 'Interact with business and technologies peers', 'Minimum Qualifications- Education & Prior Job Experience', 'Works in conjunction with Product Owner and Agile team', 'Provide assistance and resolution for any production related issues', '401(k) Program: Available upon hire and, depending on the workgroup, employer contributions to your 401(k) program are available after one year.', 'Demonstrated achievement in developing analytical data layers/applications with large data volumeStrong problem-solving ability with a positive ""can-do"" attitudeDevOps CI/CD using Jenkins or other competing tools in the marketA passion for technology, continuous improvement, quality and helping others grow', '5 years of hands on experience in data warehouse development including framework and application solutions leveraging DataStage and Teradata', 'Travel Perks: Ready to explore the world? You, your family and your friends can reach 365 destinations on more than 6,800 daily flights across our global network.Health Benefits: On day one, you’ll have access to your health, dental, prescription and vision benefits to help you stay well. And that’s just the start, we also offer virtual doctor visits, flexible spending accounts and more. Wellness Programs: We want you to be the best version of yourself - that’s why our wellness programs provide you with all the right tools, resources and support you need.401(k) Program: Available upon hire and, depending on the workgroup, employer contributions to your 401(k) program are available after one year.Additional Benefits: Other great benefits include our Employee Assistance Program, pet insurance and discounts on hotels, cars, cruises and more', 'Wellness Programs: We want you to be the best version of yourself - that’s why our wellness programs provide you with all the right tools, resources and support you need.', 'Health Benefits: On day one, you’ll have access to your health, dental, prescription and vision benefits to help you stay well. And that’s just the start, we also offer virtual doctor visits, flexible spending accounts and more. ', '3+ years’ of hands on experience in data warehouse development including framework and application solutions leveraging DataStage and Teradata', 'A passion for technology, continuous improvement, quality and helping others grow', ""What You'll Do"", 'DevOps CI/CD using Jenkins or other competing tools in the market', ""Bachelor's degree in Computer Science, Computer Engineering, Technology, Information Systems (CIS/MIS), Engineering or related technical discipline, or equivalent experience/training3+ years’ of hands on experience in data warehouse development including framework and application solutions leveraging DataStage and Teradata3+ years’ experience with UNIX Shell Scripting and SQLExperience delivering data solutions for, or within, an analytic or business intelligence environmentSource code management in Git or Subversion"", ""Why you'll love this job"", 'Interpret business data and data access requirements', 'Demonstrated achievement in developing analytical data layers/applications with large data volume', 'Intro', 'Complete source to target mappings', 'Skills, Licenses & Certifications', '3+ years’ experience with UNIX Shell Scripting and SQL', 'Works in conjunction with Product Owner and Agile teamInteract with business and technologies peersInterpret business data and data access requirementsProvide appropriate estimates on development tasks and capacity requirementsComplete source to target mappingsDevelop, code, test, and implement data solutions according to business requirementsDesign job/jobstream flows via scheduling tool, ensuring proper dependencies within enterprise schedulesProvide assistance and resolution for any production related issuesBe accountable for application performance monitoring and tuningContribute to continuous improvement of On Premise Data warehouse and BigData applications and the Cloud applicationsDevelop POC’s when necessary', 'Provide appropriate estimates on development tasks and capacity requirements', 'Be accountable for application performance monitoring and tuning', 'Experience delivering data solutions for, or within, an analytic or business intelligence environment', 'Additional Benefits: Other great benefits include our Employee Assistance Program, pet insurance and discounts on hotels, cars, cruises and more', 'This position is a member of the Information Technology Team, within the RPT Commercial Data Engineering & Business Analytics group supporting Revenue Management Product.The role of the Date Engineer, IT Applications will be to translate business requirements into solutions enabling business value in areas which may include analytics, data pipelines, and complex batch processing of airline commercial data.Success in this role is defined by the ability to leverage strong data application skills to open new capabilities being defined by our business community. You will collaborate with our business partners, fellow developers, and platform architects to achieve these']",Not Applicable,Full-time,Information Technology,Airlines/Aviation,2021-02-02 10:10:22
Data Visualization Engineer,Massachusetts General Hospital,"Boston, MA",19 hours ago,Be among the first 25 applicants,"['', ' Desire and motivation to learn about the organization and fundraising ', ' Adaptability and flexibility in times of shifting priorities ', ' Conduct qualitative and quantitative data analysis to recommend adjustments to donor (customer) segmentation and strategies ', ' Professionalism and a strong work ethic ', 'About Us', ' Experience with interpreting and translating analysis and reporting needs into tailored data solutions ', 'Qualifications', 'Employee Status', ' Write high quality documentation of solutions and processes for both analysts and end users ', 'Schedule', 'The Campaign for Mass General, ', '  Strong verbal and written communication skills   Organizational, time management and project management skills   Attention to detail   Professionalism and a strong work ethic   Desire and motivation to learn about the organization and fundraising   Awareness of personal work styles in self and others   Adaptability and flexibility in times of shifting priorities   Ability to work collaboratively with diverse audiences   Creativity and high levels of energy and enthusiasm   Commitment to adopting best practices and maximizing efficiency   Inclination to build relationships with people inside and outside of the organization   Good judgement and care in dealing with confidential information  ', ' Participate in professional development including taking advantage of shadowing opportunities when appropriate ', ' Translate data and analysis requests into solutions using a variety of data analysis and visualization tools, including T-SQL, Microsoft Visual Studio, Tableau, and Excel ', 'How To Apply', ' Take on other office-wide duties at the request of Development leadership ', ' Familiarity in modeling/data sciences languages ', ' Organizational, time management and project management skills ', ' Inclination to build relationships with people inside and outside of the organization ', ' Demonstrated experience with T-SQL concepts and terms including stored procedures, user-defined functions, views, triggers, and query optimization techniques ', 'Boston Globe', ' Identify and recommend improved processes and tools to fulfill the application, integration, reporting, analysis and data mining needs of the department ', ' Ability to build visually engaging and technically accurate reports using visualization tools like MS Visual Studio or Tableau ', '  Translate data and analysis requests into solutions using a variety of data analysis and visualization tools, including T-SQL, Microsoft Visual Studio, Tableau, and Excel   Review, improve and optimize existing SQL queries, views and stored procedures and implement best practices   Identify and implement data efficiency and visualization improvements to ensure end users have fast, consistent, updated and user-friendly data resources available   Lead the implementation of technology needed to facilitate the transfer of data and integrations with internal and third-party applications   Propose, define, review and test data warehouse modifications to fill identified data gaps and improve report design efficiency, including custom tables and views   Write high quality documentation of solutions and processes for both analysts and end users   Conduct qualitative and quantitative data analysis to recommend adjustments to donor (customer) segmentation and strategies   Identify and recommend improved processes and tools to fulfill the application, integration, reporting, analysis and data mining needs of the department   Work closely with internal and external partners to ensure all appropriate data protection and security procedures are in place and being adhered to   Maintain a strong understanding of all MS SQL Server database applications, as well as reporting and transformation tools   Participate in professional development including taking advantage of shadowing opportunities when appropriate   Prepare and regularly update progress reports   Contribute to maintaining positive office morale, even in the face of high work volume and challenging periods   Assist with recruitment activities as needed   Take on other office-wide duties at the request of Development leadership   Adhere to the mission, credo, and standards of behavior of Mass General Hospital  ', ' Good judgement and care in dealing with confidential information ', ' Demonstrated proficiency transferring data between cross-platform applications over the internet ', ' Creativity and high levels of energy and enthusiasm ', ' Demonstrated proficiency with MS SQL Server ', ' Contribute to maintaining positive office morale, even in the face of high work volume and challenging periods ', ' Prepare and regularly update progress reports ', '  Demonstrated proficiency with MS SQL Server   Demonstrated experience with T-SQL concepts and terms including stored procedures, user-defined functions, views, triggers, and query optimization techniques   Demonstrated proficiency transferring data between cross-platform applications over the internet   Ability to build visually engaging and technically accurate reports using visualization tools like MS Visual Studio or Tableau   An understanding of relational databases and BI tools   Familiarity in modeling/data sciences languages   Experience with interpreting and translating analysis and reporting needs into tailored data solutions   Demonstrated experience with data warehouse design and modification testing Demonstrated experience with SSIS packages and knowledge of ETL development preferred   Demonstrated experience with Microsoft’s SSAS, Excel Pivot Tables and Data Mining preferred  ', 'Job', 'Primary Location', ' Lead the implementation of technology needed to facilitate the transfer of data and integrations with internal and third-party applications ', 'Recruiting Department', 'Primary Responsibilities', ' The Campaign for Mass General ', ' Attention to detail ', ' Awareness of personal work styles in self and others ', ' Identify and implement data efficiency and visualization improvements to ensure end users have fast, consistent, updated and user-friendly data resources available ', ' Assist with recruitment activities as needed ', ' Demonstrated experience with data warehouse design and modification testing Demonstrated experience with SSIS packages and knowledge of ETL development preferred ', ' The Development Office ', 'Organization', ' Commitment to adopting best practices and maximizing efficiency ', ' Ability to work collaboratively with diverse audiences ', 'Standard Hours', ' Work closely with internal and external partners to ensure all appropriate data protection and security procedures are in place and being adhered to ', ' Maintain a strong understanding of all MS SQL Server database applications, as well as reporting and transformation tools ', 'Job Posting', 'Work Locations', 'U.S. News & World Report', ' An understanding of relational databases and BI tools ', 'Shift', ' Review, improve and optimize existing SQL queries, views and stored procedures and implement best practices ', ' Propose, define, review and test data warehouse modifications to fill identified data gaps and improve report design efficiency, including custom tables and views ', ' Strong verbal and written communication skills ', ' Adhere to the mission, credo, and standards of behavior of Mass General Hospital ', ' Demonstrated experience with Microsoft’s SSAS, Excel Pivot Tables and Data Mining preferred ']",Entry level,Full-time,Information Technology,Nonprofit Organization Management,2021-02-02 10:10:22
Data Engineer,LRW,"Los Angeles, CA",20 hours ago,Be among the first 25 applicants,"['', 'ABOUT YOU', 'Work with a variety of stakeholders to design, implement, and maintain\xa0data lake and data warehouse\xa0architecture to consolidate data from various APIs, SQL and NoSQL sources and make it\xa0accessible based on different use casesDesign and implement\xa0processes to shape and deliver data in accordance with business needs and various use casesEmploy required languages and\xa0tools to stitch a coherent system oriented toward improving data reliability, efficiency and quality', 'Experience in R and JavaScript\xa0is a plus', 'Execution platform: Apache Airflow', 'The ideal candidate should be curious, self-motivated, responsive, and articulateIs interested in the core business of the company and seeks to identify the business and use implications of various solutionsSomeone who is a tenacious problem-solver who seeks to identify core bottlenecks from both a technological as well as a process oriented stand point.With deep understanding of data processing concepts and data modeling principles (difference between OLTP, data warehouse, and data lake )With advanced knowledge of SQL, Python,\xa0and BashExperience working withcloud technologies: AWS (RDS, EC2, S3, Athena, Lambda, EMR, ECS), GCP (BigQuery, DataProc), SnowflakeData platforms: PostgreSQL, MongoDB , data in different file formats (Parquet, JSON, CSV, Excel)Execution platform: Apache AirflowDistributed processing: HDFS, Spark, HiveFamiliarity deploying solutions through docker or REST APIsExperience in R and JavaScript\xa0is a plus', 'Work with a variety of stakeholders to design, implement, and maintain\xa0data lake and data warehouse\xa0architecture to consolidate data from various APIs, SQL and NoSQL sources and make it\xa0accessible based on different use cases', 'Employ required languages and\xa0tools to stitch a coherent system oriented toward improving data reliability, efficiency and quality', 'Distributed processing: HDFS, Spark, Hive', 'With advanced knowledge of SQL, Python,\xa0and Bash', 'We are a fast-growing market research firm with an entrepreneurial culture. We’ve spent the past 40 years using analytics and research to help businesses understand their customers, and we work across industries in more than 80 countries with some of the largest brands in the world. We value diverse perspectives and believe that different voices and viewpoints make us stronger. We’re also proud to have a helpful and supportive culture, where we take time to celebrate accomplishments both large and small. And while we’re grounded in our rich history, we never stop searching for new approaches and tools; we were named the #1 Most Innovative Insights Firm in North America by the GRIT Report in 2019.\xa0', 'With deep understanding of data processing concepts and data modeling principles (difference between OLTP, data warehouse, and data lake )', 'In this role, you will:', 'With offices around the world, our 500+ teammates work across a dozen business units, collaborating with clients in entertainment and media, pharmaceuticals, technology, consumer packaged goods and more. Our experienced leadership team offers stability and structure, while our commitment to innovation fosters groundbreaking initiatives that help us improve our research approaches—like our Pragmatic Brain Science teams, who explore new psychological frameworks to better understand customer motivations.', 'ABOUT THE ROLE', 'Experience working with', 'Familiarity deploying solutions through docker or REST APIs', 'LRW is swimming in data, coming from many sources.\xa0The marketing and data science team requires an experienced and all-purpose data engineer to contribute to building\xa0of data processing infrastructure\xa0and ETL pipelines to enable access to the data in efficient ways across multiple platforms.', 'Design and implement\xa0processes to shape and deliver data in accordance with business needs and various use cases', 'Someone who is a tenacious problem-solver who seeks to identify core bottlenecks from both a technological as well as a process oriented stand point.', 'cloud technologies: AWS (RDS, EC2, S3, Athena, Lambda, EMR, ECS), GCP (BigQuery, DataProc), Snowflake', 'The ideal candidate should be curious, self-motivated, responsive, and articulate', 'ABOUT US', 'Data platforms: PostgreSQL, MongoDB , data in different file formats (Parquet, JSON, CSV, Excel)', 'Is interested in the core business of the company and seeks to identify the business and use implications of various solutions']",Associate,Full-time,Information Technology,Market Research,2021-02-02 10:10:22
Data Engineer,Lightspeed Systems,"Austin, TX",9 minutes ago,Be among the first 25 applicants,"['', 'Design data models for optimal storage and retrieval and optimize the data architecture to meet critical product and business requirements.', 'Experience designing, building and operating distributed systems, real-time data pipelines', 'Strategic thinker, self-motivated and team-oriented, should be able to work autonomously and effectively as part of geographically dispersed teams.', 'Health -- Medical, dental and vision insurance with healthy company contribution toward premiums.Wellness -- Lightspeed kicks cash into your HSA if you participate in our HDHP. Employees are provided an adjustable desk and onsite gyms at some offices. Healthy Holiday and PTO policy.Retirement -- 401(k) matching up to 6%', 'We are looking to add passionate, driven people to our team to help us fulfill our mission.', 'Establish architectural approaches and patterns that incorporate modernizing data governance, metadata management and data quality.', '3+ years of hands-on experience writing complex, highly-optimized SQL queries across large data sets.', 'Hands-on experience preferably with AWS (or another cloud platform).', 'Collaborate with engineering and data science teams to understand data challenges and provide scalable and flexible solutions.', 'Data Engineer', 'Contribute and drive high-quality engineering practices towards building data infrastructure and pipelines at scale.', 'Structuring data to be consumed easily, perform well, and be highly utilized in increasingly visual and analytic tools and use cases.', 'As a result, we are looking to add a Data Engineer to drive and continually evolve the data architecture. If you are passionate about data, understand how the attributes of data such as velocity, volume, and variety drive the underlying choices of data pipelines and datastores to be used, we want to talk to you.', 'Contribute and drive high-quality engineering practices towards building data infrastructure and pipelines at scale.Partner with cross-functional engineering teams to define highly scalable and reliable architectures for global data stores and data lakes.Provide technical leadership, oversight, and guidance for the data/schema.Design data models for optimal storage and retrieval and optimize the data architecture to meet critical product and business requirements.Collaborate with engineering and data science teams to understand data challenges and provide scalable and flexible solutions.Understand logging, event processing and how it impacts the rest of our data flow, architect logging best practices where needed.Establish architectural approaches and patterns that incorporate modernizing data governance, metadata management and data quality.Architect and implement data governance and security for the data platforms.', 'Education is undergoing a technology revolution with new devices and tools being added to the classroom every day and IT departments are responsible for keeping all this technology managed, safe and working. That is where we come in! Lightspeed Systems, ed-tech provider and leader in K-12 device filtering for 20 years, partners with schools to make learning safe, managed and mobile. Learn more at www.lightspeedsystems.com.', 'Nice to Have', 'Lightspeed Systems partners with schools to make learning safe, mobile and easily managed. We are growing and currently serve 15 million students and 28,000 schools in 35 countries.', 'Wellness -- Lightspeed kicks cash into your HSA if you participate in our HDHP. Employees are provided an adjustable desk and onsite gyms at some offices. Healthy Holiday and PTO policy.', 'Retirement -- 401(k) matching up to 6%', 'Partner with cross-functional engineering teams to define highly scalable and reliable architectures for global data stores and data lakes.', 'Building AI/ML pipelines is an asset', 'Provide technical leadership, oversight, and guidance for the data/schema.', 'Building AI/ML pipelines is an assetBig Data visualization frameworks e.g', 'ABOUT YOU:', ""As our ideal person for this role, you will understand how data should be securely stored, consumed, and managed. You'll use your expertise to develop and innovate technical solutions to meet the needs of the business. You are passionate about data, problem-solving, and driven to achieve great results for customers."", 'Understand logging, event processing and how it impacts the rest of our data flow, architect logging best practices where needed.', 'Over 3 years of experience or sufficient experience with a range of database/data warehouse/data engineering technologies and data architecture with demonstrated results implementing for business intelligence and analytics use.Bachelor’s or Master’s degree in Computer Science, Computer Engineering or a related technical field.Hands-on experience preferably with AWS (or another cloud platform).Experience designing, building and operating distributed systems, real-time data pipelines3+ years of hands-on experience writing complex, highly-optimized SQL queries across large data sets.Structuring data to be consumed easily, perform well, and be highly utilized in increasingly visual and analytic tools and use cases.A track record of leading, designing and building cost-effective data solutions in AWS using products such as EC2, EMR, S3, Athena, DynamoDB, CloudFormation, CodeBuild, ECSStrategic thinker, self-motivated and team-oriented, should be able to work autonomously and effectively as part of geographically dispersed teams.Successful in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy', 'We love our employees, and we show it. A sneak peek into our BENEFITS & PERKS include:', 'Over 3 years of experience or sufficient experience with a range of database/data warehouse/data engineering technologies and data architecture with demonstrated results implementing for business intelligence and analytics use.', 'Architect and implement data governance and security for the data platforms.', 'ABOUT THE ROLE:', 'A track record of leading, designing and building cost-effective data solutions in AWS using products such as EC2, EMR, S3, Athena, DynamoDB, CloudFormation, CodeBuild, ECS', 'Health -- Medical, dental and vision insurance with healthy company contribution toward premiums.', 'Successful in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy', 'Bachelor’s or Master’s degree in Computer Science, Computer Engineering or a related technical field.', 'Sponsorship is not available for this position.', 'ABOUT US', 'Big Data visualization frameworks e.g', 'We require all qualified applicants, as part of the application process, to complete a set of assessments. These will be transmitted separately.']",Entry level,Full-time,Information Technology,Computer Software,2021-02-02 10:10:22
Data Engineer,ettain group,"Charlotte, NC",11 hours ago,Be among the first 25 applicants,"['', 'Builds complex technical solutions that turn business requirements into operational processes ', 'Design, build and optimize big data pipelines, architectures and data sets', 'years of experience applying Agile development practices and working with distributed, component-based architectures.', 'Communicates clearly and concisely to key leadership and stakeholders to ensure alignment on project status and deliverables', 'Demonstrated strength in data modeling, ETL development and asynchronous pipelines.', 'Retail experience is a plus', 'Responsibilities: ', 'Partners with Big Data architects and other key stakeholders to ensure that project deliverables align with cost and timing standards', 'Are you a Data Engineer with at least 4 years of professional experience data modeling?! Join a Fortune 40 retailer in Charlotte, NC develop unique retail solutions!', '\xa0', 'Aligns complex technical solutions to corporate governance requirements to ensure data security and maintain data quality standards', 'Builds automation and self-service consumption tools to operationalize data science solutions', 'Experience in Big Data Solutions using technologies including one or more of the followings: Hadoop, Hive, HBase, MapReduce, Spark, Sqoop, Oozie.', 'Strong SQL skills.', 'Create data models and logical mappings to enable new data pipelines', 'Requirements:', '4+ years of  Data Engineering/Management experience Knowledge of data management fundamentals and data storage principles.Demonstrated strength in data modeling, ETL development and asynchronous pipelines.Strong experience with using IDEs and Software development environments including one or more of the followings: Eclipse, NetBeans and Intellij Idea.years of experience applying Agile development practices and working with distributed, component-based architectures.Experience in Big Data Solutions using technologies including one or more of the followings: Hadoop, Hive, HBase, MapReduce, Spark, Sqoop, Oozie.Strong SQL skills.1-2 years of experience in Python or Java.Retail experience is a plus', 'Design, implement, maintain, and support end-to-end ETL solutions', '1-2 years of experience in Python or Java.', ""Remains on the cutting edge of industry trends to ensure that the Lowe's COE is aligned with industry best practices"", '4+ years of  Data Engineering/Management experience ', 'Aligns technical solutions to corporate standards to ensure that security and privacy requirements are met', ""Builds complex technical solutions that turn business requirements into operational processes Design, build and optimize big data pipelines, architectures and data setsCreate data models and logical mappings to enable new data pipelinesDesign, implement, maintain, and support end-to-end ETL solutionsAligns complex technical solutions to corporate governance requirements to ensure data security and maintain data quality standardsRemains on the cutting edge of industry trends to ensure that the Lowe's COE is aligned with industry best practicesBuilds automation and self-service consumption tools to operationalize data science solutionsAligns technical solutions to corporate standards to ensure that security and privacy requirements are metCommunicates clearly and concisely to key leadership and stakeholders to ensure alignment on project status and deliverablesPartners with Big Data architects and other key stakeholders to ensure that project deliverables align with cost and timing standardsPartners with the main data platform engineering team and data architects to build the data pipelines"", 'Strong experience with using IDEs and Software development environments including one or more of the followings: Eclipse, NetBeans and Intellij Idea.', 'Partners with the main data platform engineering team and data architects to build the data pipelines', 'Knowledge of data management fundamentals and data storage principles.']",Mid-Senior level,Full-time,Information Technology,Retail,2021-02-02 10:10:22
Data Science Engineer,Warner Music Group,"New York, NY",7 hours ago,Be among the first 25 applicants,"['', 'scaling and optimizing Machine Learning models for deployment', 'About Us', 'using scripting languages (e.g. Python, Julia, Scala) at an expert level', ' At Warner Music Group we’re all about our people. Our global company is made up of knowledgeable, passionate, and creative individuals. Our commitment to Diversity, Equity and Inclusion fosters a culture where you can truly belong, contribute, and grow. We believe in each individual’s value and encourage applications from people of any age, gender identity, sexual orientation, race, religion, ethnicity, disability, veteran status, and any other characteristic or identity. ', 'Experience with Data Science tasks and knowledge of additional programming languages a plus', 'Experience using and deploying containers a bonus', ' Consider a career at WMG and be a part of one of the most influential forces in culture today. ', ' It would be music to our ears if you also had: ', 'Experience with Data Science tasks and knowledge of additional programming languages a plusExperience using and deploying containers a bonus', 'Execute tasks in time and with exceptional attention to detail', 'Job Title: ', 'expertly using SQL and NoSQL databases', 'Data Science Engineer', 'Harden and optimize ETL scripts and data pipelines built by Data Scientists for data exploration, modeling and visualizations,Deploy model pipelines and build task orchestration,Ensure computation efficiency in deployed algorithms and data pipelinesBuild and maintain tables and APIs (or other endpoints) where model outputs can be read by dashboards or visualization toolsServe as a technical liaison with Data Engineers in various technology organizations across Warner Music GroupExecute tasks in time and with exceptional attention to detail', 'Deploy model pipelines and build task orchestration,', 'Aptitude to learn new technologies, tools and methods as required', 'Build and maintain tables and APIs (or other endpoints) where model outputs can be read by dashboards or visualization tools', 'performing Data Engineering, Machine Learning Engineering or Machine Learning Operations tasks training, tuning, validating Machine Learning modelsscaling and optimizing Machine Learning models for deploymentexpertly using SQL and NoSQL databasesoperating in cloud environments (e.g. AWS, Azure, GCP)using tools for large-scale data processing (e.g. Spark, Hadoop) with high proficiencyusing scripting languages (e.g. Python, Julia, Scala) at an expert level', '2 years of experience in a business settingperforming Data Engineering, Machine Learning Engineering or Machine Learning Operations tasks training, tuning, validating Machine Learning modelsscaling and optimizing Machine Learning models for deploymentexpertly using SQL and NoSQL databasesoperating in cloud environments (e.g. AWS, Azure, GCP)using tools for large-scale data processing (e.g. Spark, Hadoop) with high proficiencyusing scripting languages (e.g. Python, Julia, Scala) at an expert level', 'A Little Bit About Our Team', 'Why This Could Be Your Next Big Break', 'Ensure computation efficiency in deployed algorithms and data pipelines', 'training, tuning, validating Machine Learning models', 'operating in cloud environments (e.g. AWS, Azure, GCP)', 'using tools for large-scale data processing (e.g. Spark, Hadoop) with high proficiency', 'Here You’ll Get To', 'Graduate Degree (Masters encouraged) in Computer Science, Information Technology, Software Engineering, Data Engineering, Data Science, or related fields2 years of experience in a business settingperforming Data Engineering, Machine Learning Engineering or Machine Learning Operations tasks training, tuning, validating Machine Learning modelsscaling and optimizing Machine Learning models for deploymentexpertly using SQL and NoSQL databasesoperating in cloud environments (e.g. AWS, Azure, GCP)using tools for large-scale data processing (e.g. Spark, Hadoop) with high proficiencyusing scripting languages (e.g. Python, Julia, Scala) at an expert levelAptitude to learn new technologies, tools and methods as required', 'Serve as a technical liaison with Data Engineers in various technology organizations across Warner Music Group', "" It is the mission of every member of the WMG team around the world to create a nurturing environment for artists, songwriters, and the people behind the music – at every stage of their career. We strive to set WMG apart by embracing innovation – an integral part of our company's DNA. "", 'Graduate Degree (Masters encouraged) in Computer Science, Information Technology, Software Engineering, Data Engineering, Data Science, or related fields', 'performing Data Engineering, Machine Learning Engineering or Machine Learning Operations tasks ', 'Harden and optimize ETL scripts and data pipelines built by Data Scientists for data exploration, modeling and visualizations,', 'Job Description', 'Data Science Engineer ', 'Desired Characteristics']",Entry level,Full-time,Information Technology,Entertainment,2021-02-02 10:10:22
Data Engineer II,Net Health,"Pittsburgh, PA",5 hours ago,Be among the first 25 applicants,"['', 'Unlimited PTO: ', 'Voice:', 'Must be comfortable working in ambiguous and/or stressful situations. ', 'Casual Dress Code:', 'Troubleshoots problems, identifies possible solutions, and resolves accordingly ', 'Qualifications', 'Work from Anywhere: ', 'Responsibilities And Duties', 'Makes sure data pipelines and infrastructure are built to meet goals around resiliency, performance, and scalability ', ' We Invest in Our Employees and Watch Them Flourish… ', 'Analyzes data to spot anomalies, trends, and correlate similar data sets Analyze large, complex datasets to drive actionable insights and recommendations Troubleshoots problems, identifies possible solutions, and resolves accordingly Makes sure data pipelines and infrastructure are built to meet goals around resiliency, performance, and scalability Participate in Agile/Scrum process to refine, prioritize, and build solutions to meet customer needs ', '2 - 4 Years’ experience building pipelines with iPaaS tools ', 'About Net Health', 'Exceptional organization and time management skills. ', 'Build, evolve, and scale out infrastructure to ingest, process, and extract meaning out of data ', 'Cooperate with matrixed team members to meet goals or complete tasks. ', 'Resolve complex problems independently, using current job knowledge, research, and external resources ', 'Bachelor’s degree in computer science, computer engineering or equivalent work experience 4 - 6 Years’ experience using SQL to query and manipulate data ', 'Analyze large, complex datasets to drive actionable insights and recommendations ', 'Bachelor’s degree in computer science, computer engineering or equivalent work experience ', 'Flexibility, ability to change priorities quickly, and capacity to handle multiple tasks. Effective collaborator with proven process improvement skills. Exceptional organization and time management skills. Excellent communication and interpersonal skills. Ability to consistently learn new technologies and apply those concepts to customer’s needs. ', 'Effective collaborator with proven process improvement skills. ', 'Keep up-to-date on technology trends, developments & best practices. ', '1 - 2 Years’ experience with Python, JavaScript, or other scripting language ', 'Build systems and infrastructures that collect and process data according to company needs and goals ', 'Analyzes data to spot anomalies, trends, and correlate similar data sets ', 'Job Overview', 'Communication And Cognitive Abilities', 'Must be self-motivated and know when to seek guidance; detail-orientation is a must. ', 'Cooperate with matrixed team members to meet goals or complete tasks. Must be comfortable working in ambiguous and/or stressful situations. Must be self-motivated and know when to seek guidance; detail-orientation is a must. ', 'Diversity & Inclusion:', 'Ability to work independently and as part of a team. ', 'Note: This job description is not intended to be all-inclusive. Employee may perform other related duties as requested to meet the ongoing needs of the organization.', 'Build systems and infrastructures that collect and process data according to company needs and goals Build, evolve, and scale out infrastructure to ingest, process, and extract meaning out of data Leverages existing data infrastructure to fulfill all data-related requests, perform necessary data housekeeping, data cleansing, normalization, hashing, and implementation of required data model changes ', 'Ability to work as part of a geographically dispersed team. Ability to work independently and as part of a team. Keep up-to-date on technology trends, developments & best practices. Ability to communicate effectively to both technical & non-technical audiences. ', 'Ability to communicate effectively to both technical & non-technical audiences. ', 'Prioritized Employee Wellness: ', 'Benefits', 'Ability to consistently learn new technologies and apply those concepts to customer’s needs. ', 'Participate in Agile/Scrum process to refine, prioritize, and build solutions to meet customer needs ', 'Excellent communication and interpersonal skills. ', 'Manage multiple priorities and work within a team-oriented environment ', 'Interaction', 'Excellent communication and organizational skills ', 'Leverages existing data infrastructure to fulfill all data-related requests, perform necessary data housekeeping, data cleansing, normalization, hashing, and implementation of required data model changes ', 'Ability to work as part of a geographically dispersed team. ', '2 - 4 Years’ experience building pipelines with iPaaS tools 1 - 2 Years’ experience with Python, JavaScript, or other scripting language Resolve complex problems independently, using current job knowledge, research, and external resources Manage multiple priorities and work within a team-oriented environment Excellent communication and organizational skills ', 'Flexibility, ability to change priorities quickly, and capacity to handle multiple tasks. ', 'Comprehensive Benefits Package:', '4 - 6 Years’ experience using SQL to query and manipulate data ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-02-02 10:10:22
Data Engineer - Remote,"Fetch Rewards, Inc.","Remote, OR",46 minutes ago,Be among the first 25 applicants,"['', ' REQUIRED: Python programming skills', ' The desire to work with other teams in the organization (e.g., Development, Business Intelligence, Data Science) to build tools and solutions that support and help manage data within the Fetch ecosystem', ' Big data development skills (e.g., Spark, Hadoop, MPP DW)', ' Familiarity with messaging and asynchronous technologies (e.g., SQS, Kinesis, RabbitMQ, Kafka)', ' REQUIRED: Python programming skills  Solid SQL skills  Familiarity with Unix systems, shell scripting, and Git  Experience with relational (SQL), non-relational (NoSQL), and/or object data stores (e.g., Snowflake, MongoDB, S3, HDFS, Postgres, Redis, DynamoDB)  Interest in building and experimenting with different tools and tech, and sharing your learnings with the broader organization  The desire to work with other teams in the organization (e.g., Development, Business Intelligence, Data Science) to build tools and solutions that support and help manage data within the Fetch ecosystem  Bachelor’s degree in Computer Science (or equivalent)  At least 3 years of relevant full-time work experience', ' Interest in building and experimenting with different tools and tech, and sharing your learnings with the broader organization', ' Solid SQL skills', 'Who We Are', "" Love of Dogs! . . . Or just tolerance. We're a very canine-friendly workplace"", ' Cloud engineering and DevOps skills (e.g., AWS, CloudFormation, Docker)', ' Experience with visualization tools (e.g., Tableau)', ' Experience with relational (SQL), non-relational (NoSQL), and/or object data stores (e.g., Snowflake, MongoDB, S3, HDFS, Postgres, Redis, DynamoDB)', 'The Role', ' Bachelor’s degree in Computer Science (or equivalent)', ' ETL process, data pipeline, and/or micro-service development experience', "" Excellent written and verbal communication skills  Familiarity with open source software and dependency management  ETL process, data pipeline, and/or micro-service development experience  Cloud engineering and DevOps skills (e.g., AWS, CloudFormation, Docker)  Familiarity with messaging and asynchronous technologies (e.g., SQS, Kinesis, RabbitMQ, Kafka)  Big data development skills (e.g., Spark, Hadoop, MPP DW)  Experience with visualization tools (e.g., Tableau)  Love of Dogs! . . . Or just tolerance. We're a very canine-friendly workplace"", 'Bonus Points For', ' Familiarity with open source software and dependency management', ' Why Join the Fetch Family?', ' Familiarity with Unix systems, shell scripting, and Git', ' At least 3 years of relevant full-time work experience', ' Excellent written and verbal communication skills']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-02-02 10:10:22
"Data Engineer, Data Science & Analytics",Earnin,"Palo Alto, CA",5 hours ago,Be among the first 25 applicants,"['', 'Translate complex, open-ended problems into elegant design and build high quality, maintainable data products and tools.', 'Communicate the tradeoffs of technical decisions to multiple stakeholders, including non-technical audiences.', 'Champion data quality and governance throughout Earnin by maintaining and extending a clean data ecosystem for the company. ', 'Experience with physical data modeling on cloud storage: file formats, compression, partitioning strategies, etc. ', 'Nice To Haves', 'Experience working with streaming infrastructure like AWS Kinesis and/or Kafka is a big plus.', 'We are a collaborative team and genuinely enjoy working with each other. ', 'Experience with cloud data platforms like Snowflake or Databricks and/or cloud data warehouses like Redshift. ', 'Work cross functionally with other teams (data science, design, product, marketing, and analytics) in high visibility roles.', 'Experience with BI tools like Looker, Tableau, and Periscope.', 'Hands-on experience working with a varied set of data storage technologies (e.g. Mysql, Postgres, DynamoDB, S3, etc.). You know where and when to use each. ', 'Experience building, deploying, maintaining, and tuning Spark-based applications.', 'High impact roles at a relatively small company that’s aggressively growing our user base. ', 'Experience deploying microservices and jobs on Kubernetes infrastructure is a big plus.', 'Experience building and deploying AWS Lambda applications.', 'We believe in empowering our people to be successful.', 'Substantial experience working at a startup. ', 'Substantial experience with testing, data validation, and data quality assurance.', 'Build robust and reliable data products that enable automated reporting, experimentation, A/B testing, anomaly detection, and root cause analysis.', 'Taking pride in your code quality and helping others elevate their own code quality.', 'What Sets Us Apart', 'BS or MS degree in Computer Science, Engineering, or a related technical field.', ""What You'll Do"", 'Actively engage and drive design reviews and code reviews. ', 'Advanced knowledge of analytical SQL and data modeling. ', 'Experience using query engines like Athena, Presto, and Impala is a big plus.', 'We’re building a product that inspires fairness across the financial world.', 'Excellent written and verbal communication skills, including the ability to identify and communicate data driven insight. ', 'Experience with data modeling in Redshift is a big plus.', '4+ years of experience working with analytical data systems and building production applications. ', 'Experience building and deploying machine learning models is a big plus.', 'Experience with workflow orchestration tools like Airflow.', 'Curiosity and a drive to learn. Willingness to be assertive and drive solutions independently. ', ' High impact roles at a relatively small company that’s aggressively growing our user base.  We are a collaborative team and genuinely enjoy working with each other.  We believe in empowering our people to be successful. We’re building a product that inspires fairness across the financial world. ', 'Strong Python programming skills.', 'Design and build massively scalable, production-grade data services and pipelines that power machine learning model development and actionable analytics.', 'Substantial experience developing production ETL processes. ', 'About Earnin', 'Experience using Terraform is a big plus. ', ' BS or MS degree in Computer Science, Engineering, or a related technical field. Excellent written and verbal communication skills, including the ability to identify and communicate data driven insight.  Curiosity and a drive to learn. Willingness to be assertive and drive solutions independently.  4+ years of development experience in a fast-paced environment. 4+ years of experience working with analytical data systems and building production applications.  Advanced knowledge of analytical SQL and data modeling.  Strong Python programming skills. Experience building, deploying, maintaining, and tuning Spark-based applications. Taking pride in your code quality and helping others elevate their own code quality. Substantial experience developing production ETL processes.  Substantial experience with testing, data validation, and data quality assurance. Hands-on experience working with a varied set of data storage technologies (e.g. Mysql, Postgres, DynamoDB, S3, etc.). You know where and when to use each.  Experience with physical data modeling on cloud storage: file formats, compression, partitioning strategies, etc.  Experience with cloud data platforms like Snowflake or Databricks and/or cloud data warehouses like Redshift.  Experience with BI tools like Looker, Tableau, and Periscope. ', ""What We're Looking For"", 'About The Team', 'Work with data scientists and analysts to productionalize model deployments and pipelines.', ' Translate complex, open-ended problems into elegant design and build high quality, maintainable data products and tools. Design and build massively scalable, production-grade data services and pipelines that power machine learning model development and actionable analytics. Build robust and reliable data products that enable automated reporting, experimentation, A/B testing, anomaly detection, and root cause analysis. Work with data scientists and analysts to productionalize model deployments and pipelines. Champion data quality and governance throughout Earnin by maintaining and extending a clean data ecosystem for the company.  Actively engage and drive design reviews and code reviews.  Work cross functionally with other teams (data science, design, product, marketing, and analytics) in high visibility roles. Communicate the tradeoffs of technical decisions to multiple stakeholders, including non-technical audiences. ', '4+ years of development experience in a fast-paced environment.', 'Experience working with alerting and monitoring tools like DataDog and PagerDuty.', ' Experience building and deploying machine learning models is a big plus. Experience working with streaming infrastructure like AWS Kinesis and/or Kafka is a big plus. Experience with data modeling in Redshift is a big plus. Experience using query engines like Athena, Presto, and Impala is a big plus. Experience using Terraform is a big plus.  Experience deploying microservices and jobs on Kubernetes infrastructure is a big plus. Experience working with alerting and monitoring tools like DataDog and PagerDuty. Substantial experience working at a startup.  Experience building and deploying AWS Lambda applications. Experience with workflow orchestration tools like Airflow. ']",Entry level,Full-time,Information Technology,Computer Software,2021-02-02 10:10:22
Data Engineer (Remote),Michael Page,"New York, NY",4 hours ago,Be among the first 25 applicants,"['', 'High growth opportunity within a start-up', 'Description', 'Exposure to modern technologies ', 'Supporting, deploying and driving distributed systems technologies', 'Experience working in E-commerce is ideal ', ""What's On Offer"", 'Contact: Michael Lopez', 'Implementing and managing a hybrid cloud environment', 'Strong SQL experience ', 'Strong SQL experience 4-6 years of professional Data Engineering experience Experience working in E-commerce is ideal ', 'Gather business requirements and create technical implementations and end-user documentation.', 'Gather business requirements and create technical implementations and end-user documentation.Be active in data modeling and structure design with the Product and Engineering teams.Supporting, deploying and driving distributed systems technologiesImplementing and managing a hybrid cloud environment', 'Quote job ref: 1507959', 'Be active in data modeling and structure design with the Product and Engineering teams.', ""MPI does not discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity or expression, national origin, age, disability, veteran status, marital status, or based on an individual's status in any group or class protected by applicable federal, state or local law. MPI encourages applications from minorities, women, the disabled, protected veterans and all other qualified applicants."", '4-6 years of professional Data Engineering experience ', 'Job Description', 'The Successful Applicant', 'Data Engineer (Remote)', ' Competitive base salary and benefits * Remote work']",Entry level,Full-time,Information Technology,Insurance,2021-02-02 10:10:22
Software/Data Developer,The University of New Mexico,"Albuquerque, NM",13 hours ago,Be among the first 25 applicants,"['Develop, implement and document new code using best practices. Completing testing and quality assurance of developed code ', 'Identify way to improve data reliability and quality and to prepare data for predictive and prescriptive modeling', 'Use data to discover tasks that can be automated', 'Integrate third party solutions into out ECHO Digital system. This may include business intelligence solutions', 'Work closely with the research, evaluation, data and insights teams to understand reporting needs and propose data engineering solutions, which may include development of scripts to clean, validate, and consolidate data', 'Work collaboratively with other programmer analysts and under the direction of the lead engineering manager', 'Upgrade and maintain tools, libraries and other dependencies we use to ensure continued functionality of the systems', ""Stay current of best practices for software and data engineering. Participating in conferences and training events synchronously and asynchronouslyProject ECHO is a telemedicine and distance-learning program with partners all over the world. Some work hours outside of normal business hours may be required. Additionally, we often use remote collaboration tools to enable some telework.Project ECHO prides itself on being a values-based organization. Our seven values include: Service to the Underserved, Demonopolize Knowledge, Mutual Trust and Respect, Teamwork, Excellence and Accountability, Innovation and Learning and Joy of Work. We strive to find individuals who can embrace and exemplify these values.Project ECHO is committed to democratizing medical knowledge and getting best practice care to underserved people all over the world. We are funded in part by grants from the GE Foundation, the Helmsley Charitable Trust, the Bristol Myers Squibb Foundation, the Merck Foundation, Robert Wood and the US Government. We have received support from the NM Legislature and the New Mexico Department of Health.Project ECHO's goal is to touch the lives of 1 billion people by 2025 and we are looking for mission-driven high performers who share similar values to join our team and help us achieve this goal.See the Position Description for additional information.Conditions of EmploymentSpecialty licensure/certification may be required, as specified by the department.Successful candidate may be subject to a criminal background check prior to starting work.Minimum QualificationsBachelor's degree; at least 1 year of progressively responsible experience directly related to the duties and responsibilities specified.Higher education and/or experience that is directly related to the duties and responsibilities specified may be interchangeable on a year for year basis.Preferred QualificationsAdditional RequirementsCampusHealth Sciences Center (HSC) - Albuquerque, NMDepartmentProject ECHO (259B)Employment TypeStaffStaff TypeOn-CallTerm End DateStatusNon-ExemptPay$29.72 HourlyBenefits EligibleERB StatementTemporary and on-call employees working an appointment percentage of 26 (.26 FTE) or greater, per quarter, will be eligible to earn retirement service credits and thus are required to make New Mexico Educational Retirement Board (NMERB) contributions. More information pertaining to your FTE and NMERB contributions can be reviewed on the NMERB Guidelines Clarified webpage.Background Check RequiredNoFor Best Consideration Date1/25/2021Application Instructions Only applications submitted through the official UNMJobs site will be accepted. If you are viewing this job advertisement on a 3rd party site, please visit UNMJobs to submit an application. For consideration, please submit your current resume and a cover letter. Your resume must include complete dates of employment (year and month) and indicate average number of hours worked per week. Please describe how you meet the preferred qualifications of this position in the cover letter. Positions posted with a Staff Type of Regular or Term are eligible for the Veteran Preference Program. See the Veteran Preference Program webpage for additional details. The University of New Mexico is committed to hiring and retaining a diverse workforce. We are an Equal Opportunity Employer, making decisions without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, veteran status, disability, or any other protected class.""]",Entry level,Temporary,Information Technology,Nonprofit Organization Management,2021-02-02 10:10:22
Data Engineer,The Judge Group,"Atlanta, GA",14 hours ago,Be among the first 25 applicants,"['', 'AWS Data Engineer with work in a group that is responsible for creating sustainable value and competitive advantage by leveraging analytics, information technology, and actionable insights across the enterprise while focusing on futuristic possibilities of analytics.', '5+ years of Data Engineering experience\xa0', 'Assist Data Science teams with preparing, cleansing, and delivering analytical datasets for machine learning models.', 'Data Engineer (AWS) Position', 'ROLE/RESPONSIBILITIES', 'Enhance and optimize exiting data quality processes including automated testing and alerting on critical data assets.', 'JOB DESCRIPTION', 'Starting Pay Rate: $110.00 hrly to $120.00 hrly,', 'REQUIRED SKILLS/EXPERIENCE', '\xa0', 'Responsible for hands-on data engineering for the Enterprise data and analytics team focused on managing the Data Lake and helping the business develop, deploy and manage predictive and prescriptive models to create business value through optimization of manufacturing facilities.Develop critical data pipelines and data quality jobs in the AWS environment working with Lambda, Glue, Python, SQL and noSQL databases.Enhance and optimize exiting data quality processes including automated testing and alerting on critical data assets.Assist Data Science teams with preparing, cleansing, and delivering analytical datasets for machine learning models.Monitor, deploy and troubleshoot SAS models in production', 'Bachelor’s degree', 'JOB DESCRIPTION:', 'Must have strong AWS skills (specifically in Lambda, Kinesis & RedShift) as well as strong Python coding ability.', 'Responsible for hands-on data engineering for the Enterprise data and analytics team focused on managing the Data Lake and helping the business develop, deploy and manage predictive and prescriptive models to create business value through optimization of manufacturing facilities.', 'At least 2 years of Python development experience.', 'Our client is only able to hire current Green Card Holders or US Citizens.', 'Monitor, deploy and troubleshoot SAS models in production', '2+ years working with AWS serverless technologies like AWS Lambda, AWS Glue, Kinesis, DynamoDB and Redshift', 'Bachelor’s degree5+ years of Data Engineering experience\xa02+ years working with AWS serverless technologies like AWS Lambda, AWS Glue, Kinesis, DynamoDB and RedshiftAt least 2 years of Python development experience.', 'Starting Pay Rate: $110.00 hrly to $120.00 hrly,Location: Atlanta Downtown area (100% Remote, to start, then hybrid position)', 'AWS Data Engineer with strong experience in Advanced Analytics (specifically predictive capabilities, real-time streaming, and machine learning).Must have strong AWS skills (specifically in Lambda, Kinesis & RedShift) as well as strong Python coding ability.AWS Data Engineer with work in a group that is responsible for creating sustainable value and competitive advantage by leveraging analytics, information technology, and actionable insights across the enterprise while focusing on futuristic possibilities of analytics.', 'current Green Card Holders or US Citizens', 'Location: Atlanta Downtown area (100% Remote, to start, then hybrid position)', 'Develop critical data pipelines and data quality jobs in the AWS environment working with Lambda, Glue, Python, SQL and noSQL databases.', '(Our client) is an Equal Opportunity Employer/Minorities/Females/Disabled/Veterans', 'AWS Data Engineer with strong experience in Advanced Analytics (specifically predictive capabilities, real-time streaming, and machine learning).']",Director,Full-time,Information Technology,Information Technology and Services,2021-02-02 10:10:22
Data Engineer - Phoenix,Compunnel ,"Phoenix, AZ",4 hours ago,Be among the first 25 applicants,"[' AWS is a plus Education: Bachelors Degree', ' 1-2+ years’ working with Relational Database Management Systems', ' ETL Tools (Informatica or Matillion) is a plus', ' Database Schema', ' Education:', ' Good SQL skills to maintain database']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-02-02 10:10:22
Senior Software Engineer,Walmart,"Seattle, WA",8 hours ago,Be among the first 25 applicants,"['', ' Outlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.  ', ""What You'll Do"", ""Leads and participates in medium- to large-scale projects by reviewing project requirements; translating requirements into technical solutions; gathering requested information (for example, design documents, product requirements, wire frames); writing and developing code; conducting unit testing; communicating status and issues to team members and stakeholders; collaborating with project team and cross functional teams; troubleshooting open issues and bug-fixes; ensuring on-time delivery and hand-offs; interacting with project manager to provide input on project plan; and providing leadership to the project team.Leads the work of other small groups of four to six engineers, including offshore associates, for assigned Engineering projects by proving pertinent documents, direction, and examples; identifying short and long term solutions and timeline; reviewing and providing feedback for proposed solutions; and performing design and code reviews of changes.Troubleshoots business and production issues by gathering information (for example, issue, impact, criticality, possible root cause); engaging support teams to assist in the resolution of issues; formulating an action plan; performing actions as designated in the plan; interpreting the results to determine further action; performs root cause analysis to prevent future occurrence of issues; and completing online documentation.Provides support to the business by responding to user's questions, concerns, and issues (for example, technical feasibility, implementation strategies); identifying short- and long-term solutions; facilitating resolutions; and leading cross-functional partnership.Leads the discovery phase of medium to large projects to come up with high level design by partnering with the product management, project management, business and user experience teams; and obtaining cross-function approvals.Demonstrates up-to-date expertise and applies this to the development, execution, and improvement of action plans by providing expert advice and guidance to others in the application of information and best practices; supporting and aligning efforts to meet customer and business needs; and building commitment for perspectives and rationales.Provides and supports the implementation of business solutions by building relationships and partnerships with key stakeholders; identifying business needs; determining and carrying out necessary processes and practices; monitoring progress and results; recognizing and capitalizing on improvement opportunities; and adapting to competing demands, organizational changes, and new responsibilities.Models compliance with company policies and procedures and supports company mission, values, and standards of ethics and integrity by incorporating these into the development and implementation of business plans; using the Open Door Policy; and demonstrating and assisting others with how to apply these in executing business processes and practices."", 'Outlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications. ', 'Models compliance with company policies and procedures and supports company mission, values, and standards of ethics and integrity by incorporating these into the development and implementation of business plans; using the Open Door Policy; and demonstrating and assisting others with how to apply these in executing business processes and practices.', ""Provides support to the business by responding to user's questions, concerns, and issues (for example, technical feasibility, implementation strategies); identifying short- and long-term solutions; facilitating resolutions; and leading cross-functional partnership."", 'Leads the discovery phase of medium to large projects to come up with high level design by partnering with the product management, project management, business and user experience teams; and obtaining cross-function approvals.', 'Minimum Qualifications...', 'Provides and supports the implementation of business solutions by building relationships and partnerships with key stakeholders; identifying business needs; determining and carrying out necessary processes and practices; monitoring progress and results; recognizing and capitalizing on improvement opportunities; and adapting to competing demands, organizational changes, and new responsibilities.', 'Outlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications. ', 'Troubleshoots business and production issues by gathering information (for example, issue, impact, criticality, possible root cause); engaging support teams to assist in the resolution of issues; formulating an action plan; performing actions as designated in the plan; interpreting the results to determine further action; performs root cause analysis to prevent future occurrence of issues; and completing online documentation.', 'Preferred Qualifications', 'Leads the work of other small groups of four to six engineers, including offshore associates, for assigned Engineering projects by proving pertinent documents, direction, and examples; identifying short and long term solutions and timeline; reviewing and providing feedback for proposed solutions; and performing design and code reviews of changes.', 'Leads and participates in medium- to large-scale projects by reviewing project requirements; translating requirements into technical solutions; gathering requested information (for example, design documents, product requirements, wire frames); writing and developing code; conducting unit testing; communicating status and issues to team members and stakeholders; collaborating with project team and cross functional teams; troubleshooting open issues and bug-fixes; ensuring on-time delivery and hand-offs; interacting with project manager to provide input on project plan; and providing leadership to the project team.', 'Demonstrates up-to-date expertise and applies this to the development, execution, and improvement of action plans by providing expert advice and guidance to others in the application of information and best practices; supporting and aligning efforts to meet customer and business needs; and building commitment for perspectives and rationales.']",Associate,Full-time,Engineering,Information Technology and Services,2021-02-02 10:10:22
Data Engineer,Clarus Group,"Washington, DC",21 hours ago,Be among the first 25 applicants,"['', 'Data Warehouse ETL Testing (3+ years)', 'Informatica Enterprise Data Catalog (3+ years)', 'Bachelor’s Degree', 'Our Professional Services team is currently looking for a Data Engineer to deliver innovative solutions to our client. ', 'Clarus Group is firmly committed to Equal Employment Opportunity (EEO) and to compliance with all Federal, State and local laws that prohibit employment discrimination on the basis of age, race, color, gender, national origin, sexual orientation, religion, disability, protected veteran status and other protected classifications. This policy applies to all employment decisions including, but not limited to, recruiting, hiring, training, promotions, pay practices, benefits, disciplinary actions and terminations. If you require reasonable accommodation in the application process, call Human Resources at 913-599-5255.\xa0', 'Experience Required:', 'Data Warehouse Tools (3+ years)', 'Support creation of a Cloud-based data management Infrastructure through the development of data lakes and a data warehouse. Candidate will be involved in all aspects of the modeling process, including data discovery, data quality, and logical/physical modeling. The analyst should have experience with particular Informatica tools (EDC, IDQ, and Axon) in an Azure Cloud environment, as well as other Azure based tools such as Polybase, and Synapse.\xa0', '**Per our Federal Contract Requirements, candidates must be US Citizens and hold ACTIVE SECRET CLEARANCE**', '\xa0', 'Clarus Group is a veteran-owned management consulting and technology solutions company that believes our employees are critical to our overall success. We provide a workplace that encourages growth and career development, and rewards excellence and hard work.\xa0Our consultants provide our clients with the benefit of years of experience, education, and the desire to partner with them to accomplish their goals.', 'Job Description:', 'Informatica Data Quality (3+ years)', 'Data Audit and Profiling (3+ years)', 'Data Engineer ', 'Education:', 'Informatica Axon (3+ years)', 'Support creation of a Cloud-based data management Infrastructure through the development of data lakes and a data warehouse. ', '***NO THIRD PARTIES***', 'Candidate will be involved in all aspects of the modeling process, including data discovery, data quality, and logical/physical modeling. ', 'Data Warehouse Tools (3+ years)Data Audit and Profiling (3+ years)Data Warehouse ETL Testing (3+ years)Informatica Axon (3+ years)Informatica Data Quality (3+ years)Informatica Enterprise Data Catalog (3+ years)', 'The analyst should have experience with particular Informatica tools (EDC, IDQ, and Axon) in an Azure Cloud environment, as well as other Azure based tools such as Polybase, and Synapse.\xa0']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-02-02 10:10:22
Data Scientist,Notion,"San Francisco, CA",12 hours ago,31 applicants,"['', 'You have experience building predictive models, and you know how to evaluate their effectiveness.', 'About Us', 'You have expertise in at least one scripting language (ideally Python or R).', 'You have strong product sense and user intuition which inform the questions you ask of the data you are analyzing.', 'You have prior experience with data-distributed tools (Scalding, Hadoop, Pig, etc) and statistical libraries.', "" You'll set the foundation for what data means at Notion, determining how we balance product intuition with analytical rigor. You'll build data pipelines that transform messy raw data into tables that are reliable, broadly useful, and clear. (We currently use DBT, Snowflake, Mode Analytics, Fivetran, Census, Amplitude, Segment, and more). You'll collaborate with team leads across the business to understand and support their data needs. You'll design and enact the process for using statistical inference to evaluate the success of changes, from product launches to marketing campaigns. You'll build models to understand and predict things like user growth, product engagement, conversion, churn, and more. You'll build and maintain KPI dashboards that teams rely on for making key decisions. You'll communicate your findings with the rest of the company, and drive and verify change in our product and business (Insights are useful. Impact is even better!) "", ' You have led or managed a Data Science team. You have led initiatives across multiple product areas and communicated findings with leadership and product teams. You have worked at a fast-growing start-up. You have experience building out data infrastructure that facilitates speed, reliability, and scalability. You have prior experience with data-distributed tools (Scalding, Hadoop, Pig, etc) and statistical libraries.', ""You've spent meaningful time as a data scientist."", ""You'll build and maintain KPI dashboards that teams rely on for making key decisions."", 'You have experience building out data infrastructure that facilitates speed, reliability, and scalability.', 'You know how to use statistical inference to drive product and business recommendations.', ""You'll collaborate with team leads across the business to understand and support their data needs."", "" You've spent meaningful time as a data scientist. You are a SQL expert. You have no problem regularly leveraging window functions, UDFs, self-joins, and other complex SQL functions to accomplish your data analysis goals. You have expertise in at least one scripting language (ideally Python or R). You know how to use statistical inference to drive product and business recommendations. You have experience building predictive models, and you know how to evaluate their effectiveness. You have a bias for using the right tools to get a job done with maximum efficiency. You have experience making tradeoffs between speed and accuracy. You have strong product sense and user intuition which inform the questions you ask of the data you are analyzing. "", ""You'll set the foundation for what data means at Notion, determining how we balance product intuition with analytical rigor."", ""You'll communicate your findings with the rest of the company, and drive and verify change in our product and business (Insights are useful. Impact is even better!)"", 'You have worked at a fast-growing start-up.', ""You'll build models to understand and predict things like user growth, product engagement, conversion, churn, and more."", ""What You'll Do"", 'About The Role', 'You are a SQL expert. You have no problem regularly leveraging window functions, UDFs, self-joins, and other complex SQL functions to accomplish your data analysis goals.', 'You have led initiatives across multiple product areas and communicated findings with leadership and product teams.', ""You'll design and enact the process for using statistical inference to evaluate the success of changes, from product launches to marketing campaigns."", 'Bonus Points', ""You'll build data pipelines that transform messy raw data into tables that are reliable, broadly useful, and clear. (We currently use DBT, Snowflake, Mode Analytics, Fivetran, Census, Amplitude, Segment, and more)."", 'You have led or managed a Data Science team.', ""What We're Looking For"", 'You have a bias for using the right tools to get a job done with maximum efficiency. You have experience making tradeoffs between speed and accuracy.']",Entry level,Full-time,Engineering,Information Technology and Services,2021-02-02 10:10:22
Data Engineer,Guy Carpenter,"New York, NY",5 hours ago,84 applicants,"['', 'Strong analytical skills and intellectual curiosity as demonstrated through academic experience or work assignments ', 'Construct and maintain data pipelines between GC’s databases, and other sources, with the data lake utilizing modern ETL frameworks ', 'Strong understanding of entity resolution, streaming technologies, and ELT/ETL frameworks Ability to articulate the advantages of various cloud and on-premises deployment options Experience with Master Data ManagementExperience with web scraping and crowd sourcing technologies Familiarity with modern data productivity frameworks and their alternatives such as Databricks, DataRobot, and Alteryx Experience with the MS Azure cloud environment, including ARM template deployments Strong knowledge of CI/CD principles and practical experience with a CI/CD technology (Azure Devops, GitLab, Travis, Jenkins) ', 'Good ability to prioritize workload according to volume, urgency, etc. and to deliver on required projects in a timely fashion ', ""Bachelor’s or master's degree in data science, computer science or related quantitative field such as applied mathematics, statistics, engineering, or operations research "", 'Experience working in an Agile environment to facilitate the quick and effective fulfillment of group goals ', 'Data Engineer', 'Experience deploying/maintaining cloud resources (AWS, Azure, or GCP) ', 'We will count on you to:', 'Innovate new ways to leverage enormous amounts of various datasets to drive revenues via the development of new products with the Data Strategy team, as well as the enhanced delivery of existing products ', 'Ability to articulate the advantages of various cloud and on-premises deployment options ', 'Knowledge of various industry-leading SQL and NoSQL database systems ', 'What can you expect?', ""3-5 years of relevant experience as a data engineer or in a similar role Bachelor’s or master's degree in data science, computer science or related quantitative field such as applied mathematics, statistics, engineering, or operations research Extensive experience with Spark, Python, and SQL Extensive experience integrating data from semi-structured Experience deploying/maintaining cloud resources (AWS, Azure, or GCP) Knowledge of various industry-leading SQL and NoSQL database systems Experience working in an Agile environment to facilitate the quick and effective fulfillment of group goals Good interpersonal skills for establishing and maintaining good internal relationships, working well as part of a team and for presentations and discussions Strong analytical skills and intellectual curiosity as demonstrated through academic experience or work assignments Good ability to prioritize workload according to volume, urgency, etc. and to deliver on required projects in a timely fashion "", 'Experience with Master Data Management', 'What makes you stand out?', 'Extensive experience with Spark, Python, and SQL ', 'What You Need to Have:', '3-5 years of relevant experience as a data engineer or in a similar role ', 'Strong knowledge of CI/CD principles and practical experience with a CI/CD technology (Azure Devops, GitLab, Travis, Jenkins) ', 'Experience with web scraping and crowd sourcing technologies ', 'Enforce strong development standards across the team through code reviews, unit testing, and monitoring ', 'Own the role of data steward for a variety of high value datasets and implement innovative quality assurance practices ', ""Develop, implement, and deploy custom data pipelines powering machine learning algorithms, insights generation, client benchmarking tools, business intelligence dashboards, reporting and new data products. Innovate new ways to leverage enormous amounts of various datasets to drive revenues via the development of new products with the Data Strategy team, as well as the enhanced delivery of existing products Consume data from a variety of sources (relational DBs, APIs, NetApp and other cloud storage, FTPs) & formats (excel, CSV, XML, parquet, unstructured)) Construct and maintain data pipelines between GC’s databases, and other sources, with the data lake utilizing modern ETL frameworks Own the role of data steward for a variety of high value datasets and implement innovative quality assurance practices Establish and implement metadata management standards and capabilities, including lineage mapping Enforce strong development standards across the team through code reviews, unit testing, and monitoring Perform basic data analysis within Jupyter Notebooks to validate the fulfillment of requirements for data pipelines Evangelize data strategy techniques and best practices throughout global strategic advisory Keep up-to-date on the latest trends and innovation in data technology and how these trends apply to GC's business and data strategy "", 'Establish and implement metadata management standards and capabilities, including lineage mapping ', ' R_107255', 'Develop, implement, and deploy custom data pipelines powering machine learning algorithms, insights generation, client benchmarking tools, business intelligence dashboards, reporting and new data products. ', 'Consume data from a variety of sources (relational DBs, APIs, NetApp and other cloud storage, FTPs) & formats (excel, CSV, XML, parquet, unstructured)) ', 'Strong understanding of entity resolution, streaming technologies, and ELT/ETL frameworks ', 'Evangelize data strategy techniques and best practices throughout global strategic advisory ', 'Good interpersonal skills for establishing and maintaining good internal relationships, working well as part of a team and for presentations and discussions ', 'Perform basic data analysis within Jupyter Notebooks to validate the fulfillment of requirements for data pipelines ', 'Extensive experience integrating data from semi-structured ', 'Experience with the MS Azure cloud environment, including ARM template deployments ', ""Keep up-to-date on the latest trends and innovation in data technology and how these trends apply to GC's business and data strategy "", 'Familiarity with modern data productivity frameworks and their alternatives such as Databricks, DataRobot, and Alteryx ']",Not Applicable,Full-time,Information Technology,Insurance,2021-02-02 10:10:22
Data Engineer,TechMasters corp,"Manhattan, NY",3 hours ago,Be among the first 25 applicants,"['', 'Extensive hands on with Spark, Python, and SQLExperience integrating data from semi-structuredExperience deploying/maintaining cloud resources using any AWS, Azure, or GCPKnowledge of various database systens - SQL and NoSQLSome experience or academic background in data science, applied mathematics, statistics, or operations research will be a excellent.', 'data science, applied mathematics, statistics', 'Extensive hands on with Spark, Python, and SQL', 'Detailed JD is available on request. Apply with updated resume and work visa status.', 'Skills: ETL, MDM, Cloud - AWS/Azure/GCP, SQL and NoSQL database systems, Agile, Spark, Python. ', 'operations research ', 'M hiring for my client, a fintech company having global presence. This is full time role direct hire with the client. Experience range 5-12 years.', 'Experience integrating data from semi-structured', 'Experience deploying/maintaining cloud resources using any AWS, Azure, or GCP', 'Knowledge of various database systens - SQL and NoSQL', 'Some experience or academic background in data science, applied mathematics, statistics, or operations research will be a excellent.']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-02-02 10:10:22
Data Analyst Developer,Saasvaap Inc.,"San Jose, CA",12 hours ago,Be among the first 25 applicants,"['', 'modify Teradata SQL queries and output results in Excel.', '· \xa0 \xa0 \xa0 Converting Tableau dashboard queries to Hadoop and converting Teradata SQL to Spark SQL\xa0 ', 'Data Analyst/Developer', 'Intermediate SQL, Teradata, Hadoop, prior eBay experience is a big plus.', '· \xa0 \xa0 \xa0 Support Ad-hoc requests from legal team: develop and modify Teradata SQL queries and output results in Excel.', ' ', 'Exp: 5+ years. ', 'Exp: 5+ years.', 'Data Analyst/Developer ', 'Location: San Jose/Salt Lake City/Austin Texas - Remote work in 2021. ', 'Skills required: Intermediate SQL, Teradata, Hadoop, prior eBay experience is a big plus. ', 'Responsibilities ', 'Teradata source to Hadoop', '· \xa0 \xa0 \xa0 Migrate all process and queries for the legal dashboards, SAS programs and automated SQL queries from the current Teradata source to Hadoop\xa0 ', 'Teradata SQL to Spark SQL', 'Responsibilities']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-02-02 10:10:22
Data Solutions Engineer,Epsilon,"Chicago, IL",4 hours ago,Be among the first 25 applicants,"['', 'Ability to troubleshoot production issues and solve for performance bottlenecks', 'You enjoy working with numerous programming languages, relational databases, and distributed systems. Our platform is ever evolving, but currently is a combination of Kafka, Flume, Spark, Scala, Java, Python, NoSQL (HBase, Cassandra and ScyllaDB), MPP RDBMS, Postgres, Hadoop, AWS, AirFlow, Docker, Kubernetes and Elastic', 'Keep yourself informed and up-to-date with technologies', 'Continuous improvement of our system, tests, and data quality indicators', 'Key Duties, Tasks And Responsibilities', 'Expert level skills in Python or other language (JAVA or Scala)', 'Ability to analyze data and identify business possibilities for better operational processes and business opportunities', 'Ability to thrive in a collaborative team environment', 'Interface with analysts, data scientists, and engineers to enable data oriented solutions', 'Build and maintain data quality services with Python', 'About this role', 'Build data expertise on subject matter and be able to speak to data warehouse constructs and data architecture', 'Fluent SQL with ability to ingest complex use cases, refactor and ask questions', 'Spark and ML are a plus.', 'Excellent communication skills and ability to work with the internal analyst community', 'Build and maintain data quality services with PythonFluent SQL with ability to ingest complex use cases, refactor and ask questionsContinuous improvement of our system, tests, and data quality indicatorsInfluence our technical decisionsKeep yourself informed and up-to-date with technologiesInterface with analysts, data scientists, and engineers to enable data oriented solutionsBuild data expertise on subject matter and be able to speak to data warehouse constructs and data architectureAbility to troubleshoot production issues and solve for performance bottlenecksExpert level skills in Python or other language (JAVA or Scala)Ability to analyze data and identify business possibilities for better operational processes and business opportunitiesExcellent communication skills and ability to work with the internal analyst communityAbility to thrive in a collaborative team environmentYou enjoy working with numerous programming languages, relational databases, and distributed systems. Our platform is ever evolving, but currently is a combination of Kafka, Flume, Spark, Scala, Java, Python, NoSQL (HBase, Cassandra and ScyllaDB), MPP RDBMS, Postgres, Hadoop, AWS, AirFlow, Docker, Kubernetes and ElasticInternet/Digital Advertising ecosystem knowledge is a plusSpark and ML are a plus.', 'Influence our technical decisions', 'Company Description', 'Job Description', 'Great People, Deserve Great Benefits', 'Internet/Digital Advertising ecosystem knowledge is a plus']",Not Applicable,Full-time,Information Technology,Computer Software,2021-02-02 10:10:22
Senior Data Engineer,Vanguard,"Malvern, PA",3 hours ago,Be among the first 25 applicants,"['', 'Deep technical knowledge – including proficiency in at least two of Python, SQL, Hive, Spark, Amazon Web Services / cloud computing (e.g., Elastic MapReduce, EC2, S3), Bash shell scripting', 'Qualifications', 'Participates in special projects and performs other duties as assigned.', 'Minimum of five years data analytics, programming, database administration, or data management experience.', 'Troubleshoots software and processes for data consistency and integrity. Integrates complex and large scale data from a variety of sources for business partners to generate insight and make decisions.', 'Understanding and applied knowledge of Agile delivery methodologies', 'Translates business specifications into design specifications and code. Responsible for writing complex programs, ad hoc queries, and reports. Ensures that all code is well structured, includes sufficient documentation, and is easy to maintain and reuse.', 'Inclusion Statement', 'Experience writing production quality code to create data products', 'Do you enjoy building strategic data capabilities and driving technology innovation?', 'Deep technical knowledge – including proficiency in at least two of Python, SQL, Hive, Spark, Amazon Web Services / cloud computing (e.g., Elastic MapReduce, EC2, S3), Bash shell scriptingExperience writing production quality code to create data productsAbility to effectively communicate technical concepts to non-technical audiencesUnderstanding and applied knowledge of Agile delivery methodologies', 'Tests and implements new software releases through regression testing. Identifies issues and engages with vendors to resolve and elevate software into production.', 'Undergraduate degree or equivalent combination of training and experience.', 'What We Are Looking For', 'Assesses data quality and tests code thoroughly for accuracy of intended purpose. Provides data analysis guidance and serves as a technical consultant for the client.', 'Ability to effectively communicate technical concepts to non-technical audiences', 'The team: ', 'Writes ETL (Extract / Transform / Load) processes, designs database systems and, develops tools for real-time and offline analytic processing.Troubleshoots software and processes for data consistency and integrity. Integrates complex and large scale data from a variety of sources for business partners to generate insight and make decisions.Translates business specifications into design specifications and code. Responsible for writing complex programs, ad hoc queries, and reports. Ensures that all code is well structured, includes sufficient documentation, and is easy to maintain and reuse.Partners with internal clients to gain an expert understanding of business functions and informational needs. Works closely with other technical and data analytics experts across the business to implement data solutions.Leads all phases of solution development. Explains technical considerations at related meetings, including those with internal clients and less experienced team members.Assesses data quality and tests code thoroughly for accuracy of intended purpose. Provides data analysis guidance and serves as a technical consultant for the client.Educates and develops junior data engineers on the team while applying quality control to their work. Develops data engineering standards and contributes expertise to other data expert teams across Vanguard.Tests and implements new software releases through regression testing. Identifies issues and engages with vendors to resolve and elevate software into production.Participates in special projects and performs other duties as assigned.', 'Minimum of five years data analytics, programming, database administration, or data management experience.Undergraduate degree or equivalent combination of training and experience.', 'Core Responsibilities', 'About Vanguard', 'Writes ETL (Extract / Transform / Load) processes, designs database systems and, develops tools for real-time and offline analytic processing.', 'Educates and develops junior data engineers on the team while applying quality control to their work. Develops data engineering standards and contributes expertise to other data expert teams across Vanguard.', 'Partners with internal clients to gain an expert understanding of business functions and informational needs. Works closely with other technical and data analytics experts across the business to implement data solutions.', 'Leads all phases of solution development. Explains technical considerations at related meetings, including those with internal clients and less experienced team members.']",Mid-Senior level,Full-time,Research,Financial Services,2021-02-02 10:10:22
Data Engineer,"Intelliswift Software, Inc.","Cupertino, CA",19 minutes ago,Be among the first 25 applicants,"['Description', ' Python', 'Title', ' Description: ', ' Need very strong in', ' Spark Streaming, HDFS commands , BASH', ' QA and validating pipelines, lot of monitoring', 'Title: Data Engineer', ' Airflow', ' Data analytics', ' Location: Cupertino/Remote, CA, United States', ' AWS', ' Linux, AWS with Spark on top', ' ', ' SQL', ' HDFS Commands, bash', 'Top Skills: SQL, Python, Spark, Airflow', ' Not creating pipelines, they already have created, adding new features, ingesting new tables, validating those ', 'Location']",Mid-Senior level,Contract,Engineering,Information Technology and Services,2021-02-02 10:10:22
Data Engineer,Zennify,"Sacramento, CA",16 hours ago,Be among the first 25 applicants,"['', 'Thrives in a team-based, high energy and fast-paced environmentService-oriented and innately driven to produce outstanding customer satisfaction and resultsEnjoys discovering, learning about and implementing new technologiesAnalytical and able to logically and methodically work through problemsStrong aptitude for prioritization and multitasking in a deadline-driven environmentPossess a sense of urgency with strong organizational and follow-up skills\xa0Ability to meet travel requirements, when applicable (up to 25% travel projected)', 'Examine and identify database structural necessities by evaluating client operations, applications, and programming', 'To succeed in this role, you should know how to examine new data system requirements and implement migration models. The ideal candidate will also have proven experience in data analysis and management, with excellent analytical and problem-solving abilities.', 'Significant experience with SOAP & REST API Integration, data, and security', 'Lead and mentor the data team on the project', 'Possess a sense of urgency with strong organizational and follow-up skills\xa0', 'Lives the company’s core values; shows integrity, transparency, and reliability', 'These technologies include but are not limited to: Salesforce.com products and APIs, the Salesforce data model, Amazon Web Services (AWS), Heroku, integration/ETL technologies. You will also maintain an ongoing comprehensive understanding of data migration/integration tools, patterns and best practices.', 'Ability to develop extensible and flexible data models based on long term strategy, and articulate benefit of reuse', 'Ability to reverse engineer existing data models into a conceptual, logical data model constructs', 'Bachelor’s degree in Computer Science, Information Systems,\xa0or relevant field strongly preferred', ""Specializes in gathering and analyzing information and designing comprehensive solutions that are flexible and adaptable to the client's needs"", 'Preferred Qualifications', 'Work closely with Delivery Managers, Functional Solution Architects, Technical Architects and clients in order support the technology solution architecture through applying data architecture principles', 'Design and implement data migration and integration solutions and data models to store and retrieve data', 'Open minded to collaborate with various team members and able to give and handle constructive feedback', 'Conduct data requirements working sessions focused on data object mapping, data transformation and planning directly with the functional architect, technical architect, and client stakeholders', 'The Data Engineer is responsible for, but not limited to the following: data modeling, data migration and data integration, ETL tooling, data best practices, data movement, building and optimizing “big data” data pipelines and architectures, and the building of end to end data solutions for our customers.\xa0Your duties may include preparing architect reports, monitoring the system, supervising system migrations, and performing root cause analysis on external and internal processes and data to identify opportunities for improvement.\xa0', 'Strong conceptual and logical data modeling skills', 'Leadership Skills:', 'Significant experience with writing unit tests', 'About Us', 'Hands-on experience with data architecture tools, such as Talend, Jitterbit, Capstorm, Informatica; or others such as Hyperion, Erwin, SQL Server; etc. are helpful\xa0', 'Perform and/or lead necessary unit testing on all developed data scripts', 'Offer support by responding to system problems in a timely manner', 'Proficient at collaboration and working with members of a team', 'Work directly with QA team to review UAT test cases in an effort to ensure accurate data is migrated and available for testing', 'Experience with migrating data into Salesforce and understanding the Salesforce data model', 'Identify and lead internal strategic initiatives to grow the Data Integration services offering; play an active role developing the knowledgebase and expertise of the Data team', 'Monitor the system performance by performing regular tests, troubleshooting and integrating new features', 'Embodies Zennify culture; a team player that everyone enjoys working with', 'Analytical and able to logically and methodically work through problems', 'Highly detail-oriented individual with the ability to rapidly learn and take advantage of new concepts, business models, and technologies', 'Educate staff members through training and individual support', 'Balancing industry leading Salesforce technical expertise with clear purpose and line of sight to our client’s business objectives, we deliver innovative and contextual solutions that scale.\xa0', 'Prepare accurate database design and architectural design documentation', 'Our purpose is to provide opportunities for professionals to develop amazing careers that drive compelling business impact. We promote and grow from within, building on strengths and investing in development.', 'Service-oriented and innately driven to produce outstanding customer satisfaction and results', 'Own and drive data architecture solutions, technology and web flows', 'Mentor other ETL/Data team members and maintain best practices are followed within the team', '8+ years experience in developing technology solutions', 'Confidently and effectively facilitates and presents; ably defends point-of-view; keeps audiences engaged and delivers a clear and memorable message', 'A data engineer certification, e.g. Google’s Certified Professional Data Engineer, IBM Certified Data Engineer, CCP Data Engineer for Cloudera, MCSE/MCSE: Data Management and Analytics, is a plus.', 'Experience using JIRA or similar software to manage user stories and workload', 'Support the successful delivery of the proposed technical solution, including data migration/integration scripts used in one time migrations, performing migrations, ongoing integrations, etc.', 'Ability to work independently and be a self-starter', 'Technical Requirements', 'Oversee the migration of data from legacy systems to new solutions', 'Development and refinement of production deployment documentation related to data migrations and integrations', 'Experience establishing data pipelines for large data sets.', 'Founded in 2013, Zennify is a Platinum Salesforce consulting partner focused in Financial Services, Health and Life Sciences, and other key industries that prioritize the customer experience. With consistently high Customer Satisfaction scores, we live to provide epic solutions that solve our customers’ challenges, and exceed expectations! Our commitment to People Development and Customer Success are the driving forces behind our firm.', 'Thrives in a team-based, high energy and fast-paced environment', 'Inspire Passion, Embrace Equality, Be Authentic, Integrity Matters.', 'Speaks and writes with clarity, brevity, and purpose; explains area of expertise clearly and confidently to others; influences and engages C-Level with authority and confidence', 'Self-aware and strategic thinker; proficient at building strong relationships', 'Assess database implementation procedures to ensure they comply with internal and external regulations', 'Experience overseeing team members', 'Work closely with Delivery Managers, Functional Solution Architects, Technical Architects and clients in order support the technology solution architecture through applying data architecture principlesExamine and identify database structural necessities by evaluating client operations, applications, and programmingConduct data requirements working sessions focused on data object mapping, data transformation and planning directly with the functional architect, technical architect, and client stakeholdersCollaborate with functional and technical architect(s) to help finalize the Salesforce data model necessary for successful project deliveryServe as a trusted advisor to the client and recommend solutions to improve new and existing database systemsBuilding analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition.Design and implement data migration and integration solutions and data models to store and retrieve dataSupport the successful delivery of the proposed technical solution, including data migration/integration scripts used in one time migrations, performing migrations, ongoing integrations, etc.Oversee the migration of data from legacy systems to new solutionsAssess database implementation procedures to ensure they comply with internal and external regulationsPrepare accurate database design and architectural design documentationMonitor the system performance by performing regular tests, troubleshooting and integrating new featuresPerform and/or lead necessary unit testing on all developed data scriptsWork directly with QA team to review UAT test cases in an effort to ensure accurate data is migrated and available for testingPerform and/or lead project data consultants in dry run data load testingDevelopment and refinement of production deployment documentation related to data migrations and integrationsLead and mentor the data team on the projectEnforce sound data migration and integration best practices in order to ensure the quality of deliverables and scalabilityIdentify and lead internal strategic initiatives to grow the Data Integration services offering; play an active role developing the knowledgebase and expertise of the Data teamOwn and drive data architecture solutions, technology and web flowsEducate staff members through training and individual supportMentor other ETL/Data team members and maintain best practices are followed within the teamOffer support by responding to system problems in a timely manner', 'Understanding translation between logical data structures & physical database objects', 'Passionate about Customer Success', '5+ years experience in managing external client projects from a data solution perspective', 'Zennify is looking for a qualified candidate to join their team as a\xa0Data Engineer! The Data Engineer\xa0will work directly with our customers to not only enable them to be successful but to also help guide them through high quality data driven practices and project scope.\xa0You will work with them and our delivery teams to develop, optimize and oversee our client’s conceptual and logical data systems.\xa0', 'Serve as a trusted advisor to the client and recommend solutions to improve new and existing database systems', 'Significant experience with Git and standard branching strategies', 'Significant Java development experience', 'Leads internal initiatives; actively contributes to the Community’s knowledge and resource base', 'Expected to be able use various data modeling tools and processes as required', 'Hands-on experience with data architecture tools, such as Talend, Jitterbit, Capstorm, Informatica; or others such as Hyperion, Erwin, SQL Server; etc. are helpful\xa0Understanding translation between logical data structures & physical database objectsExperience with migrating data into Salesforce and understanding the Salesforce data modelExperience establishing data pipelines for large data sets.Significant Java development experienceSignificant experience with SOAP & REST API Integration, data, and securitySignificant experience with writing unit testsSignificant experience with Git and standard branching strategiesExperience using JIRA or similar software to manage user stories and workloadHas experience with and is currently willing to be hands on contributing developer in addition to leadership responsibilities', 'Zennify’s mission is to be problem solvers and relationship builders, striving to create solutions, opportunities and sustained success for our people, customers, community and future generations. We aspire to be the most trusted, impactful and inspirational advisor in the consulting ecosystem.\xa0', 'Has experience with and is currently willing to be hands on contributing developer in addition to leadership responsibilities', 'Possesses strong knowledge of data modeling principles and best practices', 'Building analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition.', 'Enjoys discovering, learning about and implementing new technologies', 'Cutting edge innovator who continually studies new technologies and functionality, and is involved in projects that push the capabilities of existing technologies', 'Primary Responsibilities', 'Enforce sound data migration and integration best practices in order to ensure the quality of deliverables and scalability', 'Perform and/or lead project data consultants in dry run data load testing', 'Ability to meet travel requirements, when applicable (up to 25% travel projected)', 'Strong aptitude for prioritization and multitasking in a deadline-driven environment', 'Qualities of the Ideal Candidate', 'Passionate about Customer SuccessAlways learning; approaches each interaction with open mind; great listener and hands-onSelf-aware and strategic thinker; proficient at building strong relationshipsSpeaks and writes with clarity, brevity, and purpose; explains area of expertise clearly and confidently to others; influences and engages C-Level with authority and confidenceConfidently and effectively facilitates and presents; ably defends point-of-view; keeps audiences engaged and delivers a clear and memorable messageProficient at collaboration and working with members of a teamAbility to move fast and drive business value and resultsEmbodies Zennify culture; a team player that everyone enjoys working withLives the company’s core values; shows integrity, transparency, and reliabilityLeads internal initiatives; actively contributes to the Community’s knowledge and resource base', 'Collaborate with functional and technical architect(s) to help finalize the Salesforce data model necessary for successful project delivery', ""Bachelor’s degree in Computer Science, Information Systems,\xa0or relevant field strongly preferred8+ years experience in developing technology solutions5+ years experience in managing external client projects from a data solution perspectiveA data engineer certification, e.g. Google’s Certified Professional Data Engineer, IBM Certified Data Engineer, CCP Data Engineer for Cloudera, MCSE/MCSE: Data Management and Analytics, is a plus.Ability to reverse engineer existing data models into a conceptual, logical data model constructsStrong conceptual and logical data modeling skillsPossesses strong knowledge of data modeling principles and best practicesExpected to be able use various data modeling tools and processes as requiredOpen minded to collaborate with various team members and able to give and handle constructive feedbackSpecializes in gathering and analyzing information and designing comprehensive solutions that are flexible and adaptable to the client's needsAbility to develop extensible and flexible data models based on long term strategy, and articulate benefit of reuseAble to design integrated data model based on understanding of business, and use of abstract conceptsExperience overseeing team membersHighly detail-oriented individual with the ability to rapidly learn and take advantage of new concepts, business models, and technologiesAbility to work independently and be a self-starterCutting edge innovator who continually studies new technologies and functionality, and is involved in projects that push the capabilities of existing technologies"", 'As professionals we put in the work and are committed to milestones, but at Zennify having fun and giving back are part of the journey. We offer a culture that inspires, a place people love coming to work, and believe in open communication.\xa0It’s all part of the Zenn!\xa0\xa0', 'Always learning; approaches each interaction with open mind; great listener and hands-on', 'Ability to move fast and drive business value and results', 'Able to design integrated data model based on understanding of business, and use of abstract concepts']",Mid-Senior level,Full-time,Information Technology,Computer Software,2021-02-02 10:10:22
Data Engineer,Dice,"Hillsboro, OR",3 hours ago,Be among the first 25 applicants,[''],Entry level,Full-time,Information Technology,Information Technology and Services,2021-02-02 10:10:22
