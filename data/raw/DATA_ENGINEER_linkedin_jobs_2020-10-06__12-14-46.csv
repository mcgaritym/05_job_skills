job_title,company,location,date_posted,applicants,job_text,seniority_level,employment_type,job_function,industries,date_scraped
Data Engineer - REMOTE,Optello,"Remote, OR",1 day ago,27 applicants,"['', ' Build upon existing data service architecture to support internal and external applications', 'Please do NOT change the email subject line in any way. You must keep the JobID: linkedin : TB10-1597099 -- in the email subject line for your application to be considered.***', 'Job Duration:', ' Experience with Cassandra databases', ' Amazing healthcare benefits', 'Bonus Skills', ' RESTful API and server-side API integration experience', ' Stipend for workspace', 'Requirements:', 'Optello is proud to be an Equal Opportunity Employer', ' Working in a remote setting with virtual team meetings on a regular basis', ' Competitive starting salary Amazing healthcare benefits 401k + match Generous PTO Equipment provided Stipend for workspace', ' 7+ years of Java programming experience 5+ years of Python programming experience 5+ years of SQL database experience Strong verbal and written communication experience RESTful API and server-side API integration experience', ' Childcare assistance', ' Experience with Cassandra databases Additional NoSQL database experience Experience with Maria databases Salesforce integration experience', ' Work supporting ETL pipelines, real time streams, and data warehouses', ' Design and implement process improvements, automate manual processes, and redesign infrastructure', ' Equipment provided', 'Job Location:', ' Build scalable pipelines capable of processing massive amounts of data', ' Generous PTO', 'Must Have Skills', 'Your Right to Work', ' Build scalable pipelines capable of processing massive amounts of data Build upon existing data service architecture to support internal and external applications Work supporting ETL pipelines, real time streams, and data warehouses Design and implement process improvements, automate manual processes, and redesign infrastructure Build upon on the infrastructure required to optimize the extraction, transformation, and loading of data using Java, Python, SQL Working in a remote setting with virtual team meetings on a regular basis Collaborate with business intelligence and analytics teams to optimize Tableau report queries', ' Collaborate with business intelligence and analytics teams to optimize Tableau report queries', ' Competitive starting salary', ' Catered breakfast and lunch each week', ' 5+ years of Python programming experience', ' Experience with Maria databases', ' Strong verbal and written communication experience', ' Additional NoSQL database experience', ' 401k + match', ' Catered breakfast and lunch each week Fully stocked snacks Pet-friendly office Childcare assistance', ' 7+ years of Java programming experience', 'Job Title:', ' Build upon on the infrastructure required to optimize the extraction, transformation, and loading of data using Java, Python, SQL', 'Email Your Resume In Word To', ' 5+ years of SQL database experience', ' Salesforce integration experience', ' Pet-friendly office', ' Fully stocked snacks']",Entry level,Full-time,Information Technology,Construction,2020-10-06 12:14:13
Data & Reporting Engineer,Nike,"Boston, MA",7 hours ago,37 applicants,"['', ' Interact with many types of data in both SQL and non-SQL based environments.', ' Solid SQL skills; ability to query data in a data warehouse and prepare data for reporting and insights automation needs. Experience with using Python/Pandas for data preparation/cleansing and analysis ideal but not required.', ' Experience working in an agile setting, working with developers to release in an iterative fashion.', ' Partner with stakeholders in Global Finance to identify key reports, business questions, hypotheses, and areas of exploration that will have a material impact on business decision-making, and translate these areas into requirements for insights, reports and data visualizations.', ' Strong SQL ability; able to write complex SQL queries against data warehouse/databases for the purposes of extracting required data and building data models for automated reporting and insights use case. Able to parse different types of data (JSON, parquet, excel/csv) for analysis using tools like R or Python. Able to quickly visualize and profile data in tools like Tableau, PowerPivot, etc.', ' Interact with many types of data in both SQL and non-SQL based environments. Parse data in common formats such as JSON, Parquet, excel/csv so that it can be usable for insights and analysis. Partner with the Converse Analytics Technology team to define data requirements and source data, either in terms of raw data or modeled data and logic to support insights, dashboard and report requirements.', 'Qualifications', ' Work as part of an agile team, delivering reports and insights in an iterative manner with a “progress over perfection” mindset.', ' Strong SQL ability; able to write complex SQL queries against data warehouse/databases for the purposes of extracting required data and building data models for automated reporting and insights use case.', ' Provide front-line support for data visualizations, dashboards and reports, answering user questions and resolving issues. Work with the Analytics Technology team to address back-end data or platform issues affecting reports.', ' Able to parse different types of data (JSON, parquet, excel/csv) for analysis using tools like R or Python.', ' Able to quickly visualize and profile data in tools like Tableau, PowerPivot, etc.', ' Automation mindset; Identify opportunities to automate insights generation work and ability to partner with Technology teams to implement solutions where possible.', ' Strong communication skills; Ability to engage with stakeholders on key business questions and hypotheses and tell compelling stories with data through visualization and reporting tools.', ' Experience working in a business setting; Experience working in a Corporate Finance and/or Controlling context and familiarity with key Finance terms, KPIs and typical reports strongly preferred.', ' Partner with the Converse Analytics Technology team to define data requirements and source data, either in terms of raw data or modeled data and logic to support insights, dashboard and report requirements.', ' Review, assess and automate the existing set of reports that are being run across Global Finance; Reports are run across a variety of tools, such as Business Objects, Excel and Tableau.', ' Partner with stakeholders in Global Finance to identify key reports, business questions, hypotheses, and areas of exploration that will have a material impact on business decision-making, and translate these areas into requirements for insights, reports and data visualizations. Review, assess and automate the existing set of reports that are being run across Global Finance; Reports are run across a variety of tools, such as Business Objects, Excel and Tableau. Provide front-line support for data visualizations, dashboards and reports, answering user questions and resolving issues. Work with the Analytics Technology team to address back-end data or platform issues affecting reports.', "" Bachelor's degree in Information Systems, Information Technology or Computer Science or equivalent professional experience; MBA or other graduate specialization in Finance and/or Accounting preferred Solid SQL skills; ability to query data in a data warehouse and prepare data for reporting and insights automation needs. Experience with using Python/Pandas for data preparation/cleansing and analysis ideal but not required. Experience with a variety of reporting and data visualization tools; Tableau, Business Objects, and PowerPivot preferred. Proven track record building and automating reports, data visualizations and dashboards in a business context for a variety of stakeholders, from leadership oriented summary dashboards to more granular and detailed analyses. Strong communication skills; Ability to engage with stakeholders on key business questions and hypotheses and tell compelling stories with data through visualization and reporting tools. Automation mindset; Identify opportunities to automate insights generation work and ability to partner with Technology teams to implement solutions where possible. Experience working in an agile setting, working with developers to release in an iterative fashion. Experience working in a business setting; Experience working in a Corporate Finance and/or Controlling context and familiarity with key Finance terms, KPIs and typical reports strongly preferred."", "" Bachelor's degree in Information Systems, Information Technology or Computer Science or equivalent professional experience; MBA or other graduate specialization in Finance and/or Accounting preferred"", ' Experience with a variety of reporting and data visualization tools; Tableau, Business Objects, and PowerPivot preferred. Proven track record building and automating reports, data visualizations and dashboards in a business context for a variety of stakeholders, from leadership oriented summary dashboards to more granular and detailed analyses.', ' Parse data in common formats such as JSON, Parquet, excel/csv so that it can be usable for insights and analysis.', ' Strong business analysis capabilities; Ability to ask go beyond reporting and ask “why”; Strong drive to drill into data and uncover root causes. Strong ability to “tell stories” with data to business stakeholders, particularly stakeholders within our Finance and Controlling organizations. Work as part of an agile team, delivering reports and insights in an iterative manner with a “progress over perfection” mindset.', ' Strong ability to “tell stories” with data to business stakeholders, particularly stakeholders within our Finance and Controlling organizations.', ' Strong business analysis capabilities; Ability to ask go beyond reporting and ask “why”; Strong drive to drill into data and uncover root causes.']",Entry level,Full-time,Information Technology,Apparel & Fashion,2020-10-06 12:14:13
Data Engineer,The Asbury Group Integrated Technologies,"Frederick, MD",16 hours ago,Be among the first 25 applicants,"['', ""Find purpose in your career!\xa0The Asbury Group Integrated Technologies is an IT consulting arm of Asbury Communities, Inc.\xa0completely focused on doing all the good we can for those we serve.\xa0When you join our family, you'll enjoy the personal fulfillment that comes from making a difference in someone's life every day.\xa0Asbury\xa0is honored to have earned certification as a Great Place to Work for three years in a row based on associate feedback\xa0to questions related to trust, culture, and the meaning they derive from their jobs."", 'We are currently seeking talented and highly motivated Data Scientists or Data Engineers to lead in the development of our discovery and support platform. The successful candidate will join a small, global team of data focused associates that have successfully built, and maintained a best of class traditional, Kimball based, SQL server founded, data warehouse.\xa0The successful candidate will lead the conversion of the existing data structure into an AWS focused, big data framework and assist in identifying and pipelining existing and augmented data sets into this environment.\xa0In addition, the successful candidate will assist leadership in growing the team required to complete the development of various computational algorithms associated with aging in place.\xa0The successful candidate can be either data engineering or science focused but must be able to lead and assist in architecting and constructing the AWS foundation and initial data ports.\xa0\xa0\xa0\xa0', 'Contact us to learn more about this fantastic opportunity!', 'Ensuring that we provide a safe and healthy environment for those who work and live at our communities is our highest priority.\xa0We are proud of the incredible work being done by our associates and grateful for the support we are receiving from our many residents and family members as we work through this pandemic together.\xa0We continue to aggressively work to minimize the risks of the COVID-19 virus for\xa0residents and associates in our communities.']",Associate,Full-time,Information Technology,Information Technology and Services,2020-10-06 12:14:13
Data Engineer,Conduent,United States,8 hours ago,Be among the first 25 applicants,"['', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience in cloud-based ETL development processes.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Learn and develop new ETL techniques as required to keep up with contemporary technologies.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Worked in big data environments, cloud data stores, different RDBMS, and OLAP solutions.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Support presentations to Customers and Partners', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Advising on new technology trends and possible adoption to maintain a competitive advantage', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0A BS or Masters degree in Computer Science or related technical discipline is required', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Excellent SQL coding experience with performance optimization for data queries.', '\xa0', 'Prior experience with application delivery using an Onshore/Offshore model', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa05+ years of related experience is required.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Develop/maintain efficient data collection systems and sound strategies for getting quality data from different sources', 'Demonstrated ability to have successfully completed multiple, complex technical projectsPrior experience with application delivery using an Onshore/Offshore modelExperience with business processes across multiple Master data domains in a services-based companyDemonstrates a rational and organized approach to the tasks undertaken and an awareness of the need to achieve quality.Demonstrates high standards of professional behavior in dealings with clients, colleagues and staff.Is able to make sound and far-reaching decisions alone on major issues and to take full responsibility for them on a technical basis.Strong written communication skills. Is effective and persuasive in both written and oral communication.Experience with gathering end-user requirements and writing technical documentationTime management and multitasking skills to effectively meet deadlines under time-to-market pressureRequires some travel (on average 10%-20%)', 'Experience with gathering end-user requirements and writing technical documentation', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Performs data management tasks, such as to conduct data profiling, assess data quality, and write SQL queries to extract and integrate data', 'Demonstrates a rational and organized approach to the tasks undertaken and an awareness of the need to achieve quality.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience connecting to varied data sources', 'Time management and multitasking skills to effectively meet deadlines under time-to-market pressure', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Has a strong technical background and remains evergreen with technology and industry developments.', 'Additional Requirements', 'Demonstrates high standards of professional behavior in dealings with clients, colleagues and staff.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Collaborate in design reviews and code reviews to ensure standards are met. Recommend new standards for visualizations.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Is familiar with the principles and practices involved in development and maintenance of software solutions and architectures and service delivery.', 'Strong written communication skills. Is effective and persuasive in both written and oral communication.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Engineer and maintain a modern Cloud data pipeline to collect, organize, and process data from disparate sources.', 'Fulltime Remote position ', 'Responsibilities:', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Consume and analyze data from the data pool to support inference, prediction, and recommendation of actionable insights to support business growth.', 'Experience Needed:', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Design, develop, and maintain ETL processes using tools and scripting. Troubleshoot and debug ETL processes. Performance tuning and optimization of the ETL processes.', 'Is able to make sound and far-reaching decisions alone on major issues and to take full responsibility for them on a technical basis.', 'Experience with business processes across multiple Master data domains in a services-based company', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Provide support to new or existing applications while recommending best practices and leading projects to implement new functionality.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience in the deployment and maintenance of ETL Jobs.', 'Demonstrated ability to have successfully completed multiple, complex technical projects', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Understands different data models like normalized, de-normalied, stars, and snowflake models. Worked with transactional, temporal, time series, and structured and unstructured data.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Reviews the solution requirements and architecture to ensure the selection of appropriate technology, efficient use of resources, and integration of multiple systems and technology.', 'We are looking for a Data Engineer who will be part of our Analytics Practice and will be expected to actively work in a multi-disciplinary fast-paced environment. This role requires a broad range of skills and the ability to step into different roles depending on the size and scope of the project; its primary responsibility is the development and maintenance of data pipelines including acquisition, transformation, loading, and processing of data.\xa0', 'Data Engineer', 'Requires some travel (on average 10%-20%)', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0ETL experience with data integration to support data marts, extracts, and reporting']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2020-10-06 12:14:13
Data Engineer (Remote),Robert Half,"Dallas, TX",1 hour ago,Be among the first 25 applicants,"['', 'Description', ' Experience implementing automated ETL workflows ', ' Experience in SQL queries, creating data models ', ' Experience with PythonRobert Half Technology matches IT professionals with some of the best companies on a temporary, project or full-time basis. From roles in software and applications to IT infrastructure and operations, we provide you unparalleled access to exciting career opportunities. Our personalized approach, innovative matching technology and global network with local market expertise help you find the technology jobs that match your skills and priorities — fast. By working with us, you have access to challenging opportunities, competitive compensation and benefits, and training to enhance your skill sets.From philanthropy to environmental stewardship to employee programs, Robert Half is proud to have an active role in the communities in which we live and work. Our company has appeared on FORTUNE’s “Most Admired Companies” list every year since 1998.Download our mobile app to take your job search on the go!Contact your local Robert Half Technology office at 888.490.4429 or visit www.roberthalf.com/jobs/technology to apply for this job now or find out more about other job opportunities.All applicants applying for U.S. job openings must be authorized to work in the United States. All applicants applying for Canadian job openings must be authorized to work in Canada.© 2020 Robert Half Technology. An Equal Opportunity Employer M/F/Disability/Veterans.', ' Bachelor’s degree or equivalent working experience ', 'Requirements']",Entry level,Temporary,Information Technology,Information Technology and Services,2020-10-06 12:14:13
Data Engineer,Facebook,"Menlo Park, CA",14 hours ago,Be among the first 25 applicants,"['', 'Experience working with cloud or on-prem Big Data/MPP analytics platform(i.e. Netezza, Teradata, AWS Redshift, Google BigQuery, Azure Data Warehouse, or similar).', '3+ years experience with Data Modeling.', 'Partner with leadership, engineers, program managers and data scientists to understand data needs.', 'Experience with designing and implementing real-time pipelines.', '3+ years of experience with workflow management engines (i.e. Airflow, Luigi, Prefect, Dagster, digdag.io, Google Cloud Composer, AWS Step Functions, Azure Data Factory, UC4, Control-M).', 'Partner with leadership, engineers, program managers and data scientists to understand data needs.Design, build and launch extremely efficient and reliable data pipelines to move data across a number of platforms including Data Warehouse, online caches and real-time systems.Communicate, at scale, through multiple mediums: Presentations, dashboards, company-wide datasets, bots and more.Educate your partners: Use your data and analytics experience to ‘see what’s missing’, identifying and addressing gaps in their existing logging and processes.Leverage data and business principles to solve large scale web, mobile and data infrastructure problems.Build data expertise and own data quality for your areas.', 'Communicate, at scale, through multiple mediums: Presentations, dashboards, company-wide datasets, bots and more.', 'Educate your partners: Use your data and analytics experience to ‘see what’s missing’, identifying and addressing gaps in their existing logging and processes.', 'Experience with Airflow.', 'Experience with more than one coding language.Experience with designing and implementing real-time pipelines.Experience with data quality and validation.Experience with SQL performance tuning and E2E process optimization.Experience with anomaly/outlier detection.Experience with notebook-based Data Science workflow.Experience with Airflow.Experience querying massive datasets using Spark, Presto, Hive, Impala, etc.', 'Experience with SQL performance tuning and E2E process optimization.', '5+ years of SQL experience.', 'Design, build and launch extremely efficient and reliable data pipelines to move data across a number of platforms including Data Warehouse, online caches and real-time systems.', '5+ years experience in custom ETL design, implementation and maintenance.', 'Experience analyzing data to discover opportunities and address gaps.', 'Build data expertise and own data quality for your areas.', 'Experience querying massive datasets using Spark, Presto, Hive, Impala, etc.', 'Leverage data and business principles to solve large scale web, mobile and data infrastructure problems.', 'Experience with anomaly/outlier detection.', 'Experience with data quality and validation.', '5+ years of Python development experience.5+ years of SQL experience.3+ years of experience with workflow management engines (i.e. Airflow, Luigi, Prefect, Dagster, digdag.io, Google Cloud Composer, AWS Step Functions, Azure Data Factory, UC4, Control-M).3+ years experience with Data Modeling.Experience analyzing data to discover opportunities and address gaps.5+ years experience in custom ETL design, implementation and maintenance.Experience working with cloud or on-prem Big Data/MPP analytics platform(i.e. Netezza, Teradata, AWS Redshift, Google BigQuery, Azure Data Warehouse, or similar).', '5+ years of Python development experience.', 'Experience with notebook-based Data Science workflow.', 'Experience with more than one coding language.']",Associate,Full-time,Information Technology,Internet,2020-10-06 12:14:13
Data Engineer,DISYS,"Richmond, VA",2 hours ago,Be among the first 25 applicants,"['', 'Preferred Qualifications:', 'TECHNICAL EXPERIENCE: At least 5 years experience with Java, Python, Spring, Kafka, Postgres ', 'Create and Manage Springs Data Pipeline - Take data from multiple disparate sources (Postgres, OneLake or Snowflake, CMS etc), transform and store them in a single place that represents the data uniformly as a single source of truth ', 'EDUCATION: Bachelors degree (any subject) or military experience ', 'AGILE DEVELOPMENT: Knowledge and experience in Agile development processes ', 'Job Description: ""Spring Data Infrastructure - Data Engineer\xa0 ', 'EDUCATION: Masters degree (any subject) ', 'Basic Qualifications: ', 'DATA EXPERIENCE: At least 5 years prior experience as a Data Engineer/Architect ', 'What you bring to the table:', '\xa0 ', 'Own Data Quality - Spearhead the identification of data issues and formulate solutions for data issues and bugs ', 'AGILE EXPERIENCE: At least 1 year demonstrable experience working in an Agile environment ', 'TECHNICAL EXPERIENCE: At least 3 years experience with Java, Python, Spring, Kafka, Postgres ', 'As a Data Engineer with our team, you would:', 'CREATIVE ANALYSIS: Flexible, adaptable, and creative analytical skills ', 'Participate in Feature development with Software Engineering team - Engage with Product and Data Analysis teams to groom out requirements for the data implications of new Feature development ', 'COLLABORATION: The ability to collaborate closely with key stakeholders, including business partners and engineering or development teams ', 'While not required, any of the following will be considered favorably and given extra weight in evaluating candidates for the role: ', 'DATA ENGINEER EXPERIENCE: Proven experience and success as a Data Evangelist; experience in working with diverse data systems and teams to deliver elegant and error-free data solutions ', 'As a Data Engineer with our team, you would: ', 'Basic Qualifications:', ' ', 'DATA EXPERIENCE: At least 7 years prior experience as a Data Engineer/Architect ', 'COMMUNICATION: Outstanding listening and presentation skills, honed by hearing customers and stakeholders, and sharing key communications with peers, management, and executives ', 'Preferred Qualifications: ', 'AGILE EXPERIENCE: At least 3 years demonstrable experience working with data in an Agile environment, plus familiarity with the work and collaboration tools used in an Agile environment  (such as Confluence, Jira, Rally, or other similar tools) ', 'Job Description: ""Spring Data Infrastructure - Data Engineer\xa0', 'DATA TRAINING/CERTIFICATION: Data Engineering training and/or certification, either as part of an undergraduate or masters-degree program, or as training gained while entering the data field', 'What you bring to the table: ']",Mid-Senior level,Contract,Information Technology,Banking,2020-10-06 12:14:13
Data Engineer,WeWork,"New York, NY",13 hours ago,Be among the first 25 applicants,"['', 'Develop tools supporting self-service data pipeline management (ETL)', 'You do what you love!', ""You have the flexibility to think outside the box.We don't do everything the traditional way, and are always looking to innovate and push the envelope.You have the ability to foresee and identify needs of the team.You take an innovator and creator’s approach to any issues that may arise.You take initiative and identify ways in which we can improve our data product and service."", 'You Will', 'Leading / managing full project lifecyclesSalesforce and related toolsWorkflow management tools (Airflow, Luigi, etc..)Working in a cloud environment (AWS, GCP, Snowflake, etc)KafkaDBTDistributed execution frameworks (Spark, Apache Beam, etc.)', 'Doer', 'Experience with modern BI tools (Looker, Tableau, etc)', 'Strong communication skills, empathy and initiative3+ years of experience in data engineeringStrong background in data warehouse concept and designProficient in at least one of the SQL dialects (Snowflake, MySQL, PostgreSQL, SqlServer, Oracle)Good understanding of SQL Engine and able to conduct advanced performance tuningFamiliar with at least one scripting language (Python, Ruby, Perl, Bash)Experience with Git and the pull request workflowExperience working closely with Analytics/Data Science teamsExperience with modern BI tools (Looker, Tableau, etc)', 'Strong background in data warehouse concept and design', 'Credibility is earned at WeWork through execution and getting things done.', 'You take initiative and identify ways in which we can improve our data product and service.', 'What You Will Do', 'You do what you love!Credibility is earned at WeWork through execution and getting things done.You are able to get into the details and deliver results under highest expectations on time and quality.Be ready to get hands-on with all aspects of the daily needs. The buck stops with you.Pragmatism and outcomes orientation are valued and lead to wins.Exceptional organizational and multitasking skills.You thrive in a fast-paced environment.You are resilient and can adapt to a changing environment.', 'You must present well and communicate clearly and effectively to upper management and internal departments.', 'Proficient in at least one of the SQL dialects (Snowflake, MySQL, PostgreSQL, SqlServer, Oracle)', 'You are able to get into the details and deliver results under highest expectations on time and quality.', 'Workflow management tools (Airflow, Luigi, etc..)', 'You’re excited to solve data challenges that combines digital and physical aspects, across multiple business verticals like Real Estate, Finance, Sales & Marketing, Social Media, and regions', 'You are resilient and can adapt to a changing environment.', ""You're motivated to enable and collaborate with engineering, analytics and product teams to tackle the most challenging business needs"", 'Experience working closely with Analytics/Data Science teams', 'There is no room for “I” at WeWork. Every role and individual is in the organization to serve We.Builds trust across the organization by being a good listener and inclusively soliciting input.You are open to new and innovative solutions.You must present well and communicate clearly and effectively to upper management and internal departments.You’re willing to adjust course when appropriate new ideas or objections are raised.You love working with people!', 'You love working with people!', ""We don't do everything the traditional way, and are always looking to innovate and push the envelope."", 'Distributed execution frameworks (Spark, Apache Beam, etc.)', 'Experience with Git and the pull request workflow', 'Working in a cloud environment (AWS, GCP, Snowflake, etc)', 'There is no room for “I” at WeWork. Every role and individual is in the organization to serve We.', 'DBT', 'Builds trust across the organization by being a good listener and inclusively soliciting input.', 'Salesforce and related tools', 'Kafka', 'Be ready to get hands-on with all aspects of the daily needs. The buck stops with you.', 'You thrive in a fast-paced environment.', 'You have the ability to foresee and identify needs of the team.', 'Leading / managing full project lifecycles', 'You are open to new and innovative solutions.', 'Collaborator', 'Develop centralized source of truth data sets to encourage a democratized, data-driven culture', 'You’re willing to adjust course when appropriate new ideas or objections are raised.', 'Requirements', 'Collaborate on improving efficiency and quality of internal data processes as well as stakeholder engagement, including implementation of system and model quality tracking ', 'Familiar with at least one scripting language (Python, Ruby, Perl, Bash)', 'Solution-centric', 'Pragmatism and outcomes orientation are valued and lead to wins.', 'Who You Are', 'Collaborate closely with data engineers, analysts,data scientists, ML Engineers, as well as cross-functional teams, to leverage huge amounts of WeWork data, for data-driven business and user behavior insights', 'You take an innovator and creator’s approach to any issues that may arise.', ""You’re excited to solve data challenges that combines digital and physical aspects, across multiple business verticals like Real Estate, Finance, Sales & Marketing, Social Media, and regionsYou're motivated to enable and collaborate with engineering, analytics and product teams to tackle the most challenging business needs"", 'Leverage data expertise to help build and evolve data models in various components of the data stackWork on architecting, building, and launching highly scalable and reliable data pipelines to support WeWorkCollaborate closely with data engineers, analysts,data scientists, ML Engineers, as well as cross-functional teams, to leverage huge amounts of WeWork data, for data-driven business and user behavior insightsCollaborate on improving efficiency and quality of internal data processes as well as stakeholder engagement, including implementation of system and model quality tracking Develop centralized source of truth data sets to encourage a democratized, data-driven culture', 'Strong communication skills, empathy and initiative', 'Bonus Points For Experience In', 'Exceptional organizational and multitasking skills.', 'Leverage data expertise to help build and evolve data models in various components of the data stack', 'Work on architecting, building, and launching highly scalable and reliable data pipelines to support WeWork', 'You have the flexibility to think outside the box.', '3+ years of experience in data engineering', 'Good understanding of SQL Engine and able to conduct advanced performance tuning']",Mid-Senior level,Full-time,Information Technology,Computer Software,2020-10-06 12:14:13
Data Engineer,Arthur,"New York, NY",12 hours ago,42 applicants,"['', 'What we can offer you:', 'A competitive salary', 'Competence writing production-ready code in Python', 'We’ll Want You to Have:', 'Arthur is looking for a Data Engineer to join our growing New York based team!\xa0 Arthur is an early-stage venture-backed startup shaking up the $64 Billion/year art market.\xa0 Our goal is to make the data around the art market more accessible to collectors, investors, dealers, and the artists themselves, giving more people the confidence to discover new art and invest in an informed manner.', 'Experience designing ETL pipelines using cloud computing tools such as Dataflow (we use GCP, but equivalent experience with AWS or Azure is great, too)', 'It’d Be Nice if You Have:', 'Significant equity in the company, so we all share in Arthur’s success', 'Roughly 3-5 years of professional experience in Data Engineering, Data Science, and/or Data Analytics', 'Worked with commercial or open-source tools and toolkits for natural language processing, such as Google Cloud Natural Language, NLTK, or spaCyExperience extracting data from unstructured sources', 'Health/dental/vision insurance, at low cost to employees', 'Four weeks of paid vacation, as well as paid holidays', 'Opportunities for rapid career growth and leadership as Arthur grows', 'Primarily remote work to start, with flexible WFH as conditions normalize', 'Experience extracting data from unstructured sources', 'A Bachelor’s degree in a relevant field:\xa0 Computer Science, Statistics, Economics, Mathematics, or equivalent', 'A Bachelor’s degree in a relevant field:\xa0 Computer Science, Statistics, Economics, Mathematics, or equivalentRoughly 3-5 years of professional experience in Data Engineering, Data Science, and/or Data AnalyticsExperience designing ETL pipelines using cloud computing tools such as Dataflow (we use GCP, but equivalent experience with AWS or Azure is great, too)Competence writing production-ready code in PythonQuality verbal and written communication skillsA sincere desire to learn and grow - we’re still quite small, so the desire to learn and grow as the company grows is essential!', 'Worked with commercial or open-source tools and toolkits for natural language processing, such as Google Cloud Natural Language, NLTK, or spaCy', 'A sincere desire to learn and grow - we’re still quite small, so the desire to learn and grow as the company grows is essential!', 'Quality verbal and written communication skills', 'A competitive salarySignificant equity in the company, so we all share in Arthur’s successFour weeks of paid vacation, as well as paid holidaysPrimarily remote work to start, with flexible WFH as conditions normalizeHealth/dental/vision insurance, at low cost to employeesOpportunities for rapid career growth and leadership as Arthur grows']",Mid-Senior level,Full-time,Information Technology,Internet,2020-10-06 12:14:13
Data Engineer,Nomi Health,"Austin, TX",19 hours ago,67 applicants,"['', 'OK, HOW ABOUT A FEW SPECIFIC RESPONSIBILITIES?', 'Must have AWS or Azure experience. (Snowflake, Databricks, S3 desirable)', 'Familiarity of system concepts and tools within an enterprise architecture framework.', 'Contribute to the development of the ML capabilities for the Nomi HealthDevelop and implement data models to guide business decisionsMapping data sources, including descriptions of the business meaning of the data, its uses, its quality, the applications that maintain it and the database technology in which it is stored. Documentation of a data source must describe the semantics of the data so that the occasional subtle differences in meaning are understood.Documenting interfaces and data movement by recording how mapped data is moved around the virtual enterprise. This includes the frequency of movement, the source and destination of each step, how the data is transformed as it moves, and any aggregation or calculations.Designing the movement of data through the enterprise, including sources of data and how the data is moved around in order to be improved.Defining integrative views of data to draw together data from across the enterprise. Some views will use a database of extracted data and others will bring together data in near real time, considering data currency, availability, response times and data volumes. Designing canonical data views to limit technical debt as data flows from point-to-point transformation.Defining technical standards and guidelines. Assess and document when and how to use the architected producers and consumers, the technologies to be used for various purposes, and models of selected entities, objects and processes. The guidelines should encourage reuse of existing data stores, as well as address issues of security, timeliness and quality.Investigate and participate in emerging technologies and new release Proofs of Concept (PoCs).Leveraging existing [core] data assets.Managing related metadata to include business descriptions of the data, details of any calculations or summaries, descriptions of the sources of the data, and indications of data quality and currency.Communicating the data architecture across the enterprise.Ensuring a focus on data quality by working effectively with data stewards so they can understand data semantics and identify opportunities for improving data quality.', 'Documenting interfaces and data movement by recording how mapped data is moved around the virtual enterprise. This includes the frequency of movement, the source and destination of each step, how the data is transformed as it moves, and any aggregation or calculations.', 'A minimum of 2-3 years experience in a similar role.', 'Excellent organizational and analytical abilities.', 'Must have coding experience (python, JAVA, R),\xa0', 'WHICH LEADER IS RESPONSIBLE FOR YOUR GROWTH AND DEVELOPMENT?', ""Nomi Health is led by Mark Newman (Founder/CEO) , Josh Walker (Co-Founder, COO) and Boe Hartman (Co-Founder, CTO). Both Mark, Josh and Boe have a proven track record in the HR Tech, Health Tech, Health plans, Pharmacy and Financial Service industries. Mark, previously Founder/CEO of HireVue where he scaled the company from 0 to $40m ARR with a successful exit of over $500M. Josh, comes with 18+ years of experience in healthcare as COO at Imagine Health, Upwell and OptumInsight where he has first-hand experience in international and domestic healthcare markets leading and expanding business growth. Boe has served in multiple senior positions across global banking for over 25 years.\xa0Most recently, at Goldman Sachs as a Partner, where he led the firm’s launch of it’s digital consumer bank; Marcus By Goldman Sachs and the firm's entry into the consumer credit card market."", 'Data Science Engineer requirements are:', 'Chief Technology Officer', 'Health, Dental, Vision, 401k with match, Commuter benefits with Great Pay, plus Equity.', 'Investigate and participate in emerging technologies and new release Proofs of Concept (PoCs).', 'Communicating the data architecture across the enterprise.', 'Benefits', 'Mapping data sources, including descriptions of the business meaning of the data, its uses, its quality, the applications that maintain it and the database technology in which it is stored. Documentation of a data source must describe the semantics of the data so that the occasional subtle differences in meaning are understood.', 'Defining integrative views of data to draw together data from across the enterprise. Some views will use a database of extracted data and others will bring together data in near real time, considering data currency, availability, response times and data volumes. Designing canonical data views to limit technical debt as data flows from point-to-point transformation.', 'Knowledge of various modern data formats, tools, and methodologies. (Infomatics desirable)', '\ufeffNomi Health is offering a highly competitive compensation package with an attractive base salary as well as a significant equity stake into the company at an early stage.', ""Bachelor's degree in Computer Science, Computer Engineering or relevant field.A minimum of 2-3 years experience in a similar role.Must have AWS or Azure experience. (Snowflake, Databricks, S3 desirable)Must have ELT/ETL experience (Talend, MDM, other ETL/ELT tools)Must have coding experience (python, JAVA, R),\xa0Familiarity of system concepts and tools within an enterprise architecture framework.Knowledge of various modern data formats, tools, and methodologies. (Infomatics desirable)Excellent organizational and analytical abilities.Outstanding problem solver.Good written and verbal communication skills."", 'Contribute to the development of the ML capabilities for the Nomi Health', 'Defining technical standards and guidelines. Assess and document when and how to use the architected producers and consumers, the technologies to be used for various purposes, and models of selected entities, objects and processes. The guidelines should encourage reuse of existing data stores, as well as address issues of security, timeliness and quality.', 'Requirements', 'Nomi Health is the modern payment system for employee healthcare. We sidestep the middlemen, and connect employers providers and families directly at scale to cut healthcare costs by 30% across America. We will do this by eliminating high cost “out of network” charges, making the process all digital with easy access to real data and understandable actions.\xa0', 'Develop and implement data models to guide business decisions', 'We are looking for a Data Science Engineer who can help pave the way for more work in the data field and start by analyzing our current needs and use data to generate value with Nomi Health’s end goal in mind. From this goal, you will design the architecture and the analytics pipelines while taking into account appropriate time frames, and costs.', 'Ensuring a focus on data quality by working effectively with data stewards so they can understand data semantics and identify opportunities for improving data quality.', 'As a Data Science Engineer you will provide technical and domain subject knowledge to the company and future customers. You should be able to know how to examine new data systems requirements and implement migration models. You will also spend a good deal of time problem solving, analyzing architecture and assessing architect models, reviewing data migrations, selecting platforms and on-boarding of data management solutions that meet the technical and operational needs of the business.\xa0You must be hands-on with tools and code.', 'Since we are early in our journey and we will be soon dealing with a lot of data, this person will set the stage for future work of data scientists and data engineers.\xa0', 'WHO ARE WE?\xa0', 'Leveraging existing [core] data assets.', 'WHAT IS A DAY IN THE LIFE?', 'Outstanding problem solver.', 'WHY IS THIS ROLE CRITICAL?', 'Designing the movement of data through the enterprise, including sources of data and how the data is moved around in order to be improved.', 'Must have ELT/ETL experience (Talend, MDM, other ETL/ELT tools)', ""Bachelor's degree in Computer Science, Computer Engineering or relevant field."", 'Managing related metadata to include business descriptions of the data, details of any calculations or summaries, descriptions of the sources of the data, and indications of data quality and currency.', 'Good written and verbal communication skills.', 'The company is in stealth mode right now and has raised over $10M in seed funding which will be used to hire and develop the product, technology and go-to-market team.\xa0']",Associate,Full-time,Engineering,Computer Software,2020-10-06 12:14:13
Data Engineer,ServiceTitan,"Atlanta, GA",7 hours ago,65 applicants,"['', 'Map data', 'Contribute material input to go/no-go/continue decisions upon test completion', 'Vertical SaaS experience is highly desirable', ' Health & Wellness: ', 'Results and solution oriented - we want to know how we can win, not why we can’t', 'Equal Opportunity Employer', ' Work/Life Balance: ', '2-5 years of experience with SQL Server 2008/2012/2014/2016', ' Enrichment: ongoing learning culture with access to Linkedin Learning and professional development workshops, diversity charter groups, orientation program, career pathing opportunities, mentorship programs', 'Strong analytical thinking skills', ' Family-Friendly Benefits:', 'Contribute', 'Advanced knowledge and experience in T-SQL, complex ETL tools and operations, and SSIS', ' Family-Friendly Benefits: extended parental leave, pregnancy support, 20k in adoption reimbursement, Snoo Smart Sleeper, back-up childcare credits, legal benefit, discounted pet insurance', 'As Our Data Engineer, You Will', 'Establish quality working relationships with internal stakeholders', ' Work/Life Balance: flexible work schedule, flexible PTO', 'Develop', 'Given the experimental nature of this job, we will require very tight compliance when it comes to data - we need to focus on learning', 'Map data from various legacy databases into the ServiceTitan platform, subsequently developing SQL scripts that will extract the information efficiently and accurately', ' Health & Wellness: company-paid medical/vision/dental/life insurance/disability, employer HSA contribution, free One Medical membership, care coordination support, 401(k) with company match, stipend for home office equipment/supplies, gym discounts, monthly cell phone stipend', 'Develop automated scripts to validate legacy database values and identify previously unmapped fields prior to loading them into the ServiceTitan platform', 'About ServiceTitan', 'Establish', 'Apply feedback from customers and internal stakeholders on data import quality into previously developed extraction scripts', 'Ability to work independently and cross functionally', ' 2-5 years of experience with SQL Server 2008/2012/2014/2016 Advanced knowledge and experience in T-SQL, complex ETL tools and operations, and SSIS Given the experimental nature of this job, we will require very tight compliance when it comes to data - we need to focus on learning Strong analytical thinking skills Expert level understanding of database and data model concepts Vertical SaaS experience is highly desirable Results and solution oriented - we want to know how we can win, not why we can’t Ability to work independently and cross functionally ', ""To Be Successful In This Role, You'll Need"", 'Perks & Benefits', 'CCPA Notice for CA Residents applying to Jobs at ServiceTitan', ' Map data from various legacy databases into the ServiceTitan platform, subsequently developing SQL scripts that will extract the information efficiently and accurately Develop automated scripts to validate legacy database values and identify previously unmapped fields prior to loading them into the ServiceTitan platform Apply feedback from customers and internal stakeholders on data import quality into previously developed extraction scripts Discover opportunities to leverage information from legacy databases into the implementation process to avoid inquiring for additional information from customers Establish quality working relationships with internal stakeholders Contribute material input to go/no-go/continue decisions upon test completion ', 'Apply ', 'Discover ', 'Life at ServiceTitan ', 'Expert level understanding of database and data model concepts', 'Enrichment: ', 'Discover opportunities to leverage information from legacy databases into the implementation process to avoid inquiring for additional information from customers']",Entry level,Full-time,Information Technology,Information Technology and Services,2020-10-06 12:14:13
Data Engineer,iHeartMedia,"San Antonio, TX",8 hours ago,Be among the first 25 applicants,"['', 'Assemble large, complex data sets that meet functional and non-functional business requirements', 'Experience writing automated tests', 'Strong understanding of ETL processes', 'Proven programming skills in Python or similar programming language', 'Experience with Apache Airflow', 'Current employees and contingent workers click here to apply and search by the Job Posting Title. ', 'Experience supporting and working with cross-functional teams in a dynamic environment', '5+ years of experience in data or software engineering', 'Experience with AWS and/or GCP', 'Communicate complex solutions and ideas to a variety of stakeholders (other team members, IT leadership, and business leaders) in easily understandable language', 'Hands-on knowledge of SQL and experience with both relational and distributed databases', 'Responsibilities', 'Utilize and stay current in programming languages and software technology', 'Job Summary', 'Working knowledge of CI/CD processes and Git source control', 'Experience with AWS tools and services such as Redshift, Athena, Lambda Functions, Step Functions', 'Experience with AWS and/or GCPExperience with Apache AirflowExperience with AWS tools and services such as Redshift, Athena, Lambda Functions, Step FunctionsExperience building data pipelines using PythonExperience with big data tools: Hadoop, Spark, Kafka, etc.5+ years of experience in data or software engineering', 'Minimum Qualifications', 'Bachelor’s Degree in Computer Science, Information Technology, Informatics, or Applied Math', 'Assemble large, complex data sets that meet functional and non-functional business requirementsBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sourcesDevelop and maintain standards for administration and operation including the scheduling, running, monitoring, logging, error management, failure recovery, and output validationIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Work with cross functional teams to strive for greater functionality in our data systems, and recommend and implement ways to improve data reliability, efficiency, and qualityContribute to the project planning process by estimating tasks and deliverablesCommunicate complex solutions and ideas to a variety of stakeholders (other team members, IT leadership, and business leaders) in easily understandable languageUtilize and stay current in programming languages and software technology', 'Experience building data pipelines using Python', '3-5 years of commercial experience in a data engineering role with a proven record of manipulating, processing and extracting value from large disconnected datasets', 'Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.', 'Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources', 'Develop and maintain standards for administration and operation including the scheduling, running, monitoring, logging, error management, failure recovery, and output validation', 'Bachelor’s Degree in Computer Science, Information Technology, Informatics, or Applied Math3-5 years of commercial experience in a data engineering role with a proven record of manipulating, processing and extracting value from large disconnected datasetsStrong understanding of ETL processesHands-on knowledge of SQL and experience with both relational and distributed databasesProven programming skills in Python or similar programming languageWorking knowledge of CI/CD processes and Git source controlExperience writing automated testsExperience working with REST APIsStrong organizational, communication, and presentation skillsExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvementExperience supporting and working with cross-functional teams in a dynamic environment', 'Strong organizational, communication, and presentation skills', 'Experience with big data tools: Hadoop, Spark, Kafka, etc.', 'Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement', 'Contribute to the project planning process by estimating tasks and deliverables', 'Work with cross functional teams to strive for greater functionality in our data systems, and recommend and implement ways to improve data reliability, efficiency, and quality', 'Preferred Qualifications', 'Experience working with REST APIs', 'Location']",Entry level,Full-time,Information Technology,Marketing and Advertising,2020-10-06 12:14:13
Data Engineer,OmniData,"Portland, OR",7 hours ago,Be among the first 25 applicants,"['We operate according to a methodology we call the Modern Data Estate Framework.', '1+ years of experience in Analytics and Data Warehousing, with Azure knowledge a plus.  ', 'A technical mindset and strong business acumen.', 'OmniData is offering you the opportunity to work with the entire lifecycle of large Data Projects, focused on cloud-based data warehousing, with surface points to Analytics, Machine Learning and AI. You will get to work closely with very experienced consultants who will be able to provide mentorship and career guidance. ', 'Responsibilities and Duties', 'You will work on various Big Data, Data Warehouse Automation, and Data Analytics projects for our world class clients, analyzing requirements, building data architecture and implementing state-of-the-art Data Warehouse projects using Microsoft platform and tools.  Deep knowledge of Azure Cloud is a plus.', 'Qualifications and Skills', 'Salary and benefits commensurate with experience.  High growth potential for those with an entrepreneurial spirit.', 'Data Warehousing and Data Warehouse AutomationAzure, SQL Server, SSASPower BI, DAXRequirements Analysis and Project Delivery methodology', 'Job Summary', ""You must be humble, hungry and a fast learner.  OmniData and our clients place a high value on inventiveness.1+ years of experience in Analytics and Data Warehousing, with Azure knowledge a plus.  A technical mindset and strong business acumen.Willingness to travel and work with a consultant's diligence.The position will report directly to a founding partner of the company.  As a consultant, you are also responsible to our clients.We operate according to a methodology we call the Modern Data Estate Framework.SQL Experience is required and Microsoft Azure experience is a plus."", 'SQL Experience is required and Microsoft Azure experience is a plus.', 'Power BI, DAX', ""Willingness to travel and work with a consultant's diligence."", 'Requirements Analysis and Project Delivery methodology', 'You must be humble, hungry and a fast learner.  OmniData and our clients place a high value on inventiveness.', 'Data Warehousing and Data Warehouse Automation', 'OmniData', 'Azure, SQL Server, SSAS', 'High growth potential for those with an entrepreneurial spirit.', 'Salary and benefits commensurate with experience.  ', 'OmniData is a US-based Data and Analytics consulting firm. We have deep experience in Data Architecture and Business Analytics, and we help organizations build their Modern Data Estates, designed to serve their needs well for many years to come.  We simplify the complex, and we are Microsoft Gold Data Partners in both Analytics and Data Platform.  Are you a Microsoft Certified Azure Data Engineer?  Do you feel that you could be?  How are your PowerBI skills?', 'The position will report directly to a founding partner of the company.  As a consultant, you are also responsible to our clients.', 'You need to have solid experience working with data and analytics, a strong technical aptitude, and be a quick learner.  In return, we offer an exciting position at a young startup experiencing rapid growth, deep mentorship and the opportunity to be part of creating a consulting firm that makes a difference for our clients every day we are with them.  ', 'Benefits and Perks']",Entry level,Full-time,Information Technology,Computer Software,2020-10-06 12:14:13
Data Engineer - Mid Level,Strategic Staffing Solutions,Washington DC-Baltimore Area,3 hours ago,Be among the first 25 applicants,"['', 'Design, document, build, test and deploy data pipelines that assemble large complex datasets from various sources and integrate them into a unified view.', 'Design, build, and manage analytics infrastructure that can be utilized by data analysts, data scientists, and non-technical data consumers, which enables functions of the big data platform for Analytics.', '\xa0', 'Develop, construct, test, and maintain architectures, such as databases and large-scale processing systems that help analyze and process data in the way the Analytics organization requires.', 'No C2C. We can offer a partner fee but no C2C. Remote then onsite in Charlotte, NC. Ideally looking for OPT, H4, GC or USC.', 'Scalability Experience', 'Big Data Experience', 'SQL', 'Degree in Computer Science, Engineering, or related fields.3-5 years of experienceDesign, build, and manage analytics infrastructure that can be utilized by data analysts, data scientists, and non-technical data consumers, which enables functions of the big data platform for Analytics.Develop, construct, test, and maintain architectures, such as databases and large-scale processing systems that help analyze and process data in the way the Analytics organization requires.Develop highly scalable data management interfaces, as well as software components by employing programming languages and tools.Design, document, build, test and deploy data pipelines that assemble large complex datasets from various sources and integrate them into a unified view.Identify, design, and implement operational improvements: automating manual processes, data quality checks, error handling and recovery, re-designing infrastructure as needed.Create data models that will allow analytics and business teams to derive insights about customer behaviorsBuild new data pipelines, identify existing data gaps and provide automated solutions to deliver analytical capabilities and enriched data to applications.Responsible for obtaining data from the System of Record and establishing batch or real-time data feed to provide analysis in an automated fashion.', 'Must Have:', 'Identify, design, and implement operational improvements: automating manual processes, data quality checks, error handling and recovery, re-designing infrastructure as needed.', 'Build new data pipelines, identify existing data gaps and provide automated solutions to deliver analytical capabilities and enriched data to applications.', '3-5 years of experience', 'Degree in Computer Science, Engineering, or related fields.', 'Mongo DB', 'NoSQL', 'SQLNoSQLMongo DBBig Data ExperienceScalability ExperienceETL Pipelines', 'Responsible for obtaining data from the System of Record and establishing batch or real-time data feed to provide analysis in an automated fashion.', 'Develop highly scalable data management interfaces, as well as software components by employing programming languages and tools.', 'ETL Pipelines', 'Create data models that will allow analytics and business teams to derive insights about customer behaviors']",Mid-Senior level,Contract,Engineering,Information Technology and Services,2020-10-06 12:14:13
Software Data Engineer,Garmin,"Olathe, KS",19 hours ago,26 applicants,"['', 'Build and architect big data architecture in software development cycle', 'Develop cloud-like self-service solutions', 'Communicate in written and verbal form effectively', 'Learn new data technologies and practices proactively', 'Ability to write scripts as needed to interact with big data clusters', 'Thoroughly document work in an organized manner', 'Ability to prioritize and multi-task independently in a flexible, fast paced and challenging environment', 'Must be detail-orientated and have the ability to work proactively and effectively with minimal supervision', 'Identify and resolve problems proactively', 'Data Engineer ', 'Build and architect big data architecture in software development cycle Ability to write scripts as needed to interact with big data clusters Develop cloud-like self-service solutions Learn new data technologies and practices proactively Identify and resolve problems proactively Analyze existing data solutions and recommend improvements Thoroughly document work in an organized manner Communicate in written and verbal form effectively Must be detail-orientated and have the ability to work proactively and effectively with minimal supervision Ability to prioritize and multi-task independently in a flexible, fast paced and challenging environment ', 'Analyze existing data solutions and recommend improvements']",Entry level,Full-time,Information Technology,Consumer Goods,2020-10-06 12:14:13
Data Engineer,Randstad USA,"Cockeysville, MD",11 hours ago,Be among the first 25 applicants,"['', 'Technical algorithms', 'Network and hardware topology', 'SSIS', 'Object-oriented programming languages (Java, JavaScript, Python, C#)', 'Data governance', 'PowerBI', 'Designing ELT/ETL procedures to transfer data between internal and external sources via ETL, APIs', ' REST API conceptsPowershellT-SQL (SQL Server)PowerBISSIS', 'Responsibilities', 'Qualifications', 'Modern cloud data platforms Preferably Azure Data Lake (Storage and Analytics), Data Factory, Data Warehouse, Data Bricks, Amazon Redshift, Synapse Analytics, Logic AppsAWS experience acceptable ', 'Job Summary', 'Ensuring accuracy of data collections', 'Communicating with stakeholders to gather data requirements', 'Data visualization and reporting tools (Tableau, SSRS, Data Studio, Cognos, Qlik)', 'Preferably Azure Data Lake (Storage and Analytics), Data Factory, Data Warehouse, Data Bricks, Amazon Redshift, Synapse Analytics, Logic Apps', 'Data lakes and catalogs', 'Sensitive data and PII best practices, guidelines, and other governance', 'Powershell', 'Database warehousing, design, and architecture', ' Preferably Azure Data Lake (Storage and Analytics), Data Factory, Data Warehouse, Data Bricks, Amazon Redshift, Synapse Analytics, Logic AppsAWS experience acceptable', ' Communicating with stakeholders to gather data requirementsDesigning ELT/ETL procedures to transfer data between internal and external sources via ETL, APIsEnsuring accuracy of data collectionsNetwork and hardware topologyObject-oriented programming languages (Java, JavaScript, Python, C#)Modern cloud data platforms Preferably Azure Data Lake (Storage and Analytics), Data Factory, Data Warehouse, Data Bricks, Amazon Redshift, Synapse Analytics, Logic AppsAWS experience acceptable Data lakes and catalogsDatabase warehousing, design, and architectureData visualization and reporting tools (Tableau, SSRS, Data Studio, Cognos, Qlik)Sensitive data and PII best practices, guidelines, and other governanceData governanceTechnical algorithms', 'AWS experience acceptable', 'REST API concepts', 'T-SQL (SQL Server)']",Entry level,Full-time,Information Technology,Information Technology and Services,2020-10-06 12:14:13
Data Engineer (Data Visualization Engineer),Innosoft Corporation,"Baltimore, MD",,Be among the first 25 applicants,"['', 'Develop and extend web presentation layer for user portals.', 'Experience in presenting business critical insights to a large audience', ""Bachelor's Degree/Master's Degree in Math, Applied Math, Statistics, Computer Science, related field."", 'Qualification: ', 'Capable of and professionally experienced with coming up with designs combining large volumes of relevant data across different businesses/departments', 'Responsibilities: ', '5 years relevant work experience in data visualization and data analytics.', 'Work with large healthcare and financial datasets to provide clear insights via metrics, trendlines, interactive graphs and other presentation techniques.', 'Make large and/or complex data more accessible, understandable and usable.Deliver data in a useful and appealing way to users.Work with large healthcare and financial datasets to provide clear insights via metrics, trendlines, interactive graphs and other presentation techniques.Develop and extend web presentation layer for user portals.Investigate, rationalize, and recommend optimal approaches for architecting performant web presentation layer, evaluating data handling across app layers.', 'Experience with D3.js.', 'Innosoft is looking for a Data Visualization Engineer to join our growing team of software professionals delivering phenomenal results to various government agencies. You will be working with senior software developers, business analysts and other stakeholders for better user experience on a government website improvement project.\xa0', 'Investigate, rationalize, and recommend optimal approaches for architecting performant web presentation layer, evaluating data handling across app layers.', 'Make large and/or complex data more accessible, understandable and usable.', ""Bachelor's Degree/Master's Degree in Math, Applied Math, Statistics, Computer Science, related field.5 years relevant work experience in data visualization and data analytics.Experience with D3.js.Experience in presenting business critical insights to a large audienceCapable of and professionally experienced with coming up with designs combining large volumes of relevant data across different businesses/departments"", 'Deliver data in a useful and appealing way to users.']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2020-10-06 12:14:13
Data Engineer,LTI - Larsen & Toubro Infotech,"San Jose, CA",2 hours ago,Be among the first 25 applicants,"['', '·\xa0\xa0Rewards and recognition programs', 'A little about us...', ""What's in it for you? "", '·\xa0We provide our employees with a learning environment that promotes growth and creativity.', 'Java / API skills are nice to have', '·\xa0\xa0Commuter benefits', 'L&T Infotech is one of the largest global technology consulting and digital solutions company -holding an annual revenue of $1.4 bn. We were founded 20 years ago as the information technology arm of the Larsen & Toubro group. We are currently partnered with more than 350 clients (66 of which are Fortune 500 companies). We operate in 28 countries - employing over 28,000 employees world-wide!', 'Strong Python based Data Science experience.Good skills / exposure to Mongo DBJava / API skills are nice to haveStrong communication and analytical skillsExposure to Subscription / SaaS domain is an advantage\xa0', '·\xa0We lead in providing the best experiences for our clients and their customers.', '·\xa0Our role-based workshop helps us groom future leaders for LTI', '·\xa0Role-based Training programs', '·\xa0We encourage you to acquire various beneficial international certifications, with costs s reimbursed', '·\xa0Continuing Education Programs (CEP) to enhance your knowledge, skills, and attitude as a professional', '·\xa0\xa0Certification reimbursement', 'How will you grow?', 'Good skills / exposure to Mongo DB', '·\xa0\xa0Excellent growth and advancement opportunities', 'How will you grow? ', 'Strong Python based Data Science experience.', 'Exposure to Subscription / SaaS domain is an advantage\xa0', 'Job Description –', 'To learn more please visit us at www.lntinfotech.com follow us on Twitter @LTI_Global.', '·\xa0\xa0Excellent benefits plan: medical, dental, vision, life, FSA, & PTO', '·\xa0\xa0Innovative and collaborative company culture', 'We are an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, gender identity, sexual orientation, disability status, protected veteran status, or any other characteristic protected by law.', ' ', ""What's in it for you?"", '·\xa0\xa0Roll over vacation days', 'Strong communication and analytical skills']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2020-10-06 12:14:13
Data Engineer,Ebusiness Technologies Inc,Dallas-Fort Worth Metroplex,,Be among the first 25 applicants,"['', 'architecture, evaluation of technology alternatives, cost impact', 'software developers, security, and IT to design and build', '6. Extensive experience supporting software development projects', '8. Stay up to date with existing and new cloud services, providing', 'applications and services within GCP.', 'analyses and consistency across Information Systems.', 'highly reliable and fault-tolerant applications on GCP.', '5. Extensive experience and hands-on knowledge across the catalog', '1. Ensure success in designing, building and migrating applications,', '4. Architectural design reviews, including alignment with future state', 'Experience in designing dynamically scalable, highly available,', 'patterns for Cloud Based Applications.', 'of GCP technologies.', 'and Data proc', '3. Demonstrated experience &amp; Implementation of common design', 'architect, software developer, and/or systems engineer.', '3. Provide end-to-end solution expertise to determine best architecture', 'Roles &amp; Responsibilities ', 'building scalable Cloud Applications on GCP.', '4. Deep knowledge /experience in Big Table, Big Query, Data flow', 'solutions.', 'utilizing GCP technologies. Past roles may include systems', '2. Act as subject matter expert on AWS and work with business units,', 'guidance to technical teams as available cloud capabilities evolve.', '7. Experience migrating systems to hybrid or fully cloud-based', 'software, and services on the GCP platform.', '2. Demonstrated development &amp; implementation experience in', 'and cloud services to meet the business requirements.']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2020-10-06 12:14:13
Backend/Data Engineer - Fully Remote (USA) - FinTech Start Up,Huxley,New York City Metropolitan Area,,Be among the first 25 applicants,"['', 'You take pride in developing ETL and data pipelines that are optimized for performance, reliability, and scale. ', '2-5 years of hands on development experience in (Java, Kotlin or Scala) and ideally with Python as well ', 'Desire to work in a fast-paced start up environment. Previous startup experience is a plus. ', '2-5 years of hands on development experience in (Java, Kotlin or Scala) and ideally with Python as well Desire to work in a fast-paced start up environment. Previous startup experience is a plus. Experience with NoSQL is a plus: DynamoDB / BigTable, Elasticsearch, Redis', 'Experience with NoSQL is a plus: DynamoDB / BigTable, Elasticsearch, Redis', 'You might be a great fit if:', 'Requirements: ', 'You get excited learning about bleeding-edge technologies and are not afraid to experiment with them on the job. Quickly translating what you learn into prototypes and production applications. ', 'You take pride in writing well documented, well-rationalized microservices conforming OpenAPI specifications, then letting teams and users consume your APIs using auto-generated SDKs. ', 'You enjoy using a modern technology stack to build services that communicate with each other over REST or gRPC backed by modern technologies such as Kubernetes, DynamoDB, and Elasticsearch. ', 'You take pride in developing ETL and data pipelines that are optimized for performance, reliability, and scale. You enjoy using a modern technology stack to build services that communicate with each other over REST or gRPC backed by modern technologies such as Kubernetes, DynamoDB, and Elasticsearch. You take pride in writing well documented, well-rationalized microservices conforming OpenAPI specifications, then letting teams and users consume your APIs using auto-generated SDKs. You get excited learning about bleeding-edge technologies and are not afraid to experiment with them on the job. Quickly translating what you learn into prototypes and production applications. ']",Associate,Full-time,Engineering,Computer Software,2020-10-06 12:14:13
Data Engineer,Cognizant,"Wilmington, DE",13 hours ago,Be among the first 25 applicants,"['', ' Delaware, New Jersey, PA, VA.', 'You must be legally authorized to work in the United States without the need for employer sponsorship, now or at any time in the future.', 'About Cognizant', 'Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘Big data’ technologies.', ' Experience with stream-processing systems: Storm, Spark-Streaming, etc. (Nice to have)', ' 3+ years of experience (Mid-level) Experience with big data tools: Hadoop, Apache Spark, Kafka, etc', ' 3+ years of experience (Mid-level) Strong Programming experience with object-oriented/object function scripting languages: Python/Scala, Spark.', 'Responsibilities', 'Qualifications', ' Candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.', 'Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability etc.', 'Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.', 'Create and maintain optimal data pipeline architecture, Assemble large, sophisticated data sets that meet functional / non-functional business requirements.Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability etc.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘Big data’ technologies.Build analytics tools that utilize the data pipeline to deliver impactful insights into customer acquisition, operational efficiency and other key business performance metrics.Work with partners including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.', 'Work with partners including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.', ' 1+ Years of experience with relational SQL, Snowflake and NoSQL databases, including Postgres and Cassandra. Cognizant will not be able to provide sponsorship for this role. Candidates have the option of working remotely.', 'Build analytics tools that utilize the data pipeline to deliver impactful insights into customer acquisition, operational efficiency and other key business performance metrics.', ' 1+ years of experience with AWS cloud services: S3, EC2, EMR, RDS, Redshift', 'Create and maintain optimal data pipeline architecture, Assemble large, sophisticated data sets that meet functional / non-functional business requirements.']",Entry level,Full-time,Information Technology,Information Technology and Services,2020-10-06 12:14:13
Data Engineer - Remote,Dice,"Lilburn, GA",5 hours ago,Be among the first 25 applicants,"['', 'Required Experience']",Entry level,Full-time,Information Technology,Internet,2020-10-06 12:14:13
Data Engineer,Cogito Corp,"Boston, MA",14 hours ago,Be among the first 25 applicants,"['', 'Exploratory data analysis and data interpretation.', 'Be comfortable working in agile sprints methodology to co-discover the best approach with our stakeholders.', 'Modern Big Data ETL toolsPython Docker and KubernetesSQLAirFlow/Jenkins preferredDataOps methodologyAbility to collaborate with fellow CogiciansAbility to multi-task and work in a fast-paced start-up cultureKnowledge of git and code repository methods ', ' Your choice of comprehensive benefits for you and your family’s health, dental, vision, disability, and life insurance', ' Office Optional policy where Cogician’s choose where they work either primarily remote, primarily in office or hybrid', 'AirFlow/Jenkins preferred', 'Minimum Masters degree in advanced technical field, or business analytics, or related prior work experience. ', 'You Will', 'Knowledge of git and code repository methods ', ' 20 days vacation time, 5 days sick time, 2 floating holidays and 11 company holidays (yes, Patriot’s Day is a holiday)', ' Casual and inclusive office atmosphere', ' Stock options via equity grants', 'Equal Opportunity Employer', 'Love being a part of a high-performing analytics team.', 'Build and maintain robust and resilient data pipelines (ETL).', ' Frequent catered lunch and live product demos', 'Python ', 'Docker and Kubernetes', 'SQL', 'Ability to multi-task and work in a fast-paced start-up culture', 'Benefits', 'Responsibilities', 'Work collaboratively with our fellow Cogicians.', ' Company paid parental leave upon hire', 'Build and maintain robust and resilient data pipelines (ETL).Exploratory data analysis and data interpretation.Lead with best practices in DataOps to scale our team efficiently.Work collaboratively with our fellow Cogicians.', ' Ability to support Cogician’s anywhere in the US through our Office Optional policy', ' 2 ""Be Gentle"" personal days', 'Modern Big Data ETL tools', 'DataOps methodology', 'Be passionate about DataOps best practices to work smarter, not harder, to ensure baked in quality and efficiency in our data pipelines.', 'Authorization to Work', 'Love working with very large data; exploring data and mining it for insights.', 'Skills', ' Competitive pay, stock options, and annual bonus eligibility', ' Eligibility for annual bonus for all non commissioned employees', 'Requirements', 'Ability to collaborate with fellow Cogicians', ' Ongoing professional development and cross-training', 'Build and maintain robust and resilient data pipelines that enable complex analysis by data science and business intelligence reporting.', 'Lead with best practices in DataOps to scale our team efficiently.', ' 401(k) retirement plan options', 'Be intellectually curious and enjoy learning, teaching yourself and others the latest big data tools.']",Mid-Senior level,Full-time,Information Technology,Computer Software,2020-10-06 12:14:13
Data Engineer,IRIS Consulting Corporation,"Minnesota, United States",23 hours ago,55 applicants,"['', 'Location: Plymouth, MN', 'Minimum 3+ year experience on Data Warehousing using Microsoft Sql Server SSIS, SSAS (MDX, DAX), SSRS and Azure for Data and Analytics solutions (Functions, Data Factory, Data Lake, Data Warehouse, Analysis Services) 2+ years', 'Data & Analytics team works with business leaders across the enterprise to drive optimized strategy using data.\xa0This position contributes to company’s success by building cloud-based data services for analytic solutions. You will build data pipelines that are scalable, repeatable, secure and adhere to data quality standards. Responsibilities also include evangelizing data on cloud solutions with customers, leading business and IT stakeholders through designing a robust, secure and optimized Azure architecture, and delivering the target solution. This role will involve working with customers and collaborating with internal engineering teams, development of new application functionality, as well as maintenance and support of source code for existing Azure data platforms using the Azure data services', 'Microsoft Sql Server SSIS, SSAS (MDX, DAX), SSRS and Azure for Data and Analytics solutions (Functions, Data Factory, Data Lake, Data Warehouse, Analysis Services)', 'Overall Data Engineering experience 3+ year', 'Bachelor’s degree in computer science, management information systems, or related, or equivalent experienceMinimum 3+ year experience on Data Warehousing using Microsoft Sql Server SSIS, SSAS (MDX, DAX), SSRS and Azure for Data and Analytics solutions (Functions, Data Factory, Data Lake, Data Warehouse, Analysis Services) 2+ yearsOverall Data Engineering experience 3+ yearOn Prem to Cloud (Azure) Migration experience\xa0is preferred.', 'Translate business requirements into technical requirements to ensure solutions meet business needs', 'Support and execute on Enterprise Data Warehouse migration to Azure Cloud and Cloud Data Warehouse (Snowflake)', 'Analyze, design, build, query, troubleshoot and maintain data pipelines (SSIS, ADF, Snowflake)', 'Translate business requirements into technical requirements to ensure solutions meet business needsAnalyze, design, build, query, troubleshoot and maintain data pipelines (SSIS, ADF, Snowflake)Support On Prem EDW and resolve incidents covering both data analysis and data development requests.Support and execute on Enterprise Data Warehouse migration to Azure Cloud and Cloud Data Warehouse (Snowflake)Ability to multitask on different domains and technologies and faster resolution time of tickets.', 'Bachelor’s degree in computer science, management information systems, or related, or equivalent experience', 'Qualifications:', 'Experience working directly with key business leaders providing data to help them create value. Experience executing migration of on-prem data warehouses to cloud solutions. Experience in an Agile fast-paced environment with continuously evolving priorities.', 'Duration: 6 Months+', 'Responsibilities:', 'On Prem to Cloud (Azure) Migration experience\xa0is preferred.', 'Support On Prem EDW and resolve incidents covering both data analysis and data development requests.', 'Ability to multitask on different domains and technologies and faster resolution time of tickets.']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2020-10-06 12:14:13
Data Engineer,Rekruiters,"Houston, TX",19 hours ago,Over 200 applicants,"['', 'https://www.facebook.com/rekruiters/ – Facebook', '- 3yrs+ experience with APIs and has a good understanding of them.', '- Has worked directly with business teams to understand needs and engineer solutions.', '- Has worked on streaming architectures and moving large amounts of data.', 'Requirements:', 'MUST HAVE EXPERIENCE WITH SQL AND PYTHON', '_________________________________________________________________', '________________________________________________________', 'Specific needs for project:', 'Rekruiters has been named by business journals as one of the best places to work.', 'For more information on this job visit: https://rekruiters.com/jobs/', '- 2yrs+ experience with containerization (Docker or Kubernetes) -', '4yrs+ experience with Python using the data processing packages like Pandas, NumPy', 'https://www.rekruiters.com – Main Site', '- 4yrs+ experienced with databases, both relational and non-relational', 'Location: Houston, Area', 'Corporate:', '@rekruiters.com – Twitter', 'We offer benefits such as weekly pay, health insurance, 401k and even profit sharing to our consultants.']",Mid-Senior level,Contract,Information Technology,Oil & Energy,2020-10-06 12:14:13
