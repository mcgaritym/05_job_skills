job_title,company,location,date_posted,applicants,job_text,seniority_level,employment_type,job_function,industries,date_scraped
Data Engineer,Apple,"Cupertino, CA",1 hour ago,Be among the first 25 applicants,"['', 'Key Qualifications', 'Description', 'Summary', 'Education & Experience']",Not Applicable,Full-time,Information Technology,Consumer Electronics,2021-01-26 11:43:08
Data Engineer,Bloomberg LP,"Princeton, NJ",11 hours ago,Be among the first 25 applicants,"['', 'A BA/BS degree or higher in Computer Science, Mathematics, or relevant data technology field, or equivalent professional work experience in software development, data engineering, data science or information technology', 'Develop your career: sharpen your technical skills and strengthen relationships through project management, partnering with stakeholders across the firm, and establishing scalable architecture', 'Apply your coding skills: Automate the influx of data and build flexible solutions for data acquisition, ETL and machine learning pipelines, and human-in-the-loop data processing to drive successful product adoption', 'Deep understanding of large-scale, distributed systems', 'Apply your coding skills: Automate the influx of data and build flexible solutions for data acquisition, ETL and machine learning pipelines, and human-in-the-loop data processing to drive successful product adoptionInspire and impact our business: Act as an internal consultant to influence and implement more efficient products through analysis, dashboards, web apps, and user documentationDevelop your career: sharpen your technical skills and strengthen relationships through project management, partnering with stakeholders across the firm, and establishing scalable architectureChampion improvements: identify strategic technical gaps in our ecosystem and advocate for solutions over workaroundsGrow our business: Approach each day knowing this role is mission-critical to the success of the firm. We will rely on your expertise, and our flat structure allows for you to make real impact, real quick', ""At Bloomberg, our products are fueled by powerful information. We combine data and context to paint the whole picture for our clients, around the clock – from around the world. In Global Data, we're responsible for delivering this information through innovative technology - quickly and accurately."", '2+ years of experience working with restful APIs and data modeling within SQL and NoSQL databases  ', 'Legal authorization to work full-time in the United States without requiring visa sponsorship', ""Bloomberg is committed to diversity. It drives our innovation. At Bloomberg, you'll have the opportunity to go above and beyond and to take risks. You'll be a part of an organization that is entering new markets, launching new ventures, and pushing boundaries. Our ever-expanding array of technology, data, news, and media services champions innovation and empowers clients -- and offers nearly limitless opportunities for career growth."", '2+ years of Python programming and scripting in a production environment', 'Inspire and impact our business: Act as an internal consultant to influence and implement more efficient products through analysis, dashboards, web apps, and user documentation', ""Apply if you think we're a good match. We'll get in touch to let you know what the next steps are."", 'Champion improvements: identify strategic technical gaps in our ecosystem and advocate for solutions over workarounds', 'A BA/BS degree or higher in Computer Science, Mathematics, or relevant data technology field, or equivalent professional work experience in software development, data engineering, data science or information technology2+ years of Python programming and scripting in a production environment2+ years of experience working with restful APIs and data modeling within SQL and NoSQL databases  Deep understanding of large-scale, distributed systemsLegal authorization to work full-time in the United States without requiring visa sponsorship', ""Enterprising – and not defined by conventional roles. Our goal is to innovate, redefine, and break boundaries in expanding our modern data business. In Global Data's Technical Operations, our focus is on data engineering and how data is acquired, processed, validated and stored. We're constantly innovating to create better, more efficient systems to handle the huge variety of data we acquire and deliver to our clients."", 'We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. We bring out the best in each other.', ""We see the impact of our work every day, whether it's using machine learning pipelines to estimate financial product data, architecting systems that guide decision-making, or finding ways for our colleagues to more easily do their jobs. We get to work with market data to detect anomalies, assess the quality of data feeds to evaluate the accuracy of forecasts, and figure out how to best combine system automation with a human touch, bringing value to our businesses and ultimately our clients."", 'Grow our business: Approach each day knowing this role is mission-critical to the success of the firm. We will rely on your expertise, and our flat structure allows for you to make real impact, real quick']",Not Applicable,Full-time,Information Technology,Financial Services,2021-01-26 11:43:08
Data Engineer,Cooler Screens Inc.,"Chicago, IL",11 hours ago,Be among the first 25 applicants,"['', 'You have a deep understanding of the data architecture and relevant technologies', 'You’re passionate about building scalable data processing systems and turning data into insights', 'You’re experienced and comfortable in managing data from a variety of sources and formats and using best practices to ensure consistency and quality', 'You have extensive experience with database modeling and design', 'You have 4+ years of data engineering development experience ', 'You’re comfortable working with a variety of BI tools and helping users to develop solutions to gain key data insights', 'You have 4+ years of data engineering development experience You have a deep understanding of the data architecture and relevant technologiesYou have hands-on experience with Big Data technologies such as Spark and HadoopYou have an advanced understanding of cloud data warehousing solutionsYou have extensive experience with database modeling and design', 'Who You Are:', 'Data Engineer', 'You thrive working both independently and in a team environment', 'What You Need To Succeed', 'You know how to develop solutions to transform and optimize data and have expertise in solutions to build robust data pipelines and repositories', 'You’re excited to contribute to building a robust scalable and robust data platform and shaping an early stage startup', 'You have hands-on experience with Big Data technologies such as Spark and Hadoop', 'Cooler Perks ', 'You’re a critical thinker and enjoy measuring, analyzing, and solving complex problems', 'You’re excited to contribute to building a robust scalable and robust data platform and shaping an early stage startupYou’re passionate about building scalable data processing systems and turning data into insightsYou know how to develop solutions to transform and optimize data and have expertise in solutions to build robust data pipelines and repositoriesYou’re comfortable working with a variety of BI tools and helping users to develop solutions to gain key data insightsYou’re a critical thinker and enjoy measuring, analyzing, and solving complex problemsYou’re experienced and comfortable in managing data from a variety of sources and formats and using best practices to ensure consistency and qualityYou thrive working both independently and in a team environment', 'Who we are: ', 'You have an advanced understanding of cloud data warehousing solutions']",Mid-Senior level,Full-time,Information Technology,Marketing and Advertising,2021-01-26 11:43:08
Data Engineer/Developer,"Aqueduct Technologies, Inc.","Weymouth, MA",11 hours ago,Be among the first 25 applicants,"['', 'Knowledge of relational databases: MySQL/MariaDB, Oracle, MS SQL, Big Query', ""Ability to build and optimize data sets, 'big data' data pipelines and architectures"", 'Identifying, designing and implementing internal process improvements, optimizing data delivery, and automating manual processes', 'Data modelling: working with the data analyst and GCP engineers to define the data requirements, the source of the data and the formats', 'Functional knowledge of encryption technologies: SSL, TLS, SSH, etc.', 'Must be able to be present in the Boston area office 2x a week beginning Spring 2021', 'Familiarity accessing APIs and consumer Interfaces that utilize XML, JSON, REST, SOAP, etc.', 'Strong initiative to find ways to improve solutions, systems, and processes. ', 'Assembling large, complex sets of data that meet non-functional and functional business requirementsIdentifying, designing and implementing internal process improvements, optimizing data delivery, and automating manual processesBuilding required infrastructure for optimal extraction, transformation and loading of data from various data sources including MySQL, Oracle, flat files, CSV into GCPThe process involves:Data modelling: working with the data analyst and GCP engineers to define the data requirements, the source of the data and the formatsData Architecture: defining, together with the GCP engineers, the architecture of the GCP databasesData pipeline development: this includes extraction, formatting, and uploading of the data.', 'Ability to perform root cause analysis on external and internal processes and data to identify opportunities for improvements', 'Building required infrastructure for optimal extraction, transformation and loading of data from various data sources including MySQL, Oracle, flat files, CSV into GCP', 'Scripting/programming: Powershell, PERL, Bash etc', '5+ years of data engineering experience', 'Essential Job Functions', 'Experience with UNIX/Linux environments', 'Required Skills and Qualifications', 'Data modelling: working with the data analyst and GCP engineers to define the data requirements, the source of the data and the formatsData Architecture: defining, together with the GCP engineers, the architecture of the GCP databasesData pipeline development: this includes extraction, formatting, and uploading of the data.', 'Ability to analyze existing systems and software to understand current processes and designs', 'The process involves:Data modelling: working with the data analyst and GCP engineers to define the data requirements, the source of the data and the formatsData Architecture: defining, together with the GCP engineers, the architecture of the GCP databasesData pipeline development: this includes extraction, formatting, and uploading of the data.', 'Data Architecture: defining, together with the GCP engineers, the architecture of the GCP databases', ""5+ years of data engineering experienceAbility to build and optimize data sets, 'big data' data pipelines and architecturesAbility to perform root cause analysis on external and internal processes and data to identify opportunities for improvementsExcellent analytic skills associated with working on unstructured datasetsAbility to build automated processes that support data transformation, workload management, data structures, dependency and metadata using tools and scripting languages readily available on a variety of operating systemsAbility to analyze existing systems and software to understand current processes and designsExperience with UNIX/Linux environmentsFamiliarity accessing APIs and consumer Interfaces that utilize XML, JSON, REST, SOAP, etc.Scripting/programming: Powershell, PERL, Bash etcFunctional knowledge of encryption technologies: SSL, TLS, SSH, etc.Knowledge of relational databases: MySQL/MariaDB, Oracle, MS SQL, Big QueryStrong initiative to find ways to improve solutions, systems, and processes. Must be able to be present in the Boston area office 2x a week beginning Spring 2021Bachelor's degree in Information Technology, Computer Science or equivalent combination of training and/or experience"", 'Ability to build automated processes that support data transformation, workload management, data structures, dependency and metadata using tools and scripting languages readily available on a variety of operating systems', 'Data pipeline development: this includes extraction, formatting, and uploading of the data.', ""Bachelor's degree in Information Technology, Computer Science or equivalent combination of training and/or experience"", 'Excellent analytic skills associated with working on unstructured datasets', 'Assembling large, complex sets of data that meet non-functional and functional business requirements']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-01-26 11:43:08
Data Engineer,Discovery Inc,"Bellevue, WA",4 hours ago,Be among the first 25 applicants,"['', "" Bachelor's Degree or higher in Computer Sciences or similar Minimum 2 years of Software Industry experience 2+ years of development experience with AWS services Must have EMR or Glue, Data Pipeline or Airflow, S3, Jenkins or other CI/CD 2+ years of extensive working knowledge in spark (pyspark or scala). 1+ years of development experience with SQL queries. 1+ years of development experience with realtime data streams Kafka/Kinesis Proficiency working with structured, semi-structured, and unstructured data sets and real-time streaming data feeds Fluency in SQL, Python, or a similar modeling language. Experience with data warehousing databases Redshift, Oracle, and techniques is preferred Expert level usage with Jenkins, GitHub is preferred Experience in the media industry is a plus Must have the legal right to work in the United States "", '2+ years of development experience with AWS services Must have EMR or Glue, Data Pipeline or Airflow, S3, Jenkins or other CI/CD', '2+ years of extensive working knowledge in spark (pyspark or scala).', 'About Us: ', 'Position Summary', 'Must have the legal right to work in the United States', 'Collaborate with data scientists, BIEs, and BAs to deliver high-quality data architecture and pipelines.', ' Contribute to the architecture, design, and implementation of next-generation Data solutions – including streaming data applications Manage AWS resources including Glue, EMR, Kinesis, Lambda etc. Collaborate with data scientists, BIEs, and BAs to deliver high-quality data architecture and pipelines. Interface with other technology teams to extract, transform and load data from a wide variety of data sources Continually improve ongoing processes and pipelines, automating or simplifying self-service support for customers ', 'Manage AWS resources including Glue, EMR, Kinesis, Lambda etc.', 'Requirements', '1+ years of development experience with SQL queries.', 'EEO is the LawPay Transparency Policy StatementCalifornia Job Applicant Privacy Policy', ""Bachelor's Degree or higher in Computer Sciences or similar"", 'Expert level usage with Jenkins, GitHub is preferred', '1+ years of development experience with realtime data streams Kafka/Kinesis', 'Fluency in SQL, Python, or a similar modeling language.', 'Experience with data warehousing databases Redshift, Oracle, and techniques is preferred', 'Interface with other technology teams to extract, transform and load data from a wide variety of data sources', 'Responsibilities', 'Minimum 2 years of Software Industry experience', 'Proficiency working with structured, semi-structured, and unstructured data sets and real-time streaming data feeds', 'Job Summary', 'Continually improve ongoing processes and pipelines, automating or simplifying self-service support for customers', 'Contribute to the architecture, design, and implementation of next-generation Data solutions – including streaming data applications', 'Experience in the media industry is a plus']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-01-26 11:43:08
Data Engineer,Kelly,United States,16 hours ago,32 applicants,"['', 'Familiarity with Python micro frameworks such as Flask', 'Bachelors’ Degree’s in related field.Strong competency in the programming language Python, or similar languagesCompetency in SQL and procedural SQL (preferable PostgreSQL and PL/pgSQL)ETL expertise, focused on taking data from multiple sources and storing in a destination systemFamiliarity with Python micro frameworks such as FlaskProficiency in handling large data filesCritical thinking and driven problem solverIdeally knowledgeable about copyright and royalty data', 'The Data Engineer will be part of the Operations team who, amongst other tasks, will assist in the development of automated processes relating to music publishing catalog and royalty information. You will:', 'Ideally knowledgeable about copyright and royalty data', 'Location: Fully Remote ', 'Strong competency in the programming language Python, or similar languages', 'Job Requirements:', 'Critical thinking and driven problem solver', 'ETL expertise, focused on taking data from multiple sources and storing in a destination system', 'Regret that there is no c2C option for this role. Candidate must be eligible for direct W2 contract without employment sponsorship.', 'Bachelors’ Degree’s in related field.', 'Job Descriptions:', 'Data Engineer', 'Proficiency in handling large data files', 'Competency in SQL and procedural SQL (preferable PostgreSQL and PL/pgSQL)', 'Reviewing validations errors and working to resolve the issues', 'Duration: 6 months contract with possibility of renewal', 'Taking business requirements and implementing automated processes to ingests and validate publishing dataExtracting data from multiple sources and transforming it to be stored in a production databaseReviewing validations errors and working to resolve the issues', 'Location: ', 'Duration: ', 'Taking business requirements and implementing automated processes to ingests and validate publishing data', 'Extracting data from multiple sources and transforming it to be stored in a production database', 'The successful candidate will have the opportunity to work with our client, a multinational IT company specialized in consumer products (hardware & software) and services.\xa0']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-01-26 11:43:08
Data Engineer,Apex Systems,"Charlotte, NC",57 minutes ago,Be among the first 25 applicants,"['', 'Apex Systems is looking to hire a Data Engineer for one of their clients they support out of the Charlotte, NC area.', 'The client is looking to interview today and tomorrow (1/26-1/27).', 'API Integration experience', 'Data Engineer ', 'Requirements:', '100% Remote ', '3+ years of Data Engineering/ETL experience3+ years of SQL or Python scriptingExperience working in Cloud EnvironmentAPI Integration experience', '3+ years of SQL or Python scripting', 'Experience working in Cloud Environment', '\xa0', '3+ years of Data Engineering/ETL experience', 'Please apply today or send your resume directly to cmallory@apexsystems.com']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-01-26 11:43:08
Data Engineer,Tekniforce,Raleigh-Durham-Chapel Hill Area,1 hour ago,Be among the first 25 applicants,"['·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Assist in developing best BI and data services deployment practices such as Azure Data Factories and Azure CI/CD pipelines, including development, governance and maintenance.', '', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa04+ years of experience developing with PowerBI', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Ability to both work collaboratively with teams and excels being a contributor', ""We are looking for a Data Engineer. This is a full time permanent hire position in Raleigh, NC with our client. In this role as a Data Engineer, you will interact with stakeholders to analyze, explain, design, and develop new data services and capabilities supporting the enable company's business to take data driven decisions"", 'Required Skills for a Data Engineer:', 'Responsibilities as a Data Engineer:', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Propose and roll-out improvements to culture, process, tools, technology, and architecture.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience with Azure Data Analytics Architecture preferred', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Mentor and provide assistance with Power BI to partners across the organization', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Advanced level of SQL programming', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Expected to challenge teammates in a constructive and professional manner', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Invent ways to answer key business questions by leveraging existing data assets or assisting in creating new ones', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0High level of business acumen with clear understanding of how data and information support business objectives', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience with ETL tools such as Pentaho, KNIME preferred', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Lead the development of certified Power BI Data Models and Data Sources, utilizing internal and external data (APIs)', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Intermediate knowledge of MS Excel', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Define and evolve company’s data movement, data access, MDM, and data visualization strategies', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience working with data sourced from REST and/or SOAP APIs preferred', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Bachelor’s degree preferably in Computer Science, Management Information Systems (MIS), or Business Field with emphasis in Information Technology', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Intermediate knowledge of R or Python preferred', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Be customer oriented, and provide data and support to business partners as necessary', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Work in dynamic self-organized agile teams to develop high-quality solutions using the best technology stack, design, and architecture.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Participate in designing and implementing RESTful services and microservices.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Advanced level of DAX programming']",Mid-Senior level,Full-time,Information Technology,Retail,2021-01-26 11:43:08
Staff Data Engineer,Walmart,"Dallas, TX",1 hour ago,Be among the first 25 applicants,"['', '\ufeffPreferred Qualifications...', 'Want to use your experience working customers and your analytic skills to drive value through data?', 'You will solve customer issues with data solutions that\xa0positively impact their business by building\xa0highly engaged relationships, crafting solutions that solve critical priorities, ensuring our offerings are understood and driving value, serving as a “first line of defense” for data-related topics and by collaborating with our other tech and engineering teams to resolve customer obstacles.', 'You’ll make an impact by:', 'You’ll sweep us off our feet if…', 'Troubleshoots business and production issues by reviewing and analyzing information (for example, issue, impact, criticality, possible root cause); engaging support teams to assist in the resolution of issues; formulating an action plan; directing actions as designated in the plan; interpreting the results to determine further action; performs root cause analysis to prevent future occurrence of issues; and completing online documentation.', 'Data Modeling: Influences the overarching data strategy and vision for data pipelines and data products. Oversees and governs the expansion of existing data architecture and the optimization of data query performance via best practices. Presents and socializes data models to business and information technology stakeholders.', ""Bachelor’s degree in Computer Science and 5 years' experience in software engineering or related field OR 3: Master's degree in Computer Science and 3 years' experience in software engineering or related field."", 'Develops Innovation strategies, processes, and best practices by leading internal technical teams; partnering with cross-functional teams across the business; developing assessments of key opportunities; documenting project scopes; developing long-range plans and project timelines; communicating with and influencing decision-makers and executives within the organization; and resolving technology differences across teams through informed discussions.', ""Utilizes industry research to improve Wal-Mart's technology environment by analyzing industry best practices; benchmarking industry against internal processes and solutions; researching or influencing future industry solutions for fit with internal needs; and defining software development guidelines, standards and processes."", 'Working virtually this year has helped us make quicker decisions, remove location barriers across our global team, be more flexible in our personal lives and spend less time commuting.\xa0Today, we are reimagining the tech workplace of the future by making a permanent transition to virtual work for most of our team. Of course, being together in person is an important part of our culture and shared success. We’ll collaborate in person at a regular cadence and with purpose.\xa0', 'Being human-led is our true disruption.', 'Minimum Qualifications...', 'Supports business objectives by collaborating with business partners to identify opportunities; addressing high-priority initiatives (for example, business strategy, technical feasibility, implementation alternatives); identifying short- and long-term solutions; and leading cross-functional partnership.', 'Data Architecture: Designs, implements, and improves processes in data management.\xa0', ""What you'll do..."", '\xa0Evaluates proposed business cases for projects and initiatives. Translates business requirements into specific initiatives and drives execution of deliverables.\xa0', 'Leads and participates in medium- to large-scale, complex, cross-functional projects by reviewing project requirements; translating requirements into technical solutions; gathering requested information (for example, design documents, product requirements, wire frames); writing and developing code; conducting unit testing; communicating status and issues to team members and stakeholders; collaborating with project team and cross functional teams; troubleshooting open issues and bug-fixes; enhancing design to prevent reoccurrences of defects; ensuring on-time delivery and hand-offs; interacting with project manager to provide input on project plan; and providing leadership to the project team.', 'Leads the discovery phase of medium to large projects to come up with high level design by partnering with the product management, Web Operations, project management, business and user experience teams; obtaining cross-function approvals; driving proof-of-concept; and implementing prototypes to validate ideas.', ""Data Modeling: Influences the overarching data strategy and vision for data pipelines and data products. Oversees and governs the expansion of existing data architecture and the optimization of data query performance via best practices. Presents and socializes data models to business and information technology stakeholders.Data Architecture: Designs, implements, and improves processes in data management.\xa0Culture champion: Standardize and build a fact based metric driven engineering culture across the engineering organization. Help hire, upskill and mentor a strong team of architects and engineers. Cultivates an environment where associates respect and adhere to company standards of integrity and ethics. Pro-actively act as Wamart’s engineering tech brand ambassadorLeads and participates in medium- to large-scale, complex, cross-functional projects by reviewing project requirements; translating requirements into technical solutions; gathering requested information (for example, design documents, product requirements, wire frames); writing and developing code; conducting unit testing; communicating status and issues to team members and stakeholders; collaborating with project team and cross functional teams; troubleshooting open issues and bug-fixes; enhancing design to prevent reoccurrences of defects; ensuring on-time delivery and hand-offs; interacting with project manager to provide input on project plan; and providing leadership to the project team.Leads the work of other small groups of six to ten engineers, including offshore associates, for assigned Engineering projects by providing pertinent documents, direction, and examples; identifying short- and long- term solutions and timeline; reviewing and approving proposed solutions; implementing new architectural patterns; and performing design and code reviews of changes.Troubleshoots business and production issues by reviewing and analyzing information (for example, issue, impact, criticality, possible root cause); engaging support teams to assist in the resolution of issues; formulating an action plan; directing actions as designated in the plan; interpreting the results to determine further action; performs root cause analysis to prevent future occurrence of issues; and completing online documentation.Supports business objectives by collaborating with business partners to identify opportunities; addressing high-priority initiatives (for example, business strategy, technical feasibility, implementation alternatives); identifying short- and long-term solutions; and leading cross-functional partnership.Leads the discovery phase of medium to large projects to come up with high level design by partnering with the product management, Web Operations, project management, business and user experience teams; obtaining cross-function approvals; driving proof-of-concept; and implementing prototypes to validate ideas.Develops Innovation strategies, processes, and best practices by leading internal technical teams; partnering with cross-functional teams across the business; developing assessments of key opportunities; documenting project scopes; developing long-range plans and project timelines; communicating with and influencing decision-makers and executives within the organization; and resolving technology differences across teams through informed discussions.Utilizes industry research to improve Wal-Mart's technology environment by analyzing industry best practices; benchmarking industry against internal processes and solutions; researching or influencing future industry solutions for fit with internal needs; and defining software development guidelines, standards and processes.Drives the execution of multiple business plans and projects by identifying customer and operational needs; developing and communicating business plans and priorities; removing barriers and obstacles that impact performance; providing resources; identifying performance standards; measuring progress and adjusting performance accordingly; developing contingency plans; and demonstrating adaptability and supporting continuous learning.Provides supervision and development opportunities for associates by selecting and training; mentoring; assigning duties; building a team-based work environment; establishing performance expectations and conducting regular performance evaluations; providing recognition and rewards; coaching for success and improvement; and ensuring diversity awareness.Promotes and supports company policies, procedures, mission, values, and standards of ethics and integrity by training and providing direction to others in their use and application; ensuring compliance with them; and utilizing and supporting the Open Door Policy.Ensures business needs are being met by evaluating the ongoing effectiveness of current plans, programs, and initiatives; consulting with business partners, managers, co-workers, or other key stakeholders; soliciting, evaluating, and applying suggestions for improving efficiency and cost effectiveness; and participating in and supporting community outreach events.Monitoring the industry for developments in data and analytics capabilities, and presents to customer to drive incremental value"", 'We’re virtual', ""Data engineering, database engineering, business intelligence, or business analytics, ETL tools and working with large data sets in the cloud, Master’s degree in Computer Science or related field and 5 years' experience in software engineering or related field"", 'Leads the work of other small groups of six to ten engineers, including offshore associates, for assigned Engineering projects by providing pertinent documents, direction, and examples; identifying short- and long- term solutions and timeline; reviewing and approving proposed solutions; implementing new architectural patterns; and performing design and code reviews of changes.', 'Drives the execution of multiple business plans and projects by identifying customer and operational needs; developing and communicating business plans and priorities; removing barriers and obstacles that impact performance; providing resources; identifying performance standards; measuring progress and adjusting performance accordingly; developing contingency plans; and demonstrating adaptability and supporting continuous learning.', 'The Staff Data Engineer role is a critical position focused on becoming a trusted partner and delivering value through data for our customers.\xa0You will use your skills to understand our customers’ businesses and provide data solutions that help solve their challenges. As a Principal Data Engineer you will ensure our customers’ data needs and pain points are addressed by providing outstanding service, assistance and knowledge of the Data Strategy & Insight team’s capabilities.', 'Provides supervision and development opportunities for associates by selecting and training; mentoring; assigning duties; building a team-based work environment; establishing performance expectations and conducting regular performance evaluations; providing recognition and rewards; coaching for success and improvement; and ensuring diversity awareness.', 'About Global Tech', 'You understand data Strategy:', 'Monitoring the industry for developments in data and analytics capabilities, and presents to customer to drive incremental value', 'Ensures business needs are being met by evaluating the ongoing effectiveness of current plans, programs, and initiatives; consulting with business partners, managers, co-workers, or other key stakeholders; soliciting, evaluating, and applying suggestions for improving efficiency and cost effectiveness; and participating in and supporting community outreach events.', 'Culture champion: Standardize and build a fact based metric driven engineering culture across the engineering organization. Help hire, upskill and mentor a strong team of architects and engineers. Cultivates an environment where associates respect and adhere to company standards of integrity and ethics. Pro-actively act as Wamart’s engineering tech brand ambassador', 'You have applied business acumen:\xa0Evaluates proposed business cases for projects and initiatives. Translates business requirements into specific initiatives and drives execution of deliverables.\xa0You understand data Strategy:Develops and executes technical software development strategy using current business roadmap by analyzing competitive industry trends, technology landscape,\xa0and plans next gen data strategy to complex business problems. (Ex: cloud strategy, tech modernization)', 'Develops and executes technical software development strategy using current business roadmap by analyzing competitive industry trends, technology landscape,\xa0and plans next gen data strategy to complex business problems. (Ex: cloud strategy, tech modernization)', 'Promotes and supports company policies, procedures, mission, values, and standards of ethics and integrity by training and providing direction to others in their use and application; ensuring compliance with them; and utilizing and supporting the Open Door Policy.', 'You have applied business acumen:', 'Imagine working in an environment where one line of code can make life easier for hundreds of millions of people and put a smile on their face. That’s what we do at Walmart Global Tech. We’re a team of 15,000+ software engineers, data scientists and service professionals within Walmart, the world’s largest retailer, delivering innovations that improve how our customers shop and empower our 2.2 million associates. To others, innovation looks like an app, service or some code, but Walmart has always been about people. People are why we innovate, and people power our innovations. Being human-led is our true disruption.']",Mid-Senior level,Full-time,Engineering,Retail,2021-01-26 11:43:08
Data Engineer,CapCenter,"Richmond, VA",2 hours ago,Be among the first 25 applicants,"['', 'A competitive salary & annual bonus ', 'Qualifications', 'A competitive salary & annual bonus 401k w/ match, health, dental & vision benefits Exposure to the mortgage, real estate and insurance industries, front to back To participate in cross-functional collaboration and leadership The opportunity to lead transformative projects for an innovative and disruptive business', 'Collaborate across the organization to support mission-critical business processes with data', 'Master’s DegreeAt least 1 year of cloud service experienceAt least 1 year of application development (Node, .NET, etc.) experienceAt least 1 year of scripting language (Python, R, Perl, JavaScript, Shell, etc.) experienceAt least 1 year of big data technology (Spark, Hadoop, Snowflake, etc.) experienceAt least 1 year of NoSQL experienceAt least 1 year of Agile engineering practices experienceAt least 1 year of machine learning/predictive analytics experience', 'At least 1 year of NoSQL experience', 'To participate in cross-functional collaboration and leadership ', 'At least 1 year of Agile engineering practices experience', 'Bachelor’s DegreeAt least 1 year of API development experienceAt least 1 year of ETL development experienceAt least 1 year of SQL database experienceOrganized, entrepreneurial, ambitious, and attentive to detail', 'What You’ll Do', 'At least 1 year of ETL development experience', 'Master’s Degree', 'Collaborate across the organization to support mission-critical business processes with dataMaintain and develop existing data infrastructure including overall design, ETL, and systemsDesign and construct ETLs that communicate with APIs and internal systemsOver time, assume responsibility for furthering machine learning within the firm', 'Over time, assume responsibility for furthering machine learning within the firm', 'Organized, entrepreneurial, ambitious, and attentive to detail', 'Preferred Qualifications', 'At least 1 year of SQL database experience', 'Exposure to the mortgage, real estate and insurance industries, front to back ', 'At least 1 year of big data technology (Spark, Hadoop, Snowflake, etc.) experience', 'Maintain and develop existing data infrastructure including overall design, ETL, and systems', 'At least 1 year of machine learning/predictive analytics experience', '401k w/ match, health, dental & vision benefits ', 'At least 1 year of cloud service experience', 'The opportunity to lead transformative projects for an innovative and disruptive business', 'At least 1 year of scripting language (Python, R, Perl, JavaScript, Shell, etc.) experience', 'You’ll get ', 'At least 1 year of application development (Node, .NET, etc.) experience', 'Bachelor’s Degree', 'At least 1 year of API development experience', 'Design and construct ETLs that communicate with APIs and internal systems']",Entry level,Full-time,Information Technology,Insurance,2021-01-26 11:43:08
Data Engineer,Amazon,"North Reading, MA",4 hours ago,Be among the first 25 applicants,"['', ' Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers', ' 2+ years of experience as a Data Engineer or in a similar role Experience with data modeling, data warehousing, and building ETL pipeline Experience in SQL and Linux/UNIX to process large data sets Bachelor or higher degree in a quantitative/technical field such as Computer Science, Statistics, Engineering. Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations.', ' Experience in SQL and Linux/UNIX to process large data sets', ' Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations.', ' Drive architectural plans and implementation for future data storage, reporting, and analytic solutions', 'Job Responsibilities', 'Basic Qualifications', ' Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets Experience with AWS services including S3, Redshift, EMR and RDS Experience in gathering requirements and formulating business metrics for reporting Excellent data presentation skills and demonstrated ability to successfully partner with business and technical teams', ' Bachelor or higher degree in a quantitative/technical field such as Computer Science, Statistics, Engineering.', 'Description', ' Experience in gathering requirements and formulating business metrics for reporting', 'Preferred Qualifications', ' 2+ years of experience as a Data Engineer or in a similar role', 'Company', ' Experience with AWS services including S3, Redshift, EMR and RDS', ' Monitor and troubleshoot operational or data issues in the data pipelines using industry standard frameworks.', ' Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets', ' Experience with data modeling, data warehousing, and building ETL pipeline', ' Design data schema and own internal data warehouses and SQL/NoSQL database systems Design, build and implement the right ETL processes using AWS and similar technologies. Drive architectural plans and implementation for future data storage, reporting, and analytic solutions Monitor and troubleshoot operational or data issues in the data pipelines using industry standard frameworks. Partner with additional engineering teams to enhance data infrastructure, data availability, and broad access to customer insights made available through BI tools across the organization. Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using SQL to support business analyst and customer reporting needs. Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers Establish and implement technology best practices that should be followed across the team.', ' Design, build and implement the right ETL processes using AWS and similar technologies.', ' Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using SQL to support business analyst and customer reporting needs.', ' Establish and implement technology best practices that should be followed across the team.', ' Partner with additional engineering teams to enhance data infrastructure, data availability, and broad access to customer insights made available through BI tools across the organization.', ' Excellent data presentation skills and demonstrated ability to successfully partner with business and technical teams', ' Design data schema and own internal data warehouses and SQL/NoSQL database systems']",Not Applicable,Full-time,Information Technology,Computer Software,2021-01-26 11:43:08
Data Engineer,Evalueserve,"New York, United States",4 hours ago,29 applicants,"['', '•\tExperience with distributed systems', '•\tCompetitive compensation', 'Compensation & Benefits:', '•\tBachelor’s in Computer Science or related quantitative field', 'Location: New York, NY\xa0\xa0', ""The team's focus is building scalable frameworks and services used to analyze large datasets in the context of financial analysis. They leverage well-established engineering processes and techniques pioneered by giants in the tech industry. Projects span general pipelining to building internal web tools to helping develop low-latency query and analysis engines."", 'Requirements:', 'Position at a glance:\xa0\xa0', '•\tMaintain a high standard of code quality within the broader engineering team', '•\tEstablished track record in designing, building, deploying, and maintaining scalable systems', '•\tSecond-nature knowledge of algorithms and data structures', '•\tAdapt the latest data processing and infrastructure techniques to our growing stack', 'The Data Engineer will be supporting our client, global investment bank, located in New York, NY. The Data Engineer will be responsible for designing the data warehouse and all related extraction, transformation and load of data functions. The Data Engineer will also test your designs to ensure the system runs smoothly.', '•\tKnowledge developing and debugging Python', 'Job Description:\xa0', '•\tComplete Benefits Package', '•\tDevelop highly scalable data processing pipelines', 'We’re powered by mind+machine – a unique combination of human expertise and best-in-class technologies that use smart algorithms to simplify key tasks. This approach enables us to design and manage processes that can generate and harness insights on a large scale, significantly cutting costs and timescales and helping businesses that partner with us to overtake the competition. We work with clients across a wide range of industries and business functions, helping them to make better decisions faster; reach new levels of efficiency and effectiveness; and see a tangible impact on their top and bottom line.\xa0', 'Job Posting | Data Engineer\xa0', '•\tLeading edge global work environment', 'Department: Financial Services\xa0', 'Duration: Contract 6 months', '•\tCertification Reimbursement', 'Preferred Requirements:', 'Evalueserve is a global professional services provider offering research, analytics, and data management services.\xa0', '•\tDevelop data query and analysis services to be leveraged by clients, internally and externally', '•\tExperience with Hadoop or Spark', 'Responsibilities include and are not limited to:', 'Evalueserve is committed to providing equal opportunities (EEO) globally, eliminating discrimination and promoting good relations among employees, regardless of age, disability, race, ethnicity or origin, religion or belief, sex, gender assignment, gender identity, sexual orientation, marital or civil partnership status. Evalueserve is an inclusive employer and is proud of its diverse workforce.\xa0\xa0\xa0']",Associate,Contract,Information Technology,Information Technology and Services,2021-01-26 11:43:08
Data Engineer,Prealize Health,"Palo Alto, CA",11 hours ago,Be among the first 25 applicants,"['', 'Develop systems to support the ingestion of both streaming and batch data. ', 'You are passionate about working with data. You are passionate about making an impact in healthcare and want to work in a fast-paced environment. You are passionate and energetic about helping people and enjoy bringing diverse individuals together. You are a good communicator, able to work with cross-functional groups. ', 'Develop framework, processes, and tests to monitor and ensure data quality. ', 'Experience in CI/CD, and modern DevOps practices ', 'Minimum 4 years of programming experience in Python and SQL Minimum 1 years of Spark data platform experience (PySpark, Databricks, AWS EMR). Minimum 1 years of AWS cloud platform experience Experience with near real-time stream data processing (Spark Streaming). Experience in CI/CD, and modern DevOps practices ', 'Seasoned engineer built scalable data platform for large volume and complex data structure ', 'Investigate new technologies to support the continuous improvement of our data-pipeline. ', 'Minimum 4 years of programming experience in Python and SQL ', 'Minimum 1 years of Spark data platform experience (PySpark, Databricks, AWS EMR). ', 'Create data assets (data warehouses, data lakes, etc.) based on complex structured/unstructured data sources. ', 'Prior experience in the Healthcare Industry and FHIR standards is a plus ', 'Develop and maintain a robust and highly scalable data-pipeline to support our research and operations teams. Create data assets (data warehouses, data lakes, etc.) based on complex structured/unstructured data sources. ', 'Experience building systems governed under\u202fHIPAA, PHI\u202fand PII compliance\u202frequirements ', 'Develop automation tools to build more reliable systems and facilitate the ingestion of new data sources. ', 'You are passionate about working with data. ', 'You can see the bigger picture while also enjoying getting into the details. You are always trying to improve systems. ', 'You can see the bigger picture while also enjoying getting into the details. ', 'You are passionate and energetic about helping people and enjoy bringing diverse individuals together. ', 'Work with other teams (data science and clinical) to develop feature sets for their research efforts. ', 'Develop and maintain a robust and highly scalable data-pipeline to support our research and operations teams. ', 'BS/MS in Engineering, Computer Science, or equivalent. Seasoned engineer built scalable data platform for large volume and complex data structure ', 'You are passionate about making an impact in healthcare and want to work in a fast-paced environment. ', 'Develop framework, processes, and tests to monitor and ensure data quality. Work with other teams (data science and clinical) to develop feature sets for their research efforts. Investigate new technologies to support the continuous improvement of our data-pipeline. Develop automation tools to build more reliable systems and facilitate the ingestion of new data sources. Develop systems to support the ingestion of both streaming and batch data. ', 'Minimum 1 years of AWS cloud platform experience ', 'You are always trying to improve systems. ', 'Experience with near real-time stream data processing (Spark Streaming). ', 'Experience building systems governed under\u202fHIPAA, PHI\u202fand PII compliance\u202frequirements Prior experience in the Healthcare Industry and FHIR standards is a plus ', 'BS/MS in Engineering, Computer Science, or equivalent. ', 'You are a good communicator, able to work with cross-functional groups. ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-01-26 11:43:08
Data Engineer,Science 37,United States,1 hour ago,Be among the first 25 applicants,"['', 'Understanding of regulatory framework for software delivery', 'Knowledge of SDLC and project management methodologies (JIRA experience is a plus)', 'Some Experience with highly available database technologies like clustering, replication, mirroring, etc.', 'Experience with data tools like Jupyter', 'Ability to work with all levels of the organization', 'Data Engineer', 'As part of the Science 37 Tech team, the Data Engineer collaborates with motivated, energetic, and entrepreneurial individuals working together to achieve Science 37’s mission of changing the world of clinical research through patient-centered design. They have a hands-on role with the building and developing the data pipeline/platform that enables Science 37’s groundbreaking clinical research model and collaborates with Product, Data, Clinical Operations, and other relevant stakeholders to define study-specific platform requirements.', 'Knowledge of administration, replication, backup and restore of relational databases', 'Experience with operational efficiency improvement initiatives', 'Apache Cassandra', '4 weeks of Paid Time Off (PTO)', ""Science 37 is accelerating the research and development of breakthrough biomedical treatments by bringing clinical trials to patients' homes. By leveraging the latest innovations in mobile technology, cloud services, telemedicine, we are breaking down traditional geographic barriers to patient trial participation while shortening the time needed to bring new treatments to the market.Backed by venture investors such as Glynn Capital, Google Ventures, Redmile Group, dRx Capital and Lux Capital, we are revolutionizing the clinical trial industry one patient at a time. To help us achieve our goal, we are seeking a\xa0Data Engineer\xa0eager to make an impact within a mission-driven organization.\xa0"", 'Must have experience with AWS (other cloud providers are a plus)', 'None', '\xa0', 'Implement solutions for database performance monitoring and tuning', 'Some Experience with Cloud Computing management on the AWS platform', '2 weeks of paid sick days', 'Supervision', 'Experience with CSV (Computer Systems Validation)', 'Work with development team design and implement reporting capabilities', 'Stock option grant', 'Creating a data pipelines to help with Internal and External analytics users', 'This full-time opportunity offers a comprehensive total rewards package that includes:', 'Knowledge architectural & database design skills', 'United States (Remote)', 'And much more!\xa0', 'The Data Engineer helps drive data democracy at Science. This position will report to the Data Architect and will work with our architects, software engineers, product managers, and DevOps to help design and build data solutions and architecture.\xa0They will learn how Science 37 data is used and help make and drive the accessibility of the data that is needed, keeping in mind regulations and data privacy policies.\xa0They will be able to use data processing libraries and tools to help the end users of our data get the insights they need.', 'Science 37 values the well-being of its employees and aims to provide team members with everything they need to succeed. We are an equal opportunity/affirmative action employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected Veteran status, age, or any other characteristic protected by law.', 'Process database change requests, including the creation and modification of databases, tables, views, stored procedures, triggers, jobs, etc. in accordance with change control policies', 'Experience programming in Python, Java, or Scala', 'Optimize database performance by identifying and resolving application bottlenecks, tuning of DB queries, implementation of stored procedures, conducting performance tests, troubleshooting and integrating new elements', 'Experience designing and building data lake and data warehouse solutions', 'Medical, dental, and vision insurance plans', 'Competencies', 'Excellent troubleshooting skills and ability to understand complex relationships between components of multi-tiered and distributed applications.', 'Neo4j', 'Linux Server basic hands-on admin experience.', 'Scripting experience with Python or Bash required.', 'Experience with Monitoring/Alert planning for data services.', 'Qualifications', 'Strong work ethic with good time management with the ability to work with diverse teams and lead meetings', 'Experience using both SQL, NoSQL and Graph Databases', 'Working with cloud vendors like AWS or GCP', 'Thrive in fast-paced, agile environments, and able to learn new areas quickly', 'Working with cloud distributed file systems, data lakes, and data warehouses', 'Thrive in fast-paced, agile environments, and able to learn new areas quicklyBroad knowledge of common infrastructure technologies such as web servers, load balancers, etc.Excellent troubleshooting skills and ability to understand complex relationships between components of multi-tiered and distributed applications.Solid understanding of load balancing and high volume, high availability environmentsKnowledge of SDLC and project management methodologies (JIRA experience is a plus)Able to analyze and review current functionality to determine potential areas of improvement and cost savingsAbility to work independently with minimal guidance in a fast-paced environmentDemonstrate excellent communication skills including the ability to effectively communicate with internal and external customersStrong work ethic with good time management with the ability to work with diverse teams and lead meetingsAbility to work with all levels of the organizationExperience using both SQL, NoSQL and Graph DatabasesExperienced in automation and automation tools such as Jenkins, Puppet, Chef, etc.Amazon: RDS, Aurora, Athena, DocumentDB, DynamoDB, Neptune,Apache CassandraNeo4jSnowflakeExperience programming in Python, Java, or Scala', 'Preferred Qualifications', 'Experienced in automation and automation tools such as Jenkins, Puppet, Chef, etc.', 'Ability to work independently with minimal guidance in a fast-paced environment', 'Define and implement database schemas and configurations working with our development teams', ""Bachelor's degree in Computer Science or equivalent"", 'Demonstrate excellent communication skills including the ability to effectively communicate with internal and external customers', 'The incumbent reports to the Data Architect, who will also assign projects, provide general direction and guidance. Incumbent is expected to perform duties and responsibilities with minimal supervision.', 'Amazon: RDS, Aurora, Athena, DocumentDB, DynamoDB, Neptune,', 'Duties include but are not limited to:', 'Experience using SQL, NoSQL and Graph Databases', 'Understand how to Install, configure, monitor and maintain databases in the production, development, testing environments', 'Able to analyze and review current functionality to determine potential areas of improvement and cost savings', 'Recommend operational efficiencies, eliminate duplicate work efforts and remove unnecessary complexities; create and implement new procedures and workflows', 'Experience with JIRA, Confluence, SpiraTest is a plus', 'Experience with SAFe methodology', 'Utilize an understanding of Agile management to help the team with all release and configuration related tasks around software builds into preproduction and production environments.', ""Bachelor's degree in Computer Science or equivalentKnowledge architectural & database design skillsExperience using SQL, NoSQL and Graph DatabasesMust have experience with AWS (other cloud providers are a plus)Scripting experience with Python or Bash required.Proficient with SQL and Programming Languages like Python, Java, or ScalaHave an understanding of data architecture for microservicesExperience across different database platforms and tools such as MySQL, PostgreSQL, SQL Server, DynamoDB, MongoDB, AWS Neptune, Cassandra, Neo4jExperience designing and building data lake and data warehouse solutionsLinux Server basic hands-on admin experience.Some Experience with Cloud Computing management on the AWS platformExperience with Monitoring/Alert planning for data services.Some Experience with highly available database technologies like clustering, replication, mirroring, etc.Knowledge of administration, replication, backup and restore of relational databasesExperience with data tools like Jupyter"", 'Benefits:\xa0', 'Direct Reports', 'Solid understanding of load balancing and high volume, high availability environments', '401(k) with a company match', 'Have an understanding of data architecture for microservices', 'Science 37', 'Understand how to Install, configure, monitor and maintain databases in the production, development, testing environmentsWorking with cloud vendors like AWS or GCPWorking with cloud distributed file systems, data lakes, and data warehousesCreating a data pipelines to help with Internal and External analytics usersDefine and implement database schemas and configurations working with our development teamsOptimize database performance by identifying and resolving application bottlenecks, tuning of DB queries, implementation of stored procedures, conducting performance tests, troubleshooting and integrating new elementsWork with development team design and implement reporting capabilitiesImplement solutions for database performance monitoring and tuningRecommend operational efficiencies, eliminate duplicate work efforts and remove unnecessary complexities; create and implement new procedures and workflowsProcess database change requests, including the creation and modification of databases, tables, views, stored procedures, triggers, jobs, etc. in accordance with change control policiesUtilize an understanding of Agile management to help the team with all release and configuration related tasks around software builds into preproduction and production environments.', 'Benefits:\xa0\xa0', 'Experience with MuleSoft Anypoint Platform and Dataweave a plus.', 'Experience in Clinical Trials and/or life science industry', 'Experience with MuleSoft Anypoint Platform and Dataweave a plus.Experience in Clinical Trials and/or life science industryUnderstanding of regulatory framework for software deliveryExperience with operational efficiency improvement initiativesExperience with CSV (Computer Systems Validation)Experience with SAFe methodologyExperience with JIRA, Confluence, SpiraTest is a plus', 'Snowflake', 'Experience across different database platforms and tools such as MySQL, PostgreSQL, SQL Server, DynamoDB, MongoDB, AWS Neptune, Cassandra, Neo4j', 'Proficient with SQL and Programming Languages like Python, Java, or Scala', 'Broad knowledge of common infrastructure technologies such as web servers, load balancers, etc.']",Mid-Senior level,Full-time,Information Technology,Research,2021-01-26 11:43:08
Data Engineer,Navarc ,"Chicago, IL",20 hours ago,64 applicants,"['', 'Expertise transforming large data sets in the python library “pandas”', 'Benefits', 'Strong problem solving and verbal and written communication skills.', 'Quarterly & annual bonus plan', 'Navarc is a cutting edge eCommerce platform for brands and agencies that want to exponentially grow their sales and visibility on marketplaces. We are a start-up, our team is growing, fun, collaborative and has a diverse set of skills that enjoys solving tough problems, together.\xa0We are currently looking for fun, motivated and intellectually curious individuals who have a desire\xa0to go beyond their comfort zone, and have a hand in shaping the future of eCommerce data intelligence.\xa0', 'Company Background ', 'Understanding of API calls and the python library “requests”Familiarity with the Flask framework in pythonPassionate about adopting and implementing new technologiesInterest in cyber securityExperience in eCommerce\xa0', 'Passionate about adopting and implementing new technologies', 'Good understanding of math and statistics', 'Direct Commerce Group is proud to be an Equal Employment Opportunity Employer. We do not discriminate based upon race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.\xa0', 'Expertise transforming large data sets in the python library “pandas”Experience with data transformation in some SQL databaseGood understanding of Cloud platforms like Google CloudGood understanding of math and statisticsBroad understanding of database technologies and their strengthsStrong understanding of the web development cycle and programming techniques.Passion for data visualizationGrowth mindset for continual learning and teaching peers on new approaches and technologiesStrong problem solving and verbal and written communication skills.', 'Experience in eCommerce\xa0', 'Passion for data visualization', 'Familiarity with the Flask framework in python', 'Good understanding of Cloud platforms like Google Cloud', 'Growth mindset for continual learning and teaching peers on new approaches and technologies', 'Competitive salary', 'Broad understanding of database technologies and their strengths', 'Competitive salaryQuarterly & annual bonus plan2-3 weeks of vacationSmart and fun co-workers who will quickly feel like familyHealth, Dental, and Vision insurance401k plan', 'Health, Dental, and Vision insurance', 'Direct Commerce Group is proud to be an Equal Employment Opportunity Employer. We do not discriminate based upon race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.', 'Interest in cyber security', 'Required skills:', 'Smart and fun co-workers who will quickly feel like family', 'Sound like a fit? Apply now! We can’t wait to meet you.', 'Navarc is looking for a data engineer with excellent coding skills in python and an interest in data science.', '2-3 weeks of vacation', 'Experience with data transformation in some SQL database', 'Nice to Haves:\xa0', 'Understanding of API calls and the python library “requests”', '\xa0', '401k plan', 'Strong understanding of the web development cycle and programming techniques.']",Associate,Full-time,Information Technology,Computer Software,2021-01-26 11:43:08
Data Engineer,Pluralsight,"Draper, UT",22 hours ago,30 applicants,"['Our work includes: building pipelines which curate and land data, deploying data science models, and maintaining data infrastructure. You’ll have the opportunity to work with data tools, like Python and Spark, as well as web analytics and streaming data from our data platform.', 'You utilize a multidisciplinary approach to providing solutions for the business, combining technical, analytical, and domain knowledge.', 'Experience You’ll Need', 'You have strong development skills, experience transforming and profiling data', 'Building production applications from data science research and exploratory analytical work', 'You love interfacing with data scientists and analysts to understand their needs.', 'Modeling and curating product data sets, such as web analytics and kafka topics', 'Developing tooling and solutions for data practitioners using a deep understanding of their objectives and pain points', 'You utilize a multidisciplinary approach to providing solutions for the business, combining technical, analytical, and domain knowledge.You have strong development skills, experience transforming and profiling dataYou understand the benefits and risks of a variety of data technology solutions, which guide your implementation decisions.You love interfacing with data scientists and analysts to understand their needs.You have an eagerness to dive in to data sources to understand availability, utility, and integrity of our data ', 'You have an eagerness to dive in to data sources to understand availability, utility, and integrity of our data ', 'You understand the benefits and risks of a variety of data technology solutions, which guide your implementation decisions.', 'Managed systems with complex dependency management and orchestration requirements', 'Strong capability to manipulate and analyze complex, high-volume data from a variety of sources', 'Our Data Engineering & Operations team is a force multiplier for data practitioners at Pluralsight. We provide tooling and data sets to make Pluralsight a data-driven organization. ', 'Job Description:', '5+ years of taking a multidisciplinary approach to data development: we emphasize picking the right tool for the job', 'Improving observability in our data environment, including uptime, usage, data quality, and data freshness', '5+ years of taking a multidisciplinary approach to data development: we emphasize picking the right tool for the jobDeep experience with a number of data tools: e.g. SQL, Spark, Hadoop, PythonManaged systems with complex dependency management and orchestration requirementsStrong capability to manipulate and analyze complex, high-volume data from a variety of sourcesEffective communication skills with technical team members as well as business partners. Able to distill complex ideas into straightforward languageAbility to problem solve independently and prioritize work based on the anticipated business value', 'This position is available for remote employment in these areas:', 'Building and maintaining production data pipelines for data science and analytics', 'Ability to problem solve independently and prioritize work based on the anticipated business value', 'Building and maintaining production data pipelines for data science and analyticsDeveloping tooling and solutions for data practitioners using a deep understanding of their objectives and pain pointsModeling and curating product data sets, such as web analytics and kafka topicsImproving observability in our data environment, including uptime, usage, data quality, and data freshnessBuilding production applications from data science research and exploratory analytical work', 'Effective communication skills with technical team members as well as business partners. Able to distill complex ideas into straightforward language', 'What You’ll Own', 'Deep experience with a number of data tools: e.g. SQL, Spark, Hadoop, Python', 'Who You’re Committed To Being']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-01-26 11:43:08
Data Engineer,Bluetree Network,United States,41 minutes ago,Be among the first 25 applicants,"['A deep conceptual knowledge and demonstrated practical understanding of data\u202fmodeling\u202ftechniques and best practices \xa0', 'Make commitments – and keep commitments\xa0\xa0', 'Data Engineer-Specific Responsibilities', 'Life as a Blueleaf', 'DESIRED QUALIFICATIONS', 'Bluetree’s\xa0Data Analytics & Informatics team is comprised of data consultants with deep expertise in Epic and the overall healthcare data ecosystem. We come from varied backgrounds, ex-Epic developers, healthcare operations, financial services and academia. We are a fully remote team that leverages technology to collaborate and support each other. With our collective experience, we’ve likely seen that request/question/issue before, and we leverage that to deliver exceptional service to our clients and grow our internal team. We are passionate about learning and love working with early adopters, utilizing Epic’s newest development and the latest advancements in the data space.\u202f\xa0', 'Formal process improvement certification –\xa0ex:\xa0Lean Six Sigma or ITIL', 'Working independently or as part of a project team on a client engagement. Could be full-time on a single customer engagement or part-time across customersServing as a liaison between diverse IT and operations groups\xa0Facilitating meetings and owning meeting scheduling and coordination, preparation, documentation, and follow-up\xa0\xa0Utilizing, reviewing, and creating\xa0project tools and templates for assigned projects\xa0Creating and maintaining project plans\xa0Evaluating and documenting current-state processes through discovery and analysis. Presenting recommendations for improvements based on industry experience and\xa0best-practices\xa0Facilitating future-state workflow, policy, and process design and planning\xa0Building, testing, training, converting and/or deploying new infrastructure, workflows, policies, and processes\xa0\xa0Participating in major milestone reviews and decision gates\xa0Presenting to a wide variety of audiences\xa0\xa0Documenting measurable outcomes resulting from initiatives through KPI analysis and impact tracking\xa0Effectively utilizing communication, decision-making, and escalation pathways\xa0\xa0Executing effective project wrap-up through outcomes documentation, lessons-learned, and leave-behind materials allowing customers to sustain ongoing operations\xa0Mentoring Associate(s) on project activities and deliverables and collaborating with others on the same\xa0Mentoring customer counterparts for successful, long-term ownership and growth\xa0', 'Direct Reports:\xa0\xa0No', 'Create and maintain optimal data pipeline architecture\u202fwhile working within time and budget constraints\xa0\xa0\xa0Work with stakeholders to assist with data-related issues and support their data infrastructure needs\xa0\xa0Consult, assess and provide recommendations for improvement in client’s current data architecture\xa0\xa0Assemble large, complex, and disparate datasets to meet functional and non-functional business needs\xa0\xa0Utilize multiple programming languages to develop the best solution for the client (i.e.\xa0SQL, R, Python, C++, etc.)\xa0\xa0Research and develop\u202fprocesses\u202ffor utilizing\u202fcloud-based\u202fservices (i.e.\u202fSnowflake,\u202fGoogle Cloud Services, Amazon Web Services, and Microsoft Azure)\xa0Create data tools for analytics and data science team members that will aid in the development and optimization of our current and future services into an industry leader in healthcare analytics\u202f\u202f', 'In-depth understanding of data warehouse design with advanced knowledge in querying languages such as SQL \xa0', 'Create and maintain optimal data pipeline architecture\u202fwhile working within time and budget constraints\xa0\xa0\xa0', 'Some experience implementing, supporting, optimizing, and upgrading Epic', 'Bluetree Network is an equal employment opportunity employer and provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, Bluetree will provide reasonable accommodations for qualified individuals with disabilities. All qualified candidates are encouraged to apply. Bluetree is unable to sponsor candidates and/or employees for any type of employment visa (i.e. H-1B). If you require a sponsorship or are not legally authorized to work in the United States, we are likely unable to consider your candidacy at this time.', 'FLSA Status:', 'Capable of and comfortable with working remotely\xa0', 'Employment Eligibility:', 'Compensation:\xa0', 'Mentoring customer counterparts for successful, long-term ownership and growth\xa0', 'FLSA Status:\xa0Exempt', 'Direct Reports:', 'Utilize multiple programming languages to develop the best solution for the client (i.e.\xa0SQL, R, Python, C++, etc.)\xa0\xa0', 'Overview and Responsibilities of the Role', 'Blueleaves do better. We’re smart, driven, and humble. Together we’re committed to making a real impact in healthcare by helping our customers tackle their biggest challenges. It’s hard work but we balance it with a workplace that is inclusive and friendly; a place where you can be your authentic self. We trust our employees to work autonomously as we supply the direct feedback, transparency, and support needed to take on important work – and crush it. And we make the space for you to do that in a way that works for you by providing a generous PTO policy, remote work opportunities, and flexibility.', 'KEY RESPONSIBILITIES OR ASSIGNMENTS IN THIS ROLE MAY INCLUDE:', 'Participating in major milestone reviews and decision gates\xa0', 'Documenting measurable outcomes resulting from initiatives through KPI analysis and impact tracking\xa0', 'Evaluating and documenting current-state processes through discovery and analysis. Presenting recommendations for improvements based on industry experience and\xa0best-practices\xa0', 'In 2019, Bluetree became a part of Tegria: a family of top-tier healthcare technology, revenue cycle management, and professional services firms aligned closely with Providence, one of the largest health systems in the United States. In joining Tegria, we’ve extended our customer reach, gained a partnership with Providence to pilot our ideas, and are currently expanding into new offerings, new markets, and new geographies.\xa0These relationships allow us to bring enhanced capabilities to the market that differ from our competition to tackle the hardest challenges in healthcare. We are looking for a new internal team member who shares our propensity for innovation to join our team and help us create a real impact in healthcare.', 'Building, testing, training, converting and/or deploying new infrastructure, workflows, policies, and processes\xa0\xa0', 'Division:', 'Compensation:\xa0Salary varies based on experience', 'Client Engagement Delivery', 'You will receive full benefits for this position, including access to health insurance and company-paid long-term disability, short-term disability, and dental insurance premiums. We also offer a 401(k). We have an open paid time off policy in which each employee is afforded the flexibility to take vacation, take time off for illness, and shift schedules as needed.', 'Work Location: This position is remote.\xa0Must work in a location within the United States', 'Facilitating meetings and owning meeting scheduling and coordination, preparation, documentation, and follow-up\xa0\xa0', 'Assemble large, complex, and disparate datasets to meet functional and non-functional business needs\xa0\xa0', 'Experience working with unstructured and semi structured data (JSON, free-text entries,\u202fetc.) \xa0', 'Work Location: ', 'Your work at Bluetree will center on strategic opportunities and implementation, process improvement, and growing Bluetree as a company.\xa0This role will focus on developing, constructing, testing, and maintaining\u202fdata structures, data pipelines, and architecture. You will\u202frecommend and implement ways to improve the readability, efficiency, and quality of data. A Data Engineer at\u202fBluetree\u202fwill also be focused on delivering data sets for data modeling, data mining, and production reporting.', 'Prior consulting experienceSome experience implementing, supporting, optimizing, and upgrading EpicCertification in one or more Epic\xa0data model and/or\xa0application(s)Formal project management certification – either PMP or CSMFormal process improvement certification –\xa0ex:\xa0Lean Six Sigma or ITIL', 'Experience working with SSIS and other data integration tools\u202fand a desire to stay current as the data engineering field advances', 'Learn from their mistakes.\xa0\xa0Are\xa0kind and create a safe environment for others to make mistakes and learn\xa0', 'Serving as a liaison between diverse IT and operations groups\xa0', 'Capable of and comfortable with traveling to client sites as needed', 'Qualifications', 'Are\xa0independent and can create and manage workplans individually.\xa0Are\xa0comfortable operating without predefined deliverables or boundaries\xa0\xa0Display a bias to action and\xa0are\xa0energized by, not paralyzed by, problemsDeliver presentations\xa0with confidence and flair.\xa0Communicate in a manner that resonates clearly with your intended audience.\xa0Are\xa0clear and concise in their written and verbal communication\xa0Can\xa0anticipate issues and escalate\xa0effectively.\xa0Believe they have input and the ability to influence the process\xa0Are adaptable and flexible\xa0– willing to adjust their approach to the scenario. Are\xa0willing to push their comfort zone and be a team player\xa0Make commitments – and keep commitments\xa0\xa0Learn from their mistakes.\xa0\xa0Are\xa0kind and create a safe environment for others to make mistakes and learn\xa0Invest time in people, display compassion, and listen intently.\xa0Enjoy teaching and helping others grow\xa0', 'Display a bias to action and\xa0are\xa0energized by, not paralyzed by, problems', 'Mentoring Associate(s) on project activities and deliverables and collaborating with others on the same\xa0', 'Success Criteria:', '5+ years of professional experience\xa0working as a Data Architect, Data Engineer, ETL\u202fDeveloper,\u202fDatabase Administrator, or similar positions', 'Effectively utilizing communication, decision-making, and escalation pathways\xa0\xa0', 'Work Hours:\xa0Work is done during standard business hours for the client(s) that you are assigned to.\xa0This could vary and be spread across time zones.', 'Utilizing, reviewing, and creating\xa0project tools and templates for assigned projects\xa0', 'Prior consulting experience', 'MINIMUM QUALIFICATIONS', 'Facilitating future-state workflow, policy, and process design and planning\xa0', 'Want in?\xa0Come join us in our commitment to Done. Better.', 'Reports To:', 'Work with stakeholders to assist with data-related issues and support their data infrastructure needs\xa0\xa0', 'Consult, assess and provide recommendations for improvement in client’s current data architecture\xa0\xa0', 'Travel:\xa025-50%', 'Employment Eligibility:\xa0Must be legally authorized to work in the United States without sponsorship', 'Benefits Eligible:\xa0Yes', 'Can\xa0anticipate issues and escalate\xa0effectively.\xa0Believe they have input and the ability to influence the process\xa0', 'Deliver presentations\xa0with confidence and flair.\xa0Communicate in a manner that resonates clearly with your intended audience.\xa0Are\xa0clear and concise in their written and verbal communication\xa0', 'Creating and maintaining project plans\xa0', 'Benefits', 'Formal project management certification – either PMP or CSM', 'Are\xa0independent and can create and manage workplans individually.\xa0Are\xa0comfortable operating without predefined deliverables or boundaries\xa0\xa0', 'A Day In The Life:\xa0Responsibilities and Assignments', 'Work Hours:', 'Presenting to a wide variety of audiences\xa0\xa0', 'Are adaptable and flexible\xa0– willing to adjust their approach to the scenario. Are\xa0willing to push their comfort zone and be a team player\xa0', 'Invest time in people, display compassion, and listen intently.\xa0Enjoy teaching and helping others grow\xa0', 'Certification in one or more Epic\xa0data model and/or\xa0application(s)', 'Reports To:\xa0Data Analytics & Informatics – Manager', 'Research and develop\u202fprocesses\u202ffor utilizing\u202fcloud-based\u202fservices (i.e.\u202fSnowflake,\u202fGoogle Cloud Services, Amazon Web Services, and Microsoft Azure)\xa0', 'Working independently or as part of a project team on a client engagement. Could be full-time on a single customer engagement or part-time across customers', 'About Bluetree', 'Create data tools for analytics and data science team members that will aid in the development and optimization of our current and future services into an industry leader in healthcare analytics\u202f\u202f', 'Demonstrated ability in project management\xa0(waterfall and/or agile), and other organizational management such as risk management,\u202for\u202fchange management\xa0\xa0', 'Benefits Eligible:', 'PEOPLE WHO ARE SUCCESSFUL IN THIS JOB DISPLAY THE FOLLOWING:', 'Bluetree Network is a rapidly-growing strategy and support firm in the field\xa0of healthcare information technology. We’re headquartered in Madison, WI and have more than 350 Blueleaves across the nation helping our customers tackle their biggest challenges and maximize return on their healthcare IT investments. Our results speak for themselves, as 70% of our business comes from referrals and over 80% of clients who recommend us cite the quality of our employees.', 'Division:\xa0\xa0Consulting & Technology', 'Executing effective project wrap-up through outcomes documentation, lessons-learned, and leave-behind materials allowing customers to sustain ongoing operations\xa0', '5+ years of professional experience\xa0working as a Data Architect, Data Engineer, ETL\u202fDeveloper,\u202fDatabase Administrator, or similar positionsExperience working with SSIS and other data integration tools\u202fand a desire to stay current as the data engineering field advancesA deep conceptual knowledge and demonstrated practical understanding of data\u202fmodeling\u202ftechniques and best practices \xa0In-depth understanding of data warehouse design with advanced knowledge in querying languages such as SQL \xa0Experience working with unstructured and semi structured data (JSON, free-text entries,\u202fetc.) \xa0Demonstrated ability in project management\xa0(waterfall and/or agile), and other organizational management such as risk management,\u202for\u202fchange management\xa0\xa0Capable of and comfortable with working remotely\xa0Capable of and comfortable with traveling to client sites as needed']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-01-26 11:43:08
Lead Data Engineer,General Motors,"Austin, TX",11 hours ago,Be among the first 25 applicants,"['', 'Create and maintain optimal data pipeline architecture', 'Communicate and maintains Master Data, Metadata, Data Management Repositories, Logical Data Models, Data StandardsCreate and maintain optimal data pipeline architectureYou will assemble large, complex data sets that meet functional / non-functional business requirementsYou will identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Build industrialized analytic datasets and delivery mechanisms that utilize the data pipeline to deliver actionable insights into vehicle quality, operational efficiency and other key business performance metricsWork with business partners on data-related technical issues and develop requirements to support their data infrastructure needsCreate highly consistent and accurate analytic datasets suitable for business intelligence and data scientist team members', ""developing data architecture and ETL solutions using sound, repeatable, industry standard methodologies. You will have the opportunity to work hands-on defining ETL solutions based on business requirements and system specifications.Development activities will include enhancing existing data systems and optimizing ETL systems.Additionally you will lead development activities to migrate out of legacy technologies to GM's Big Data Platform utilizing new technologies such as, Kafka, Hadoop, PySpark, Greenplum and GM's internally developed Big Data tools."", 'Master databases - Advanced SQL and NoSQL databases, including Postgres and Cassandra, Oracle and Greenplum', '7 or more years with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.', 'Preferred', 'Benefits Overview', 'Communicate and maintains Master Data, Metadata, Data Management Repositories, Logical Data Models, Data Standards', 'About GM', 'Demonstrate mastery of many programming languages, tools and/or technologies with emphasis on ETL & Database development', ' Paid time off including vacation days, holidays, and parental leave for mothers, fathers and adoptive parents;', ""At least 3 years of hands on experience with Big Data Tools: Hadoop, Spark, Kafka, etc.Master databases - Advanced SQL and NoSQL databases, including Postgres and Cassandra, Oracle and GreenplumData Wrangling and Preparation using GM's approved tools: PySpark, Jupyter etc.Stream-processing systems: Storm, Spark-Streaming, etc.Ability to tackle software engineering and data problems quickly and completelyAbility to identify tasks which require automation and automate themAbility to multi-task and stay organized in a dynamic work environment and work collaboratively with other IT organizations (GDAAS, Platform Engineering etc)"", 'Create highly consistent and accurate analytic datasets suitable for business intelligence and data scientist team members', 'At least 3 years of hands on experience with Big Data Tools: Hadoop, Spark, Kafka, etc.', ' Healthcare (including a triple tax advantaged health savings account and wellness incentive), dental, vision and life insurance plans to cover you and your family;', ' Company and matching contributions to 401K savings plan to help you save for retirement;', 'You will assemble large, complex data sets that meet functional / non-functional business requirements', 'We are not able to accommodate international relocation.', 'developing data architecture and ETL solutions using sound, repeatable, industry standard methodologies. You will have the opportunity to work hands-on defining ETL solutions based on business requirements and system specifications.', 'Work with business partners on data-related technical issues and develop requirements to support their data infrastructure needs', 'Requirements', 'Build industrialized analytic datasets and delivery mechanisms that utilize the data pipeline to deliver actionable insights into vehicle quality, operational efficiency and other key business performance metrics', ' Global recognition program for peers and leaders to recognize and be recognized for results and behaviors that reflect our company values;', 'You will identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.', 'In recent years, GM Information Technology has successfully executed the largest IT transformation in the history of the automotive industry, fully insourcing what once was a nearly completely outsourced IT function. Today GM IT is a dynamic and fast paced organization that designs, develops and maintains all IT infrastructure, applications and solutions enabling GM’s global operations. From designing and building the next generation of electric and other vehicles to developing a world-class GM experience for our dealers and customers, GM IT is driving real change in the most iconic automaker on the planet. Our team delivers unique enterprise-wide IT solutions in cutting-edge technologies such as mobility, telematics, mission-critical business systems, supercomputing, cloud, vehicle engineering and real-time computing. We offer challenging positions for passionate professionals looking to advance their careers and be a part of an IT organization focused on innovation, speed and business value.', ' Discount on GM vehicles for you, your family and friends.', ""Data Wrangling and Preparation using GM's approved tools: PySpark, Jupyter etc.Stream-processing systems: Storm, Spark-Streaming, etc."", 'For This Role You Will Be Responsible For', ' Tuition assistance and student loan refinancing;', 'Responsibilities', 'In this role you will also lead junior developers, mentor, coach and help them to develop their software development skills in Data Engineering technologies.Demonstrate mastery of many programming languages, tools and/or technologies with emphasis on ETL & Database developmentUnderstand and maintain compliance with GM standards and industry standard methodology', 'Ability to identify tasks which require automation and automate them', 'As a Data Engineer, you will build industrialized data assets and optimize data pipelines in support of Vehicle Quality Business Intelligence and Advance Analytics objectives. You will work closely with our Quality Business teams, forward-thinking Data Scientists, BI Developers, System Architects and Data Architects to deliver value to our vision for the future.', 'In this role you will also lead junior developers, mentor, coach and help them to develop their software development skills in Data Engineering technologies.', 'Ability to tackle software engineering and data problems quickly and completely', 'Understand and maintain compliance with GM standards and industry standard methodology', 'Ability to multi-task and stay organized in a dynamic work environment and work collaboratively with other IT organizations (GDAAS, Platform Engineering etc)', 'Job Description', ""Development activities will include enhancing existing data systems and optimizing ETL systems.Additionally you will lead development activities to migrate out of legacy technologies to GM's Big Data Platform utilizing new technologies such as, Kafka, Hadoop, PySpark, Greenplum and GM's internally developed Big Data tools.""]",Not Applicable,Full-time,Information Technology,Automotive,2021-01-26 11:43:08
Data Engineer,Collabera Inc.,"Chicago, IL",,N/A,"['', 'AWS (must be proficient)', 'Perform unit tests and conducting reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance', 'Must haves:', 'Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment', 'Experience processing data using Spark/Scala', 'Experience with Java', '5-10 years experience of hands on Data Engineering', 'Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Snowflake', 'Experience using python to create data pipelines, write ETL scripts, and perform analysis', 'Fullstack Engineer with experience in REST API’sFront end with React or Angular', 'Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologiesWork with a team of developers with deep experience in machine learning, distributed microservices, and full stack systemsUtilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as SnowflakeShare your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering communityCollaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowermentPerform unit tests and conducting reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance', 'Plusses:', 'Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community', 'Daily Responsibilities:', 'At least 2 years of experience in big data technologies (Cassandra, Accumulo, HBase, Spark, Hadoop, HDFS, AVRO, MongoDB, or Zookeeper)', 'Experience with end to end production (From development to production to product support)', 'Fullstack Engineer with experience in REST API’s', 'Development and testing', 'Front end with React or Angular', 'Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies', '5-10 years experience of hands on Data EngineeringExperience with JavaExperience processing data using Spark/ScalaAWS (must be proficient)Experience using python to create data pipelines, write ETL scripts, and perform analysisDevelopment and testingExperience with end to end production (From development to production to product support)At least 2 years of experience in big data technologies (Cassandra, Accumulo, HBase, Spark, Hadoop, HDFS, AVRO, MongoDB, or Zookeeper)', 'Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems', '\xa0', 'Collabera is looking for a mid-senior level Data Engineer for one of our financial clients in Chicago, IL. This position will be a 12-month contract that will start remotely.']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-01-26 11:43:08
Data Engineer,Lockheed Martin,"Florida, NY",13 hours ago,Be among the first 25 applicants,[''],Entry level,Full-time,Information Technology,Construction,2021-01-26 11:43:08
Data Engineer,LyondellBasell,"Houston, TX",15 hours ago,Be among the first 25 applicants,"['', 'Strong programming and scripting skills in other languages, such as C#, C++, Java, Julia, or Scala', 'Experience with GPU technology, NVIDIA data science development stacks, and CUDA programming', 'Basic Function', 'Experience in providing data engineering solutions in various Data Science domains, such as Predictive Maintenance, Forecasting, Real Time streaming analytics, Image Analysis leveraging deep learning methodologies, and optimization', 'BS degree in Computer Science, Software Engineering, Computer Engineering, or related technical field.', 'Work collaboratively with Data Scientists and business SMEs (Process Engineers, Automation Engineers, Reliability Engineers, other business domain experts) in providing data pipelines and workflows which are critical to delivering data driven solutions on high impact problems', 'Experienced working with Data Scientists on architecting and delivering scalable, end to end advanced analytics solutions', 'Experience with C3.ai is highly favorable, but is not expected', 'Demonstrated ability to work under the direction of others and in a team', 'High level of enthusiasm and a love of data, software development, and data engineering', 'Experience in Real Time historian data, SQL, NoSQL, and/or NewSQL database technologies', 'Develop technical platforms, frameworks, and applications, to provide data, business intelligence, and information', ' MS strongly preferred or PhD 3+ years professional hands on technical experience when combined with advanced education, in software development and/or developing and implementing Data Engineering workflows, pipelines, and applications, with additional experience required in the place of advanced education Strong programming and scripting skills in other languages, such as C#, C++, Java, Julia, or Scala Experience with Azure DevOps, Jupyter Notebooks, or VS Code Experience with stream-processing systems, Big Data querying tools, MapReduce, MongoDB, Cassandra, integration of data from multiple data sources, ETL techniques and frameworks, and/or messaging systems Experienced working with Data Scientists on architecting and delivering scalable, end to end advanced analytics solutions Knowledge of ML techniques & algorithms, such as ANNs, SVM, GBM, Decision Forests, Clustering algorithms Experience with GPU technology, NVIDIA data science development stacks, and CUDA programming Experience with C3.ai is highly favorable, but is not expected ', 'Experience with Linux, Unix, Git, and software development testing frameworks', 'Leverage deep data engineering, architecture, advanced technologies, software development expertise, and code development standards to enable the delivery of advanced analytics projects', 'Experience with modern Data Engineering, ML and Data Science libraries such as Pandas, NumPy, Scikit-Learn, NLTK, Seaborn, Dplyr', 'Preferred Qualifications', ' Place emphasis on delivering value through data engineering pipelines and solutions, through excellent problem solving, development, and implementation of scalable solutions Work collaboratively with Data Scientists and business SMEs (Process Engineers, Automation Engineers, Reliability Engineers, other business domain experts) in providing data pipelines and workflows which are critical to delivering data driven solutions on high impact problems Work collaboratively with IT to ensure proper architecture, security, exception handling, testing, and code development standards are adhered to Develop data workflows and pipelines necessary for algorithm and ML solution deployment and maintenance for chemical manufacturing domains including predictive maintenance, reliability, preventing downtime, industrial automation and optimization, demand forecasting, and improving health and safety Leverage deep data engineering, architecture, advanced technologies, software development expertise, and code development standards to enable the delivery of advanced analytics projects Develop technical platforms, frameworks, and applications, to provide data, business intelligence, and information Network internally and externally to build relationships that foster technology transfer and collaboration ', 'Work collaboratively with IT to ensure proper architecture, security, exception handling, testing, and code development standards are adhered to', 'Experience working with large structured and/or unstructured data and technologies', 'Place emphasis on delivering value through data engineering pipelines and solutions, through excellent problem solving, development, and implementation of scalable solutions', 'Experience with stream-processing systems, Big Data querying tools, MapReduce, MongoDB, Cassandra, integration of data from multiple data sources, ETL techniques and frameworks, and/or messaging systems', 'Competencies', 'Roles & Responsibilities', "" BS degree in Computer Science, Software Engineering, Computer Engineering, or related technical field. 3 years professional hands on technical experience in software development and/or developing and implementing Data Engineering workflows, pipelines, and applications. A higher degree (Master's or PhD) with less experience is also acceptable. Demonstrated ability to work under the direction of others and in a team High level of enthusiasm and a love of data, software development, and data engineering Strong quantitative and problem-solving skills, including strong data, technical, and mathematical knowledge and skills Strong Python, JavaScript, and/or SQL programming and scripting skills Experience in Real Time historian data, SQL, NoSQL, and/or NewSQL database technologies Experience working with large structured and/or unstructured data and technologies Experience with modern Data Engineering, ML and Data Science libraries such as Pandas, NumPy, Scikit-Learn, NLTK, Seaborn, Dplyr Experience in large scale computing, cluster computing, and/or cloud computing (Azure preferably) Experience with Linux, Unix, Git, and software development testing frameworks Experience in providing data engineering solutions in various Data Science domains, such as Predictive Maintenance, Forecasting, Real Time streaming analytics, Image Analysis leveraging deep learning methodologies, and optimization "", 'Experience with Azure DevOps, Jupyter Notebooks, or VS Code', 'Min. Qualifications', 'Develop data workflows and pipelines necessary for algorithm and ML solution deployment and maintenance for chemical manufacturing domains including predictive maintenance, reliability, preventing downtime, industrial automation and optimization, demand forecasting, and improving health and safety', '3+ years professional hands on technical experience when combined with advanced education, in software development and/or developing and implementing Data Engineering workflows, pipelines, and applications, with additional experience required in the place of advanced education', 'Strong Python, JavaScript, and/or SQL programming and scripting skills', 'Network internally and externally to build relationships that foster technology transfer and collaboration', 'MS strongly preferred or PhD', 'Knowledge of ML techniques & algorithms, such as ANNs, SVM, GBM, Decision Forests, Clustering algorithms', ""3 years professional hands on technical experience in software development and/or developing and implementing Data Engineering workflows, pipelines, and applications. A higher degree (Master's or PhD) with less experience is also acceptable."", 'Experience in large scale computing, cluster computing, and/or cloud computing (Azure preferably)', 'Strong quantitative and problem-solving skills, including strong data, technical, and mathematical knowledge and skills']",Entry level,Full-time,Information Technology,Machinery,2021-01-26 11:43:08
Data Engineer,Harnham,"New York, United States",3 hours ago,Be among the first 25 applicants,"['', '$150,000 - $170,000 base salaryHealth benefits401KPTO and sick time off', 'THE BENEFITS:', 'Health benefits', 'Big data experience and using a tool like Hadoop, Spark, or Kafka', 'They are a technology company that grows awareness for local restaurants to compete against major corporate chains. The platform gives the user local recommendations based on their tastes. The food the user orders can be picked up or sent out for delivery.', 'Create and maintain data pipelinesWork with the product team to maintain digital analytics standardsEnsure that data is defined, transformed, optimized, and ready for analystsWork with digital marketing tools like Facebook Ads, AdSense, AdWords, etcMake sure privacy regulations are adhered to', 'Create and maintain data pipelines', 'Commercial experience as a data engineer in a digital marketing analytics space', 'PTO and sick time off', 'KEYWORDS', 'Work with the product team to maintain digital analytics standards', 'Python, Java, Scala, Data Engineering, Digital Marketing, NYC, Facebook Ads, AdWords, Adsense, Hadoop, Spark, Kafka, AWS, SQL', 'NYC', 'YOU WILL NEED:', 'Data Engineer', 'Digital Marketing Analytics\xa0', 'Please register your interest by sending your resume to Jacob Ragland via the Apply link on this page.', 'Work with digital marketing tools like Facebook Ads, AdSense, AdWords, etc', 'Experience with AWS', 'Commercial experience as a data engineer in a digital marketing analytics spaceMust have experience with Facebook Ads, Adsense, AdWords, etcAdvanced SQL knowledgeCommercial experience with Java, Scala, or PythonExperience with AWSBig data experience and using a tool like Hadoop, Spark, or Kafka', '$150,000 - $170,000 base salary', 'Make sure privacy regulations are adhered to', 'THE COMPANY:', 'As a Data Engineer, you will be responsible for creating and maintaining a data pipeline driving Digital Marketing strategy. Having comical experience in digital marketing is a must! You will be building the end-user analytics tools so having this background is a must.', '$150,000 - $170,000', 'Commercial experience with Java, Scala, or Python', '401K', 'Advanced SQL knowledge', 'Ensure that data is defined, transformed, optimized, and ready for analysts', 'Must have experience with Facebook Ads, Adsense, AdWords, etc', 'THE ROLE – Data Engineer', 'HOW TO APPLY', '\xa0']",Mid-Senior level,Full-time,Information Technology,Staffing and Recruiting,2021-01-26 11:43:08
Data Engineer,Austin Fraser,"Austin, Texas Metropolitan Area",,N/A,"['', 'Requirements:', 'Build the infrastructure to capture large amounts of company data', 'What You Get:', 'Competitive Bonus Package', '2+ years as a Data EngineerPython, SQL, AWS, and CI/CD', 'Use your python programming ability to interact with third-party APIs as data sources', '2+ years as a Data Engineer', 'Organization heavily investing in their Data Department', 'Python, SQL, AWS, and CI/CD', 'Collaborate with Data Scientists to find requirements and then source the data', 'In this role you will:', 'Opportunity to work on interesting projects and stay up to date with the latest technology', 'For further details please submit a resume.', 'Location: Austin or San Antonio', 'Austin Fraser Inc is acting as an employment business in relation to this advert. As a professional company, we gladly welcome applications from persons of any age and background and do not intend to discriminate with advert text and terminology.', 'Location: ', '$110k - $130k Base Salary', 'What You Get:\xa0', 'Build the infrastructure to capture large amounts of company dataCollaborate with Data Scientists to find requirements and then source the dataUse your python programming ability to interact with third-party APIs as data sourcesBe a key contributor to a cloud-based data pipeline creation & orchestration', ""As a Data Engineer, you will heavily assist with moving data through the organization's infrastructure. Your support will allow the continued development of innovative analytic tools and data sets that allow customers to make data-driven decisions."", 'Grow, learn, and lead on this team!', '$110k - $130k Base SalaryCompetitive Bonus PackageOrganization heavily investing in their Data DepartmentOpportunity to work on interesting projects and stay up to date with the latest technology', '\xa0', 'Be a key contributor to a cloud-based data pipeline creation & orchestration']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-01-26 11:43:08
Data Engineer,"Pyramid Consulting, Inc","Phoenix, AZ",32 minutes ago,Be among the first 25 applicants,[''],Entry level,Contract,Information Technology,Information Technology and Services,2021-01-26 11:43:08
Data Engineer,Cerner Corporation,"Kansas City, KS",3 hours ago,Be among the first 25 applicants,"['Qualifications', "" Master's degree in Computer Science, Computer Engineering, Software Engineering, Information Systems or related field, or equivalent, relevant work experience  At least 1 year Amazon Web Services (AWS) work experience  At least 1 year cloud native software development work experience "", ' Expectations : ', ' At least 1 year of experience working with big data or cloud technology, including data analysis, data ingestion, data modeling and/or machine learning ', "" Master's degree in Computer Science, Computer Engineering, Software Engineering, Information Systems or related field, or equivalent, relevant work experience "", ' At least 1 year Amazon Web Services (AWS) work experience ', "" Bachelor's degree in Computer Science, Computer Engineering, Software Engineering, Information Systems or related field, or equivalent, relevant work experience "", ' Willing to work additional or irregular hours as needed and allowed by local regulations ', ' Basic Qualifications : ', ' Must currently reside in the Kansas City, MO metro  Willing to work additional or irregular hours as needed and allowed by local regulations  Work in accordance with corporate and organizational security policies and procedures, understand personal role in safeguarding corporate and client assets, and take appropriate action to prevent and report any compromises of security within scope of position  Perform other responsibilities as assigned', ' Perform other responsibilities as assigned', ' Must currently reside in the Kansas City, MO metro ', ' Work in accordance with corporate and organizational security policies and procedures, understand personal role in safeguarding corporate and client assets, and take appropriate action to prevent and report any compromises of security within scope of position ', "" Bachelor's degree in Computer Science, Computer Engineering, Software Engineering, Information Systems or related field, or equivalent, relevant work experience  At least 1 year of experience working with big data or cloud technology, including data analysis, data ingestion, data modeling and/or machine learning  At least 3 years of software engineering work experience "", ' At least 3 years of software engineering work experience ', ' Preferred Qualifications : ', ' At least 1 year cloud native software development work experience ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-01-26 11:43:08
Data Engineer,Mitek Systems,United States,12 hours ago,Be among the first 25 applicants,"['', 'Training and experience in statistics, data manipulation, visualization, and analysis', 'Experience using Python or Go/Golang to perform scripting/development, data management, and data manipulation', 'Experience creating dashboards and effective data visualization', 'Mitek', 'Knowledge of data mining, machine learning, natural language processing, or information retrieval\xa02-4 years of experience in a quantitative role\xa0Experience using Tableau as a tool for the development of data analytics and decision support solutions\xa0Experience deploying software to a cloud platform environment. AWS, GCP, Azure.\xa0Exposure to Big Data platforms and technologies\xa0\xa0Exposure to SQL and NoSQL databases and document stores such as MySQL, Aurora, RedShift, MongoDB, RavenDB, etc.\xa0Experience processing large amounts of structured and unstructured data\xa0Prior experience in secure practices of handling sensitive data and PII\xa0', '3+ years of experience in data engineering or software engineering', 'Develop and maintain processes to ensure the quality,\xa0consistency,\xa0and\xa0versioning\xa0of datasets used in the delivery of Mitek products\xa0', 'To learn more about life at Mitek, visit us at https://www.linkedin.com/company/miteksystems/life/miteknation/', 'What You Bring:', '2-4 years of experience in a quantitative role\xa0', 'Provide additional support as necessary to create and modify\xa0datasets, label data, and manipulate data for product improvement\xa0or development\xa0purposes\xa0\xa0\xa0', 'What Would Be Nice:', ""Bachelor's degree in Mathematics, Statistics, Computer Science, or a related discipline3+ years of experience in data engineering or software engineeringExperience using Python or Go/Golang to perform scripting/development, data management, and data manipulationExperience working with datasets used in the delivery of machine learning-based solutionsExperience with distributed messaging and streaming technologies (RabbitMQ, Kinesis, Kafka)Experience creating dashboards and effective data visualizationTraining and experience in statistics, data manipulation, visualization, and analysisSuccessful history of creating and providing documentation to convey information and drive decision making"", 'Develop, test, and maintain data processing pipelines that\xa0provide access to critical data\xa0and train machine learning models\xa0that are core to\xa0Mitek’s\xa0core product\xa0Develop and maintain processes to ensure the quality,\xa0consistency,\xa0and\xa0versioning\xa0of datasets used in the delivery of Mitek products\xa0Leverage\xa0Tableau for the development of analysis, dashboarding, and reporting solutions\xa0to monitor key measures\xa0associated with the performance of data management initiatives\xa0Design data collection methods, evaluate large amounts of data, decompose high-level information into details, abstract up from low-level information to a general understanding, analyze trends, and distinguish appropriate requirements\xa0Provide additional support as necessary to create and modify\xa0datasets, label data, and manipulate data for product improvement\xa0or development\xa0purposes\xa0\xa0\xa0', ""Bachelor's degree in Mathematics, Statistics, Computer Science, or a related discipline"", 'Experience deploying software to a cloud platform environment. AWS, GCP, Azure.\xa0', 'Prior experience in secure practices of handling sensitive data and PII\xa0', 'Data Engineer', 'Exposure to Big Data platforms and technologies\xa0\xa0', 'Leverage\xa0Tableau for the development of analysis, dashboarding, and reporting solutions\xa0to monitor key measures\xa0associated with the performance of data management initiatives\xa0', 'Mitek\xa0is seeking a\xa0Data Engineer\xa0to join our US Data Science team. As the analytic muscle behind our large-scale, globally distributed Digital Identity Verification cloud platforms, our technology teams rely on us to deliver\xa0large\xa0quantities of\xa0structured, semi-structured, and unstructured\xa0data that enable them to build\xa0new\xa0capabilities\xa0and\xa0derive insights that drive all aspects of our\xa0platform delivery. Our goal is to continually build and implement solutions that support the operationalization of data acquisition and pipelining.', 'Experience processing large amounts of structured and unstructured data\xa0', ""You'll need to have strong skills in Python development, as well as SQL. Experience in a big data environment like Hadoop is critical. And if you know some Golang, that'll be helpful too!"", ""What You'll Do:"", 'Experience working with datasets used in the delivery of machine learning-based solutions', 'Successful history of creating and providing documentation to convey information and drive decision making', 'Experience using Tableau as a tool for the development of data analytics and decision support solutions\xa0', 'Design data collection methods, evaluate large amounts of data, decompose high-level information into details, abstract up from low-level information to a general understanding, analyze trends, and distinguish appropriate requirements\xa0', ""As a Data Engineer at Mitek, you'll join us in continually building and implementing solutions that support the operationalization of data acquisition and pipelining. The data solutions you create will directly impact our machine learning capabilities and global document coverage. We're a big data environment, and you'll be building data sets, working with data sources and data pipelines, and potentially even owning our data modeling. You'll work directly with our internal leaders in our Engineering, Product, R&D, and Testing groups to support and ensure our teams can measure the performance and success of their efforts."", 'Develop, test, and maintain data processing pipelines that\xa0provide access to critical data\xa0and train machine learning models\xa0that are core to\xa0Mitek’s\xa0core product\xa0', 'Experience with distributed messaging and streaming technologies (RabbitMQ, Kinesis, Kafka)', '\xa0', 'Exposure to SQL and NoSQL databases and document stores such as MySQL, Aurora, RedShift, MongoDB, RavenDB, etc.\xa0', 'Knowledge of data mining, machine learning, natural language processing, or information retrieval\xa0']",Associate,Full-time,Engineering,Computer Software,2021-01-26 11:43:08
Senior Data Engineer,Turo,"San Francisco, CA",13 hours ago,Be among the first 25 applicants,"['', 'Benefits', 'Develop, deploy and maintain workflow management tools such as Airflow, Jenkins etc in cloud environments.', 'Monthly Fringe benefits stipend', ' On a daily basis, you will work with members of the team to update our data engineering roadmap and execute upon those initiatives Build new technology stack for highly scalable and available data pipelines used by Turo Product, Engineering, Data Scientists, Marketing, Customer Operations, and Finance teams Design canonical data models for various business domains. Formulate a vision to connect all data in the Turo ecosystem Develop, deploy and maintain workflow management tools such as Airflow, Jenkins etc in cloud environments. Develop, deploy and maintain streaming solutions such as Apache Spark streaming, Kafka and Apache Flink in cloud environments. Using cloud technology such as AWS, Kubernetes, Docker, Redshift, EMR Improve the efficiency of the team through mentoring of junior engineers. Data security automation Data microservices development ', 'Experience working with data tools in the public cloud (AWS, GCP, Azure)', 'Monthly Turo travel credit ', ' Competitive salary and equity for all full-time employees Employer-paid medical, dental, and vision insurance and 401k match Flexible paid time off, 9 paid holidays, paid volunteer time off, and paid parental leave Turo host matching and vehicle reimbursement program Monthly Fringe benefits stipend Monthly Turo travel credit  ', 'Competitive salary and equity for all full-time employees', 'Data microservices development', 'Requirements', 'Strong programmer who views their code as a craft', 'Able to understand technical details and communicate with other engineers, as well as communicating with less technical members from other teams.', 'Experience with a workflow manager — Airflow, Luigi, Jenkins, etc.', 'Using cloud technology such as AWS, Kubernetes, Docker, Redshift, EMR', 'Build new technology stack for highly scalable and available data pipelines used by Turo Product, Engineering, Data Scientists, Marketing, Customer Operations, and Finance teams', '3+ years of relevant experience', 'Enjoys mentoring & teaching other engineers', 'Improve the efficiency of the team through mentoring of junior engineers.', 'Data security automation', 'Design canonical data models for various business domains. Formulate a vision to connect all data in the Turo ecosystem', 'About Turo', ' Past experience building ETL processes Strong programmer who views their code as a craft Experience with a workflow manager — Airflow, Luigi, Jenkins, etc. Experience working with data tools in the public cloud (AWS, GCP, Azure) Able to understand technical details and communicate with other engineers, as well as communicating with less technical members from other teams. Enjoys mentoring & teaching other engineers Familiar with Spark or other big data processing framework is a plus 3+ years of relevant experience ', 'Responsibilities', 'Turo host matching and vehicle reimbursement program', 'Flexible paid time off, 9 paid holidays, paid volunteer time off, and paid parental leave', 'About You', 'Familiar with Spark or other big data processing framework is a plus', 'Past experience building ETL processes', 'Develop, deploy and maintain streaming solutions such as Apache Spark streaming, Kafka and Apache Flink in cloud environments.', 'Employer-paid medical, dental, and vision insurance and 401k match', 'On a daily basis, you will work with members of the team to update our data engineering roadmap and execute upon those initiatives']",Associate,Full-time,Information Technology,Computer Software,2021-01-26 11:43:08
Data Engineer,BioSpace,"Pasadena, CA",17 hours ago,Be among the first 25 applicants,"['', 'Expert in building and managing scalable relational databases, preferably in the life sciences space', 'Proficiency in Linux environment, experience with database languages (e.g., SQL) and experience with version control practices and tools (Git)', 'Position Summary:', 'Build and manage our databases of billions to trillions of chemical structures, intensities, affinities, and data from other biological assays', 'Experience with high-end distributed data processing environments (Spark, Hadoop, etc.)', 'Expert in engineering big data pipelines using modern technologies and cloud infrastructures', 'Highly proficient in Python and the PyData stack (numpy, pandas, scipy, dask, etc.)', 'The Core Responsibilities Of This Job Will Be', 'Experience and Qualifications: ', 'Qualifications Include', 'Company Overview:', 'Design and architect a data warehouse to support downstream analytics', 'Experience with cloud computing services, preferably AWS (EMR, Redshift)', 'Work with our data scientists to incorporate our image processing workflow into the data pipeline', ' Expert in engineering big data pipelines using modern technologies and cloud infrastructures Expert in building and managing scalable relational databases, preferably in the life sciences space Experience with cloud computing services, preferably AWS (EMR, Redshift) Experience with high-end distributed data processing environments (Spark, Hadoop, etc.) Proficiency in Linux environment, experience with database languages (e.g., SQL) and experience with version control practices and tools (Git) Experience with pipeline/workflow managers (Luigi, Airflow, Nextflow, etc.) Highly proficient in Python and the PyData stack (numpy, pandas, scipy, dask, etc.) ', 'Manage and improve our data lake of millions of fluorescence microscopy images', 'Experience with pipeline/workflow managers (Luigi, Airflow, Nextflow, etc.)']",Entry level,Full-time,Information Technology,Online Media,2021-01-26 11:43:08
Data Engineer,Luxoft,"Remote, OR",11 hours ago,Be among the first 25 applicants,"['', 'Languages', ' Insurance domain knowledge; preferably with an insurance carrier.HIPAA and other regulations that govern the access of data ', 'Responsibilities', 'Skills', ' Review and understand data requirements for operational and analytic projects, with a special emphasis on developing scalable data solutions involving data integration, reporting, analytics and data warehousing.Collaborate with business and IT partners to refine data requirements and perform data analysis activities including data profiling, creation of data dictionaries, data transformation rules and integration requirements.Develop and maintain data pipelines and build out new API integrations to support continuing increases in data volume and complexity.Integrate data from a variety of systems into refined data products available to the rest of the enterprise to support both real-time and batch processing.Implement processes and systems to monitor data quality, ensuring production data accuracy and assist in data analysis to troubleshoot and resolve data issues ', ' 10+ Years of IT industry experience, with 5+ years of experience as a Data Engineer or in data related areas.5+ years of strong SQL, PL/SQL experience3+ years of Python development experienceStrong UNIX Scripting experience.Experience working with and/or creating APIs and web servicesSignificant experience interfacing with both customers and management.Strong Hadoop and the Data Lake experience including familiarity with Hive, Alteryx or any other related technologies would be a huge plus.Experience with Data Integration - Informatica, SSIS, Syncsort and Business Intelligence tools - Tableau, SSRS, SAP BusinessObjects ', 'Project Description']",Entry level,Full-time,Other,Information Technology and Services,2021-01-26 11:43:08
Data Engineer,SoFi,"Helena, MT",17 hours ago,Be among the first 25 applicants,"['', 'Passion and curiosity for FinTech', 'Familiarity with big data platforms and tooling (AWS, Snowflake, Kafka, Luigi, Hadoop, Hive, Spark, Cassandra, Airflow, etc).', 'Tuition reimbursement on approved programs', '401(k) and education on retirement planning', 'Competitive salary packages and bonuses', 'Ability to focus on tasks and drive work to completion', 'Monthly contribution up to $200 to help you pay off your student loans', 'Nice To Have', 'Partner with team members to implement and design interfaces and abstractions', 'Bachelor’s degree, ideally in a technical field', 'Comprehensive medical, dental, vision and life insurance benefits', 'Who we are', 'The role', 'A passion and instinct for data quality', 'What You’ll Do', '2+ years experience as a software Engineer', 'What You’ll Need', 'Description', 'Fully stocked kitchen (snacks and drinks)', 'Data exploration and analysis experience using SQL/Python/R/Tableau. Experience surfacing insights using math/statistics/ML techniques', 'Skilled at reading and understanding technical documentation', 'Great health & well-being benefits including: telehealth parental support, subsidized gym program', 'Sharpen your skills as a developer and build technical domain knowledge on data infrastructure modernization', 'Bachelor’s degree, ideally in a technical field2+ years experience as a software EngineerIntellectual curiosity and aptitude to pick up new technical skillsSkilled at reading and understanding technical documentationAbility to focus on tasks and drive work to completionA passion and instinct for data qualityAbility to influence outcomes and discuss technical challenges with team membersStrong fundamentals of data structures, algorithms, and design patternsSoftware development experience in Java, C/C++, or C#Experience building solutions using public clouds (Azure, AWS, GCP)Proficiency with SQL and strong Python development skillsFamiliarity with big data platforms and tooling (AWS, Snowflake, Kafka, Luigi, Hadoop, Hive, Spark, Cassandra, Airflow, etc).', 'Work with amazing product and business managers to deliver compelling features ', 'Why you’ll love working here', 'Intellectual curiosity and aptitude to pick up new technical skills', 'Strong fundamentals of data structures, algorithms, and design patterns', 'Experience building solutions using public clouds (Azure, AWS, GCP)', 'Proficiency with SQL and strong Python development skills', 'Position at SoFi', 'Competitive salary packages and bonusesComprehensive medical, dental, vision and life insurance benefitsGenerous vacation and holidaysPaid parental leave for eligible employees401(k) and education on retirement planningTuition reimbursement on approved programsMonthly contribution up to $200 to help you pay off your student loansGreat health & well-being benefits including: telehealth parental support, subsidized gym programEmployer paid lunch program (except for remote employees)Fully stocked kitchen (snacks and drinks)', 'Manage and evolve cloud-based data warehousing services', 'Generous vacation and holidays', 'Ability to influence outcomes and discuss technical challenges with team members', 'Prior experience with CI/CD (gradle, git, automated testing and deployments)Data exploration and analysis experience using SQL/Python/R/Tableau. Experience surfacing insights using math/statistics/ML techniquesPassion and curiosity for FinTech', 'Paid parental leave for eligible employees', 'Employer paid lunch program (except for remote employees)', 'Directly contribute to high-performance batch and stream data processing systems', 'Directly contribute to high-performance batch and stream data processing systemsManage and evolve cloud-based data warehousing servicesWork with amazing product and business managers to deliver compelling features Partner with team members to implement and design interfaces and abstractionsSharpen your skills as a developer and build technical domain knowledge on data infrastructure modernization', 'Prior experience with CI/CD (gradle, git, automated testing and deployments)', 'Software development experience in Java, C/C++, or C#']",Not Applicable,Full-time,Information Technology,Financial Services,2021-01-26 11:43:08
Data Engineer,EMD Performance Materials,"Tempe, AZ",11 hours ago,Be among the first 25 applicants,"['', 'Your role:', 'Review code developed by other data engineers and check against platform-specific standards, cross-cutting concerns, coding and configuration standards and functional specification of the pipeline', 'Implementation of changes and bug fixes via our company’s change management framework and according to system engineering practices (additional training will be provided)', 'Hadoop general, Data Management, ETL, SQL, DevOps, CI/CD, Programming (Python, Java), REST APIs, General Knowledge of AWS Stack (EC2, S3, EBS…)', 'Work out the best possible balance between technical feasibility and business requirements', 'Acts as business analyst for developing requirements, communicates with internal customers and has interest in driving projects.', 'Participate in end to end project lifecycle, from requirements analysis to go-live and operations of an application', 'Besides working on projects, act as third level support for critical applications; analyze and resolve complex incidents/problems; debug problems', 'Develop data pipelines by ingesting various data sources – structured and un-structured', 'B.Sc. (or higher) degree in Computer Science, Engineering, Mathematics, Physical Sciences or related fields', 'Fluent English skills', 'DevOps project setup following Agile principles (e.g. Scrum)', ' ', '2+ years of experience in system engineering or software development with experience in ETL type work with databases', 'Advanced degree in Computer Science, Engineering, Mathematics, Physical Sciences or related fields', 'Develop data pipelines by ingesting various data sources – structured and un-structured Participate in end to end project lifecycle, from requirements analysis to go-live and operations of an application Review code developed by other data engineers and check against platform-specific standards, cross-cutting concerns, coding and configuration standards and functional specification of the pipeline Document technical work in a professional and transparent way. Create high quality technical documentation Work out the best possible balance between technical feasibility and business requirements Deploy applications on the platform infrastructure with clearly defined checks Implementation of changes and bug fixes via our company’s change management framework and according to system engineering practices (additional training will be provided) DevOps project setup following Agile principles (e.g. Scrum) Besides working on projects, act as third level support for critical applications; analyze and resolve complex incidents/problems; debug problems Work closely with business users, data scientists/analysts to design physical data models ', 'Work closely with business users, data scientists/analysts to design physical data models', 'Who you are:\xa0', 'B.Sc. (or higher) degree in Computer Science, Engineering, Mathematics, Physical Sciences or related fields 2+ years of experience in system engineering or software development with experience in ETL type work with databases ', 'Hadoop general, Data Management, ETL, SQL, DevOps, CI/CD, Programming (Python, Java), REST APIs, General Knowledge of AWS Stack (EC2, S3, EBS…) Advanced degree in Computer Science, Engineering, Mathematics, Physical Sciences or related fields Acts as business analyst for developing requirements, communicates with internal customers and has interest in driving projects. Fluent English skills ', 'Deploy applications on the platform infrastructure with clearly defined checks', 'What we offer: ', 'Curious? ', 'Document technical work in a professional and transparent way. Create high quality technical documentation']",Mid-Senior level,Full-time,Information Technology,Chemicals,2021-01-26 11:43:08
Data Engineer,Vaco,"Brentwood, TN",20 hours ago,Be among the first 25 applicants,"['Experience with cloud technologies like Azure, Google Cloud, or AWS strongly preferred.', '3+ years of in-depth Microsoft SQL development experience.3+ years of experience maintaining Microsoft SQL environment up to current version (at least SQL/Server 2016/2017) Extensive knowledge of T-SQL, ETL, Stored Procedures, SSIS, SSRS, and BI toolsExperience working in a fast paced Agile (Scrum/Kanban) environmentEffective communication skills, written, verbal and interpersonal.Programming experience with C#, .net core, or javascript preferred Experience with cloud technologies like Azure, Google Cloud, or AWS strongly preferred.', '3+ years of experience maintaining Microsoft SQL environment up to current version (at least SQL/Server 2016/2017) ', 'Responsibilties:', 'Drive data architecture and modeling recommendations', 'Conduct data analysis and interpret data to answer key business questions', 'Experience working in a fast paced Agile (Scrum/Kanban) environment', 'Requirements:', 'Extensive knowledge of T-SQL, ETL, Stored Procedures, SSIS, SSRS, and BI tools', 'Programming experience with C#, .net core, or javascript preferred ', '3+ years of in-depth Microsoft SQL development experience.', 'Perform data ETL processing from multiple sources ', 'Develop report deliverables and data visualizations producing valuable insights to stakeholders', 'Work closely with client stakeholders and coordinate delivery of projects in scope and on timeWork as part of a team to deliver a successful large scale data migration Drive data architecture and modeling recommendationsPerform data ETL processing from multiple sources Conduct data analysis and interpret data to answer key business questionsDevelop report deliverables and data visualizations producing valuable insights to stakeholders', 'We are unable to sponsor or transfer H1B Visas at this time.', 'Effective communication skills, written, verbal and interpersonal.', 'Work closely with client stakeholders and coordinate delivery of projects in scope and on time', 'Work as part of a team to deliver a successful large scale data migration ']",Entry level,Full-time,Information Technology,Computer Software,2021-01-26 11:43:08
Associate Data Scientist Engineer,Gap Inc.,"San Francisco, CA",12 hours ago,28 applicants,"['', 'BA/BS in a technical or engineering field (Master’s preferred).', 'Employees can take up to five “on the clock” hours each month to volunteer at a charity of their choice.*', ' Merchandise discount for our brands: 50% off regular-priced merchandise at Old Navy, Gap, Banana Republic and Athleta, and 30% off at Outlet for all employees. One of the most competitive Paid Time Off plans in the industry.* Employees can take up to five “on the clock” hours each month to volunteer at a charity of their choice.* Extensive 401(k) plan with company matching for contributions up to four percent of an employee’s base pay.* Employee stock purchase plan.* Medical, dental, vision and life insurance.* See more of the benefits we offer. ', 'Maintain and support deployed solutions and data products.', 'About The Role', 'Strong understanding of relational databases and SQL.', 'Medical, dental, vision and life insurance.*', 'Excellent communication skills. Ability to effectively communicate with both technical and non-technical audiences.', 'Navigate various data sources and efficiently locate data in a complex data ecosystem.', ' BA/BS in a technical or engineering field (Master’s preferred). 1-3 years of experience in a data engineering or full-stack data scientist role. Strong understanding of relational databases and SQL. Solid programming foundations and proficiency with data related languages such as Python/Spark/R. Excellent communication skills. Ability to effectively communicate with both technical and non-technical audiences.', 'See more of the benefits we offer.', 'Work closely with our data scientists to ensure production models are built using a scalable back-end.', 'Solid programming foundations and proficiency with data related languages such as Python/Spark/R.', 'About Gap Inc.', 'One of the most competitive Paid Time Off plans in the industry.*', '1-3 years of experience in a data engineering or full-stack data scientist role.', 'Merchandise discount for our brands: 50% off regular-priced merchandise at Old Navy, Gap, Banana Republic and Athleta, and 30% off at Outlet for all employees.', 'Develop, deploy, and support analytic data products, such as data marts, ETL’s (extract/transform/load), functions (in Python/SQL/Spark/R), and visualizations.', ' Partner with internal customers to understand business needs and build strong relationships with key stakeholders. Develop, deploy, and support analytic data products, such as data marts, ETL’s (extract/transform/load), functions (in Python/SQL/Spark/R), and visualizations. Navigate various data sources and efficiently locate data in a complex data ecosystem. Work closely with our data scientists to ensure production models are built using a scalable back-end. Maintain and support deployed solutions and data products.', 'For eligible employeesGap Inc. is an equal-opportunity employer and is committed to providing a workplace free from harassment and discrimination. We are committed to recruiting, hiring, training and promoting qualified people of all backgrounds, and make all employment decisions without regard to any protected status. We have received numerous awards for our long-held commitment to equality and will continue to foster a diverse and inclusive environment of belonging. This year, we’ve been named as one of the Best Places to Work by the Humans Rights Campaign for the fourteenth consecutive year and have been included in the 2019 Bloomberg Gender-Equality Index for the second year in a row.', 'Extensive 401(k) plan with company matching for contributions up to four percent of an employee’s base pay.*', 'Employee stock purchase plan.*', 'Partner with internal customers to understand business needs and build strong relationships with key stakeholders.']",Entry level,Full-time,Engineering,Marketing and Advertising,2021-01-26 11:43:08
Data Engineer,Dashlane,"New York, NY",6 hours ago,Over 200 applicants,"['', 'You have experience with data lakes and designing and maintaining data solutions using Spark and AWS serverless services such as Kinesis, Lambda, or SQS', ""We're Also Looking For"", '  You have 3+ years of experience as a data engineer, business intelligence analyst, or in a highly analytical role  You have 3+ years of experience with a scripting language (preferably Python) for data processing and analysis You have experience designing SQL tables, choosing indexes, tuning queries, and optimizations ac ross different functional en vironments.   You have experience writing complex SQL queries and using a BI tool  ', ' You have a passion for sharing the value of data and communicating insights to a broad audience with varying levels of technical expertise You have experience with data lakes and designing and maintaining data solutions using Spark and AWS serverless services such as Kinesis, Lambda, or SQS You have experience administrating, ingesting, and monitoring data in data warehouses such as Amazon Redshift or Microsoft SQL Server You are a self-starter that can work in a fast-paced, distributed environment, as you will be collaborating with our Paris and Lisbon teams ', ' Build data pipelines and python-based ETL tools for acquiring, processing, and delivering data Develop data models and schemas in our data warehouse that enable performant, intuitive analysis Handle the challenges that come with managing terabytes of data Collaborate with business leaders and analysts to define key metrics and build reporting to monitor and understand company performance Develop the server applications and APIs that are used by our Data Team ', 'Develop the server applications and APIs that are used by our Data Team', 'Build data pipelines and python-based ETL tools for acquiring, processing, and delivering data', 'Requirements', 'You have experience designing SQL tables, choosing indexes, tuning queries, and optimizations ac ross different functional en vironments. ', 'Handle the challenges that come with managing terabytes of data', 'Your Interview Experience', 'You have 3+ years of experience with a scripting language (preferably Python) for data processing and analysis', 'You are a self-starter that can work in a fast-paced, distributed environment, as you will be collaborating with our Paris and Lisbon teams', 'Develop data models and schemas in our data warehouse that enable performant, intuitive analysis', 'At Dashlane You Will', 'You have a passion for sharing the value of data and communicating insights to a broad audience with varying levels of technical expertise', ' You have experience writing complex SQL queries and using a BI tool ', 'Diversity, Equity, Inclusion And Belonging At Dashlane', 'You have experience administrating, ingesting, and monitoring data in data warehouses such as Amazon Redshift or Microsoft SQL Server', 'Collaborate with business leaders and analysts to define key metrics and build reporting to monitor and understand company performance', ""We continue to hire passionate people to join our mission of fixing the UX of the internet. Due to the ongoing public health crisis, our interview and onboarding processes will remain fully remote until further notice. At times like this, we're even more committed to providing you support and flexibility while you interview and onboard for your new job. "", ' You have 3+ years of experience as a data engineer, business intelligence analyst, or in a highly analytical role ', 'About Dashlane', 'About The Role']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-01-26 11:43:08
Data Engineer,Scalable,Dallas-Fort Worth Metroplex,21 hours ago,Be among the first 25 applicants,"['', '\xa0Above market compensation package\xa0Career growth opportunities\xa0Global working environment', 'Experience building data pipelines and ETL design (implementation and maintenance).', 'Experience with AWS or other cloud provider.', 'Preferred:\xa0', 'We’re looking for an experienced Data Engineer to help deliver critical business intelligence through our data warehouse\xa0(Redshift)\xa0and data lake\xa0(AWS\xa0S3). Our Data Engineering team handles all aspects of managing our batched and real-time data pipelines from four e-commerce products.', '\xa0Career growth opportunities', 'Analytical experience debugging slow queries and scripts.', 'Scalable', 'Benefits & Perks:', ""Scalable\xa0is on an incredible growth trajectory and we're looking for passionate software engineers to help us build the future of ecommerce.\xa0Scalable builds world-class physical and software infrastructure to help entrepreneurs compete against the today’s giants in ecommerce"", 'Proficiency in using BI dashboard tools.', 'Deep understanding of AWS services: EMR, S3, Redshift and Terraform.', '\xa0Above market compensation package', 'Experience writing and executing complex SQL queries.', 'Robust experience with Spark, Redshift, Python and Jenkins.', '2+ years of data engineering experience.', 'Scrum/Agile software development process.', 'Requirements:\xa0', '\xa0Benefits & Perks:', 'Experience with data technologies, Hadoop, Hive, Postgres, Kafka, Solr.', 'Experience with data technologies, Hadoop, Hive, Postgres, Kafka, Solr.Deep understanding of AWS services: EMR, S3, Redshift and Terraform.Operational experience with Jenkins or Airflow.Proficiency in using BI dashboard tools.Analytical experience debugging slow queries and scripts.', '2+ years of data engineering experience.Regularly processes TBs of data quickly with cloud based distributed solutions.Robust experience with Spark, Redshift, Python and Jenkins.Experience writing and executing complex SQL queries.Experience building data pipelines and ETL design (implementation and maintenance).Experience with AWS or other cloud provider.Scrum/Agile software development process.', '\xa0Global working environment', 'Regularly processes TBs of data quickly with cloud based distributed solutions.', 'This position is responsible for understanding stakeholders requirements and building out ETL from variety of data sources\xa0(Mongo,\xa0MySQL, Kafka, ElasticSearch, Third Party REST APIs, etc).', ':\xa0', 'Operational experience with Jenkins or Airflow.']",Associate,Full-time,Information Technology,Information Technology and Services,2021-01-26 11:43:08
Data Engineer,Centene Corporation,"Detroit, MI",22 hours ago,Be among the first 25 applicants,"['', ' Contribute to the development and maintenance of real-time processing applications Contribute to the creation and maintenance of optimal data pipeline architectures Conduct maintenance and support for core infrastructure health, system upgrades, monitoring, CI/CD and logging Research streaming best practices and proper stream architecture Collaborate with team members to better understand existing data requirements and validation rules Analyze trends in data sets and contribute to the development of algorithms in order to improve upon the usefulness of raw data ', 'Analyze trends in data sets and contribute to the development of algorithms in order to improve upon the usefulness of raw data', 'Centene is an equal opportunity employer that is committed to diversity, and values the ways in which we are different. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, veteran status, or other characteristic protected by applicable law.', 'Contribute to the development and maintenance of real-time processing applications', 'Collaborate with team members to better understand existing data requirements and validation rules', 'Position Purpose', 'Contribute to the creation and maintenance of optimal data pipeline architectures', 'Preferred', 'Research streaming best practices and proper stream architecture', 'Conduct maintenance and support for core infrastructure health, system upgrades, monitoring, CI/CD and logging', 'Education/Experience']",Not Applicable,Full-time,Information Technology,Hospital & Health Care,2021-01-26 11:43:08
Data Engineer,N/A,"Los Altos, CA",2 hours ago,Be among the first 25 applicants,"['', 'Cerebras Systems develops computing systems to accelerate deep learning in the data center. Our first product, the CS-1, is the first computer ever made to host a chip as large as a whole silicon wafer, the Wafer Scale Engine. It was built from the ground up, it is programmable with the Cerebras software platform and is powered by a radically innovative system that fits directly into existing data center infrastructure.', 'Experience with Agile development methodologies and industry standard software development lifecycle processes (Jira, Git, code review, design documentation).', 'Continuously improve on the data analytics team’s best practices in continuous integration and development (CI/CD).', '\ufeffNice to have:', 'Proficiency in SQL and no-SQL databases, such as PostgreSQL, InfluxDB, MongoDB.', 'Experience with RESTful APIs and networking.', 'Build and maintain a data lake in AWS S3, ensuring that data can be made available for consumption in AWS Athena and AWS Quicksight.Design and implement end-to-end data pipelines using Airflow, Spark, and various AWS tools such as Glue, Lambda, and RDS.Improve approaches to efficiently handle ever-increasing volumes of data.Continuously improve on the data analytics team’s best practices in continuous integration and development (CI/CD).Interface regularly with multiple hardware/software development groups in order to enhance performance, functional coverage and usability of the analytics tools.Support existing processes running in production.Partner with hardware, manufacturing engineers and program management to translate data insights into decisions and actions.Support engineering teams with ad-hoc data analysis needs, identifying opportunities for automation.Continuously evaluate team’s processes to maintain a positive and efficient engineering culture.', 'You will be building the data analytics infrastructure that serves the increasing needs of a rapidly expanding and data-driven hardware team. You will work with leaders from industry, alongside system, electrical and ASIC engineers, as well as other software and data engineers and scientists. This role is highly cross-functional, collaborative and has direct impact on effective development and manufacture of Cerebras’ wafer-scale computing systems.', 'Ability to bring thoughtful perspectives, creativity, and a positive attitude to solve problems at scale.', 'Responsibilities include:', 'Support existing processes running in production.', 'Interface regularly with multiple hardware/software development groups in order to enhance performance, functional coverage and usability of the analytics tools.', 'Improve approaches to efficiently handle ever-increasing volumes of data.', 'Experience with a variety of technologies and ability to pick the right tool for the job.', 'Skills and Qualifications', 'Design and implement end-to-end data pipelines using Airflow, Spark, and various AWS tools such as Glue, Lambda, and RDS.', 'Continuously evaluate team’s processes to maintain a positive and efficient engineering culture.', 'Well-versed in AWS cloud computing and tools for data analytics, in particular AWS S3, AWS RDS, and Airflow.Excellent Python programming skills.Proficiency in SQL and no-SQL databases, such as PostgreSQL, InfluxDB, MongoDB.Experience building CI/CD pipelines.Experience with Agile development methodologies and industry standard software development lifecycle processes (Jira, Git, code review, design documentation).Experience with a variety of technologies and ability to pick the right tool for the job.Ability to bring thoughtful perspectives, creativity, and a positive attitude to solve problems at scale.', 'The Role', 'Partner with hardware, manufacturing engineers and program management to translate data insights into decisions and actions.', 'Excellent Python programming skills.', 'Well-versed in AWS cloud computing and tools for data analytics, in particular AWS S3, AWS RDS, and Airflow.', 'Experience with RESTful APIs and networking.Familiarity with gRPC.JavaScript.HTML.', 'Required:', 'Experience building CI/CD pipelines.', 'Build and maintain a data lake in AWS S3, ensuring that data can be made available for consumption in AWS Athena and AWS Quicksight.', 'HTML.', 'JavaScript.', 'Support engineering teams with ad-hoc data analysis needs, identifying opportunities for automation.', 'Familiarity with gRPC.']",Mid-Senior level,Full-time,Information Technology,Computer Hardware,2021-01-26 11:43:08
"Associate Data Engineer, Commercial Analytics",EAB,"Richmond, VA",16 hours ago,Be among the first 25 applicants,"['', ' Collaborate with your colleagues to understand team needs and discover the best solutions. ', 'Benefits', ' Leverage best practices in continuous integration and delivery. ', ' 20+ days of PTO annually, in addition to paid firm holidays ', 'Primary Responsibilities', ' 401(k) retirement plan with company match ', ' Source, map, and integrate external data sets in to existing infrastructure, including data from newly acquired business units. ', ' Make commercial performance data simpler and easier to use for analysts and data scientists. ', ' Medical, dental, and vision insurance; dependents and domestic partners eligible ', '  Bachelor’s degree, ideally in an analytical/quantitative field (e.g. computer science, statistics, etc.).   2+ years of experience using SQL to manage complex and disparate data.   Strong interpersonal skills, including the ability to interpret nuanced, contextual requests.   Self-starter with proven ability to stay organized and multi-task in a fast-paced environment.   Strong desire to solve challenging problems, and to constantly improve technical/analytical skills by learning new methods.  ', ' Strong interpersonal skills, including the ability to interpret nuanced, contextual requests. ', 'Associate', ' Bachelor’s degree, ideally in an analytical/quantitative field (e.g. computer science, statistics, etc.). ', '  Use SQL to develop & maintain efficient and reliable data pipelines that support advanced analytics.   Make commercial performance data simpler and easier to use for analysts and data scientists.   Source, map, and integrate external data sets in to existing infrastructure, including data from newly acquired business units.   Collaborate with your colleagues to understand team needs and discover the best solutions.   Leverage best practices in continuous integration and delivery.   Help drive optimization, testing and tooling to improve data quality.  ', ' Self-starter with proven ability to stay organized and multi-task in a fast-paced environment. ', 'Basic Qualifications', ' Consistent with our belief that our employees are our most valuable resource, EAB offers a competitive and inclusive benefits package. ', ' Experience with Salesforce or other customer relationship management (CRM) systems. ', 'Data Engineer', 'Commercial Analytics', ' Programming experience with R and/or Python. ', ' Benefits kick in day one, see the full details here. ', 'The Role In Brief', 'Ideal Qualifications', ' Help drive optimization, testing and tooling to improve data quality. ', ' Infertility treatment coverage and adoption or surrogacy assistance ', '  Experience with Salesforce or other customer relationship management (CRM) systems.   Strong project management skills.   Programming experience with R and/or Python.   Experience in server/database administration.  ', ' Experience in server/database administration. ', ' Use SQL to develop & maintain efficient and reliable data pipelines that support advanced analytics. ', ' Strong project management skills. ', ' Daytime leave policy for community service or fitness activities (up to 10 hours a month each) ', ' Dynamic growth opportunities with merit-based promotion philosophy ', ' Phase Back to Work program for employees returning from parental leave ', ' Strong desire to solve challenging problems, and to constantly improve technical/analytical skills by learning new methods. ', ' Wellness programs including gym discounts and incentives to promote healthy living ', ' 2+ years of experience using SQL to manage complex and disparate data. ', 'About EAB', ' Paid parental leave for birthing and non-birthing parents ', '  Consistent with our belief that our employees are our most valuable resource, EAB offers a competitive and inclusive benefits package.   Medical, dental, and vision insurance; dependents and domestic partners eligible   401(k) retirement plan with company match   20+ days of PTO annually, in addition to paid firm holidays   Daytime leave policy for community service or fitness activities (up to 10 hours a month each)   Paid parental leave for birthing and non-birthing parents   Phase Back to Work program for employees returning from parental leave   Infertility treatment coverage and adoption or surrogacy assistance   Wellness programs including gym discounts and incentives to promote healthy living   Dynamic growth opportunities with merit-based promotion philosophy   Benefits kick in day one, see the full details here.  ']",Associate,Full-time,Education,Education Management,2021-01-26 11:43:08
Data Engineer,J2 Solutions,"Philadelphia, PA",23 hours ago,28 applicants,"['', '·\xa0\xa0\xa0\xa0\xa0Write, update, and maintain ETL jobs\xa0across our data pipelines (mostly in Airflow).', '·\xa0\xa0\xa0\xa0\xa0Experience documenting both non-technical and technical requirements.', '·\xa0\xa0\xa0\xa0\xa0Also interested in relevant experience including:', 'Our client in Philadelphia is seeking a Data Engineer to joing their Data Engineering team in their Data Science Department:', '·\xa0\xa0\xa0\xa0\xa0Workflow and pipeline development\xa0to ensure reliability, availability, and consistency.', '·\xa0\xa0\xa0\xa0\xa0Communication and Relationship-building –\xa0with technical peers and some stakeholders', 'Successful candidates will demonstrate the following through their experience (typically 3-5 years as a Data Engineer):', '·\xa0\xa0\xa0\xa0\xa0Automation, monitoring, and alerting\xa0– creating these tools based on existing designs and frameworks, resolving bugs and issues.', '·\xa0\xa0\xa0\xa0\xa0Consult to software engineers on data-related changes\xa0to organization’s suite of software applications, including schema/model design, table structure, and data collection.', '·\xa0\xa0\xa0\xa0\xa0Data Encapsulation & Transfer methodologies\xa0– understands standards for file formats and transfer methods.', '\to Experience with BI implementations/uplifts (we currently use Looker) and/or Data Governance models and methods.', '·\xa0\xa0\xa0\xa0\xa0Cloud engineering\xa0– working towards certification on any of the major hyperscale cloud platforms.', '·\xa0\xa0\xa0\xa0\xa0Data Modeling and Warehousing\xa0– proficient understanding of relational data structures and schemas; some familiarity with semi-structured, unstructured (big data) schemas', '·\xa0\xa0\xa0\xa0\xa0Support the development of machine learning models\xa0with productionizing, monitoring, and alerting tools.', '·\xa0\xa0\xa0\xa0\xa0Systems Engineering\xa0– on-system service management, typically in *nix environments.', '·\xa0\xa0\xa0\xa0\xa0Cloud-based Solution Implementation, of data platforms and infrastructure, including event-driven architectures, microservices and pattern design, supporting compliance and regulated environments (including PII and PHI)', '·\xa0\xa0\xa0\xa0\xa0Engage with colleagues and collaborators\xa0using curiosity, critical thinking, a drive to completion, empathy, and a focus on impact.', '·\xa0\xa0\xa0\xa0\xa0Follow existing data access and performance design standards\xa0for the data platform, software engineering, and all products and services accessing organization’s information.', '·\xa0\xa0\xa0\xa0\xa0Collaborate with internal customers\xa0to identify ongoing platform improvements (teams including Analytics, Projects, Policy, Software Engineering, and others throughout the organization).', '\to Machine learning techniques, productionizing machine learning models, and/or creating models.', '·\xa0\xa0\xa0\xa0\xa0Implement continuous improvements\xa0using our existing tools/technologies, which include SQL, Airflow, Python, Docker/Kubernetes, and others such as Terraform and Apache Beam. May also be expected to research and select other tools when the situation demands.', '·\xa0\xa0\xa0\xa0\xa0Experience documenting data processes and data requirements in medium-to-large sized environments.']",Mid-Senior level,Full-time,Engineering,Staffing and Recruiting,2021-01-26 11:43:08
Data Engineer,Jobot,"San Jose, CA",8 hours ago,Be among the first 25 applicants,"['', 'Qualifications', 'Save Lives with San Jose based COVID Diagnostics Startup', 'Why join us?', 'Job Details', 'A Bit About Us']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-01-26 11:43:08
Software Engineer - Data Science,Cisco,"Austin, Texas Metropolitan Area",4 hours ago,Be among the first 25 applicants,"['', '#WeAreCisco, where each person is unique, but we bring our talents to work as a team and make a difference powering an inclusive future for all.', 'But “Digital Transformation” is an empty buzz phrase without a culture that allows for innovation, creativity, and yes, even failure (if you learn from it.)', 'So, you have colorful hair? Don’t care. Tattoos? Show off your ink. Like polka dots? That’s cool. Pop culture geek? Many of us are. Passion for technology and world changing? Be you, with us!', ""Who You'll Work With"", 'Strong Python development skills', 'Develop software used for data acquisition, data analytics, data visualization, data distribution, and test automation and develop client and server side software in a networked multi-processing environment. Utilize artificial intelligence/machine learning (AI/ML) pipelines as well as statistical algorithms. Collaborate with the test engineering, QA and ops teams to drive and solidify design requirements, ensure stable software deployments, and ensure smooth transitions to the operations team. Work on technical functional specifications (TFS), user guides, and training documentation to capture low-level design specifications, software usage guidelines, and user training detail, and provide training where appropriate. Travel to deployment sites to gather system requirements or ensure smooth deployments where appropriate, and provide support for production issues', ""Bachelor's Degree (BA/BS/etc.) Computer Science, Computer Engineering, Electrical Engineering and 5+ years of experience or equivalentStrong Python development skillsSolid understanding of Web Services, XML, JSON, REST, Sockets, Multi-Thread, Linux OS, Windows OS, Data Visualization Tools, JenkinsStrongly desired:\xa0Machine learning (ML) pipelines and artficial intelligence (AI) experienceGood to have: C, C++, C#, Java, JavaScript, JQuery, Crypto/Secure programming, MySQL, MS-SQL, Software Development Lifecycle, Manufacturing Test Process, Delphi, COM, Networking protocolsFluent English speaking and writing. Strong communication skills requiredMust be a great teammate with good social skillsMust have good analytic and troubleshooting skills"", 'We embrace digital, and help our customers implement change in their digital businesses. Some may think we’re “old” (36 years strong) and only about hardware, but we’re also a software company. And a security company. We even invented an intuitive network that adapts, predicts, learns and protects. No other company can do what we do – you can’t put us in a box!', 'Solid understanding of Web Services, XML, JSON, REST, Sockets, Multi-Thread, Linux OS, Windows OS, Data Visualization Tools, Jenkins', 'We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.', 'Fluent English speaking and writing. Strong communication skills required', 'We are looking for innovative engineers that like to work in a diverse worldwide team and that have a passion to base technical decision making on solid data science. You like to solve business problems through forensic data analysis and the application of AI/ML solutions. Your are curious and resourceful. You are not shy to speak you mind. Troubleshooting, root causing and solving technical/business problems is fun for you. You are always pushing the limits and try to understand the bigger picture. You are interested in collaboration across organizational boundaries.', ""What You'll Do"", 'You will work with the technical team of Supply Chain operations to deliver smart test and quality solutions for our factories and regional support teams worldwide. The team is distributed across multiple states in the US and Bangalore, India. Our internal and external partners are located in the US, Mexico, South America, Europe and South East Asia. The teams are very diverse, innovative, resourceful and result orientated. We are empowering our employees to use their expertise to take good business risks without compromising quality.', 'Must be a great teammate with good social skills', ""Bachelor's Degree (BA/BS/etc.) Computer Science, Computer Engineering, Electrical Engineering and 5+ years of experience or equivalent"", 'Must have good analytic and troubleshooting skills', 'Our Minimum Qualifications For This Role', 'Strongly desired:\xa0Machine learning (ML) pipelines and artficial intelligence (AI) experience', 'Day to day, we focus on the give and take. We give our best, give our egos a break, and give of ourselves (because giving back is built into our DNA.) We take accountability, bold steps, and take difference to heart. Because without diversity of thought and a dedication to equality for all, there is no moving forward.', 'Who You Are', 'Who We Are', 'Good to have: C, C++, C#, Java, JavaScript, JQuery, Crypto/Secure programming, MySQL, MS-SQL, Software Development Lifecycle, Manufacturing Test Process, Delphi, COM, Networking protocols']",Associate,Full-time,Engineering,Computer Networking,2021-01-26 11:43:08
Data Engineer,Robert Half,"Iselin, NJ",10 hours ago,Be among the first 25 applicants,"['', ' Algorithmic optimization to streamline analytical outcomes;', ' Creating interfaces to various data sources and work with team on data model design and implementation; Algorithmic optimization to streamline analytical outcomes; Applying advanced analytics to large data sets as needed; Works with the team to ensure successful and scalable implementation of business intelligence deliverables.', ' Strong understanding and hands-on experience with ELT and ETL principles.', ' 5 to 7 years’ experience plus appropriate degree;', ' Creating interfaces to various data sources and work with team on data model design and implementation;', ' Received a bachelor’s in the fields of Science or Mathematics or related degree;', ' Applying advanced analytics to large data sets as needed;', ' Works with the team to ensure successful and scalable implementation of business intelligence deliverables.', 'Requirements', 'Description', ' 5 to 7 years’ experience plus appropriate degree; Received a bachelor’s in the fields of Science or Mathematics or related degree; Hands on experience working with BI developers and data analysts to understand the reporting requirements and build out data models for reporting; Strong hands on experience with SQL, preferably in SQL Server, SSRS/SSAS/SSIS; Strong understanding and hands-on experience with ELT and ETL principles.', ' Hands on experience working with BI developers and data analysts to understand the reporting requirements and build out data models for reporting;', 'Roles & Responsibilities', ' Strong hands on experience with SQL, preferably in SQL Server, SSRS/SSAS/SSIS;']",Entry level,Temporary,Information Technology,Information Technology and Services,2021-01-26 11:43:08
Data Engineer,Gridiron IT,"Chantilly, VA",3 hours ago,Be among the first 25 applicants,[],Entry level,Full-time,Information Technology,Information Technology and Services,2021-01-26 11:43:08
Data Engineer,Verstand AI,United States,17 hours ago,Over 200 applicants,"['', 'Solid understanding of data design patterns and best practices', ""Our commercial practice is in need of data engineers that can design and manage all aspects of ETL/ELT requirements needed by our data analysts and data scientists. Moreover, Verstand data engineers understand and implement solid Cloud DevOps practices. As a key member of the team, you'll have the ability to shape projects, bring on new technologies and interact with stakeholders to address important issues for various companies and society. "", ""Verstand's data engineers have cloud data warehouse and strong SQL expertise along with Python development skills. The data engineering resource will be instrumental in significant initiatives to transform all aspects of data management services for a commercial and public sector clients. The work will lead to a revised approach to enterprise data warehousing, business intelligence and data wrangling/ELT/ETL and will leverage the cloud to apply advanced analytics and data mining capabilities. Moreover, s/he will play a significant role in the implementation, maintenance and continuous improvement of an enterprise data platform. This individual will work closely with the business stakeholders, software development and support teams, as well as cloud DevOps."", '6+ years of experience in cloud environment, distributed systems, system automation, and real-time platforms.', '2+ years of production level experience with Kafka, Python, SQL, and shell scripting', 'Experience with setting up and operating data pipelines (batch and real time) and data wrangling procedures using Python or SQL in a cloud environment.', 'Verstand AI is a data driven consultancy and product development company that works in multiple commercial business verticals (e.g., retail, pharma, finance, energy) and global public sector entities. The firm specializes in implementing data governance and developing data architectures, business intelligence platforms and machine learning models for client stakeholders.', 'WHAT IS VERSTAND AI ALL ABOUT?', 'Model data and metadata to support ad-hoc and pre-built reporting', 'Collaborate with engineers and business customers to understand data needs, capture requirements and deliver complete BI solutions', 'POSITION HIGHLIGHTS:', 'Familiarity with agile software development practices and drive to ship quickly', 'Retail production experience highly desired', 'Experience with setting up and operating data pipelines (batch and real time) and data wrangling procedures using Python or SQL in a cloud environment.Collaborate with engineers and business customers to understand data needs, capture requirements and deliver complete BI solutionsDesign and build data extraction, transformation, and loading processes by writing custom data pipelinesDesign, implement and support platforms that can provide ad-hoc access to large datasets and unstructured dataModel data and metadata to support ad-hoc and pre-built reportingTune application and query performance using performance profiling tools and SQLBuild data expertise and own data quality for allocated areas of ownership', 'Tune application and query performance using performance profiling tools and SQL', 'Experience with building large scale data processing systems', '5+ years of experience with data warehouse schema design and data modeling', 'Working knowledge of data visualization tools such as Tableau, PowerBI and Looker is a plus', 'Experience in leading change, taking initiative, and driving results', '3+ years experience with cloud databases (e.g., AWS, Azure, GCP)', 'Effective communication skills and strong problem-solving skills', 'KEY RESPONSIBILITIES:', 'All successful data engineers, business intelligence analysts, and data scientists share a love for solving interesting problems. As a data team, we get to work in many different fields, ranging from healthcare to finance, energy to insurance, and public policy to law enforcement.', '7+ years of experience in using SQL and databases in a business environment', 'WHY WORK AT VERSTAND AI?', ""Verstand AI is an equal opportunity employer and offers competitive compensation and benefits. The firm's goal is to make data manageable for its clients and provide the means for both commercial success and social good. To that end, Verstand staff have opportunities to work in multiple business verticals and take part in data for social benefit programs."", ""Master's degree in Computer Science is a plus"", 'Experience with microservice patterns, API development, and containers', 'Minimum Experience, Skills and Education', 'Design and build data extraction, transformation, and loading processes by writing custom data pipelines', 'JOB REQUIREMENTS:', 'Design, implement and support platforms that can provide ad-hoc access to large datasets and unstructured data', '5+ years of experience in custom ETL design, implementation, and maintenance', 'Experience in analyzing data to identify deliverables, gaps, and inconsistencies in data sets', 'Build data expertise and own data quality for allocated areas of ownership', 'Experience with batch and stream processing (Confluent preferred)', 'Proven ability and desire to mentor others in a team environment', ""Bachelor's degree from four-year College or university in Computer Science, Technology or related field"", ""We can guarantee that you will never stop learning! Our energetic and engaging environment will help you tackle the hardest problems as part of a highly-cooperative team. So, if that sounds interesting, we'd love to hear from you."", 'Minimum Experience, Skills and Education:', ""7+ years of experience in using SQL and databases in a business environment6+ years of experience in cloud environment, distributed systems, system automation, and real-time platforms.5+ years of experience in custom ETL design, implementation, and maintenance5+ years of experience with data warehouse schema design and data modeling2+ years of production level experience with Kafka, Python, SQL, and shell scripting3+ years experience with cloud databases (e.g., AWS, Azure, GCP)Experience with batch and stream processing (Confluent preferred)Experience with building large scale data processing systemsSolid understanding of data design patterns and best practicesWorking knowledge of data visualization tools such as Tableau, PowerBI and Looker is a plusExperience in analyzing data to identify deliverables, gaps, and inconsistencies in data setsFamiliarity with agile software development practices and drive to ship quicklyExperience in leading change, taking initiative, and driving resultsEffective communication skills and strong problem-solving skillsProven ability and desire to mentor others in a team environmentRetail production experience highly desiredBachelor's degree from four-year College or university in Computer Science, Technology or related fieldMaster's degree in Computer Science is a plusExperience with microservice patterns, API development, and containers""]",Entry level,Full-time,Information Technology,Information Technology and Services,2021-01-26 11:43:08
Data Engineer,NLB Services,"Philadelphia, PA",2 hours ago,Be among the first 25 applicants,"['', 'Qualifications', '•\xa0\xa0\xa0\xa0\xa0\xa0Strong knowledge of SQL, Python and relational databases.', '•\xa0\xa0\xa0\xa0\xa0\xa0Knowledge of Media/Advertising industry.', 'Duration : Fulltime Employment or Long term contract', '•\xa0\xa0\xa0\xa0\xa0\xa0Hands-on experience working with Big Data and building Data Analytics solutions on Cloud (AWS).', '•\xa0\xa0\xa0\xa0\xa0\xa05+ years of Hands-on experience in Big Data Analytics geared towards BI insights.', '•\xa0\xa0\xa0\xa0\xa0\xa0Responsible for building automated data pipelines to ingest data, integrate data from multiple data sources (On-Premise & Cloud) and create aggregated data sets for reporting needs.', 'Preferred Educational Level:', ' If you are available and interested more about the opening feel free to contact me at 904-425-1657/prafful.bhardwaj@nlbservices.com.', '•\xa0\xa0\xa0\xa0\xa0\xa0Collaborate with key stake holders and translate business requirements to technical requirements and implement solutions under the guidance of technical leads.', '•\xa0\xa0\xa0\xa0\xa0\xa0BS or MS degree in Mathematics, Computer Science, Statistics, or related field of study', ' ', '•\xa0\xa0\xa0\xa0\xa0\xa0Ability to Architect and Design Big Data Analytics solutions on cloud (AWS).', 'Experience:', '•\xa0\xa0\xa0\xa0\xa0\xa03+ years of experience working with Linux, DataBricks, and Azkaban or similar tools.', 'Role : AWS Data Engineer (Big Data Analytics on AWS)', '•\xa0\xa0\xa0\xa0\xa0\xa0Strong understanding of database structures, query languages (e.g. SQL), fundamentals of mathematics, distributed systems (Hadoop), data science, and statistical concepts.', 'Role Description', '•\xa0\xa0\xa0\xa0\xa0\xa03+ years of Hands-on experience working on data pipelines, automation of jobs using big data technologies (Spark, Python, Pyspark, Glue).', 'Location : Philadelphia, PA or WFH', '•\xa0\xa0\xa0\xa0\xa0\xa0Ability to automate PySpark Jobs using Lambda/Glue/EMR/Python and fine tune for performance.', '•\xa0\xa0\xa0\xa0\xa0\xa0Experience consolidating and integrating data from multiple sources (On-Premise & Cloud).', '•\xa0\xa0\xa0\xa0\xa0\xa0Dive into large, noisy, and complex real-world customer TV and Digital Ad viewing data to do pre-campaign planning and post campaigns performance measurement using Big data Platform (DataBricks, Pyspark, Glue, EMR).', '•\xa0\xa0\xa0\xa0\xa0\xa0Ability to analyze, transform and aggregate large data sets (Big Data) using BI tools (Hive QL, Pyspark, Jupyter Notebooks, AWS Athena, AWS Glue).', '\xa0', '•\xa0\xa0\xa0\xa0\xa0\xa0Knowledge of AWS services such as Glue, Athena, Lambda, EC2, IAM, CloudWatch, EMR, S3 and Big data Query engines like Hive, Presto, Spark.', '•\xa0\xa0\xa0\xa0\xa0\xa0Position that is responsible for the design, development, testing, and support of the Big Data Analytics solutions on cloud (AWS)\xa0in collaboration with cross functional teams.']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-01-26 11:43:08
Data Engineer,Jobs via eFinancialCareers,"Princeton, NJ",17 hours ago,Be among the first 25 applicants,"['', 'A BA/BS degree or higher in Computer Science, Mathematics, or relevant data technology field, or equivalent professional work experience in software development, data engineering, data science or information technology', 'Inspire and impact our business: Act as an internal consultant to influence and implement more efficient products through analysis, dashboards, web apps, and user documentation', 'Legal authorization to work full-time in the United States without requiring visa sponsorship', 'Develop your career: sharpen your technical skills and strengthen relationships through project management, partnering with stakeholders across the firm, and establishing scalable architecture', ""We'll Trust You To"", 'Does this sound like you?', 'Apply your coding skills: Automate the influx of data and build flexible solutions for data acquisition, ETL and machine learning pipelines, and human-in-the-loop data processing to drive successful product adoption', '2+ years of experience working with restful APIs and data modeling within SQL and NoSQL databases ', 'Champion improvements: identify strategic technical gaps in our ecosystem and advocate for solutions over workarounds', ' Apply your coding skills: Automate the influx of data and build flexible solutions for data acquisition, ETL and machine learning pipelines, and human-in-the-loop data processing to drive successful product adoption Inspire and impact our business: Act as an internal consultant to influence and implement more efficient products through analysis, dashboards, web apps, and user documentation Develop your career: sharpen your technical skills and strengthen relationships through project management, partnering with stakeholders across the firm, and establishing scalable architecture Champion improvements: identify strategic technical gaps in our ecosystem and advocate for solutions over workarounds Grow our business: Approach each day knowing this role is mission-critical to the success of the firm. We will rely on your expertise, and our flat structure allows for you to make real impact, real quick ', ""You'll Need To Have"", ' A BA/BS degree or higher in Computer Science, Mathematics, or relevant data technology field, or equivalent professional work experience in software development, data engineering, data science or information technology 2+ years of Python programming and scripting in a production environment 2+ years of experience working with restful APIs and data modeling within SQL and NoSQL databases  Deep understanding of large-scale, distributed systems Legal authorization to work full-time in the United States without requiring visa sponsorship ', 'Grow our business: Approach each day knowing this role is mission-critical to the success of the firm. We will rely on your expertise, and our flat structure allows for you to make real impact, real quick', 'Our Team', 'Deep understanding of large-scale, distributed systems', '2+ years of Python programming and scripting in a production environment']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-01-26 11:43:08
Senior Data Engineer - Remote,"Fetch Rewards, Inc.","Remote, OR",2 hours ago,Be among the first 25 applicants,"['', ' Data Engineer ', ' Familiarity with Unix systems, shell scripting, and Git', ' Senior', ' At least 2 years of relevant full-time work experience', ' Big data development skills (e.g., Spark, Hadoop, MPP DW)', 'Bonus Points For', ' The desire to work with other teams in the organization (e.g., Development, Business Intelligence, Data Science) to build tools and solutions that support and help manage data within the Fetch ecosystem', ' Familiarity with open source software and dependency management', ' Why Join the Fetch Family?', ' Solid SQL skills', ' Excellent  Python or JavaScript (NodeJS) programming skills', ' Excellent written and verbal communication skills', "" Excellent written and verbal communication skills  Familiarity with open source software and dependency management  ETL process, data pipeline, and/or micro-service development experience  Cloud engineering and DevOps skills (e.g., AWS, CloudFormation, Docker)  Familiarity with messaging and asynchronous technologies (e.g., SQS, Kinesis, RabbitMQ, Kafka)  Big data development skills (e.g., Spark, Hadoop, MPP DW)  Experience with visualization tools (e.g., Tableau)  Love of Dogs! . . . Or just tolerance. We're a very canine-friendly workplace"", "" Love of Dogs! . . . Or just tolerance. We're a very canine-friendly workplace"", ' Bachelor’s degree in Computer Science (or equivalent)', ' ETL process, data pipeline, and/or micro-service development experience', ' Cloud engineering and DevOps skills (e.g., AWS, CloudFormation, Docker)', ' Familiarity with messaging and asynchronous technologies (e.g., SQS, Kinesis, RabbitMQ, Kafka)', ' Python', ' Excellent  Python or JavaScript (NodeJS) programming skills  Solid SQL skills  Familiarity with Unix systems, shell scripting, and Git  Experience with relational (SQL), non-relational (NoSQL), and/or object data stores (e.g., Snowflake, MongoDB, S3, HDFS, Postgres, Redis, DynamoDB)  Interest in building and experimenting with different tools and tech, and sharing your learnings with the broader organization  The desire to work with other teams in the organization (e.g., Development, Business Intelligence, Data Science) to build tools and solutions that support and help manage data within the Fetch ecosystem  Bachelor’s degree in Computer Science (or equivalent)  At least 2 years of relevant full-time work experience', ' Interest in building and experimenting with different tools and tech, and sharing your learnings with the broader organization', 'You Possess', ' Experience with relational (SQL), non-relational (NoSQL), and/or object data stores (e.g., Snowflake, MongoDB, S3, HDFS, Postgres, Redis, DynamoDB)', 'Who We Are', ' Experience with visualization tools (e.g., Tableau)']",Associate,Full-time,Information Technology,Marketing and Advertising,2021-01-26 11:43:08
Data Engineer - Azure,OmniData,"Portland, OR",7 hours ago,Be among the first 25 applicants,"['2+ years of experience in Analytics and Data Warehousing, with Azure knowledge a plus.  Data Warehouse Automation experience ', 'Responsibilities and Duties', 'Our process is highly personalized. Some candidates complete their process in one week, while others can take several weeks or even months. Deciding to take a new job is a big decision, so regardless of how long or short the process may be for you, the most important thing is that you find your dream job.', 'Work independently toward client success, at the same time knowing your own limitations and when to call on your OmniData team for help.', 'Azure Synapse Analytics', 'Azure Data FactoryAzure Data LakeAzure Synapse AnalyticsAzure Analysis Services', 'Great communication skills tying technologies and architectures to business results will advance your position.', 'OmniData is small and growing.  You will be exposed to senior leaders and you will be given the opportunity to learn and grow quickly in your career.', 'About OmniData', 'Contribute collaboratively to team meetings using your experience base to further the cause of innovating for OmniData clients.', ""Willingness to travel (post-COVID) and work with a consultant's diligence."", 'Power BI, DAX', 'OmniData is a US-based Data and Analytics focused consulting firm leveraging the Microsoft technology stack to help organizations build their Modern Data Estates, designed to serve their digital innovation needs for many years to come.  To do this, we apply deep experience in Solution Architecture, Data, Analytics, and technology to simplify the complex.   ', 'OmniData is offering you the opportunity to work with the entire lifecycle of large Data Projects, focused on next generation data warehousing, with surface points to Analytics, Machine Learning and AI. We offer a collaborative work culture, that enables you to produce client results with a safety net from your team.  You will get to work closely with very experienced consultants who will be able to provide mentorship and career guidance.  At the same time, you will be rewarded for learning fast and executing within our teams to provide solutions for OmniData clients.', 'SQL Server Analysis Services (Tabular & Multi-Dimensional)', 'Data Warehouse Automation experience ', 'Execution of team-leading Solution Architect and Data Architect vision for client success, with Azure based data warehouse and business intelligence tools.', 'SSISSQL ServerSQL Server Analysis Services (Tabular & Multi-Dimensional)', 'Qualifications and Skills', 'Benefits and Perks', 'Must be decisive and show ability to work with clients and assist their business and technical decision-making.', 'Execution of team-leading Solution Architect and Data Architect vision for client success, with Azure based data warehouse and business intelligence tools.Contribute collaboratively to team meetings using your experience base to further the cause of innovating for OmniData clients.Instill confidence in the client, your team, and your team membersWork independently toward client success, at the same time knowing your own limitations and when to call on your OmniData team for help.', 'Salary and benefits commensurate with experience.  High growth potential for those with an entrepreneurial spirit.', 'SQL Server', 'Azure Analysis Services', ""Analytical approach to problem-solving; ability to use technology to solve business problemsAzure based Data Services Azure Data FactoryAzure Data LakeAzure Synapse AnalyticsAzure Analysis ServicesMicrosoft SQL base Data SkillsSSISSQL ServerSQL Server Analysis Services (Tabular & Multi-Dimensional)Power BI, DAXRequirements Analysis and Project Delivery methodologyYou must be humble, hungry and a fast learner.  OmniData and our clients place a high value on inventiveness.Great communication skills tying technologies and architectures to business results will advance your position.Experience and a proven track-record with designing and building solutions in Azure is required.Must be decisive and show ability to work with clients and assist their business and technical decision-making.Willingness to travel (post-COVID) and work with a consultant's diligence.OmniData is small and growing.  You will be exposed to senior leaders and you will be given the opportunity to learn and grow quickly in your career."", ""You will work on various Big Data, Data Warehouse Automation, and Data Analytics projects for our world class clients.  In addressing complex client needs, you will be integrated into appropriately sized and skilled teams. You'll be asked to analyze requirements, develop data and analytical solutions and execute as part of the project team, all while working with the latest tools, such as Azure Synapse Analytics and related Microsoft technologies. "", 'Azure based Data Services Azure Data FactoryAzure Data LakeAzure Synapse AnalyticsAzure Analysis Services', 'Azure Data Factory', 'You must be humble, hungry and a fast learner.  OmniData and our clients place a high value on inventiveness.', 'Requirements Analysis and Project Delivery methodology', 'OmniData Is An Equal Opportunity Employer And All Qualified Applicants Will Receive Consideration For Employment Without Regard To Race, Color, Religion, Sex, National Origin, Disability Status, Protected Veteran Status, Or Any Other Characteristic Protected By Law.', 'High growth potential for those with an entrepreneurial spirit.', 'What does our recruitment process look like?', 'Job Summary', 'Instill confidence in the client, your team, and your team members', 'Salary and benefits commensurate with experience.  ', 'Microsoft SQL base Data SkillsSSISSQL ServerSQL Server Analysis Services (Tabular & Multi-Dimensional)', 'SSIS', 'About You', 'You need to be a Microsoft Certified Azure Data Engineer or demonstrate experience in the same skill set.  You need to be proficient in Power BI.  You need to have solid experience working with data and analytics, a strong technical aptitude, and be a quick learner.  In return, we offer an exciting position at a young startup experiencing rapid growth, deep mentorship and the opportunity to be part of creating a consulting firm that makes a difference for our clients every day we are with them.  ', 'Analytical approach to problem-solving; ability to use technology to solve business problems', 'Experience and a proven track-record with designing and building solutions in Azure is required.', '2+ years of experience in Analytics and Data Warehousing, with Azure knowledge a plus.  ', 'Azure Data Lake']",Entry level,Full-time,Information Technology,Computer Software,2021-01-26 11:43:08
