job_title,company,location,date_posted,applicants,job_text,seniority_level,employment_type,job_function,industries,date_scraped
Data Engineer,Apple,"Austin, TX",6 days ago,108 applicants,"['', 'Description', 'Education & Experience', 'Summary', 'Key Qualifications']",Not Applicable,Full-time,Information Technology,Consumer Electronics,2021-03-18 14:34:51
Data Engineer,Tesla,"Sparks, NV",4 weeks ago,142 applicants,"['', 'A passion and curiosity for data and data-driven decision making', 'Required Skills', ' Experience with open source machine learning libraries and frameworks (Tensorflow, Keras, etc) Familiarity with continuous integration pipelines (Docker, Jenkins, Kubernetes) Drive to introduce a predictive model to a production environment  Success building and tuning image classification models ', 'Desired Skills', 'Success building and tuning image classification models', 'Drive to introduce a predictive model to a production environment ', 'Able to work under pressure while collaborating and managing competing demands with tight deadlines', 'Experience with multiple data architecture paradigms (e.g. SQL, NoSQL, Kafka, Spark)', 'Understanding of material flow and logistics process', 'Contribute to the development of our data systems by building software for retrieving, analyzing, and visualizing data', 'Experience and interest in frontend development, preferably with the Javascript React framework', 'Responsibilities', 'MS in engineering, physics, mathematics, or equivalent.', 'Familiarity with continuous integration pipelines (Docker, Jenkins, Kubernetes)', ' Understanding of material flow and logistics process Extensive experience writing software with Python Experience with multiple data architecture paradigms (e.g. SQL, NoSQL, Kafka, Spark) Experience and interest in frontend development, preferably with the Javascript React framework Knowledge of various data communication protocols (e.g. REST API, Websockets) Able to work under pressure while collaborating and managing competing demands with tight deadlines A passion and curiosity for data and data-driven decision making ', 'Analyze material flow data and extract useful statistics about failures in order to drive meaningful improvements to production control and customer experience', 'Extensive experience writing software with Python', ' MS in engineering, physics, mathematics, or equivalent. 3 - 5 years manufacturing experience.', 'Experience', 'Identify data sources where the potential value is not fully realized and invent new means with which to interact and gather insights from it', 'Knowledge of various data communication protocols (e.g. REST API, Websockets)', 'Deploy predictive models to assess part quality from time-series data, images, and more', ' Contribute to the development of our data systems by building software for retrieving, analyzing, and visualizing data Analyze material flow data and extract useful statistics about failures in order to drive meaningful improvements to production control and customer experience Identify data sources where the potential value is not fully realized and invent new means with which to interact and gather insights from it Deploy predictive models to assess part quality from time-series data, images, and more ', '3 - 5 years manufacturing experience.', 'Experience with open source machine learning libraries and frameworks (Tensorflow, Keras, etc)']",Entry level,Full-time,Information Technology,Automotive,2021-03-18 14:34:51
Data Engineer,CVS Health,"New York, NY",22 hours ago,Be among the first 25 applicants,"['', 'Leads portions of initiatives of limited scope, with guidance and direction.', 'Proficient in SQL and experience in one of the databases', 'Collaborates with data scientists to integrate algorithms and models into automated processes.', 'Design and implement scalable, configurable and self-learning marketing campaign platform', 'Business Overview', 'Uses expertise, judgment and precedents to contribute to the resolution of moderately complex problems.', 'Experience in Spark are preferred but not required', 'Job Description', 'Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards.', 'Proficient in Python, Java, Scala, or C++; Experience in shell scriptsProficient in SQL and experience in one of the databasesExperience in Spark are preferred but not requiredExperience with Hadoop and Hive is a plus', 'Required Qualifications', 'Experience with Hadoop and Hive is a plus', 'Education', 'Proficient in Python, Java, Scala, or C++; Experience in shell scripts', 'Preferred Qualifications', 'Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards.Collaborates with data scientists to integrate algorithms and models into automated processes.Design and implement scalable, configurable and self-learning marketing campaign platformUses expertise, judgment and precedents to contribute to the resolution of moderately complex problems.Leads portions of initiatives of limited scope, with guidance and direction.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,eNGINE ,"Pittsburgh, PA",1 day ago,55 applicants,"['', 'Candidates LOCAL TO PITTSBURGH ONLY! NO C2C', 'This person’s responsibilities will include designing and implementing data solutions for database-driven applications and services.', '\xa0', 'eNGINE builds Technical Teams. We are a Solutions and Placement firm shaped by decades of interaction with Technical professionals. Our inspiration is continuous learning and engagement with the markets we serve, the talent we represent, and the teams we build. Our Consulting Workforce is encouraged to enjoy career fulfillment in the form of challenging projects, schedule flexibility, and paid training/certifications. Successful outcomes start and finish with eNGINE.', 'eNGINE is hiring Data Engineer to support our Pittsburgh based client. This person must have professional experience with PL/SQL programming,\xa0developing scripts, and ETL development. Experience with Oracle technologies including and Cloud-based data warehouse technologies is a plus.']",Not Applicable,Contract,Information Technology,Staffing and Recruiting,2021-03-18 14:34:51
Data Engineer,Accenture,"Philadelphia, PA",18 hours ago,Be among the first 25 applicants,"['', 'Preferred Qualifications:', 'Create a value chain to help address the challenges of acquiring data, evaluating its value, distilling & analyzing ', 'Database design and performance optimization with multiple databases (relational data stores - RDS, Aurora; NoSQL data stores - DynamoDB; data warehouses -Teradata, Redshift, snowflake; graph databases)', 'For now, all Accenture business travel, international and domestic, is currently restricted to client-essential sales/delivery activity only.Please note: The safety and well-being of our people continues to be the top priority, and our decisions around travel are informed by government COVID-19 response directives, recommendations from leading health authorities and guidance from a number of infectious disease experts.', 'Basic Qualifications:', 'Work in an agile CI/CD environment, (Jenkins/Ansible experience a plus)', 'Business Translator (identifying business problem, initiative, analytics intervention, data science management, data science interpretation, storytelling)', 'Understand and used Object Orientated design techniques', '2 years of Experience writing REST or GraphQL APIs', 'Data & Analytics Transformation (current state assessment, strategy development, value case, roadmap, and blueprint)', 'Experience working with large data sets in distributed data environments ', 'Data architecture (Understanding logical ways of organizing and analyzing data and how this affects building databases, APIs, and UIs)', 'Experience with containerization platforms a plus, such as Docker, Kubernetes ', 'Process Automation, Machine Learning, and Artificial Intelligence practices (knowledge of how advancing digital tools and techniques are applied in enterprise data and analytics strategies and roadmaps)', 'Minimum of two years of experience in one or more of the following areas: ', 'Minimum of 3 years of combined data, analytics and strategic consulting experience', 'A Bachelor’s Degree or equivalent work experience (12 years) or an Associate’s Degree with 6 years of work experience', 'Experience using TDD and unit testing as part of normal software development using packages such as Postman, Jest, JUnit, PyUnit, Swagger, etc.', 'Lead data modeling activities to capture and model data requirements, business rules, and logical and physical models', 'Experience in data migration from on-prem to cloud. Multi-Cloud experience - AWS/Azure/Google a plus', 'Minimum of 3 years of hands-on experience on one or more of these technologies -Python, Scala, Spark, PySpark', 'Infrastructure as Code (Terraform or similar technology)']",Mid-Senior level,Full-time,Strategy/Planning,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,the LEGO Group,"Enfield, CT",3 weeks ago,149 applicants,"['', ' Experience in AGILE environment, which focuses on iterative development and testing', ' Provide the platform of data and tools for partners, including analysts across the business', ' Experience building complex data analyses by bring to bear languages including, R, Python, Java, Alteryx, etc. Experience with data visualization tools and crafting data visualization concepts. (Tableau, DOMO, Qlik, etc)', ' Drive better decisions through easier access to data', ' Improved end to end performance understanding across the organization through dashboards, scorecards and clear insights', ' Measure sales & marketing stimuli (A/B and multi-variant) test results in collaboration with other teams to build more robust campaigns. Define and uphold standards and processes for analysis & reporting, including evolution of important metric dashboards', ' Confirmed LEGO® Leadership Playground behaviors – Bravery, Curiosity, and Focus', ' Facilitate operational efficiency, simplicity, and openness through automation and implementation of the data tools and systems to advise real-time decisions. Own end-to-end data acquisition and organization across multiple sources with the objective of structuring and maintaining data integrity. Use common global tools and data available to advise business insights and recommendations', ' Familiarity with marketing and business metrics; experienced in the design, development and preparation of business presentations and reports', ' Play a key role in driving eCommerce results by leading test & learn initiatives that lead to more effective shopper targeting and sales conversion. Execute relevant data analysis for business-as-usual and specific LEGO campaigns (e.g. holidays, product releases, etc.) across a range of eCommerce partner sites or other areas and Initiatives', ' Responsible for the software creation process (from development through maintenance). Proactively investigate new opportunities to add business value and support the evolution of our digital business through a dynamic technical infrastructure in close collaboration with multi-functional partners', ' Ability to travel domestically and internationally ~5%', ' 3+ years of data science or data architecture experience with in-depth knowledge in sophisticated analytics. Experience working with enterprise reporting systems (SAP), large data sets and writing sophisticated SQL queries', ' Proven expertise in data manipulation, test design, predictive analytics, segmentation, and related areas. Ability to synthesize results, develop stories, and make recommendations to business and technical leader. Good interpersonal communication, organization, and time-management skills']",Entry level,Full-time,Other,Consumer Goods,2021-03-18 14:34:51
Data Engineer,Veritas Partners,"Cockeysville, MD",,N/A,"['', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Mindful of practices, procedures and legal guidelines that govern PII and other sensitive data', 'This role is a hybrid of software development and analytics, so either of the following background will be relevant: a Software Developer with experience in data interpretation and desire to focus on the analytics space, or a Business Analyst with experience in software development, especially focused on process automation.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience with Object-Oriented Programming Languages such as Python, JavaScript, Java, C#, etc.\xa0', 'Also Desired: ', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Gathering data requirements from stakeholders', '\xa0Desired Skills and Experience:\xa0', 'Veritas Partners has an immediate need for a full-time Data Engineer to join our team in the greater Baltimore, MD area!', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Fundamental understanding of Data Lakes, Data Catalogs, Hardware and Network Topology', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Ability to tackle complex problems with creative solutions when the path may not be clear.', 'o\xa0\xa0Azure Data Lake (Analytics and Storage), Data Warehouse / Synapse Analytics / Amazon Redshift, Data Factory, Logic Apps, Data Bricks', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience using data reporting and visualization tools (e.g. Cognos, SSRS, Qlik, Data Studio, Power BI, Tableau, etc.)', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Data Governance concepts', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Algorithmic concepts from at least one information-centric discipline (e.g. statistics, machine learning, information processing, natural language processing, etc.)', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Familiar with the modern cloud data platforms (Azure preferred but AWS is ok). Experience in some of the following is preferred:', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Evaluating data collection for accuracy', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience in database design, architecture and warehousing', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience creating ETL/ELT processes to move data between internal and external sources using APIs, ETL.\xa0', 'Data Engineer', '\xa0', 'o\xa0\xa0Other preferred technologies include: Advanced T-SQL (SQL Server), REST API concepts, PowerShell, SSIS, PowerBI', 'This is an excellent opportunity for you to join a rapidly growing organization who utilizes the latest Microsoft/Big Data Technology!']",Mid-Senior level,Full-time,Engineering,Broadcast Media,2021-03-18 14:34:51
Data Engineer,Overhaul,"Austin, TX",23 hours ago,37 applicants,"['Overhaul is a supply chain integrity solutions company that allows shippers to connect disparate sources of data into the first fully transparent situational analysis engine designed for the logistics industry. Data that is transformed into critical insights can instantly trigger corrective actions, impacting everything from temperature control to handling requirements or package-level tracking, ensuring cargo arrives at its destination safely, undamaged, and on time. We are a dynamic, innovative, and fun team who is highly committed to our customers’ experiences and our Mission and Vision. ', '3+      years of SQL experience with ability to write and tune SQL jobs for a      variety of usage patterns ', 'Strong      interpersonal skills and experience interfacing with others internally and      externally from the company ', 'Understanding      of ETL, ELT, star schema, and other data model and data warehouse      concepts, techniques, and best practices ', 'As a member of our Data Science and Analytics team, you will establish yourself as a key expert and evangelist on our data, working cross-functionally to ensure data self-service and generate insights about our business in support of key initiatives. You’ll develop infrastructure, schema, and pipelines that become a part of Business Intelligence workflows, Data Science processes, and the product itself. You will build and iterate on a modern, SaaS-based data warehouse, analytics, visualization, and catalog infrastructure stack. ', 'Administer      and optimize our SaaS-based data warehouse and BI infrastructure in support      of self-service analytics dashboards, ad hoc analytics, and reporting ', 'Minimum requirements: ', '3+      years of experience in scripting languages like Python etc ', 'Proven      work experience as a Data Engineer or similar role ', "" 3+      years of experience working with BI or data warehouse technologies in      support of insights and reporting  3+      years of SQL experience with ability to write and tune SQL jobs for a      variety of usage patterns  3+      years of relevant experience in one of the following areas: Data      engineering, database engineering, business intelligence or business      analytics  3+      years of experience in scripting languages like Python etc  Bachelor's      degree or higher in a quantitative/technical field (e.g. Computer Science,      Statistics, Engineering)  Experience      with AWS services including S3, EKS, ECR, EMR, and Kinesis  Strong      interpersonal skills and experience interfacing with others internally and      externally from the company  Understanding      of ETL, ELT, star schema, and other data model and data warehouse      concepts, techniques, and best practices  Good      communication and presentation skills with the ability to explain concepts      and conclusions around data and insights in a clear, concise, and      compelling way  Experience      working with Snowflake or other applicable SQL data warehouse technologies  Experience      with Data Lake architectures, and with combining structured and      unstructured data into unified representations  Experience      with Docker and Kubernetes  Experience      with data pipeline tools such as Airflow or Pachyderm  Experience      working with Tableau, Looker, or other modern data visualization tools  "", '• \xa0 \xa0 \xa0 \xa0  Progressive advancement opportunity and career mobility ', 'Create      a star schema data model (or better!) and architecture to ensure      performance and usefulness across the organization ', '\xa0 ', 'Good      communication and presentation skills with the ability to explain concepts      and conclusions around data and insights in a clear, concise, and      compelling way ', 'Experience      with Docker and Kubernetes ', 'Our culture ', 'Minimum requirements:', '• \xa0 \xa0 \xa0 \xa0  Top employee health and well- being benefits ', 'Diversity and Inclusivity Statement ', '• \xa0 \xa0 \xa0 \xa0  Flexible working ', 'Maintain      diagrams and documentation of data models and data flows as needed to      support understanding and troubleshooting of data infrastructure ', 'Experience      with Data Lake architectures, and with combining structured and      unstructured data into unified representations ', 'Be      customer zero, leveraging our product and providing feedback as one of the      key target personas that the product intends to provide value for ', '3+      years of experience working with BI or data warehouse technologies in      support of insights and reporting ', 'Experience      working with Snowflake or other applicable SQL data warehouse technologies ', '• \xa0 \xa0 \xa0 \xa0  Rotating company ‘Perks at work’ program ', 'Our culture', 'Overhaul has always been, and always will be, committed to diversity and inclusion. Our Overhaul Culture Code’s top listed commitment is to “Diversity and Synergy.”  All aspects of employment will be based on merit, competence, performance, and business needs. We do not discriminate on the basis of race, color, religion, marital status, age, national origin, ancestry, physical or mental disability, medical condition, pregnancy, genetic information, gender, sexual orientation, gender identity or expression, veteran status, or any other status protected under federal, state, or local law. We strongly encourage people from underrepresented groups to apply!', 'Who We Are ', 'Manage      and drive improvements for the metrics collection pipeline, data      processing, and self-service data & insight tools ', 'Experience      working with Tableau, Looker, or other modern data visualization tools ', '• \xa0 \xa0 \xa0 \xa0  Competitive starting base salary ', 'The Role ', ' Collaborate      with business groups at Overhaul to gather requirements around key      insights and reporting needs  Develop      effective schema and queries on structured data in support of insights for      both internal-facing and customer-facing use cases, and effective      repositories of unstructured data in support of same  Develop      new data and analytics capabilities for our customer-facing across a      variety of initiatives  Administer      and optimize our SaaS-based data warehouse and BI infrastructure in support      of self-service analytics dashboards, ad hoc analytics, and reporting  Create      a star schema data model (or better!) and architecture to ensure      performance and usefulness across the organization  Write      and maintain containerized (e.g. Docker) ETL code using Python, R, or      other programming language  Write      and maintain SQL jobs in support of ETL/ELT and BI analysis, reporting,      and visualization, ability to troubleshoot SQL jobs as required  Engage      and coordinate with data science, engineering, analytics, and others at      Overhaul in support of data initiatives  Maintain      diagrams and documentation of data models and data flows as needed to      support understanding and troubleshooting of data infrastructure  Build      and maintain internal data catalog including data dictionaries, glossary,      and curated datasets in support of easy consumption by the rest of the      company  Manage      and drive improvements for the metrics collection pipeline, data      processing, and self-service data & insight tools  Be      stewards and evangelists for data driven culture and data best practices      within the company  Be      customer zero, leveraging our product and providing feedback as one of the      key target personas that the product intends to provide value for  ', 'Excellent      time-management skills ', 'Experience      with AWS services including S3, EKS, ECR, EMR, and Kinesis ', '• \xa0 \xa0 \xa0 \xa0  Free parking ', ' Proven      work experience as a Data Engineer or similar role  Collaborative,      communicative, and consultative work style  Detail-oriented      with the ability to effectively manage multiple competing priorities  Ability      to succeed in a fast-paced, innovative, and rapidly evolving industry and      business organization  Excellent      time-management skills  ', 'Ability      to succeed in a fast-paced, innovative, and rapidly evolving industry and      business organization ', 'Write      and maintain containerized (e.g. Docker) ETL code using Python, R, or      other programming language ', 'What we can offer: ', 'Develop      effective schema and queries on structured data in support of insights for      both internal-facing and customer-facing use cases, and effective      repositories of unstructured data in support of same ', 'What your day to day will look like:', 'Experience      with data pipeline tools such as Airflow or Pachyderm ', 'Collaborative,      communicative, and consultative work style ', '3+      years of relevant experience in one of the following areas: Data      engineering, database engineering, business intelligence or business      analytics ', 'The Role', 'Be      stewards and evangelists for data driven culture and data best practices      within the company ', '• \xa0 \xa0 \xa0 \xa0  Unlimited paid time off ', '• \xa0 \xa0 \xa0 \xa0  Employee Assistance Program ', 'Build      and maintain internal data catalog including data dictionaries, glossary,      and curated datasets in support of easy consumption by the rest of the      company ', 'Collaborate      with business groups at Overhaul to gather requirements around key      insights and reporting needs ', ""Bachelor's      degree or higher in a quantitative/technical field (e.g. Computer Science,      Statistics, Engineering) "", 'Engage      and coordinate with data science, engineering, analytics, and others at      Overhaul in support of data initiatives ', 'Detail-oriented      with the ability to effectively manage multiple competing priorities ', 'What we can offer:', 'We are guided by our core values of Diversity and Synergy, Creativity, Problem Solving, Authenticity and Receptivity, Trust, Encouragement, Teaching and Learning, Wellness and Integrity. These values help us recruit aligned talent to join our rapidly expanding team around the globe. It is important to us that each and every Overhauler is not only eager to challenge themselves and knows how to get work done, but is also an awesome addition to our company culture. ', 'We’re looking for someone who is able to clearly communicate and collaborate with others and is passionate about working with data. ', 'About you ', 'What your day to day will look like: ', 'Develop      new data and analytics capabilities for our customer-facing across a      variety of initiatives ', '\xa0', '• \xa0 \xa0 \xa0 \xa0  Care giver/adoption/family leave ', 'The Overhaul team is looking for an experienced Data Engineer to serve in a role supporting the data insights / data science workflow and infrastructure (the queries and pipelines) for Overhaul and our customers. ', 'Write      and maintain SQL jobs in support of ETL/ELT and BI analysis, reporting,      and visualization, ability to troubleshoot SQL jobs as required ']",Mid-Senior level,Full-time,Information Technology,Logistics and Supply Chain,2021-03-18 14:34:51
Data Engineer - Segments Data Science,LinkedIn,"Sunnyvale, CA",2 days ago,Over 200 applicants,[''],Not Applicable,Full-time,Engineering,Internet,2021-03-18 14:34:51
Data Engineer (48221),Zipcar,"Boston, MA",19 hours ago,Be among the first 25 applicants,"['', 'Bachelor’s degree in computer science\u202for related field\u202f\u202f', ' Bachelor’s degree in computer science\u202for related field\u202f\u202f 3+\u202fyears’ experience\u202fwith data engineering or data warehousing, or an equivalent data-oriented software engineering background\u202f 3+ years’ experience with SQL, optimizing queries and tuning database performance\u202f 1+\u202fyears’ experience\u202fwith key AWS technologies (Redshift, S3, IAM;\u202falso\u202fKinesis, Lambda, EMR, Spark, Hive)\u202f 3+\u202fyears’ experience\u202fwith ETL technologies (e.g. Talend, Informatica,\u202fMatillion)\u202f 3+\u202fyears’ experience\u202fwith Java, Python,\u202fScala\u202fand other programming languages\u202f Experience with\u202fdbt\u202fand Airflow a strong plus\u202f Familiarity with data analysis and data science tools (Looker,\u202fJupyter, R) is a plus\u202f ', 'At Zipcar, we encourage new tools and ideas- your opinion always matters!', 'Generous paid time off, including holidays, vacation, personal, sick, volunteer and Parental Leave options', 'Who are we? ', '3+ years’ experience with SQL, optimizing queries and tuning database performance\u202f', '3+\u202fyears’ experience\u202fwith data engineering or data warehousing, or an equivalent data-oriented software engineering background\u202f', 'You’ll get to come to work every day knowing that you’re contributing to the success of the world’s leading car sharing network', 'What Tops Off The Tank', 'Develop solutions in bleeding-edge data technologies on AWS eco-system', 'Free Zipcar Membership and other employee discounts, including discounts on renting and buying Avis/Budget cars', 'Tax-free benefit for public transportation or parking expenses', 'What you’ll love about being a Zipster in Engineering:', 'We encourage Zipsters to bring their whole selves to work - unique perspectives, personal experiences, backgrounds, and however they identify. We are proud to be an equal opportunity employer – M/F/D/V.', ""You'll be joining a collaborative, diverse, passionate team of Zipsters"", 'Bicycle Reimbursement program', 'Experience with\u202fdbt\u202fand Airflow a strong plus\u202f', 'Competitive Medical, Dental, Vision, Life and Disability Insurance and other voluntary benefits through our parent company, Avis Budget Group', 'Qualifications', 'Create and maintain data models, data catalogs, and data security', 'The Extra Mile', 'Community involvement opportunities', 'Familiarity with data analysis and data science tools (Looker,\u202fJupyter, R) is a plus\u202f', ' Build mission critical data pipelines that power Zipcar’s business Develop solutions in bleeding-edge data technologies on AWS eco-system Partner with architects and data analysts to make the best technology decisions Create and maintain data models, data catalogs, and data security Mentor team members and assist them in developing, testing and deploying software ', '3+\u202fyears’ experience\u202fwith ETL technologies (e.g. Talend, Informatica,\u202fMatillion)\u202f', '401(k) Retirement Plan with company matched contributions', "" You’ll get to come to work every day knowing that you’re contributing to the success of the world’s leading car sharing network You'll be joining a collaborative, diverse, passionate team of Zipsters You’ll get to work in a flexible work environment that is innovative and resourceful At Zipcar, we encourage new tools and ideas- your opinion always matters! "", 'What You Will Do', 'You’ll get to work in a flexible work environment that is innovative and resourceful', 'Mentor team members and assist them in developing, testing and deploying software', 'Build mission critical data pipelines that power Zipcar’s business', '1+\u202fyears’ experience\u202fwith key AWS technologies (Redshift, S3, IAM;\u202falso\u202fKinesis, Lambda, EMR, Spark, Hive)\u202f', '3+\u202fyears’ experience\u202fwith Java, Python,\u202fScala\u202fand other programming languages\u202f', 'Partner with architects and data analysts to make the best technology decisions', ' Competitive Medical, Dental, Vision, Life and Disability Insurance and other voluntary benefits through our parent company, Avis Budget Group Generous paid time off, including holidays, vacation, personal, sick, volunteer and Parental Leave options Tax-free benefit for public transportation or parking expenses Bicycle Reimbursement program 401(k) Retirement Plan with company matched contributions Free Zipcar Membership and other employee discounts, including discounts on renting and buying Avis/Budget cars Community involvement opportunities ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Russell Tobin,"New York, NY",24 hours ago,26 applicants,"['', 'RESPONSIBILITIES:', 'Job Description:-\xa0', 'Strong knowledge of databases (relational and non-relational)', 'Participate in Schema design with our bioinformatics team', 'No Sponsorship / C2C ', 'Customer overview:', 'Job Location: New York, NY 10017, United States', 'QUALIFICATIONS:', 'Startup experience is a major plus.', 'As a Data Engineer, you will help design, build, and maintain the data pipelines that power Customer’s information and diagnostic business lines. ', 'Perform and report on ad-hoc data analyses', 'Strong, clear communication ability, work ethic', 'Proven track record of data engineering, backing real-world shipping software products.', 'Job Duration: Fulltime / PERM / DirectHire\xa0(Remote to start – will be onsite after Pandemic with flexibility)', 'Strong communication both written and verbal\xa0', 'Job Title: Data Engineer', 'Take ownership of what we’re building and participate in the technology stack decisions', 'Create quality metrics and monitoring tools to ensure high fidelity data', 'Optimize data structures, schemas, indices, and storage engines. Perform and report out ad-hoc data analyses, establish and maintain data security protocols. Optimize data storage cost.', 'Responsibilities will include:', 'Customer is into Biotechnology industry, who provide science-driven solutions and health intelligence company.', 'Job Duration:', 'You will work alongside software development and devops teams to instrument and monitor data stores with performance and latency. ', 'Assist in database administration. Backups, performance tuning, load balancing', 'Work within a software engineering team to deliver data products to market', 'Develop and maintain ETL processes', 'Job Location:', 'Customer overview: ', '\xa0', 'Proficiency in SQL and Git', 'Write high quality, well tested, production grade code.', 'Job Title: ']",Mid-Senior level,Full-time,Information Technology,Biotechnology,2021-03-18 14:34:51
Data Engineer,HealthVerity,"Philadelphia, PA",1 week ago,Be among the first 25 applicants,"['', ' Marvel at the speed with which your creation makes it into production', ' 3+ years of work experience 3+ years of experience with Python 3+ years of experience with PySpark and Spark-SQL (writing, testing, debugging spark routines) 1+ years of experience with AWS EMR, AWS S3 service. Comfortable using AWS CLI and boto3 Comfortable using *nix command line (shell scripting, AWK, SED) Experience with MySQL and Postgres', ' 3+ years of experience with Python', ' Standardizing on common data models across data types', ' 3+ years of experience with PySpark and Spark-SQL (writing, testing, debugging spark routines)', ' Experience with healthcare data', ' About You ', ' Empowering clients with highly rewarding data discovery and licensing tools Ingesting and managing billions of healthcare records from a wide variety of partners Standardizing on common data models across data types Orchestrating an industry-leading HIPAA privacy layer Innovating our proprietary de-identification and data science algorithms Building a culture that supports rapid iteration and new possibilities', ' Lead requirements gathering around data pipeline automation improvements', ' Experience with Apache Airflow', ' Innovating our proprietary de-identification and data science algorithms', ' Empowering clients with highly rewarding data discovery and licensing tools', ' Comfortable using *nix command line (shell scripting, AWK, SED)', ' Leverage distributed computing and serverless architecture such as AWS EMR & AWS Lambda, to develop pipelines for transforming data', ' Ingesting and managing billions of healthcare records from a wide variety of partners', ' Research and implement new technologies with a team of developers to execute strategies and implement solutions', ' Solve complex problems related to the real-time discovery of large data', ' Eager to both review peer code and have your code reviewed', ' Experience with Apache Airflow Experience with Apache Zeppelin Experience with healthcare data', ' Experience with MySQL and Postgres', ' Experience with Apache Zeppelin', ' Work with some of the most exciting open-source tools like Spark, Hadoop, Docker, Airflow, Zeppelin', ' 1+ years of experience with AWS EMR, AWS S3 service.', ' Experienced in writing scalable applications on distributed architectures Data driven, testing and measuring as much as you can Eager to both review peer code and have your code reviewed Comfortable on the command line and consider it an essential tool Confident in SQL, you know it, write smart queries, it’s no big deal', ' Required Skills And Experience ', ' Produce peer reviewed quality software', ' Experienced in writing scalable applications on distributed architectures', ' 3+ years of work experience', ' Confident in SQL, you know it, write smart queries, it’s no big deal', ' Troubleshoot and resolve issues relating to data integrity', "" Work with the team to load data into HealthVerity's data warehouse Troubleshoot and resolve issues relating to data integrity Help establish procedures and best practices for transforming and storing data Lead requirements gathering around data pipeline automation improvements Work with some of the most exciting open-source tools like Spark, Hadoop, Docker, Airflow, Zeppelin Leverage distributed computing and serverless architecture such as AWS EMR & AWS Lambda, to develop pipelines for transforming data Marvel at the speed with which your creation makes it into production Research and implement new technologies with a team of developers to execute strategies and implement solutions Produce peer reviewed quality software Solve complex problems related to the real-time discovery of large data"", ' Comfortable using AWS CLI and boto3', ' Data driven, testing and measuring as much as you can', ' Orchestrating an industry-leading HIPAA privacy layer', ' Comfortable on the command line and consider it an essential tool', ' Building a culture that supports rapid iteration and new possibilities', "" Work with the team to load data into HealthVerity's data warehouse"", ' Help establish procedures and best practices for transforming and storing data']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Riskalyze,"Philadelphia, PA",2 weeks ago,36 applicants,"['', 'Work with leaders across the company to develop a strong understanding of their needs for data sets in Salesforce, Domo and other tools for our customer-facing teams. Work with engineering teams in the R&D organization to understand the data schemas and structures.', 'Fully stocked drink fridges.', 'Annual bonus subject to company/individual performance.', 'Build monitoring and alerting tools to notify multiple people on the Revenue Operations team when our core systems or products have issues producing data, our data pipelines have issues extracting and transforming data, or our customer-facing systems have issues receiving data.', 'Coordinate the validation of data quality by requiring both the R&D stakeholders who have knowledge of data structure, and the customer-facing teams who have knowledge of customer activities and business operations, to review the outputs of the data pipeline and ensure it is accurate and complete.', 'Benefits', 'Medical, dental and vision with access to HSA or FSA depending on chosen medical plan.', '401(k) Retirement savings with employer matching dollar-for-dollar, up to 4%.', 'Responsibilities', 'Strong communication, collaboration, and presentation skills.', 'On-site financial planning with a registered financial advisor.', 'Requirements', 'Strong experience with ETL tools, databases, data warehousing solutions', 'Available pet insurance.', 'Design and build automated and scalable data pipelines to extract that data from our core databases and products, transform that data into usable datasets for our customer-facing teams, and load that data into Salesforce, Domo and other tools.', 'Elite at constructing data schemas and structuring datasets for ease of use by our customer-facing teams. Ability to distill down complex concepts and communicate them to non-technical teams.', 'Experience working independently on data pipeline and integration projects; owning and leading a function without a large team to fall back on; willingness to be entrepreneurial and scrappy in creating infrastructure from scratch. Strong systems thinker.', 'All hands team meetings every 6 weeks with catering.', ' Work with leaders across the company to develop a strong understanding of their needs for data sets in Salesforce, Domo and other tools for our customer-facing teams. Work with engineering teams in the R&D organization to understand the data schemas and structures. Design and build automated and scalable data pipelines to extract that data from our core databases and products, transform that data into usable datasets for our customer-facing teams, and load that data into Salesforce, Domo and other tools. Coordinate the validation of data quality by requiring both the R&D stakeholders who have knowledge of data structure, and the customer-facing teams who have knowledge of customer activities and business operations, to review the outputs of the data pipeline and ensure it is accurate and complete. Build monitoring and alerting tools to notify multiple people on the Revenue Operations team when our core systems or products have issues producing data, our data pipelines have issues extracting and transforming data, or our customer-facing systems have issues receiving data. Work with stakeholders to assist with data-related technical issues and support data infrastructure needs. Consult with R&D Product Managers to ensure that new products and features are developed to feed data seamlessly into the data pipeline. Mentor other RevOps team members on data pipelines and architecture to create resilience in our resourcing and systems. ', 'Consult with R&D Product Managers to ensure that new products and features are developed to feed data seamlessly into the data pipeline.', '3 weeks Vacation & 1 week of sick time per year + 11 paid holidays.', '5+ years of experience working with at least half of the following technologies: SQL, cURL, JSON, Javascript, Kotlin, Apex and/or Python. Clear ability to rapidly extend your natural skills to encompass the other half.', 'In office snacks 3x per week.', 'Mentor other RevOps team members on data pipelines and architecture to create resilience in our resourcing and systems.', ' Medical, dental and vision with access to HSA or FSA depending on chosen medical plan. Available pet insurance. 401(k) Retirement savings with employer matching dollar-for-dollar, up to 4%. Annual bonus subject to company/individual performance. On-site financial planning with a registered financial advisor. 3 weeks Vacation & 1 week of sick time per year + 11 paid holidays. All hands team meetings every 6 weeks with catering. Fully stocked drink fridges. In office snacks 3x per week. ', 'Work with stakeholders to assist with data-related technical issues and support data infrastructure needs.', ' Experience working independently on data pipeline and integration projects; owning and leading a function without a large team to fall back on; willingness to be entrepreneurial and scrappy in creating infrastructure from scratch. Strong systems thinker. Strong experience with ETL tools, databases, data warehousing solutions 5+ years of experience working with at least half of the following technologies: SQL, cURL, JSON, Javascript, Kotlin, Apex and/or Python. Clear ability to rapidly extend your natural skills to encompass the other half. Elite at constructing data schemas and structuring datasets for ease of use by our customer-facing teams. Ability to distill down complex concepts and communicate them to non-technical teams. Strong communication, collaboration, and presentation skills. ']",Entry level,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Engineer,Bravely,"New York, NY",6 days ago,Over 200 applicants,"['', 'Integrate data from our app and various third party tools we use to serve our users into our data warehouse', 'Working at Bravely ', 'You’re experienced with data warehousing and designing data models', 'You are fluent in SQL and a programming language like Python or Ruby', 'You are proficient in one or more relational databases such as PostgreSQL', 'What You’ll Do', 'Handy but not required: You have experience with Looker', 'Requirements', 'You prioritize understanding data context and execute data architecture that reflects the real world that data representsYou’re an engineer first, making you passionate for high quality code, automated testing, and other engineering best practicesYou’re experienced with data warehousing and designing data modelsYou can mitigate the limitations of imperfect data by building data systems that are flexible and adaptableYou tend to hold the big picture in mind while delivering continuous improvements in the short-term, to balance building a better system over time and supporting day-to-day needsYou’re a strong written and verbal communicator who collaborates well across teams', 'You have at least 1 year experience contributing significantly to the creation and maintenance of data models and ETL systems', 'Communicate fluidly with product and non-technical team members to understand data needs, establish technical requirements to support them, and manage the data engineering delivery process', 'You prioritize understanding data context and execute data architecture that reflects the real world that data represents', 'You’re a strong written and verbal communicator who collaborates well across teams', 'You have experience with cloud-based platforms, such as AWS and Google Cloud Platform', 'Work closely with our data analyst, head of product, and engineering team to ensure data from all user activity is retrievable and well-structured for analysis and tracking needs', 'You’re an engineer first, making you passionate for high quality code, automated testing, and other engineering best practices', 'Integrate data from our app and various third party tools we use to serve our users into our data warehouseTake ownership of our data’s performance, managing how we integrate data and optimizing our architecture for efficient analysis, and collaborating on strong security for sensitive dataWork closely with our data analyst, head of product, and engineering team to ensure data from all user activity is retrievable and well-structured for analysis and tracking needsCollaborate on the ongoing development of our ERD to ensure our architecture is sound and reflects the reality of our use casesCommunicate fluidly with product and non-technical team members to understand data needs, establish technical requirements to support them, and manage the data engineering delivery process', 'You are fluent in SQL and a programming language like Python or RubyYou are proficient in one or more relational databases such as PostgreSQLYou have at least 1 year experience contributing significantly to the creation and maintenance of data models and ETL systemsYou have experience with cloud-based platforms, such as AWS and Google Cloud PlatformHandy but not required: You have experience with event-driven systemsHandy but not required: You have experience with Looker', 'You tend to hold the big picture in mind while delivering continuous improvements in the short-term, to balance building a better system over time and supporting day-to-day needs', 'Overview', 'Handy but not required: You have experience with event-driven systems', 'About You', 'Take ownership of our data’s performance, managing how we integrate data and optimizing our architecture for efficient analysis, and collaborating on strong security for sensitive data', 'Collaborate on the ongoing development of our ERD to ensure our architecture is sound and reflects the reality of our use cases', 'You can mitigate the limitations of imperfect data by building data systems that are flexible and adaptable']",Entry level,Full-time,Quality Assurance,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,"Eclypsium, Inc.",United States,23 hours ago,Be among the first 25 applicants,"['', 'Execute release and deployment procedures', '3+ years of experience as a Developer with Python', 'Create and maintain crawlers and scraping processes', 'Regular events and celebrations', 'Competitive compensation & startup equity', 'Paid parental leave', 'MINIMUM QUALIFICATIONS', 'Life insurance, short term and long term disability coverage', '\ufeff', 'Experience with the Scrapy framework + Splash\xa0', 'BENEFITS', 'POSITION DESCRIPTION', 'Solid understanding of web technologies (HTML, JavaScript, CSS, XPath, JSON, etc)', 'Location:\xa0', 'Proactively identify and rectify any issues found in the scraping and data extraction pipelines.', '3+ years of experience as a Developer with PythonExperience with the Scrapy framework + Splash\xa0Knowledge of PKI and code signing conceptsComputer Science or equivalent education', 'EQUAL OPPORTUNITY', 'Location:\xa0US West coast, Oregon or SF Bay area preferred.', 'Strong understanding of web scraping techniques, data extraction and how to analyse crawling processes, create\xa0data pipelines', 'Strong troubleshooting skills', 'Eclypsium is headquartered in Portland, OR with distributed remote employees and global teams in Argentina and the Bay Area. We offer competitive compensation and benefits packages and are committed to the wellbeing of our employees and their families.\xa0', 'Comprehensive medical, dental, vision coverage', 'Provide quick resolution to data issues and new scraping requests generated by internal or external clients.', 'Competitive compensation & startup equityComprehensive medical, dental, vision coverageLife insurance, short term and long term disability coverageFlexible time off\xa0Employee assistance programPaid parental leaveHome office support for remote employeesRegular events and celebrations', 'ABOVE AND BEYOND', 'Familiarity with data processing tools (pandas, regex, SQL)', 'The scope of your responsibilities will include web-scraping, data extraction and data analytics (50%), product deployment management and signing (25%) and production support and feature implementation (25%).Create and maintain crawlers and scraping processesProactively identify and rectify any issues found in the scraping and data extraction pipelines.Provide quick resolution to data issues and new scraping requests generated by internal or external clients.Execute release and deployment proceduresPerform high level investigations for issues in client data', 'ABOUT ECLYPSIUM', 'Home office support for remote employees', 'Solid Linux and Git foundations', 'Eclypsium is seeking an engineer to support the data discovery, collection, update and the final packaging, assurance, and integration of our products. This role will support overall product quality but specifically ensure customer success by providing quick turn-around for data quality and product improvements in mission-critical customer environments.\xa0', 'Strong understanding of web scraping techniques, data extraction and how to analyse crawling processes, create\xa0data pipelinesSolid understanding of web technologies (HTML, JavaScript, CSS, XPath, JSON, etc)Strong troubleshooting skillsSolid Linux and Git foundationsHands-on experience with GPG, PGP, Yubikeys and AuthenticodeExperience with a document oriented databaseFamiliarity with data processing tools (pandas, regex, SQL)', 'Eclypsium delivers a cloud-based enterprise device security platform for modern distributed organizations. From corporate laptops and desktops, to servers in data centers, to network infrastructure devices, Eclypsium protects the devices that organizations rely on, all the way down to firmware. Eclypsium provides comprehensive device and firmware inventory, automatically identifies and patches firmware risks, scans devices for supply chain breaches, and continuously monitors devices for persistent and stealthy firmware attacks. Eclypsium’s cloud-based solution is deployed in minutes. Protecting Fortune 100 enterprises and federal agencies, Eclypsium was named a Gartner Cool Vendor in Security Operations and Threat Intelligence, a TAG Cyber Distinguished Vendor, one of the World’s 10 Most Innovative Security Companies by Fast Company, a CNBC Upstart 100, a CB Insights Cyber Defender, and an RSAC Innovation Sandbox finalist. For more information, visit\xa0eclypsium.com.', 'RESPONSIBILITIES', 'Knowledge of PKI and code signing concepts', 'Computer Science or equivalent education', 'Flexible time off\xa0', 'Employee assistance program', 'Experience with a document oriented database', 'Hands-on experience with GPG, PGP, Yubikeys and Authenticode', 'The scope of your responsibilities will include web-scraping, data extraction and data analytics (50%), product deployment management and signing (25%) and production support and feature implementation (25%).', 'Benefits & Perks include:', 'Eclypsium is an equal opportunity employer. We believe in the importance of diverse teams and value candidates of all backgrounds.\xa0We do not discriminate on the basis of age, ancestry, citizenship, color, ethnicity, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or invisible disability status, political affiliation, veteran status, race, religion, or sexual orientation.\xa0', 'Perform high level investigations for issues in client data']",Entry level,Full-time,Information Technology,Computer & Network Security,2021-03-18 14:34:51
Data Engineer,Huxley,"Chicago, IL",21 hours ago,80 applicants,"['Supporting the data quality program as well as operations, and technical documentations', 'Competitive compensation plus 15% bonus that is paid out bi-annually', 'The ability to work with company with an award winning positive cultureCompetitive compensation plus 15% bonus that is paid out bi-annuallyGenerous PTO and paid holidays', ' ', 'Experience with cloud technologies like GCP is a plus', 'Design and build out ETL Pipelines and toolsWorking in collaboration with other members from the data team implementing new ideasSupporting the data quality program as well as operations, and technical documentations', 'The ability to work with company with an award winning positive culture', 'Experience building out data pipelines from scratch using PythonSQL development experienceExperience with cloud technologies like GCP is a plus', ""What's in it for you?!"", 'Sthree US is acting as an Employment Agency in relation to this vacancy.', 'Experience building out data pipelines from scratch using Python', 'Generous PTO and paid holidays', 'My client is looking to grow their Data team notorious for their Award Winning Culture. They are currently building out their analytical warehouse and real-time data that overlooks their ecommerce platform. Experience building out data pipelines and ETL tools is a must, which seems to align with your background!', 'Working in collaboration with other members from the data team implementing new ideas', 'Must have Qualifications:', 'SQL development experience', 'Responsibilities:', 'Design and build out ETL Pipelines and tools']",Entry level,Full-time,Engineering,Computer Software,2021-03-18 14:34:51
Data Engineer,Aunalytics,"South Bend, IN",1 day ago,38 applicants,"['', 'Experience working in one of the following industries: healthcare, financial services, media, or manufacturing', 'Follow company policy and procedures which protect sensitive data and maintain compliance with established security standards and best practices\xa0', 'Recommend and implement enhancements that standardize and streamline processes, assure data quality and reliability, and reduce processing time to meet client expectations\xa0', 'Natural curiosity about what’s hidden in the data through exploration, attention to detail, and ability to see the big picture –\xa0similar to\xa0putting together a\xa010,000-piece\xa0puzzle.\xa0', 'Prior experience supporting business intelligence operations, managing technical, business, and process metadata related to data warehousing', 'Opportunity to work with a rapidly expanding tech company in the booming field of data science and cloud computing, alongside some of the brightest minds in the industryOpportunity to work with cutting-edge technology in a casual, fun environmentOpportunity to be a part of a local company committed to making a difference in our communityChance to work with a rapidly expanding tech companyFlexible schedule and paid time offFree snacks and an unlimited supply of coffeeSocial events such as happy hours, game nights, holiday parties, birthday celebrations, movie days, ice cream sundae bars, fancy coffee carts, company softball team, etc.Competitive salary and benefits package including health, vision, dental, and life insurance', ""What's in it for You?"", 'Opportunity to work with cutting-edge technology in a casual, fun environment', 'Social events such as happy hours, game nights, holiday parties, birthday celebrations, movie days, ice cream sundae bars, fancy coffee carts, company softball team, etc.', 'Share our values:\xa0growth, relationships, integrity, and passion\xa0\xa0', 'Verifies accuracy of table changes and data transformation processes. Test changes prior to deployment as appropriate.\xa0', 'Aunalytics provides a leading-edge cloud platform, where businesses run their core applications on secure, high performance computing infrastructure, create integrated data-marts using enterprise analytics software, and gain on-demand access to technology and data science experts to develop algorithms that drive digital transformation.', 'Essential Duties & Responsibilities:', 'Provides input into data governance initiatives\u202fto enhance current systems, ensure development of efficient application systems, influence the development of data policy, and support overall corporate and business goals\xa0', 'Competitive salary and benefits package including health, vision, dental, and life insurance', 'Ensure data integrity by developing and executing necessary processes and controls around the flow of data\xa0', 'Aunalytics', 'Opportunity to be a part of a local company committed to making a difference in our community', 'Communicate progress and completion to project team.\xa0Escalate roadblocks that may impact delivery schedule\xa0', 'Ability to learn quickly and contribute ideas that make the team, processes, and solutions better\xa0', 'Build and own “one source of truth” data sets to facilitate\xa0consistency and efficiency in extracting\xa0and analyzing data from disparate data sources\xa0', 'Position Overview', 'Resourceful in getting things done, self-starter, and productive working independently or collaboratively – ours is a fast-pace entrepreneurial environment with performance expectations and deadlines.\xa0', 'Experience working with relational database structures, SQL and/or flat files and performing table joins, web crawling, and web development.\xa0\xa0\xa0', 'Ability to defend your professional decisions and organize proof that your ideas and processes are correct\xa0', 'Free snacks and an unlimited supply of coffee', 'Experience working with distributed and/or parallel systems experience or knowledge of concepts', 'Innovate and improve efficiency of managing data to allow for greater speed and accuracy of producing analyses, metrics, and insights\xa0', 'Experience working with NoSQL, Hive, MapReduce, and other Big Data technologies is preferred but not required; willing to train the right candidate', 'Ability to communicate your ideas (verbal and written) so that\xa0team members and clients\xa0can understand them\xa0\xa0', 'Bachelor’s degree in Computer Science, Computer Engineering, Mathematics, or related field, or\xa03\xa0plus years of relevant work experience.\xa0Experience working with relational database structures, SQL and/or flat files and performing table joins, web crawling, and web development.\xa0\xa0\xa0Proficiency in one or more of the following programming languages: PHP, Java, or Python and a familiarity with Node.js\xa0Natural curiosity about what’s hidden in the data through exploration, attention to detail, and ability to see the big picture –\xa0similar to\xa0putting together a\xa010,000-piece\xa0puzzle.\xa0Resourceful in getting things done, self-starter, and productive working independently or collaboratively – ours is a fast-pace entrepreneurial environment with performance expectations and deadlines.\xa0Ability to learn quickly and contribute ideas that make the team, processes, and solutions better\xa0Ability to communicate your ideas (verbal and written) so that\xa0team members and clients\xa0can understand them\xa0\xa0Ability to defend your professional decisions and organize proof that your ideas and processes are correct\xa0Experience working in one of the following industries: healthcare, financial services, media, or manufacturingExperience working with commercial relational database systems such as electronic medical records or other clinical systems, customer relationship management software, or accounting systemsFamiliar with various data management methodologies, data exploration techniques, data quality assurance practices, and data discovery/ visualization toolsPrior experience supporting business intelligence operations, managing technical, business, and process metadata related to data warehousingExperience working with NoSQL, Hive, MapReduce, and other Big Data technologies is preferred but not required; willing to train the right candidateExperience working with distributed and/or parallel systems experience or knowledge of conceptsShare our values:\xa0growth, relationships, integrity, and passion\xa0\xa0', 'Who We Are', 'Additional duties as assigned to ensure client and company success\xa0', 'Flexible schedule and paid time off', 'Preferred Skills:', 'Build and own “one source of truth” data sets to facilitate\xa0consistency and efficiency in extracting\xa0and analyzing data from disparate data sources\xa0Ensure data integrity by developing and executing necessary processes and controls around the flow of data\xa0Innovate and improve efficiency of managing data to allow for greater speed and accuracy of producing analyses, metrics, and insights\xa0Collaborate with internal and external teams to\xa0understand business needs/issues, troubleshoot problems, conduct root cause analysis,\xa0and develop cost effective\xa0resolutions for data anomalies.\xa0Provides input into data governance initiatives\u202fto enhance current systems, ensure development of efficient application systems, influence the development of data policy, and support overall corporate and business goals\xa0Utilizes technology to analyze data from applicable systems to review data processes, identify issues, and determine actions to resolve or escalate problems that require data, system, or process improvement\xa0Verifies accuracy of table changes and data transformation processes. Test changes prior to deployment as appropriate.\xa0Recommend and implement enhancements that standardize and streamline processes, assure data quality and reliability, and reduce processing time to meet client expectations\xa0Communicate progress and completion to project team.\xa0Escalate roadblocks that may impact delivery schedule\xa0Stay\xa0up-to-date\xa0on data\xa0engineering and data science trends and developments\xa0Follow company policy and procedures which protect sensitive data and maintain compliance with established security standards and best practices\xa0Additional duties as assigned to ensure client and company success\xa0', 'Opportunity to work with a rapidly expanding tech company in the booming field of data science and cloud computing, alongside some of the brightest minds in the industry', 'Stay\xa0up-to-date\xa0on data\xa0engineering and data science trends and developments\xa0', 'Proficiency in one or more of the following programming languages: PHP, Java, or Python and a familiarity with Node.js\xa0', 'Experience working with commercial relational database systems such as electronic medical records or other clinical systems, customer relationship management software, or accounting systems', 'At the\xa0heart of our Data Solutions team are our super talented,\xa0highly-technical\xa0Data Engineers.\xa0Data Engineers are\xa0data experts who\xa0dive right into new client projects and make it their job to understand how a client’s data fits together and what that data means.\xa0\xa0Utilizing\xa0this knowledge\xa0and the industry’s newest technologies (Aunsight, Hadoop, Docker, etc.),\xa0they create data lakes (fed by real-time data streams) that become the very foundation of the work we do.\xa0\xa0Critical at all stages of the data science process,\xa0Data Engineers work cross-functionally with both external and internal teams – from business analysts\xa0to data scientists; mobile app developers to platform engineers; IT teams to high-level executives.\xa0\xa0Data Engineers also provide valuable feedback to our software team that helps to shape the development of\xa0Aunsight, our proprietary end-to-end cloud analytics\xa0platform;\xa0and the development of our proprietary mobile app,\xa0Sightglass.\xa0The best Data Engineers are patient, persistent, focused,\xa0creative,\xa0and incredibly curious.\xa0They love to learn and seek out opportunities to identify unexpected solutions or develop alternate ways to solve challenging problems.\xa0\xa0\xa0', 'Familiar with various data management methodologies, data exploration techniques, data quality assurance practices, and data discovery/ visualization tools', 'Utilizes technology to analyze data from applicable systems to review data processes, identify issues, and determine actions to resolve or escalate problems that require data, system, or process improvement\xa0', 'Chance to work with a rapidly expanding tech company', 'Collaborate with internal and external teams to\xa0understand business needs/issues, troubleshoot problems, conduct root cause analysis,\xa0and develop cost effective\xa0resolutions for data anomalies.\xa0', 'Bachelor’s degree in Computer Science, Computer Engineering, Mathematics, or related field, or\xa03\xa0plus years of relevant work experience.\xa0']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Tata Consultancy Services,"San Diego, CA",1 day ago,Be among the first 25 applicants,"['Perform system analysis – profiling data, understanding business rules, understanding data relations, and data models.', 'Participate in designing and developing ETL processes and complex data manipulation through batch processes, APIs, data streaming services, etc.', 'BS/BA and above in Computer Science, Engineering, Information Systems, and/or equivalent formal training or experience.', 'Experienced with batch-oriented, API, and/or streaming processes.', 'Complete analysis and communications skills.', 'What You Will Do', 'BS/BA and above in Computer Science, Engineering, Information Systems, and/or equivalent formal training or experience.5+ years of experience in the development of complex projects involving significant processing, manipulation, and volume.Experienced with batch-oriented, API, and/or streaming processes.Complete analysis and communications skills.Strong hands-on experience with Oracle – experience with MS SQL and/or Postgres, etc., are highly desirable.Experience with scripting languages like Windows PowerShell, Python, etc., is a plus.Background in a similar industry: Wealth Management, Broker-Dealer, Financial Services.', '5+ years of experience in the development of complex projects involving significant processing, manipulation, and volume.', 'Experience with scripting languages like Windows PowerShell, Python, etc., is a plus.', 'Analyze existing data repositories to identify and develop data mapping and business rules for data conversions and processing', 'What You Need To Have', 'Perform system analysis – profiling data, understanding business rules, understanding data relations, and data models.Create complex SQL queries to analyze data characteristics and quality and create meaningful reports of findings.Analyze existing data repositories to identify and develop data mapping and business rules for data conversions and processingParticipate in development activities by creating tables, views, functions, and stored procedures for data conversions and processing.Participate in designing and developing ETL processes and complex data manipulation through batch processes, APIs, data streaming services, etc.Communicate and coordinate effectively with business and technical personnel.Self-motivated – Estimate accurately and efficiently manage effort / competing priorities to complete tasks on schedule.', 'Self-motivated – Estimate accurately and efficiently manage effort / competing priorities to complete tasks on schedule.', 'Strong hands-on experience with Oracle – experience with MS SQL and/or Postgres, etc., are highly desirable.', 'Communicate and coordinate effectively with business and technical personnel.', 'Background in a similar industry: Wealth Management, Broker-Dealer, Financial Services.', 'Create complex SQL queries to analyze data characteristics and quality and create meaningful reports of findings.', 'Participate in development activities by creating tables, views, functions, and stored procedures for data conversions and processing.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Jr. Data Engineer,ServiceTitan,"Atlanta, GA",1 day ago,Be among the first 25 applicants,"['', 'Advanced knowledge and experience in T-SQL, complex ETL tools and operations, and SSIS', 'Develop automated scripts to validate legacy database values and identify previously unmapped fields prior to loading them into the ServiceTitan platform', 'About ServiceTitan', ' Enrichment: ongoing learning culture with access to Linkedin Learning and professional development workshops, diversity charter groups, orientation program, career pathing opportunities, mentorship programs', 'Strong analytical thinking skills', 'Map data', 'Discover ', '2-5 years of experience with SQL Server 2008/2012/2014/2016Advanced knowledge and experience in T-SQL, complex ETL tools and operations, and SSISGiven the experimental nature of this job, we will require very tight compliance when it comes to data - we need to focus on learningStrong analytical thinking skillsExpert level understanding of database and data model conceptsVertical SaaS experience is highly desirableResults and solution oriented - we want to know how we can win, not why we can’tAbility to work independently and cross functionally', 'Expert level understanding of database and data model concepts', 'Enrichment: ', 'As Our Jr. Data Engineer, You Will', 'Given the experimental nature of this job, we will require very tight compliance when it comes to data - we need to focus on learning', '2-5 years of experience with SQL Server 2008/2012/2014/2016', 'Life at ServiceTitan ', 'Develop', ' Family-Friendly Benefits: extended parental leave, pregnancy support, 20k in adoption reimbursement, Snoo Smart Sleeper, back-up childcare credits, legal benefit, discounted pet insurance', 'Health & Wellness:', 'Map data from various legacy databases into the ServiceTitan platform, subsequently developing SQL scripts that will extract the information efficiently and accuratelyDevelop automated scripts to validate legacy database values and identify previously unmapped fields prior to loading them into the ServiceTitan platformApply feedback from customers and internal stakeholders on data import quality into previously developed extraction scriptsDiscover opportunities to leverage information from legacy databases into the implementation process to avoid inquiring for additional information from customersEstablish quality working relationships with internal stakeholdersContribute material input to go/no-go/continue decisions upon test completion', ' Health & Wellness: company-paid medical/vision/dental/life insurance/disability, employer HSA contribution, free One Medical membership, care coordination support, 401(k) with company match, stipend for home office equipment/supplies, gym discounts, monthly cell phone stipend', 'Results and solution oriented - we want to know how we can win, not why we can’t', 'Vertical SaaS experience is highly desirable', 'Family-Friendly Benefits:', 'Perks & Benefits', 'Apply feedback from customers and internal stakeholders on data import quality into previously developed extraction scripts', 'Equal Opportunity Employer', 'Discover opportunities to leverage information from legacy databases into the implementation process to avoid inquiring for additional information from customers', ' Work/Life Balance: flexible work schedule, flexible PTO', 'Ability to work independently and cross functionally', ""To Be Successful In This Role, You'll Need"", 'Establish quality working relationships with internal stakeholders', 'Work/Life Balance:', 'Apply ', 'Contribute', 'Contribute material input to go/no-go/continue decisions upon test completion', 'Establish', 'Map data from various legacy databases into the ServiceTitan platform, subsequently developing SQL scripts that will extract the information efficiently and accurately']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Dashlane,"New York, NY",2 days ago,25 applicants,"['', 'Your Interview Experience', ' You have experience writing complex SQL queries and using a BI tool ', ""We're Also Looking For"", '  You have 3+ years of experience as a data engineer, business intelligence analyst, or in a highly analytical role  You have 3+ years of experience with a scripting language (preferably Python) for data processing and analysis You have experience designing SQL tables, choosing indexes, tuning queries, and optimizations ac ross different functional en vironments.   You have experience writing complex SQL queries and using a BI tool  ', 'About Dashlane', 'Build data pipelines and python-based ETL tools for acquiring, processing, and delivering data', 'About The Role', ' Build data pipelines and python-based ETL tools for acquiring, processing, and delivering data Develop data models and schemas in our data warehouse that enable performant, intuitive analysis Handle the challenges that come with managing terabytes of data Collaborate with business leaders and analysts to define key metrics and build reporting to monitor and understand company performance Develop the server applications and APIs that are used by our Data Team ', 'Requirements', 'You are a self-starter that can work in a fast-paced, distributed environment, as you will be collaborating with our Paris and Lisbon teams', ' You have 3+ years of experience as a data engineer, business intelligence analyst, or in a highly analytical role ', 'Develop the server applications and APIs that are used by our Data Team', 'At Dashlane You Will', 'Diversity, Equity, Inclusion And Belonging At Dashlane', 'Collaborate with business leaders and analysts to define key metrics and build reporting to monitor and understand company performance', 'You have experience designing SQL tables, choosing indexes, tuning queries, and optimizations ac ross different functional en vironments. ', 'You have a passion for sharing the value of data and communicating insights to a broad audience with varying levels of technical expertise', 'Develop data models and schemas in our data warehouse that enable performant, intuitive analysis', 'You have 3+ years of experience with a scripting language (preferably Python) for data processing and analysis', 'You have experience with data lakes and designing and maintaining data solutions using Spark and AWS serverless services such as Kinesis, Lambda, or SQS', 'You have experience administrating, ingesting, and monitoring data in data warehouses such as Amazon Redshift or Microsoft SQL Server', 'Handle the challenges that come with managing terabytes of data', ' You have a passion for sharing the value of data and communicating insights to a broad audience with varying levels of technical expertise You have experience with data lakes and designing and maintaining data solutions using Spark and AWS serverless services such as Kinesis, Lambda, or SQS You have experience administrating, ingesting, and monitoring data in data warehouses such as Amazon Redshift or Microsoft SQL Server You are a self-starter that can work in a fast-paced, distributed environment, as you will be collaborating with our Paris and Lisbon teams ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer (x2 Openings),H2O / Overgroup,"Atlanta, GA",1 day ago,33 applicants,"['', 'Atlassian products (JIRA, Confluence, etc.)Full Microsoft Stack (Visual Studio 2013, SQL Server 2014/2019, HTML5/JS)Python/Spark', 'Hands-on experience working with live client systems and configuring real production environments.', '1-2 years minimum of professional experience working with SQL (required)', 'Uses Cutting-edge Technology In The Workplace Including', 'Have worked with BI/Visualization tools (preferred)', '1-2 years minimum of professional experience working with SQL (required)1-2 years of experience with T-SQL (preferred)Bachelor’s Degree in Database Management, Information Technology, Computer Science, Computer Information Science, or other database and software development-focused majors (preferred)Have worked with ETL tools for data migration (preferred)Have worked with BI/Visualization tools (preferred)Strong communicator, self-driven, and ability to meet deadlinesAbility to work full-time onsite daily and work with the teamStrong debugging skills', '1-2 years of experience with T-SQL (preferred)', 'Python/Spark', 'Implement client-driven reporting and data migration needs within the Rev.io platform with exposure to user interface and database changes using SQL and ETL tools.', 'Full Microsoft Stack (Visual Studio 2013, SQL Server 2014/2019, HTML5/JS)', 'Position Description', 'Ability to work full-time onsite daily and work with the team', 'Strong debugging skills', 'Implement client-driven reporting and data migration needs within the Rev.io platform with exposure to user interface and database changes using SQL and ETL tools.Responsible for working with teammates and clients to determine innovative solutions to problems experienced when installing new customers.Involved in the database testing and will actively participate in workflows involving business intelligence and reporting.Gain exposure to large customer data sets involved in product migrations, and responsible for helping data flow into Rev.io from legacy customer systems.Hands-on experience working with live client systems and configuring real production environments.', 'Strong communicator, self-driven, and ability to meet deadlines', 'Specific Duties Will Include', 'Have worked with ETL tools for data migration (preferred)', 'Gain exposure to large customer data sets involved in product migrations, and responsible for helping data flow into Rev.io from legacy customer systems.', 'Bachelor’s Degree in Database Management, Information Technology, Computer Science, Computer Information Science, or other database and software development-focused majors (preferred)', 'Involved in the database testing and will actively participate in workflows involving business intelligence and reporting.', 'Company Overview', 'Atlassian products (JIRA, Confluence, etc.)', 'Responsible for working with teammates and clients to determine innovative solutions to problems experienced when installing new customers.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,The Walt Disney Company,"Lake Buena Vista, FL",4 weeks ago,51 applicants,"['', 'Partner with our MSI, Decision Science and Technology team members in various activities around data requirements gathering, data validation scripting and review, developing and monitoring ETL/ELT data pipelines', '5+ years’ experience designing, building and maintaining ETL/ELT data pipelines', 'Strong understanding of relational and non-relational database design', 'Preferred Education', 'Hands-on knowledge of Airflow, Kafka, and Glue', '2+ years’ experience working with data lakes, data warehouses and application databases', 'Job Summary', '5+ years’ experience with SQL', 'Implementing data services API’s', '6+ years’ experience in a Technical/Developer role', 'Participate in architectural evolution of data engineering patterns, frameworks, systems, and platforms including defining best practices, standards, principles, and policies', 'SQL, Python, Spark, Docker, Gitlab, Airflow, AWS S3 and Relational Databases (PostgreSQL, MariaDB, Snowflake, Teradata)', 'Responsibilities', 'Proficiency with relational database technologies such as PostgreSQL, MariaDB, or Teradata', 'Evolving our data analytics platform', 'Experience in stream data processing and real time analytics of data generated from user interaction with applications is a plus.', '2+ years’ experience with cloud based technologies, preferably AWS EMR, EC2, and S3', '6+ years’ experience in a Technical/Developer role5+ years’ experience with Python or Java5+ years’ experience with SQL5+ years’ experience designing, building and maintaining ETL/ELT data pipelinesKnowledge utilizing PySpark and/or ScalaExperience utilizing Hadoop, Hive, Spark, and Presto2+ years’ experience with cloud based technologies, preferably AWS EMR, EC2, and S3Strong understanding of relational and non-relational database designExperience leveraging containerization technologies such as Docker, Nomad or KubernetesUnderstanding differences between: data warehouses and data lakes, schema-on-read and schema-on-write, relational and non-relational databases, batch and stream processingExperience managing and deploying code using GitlabHands-on knowledge of Airflow, Kafka, and GluePrior experience with NoSQL systems such as MongoDB, DynamoDB, Neo4J or RedisProficiency with relational database technologies such as PostgreSQL, MariaDB, or TeradataExperience working with large datasets and big data technologies, preferably cloud-based, such as Snowflake, Redshift, Databricks, or similarExperience in stream data processing and real time analytics of data generated from user interaction with applications is a plus.SQL, Python, Spark, Docker, Gitlab, Airflow, AWS S3 and Relational Databases (PostgreSQL, MariaDB, Snowflake, Teradata)2+ years’ experience working on a cloud platform2+ years’ experience working with data lakes, data warehouses and application databases', 'Knowledge utilizing PySpark and/or Scala', 'Understanding differences between: data warehouses and data lakes, schema-on-read and schema-on-write, relational and non-relational databases, batch and stream processing', '2+ years’ experience working on a cloud platform', 'Prior experience with NoSQL systems such as MongoDB, DynamoDB, Neo4J or Redis', '10+ years’ experience in a Technical/Developer roleExperience in Java, PySpark, SparkSQL, Hadoop/Hive, Glue, Kafka, Lambda, BigQuery, Databricks', 'Bachelor degree in Computer Science, Mathematics, Engineering, or a related field', '10+ years’ experience in a Technical/Developer role', 'Basic Qualifications', 'Master’s degree in Computer Science, Mathematics, Engineering, or a related field', 'Experience utilizing Hadoop, Hive, Spark, and Presto', 'Experience in Java, PySpark, SparkSQL, Hadoop/Hive, Glue, Kafka, Lambda, BigQuery, Databricks', 'Experience managing and deploying code using Gitlab', 'Leverage a multitude of technologies to fulfill the work including, but not limited to SQL, Python, Spark, PySpark, SparkSQL, Hadoop/Hive, Docker, Gitlab, Airflow, Kafka, Lambda, Snowflake, and PostgreSQL.', 'Partner with our MSI, Decision Science and Technology team members in various activities around data requirements gathering, data validation scripting and review, developing and monitoring ETL/ELT data pipelinesDesigning and implementing database schema/tables/viewsImplementing data services API’sEvolving our data analytics platformParticipate in architectural evolution of data engineering patterns, frameworks, systems, and platforms including defining best practices, standards, principles, and policiesLeverage a multitude of technologies to fulfill the work including, but not limited to SQL, Python, Spark, PySpark, SparkSQL, Hadoop/Hive, Docker, Gitlab, Airflow, Kafka, Lambda, Snowflake, and PostgreSQL.Participate in driving best practices around data engineering software development processes', 'Designing and implementing database schema/tables/views', 'Participate in driving best practices around data engineering software development processes', 'Experience leveraging containerization technologies such as Docker, Nomad or Kubernetes', 'Experience working with large datasets and big data technologies, preferably cloud-based, such as Snowflake, Redshift, Databricks, or similar', '5+ years’ experience with Python or Java', 'Preferred Qualifications']",Mid-Senior level,Full-time,Quality Assurance,Marketing and Advertising,2021-03-18 14:34:51
Data Engineer,Hulu,"Santa Monica, CA",6 days ago,113 applicants,"['', 'Create runbooks and actionable alerts as part of the development process', 'Partner with technical and non-technical colleagues to understand data and reporting requirements.Work with Engineering teams to collect required data from internal and external systems.Design table structures and define ETL strategy to build performant Data solutions that are reliable and scalable in a fast growing data ecosystem.Develop Data Quality checks for source and target data sets. Develop UAT plans and conduct QA.Develop and maintain ETL routines using ETL and orchestration tools such as Airflow, Luigi and Jenkins.Document and publish Metadata and table designs to facilitate data adoption.Perform ad hoc analysis as necessary.Perform SQL and ETL tuning as necessary.Develop and maintain Dashboards/reports using Tableau and LookerCoordinate and resolve escalated production support incidents in Tier 2 support rotationCreate runbooks and actionable alerts as part of the development process', 'Familiarity with data exploration / data visualization tools like Tableau, Looker, Chartio, etc.', 'Comfortable working in a fast-paced and highly collaborative environment.', 'Degree in an analytical field such as economics, mathematics, or computer science is desired.', '1+ years of experience with workflow management tools (Airflow, Oozie, Azkaban, UC4)', 'Coordinate and resolve escalated production support incidents in Tier 2 support rotation', '2+ years of relevant Professional experience.1+ years work experience implementing and reporting on business key performance indicators in data warehousing environments. Strong understanding of data modeling principles including Dimensional modeling, data normalization principles etc.1 + years experience using analytic SQL, working with traditional relational databases and/or distributed systems such as Hadoop / Hive, BigQuery, Redshift.Experience programming languages (e.g. Python, R, bash) preferred.1+ years of experience with workflow management tools (Airflow, Oozie, Azkaban, UC4)Good understanding of SQL Engines and able to conduct advanced performance tuningExperience with Hadoop (or similar) Ecosystem (MapReduce, Yarn, HDFS, Hive, Spark, Presto, Pig, HBase)Familiarity with data exploration / data visualization tools like Tableau, Looker, Chartio, etc.Ability to think strategically, analyze and interpret market and consumer information.Strong communication skills – written and verbal presentations.Excellent conceptual and analytical reasoning competencies.Degree in an analytical field such as economics, mathematics, or computer science is desired.Comfortable working in a fast-paced and highly collaborative environment.Process-oriented with phenomenal documentation skills', 'Document and publish Metadata and table designs to facilitate data adoption.', 'Perform SQL and ETL tuning as necessary.', 'What To Bring', 'Perform ad hoc analysis as necessary.', 'Develop and maintain ETL routines using ETL and orchestration tools such as Airflow, Luigi and Jenkins.', '2+ years of relevant Professional experience.', 'Work with Engineering teams to collect required data from internal and external systems.', '1+ years work experience implementing and reporting on business key performance indicators in data warehousing environments. Strong understanding of data modeling principles including Dimensional modeling, data normalization principles etc.', 'Ability to think strategically, analyze and interpret market and consumer information.', 'Develop and maintain Dashboards/reports using Tableau and Looker', 'Summary', 'Strong communication skills – written and verbal presentations.', 'Excellent conceptual and analytical reasoning competencies.', 'Design table structures and define ETL strategy to build performant Data solutions that are reliable and scalable in a fast growing data ecosystem.', 'Partner with technical and non-technical colleagues to understand data and reporting requirements.', ""What You'll Do"", 'Experience with Hadoop (or similar) Ecosystem (MapReduce, Yarn, HDFS, Hive, Spark, Presto, Pig, HBase)', 'Experience programming languages (e.g. Python, R, bash) preferred.', '1 + years experience using analytic SQL, working with traditional relational databases and/or distributed systems such as Hadoop / Hive, BigQuery, Redshift.', 'Process-oriented with phenomenal documentation skills', 'Develop Data Quality checks for source and target data sets. Develop UAT plans and conduct QA.', 'Good understanding of SQL Engines and able to conduct advanced performance tuning']",Not Applicable,Full-time,Information Technology,Internet,2021-03-18 14:34:51
Data Engineer,Beghou Consulting,New York City Metropolitan Area,1 day ago,29 applicants,"['', 'Experience configuring AzureAD/SAML/Okta/Oauth and administering AWS or Azure security best practices', 'We are seeking candidates in Boston, Chicago, New York, or San Francisco.', 'We’ll trust you to:', '3+ years’ experience in data engineering or application development using Python, including use of pandas or PySpark', 'We’d love to see:', '3+ years’ experience in data engineering or application development using Python, including use of pandas or PySparkExperience with relational database technologies, such as PostgreSQL, Oracle, MySQL, Redshift, SnowflakeKnowledge and experience configuring AWS or Azure cloud infrastructure and managed servicesSoftware development fundamentals, including participating in Agile development, use of version control systems such as Git or DevOps, code reviews, emphasis on testing, and dedication to documentation', 'Emphasize and ensure reliability and stability of our enterprise data platform and tools used by internal teams and life sciences clientsBuild and enhance data integration, management, and analytics tools and pipelines using pandas and SparkDevelop best practice guidance and supporting materials for data management and advanced analytics pipelines', 'Emphasize and ensure reliability and stability of our enterprise data platform and tools used by internal teams and life sciences clients', 'We treat our employees with respect and appreciation, not only for what you do but who you are.We value the many talents and abilities of our employees and promote a supportive, collaborative and dynamic work environment that encourages both professional and personal growth.You will have the opportunity to work with and learn from senior staff and partners, allowing everyone to work together to develop, achieve, and succeed with every project.We have had steady growth throughout our history because the people we hire are committed not only to delivering quality results for our clients, but also to becoming leaders in sales and marketing analytics.', 'Knowledge and experience configuring AWS or Azure cloud infrastructure and managed services', ' We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.', 'Experience configuring AzureAD/SAML/Okta/Oauth and administering AWS or Azure security best practicesWeb application development experience using Flask, Django, JavaScript, Ajax, or CSS/HTMLContainer orchestration systems experience using Docker, Kubernetes, AWS ECS)Experience with WYSIWYG ETL tools (Azure Data Factory, Informatica, SnapLogic, Boomi)Experience with building systems in event-driven or streaming architecturesLife Sciences industry experience', 'Experience with building systems in event-driven or streaming architectures', 'We value the many talents and abilities of our employees and promote a supportive, collaborative and dynamic work environment that encourages both professional and personal growth.', 'All applicants must be currently authorized to work in the United States on a full-time basis.\xa0Beghou\xa0Consulting will not sponsor applicants for work visas.', 'Build and enhance data integration, management, and analytics tools and pipelines using pandas and Spark', 'Software development fundamentals, including participating in Agile development, use of version control systems such as Git or DevOps, code reviews, emphasis on testing, and dedication to documentation', 'Experience with relational database technologies, such as PostgreSQL, Oracle, MySQL, Redshift, Snowflake', 'What you should know: ', 'Develop best practice guidance and supporting materials for data management and advanced analytics pipelines', 'All applicants must be currently authorized to work in the United States on a full-time basis.\xa0Beghou\xa0Consulting will not sponsor applicants for work visas. We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.', 'Container orchestration systems experience using Docker, Kubernetes, AWS ECS)', 'You’ll need to have:', 'We treat our employees with respect and appreciation, not only for what you do but who you are.', 'We have had steady growth throughout our history because the people we hire are committed not only to delivering quality results for our clients, but also to becoming leaders in sales and marketing analytics.', 'As a Data Engineer, your goal is to develop and enhance an in-house enterprise data platform and tools used to ingest, transform, and analyze data to yield value for our clients. You will build data integrations, connectors, and analytics applications using tools like Python and Spark and support deployment to client cloud environments. You will partner with our internal consulting teams to develop data integration and analytics pipelines and will help deliver uncovered insights to our clients. You will collaborate with our data platform and technology team to craft and evolve the tools and proprietary systems we offer.\xa0', 'Web application development experience using Flask, Django, JavaScript, Ajax, or CSS/HTML', 'You will have the opportunity to work with and learn from senior staff and partners, allowing everyone to work together to develop, achieve, and succeed with every project.', 'As a result of our tremendous growth, Beghou Consulting is seeking experienced individuals to join our team of skilled data practitioners. We seek creative, adaptable, and analytical candidates who are committed to delivering innovative solutions to life sciences companies that exceed expectations. ', '\xa0', 'Experience with WYSIWYG ETL tools (Azure Data Factory, Informatica, SnapLogic, Boomi)', 'Life Sciences industry experience']",Associate,Full-time,Information Technology,Management Consulting,2021-03-18 14:34:51
Data Engineer,Capital One,"Wilmington, DE",4 weeks ago,Be among the first 25 applicants,"['', '1+ years of experience with Ansible / Terraform', ' slides 76-91', '3+ years of experience in application development', '2+ years of experience with UNIX/Linux including basic commands and shell scripting', '2+ years of experience developing Java based software solutions ', '1+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)', '1+ year experience working on streaming data applications (Spark Streaming, Kafka, Kinesis, and Flink', 'Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Snowflake', 'Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community', 'What You’ll Do', 'Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologiesWork with a team of developers with deep experience in machine learning, distributed microservices, and full stack systemsUtilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as SnowflakeShare your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering communityCollaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowermentPerform unit tests and conducting reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance', ""Master's Degree 3+ years of experience in application development1+ year experience working on streaming data applications (Spark Streaming, Kafka, Kinesis, and Flink1+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)1+ years of experience with Ansible / Terraform2+ years of experience with Agile engineering practices 2+ years in-depth experience with the Hadoop stack (MapReduce, Pig, Hive, Hbase) 2+ years of experience with NoSQL implementation (Mongo, Cassandra) 2+ years of experience developing Java based software solutions 2+ years of experience in at least one scripting language (Python, Perl, JavaScript, Shell) 2+ years of experience developing software solutions to solve complex business problems2+ years of experience with UNIX/Linux including basic commands and shell scripting"", 'Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems', 'At least 1 years of experience in big data technologies (Cassandra, Accumulo, HBase, Spark, Hadoop, HDFS, AVRO, MongoDB, or Zookeeper)', 'Capital One Data Engineer', '2+ years of experience with Agile engineering practices ', '#lifeatcapitalone', 'Bachelor’s Degree At least 2 years of experience in application developmentAt least 1 years of experience in big data technologies (Cassandra, Accumulo, HBase, Spark, Hadoop, HDFS, AVRO, MongoDB, or Zookeeper)', 'inclusive,', '2+ years in-depth experience with the Hadoop stack (MapReduce, Pig, Hive, Hbase) ', '2+ years of experience in at least one scripting language (Python, Perl, JavaScript, Shell) ', 'Bachelor’s Degree ', 'diversity & inclusion', 'Perform unit tests and conducting reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance', 'Basic Qualifications', ""Master's Degree "", 'At least 2 years of experience in application development', '2+ years of experience with NoSQL implementation (Mongo, Cassandra) ', 'Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies', '2+ years of experience developing software solutions to solve complex business problems', 'Preferred Qualifications', 'Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment']",Associate,Full-time,Information Technology,Banking,2021-03-18 14:34:51
Data Engineer,Dice,"Louisville, KY",18 hours ago,Be among the first 25 applicants,[''],Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Visual Concepts,"Agoura Hills, CA",20 hours ago,44 applicants,"['', 'Work closely with development teams to provide insights into game quality, difficulty, and fun.', 'Create statistical models that can predict and analyze player behavior, optimize engagement, and inform post-launch feature creation.', 'Minimum of 2 years of experience in data mining & analytics, building models with very large, complex and multi-dimensional data sets.', 'Using created statistical models and analyses, interpret the underlying story to answer business and game design questions.', 'Video games industry experience and/or gaming familiarity.', 'Expert in relational databases and SQL data carpentry.', 'Develop code to facilitate statistical queries and implement hooks to improve data gathering techniques.', 'WWE knowledge and familiarity a plus.', 'Collaborate with other departments to determine project needs and analytics requirements.', 'Requirements', 'Experience with varied data modeling approaches.', 'Establish and distribute regular performance reports based on telemetry and advanced models.', ' Video games industry experience and/or gaming familiarity. WWE knowledge and familiarity a plus. Experience with JIRA, DeltaDNA, Perforce, and Confluence. Experience with predictive modeling techniques and accomplished in use of R or Python machine learning and related modules.', 'Experience with predictive modeling techniques and accomplished in use of R or Python machine learning and related modules.', 'What You Will Do', 'Experience with JIRA, DeltaDNA, Perforce, and Confluence.', 'Minimum of 1 year of basic programming experience with C++, and able to demonstrate C++ coding proficiency.', ' Create statistical models that can predict and analyze player behavior, optimize engagement, and inform post-launch feature creation. Using created statistical models and analyses, interpret the underlying story to answer business and game design questions. Develop code to facilitate statistical queries and implement hooks to improve data gathering techniques. Work closely with development teams to provide insights into game quality, difficulty, and fun. Establish and distribute regular performance reports based on telemetry and advanced models. Collaborate with other departments to determine project needs and analytics requirements. Maintain data integrity and resolve issues found with incorrect or incomplete datasets. ', ' Minimum of 2 years of experience in data mining & analytics, building models with very large, complex and multi-dimensional data sets. Minimum of 1 year of basic programming experience with C++, and able to demonstrate C++ coding proficiency. Expert in relational databases and SQL data carpentry. Experience with varied data modeling approaches. Expert communication, facilitation, and collaboration skills to effectively present, explain, influence, and advise within cross-functional teams. ', 'Expert communication, facilitation, and collaboration skills to effectively present, explain, influence, and advise within cross-functional teams.', 'Preferred Qualifications', 'Maintain data integrity and resolve issues found with incorrect or incomplete datasets.']",Entry level,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Engineer,LumenData,Washington DC-Baltimore Area,2 days ago,53 applicants,"['', 'Demonstrate exceptional written communication skills with strong documentation skills focusing on accuracy, consistency, and standardized terminology.', 'Additional Qualifications:', 'Experience with data management concepts', 'Experience with working in Agile environments a plus', 'Experience working with data science and BI teams', 'Experience working with data management concepts and activities including an understanding of industry best practices for Data Sourcing, Key Data Elements, Data Quality Management, Metadata, and Enterprise Data Management Policy, Process, and Procedures', 'Familiarity with enterprise cloud architectures like AWS, Azure, Open Stack, etc.', 'Experience with hybrid data architectures consisting of on prep and cloud based COT applications', 'Ability to learn new technologies quickly', 'Leverage expertise in structured and unstructured data to perform data engineering activities to enable data intensive solutions. Architect data systems stand up data platforms, build-out ETL pipelines, write custom codes, interface with data stores, perform data ingestion, and enable automation. Assess, design, build, and maintain scalable data platform components. Perform analytical exploration and examination of data from multiple sources. Work with multi-disciplinary teams of analysts, data engineers, scientist, developers, and data consumers to deliver data solutions at scale.', 'Experience with\xa0big data technologies (e.g. Sqoop, Flume, NiFi, Kinesis, Kafka, Elasticsearch, DeltaLake, Hive, Pig, ERM, etc.)', 'Experience with working in Agile environments a plusExperience with data management conceptsExperience with hybrid data architectures consisting of on prep and cloud based COT applicationsExperience working with data science and BI teamsExperience developing solutions involving unstructured dataExperience with the Medical Product / Healthcare domainsExperience with informatica IDQ, EDC, MDM, or Axon a plusExperienced with Mulesoft or other API platform a plus', 'Experience developing software code with object-oriented programing languages, including Java, C++, or C# Java and scripting languages, including Python, R, Bash, Batch, and PowerShell', 'Experience as a data engineer, software developer, database developer or ETL developer', 'Experienced with Mulesoft or other API platform a plus', 'Experience carrying out or supporting data migrations and integration (i.e. ETL and API development)', 'Ability to work within standardized and non-standardized processes to accomplish assigned tasks.', 'Experience with informatica IDQ, EDC, MDM, or Axon a plus', 'Experience with data engineering tools and techniques in support of developing data pipelines, including testing and automation', 'Ability to effectively communicate and collaborate with internal teams', '** Please send your resume to Neha.singh@LumenData.com and\xa0subhrojit.paul@LumenData.com. Citizens/Permanent Residents only please. Location: Washington DC/Virginia/Maryland **', 'Experience developing solutions involving unstructured data', 'Basic Qualifications:', 'Experience as a data engineer, software developer, database developer or ETL developerExperience with data engineering tools and techniques in support of developing data pipelines, including testing and automationExperience developing software code with object-oriented programing languages, including Java, C++, or C# Java and scripting languages, including Python, R, Bash, Batch, and PowerShellAbility to learn new technologies quicklyExperience with\xa0big data technologies (e.g. Sqoop, Flume, NiFi, Kinesis, Kafka, Elasticsearch, DeltaLake, Hive, Pig, ERM, etc.)Familiarity with enterprise cloud architectures like AWS, Azure, Open Stack, etc.Experience carrying out or supporting data migrations and integration (i.e. ETL and API development)Experience working with data management concepts and activities including an understanding of industry best practices for Data Sourcing, Key Data Elements, Data Quality Management, Metadata, and Enterprise Data Management Policy, Process, and ProceduresDemonstrate exceptional written communication skills with strong documentation skills focusing on accuracy, consistency, and standardized terminology.Ability to work within standardized and non-standardized processes to accomplish assigned tasks.Ability to effectively communicate and collaborate with internal teams', 'Experience with the Medical Product / Healthcare domains']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,VanderHouwen,"Austin, TX",2 days ago,Be among the first 25 applicants,"['', 'Description', 'Data Engineer', 'About VanderHouwen', 'Data Engineer Responsibilities', 'Must Have Proficient Knowledge In', 'Data Engineer Qualifications', 'Nice To Have']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Junior Data Engineer ,Brooksource,"Charlotte, NC",,N/A,"['', 'Communicate with business and other technology teams onshore and offshore', 'Present data stories to business leaders', '1+ years utilizing Python to develop scripts', 'Build frameworks and reusable components for data acquisition and integration going across structured and unstructured data sources', 'POSITION SUMMARY:', 'DESIRED SKILLS:', 'Knowledge of Spark, Hadoop, Kafka, and Streaming Components', 'B.S. or M.S. in Computer Science, Data Science or similar field or equivalent experience and work record', 'Create data standards and patterns for others to use across the company', 'Experience building data integration and ETL components using DataStage', 'Cloud computing knowledge and/or experience', 'Strong problem solving and troubleshooting skills with the ability to exercise mature judgment', '1+ years building Tableau dashboards and visuals to create data stories', 'B.S. or M.S. in Computer Science, Data Science or similar field or equivalent experience and work record1+ years working within a SQL database1+ years of Application Development experience with Java or a similar Object-Oriented Programming Language1+ years utilizing Python to develop scripts1+ years of data mining or data comparison experienceStrong problem solving and troubleshooting skills with the ability to exercise mature judgmentAbility to manage multiple projects without continuous directionAbility to function effectively and proficiently in an environment of ongoing changeProactive self-starter with the ability to work well under pressure, prioritize and multitask.Experience working directly with technology, business stakeholders/leadership in formulating technical requirements', 'Director level exposure and mentorshipClose-knit junior level team', 'REQUIRED SKILLS:', 'Use data to find new efficiencies and decrease data duplication across the company', 'Director level exposure and mentorship', 'Assist with troubleshooting issues and providing end to end technical solutions quickly and accurately', 'Brooksource is seeking an Junior Data Engineer within the Chief Data Office to join our Fortune 100 financial services client in the Charlotte, NC area. As the digital transformation continues to accelerate, this team is responsible for creating a digital footprint for the enterprise . This team is responsible for figuring out how much data is in the enterprise, who is using it, where it is located, etc. An ideal candidate is highly organized with good communication skills who can manage multiple tasks simultaneously and to be able to respond to and manage unplanned priorities in a fast-paced environment.', 'KEY RESPONSIBILITIES AND DUTIES:', 'Ability to manage multiple projects without continuous direction', '1+ years working within a SQL database', 'Experience working directly with technology, business stakeholders/leadership in formulating technical requirements', 'VALUE ADD TO YOU:', 'Build frameworks and reusable components for data acquisition and integration going across structured and unstructured data sourcesUse data to find new efficiencies and decrease data duplication across the companyScan assets, pulling payment information, and inputting into corresponding applicationsCreate data standards and patterns for others to use across the companyCommunicate with business and other technology teams onshore and offshorePresent data stories to business leadersMonitor and analyze existing processes to identify enhancement opportunitiesAssist with troubleshooting issues and providing end to end technical solutions quickly and accuratelyHandle complex operational tasks and recommends processing and tech changes with minimal supervisionMaintain knowledge of emerging technologies', 'Application integration experience using API/Mulesoft', 'Handle complex operational tasks and recommends processing and tech changes with minimal supervision', 'Close-knit junior level team', 'Monitor and analyze existing processes to identify enhancement opportunities', 'Knowledge in designing and building SQL apps on Oracle, Teradata, HBase and Hive/Impala', 'Scan assets, pulling payment information, and inputting into corresponding applications', '\xa0', '1+ years of Application Development experience with Java or a similar Object-Oriented Programming Language', 'Proactive self-starter with the ability to work well under pressure, prioritize and multitask.', 'Brooksource provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, national origin, age, sex, citizenship, disability, genetic information, gender, sexual orientation, gender identity, marital status, amnesty or status as a covered veteran in accordance with applicable federal, state, and local laws.', 'Maintain knowledge of emerging technologies', 'Ability to function effectively and proficiently in an environment of ongoing change', 'Experience in the financial services industry, particularly within a data environmentKnowledge in designing and building SQL apps on Oracle, Teradata, HBase and Hive/ImpalaKnowledge of Spark, Hadoop, Kafka, and Streaming ComponentsExperience building data integration and ETL components using DataStageApplication integration experience using API/MulesoftCloud computing knowledge and/or experience1+ years building Tableau dashboards and visuals to create data stories', '1+ years of data mining or data comparison experience', 'Experience in the financial services industry, particularly within a data environment']",Entry level,Contract,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,American Express,"Phoenix, AZ",3 days ago,195 applicants,"['', 'Promote inner sourcing.', 'Partner with stakeholders to continuously upgrade Enterprise platforms as per industry standards and Technology stack latest offerings.', 'Experience in Java, Python, Scala languages.', 'Develop and improve our CI/CD workflow tools and processes', 'Perform hands on coding and configuration to deliver solutions that impact multiple platforms and products.', 'Good knowledge of Algorithms and Data structures, Software Design patterns and enterprise integration patterns.', 'Ability to implement scalable, high performing, secure, highly available solutions.', 'Minimum Qualifications', ' BS degree or higher in computer science or related discipline. 4+ years of demonstrated experience in Agile development, application design, software development, and testing. Good knowledge of Algorithms and Data structures, Software Design patterns and enterprise integration patterns. Strong Hands-on Big data technologies like Apache Spark Data frame and Datasets, Hive Query Language, HDFS. Experience in Java, Python, Scala languages. Good understanding of CI/CD processes leveraging Jenkins, SBT, XLR and Maven. Knowledge/ experience in Elastic Cache, RDBMS and NoSQL databases. Hands-on experience on implementing RESTful APIs using Java and Python frameworks is a plus. Experience in building ETL pipelines using tools like NIFI, Informatica, Ab Initio. Ability to implement scalable, high performing, secure, highly available solutions. Self-Starter, Problem solver, Highly collaborative and adaptive. Excellent communication skills, enthusiasm and ability ask questions, understand business value. Any knowledge of Salesforce ecosystem is a plus. ', ""Seek to understand your customer's needs and problems"", 'Experience in building ETL pipelines using tools like NIFI, Informatica, Ab Initio.', 'Debug complex issues spanning multiple systems.', 'Evangelize industry best practices.', 'Knowledge/ experience in Elastic Cache, RDBMS and NoSQL databases.', 'Excellent communication skills, enthusiasm and ability ask questions, understand business value.', 'Learn new technologies and drive opportunities for adoption.', 'Manage risks and issues as well as cross dependencies with other teams. Communicate effectively with internal teams and client to address technical design and functional gaps.', 'Harness the opportunities to build Intellectual Property within the team and for the enterprise.', "" Build ETL pipelines to analyze, combine and organize raw data from different sources. Sometimes be involved in conducting complex data analysis and transform the result into visual report using data visualization tools. Prepare data pipelines to create features for prescriptive and predictive modeling. Collaborate with data scientists and architects to build end to end AI powered products. Perform hands on coding and configuration to deliver solutions that impact multiple platforms and products. Debug complex issues spanning multiple systems. Harness the opportunities to build Intellectual Property within the team and for the enterprise. Develop and improve our CI/CD workflow tools and processes Promote inner sourcing. Evangelize industry best practices. Seek to understand your customer's needs and problems Learn new technologies and drive opportunities for adoption. Manage risks and issues as well as cross dependencies with other teams. Communicate effectively with internal teams and client to address technical design and functional gaps. Partner with stakeholders to continuously upgrade Enterprise platforms as per industry standards and Technology stack latest offerings. "", '4+ years of demonstrated experience in Agile development, application design, software development, and testing.', 'Hands-on experience on implementing RESTful APIs using Java and Python frameworks is a plus.', 'Strong Hands-on Big data technologies like Apache Spark Data frame and Datasets, Hive Query Language, HDFS.', 'BS degree or higher in computer science or related discipline.', 'Collaborate with data scientists and architects to build end to end AI powered products.', 'Self-Starter, Problem solver, Highly collaborative and adaptive.', 'Prepare data pipelines to create features for prescriptive and predictive modeling.', 'Sometimes be involved in conducting complex data analysis and transform the result into visual report using data visualization tools.', 'Build ETL pipelines to analyze, combine and organize raw data from different sources.', 'Any knowledge of Salesforce ecosystem is a plus.', 'Good understanding of CI/CD processes leveraging Jenkins, SBT, XLR and Maven.']",Associate,Full-time,Information Technology,Financial Services,2021-03-18 14:34:51
Data Engineer,Summit2Sea Consulting,"Arlington, VA",19 hours ago,Be among the first 25 applicants,"['', 'Must Have ', '401K', 'Summit2Sea Consulting is an Equal Opportunity Employer (EOE) and E-Verify employer. Qualified applicants are considered for employment without regard to age, race, color, religion, sex, national origin, sexual orientation, disability, or veteran status. If you need assistance or an accommodation during the application process because of a disability, it is available upon request. The company is pleased to provide such assistance, and no applicant will be penalized as a result of such a request.', 'New Sales Bonus', 'Databricks developer certification', 'Have you been looking to shift your career into high gear? This is your opportunity to take your ambitions and convert them into a solid career in a supportive and innovative environment!', 'Strong competency in building data pipelines involving ETL / ELT on multiple sources and destinations', 'Vacation/Sick Leave/Personal Time Off', ' Work to understand complex client environment and their unique business problems Build and deliver high-impact solutions for our clients Help build a new breed of products that combine ML and RPA technologies to automate business processes ', 'Quarterly Recruiting Bonus', 'Benefits', 'Benefit Components', 'Knowledge Contribution Bonus', 'Competitive Base Salary', 'Life Insurance', 'Dental Insurance', ""We value the individual and share our company's success across our team."", 'Compensation Components', 'Health Insurance', 'Requirements', 'Team player with critical thinking and a can-do attitude', ""Impactful Work You'll Do "", 'Help build a new breed of products that combine ML and RPA technologies to automate business processes', 'Medium to strong competency in AWS and Databricks environments ', 'Build and deliver high-impact solutions for our clients', ' Strong competency in building data pipelines involving ETL / ELT on multiple sources and destinations Strong competency in design and implementation of complex data transformations Medium to strong competency in AWS and Databricks environments  Medium to strong competency in python language and libraries, including pandas, numpy, pyspark Team player with critical thinking and a can-do attitude ', 'Medium to strong competency in python language and libraries, including pandas, numpy, pyspark', 'Data Engineer.', ' Experience with AWS Glue, StreamSet, MLOps, building ML models Databricks developer certification ', 'Strong competency in design and implementation of complex data transformations', ""This position is currently remote due to COVID-19.*RequirementsMust Have  Strong competency in building data pipelines involving ETL / ELT on multiple sources and destinations Strong competency in design and implementation of complex data transformations Medium to strong competency in AWS and Databricks environments  Medium to strong competency in python language and libraries, including pandas, numpy, pyspark Team player with critical thinking and a can-do attitude Nice to Have  Experience with AWS Glue, StreamSet, MLOps, building ML models Databricks developer certification BenefitsUpper Tier Compensation includes a base salary and bonuses based upon performance, business development, employee referrals and knowledge sharing. We value the individual and share our company's success across our team.Summit2Sea is committed to offering our employees a benefits package that is competitive and comprehensive enough to meet their goals and needs. As a valued member of the S2S team, employees are provided with a collection of benefits to include paid holidays, health and dental care to name a few.\u200dCompensation Components"", ""This position is currently remote due to COVID-19.*RequirementsMust Have  Strong competency in building data pipelines involving ETL / ELT on multiple sources and destinations Strong competency in design and implementation of complex data transformations Medium to strong competency in AWS and Databricks environments  Medium to strong competency in python language and libraries, including pandas, numpy, pyspark Team player with critical thinking and a can-do attitude Nice to Have  Experience with AWS Glue, StreamSet, MLOps, building ML models Databricks developer certification BenefitsUpper Tier Compensation includes a base salary and bonuses based upon performance, business development, employee referrals and knowledge sharing. We value the individual and share our company's success across our team.Summit2Sea is committed to offering our employees a benefits package that is competitive and comprehensive enough to meet their goals and needs. As a valued member of the S2S team, employees are provided with a collection of benefits to include paid holidays, health and dental care to name a few.\u200dCompensation ComponentsCompetitive Base SalaryQuarterly Recruiting BonusNew Sales BonusKnowledge Contribution BonusBenefit ComponentsPaid HolidaysVacation/Sick Leave/Personal Time OffHealth InsuranceDental InsuranceLife Insurance401KSummit2Sea Consulting is an Equal Opportunity Employer (EOE) and E-Verify employer. Qualified applicants are considered for employment without regard to age, race, color, religion, sex, national origin, sexual orientation, disability, or veteran status. If you need assistance or an accommodation during the application process because of a disability, it is available upon request. The company is pleased to provide such assistance, and no applicant will be penalized as a result of such a request."", 'Paid Holidays', 'Work to understand complex client environment and their unique business problems', 'Nice to Have ', 'Experience with AWS Glue, StreamSet, MLOps, building ML models']",Entry level,Full-time,Analyst,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Teachable,"New York, NY",2 days ago,53 applicants,"['', 'What You Will Be Doing', 'Improving data warehouse performance by building out federated data sources', 'Maintaining and improving the ETL platform', '2-3+ years of hands-on experience writing python and SQL ETL jobs and working with open-source ETL technologies', 'Maintaining and improving event collection, queueing, and processingAssisting the the next phase of data lake developmentMaintaining and improving the ETL platformImproving data warehouse performance by building out federated data sourcesResponding to issues and alerts as they arise', '2-3+ years of hands-on experience writing python and SQL ETL jobs and working with open-source ETL technologiesExperience with Airflow is a plusExperience with Docker or Kubernetes is a plusExperience with kafka or other pub/sub messaging queues is a plusWho cares about code quality and strives to balance efficiency with readability and to help teammates achieve the sameWho upholds Teachable values, including working as part of a diverse team', 'Benefits', 'We Are Looking For Someone With', 'Supporting customer-facing reportingBuilding an in-house link tracker or integrating with an attribution vendorConverting event processing from batch to streamHelping the engineering org transition from ruby sidekiq to kafka consumers for webhooks', 'Experience with kafka or other pub/sub messaging queues is a plus', ""Teachable encourages individuals from a broad diversity of backgrounds to apply for positions. We are an equal opportunity employer, meaning we're committed to a fair and consistent interview process. Please tell us in your application if you require an accommodation to apply for a job or to perform your job."", 'Assisting the the next phase of data lake development', 'Building an in-house link tracker or integrating with an attribution vendor', 'Responding to issues and alerts as they arise', 'Experience with Docker or Kubernetes is a plus', 'Who upholds Teachable values, including working as part of a diverse team', 'Converting event processing from batch to stream', 'What You Might Work On', 'Experience with Airflow is a plus', 'Who cares about code quality and strives to balance efficiency with readability and to help teammates achieve the same', 'Supporting customer-facing reporting', 'Maintaining and improving event collection, queueing, and processing', 'Helping the engineering org transition from ruby sidekiq to kafka consumers for webhooks']",Mid-Senior level,Full-time,Information Technology,Internet,2021-03-18 14:34:51
Data Engineer,Ford Motor Company,"Dearborn, MI",20 hours ago,Be among the first 25 applicants,"['', 'Our Preferred Requirements', 'Strong analytical and problem-solving skills', 'Reviews and approves Test Scenarios and Test cases. Plans and conducts UAT for the Data products', 'Ability to establish and maintain cooperative and effective working relationships with application implementation teams, IT project teams, business customers, and end users.', 'Work with data scientists and software engineers to understand data activities, data solution ideation, and implementation Work with OGC and GDI&A data governance to ensure customer privacy is respected and contractual purposes of use are honoredWork with 3rd party data teams to ensure that intended use cases are accounted for with external data contractsAudit implemented processes and procedures to ensure compliance and achievement of OKRs pertaining to data accessMonitor Data Pipelines and Data flows and address the issues within established First Responder processesDevelops use cases to drive development and prioritization of operational needsManages Product backlog, prioritize features and drive delivery of the productReviews and approves Test Scenarios and Test cases. Plans and conducts UAT for the Data products', 'At least two years of experience within a governance or audit organization', 'Bachelor’s degree Four or more years of experience in Technology projects (i.e. IT projects)At least two years of experience within a governance or audit organization', 'Ability to work as a global team member, as well as independently, in a changing environment and managing multiple priorities.', 'Work with 3rd party data teams to ensure that intended use cases are accounted for with external data contracts', 'Work with data scientists and software engineers to understand data activities, data solution ideation, and implementation ', 'Experience as a Product Owner is a plus. Ability to take business requirements and translate them into technical requirements ', 'Knowledge and experience with Agile methodology', 'Manages Product backlog, prioritize features and drive delivery of the product', 'Ability to deliver work within deadlines.', 'Work with OGC and GDI&A data governance to ensure customer privacy is respected and contractual purposes of use are honored', 'Experience building Data processing solutions ', 'Develops use cases to drive development and prioritization of operational needs', 'Bachelor’s degree ', 'Familiar with big data and machine learning tools and platforms', 'Experience building Data processing solutions Experience as a Product Owner is a plus. Ability to take business requirements and translate them into technical requirements Knowledge and experience with Agile methodologyPossess excellent oral and written communication skills, as well as facilitation and presentation skills, and engaging presentation style. Ability to establish and maintain cooperative and effective working relationships with application implementation teams, IT project teams, business customers, and end users.Strong analytical and problem-solving skillsAbility to work as a global team member, as well as independently, in a changing environment and managing multiple priorities.Ability to deliver work within deadlines.Familiar with big data and machine learning tools and platformsFamiliar with BI tools, such as Tableau, Data Stage, or QlikView etc.Proficiency in Word, Excel and PowerPointProficiency in Visio, Microsoft Project or Rally tools a plus', 'What You’ll Receive In Return', 'Audit implemented processes and procedures to ensure compliance and achievement of OKRs pertaining to data access', 'Proficiency in Visio, Microsoft Project or Rally tools a plus', 'Four or more years of experience in Technology projects (i.e. IT projects)', 'What You’ll Be Able To Do', 'Possess excellent oral and written communication skills, as well as facilitation and presentation skills, and engaging presentation style. ', 'Monitor Data Pipelines and Data flows and address the issues within established First Responder processes', 'Familiar with BI tools, such as Tableau, Data Stage, or QlikView etc.', 'Proficiency in Word, Excel and PowerPoint', 'The Minimum Requirements We Seek']",Entry level,Contract,Information Technology,Automotive,2021-03-18 14:34:51
Data Engineer,Twitch,"San Francisco, CA",3 weeks ago,Over 200 applicants,"['', '  3+ years of experience in data engineering, software engineering, or other related roles  3+ years using relational database concepts with a working knowledge of SQL, SQL Tuning, data modeling best principles, OLAP, Big Data technologies  3+ years of experience generating data pipelines from multiple data sources, in collaboration with diverse team members Experience with development best practices, including query optimization, version control, code reviews, and documentation Experience with Amazon Web Services: Redshift, S3, Glue, EMR, or Athena Experience with Python ', 'About Us', ' Delight data consumers throughout Twitch by ensuring they have the data they need to inform decisions, where and when they need it.  Define and own organization-level data architecture for a trusted, governed, dimensionally-modeled repository of data that enables Twitch staff to quickly and reliably answer their questions. Prioritize projects from a diverse set of partners Protect data sources against data quality issues: work with data producers to ensure data passes acceptance tests; design, develop and maintain data quality monitoring and assurance framework; and continuously improve the processes for developing new ones, raising the level of quality expected from our work. Improve data discovery: create data exploration processes and promote adoption of data sources across the company.  Optimize business, engineering, and data processes via data architecture, engineering, testing, and operational excellence best practices. ', ' 3+ years of experience generating data pipelines from multiple data sources, in collaboration with diverse team members', 'Protect data sources against data quality issues: work with data producers to ensure data passes acceptance tests; design, develop and maintain data quality monitoring and assurance framework; and continuously improve the processes for developing new ones, raising the level of quality expected from our work.', 'Delight data consumers throughout Twitch by ensuring they have the data they need to inform decisions, where and when they need it.', ' Define and own organization-level data architecture for a trusted, governed, dimensionally-modeled repository of data that enables Twitch staff to quickly and reliably answer their questions.', 'Experience with Python', 'Medical, Dental, Vision & Disability Insurance', ' 3+ years of experience in data engineering, software engineering, or other related roles', 'About The Role', 'Experience with Amazon Web Services: Redshift, S3, Glue, EMR, or Athena', 'Flexible PTO', 'Breakfast, Lunch & Dinner Served Daily', 'Perks', 'Prioritize projects from a diverse set of partners', 'Free Snacks and Beverages ', ' Optimize business, engineering, and data processes via data architecture, engineering, testing, and operational excellence best practices.', 'You Will', 'Amazon Employee Discount', ' 401(k) , Maternity & Parental Leave ', 'Improve data discovery: create data exploration processes and promote adoption of data sources across the company.', 'Experience with development best practices, including query optimization, version control, code reviews, and documentation', 'Monthly Contribution and Discounts for Wellness Related Activities & Programs (e.g., gym memberships, off-site massages),', ' Medical, Dental, Vision & Disability Insurance  401(k) , Maternity & Parental Leave  Flexible PTO  Commuter Benefits  Amazon Employee Discount Monthly Contribution and Discounts for Wellness Related Activities & Programs (e.g., gym memberships, off-site massages), Breakfast, Lunch & Dinner Served Daily Free Snacks and Beverages  ', 'You Have:', ' 3+ years using relational database concepts with a working knowledge of SQL, SQL Tuning, data modeling best principles, OLAP, Big Data technologies', ' Commuter Benefits ']",Not Applicable,Full-time,Information Technology,Internet,2021-03-18 14:34:51
Data Engineer,Knock,United States,1 day ago,25 applicants,"['', 'We encourage you to apply even if you don’t have every listed requirement. ', 'Have experience building data pipelines and utilizing programming tools to do so. Here Knock we use Apache Spark, Scala, Rust, Go, Python, and Rest APIs - but you can learn these technologies on the job', 'You should be versed in developing APIs to serve data produced by ETL jobs', 'This position is in the continental United States.', ' Knock is a 100% remote, work from home culture and has been since our inception in 2015  100% employee covered medical, dental, & vision premiums  Unlimited PTO (2 weeks mandatory) + flexible work schedules  Paid parental leave  $1,000 each year for education, training, and professional development  Salary + Equity. We offer equitable compensation based market data, accomplishments, experience, & your location ', 'Design data schemas and optimize internal data warehouses, augmenting data from multiple sources.', 'Design, build, and maintain REST APIs to serve data to customers', 'Build new ETL jobs from scratch, as well as maintain existing jobs.', 'All of our teams have a strong customer-first mindset and data-driven approach to our work, and that should be your approach as well', '$1,000 each year for education, training, and professional development ', 'Unlimited PTO (2 weeks mandatory) + flexible work schedules ', ' Build data pipelines and aggregate data. Design data schemas and optimize internal data warehouses, augmenting data from multiple sources. Design, build, and maintain REST APIs to serve data to customers Cross-functionally collaborate with our Data Science and Machine Learning teams.  Understand the data that powers our applications, and be able to propose appropriate data models for new features. Build new ETL jobs from scratch, as well as maintain existing jobs. Be committed to good engineering practice of testing, logging, alerting and deployment processes. Monitor and troubleshoot operational or data issues in the data pipelines. Drive architectural plans and implementation for future data storage, reporting, and analytic solutions. ', '100% employee covered medical, dental, & vision premiums ', 'Build data pipelines and aggregate data.', ' Have experience building data pipelines and utilizing programming tools to do so. Here Knock we use Apache Spark, Scala, Rust, Go, Python, and Rest APIs - but you can learn these technologies on the job You should be versed in developing APIs to serve data produced by ETL jobs You have a desire to work at a rapidly growing startup and make it a success, and are comfortable learning new technologies and tools Your experience in SQL, MySQL, or Postgres will be valuable here to identify slow queries and debugging All of our teams have a strong customer-first mindset and data-driven approach to our work, and that should be your approach as well Believe in creating diverse, equitable, and inclusive practices and programs that will further Knock’s commitment to making an impact, learning, putting people first, being open, and courageous. Have proven success working 100% remote in prior positions & are experienced working with a distributed, national team.  We encourage you to apply even if you don’t have every listed requirement.  ', 'Salary + Equity. We offer equitable compensation based market data, accomplishments, experience, & your location', 'Monitor and troubleshoot operational or data issues in the data pipelines.', 'Knock is a 100% remote, work from home culture and has been since our inception in 2015 ', 'Be committed to good engineering practice of testing, logging, alerting and deployment processes.', 'Believe in creating diverse, equitable, and inclusive practices and programs that will further Knock’s commitment to making an impact, learning, putting people first, being open, and courageous.', 'Understand the data that powers our applications, and be able to propose appropriate data models for new features.', 'Benefits, Perks, & Enjoying Life', 'Drive architectural plans and implementation for future data storage, reporting, and analytic solutions.', 'Please no recruitment firm or agency inquiries, you will not receive a reply from us.', 'Cross-functionally collaborate with our Data Science and Machine Learning teams. ', 'Paid parental leave ', 'You have a desire to work at a rapidly growing startup and make it a success, and are comfortable learning new technologies and tools', 'Have proven success working 100% remote in prior positions & are experienced working with a distributed, national team. ', 'Your experience in SQL, MySQL, or Postgres will be valuable here to identify slow queries and debugging']",Mid-Senior level,Full-time,Engineering,Internet,2021-03-18 14:34:51
Data Engineer,"Enjoy Technology, Inc.","Palo Alto, CA",3 days ago,78 applicants,"['', 'Experience with Web development technologies', 'Own and manage data engineering roadmap, architecture and deliverables by working with various engineering and business teams align on priorities', 'Proficient in Complex SQL development and optimization.', 'Always be on the lookout to automate and improve existing data processes for quicker turnaround and high productivity', 'Communicate updates with the leadership team, as well as business & engineering stakeholders.', 'Have a high sense of urgency to deliver projects as well as troubleshoot and fix data issues', 'Collaborate with onshore and offshore data team members', 'Experience in Data modeling & schema design.', 'Strong Database knowledge, Snowflake and postgres preferred', ' BS or MS in Computer Science, Software Engineering, or related field 5+ years of experience building large scale data warehouse, big data platform as well as real-time systems Strong Database knowledge, Snowflake and postgres preferred Proficient in Complex SQL development and optimization. Proficient in Python development. Proficient with SDLC methodologies Experience in Data modeling & schema design. Strong understanding of master data management & data governance Experience with Unix/Shell scripting & Cloud tools Experience with Web development technologies Experience with Data workflow tools like Airflow, Pentaho etc Experience with implementing and operationalizing data quality architecture. Experience working with unstructured data format (Json/XML/Yaml) Experience with ETL development methodologies & tools. Exceptional Excel / Data Management skills Experience in ECommerce and Retail will be a plus ', 'Experience working with unstructured data format (Json/XML/Yaml)', 'Develop a deep understanding of business & business processes.', 'Proficient with SDLC methodologies', 'Responsibilities', 'Experience with Unix/Shell scripting & Cloud tools', 'Experience in ECommerce and Retail will be a plus', 'Qualifications', 'As an equal opportunity employer, Enjoy is proud to maintain a workplace characterized by mutual respect, inclusivity, and the celebration of diversity. All qualified applicants are welcome and will be considered for employment without regard to race, color, ethnicity, national origin, religion, gender, sex, sexual orientation, gender identity, genetics, disability, veteran status, or ', 'This job description is not intended as a comprehensive or exhaustive list of all duties, responsibilities, and qualifications necessary for the position described herein. The duties, responsibilities, and qualifications of the position are subject to change without notice.', 'BS or MS in Computer Science, Software Engineering, or related field', 'Experience with Data workflow tools like Airflow, Pentaho etc', 'All offers of employment are subject to background checks and drug screens prior to start date. Enjoy will consider for employment qualified applicants with criminal histories in a manner consistent with applicable local, state, and federal laws and Fair Chance Ordinances.', 'Create optimized and scalable data models', 'Develop, build & maintain mission critical data pipelines that gather data from various internal & external sources', 'Proficient in Python development.', 'Exceptional Excel / Data Management skills', 'Experience with ETL development methodologies & tools.', 'Experience with implementing and operationalizing data quality architecture.', 'other applicable protected characteristic under local, state, or federal law.', 'Develop, manage, operationalize & improve current data systems & processes.', 'Job Description', '5+ years of experience building large scale data warehouse, big data platform as well as real-time systems', 'Strong understanding of master data management & data governance', ' Own and manage data engineering roadmap, architecture and deliverables by working with various engineering and business teams align on priorities Develop, build & maintain mission critical data pipelines that gather data from various internal & external sources Create optimized and scalable data models Develop, manage, operationalize & improve current data systems & processes. Always be on the lookout to automate and improve existing data processes for quicker turnaround and high productivity Collaborate with onshore and offshore data team members Develop a deep understanding of business & business processes. Communicate updates with the leadership team, as well as business & engineering stakeholders. Have a high sense of urgency to deliver projects as well as troubleshoot and fix data issues ']",Entry level,Full-time,Information Technology,Retail,2021-03-18 14:34:51
Data Engineer (multiple roles + levels),Civitech,"Remote, OR",2 days ago,Be among the first 25 applicants,"['', 'Perform exploratory analysis and provide ad-hoc reports', 'Collaborate with partner organizations on APIs, tools, and data sets', 'Track, evaluate, and contribute to technology advances across academic, open source, and commercial forums', 'Contribute to related tasks across the company, including technical architecture, product design, client support, marketing, and cross-training', 'Passionate about applying your technical expertise to help get Democrats elected up and down the ballot', 'Knowledge of machine learning (ML) and artificial intelligence (AI) methods', 'Solid understanding of attribute-based access control (ABAC), virtual private database, row-level security, and related enforcement approaches', 'Diagnose and resolve deficiencies in data quality and systems performance', 'Develop data pipelines', 'Design, implement, and maintain various election-related data sets', ' Other Information ', 'Comfortable with the pace and fluidity of work at a startup company', 'Design, implement, and maintain various election-related data setsDevelop data pipelinesDefine and monitor metrics for our data setsDiagnose and resolve deficiencies in data quality and systems performancePerform exploratory analysis and provide ad-hoc reportsCollaborate with partner organizations on APIs, tools, and data setsTrack, evaluate, and contribute to technology advances across academic, open source, and commercial forumsParticipate in political events that promote, train, and/or directly use our systemsContribute to related tasks across the company, including technical architecture, product design, client support, marketing, and cross-training', 'Advanced skills in database programming (every aspect of SQL, plus PL/pgSQL or PL/SQL or T-SQL)', 'Familiar with a variety of middleware approaches and tools (ORM, MQ, GraphQL, REST, webhook, etc)', ' Security Engineer  ', 'Practical experience with tools for intrusion detection, user activity monitoring, and data loss prevention', 'Experience with cloud computing environments (Google Cloud preferred, but AWS or Azure okay)', 'Experience with data visualization, dashboard, and/or reporting tools (Tableau, Jasper, or others)', ' Other desired but not required skills Experience with data visualization, dashboard, and/or reporting tools (Tableau, Jasper, or others)Familiar with a variety of middleware approaches and tools (ORM, MQ, GraphQL, REST, webhook, etc)Knowledge of machine learning (ML) and artificial intelligence (AI) methodsKnowledge of U.S. elections administration and/or campaign operations', 'Advanced knowledge of scaling and high-availability techniques for data architectures', 'Practical experience with a variety of application data workloads (OLTP, OLAP, etc.)', ' Introduction ', 'Define and monitor metrics for our data sets', 'Familiar with best practices for data privacy (for example, anonymization, masking, tokenization, etc) ', 'Participate in political events that promote, train, and/or directly use our systems', ' Security Engineer  Practical experience with tools for intrusion detection, user activity monitoring, and data loss preventionSolid understanding of attribute-based access control (ABAC), virtual private database, row-level security, and related enforcement approachesFamiliar with best practices for data privacy (for example, anonymization, masking, tokenization, etc) ', ' Performance Engineer or Database Administrator ', 'Basic skills in database administration (PostgreSQL preferred, but Oracle or MySQL okay)', 'Solid programming skills (especially SQL, Python, Go, and/or JavaScript)', 'Knowledge of U.S. elections administration and/or campaign operations', 'Experience with ETL tools (especially Airflow and DBT)', 'Practical experience with database tuning and performance optimization', ' Other desired but not required skills ', 'Relevant advanced degree, certification, or 4+ years relevant work experience', 'Relevant advanced degree, certification, or 4+ years relevant work experienceSolid programming skills (especially SQL, Python, Go, and/or JavaScript)Experience with ETL tools (especially Airflow and DBT)Basic skills in database administration (PostgreSQL preferred, but Oracle or MySQL okay)Experience with cloud computing environments (Google Cloud preferred, but AWS or Azure okay)Comfortable with the pace and fluidity of work at a startup companyPassionate about applying your technical expertise to help get Democrats elected up and down the ballot', ' Performance Engineer or Database Administrator Advanced skills in database programming (every aspect of SQL, plus PL/pgSQL or PL/SQL or T-SQL)Practical experience with database tuning and performance optimizationPractical experience with a variety of application data workloads (OLTP, OLAP, etc.)Advanced knowledge of scaling and high-availability techniques for data architectures']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Sheetz,"Pittsburgh, PA",1 day ago,Be among the first 25 applicants,"['', 'Sheetz, Inc. is a fast-growing, family-owned, food/convenience company that has been in business since 1952. Sheetz has over 600 locations in Pennsylvania, Ohio, Virginia, West Virginia, Maryland and North Carolina.', 'We also provide employee training programs to continue your growth in your field while here. We believe there’s always room to learn and we offer opportunities to gain hands on experience. Our employees are\xa0always\xa0encouraged to explore new opportunities, especially within our organization.', 'Minimum 5 years development experience in at least one object-oriented language (Python, Perl, Java, etc.) required', 'Experience building and optimizing “Big Data” pipelines, architectures and data sets required', '401(K) Retirement Savings Plan – a 4% match!', '\xa0Co-lead in the selection and build of our data and analytic tools including Enterprise Data Warehouse, Data Lake, Analytics platform and Data Catalogue.', ""WHAT YOU'LL DO"", 'Establish standardization and educate data users on query best practices allowing for re-use and analytic efficiency.', 'Manage the democratization of data knowledge across the organization through the communication and maintenance of master data, metadata, data management repositories, data models, and data standards', ""Our mission at Sheetz has been to meet the needs of customers on the go. Of course, things have changed over those nearly 70 years. Life is faster and busier, and customers expect us to be there when they need us most. One thing that hasn't changed is our commitment to our customers, our employees and the communities in which we operate. Sheetz donates millions of dollars every year to the charities it holds dear."", 'Design, implement, manage data architecture and data pipeline across multiple data sources.', 'Collaborate closely with Business and Technical Owners in identifying data sources and aggregating structured or unstructured data into dimensional data models able to be consumed by the business.', 'Employee Stock Ownership', 'always', 'Bachelor degree in Computer Science, Management Information Systems, Computer Engineering or a related field required.', ""All team members are working remote due to the\xa0pandemic. Keeping\xa0our employees safe is most important to us. After we transition back to office work, when it's safe, we do have the option for partial remote work arrangements for candidates living within our 6 state footprint."", ""HOW YOU'LL GROW"", 'ABOUT SHEETZ', 'We have all the perks you would expect from a leading employer, such as medical, dental & vision, in addition to that we have:', 'PERKZ', ""EXPERIENCE YOU'LL BRING"", 'Education', 'Bachelor degree in Computer Science, Management Information Systems, Computer Engineering or a related field required.Master’s degree in Computer Science, MIS or Computer Engineering preferred.', '\xa0Co-lead in the selection and build of our data and analytic tools including Enterprise Data Warehouse, Data Lake, Analytics platform and Data Catalogue.Design, implement, manage data architecture and data pipeline across multiple data sources.Collaborate closely with Business and Technical Owners in identifying data sources and aggregating structured or unstructured data into dimensional data models able to be consumed by the business.Identify potential process improvements and designs and implement automated solutions.Manage the democratization of data knowledge across the organization through the communication and maintenance of master data, metadata, data management repositories, data models, and data standardsEstablish standardization and educate data users on query best practices allowing for re-use and analytic efficiency.', 'End to end experience in the analytic lifecycle from structured/unstructured raw data, data wrangling, creating data pipelines, to self-service dashboards leveraged by the business required', 'We are an innovator in our space and we want to make sure you can keep doing that. We are happy to send you to conferences and continuing education to make sure you continue to be the expert in your field.', 'Employee Profit Share', 'Minimum 5 years working with large databases and data warehouses utilizing both relational and non-relational data models required', 'Employee Stock OwnershipCollege Tuition Reimbursement401(K) Retirement Savings Plan – a 4% match!Employee Profit ShareVIP Sheetz Membership (Discounts!)', 'Minimum 5 years working in a Data Engineer role required', 'College Tuition Reimbursement', 'Responsible for developing complex, large scale data models and pipelines that organize and standardize the data to make it readily accessible and consumable by the business for reporting and data science needs. Collaborate with various areas of the Business in order to determine and source the appropriate data through internal and external means. Investigate new and existing technologies and data sources and assess their viability within the Sheetz environment.', 'Experience', 'Proficiency in data visualization tool (Tableau) preferred', 'Experienced with data wrangling and preparation for use within data science, business intelligence or similar analytical functions require', 'Strong understanding of cloud computing database technologies (Azure, AWS, GCP) required', 'Identify potential process improvements and designs and implement automated solutions.', 'Minimum 5 years working in a Data Engineer role requiredMinimum 5 years working with large databases and data warehouses utilizing both relational and non-relational data models requiredEnd to end experience in the analytic lifecycle from structured/unstructured raw data, data wrangling, creating data pipelines, to self-service dashboards leveraged by the business requiredAdvanced knowledge of SQL requiredMinimum 5 years development experience in at least one object-oriented language (Python, Perl, Java, etc.) requiredProficiency in data visualization tool (Tableau) preferredStrong understanding of cloud computing database technologies (Azure, AWS, GCP) requiredExperience building and optimizing “Big Data” pipelines, architectures and data sets requiredExperienced with data wrangling and preparation for use within data science, business intelligence or similar analytical functions require', '\xa0', 'VIP Sheetz Membership (Discounts!)', 'Master’s degree in Computer Science, MIS or Computer Engineering preferred.', 'We sell hot dogs, beer, gas, and a bunch of other things, but we’re really in the business of “convenience” and that should resonate with you.\xa0At Sheetz, we have a history of making our customers’ lives easier. We pioneered self-service touch screens for ordering food. We rolled out our industry leading Sheetz mobile app long before curbside pickup was around. Although we’re not a tech company, technology has been at the forefront of our success. And who makes that happen? You do!\xa0', 'Advanced knowledge of SQL required', 'Building, integrating and supporting the cutting-edge applications that power 600+ stores, 22,000 employees and millions of customers isn’t an easy job, but that’s why we do it!\xa0And we do it in an award-winning culture that’s casual and fun. If you’re looking for a Made to Order career, hit that APPLY button and let’s chat!']",Mid-Senior level,Full-time,Information Technology,Retail,2021-03-18 14:34:51
Data Engineer,"Velocity Works, LLC","Pittsburgh, PA",1 day ago,Be among the first 25 applicants,"['', 'Work closely with product and engineering to develop strategy for long term data platform architecture.', 'Identify issues affecting work progress and recommends solutions. ', 'Works with the project team to define tasks and create team work plans with moderate supervision. ', '5+ years designing, building, and maintaining data processing systems in AWS', 'Automate read, extract, transform, stage and load data processes using cloud-based tools', 'Develop, construct, test and maintain data management systems using appropriate tools to perform data extraction, modeling, transformation, cleansing, loading, and interpretation', 'Collected data from commercial databases, client databases, EMR systems, or other structured and unstructured sources.', 'Familiar with Informatica, Pentaho, or similar', 'HL7 experience ', 'Align data pipeline, warehouse and analytics architecture with business requirements', 'Filter and ""clean"" the data to remove errors and ensure consistency and completeness of data. ', 'Define and catalog company data assets ', '5+ years of programming/scripting in Python or javascript', 'Analyze and troubleshoot data related issues ', 'Control project costs, communicating any project-related expenses and recommend ways to control costs. ', 'Implement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it.', 'Collaborate with others to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.', '6+ years of SQL or No-SQL ', '5 or more years’ experience as a data engineer6+ years of with schema design and dimensional data modeling6+ years of SQL or No-SQL 5+ years of programming/scripting in Python or javascriptFamiliar with Informatica, Pentaho, or similar5+ years designing, building, and maintaining data processing systems in AWS5+ years working with data models, database warehouse design and data ETL automation HL7 experience ', 'Use ETL tools and programming skills to write cleansing scripts to correct errors through automation.', 'Organize and store data in a structurally relevant way in data warehouse or data lakes for easy access and analysis by team members.', 'Collaborate with others to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.Work closely with product and engineering to develop strategy for long term data platform architecture.Align data pipeline, warehouse and analytics architecture with business requirementsDevelop, construct, test and maintain data management systems using appropriate tools to perform data extraction, modeling, transformation, cleansing, loading, and interpretationIntegrate up-and-coming data management and software engineering technologies into existing data structures.Implement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it.Define and catalog company data assets Automate read, extract, transform, stage and load data processes using cloud-based toolsAnalyze and troubleshoot data related issues Collected data from commercial databases, client databases, EMR systems, or other structured and unstructured sources.Filter and ""clean"" the data to remove errors and ensure consistency and completeness of data. Use ETL tools and programming skills to write cleansing scripts to correct errors through automation.Organize and store data in a structurally relevant way in data warehouse or data lakes for easy access and analysis by team members.Works with the project team to define tasks and create team work plans with moderate supervision. Identify issues affecting work progress and recommends solutions. Control project costs, communicating any project-related expenses and recommend ways to control costs. ', '6+ years of with schema design and dimensional data modeling', '5 or more years’ experience as a data engineer', 'Integrate up-and-coming data management and software engineering technologies into existing data structures.', '5+ years working with data models, database warehouse design and data ETL automation ']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,BoomTown - Real Estate Platform,"Charleston, South Carolina Metropolitan Area",1 day ago,31 applicants,"['', 'Preferred Qualifications:', 'With our product suite, you get access to world-class lead generation, consumer-websites, CRM technology, and a powerful mobile app. Together, they create a seamless experience so you can attract, convert, and win new clients easily.\xa0', 'Experience with ETL orchestration tools to run large scale ETL workflows', 'To help you stay energized, engaged and inspired, we offer a wide range of benefits including a 401k retirement plan, comprehensive healthcare, discretionary PTO after a year, and a quarterly wellness reimbursement towards health and fitness.\xa0\xa0', 'Strong knowledge of SQL and Python\xa0', 'Understand product descriptions for new features and how they fit into the greater BoomTown system while learning how our customers use them', 'Build ETL processes that integrate data from multiple, highly variant sources into analytic models and data stores that feed multiple end user solutions and perspectives', 'Experience deploying infrastructure as code with tools such as AWS CloudFormation and Terraform', 'Significant experience with large scale data sources, structures and processes', 'BoomTown is a software platform designed to help real estate professionals generate leads, manage contacts, and run their business better. Over 40,000 of the industry’s best use our products to close more deals. Their success is trademarked with growth. It’s why the Real Trends Thousand is dominated by BoomTown clients. We bring the technology and experts to turn opportunities into closings.', 'Experience working in an Agile development environment will be a plus.', 'BoomTown is looking for a Data Engineer to join a cross-functional team that serves as a virtual center of data excellence for the company.\xa0\xa0', 'Responsibilities:', 'BS degree in Computer Science or related technical field\xa0\xa0', 'Participate in technology evaluation and selection initiatives that grow the company’s core data and analytics capacity\xa0', 'BS degree in Computer Science or related technical field\xa0\xa02-4 years of relevant work experienceA deep understanding of general ETL processing and tools as well as data warehousing concepts and workflowsStrong knowledge of SQL and Python\xa0Significant experience with large scale data sources, structures and processes', 'Experience with Amazon Web Services, particularly Redshift, S3\xa0or comparable services from another cloud providerExperience with ETL orchestration tools to run large scale ETL workflowsExperience deploying infrastructure as code with tools such as AWS CloudFormation and TerraformExperience working with web data sourcesExperience working in an Agile development environment will be a plus.', '2-4 years of relevant work experience', 'A deep understanding of general ETL processing and tools as well as data warehousing concepts and workflows', 'Work closely with our operations and other development teams to optimize source data acquisition processes and strategies', 'What is BoomTown?', 'Continually research and learn the latest approaches to building analytical solutions in highly variant data environments\xa0', 'Required Qualifications:', 'Be a proactive member of an autonomous, cross-disciplined team with a goal of building best in class data solutions that delight internal and external customers.', 'BoomTown is proud to be an Equal Opportunity Employer.\xa0We evaluate qualified applicants without regard to race, color, national origin, religion, sex, sexual orientation, gender identity, disability, protected veteran status, and other statuses protected by law.', 'Experience with Amazon Web Services, particularly Redshift, S3\xa0or comparable services from another cloud provider', 'As the Data Engineer, you will design and build analytic data stores and integration processes that meet the data needs of various internal and external constituents.\xa0The Data Engineer will be responsible for designing, building, monitoring, conceptualizing, innovating, and collaborating with team members and customers.\xa0You will be in a fast-growing and rapidly changing environment that will demand a continuous improvement mindset and a hunger for the next challenge.', 'Our Benefits:', 'Be a proactive member of an autonomous, cross-disciplined team with a goal of building best in class data solutions that delight internal and external customers.Build ETL processes that integrate data from multiple, highly variant sources into analytic models and data stores that feed multiple end user solutions and perspectivesContinually research and learn the latest approaches to building analytical solutions in highly variant data environments\xa0Work closely with our operations and other development teams to optimize source data acquisition processes and strategiesParticipate in technology evaluation and selection initiatives that grow the company’s core data and analytics capacity\xa0Understand product descriptions for new features and how they fit into the greater BoomTown system while learning how our customers use themTake initiative on assigned day-to-day tasks and keep pace with the team to get the job done while knowing when to ask for help.', 'Take initiative on assigned day-to-day tasks and keep pace with the team to get the job done while knowing when to ask for help.', 'Experience working with web data sources']",Mid-Senior level,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Engineer,KeHE Distributors,"Naperville, IL",18 hours ago,Be among the first 25 applicants,"['', 'Experience utilizing big data tools such as PySpark, Scala or others', 'Develop architecture required to return data to data warehouse for front-end product utilization', 'Primary Responsibilities', 'Desire to stay up to date with current technologies and best practices for data management and data science', ' Preferred Experience and Abilities: ', 'Develop, construct, test and maintain optimal data pipeline/ETL architectures', '1-3 years of experience building data pipelines within the AWS ecosystem', 'Essential Functions', 'Work closely within the team to prepare data for predictive and prescriptive modeling', 'Optimize AWS data delivery infrastructure for greater scalability', 'Bachelor’s Degree in Computer Science, Mathematics, Engineering, Management Information Systems or related field1-3 years of experience building data pipelines within the AWS ecosystem1-3 years of experience designing and implementing data warehouse solutionsAdvanced SQL and data design conceptsProficient programing experience using Python, R or similar language with experience building production level codeProficient working with Jenkins and deploying to production via Jenkin’s jobsDesire to stay up to date with current technologies and best practices for data management and data scienceDrive innovation and efficiency through new approachesAbility to work in a team environment that promotes collaboration', 'Bachelor’s Degree in Computer Science, Mathematics, Engineering, Management Information Systems or related field', 'Experience implementing AWS architecture using Serverless FrameworkUnderstanding of C programming languageExperience utilizing big data tools such as PySpark, Scala or others', 'Work with Enterprise Cloud Architecture teams to strive for greater functionality in our data systems', '1-3 years of experience designing and implementing data warehouse solutions', 'Provide production level code reviews for the team', 'Curate data models in the data warehouse to be used by front-end advanced analytics designers', 'Understanding of C programming language', 'Develop, construct, test and maintain optimal data pipeline/ETL architecturesWork closely within the team to prepare data for predictive and prescriptive modelingOptimize AWS data delivery infrastructure for greater scalabilityUtilize SQL as well as big data tools and frameworks to optimize data acquisition and preparation from enterprise data lake and data warehouseWork with Enterprise Cloud Architecture teams to strive for greater functionality in our data systemsDevelop architecture required to return data to data warehouse for front-end product utilizationCurate data models in the data warehouse to be used by front-end advanced analytics designersProvide production level code reviews for the teamHelp design, maintain and implement quality assurance and testing approachesDeploy scripts and architectures to production via Jenkins', 'Proficient programing experience using Python, R or similar language with experience building production level code', 'Ability to work in a team environment that promotes collaboration', 'Utilize SQL as well as big data tools and frameworks to optimize data acquisition and preparation from enterprise data lake and data warehouse', 'Experience implementing AWS architecture using Serverless Framework', 'Drive innovation and efficiency through new approaches', 'Deploy scripts and architectures to production via Jenkins', 'Minimum Requirements, Qualifications, Additional Skills, Aptitude', ' Requisition ID ', 'Help design, maintain and implement quality assurance and testing approaches', 'Advanced SQL and data design concepts', 'Overview', 'Sound good?', 'Proficient working with Jenkins and deploying to production via Jenkin’s jobs']",Entry level,Full-time,Information Technology,Food & Beverages,2021-03-18 14:34:51
Jr Data Engineer,Thomson Reuters,"Eagan, MN",1 week ago,Be among the first 25 applicants,"['', 'Compensation:', 'Perks: You will have the opportunity to work for a company that has a market dominant position for both content and technology and is passionate about giving back to the community.', 'Experience with Agile software development practices', 'Benefits: Extraordinary benefits package.', 'Experience with Python or Java development', 'Be Agile', 'Perks:', 'Perks: You will have the opportunity to work for a company that has a market dominant position for both content and technology and is passionate about giving back to the community.Learning & Development: You are will be joining a growing sales team that has the commitment of the company to prioritize organic growth and has made investments to expand the sales team capability.Compensation: Base salary and a variable compensation that is directly related to your successBenefits: Extraordinary benefits package.', 'Benefits', ' Data Engineer, ', 'you will…', 'Be Customer Driven: Continuously improve software engineering practices and contribute new ideas to the team ', 'Familiarity with Serverless design, architecture and frameworks such as Lambda', 'Compensation: Base salary and a variable compensation that is directly related to your success', 'Be Customer Driven: ', 'Bachelor’s Degree or Equivalent Work Experience', 'Be a subject Matter Expert: ', 'About The Role', 'Locations', 'Be Collaborative: Work with product owner(s) and/or team members to understand product vision and requirements ', '2+ years development experience in building ETL/ELT data flows', 'Strong problem-solving and interpersonal skills', 'In this role as a', 'Learning & Development: You are will be joining a growing sales team that has the commitment of the company to prioritize organic growth and has made investments to expand the sales team capability.', 'Experience with version control systems such as Git ', 'Familiarity with Docker and Elastic Containers', 'Experience with enterprise security ', 'Be Responsible: For the end-to-end delivery of features across our product suite ', 'Experience with CI/CD pipelines and processes', 'Be Agile: Working with a team to design, build and deploy high quality software products for ISRM ', 'Experience with Shell scripting and Linux OS', 'Familiarity with IaC frameworks such as CloudFormation or Terraform', 'Benefits:', 'Degree in a technical field such as Computer Science/Engineering, Physics, Mathematics or Statistics Familiarity with Docker and Elastic ContainersFamiliarity with Serverless design, architecture and frameworks such as LambdaFamiliarity with IaC frameworks such as CloudFormation or TerraformExperience with CI/CD pipelines and processesExperience with Shell scripting and Linux OSExperience with Database/Data Warehouse technologies such as PostgreSQL or SnowflakeExperience with enterprise security Experience with Agile software development practices', 'Hands-on knowledge in using SQL queries (analytical functions) and writing and optimizing SQL queries', 'Required Qualifications:', 'Ability to perform in a changing environment', 'Experience with Database/Data Warehouse technologies such as PostgreSQL or Snowflake', 'Data Engineer', 'Thomson Reuters', 'Accessibility ', 'Bachelor’s Degree or Equivalent Work Experience2+ years development experience in building ETL/ELT data flowsExperience with Python or Java developmentHands-on knowledge in using SQL queries (analytical functions) and writing and optimizing SQL queriesExperience working with data visualization tools (Tableau, Power BI...)Experience with version control systems such as Git Experience with cloud platforms and services such as AWS/AzureStrong problem-solving and interpersonal skillsAbility to perform in a changing environment', 'Be Collaborative', 'Be a subject Matter Expert: Regularly self-improve by monitoring changes in tech landscape', 'Job Description', 'Be Agile: Working with a team to design, build and deploy high quality software products for ISRM Be Responsible: For the end-to-end delivery of features across our product suite Be Customer Driven: Continuously improve software engineering practices and contribute new ideas to the team Be Collaborative: Work with product owner(s) and/or team members to understand product vision and requirements Be a subject Matter Expert: Regularly self-improve by monitoring changes in tech landscape', 'About You', 'Learning & Development:', 'Experience working with data visualization tools (Tableau, Power BI...)', 'Be Responsible:', ""What's in it for you!"", 'Experience with cloud platforms and services such as AWS/Azure', 'Degree in a technical field such as Computer Science/Engineering, Physics, Mathematics or Statistics ', 'Preferred Qualifications']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Collabera Inc.,"Houston, TX",,N/A,"['', 'Utilize Enterprise Technology assets, when applicable.', 'Must Have ', 'Generally work is self-directed and not prescribed.', 'Experienced in building Real-time/Batch Ingestion and data processing pipelines built using Spark/Databricks', 'You will be responsible to', 'Works with less structured, more complex issues.', '6+ years of experience in software development', 'Hiring Data Engineer for 100% Remote Role', 'Contract to Hire', 'Description: ', 'Present solutions to teams and stakeholders and document design decisions', 'Set up CI/CD Pipelines and maintain.', 'Prerequisite', 'Skilled in coding in Python or Java', 'Adopt Enterprise Architecture guidelines to store and manage Big Data.', 'Work with Product owner to develop stories and user flows.', 'Strong leadership, organizational and time management skills', '2+ years of experience leading small/medium teams', 'In the role of Lead Software Engineer, Come and join our teams in building our Clinical Products and other applications. You will work independently and in a team as needed, in developing new features to build new products and enhance our applications as needed. Our platforms consume and generate a great volume of data.', ""A bachelor's degree in Computer Sciences or related field, or equivalent work experience; master's degree preferred"", 'Able to contribute towards test automation and DevOps pipelines', 'Sense of urgency in daily work ethic', 'This is Contract to Hire role with decent Salary plus benefits on conversion.Currently, not open for individuals who needs a sponsorship or has visa dependencies.', 'This is Contract to Hire role with decent Salary plus benefits on conversion.', 'Adopt Enterprise Architecture guidelines to store and manage Big Data.Develop data pipelines using Spark/DatabricksPresent solutions to teams and stakeholders and document design decisionsUtilize Enterprise Technology assets, when applicable.Work with Product owner to develop stories and user flows.Set up CI/CD Pipelines and maintain.Generally work is self-directed and not prescribed.Works with less structured, more complex issues.', ""A bachelor's degree in Computer Sciences or related field, or equivalent work experience; master's degree preferred6+ years of experience in software development2+ years of experience leading small/medium teamsExperienced in building Real-time/Batch Ingestion and data processing pipelines built using Spark/DatabricksSkilled in coding in Python or JavaAble to contribute towards test automation and DevOps pipelinesStrong interpersonal skills, coupled with equally strong Team Building and CommunicationSense of urgency in daily work ethicStrong leadership, organizational and time management skills"", 'Currently, not open for individuals who needs a sponsorship or has visa dependencies.', 'Develop data pipelines using Spark/Databricks', 'Strong interpersonal skills, coupled with equally strong Team Building and Communication']",Executive,Contract,Information Technology,Pharmaceuticals,2021-03-18 14:34:51
Data Engineer,CitiusTech,"New York, NY",2 days ago,27 applicants,"['', "" Create salable and high-performance API's for data tracking/data querying"", 'Mandatory Skills:', ' Maintain security and data privacy.', ' Performance and code optimization', ' Knowledge of ETL concepts and Shell – Dos, Bash scripting is good to have', ' Strong experience on Python, Spark and SQL is a MUST. PySpark experience will be an added advantage', 'Job Description:', 'Ganesh Nadar', 'NYC, NY', ' Strong visual and verbal communication skills', 'Ganesh.nadar@citiustech.com', ' Work in a fast-paced, creative atmosphere to develop new ideas that adapt to evolving user needs', 'Overview:', 'Data Engineer', ' Loading from disparate data sets and reconciliation', ' High-speed querying.', ' Pre-processing using Python', '617-795-3066', 'Long Term!', ' Perform analysis of vast data stores and uncover insights.', 'Thanks,']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,General Motors,"Warren, MI",1 week ago,Be among the first 25 applicants,"['', 'Desired Skills', 'You will identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing processes for greater scalability, etc.', 'Create highly consistent and accurate analytic datasets suitable for data scientist team members', 'Must be a self-starter.', 'Manage and support manual scoring of data as needed to support POCs and usage of advanced analytics models over time', 'Act as SME on analytical data sets, with ability to answer data scientist questions or find related information to support analytical modeling', ' Tuition assistance and student loan refinancing;', ' Paid time off including vacation days, holidays, and parental leave for mothers, fathers and adoptive parents;', 'Lead contract resources to support data asset creation, reporting, and scoring is done with high quality and accuracy', 'Responsibilities', ' Company and matching contributions to 401K savings plan to help you save for retirement;', 'Identify and report data quality issues as needed', 'Ability to effectively work with a team of data scientists and business stakeholders exchanging knowledge.', 'Must be detail orientated/articulate with strong time management and organizational skills', '3+ years of experience as a data engineer or in a similar role', 'Familiar with predictive / analytical modeling techniques, theories, principles, and practices.', 'Communicates and maintains metadata, logical models, and data dictionaries for analytical data assets', 'Qualifications', 'Work with business partners on data-related technical issues and develop requirements to support their data infrastructure needs', 'About GM', 'Create highly consistent and accurate analytic datasets suitable for data scientist team membersEvaluate business needs and identify and source data into analytical data sets needed to support advanced analytics models that will deliver actionable insightsUnderstand and profile data to assess usefulness to meet business objectivesCombine data from different sources and transform data as neededCommunicates and maintains metadata, logical models, and data dictionaries for analytical data assetsAct as SME on analytical data sets, with ability to answer data scientist questions or find related information to support analytical modelingYou will assemble large, complex data sets that meet business requirementsYou will identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing processes for greater scalability, etc.Work with business partners on data-related technical issues and develop requirements to support their data infrastructure needsManage and support manual scoring of data as needed to support POCs and usage of advanced analytics models over timeLead contract resources to support data asset creation, reporting, and scoring is done with high quality and accuracyIdentify and report data quality issues as needed', 'Ability to effectively work with a team of data scientists and business stakeholders exchanging knowledge.Strong project leadership and management skills to effectively meet strategic and tactical goals.Excellent verbal and written communication skills, with ability to present to a target audience.Must be detail orientated/articulate with strong time management and organizational skillsMust be a self-starter.Must have strong drive for results.Willingness to learn new skills and methods as needed – continuous learning mindset.', 'Understand and profile data to assess usefulness to meet business objectives', '3+ years of hands on experience with Big Data Tools: Hadoop, Spark, Kafka, etc.', 'You will assemble large, complex data sets that meet business requirements', 'Benefits Overview', 'Excellent verbal and written communication skills, with ability to present to a target audience.', ' Global recognition program for peers and leaders to recognize and be recognized for results and behaviors that reflect our company values;', 'Evaluate business needs and identify and source data into analytical data sets needed to support advanced analytics models that will deliver actionable insights', 'Must have strong drive for results.', 'Combine data from different sources and transform data as needed', 'Bachelor’s degree in Computer Science, MIS, or related field.', ' Healthcare (including a triple tax advantaged health savings account and wellness incentive), dental, vision and life insurance plans to cover you and your family;', 'Job Description', 'Highly proficient with querying relational databases and/or Hadoop (expertise with SQL, Hive, Spark, Python, etc.).', 'Willingness to learn new skills and methods as needed – continuous learning mindset.', 'Bachelor’s degree in Computer Science, MIS, or related field.3+ years of experience as a data engineer or in a similar role3+ years of hands on experience with Big Data Tools: Hadoop, Spark, Kafka, etc.Highly proficient with querying relational databases and/or Hadoop (expertise with SQL, Hive, Spark, Python, etc.).Familiar with predictive / analytical modeling techniques, theories, principles, and practices.', 'Strong project leadership and management skills to effectively meet strategic and tactical goals.', ' Discount on GM vehicles for you, your family and friends.']",Not Applicable,Full-time,Information Technology,Automotive,2021-03-18 14:34:51
Data Engineer,Kinesso,"New York, United States",2 days ago,106 applicants,"['', 'Collaborate with data science and machine learning specialists to produce analytics tools that utilize the data pipeline and application infrastructure to provide actionable insights into customer acquisition and implement closed-loop optimization of media buying and customer acquisition systems.', 'Experience with cloud and container systems including AWS cloud services: EC2, EMR, RDS, S3, Redshift; Docker and Kubernetes', 'In pursuing its mission, the Research & Development Team constantly identifies new data sources from both inside and outside the existing business and incorporates them into local platforms for use in its projects.\xa0The R&D Data Engineer will be responsible for efficiently architecting, building, and maintaining (1) the data infrastructure to enable the R&D Team’s machine learning and data science specialists to build applications that use this data, and (2) the application infrastructure to optimize the R&D process and eventual handoff of R&D tools for full productization elsewhere in the company.\xa0', 'Research & Development Team’s', 'Key Responsibilities ', 'Ability to communicate, collaborate and work in ambiguous and unmapped contexts a necessity.', 'Architect application infrastructure to contain and support complex AI tools connecting reinforcement learning agents, deep neural networks, proprietary algorithms, sensitive data sources, and various external partners.', 'The Kinesso Research & Development Team’s mission is to produce novel and disruptive solutions in the media, advertising, and marketing technology space.\xa0We pursue this mission by identifying key industry opportunities and trends and designing new algorithmic tools using concepts from game theory, information theory, probability & statistics, reinforcement learning, nonlinear dynamics, and various other areas of mathematics, data science, and technology, to extract commercial value.\xa0At any given time, we will be developing two to four major projects with one- to two-year time horizons and a small number of faster-cycle projects with a more narrow focus.\xa0We are a small team whose existence consists mainly of pushing at high-speed down a path while simultaneously drawing the map for that path.\xa0', 'The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. They must be self-directed, comfortable working in the presence of ambiguity, and confident enough to respond to ambiguity by making a reasoned judgment, trying an optimal approach, evaluating the success or failure of that approach, and trying again if necessary.', 'Experience architecting fast-cycle development for speculative applications', 'Create and maintain optimal data pipeline architecture, assemble large, complex data sets that meet functional / non-functional business requirements.', 'R&D Data Engineer', 'Integrate R&D Team tools with internal and external platforms via API, FTP, and other means.', 'Experience designing, programming, testing, and maintaining reliable connections to RESTful APIs\xa0', 'Experience creating database stored procedures and functions', 'Experience with cloud and container systems including AWS cloud services: EC2, EMR, RDS, S3, Redshift; Docker and KubernetesExperience with object-oriented/object function scripting languages: Python required; C++ a bonus, etc.Experience designing, programming, testing, and maintaining reliable connections to RESTful APIs\xa0Experience creating database stored procedures and functionsExperience architecting fast-cycle development for speculative applicationsExperience with reinforcement learning environments a bonusAbility to communicate, collaborate and work in ambiguous and unmapped contexts a necessity.', 'Create and maintain optimal data pipeline architecture, assemble large, complex data sets that meet functional / non-functional business requirements.Architect application infrastructure to contain and support complex AI tools connecting reinforcement learning agents, deep neural networks, proprietary algorithms, sensitive data sources, and various external partners.Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Build and manage the infrastructure required for optimal automated extraction, transformation, and loading of data from a wide variety of data sources using SQL, Snowflake, and AWS ‘big data’ technologies, for use in speculative machine learning and AI applications.Collaborate with data science and machine learning specialists to produce analytics tools that utilize the data pipeline and application infrastructure to provide actionable insights into customer acquisition and implement closed-loop optimization of media buying and customer acquisition systems.Integrate R&D Team tools with internal and external platforms via API, FTP, and other means.Work with data science and machine learning/artificial intelligence experts to strive for greater functionality in our data systems.', 'Build and manage the infrastructure required for optimal automated extraction, transformation, and loading of data from a wide variety of data sources using SQL, Snowflake, and AWS ‘big data’ technologies, for use in speculative machine learning and AI applications.', 'Experience with reinforcement learning environments a bonus', 'Work with data science and machine learning/artificial intelligence experts to strive for greater functionality in our data systems.', 'Experience with object-oriented/object function scripting languages: Python required; C++ a bonus, etc.', '\xa0', 'Desired Skills & Experience ', 'Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.']",Mid-Senior level,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Data Engineer,Bounteous,"Wilmington, DE",3 weeks ago,26 applicants,"['', 'College degree in Computer Science, Data Science, Analytics or related field5+ years of experience building data pipelines and extracting, transforming, and loading marketing and customer data Strong SQL skills Proficient in at least one programming language such as Python, Java, ScalaExperience working with data warehouse solutions like Amazon Redshift, Google BigQuery, Snowflake, or similarExperience working with cloud technologies such as AWS, Google Cloud, Azure, or similar Proficient in code version control systems like GitStrong understanding of customer data platformsExposure to Spark, Hadoop, and other big data technologies is a plus', 'Must be legally eligible to work in Canada.', 'Be a platform expert in leading CDP solutions like AEP, TreasureData, RedPoint or similarDevelop a deep expertise in our client’s data infrastructure and partner with the respective teams Work with the Solution Architect and extract, transform and load marketing and customer data into the platform in an automated and scalable mannerBuild the unified customer profile Work with Consultants and Data Scientists to identify and write the necessary queries needed for segmentation, reporting, analysis, and ML models', '5+ years of experience building data pipelines and extracting, transforming, and loading marketing and customer data ', 'For Employment Opportunities Based In Canada', 'Work with the Solution Architect and extract, transform and load marketing and customer data into the platform in an automated and scalable manner', 'Work with Consultants and Data Scientists to identify and write the necessary queries needed for segmentation, reporting, analysis, and ML models', 'Proficient in code version control systems like Git', 'Experience working with data warehouse solutions like Amazon Redshift, Google BigQuery, Snowflake, or similar', 'Build the unified customer profile ', 'Experience working with cloud technologies such as AWS, Google Cloud, Azure, or similar ', 'Develop a deep expertise in our client’s data infrastructure and partner with the respective teams ', 'Data Engineer ', 'Proficient in at least one programming language such as Python, Java, Scala', 'Bounteous is an equal opportunity employer. We embrace diversity and are committed to creating an inclusive workplace. In accordance with the Ontario Human Rights Code and Accessibility for Ontarians with Disabilities Act, 2005, accommodation will be provided at any point throughout the hiring process, provided the candidate makes their accommodation needs known to Bounteous. We welcome applications from all qualified candidates. ', 'Role And Responsibilities', 'Strong SQL skills ', 'Strong understanding of customer data platforms', 'Be a platform expert in leading CDP solutions like AEP, TreasureData, RedPoint or similar', 'College degree in Computer Science, Data Science, Analytics or related field', 'Preferred Qualifications', 'Exposure to Spark, Hadoop, and other big data technologies is a plus']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Data Engineer-Data Platform,TikTok,"Mountain View, CA",2 weeks ago,124 applicants,"['', ""As a data engineer in the data platform team, you will have the opportunity to build, optimize and grow one of the largest data platforms in the world. You'll have the opportunity to gain hands-on experience on all kinds of systems in the data platform ecosystem. Your work will have a direct and huge impact on the company's core products as well as hundreds of millions of users."", 'BS or MS degree in Computer Science or related technical field or equivalent practical experience', 'Responsibilities', 'Design and implement reliable, scalable, robust and extensible big data systems that support core products and business', 'Experience with ETL(Extraction, Transformation & Loading) and architecting data systems', 'Passionate and self-motivated about technologies in the Big Data area', 'Establish solid design and best engineering practice for engineers as well as non-technical people', 'Experience in the Big Data technologies(Hadoop, M/R, Hive, Spark, Metastore, Presto, Flume, Kafka, ClickHouse, Flink etc.)', 'Qualifications', 'Design and build data transformations efficiently and reliably for different purposes (e.g. reporting, growth analysis, multi-dimensional analysis)Design and implement reliable, scalable, robust and extensible big data systems that support core products and businessEstablish solid design and best engineering practice for engineers as well as non-technical people', 'BS or MS degree in Computer Science or related technical field or equivalent practical experienceExperience in the Big Data technologies(Hadoop, M/R, Hive, Spark, Metastore, Presto, Flume, Kafka, ClickHouse, Flink etc.)Experience with performing data analysis, data ingestion and data integrationExperience with ETL(Extraction, Transformation & Loading) and architecting data systemsExperience with schema design, data modeling and SQL queriesPassionate and self-motivated about technologies in the Big Data area', 'TikTok is committed to providing reasonable accommodations during our recruitment process. If you need assistance or an accommodation, please reach out to us at usrc@tiktok.com', 'Experience with performing data analysis, data ingestion and data integration', 'Experience with schema design, data modeling and SQL queries', ""TikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We believe individuals shouldn't be disadvantaged because of their background or identity, but instead should be considered based on their strengths and experience. We are passionate about this and hope you are too."", 'TikTok is the leading destination for short-form mobile video. Our mission is to inspire creativity and bring joy. TikTok has global offices including Los Angeles, New York, London, Paris, Berlin, Dubai, Mumbai, Singapore, Jakarta, Seoul and Tokyo.', 'Design and build data transformations efficiently and reliably for different purposes (e.g. reporting, growth analysis, multi-dimensional analysis)', '\xa0']",Mid-Senior level,Full-time,Engineering,Internet,2021-03-18 14:34:51
Data Engineer,Amazon,"Seattle, WA",1 day ago,Be among the first 25 applicants,"['', ' Work closely with analytics and product leads to scale data', ' 5+ years of experience as a Data Engineer, BI Engineer or related field in a company with large, complex data sources.', ' Hands-on experience and advanced knowledge of SQL', ' Enable more efficient adhoc queries & analysis', ' Solid understanding of data design approaches (and how to best use them)', ' Explore and learn the latest AWS technologies to provide new capabilities and increase efficiencies', ' Raise the bar on the importance of data within the product and analytics teams', ' Experience working with AWS big data technologies (Redshift, S3, EMR)', ' Support the teams through our 3 year journey, helping design our future data architecture', ' Work closely with product and analytic leads to design and implement relevant monitoring and alerting', ' Data modeling to support realtime & batch monitoring & alerts, Enable more efficient adhoc queries & analysis Build data pipelines to feed machine learning models and decision engines for real-time and large-scale offline use cases Work closely with analytics and product leads to scale data Work closely with product and analytic leads to design and implement relevant monitoring and alerting Ensure consistency between various platform, operational, and analytic data sources to enable faster and more efficient detection and resolution of issues Explore and learn the latest AWS technologies to provide new capabilities and increase efficiencies Raise the bar on the importance of data within the product and analytics teams Support the teams through our 3 year journey, helping design our future data architecture', ' Data modeling to support realtime & batch monitoring & alerts,', 'This Individual Will Be Responsible For Driving', ' Masters in computer science, mathematics, statistics, economics, or other quantitative fields. 5+ years of experience as a Data Engineer, BI Engineer or related field in a company with large, complex data sources. Experience working with AWS big data technologies (Redshift, S3, EMR) Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy. Familiarity with data quality automation.', ' Ensure consistency between various platform, operational, and analytic data sources to enable faster and more efficient detection and resolution of issues', 'Description', "" Bachelor's degree in Computer Science, Engineering, Mathematics, or a related technical discipline 4+ years of industry experience in software development, data engineering, business intelligence, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets Hands-on experience and advanced knowledge of SQL Experience in Data Modeling, ETL Development, and Data Warehousing Solid understanding of data design approaches (and how to best use them) Knowledge of data management fundamentals and data storage principles Knowledge of distributed systems as it pertains to data storage and computing"", ' Build data pipelines to feed machine learning models and decision engines for real-time and large-scale offline use cases', "" Bachelor's degree in Computer Science, Engineering, Mathematics, or a related technical discipline"", ' 4+ years of industry experience in software development, data engineering, business intelligence, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets', 'Company', ' Knowledge of data management fundamentals and data storage principles', ' Masters in computer science, mathematics, statistics, economics, or other quantitative fields.', ' Experience in Data Modeling, ETL Development, and Data Warehousing', ' Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations', 'Basic Qualifications', ' Familiarity with data quality automation.', ' Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy.', ' Knowledge of distributed systems as it pertains to data storage and computing', 'Preferred Qualifications']",Not Applicable,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Junior Data Engineer,Mindtree,"Bethesda, MD",3 weeks ago,Over 200 applicants,"['', '\xa0Qualifications', 'Why Work with Us:', 'Good work-life balance encouraged', 'Bachelor’s degree in Computer Science, Information Systems, Economics, Engineering, or a comparable discipline', '\xa0Mindtree Equal Employment Opportunity Policy', 'Technical degree required; computer science or electronics engineering degree desired;', '\xa0Experience Required\xa0', 'Welcome to possible!', '\xa0About Mindtree\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0', 'Problem solving skills & learning attitude', 'Mindtree [NSE: MINDTREE] is a global technology consulting and services company, helping enterprises marry scale with agility to achieve competitive advantage. “Born digital,” in 1999 and now a Larsen & Toubro Group Company, Mindtree applies its deep domain knowledge to 300+ enterprise client engagements to break down silos, make sense of digital complexity and bring new initiatives to market faster. We enable IT to move at the speed of business, leveraging emerging technologies and the efficiencies of Continuous Delivery to spur business innovation. Operating in 18 countries and over 40 offices across the world, we’re consistently regarded as one of the best places to work, embodied every day by our winning culture made up of over 21,000 entrepreneurial, collaborative and dedicated “Mindtree Minds.”\xa0', 'Recognition is the cornerstone of our culture', 'Employment Type', 'High integrity', 'Values-driven and engaged leadership', 'Full-time', 'Industry', 'As a Big data Developer, you will be', 'Recognized for our employee learning and talent development practicesGood work-life balance encouragedRecognition is the cornerstone of our cultureValues-driven and engaged leadership', 'Welcome to possible', '\xa0Educational Qualifications', 'Soft skills', 'Able to deal with a diverse set of stakeholders and facilitate workshops', '·\xa0\xa0\xa0\xa0\xa0\xa0Min. 2+ yrs. experience in Core Java, Hibernate, Multithreading & Spark\xa0', 'Recognized for our employee learning and talent development practices', 'Responsibilities\xa0', '·\xa0\xa0\xa0\xa0\xa0\xa0Good knowledge of SQL Low level design, coding, unit test and functional test support\xa0', 'Mindtree provides equal employment opportunities to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics.', 'Mindtree is looking for Strong Big data Developer for its project in Bethesda, MD, Who is expert in Spark , Scala, Java', '·\xa0\xa0\xa0\xa0\xa0\xa0Good understanding of Spring (Core) framework', 'Proficient in articulation, communication and presentation', '·\xa0\xa0\xa0\xa0\xa0\xa0Must to have – min. 2yrs of Spark & Scala – Development experience', 'Able to deal with a diverse set of stakeholders and facilitate workshopsProficient in articulation, communication and presentationHigh integrityProblem solving skills & learning attitudeTeam player', '4+ years of industry experience', 'Team player', '·\xa0\xa0\xa0\xa0\xa0\xa0Has hands on exp on Hadoop (HDFS, HBase, Hive and Mapreduce)\xa0', 'Information Technology and Services']",Mid-Senior level,Full-time,Engineering,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Gusto,"Denver, CO",3 weeks ago,127 applicants,"['', 'Experience with systems for transforming large datasets such as Spark and Hadoop', 'Here’s What You’ll Do Day-to-day', 'About Gusto', 'Our Engineering Culture and Values', 'How We Built a Service-Driven Team', 'Here’s What We’re Looking For', 'Build and refine a fault-tolerant data ingestion pipeline into our data warehouse, while helping guide the decisions about the future of our data infrastructure', ' Be an owner as you architect, build, and refine our data infrastructure technologies, using our development workflow — the outcome of your work will drive decisions that affect billions of dollars of transactions Build and refine a fault-tolerant data ingestion pipeline into our data warehouse, while helping guide the decisions about the future of our data infrastructure Work with engineers and product managers to analyze edge cases, clear ambiguities, and plan for architectural scalability Write complex and efficient queries to transform raw data sources into easily accessible models for our teams (e.g., Product, Growth, Finance, Risk) Care deeply about helping millions of small business owners focus on what they love Be surrounded by people who are doing the best work of their lives and loving every moment of it ', 'Ability to turn vague requirements into clear deliverables with minimal guidance', 'Be surrounded by people who are doing the best work of their lives and loving every moment of it', 'Work with engineers and product managers to analyze edge cases, clear ambiguities, and plan for architectural scalability', ' At least 2 years of software engineering experience (Python, Ruby or Java). Passion for creating data infrastructure technologies from scratch using the right tools for the job Experience with AWS tools Experience building out data pipelines, efficient ETL design, implementation, and maintenance Experience building and maintaining a data warehouse in production environments Experience with systems for transforming large datasets such as Spark and Hadoop Ability to turn vague requirements into clear deliverables with minimal guidance ', 'Passion for creating data infrastructure technologies from scratch using the right tools for the job', 'Our Diversity Goals and Efforts', 'Experience with AWS tools', 'Learn More About The Team', 'Experience building out data pipelines, efficient ETL design, implementation, and maintenance', 'Experience building and maintaining a data warehouse in production environments', ' Our Engineering Culture and Values How We Built a Service-Driven Team Our Diversity Goals and Efforts ', 'Care deeply about helping millions of small business owners focus on what they love', 'Be an owner as you architect, build, and refine our data infrastructure technologies, using our development workflow — the outcome of your work will drive decisions that affect billions of dollars of transactions', 'Write complex and efficient queries to transform raw data sources into easily accessible models for our teams (e.g., Product, Growth, Finance, Risk)', 'At least 2 years of software engineering experience (Python, Ruby or Java).']",Entry level,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Engineer,Crystal Equation Corporation,"New York, NY",2 days ago,101 applicants,"['Work with data infrastructure to triage infra issues and drive to resolution', 'Optimize and maintain existing pipelines, ensuring that data arrives accurately and on-time', 'Experience with programming languages, Python', 'BS/BTech in Computer Science, Math or related field', 'Build and launch efficient & reliable data pipelines to move and transform data (both large and small amounts)', 'Create scripts to automate operational processes', 'Support on-call shift as needed to support the team', 'Build and ensure data tables can serve as the source of truth for various high-level priorities', '4+ experience (Oracle, MySQL) writing queries', 'Responsibilities:', 'Familiar with version control systems (git, mercurial, etc.)', '4+ custom ETL/data pipeline design, implementation and maintenance\xa0', 'Build visualization to drive business decisions', ""4+ years' experience in the data warehouse space"", 'Experience with large data sets, Hadoop, and data visualization tools', 'Build and launch efficient & reliable data pipelines to move and transform data (both large and small amounts)Optimize and maintain existing pipelines, ensuring that data arrives accurately and on-timeBuild and ensure data tables can serve as the source of truth for various high-level prioritiesCreate scripts to automate operational processesWork cross-functionally to define problem statements, collect data, and make recommendationsBuild data expertise and own data quality for allocated areas of ownershipDefine and manage SLA for all data sets in allocated areas of ownershipWork with data infrastructure to triage infra issues and drive to resolutionSupport on-call shift as needed to support the teamBuild visualization to drive business decisions', 'Define and manage SLA for all data sets in allocated areas of ownership', ""BS/BTech in Computer Science, Math or related field4+ years' experience in the data warehouse space4+ custom ETL/data pipeline design, implementation and maintenance\xa04+ of SQL (Oracle, Vertica, Hive, etc.) or relational database4+ experience (Oracle, MySQL) writing queriesExperience with programming languages, PythonExperience with data architecture, data modeling, schema design and software developmentExperience with large data sets, Hadoop, and data visualization toolsExperience initiating and driving projects, and communicating data warehouse plans to internal clients/stakeholdersFamiliar with version control systems (git, mercurial, etc.)"", '4+ of SQL (Oracle, Vertica, Hive, etc.) or relational database', 'Work cross-functionally to define problem statements, collect data, and make recommendations', 'Skills: Minimum Qualifications', 'Experience initiating and driving projects, and communicating data warehouse plans to internal clients/stakeholders', 'Experience with data architecture, data modeling, schema design and software development', '\xa0', 'Build data expertise and own data quality for allocated areas of ownership']",Entry level,Contract,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Epiq,"Kansas, United States",23 hours ago,Be among the first 25 applicants,"['', 'Provide documentation of solution approaches and processes', 'Learning Ability', 'Reporting Services (SSRS)Creating simple and complex reports in SSRS, including matrix / pivot outputs, charts and graphs, Drill Through and Drill Down', 'Unifying data from multiple sourcesCreating interactive dashboards and reportsProviding Analytics Solutions', 'Integrity', 'Team Player', 'Excellent communication skills; ability to express complex ideas in writing at appropriate level of detail for intended audience; fluent in English', 'Position requires no significant manual labor.', 'Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources (including SQL, Azure, and Power Platforms)', 'Extensive experience in SQL Server Business Intelligence Development Studio(BIDs):Integration Services (SSIS)Extracting, Transforming, and Loading (ETL) data from MS Excel, flat files, or other sources to SQLReporting Services (SSRS)Creating simple and complex reports in SSRS, including matrix / pivot outputs, charts and graphs, Drill Through and Drill Down', 'Expertise with Index management: creation, maintenance, and tuning', 'Identify, design, and implement internal process improvements: automation of manual processes, optimization of data delivery methods, re-design of infrastructure for greater scalability, etc.', 'Stored Procedures', 'Position does require moderate standing or sitting.', 'Analysis', 'Team player, willing to engage within and across the organization', 'Ability to support data needs of multiple initiatives, systems, and products concurrently in a fast-paced environment', '5+ years as a software developer and/or SQL developer designing and building software and database solutions using SQL Server 2008R2 or newerDemonstrated experience designing and developing medium to large-scale, mission-critical database solutionsThorough understanding of key performance indicators (KPI) reporting and data mining structuresExpert data modeler and proficient with modeling toolsExtensive knowledge of data normalization concepts, Relational Database Management Systems, data warehouse systems, and star schemasExpertise with Index management: creation, maintenance, and tuning', 'Adaptability', 'Assemble large, complex data sets that meet functional / non-functional business requirements', 'Creativity', 'Extensive experience in SQL Server Business Intelligence Development Studio(BIDs):Integration Services (SSIS)Extracting, Transforming, and Loading (ETL) data from MS Excel, flat files, or other sources to SQLReporting Services (SSRS)Creating simple and complex reports in SSRS, including matrix / pivot outputs, charts and graphs, Drill Through and Drill DownExtensive experience with T-SQL, including but not limited to:TroubleshootingTriggersStored ProceduresFunctionsViewsPerformance tuning', 'Thorough understanding of key performance indicators (KPI) reporting and data mining structures', 'Demonstrated experience designing and developing medium to large-scale, mission-critical database solutionsThorough understanding of key performance indicators (KPI) reporting and data mining structuresExpert data modeler and proficient with modeling toolsExtensive knowledge of data normalization concepts, Relational Database Management Systems, data warehouse systems, and star schemasExpertise with Index management: creation, maintenance, and tuning', 'Extensive knowledge of data normalization concepts, Relational Database Management Systems, data warehouse systems, and star schemas', 'Troubleshooting', 'Creating simple and complex reports in SSRS, including matrix / pivot outputs, charts and graphs, Drill Through and Drill Down', 'Extracting, Transforming, and Loading (ETL) data from MS Excel, flat files, or other sources to SQL', 'Attention to Detail', 'Verbal & Written Communication', 'It is Epiq’s policy to comply with all applicable equal employment opportunity laws by making all employment decisions without unlawful regard or consideration of any individual’s race, religion, ethnicity, color, sex, sexual orientation, gender identity or expressions, transgender status, sexual and other reproductive health decisions, marital status, age, national origin, genetic information, ancestry, citizenship, physical or mental disability, veteran or family status or any other basis protected by applicable national, federal, state, provincial or local law. Epiq’s policy prohibits unlawful discrimination based on any of these impermissible bases, as well as any bases or grounds protected by applicable law in each jurisdiction. In addition Epiq will take affirmative action for minorities, women, covered veterans and individuals with disabilities. If you need assistance or an accommodation during the application process because of a disability, it is available upon request. Epiq is pleased to provide such assistance and no applicant will be penalized as a result of such a request. Pursuant to relevant law, where applicable, Epiq will consider for employment qualified applicants with arrest and conviction records.', 'Physical Demands', 'Build analytics tools that utilize the data pipeline to provide actionable insights into business usage and needs, operational efficiency, and other key business performance metrics', 'Extensive experience with T-SQL, including but not limited to:TroubleshootingTriggersStored ProceduresFunctionsViewsPerformance tuning', 'Views', 'Microsoft Azure Administrator skills preferredExcellent communication skills; ability to express complex ideas in writing at appropriate level of detail for intended audience; fluent in EnglishTeam player, willing to engage within and across the organizationAbility to support data needs of multiple initiatives, systems, and products concurrently in a fast-paced environmentSelf-directed professional who seeks out opportunities and areas where improvements can be realizedPassionate about technology, automation, and eliminating human toil', 'Client Focus – ', 'Listening', 'Essential Job Responsibilities', 'Microsoft Azure Administrator skills preferred', 'Job Description:', 'Passionate about technology, automation, and eliminating human toil', 'Integrity – ', 'Integration Services (SSIS)Extracting, Transforming, and Loading (ETL) data from MS Excel, flat files, or other sources to SQLReporting Services (SSRS)Creating simple and complex reports in SSRS, including matrix / pivot outputs, charts and graphs, Drill Through and Drill Down', 'Position requires close vision.', 'TroubleshootingTriggersStored ProceduresFunctionsViewsPerformance tuning', 'Qualifications', 'Entrepreneurial Orientation', 'Development of Others', 'Performance tuning', 'Work Environment', 'Summary', 'Expert data modeler and proficient with modeling tools', 'Demonstrated experience designing and developing medium to large-scale, mission-critical database solutions', 'Functions', 'Unifying data from multiple sources', 'Integration Services (SSIS)Extracting, Transforming, and Loading (ETL) data from MS Excel, flat files, or other sources to SQL', 'BS Computer Science or related discipline, or equivalent industry experience5+ years as a software developer and/or SQL developer designing and building software and database solutions using SQL Server 2008R2 or newerDemonstrated experience designing and developing medium to large-scale, mission-critical database solutionsThorough understanding of key performance indicators (KPI) reporting and data mining structuresExpert data modeler and proficient with modeling toolsExtensive knowledge of data normalization concepts, Relational Database Management Systems, data warehouse systems, and star schemasExpertise with Index management: creation, maintenance, and tuning', 'Self-directed professional who seeks out opportunities and areas where improvements can be realized', 'Create and maintain optimal data architectureBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources (including SQL, Azure, and Power Platforms)Build analytics tools that utilize the data pipeline to provide actionable insights into business usage and needs, operational efficiency, and other key business performance metricsProvide documentation of solution approaches and processesServe as a Subject Matter Expert (SME) regarding data architecture, as well as proper data collection and storage methodsAssemble large, complex data sets that meet functional / non-functional business requirementsIdentify, design, and implement internal process improvements: automation of manual processes, optimization of data delivery methods, re-design of infrastructure for greater scalability, etc.', 'Position requires talking and listening to communications with clients and employees. ', 'BS Computer Science or related discipline, or equivalent industry experience', 'Triggers', '1+ years of Power BI experience including:Unifying data from multiple sourcesCreating interactive dashboards and reportsProviding Analytics Solutions', 'Providing Analytics Solutions', 'Competencies ', 'Energy', 'Creating interactive dashboards and reports', 'Create and maintain optimal data architecture', 'Serve as a Subject Matter Expert (SME) regarding data architecture, as well as proper data collection and storage methods', 'Decisiveness', 'Results-Driven', 'Position requires no significant manual labor.Position does require moderate standing or sitting.Position requires close vision.Position requires talking and listening to communications with clients and employees. ', 'Flexibility']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer - 100% REMOTE,Sonatype,"Boston, MA",3 days ago,Be among the first 25 applicants,"['', ""The opportunity to be part of an incredible, high-growth company, working on a team of experienced colleaguesCompetitive salary packageMedical/Dental/Vision benefitsPaid Parental LeavePaid Volunteer Time Off (VTO)Business casual dressFlexible work schedules that ensure time for you to be you2019 Best Places to Work Washington Post and Washingtonian2019 Wealthfront Top Career Launch CompanyEY Entrepreneur of the Year 2019Fast Company Top 50 Companies for InnovatorsGlassdoor ranking of 4.9Come see why we've won all of these awards"", 'Database and data manipulation skills working with relational or non-relational models.', 'Experience working in a highly distributed environment, using modern collaboration tools to facilitate team communication.', 'Required Skills And Experience', 'Business casual dress', 'Knowledge and experience with large scale data tools and techniques (i.e. MapReduce, Hadoop, Hive, Spark).', 'Deep software engineering experience; we primarily use Java.Database and data manipulation skills working with relational or non-relational models.Strong ability to select and integrate appropriate tools, frameworks, systems to build great solutions.Deep curiosity for how things work and desire to make them better.Currently reside in either Canada, Colombia, or the United States of America and are legally authorized to work without sponsorship in the corresponding country.', 'What We Offer', 'Glassdoor ranking of 4.9', 'Desired Skills And Experience', 'Knowledge and experience with AWS Big Data services (i.e. EMR, ElasticSearch).', 'The opportunity to be part of an incredible, high-growth company, working on a team of experienced colleagues', 'Knowledge and experience with non-relational databases (i.e. Hbase, MongoDB, Cassandra).', 'Competitive salary package', '2019 Wealthfront Top Career Launch Company', 'Fast Company Top 50 Companies for Innovators', '2019 Best Places to Work Washington Post and Washingtonian', 'Medical/Dental/Vision benefits', 'Strong ability to select and integrate appropriate tools, frameworks, systems to build great solutions.', 'Currently reside in either Canada, Colombia, or the United States of America and are legally authorized to work without sponsorship in the corresponding country.', ""Come see why we've won all of these awards"", 'EY Entrepreneur of the Year 2019', 'Paid Volunteer Time Off (VTO)', 'Deep curiosity for how things work and desire to make them better.', 'Paid Parental Leave', 'Degree in Computer Science, Engineering, or another quantitative field.', 'Flexible work schedules that ensure time for you to be you', 'Degree in Computer Science, Engineering, or another quantitative field.Knowledge and experience with non-relational databases (i.e. Hbase, MongoDB, Cassandra).Knowledge and experience with large scale data tools and techniques (i.e. MapReduce, Hadoop, Hive, Spark).Knowledge and experience with AWS Big Data services (i.e. EMR, ElasticSearch).Experience working in a highly distributed environment, using modern collaboration tools to facilitate team communication.', 'Deep software engineering experience; we primarily use Java.']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Kootenai Health,"Coeur d'Alene, ID",2 weeks ago,149 applicants,"['', 'Snowflake EDW experience recommended', 'Experience with SQL, SSIS, SQL Server, SQL Server Analysis Services (Tabular and Multidimensional), DataVault 2.0', 'Bachelor’s Degree in Computer Science, Engineering, Math or related field OR seven years of equivalent experience in a Data Warehousing/Analytics required', 'Requirements and Minimum Qualifications', 'Competitive wages and annual increases!', 'Must be able to maintain a sitting position', 'Contributes to the technical support of Clinical, Quality, and Financial Analytics departments', '28810', 'Implements and maintains ELT processes for source data', 'Analytical approach to problem solving; ability to use technology to solve business problems', 'An award winning and incentive-driven wellness program', 'Regular and predictable attendance is an essential job function', 'Experience with Python, Pandas, ScKit-learn, PySpark', 'Employer Benefits:', 'Experience with Azure Data Services, Azure Data Lake, Azure Databricks, Azure HD Insight', 'Medical, Dental, and Vision Benefits! 100% employer-paid health insurance premiums for full-time employees.A tuition assistance program! If you want to further your education, we’ll help you pay for it!Looking forward to retirement? We’ll help you with that! We offer a 457 retirement plan through Fidelity with a match of 3-6% based on years of service and defined contribution account which puts 2% of your annual income into a retirement account for you.Convenient Daycare near campus!Competitive wages and annual increases!An award winning and incentive-driven wellness programEmployee reward, recognition, and retention programs!Professional\xa0development and leadership training opportunities!', 'Bachelor’s Degree in Computer Science, Engineering, Math or related field OR seven years of equivalent experience in a Data Warehousing/Analytics requiredTwo years or more of experience in Azure Analytics, Data Warehousing, and/or Data Warehouse Automation experience requiredAnalytical approach to problem solving; ability to use technology to solve business problemsExperience with SQL, SSIS, SQL Server, SQL Server Analysis Services (Tabular and Multidimensional), DataVault 2.0Experience with Azure Data Services, Azure Data Lake, Azure Databricks, Azure HD InsightExperience with Python, Pandas, ScKit-learn, PySparkSnowflake EDW experience recommended', 'Responsibilities:', 'Typical equipment used in an office job', 'Must be able to lift and move up to 10 lbs', 'Requirements and Minimum Qualifications:', 'Two years or more of experience in Azure Analytics, Data Warehousing, and/or Data Warehouse Automation experience required', 'A tuition assistance program! If you want to further your education, we’ll help you pay for it!', 'Position Summary:', 'Working Conditions:', 'Works with the IT Data Services Team and Analytics team to prioritize and move data to execute the Data Warehouse strategy', 'Partners with Financial and Clinical Analytics departments to prioritize and move data into a reportable structure in the Enterprise Data Warehouse', 'Familiar with standard concepts, practices, and procedures within the field', 'Works with the IT Data Services Team and Analytics team to prioritize and move data to execute the Data Warehouse strategyImplements and maintains ELT processes for source dataContributes to the technical support of Clinical, Quality, and Financial Analytics departmentsPartners with Financial and Clinical Analytics departments to prioritize and move data into a reportable structure in the Enterprise Data WarehouseFamiliar with standard concepts, practices, and procedures within the fieldRegular and predictable attendance is an essential job functionPerforms other related duties as assignedCompetent to meet age specific needs of the unit assigned', 'Kootenai Health complies with applicable federal civil rights laws and does not discriminate on the basis of race, color, national origin, age, disability, veteran status, or sex. Kootenai Health does not exclude people or treat them differently because of race, color, national origin, age, disability, veteran status, or sex.', 'Data Engineer', 'Must be able to reach arms above or below shoulder height', 'The Data Engineer will work on various Big Data, Data Warehousing and Analytics projects. Assists in creating and maintaining the technical architecture of the Enterprise Data Warehouse to consolidate data from many systems into a single source for the analytics and reporting across Kootenai Health departments. Strong technical mindset, understanding of big data methodologies and business acumen required.', 'Must be able to lift and move up to 10 lbsMust be able to reach arms above or below shoulder heightMust be able to maintain a sitting positionTypical equipment used in an office jobRepetitive movements', 'Convenient Daycare near campus!', 'Performs other related duties as assigned', 'Competent to meet age specific needs of the unit assigned', 'Looking forward to retirement? We’ll help you with that! We offer a 457 retirement plan through Fidelity with a match of 3-6% based on years of service and defined contribution account which puts 2% of your annual income into a retirement account for you.', 'Employee reward, recognition, and retention programs!', 'Medical, Dental, and Vision Benefits! 100% employer-paid health insurance premiums for full-time employees.', 'Repetitive movements', 'Professional\xa0development and leadership training opportunities!']",Entry level,Full-time,Information Technology,Hospital & Health Care,2021-03-18 14:34:51
Data Engineer,N/A,"Austin, TX",21 hours ago,59 applicants,"['Function as the solution lead for building the data pipelines to support the development / enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into data warehouse, data marts and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.', 'Distributed data & compute (partitioning, indexes, access patterns)', 'Core Data Experience – Must show competency in core data & analytics conceptsData architecture experience – designing data lakes, AWS S3 bucket management, datawarehouse modeling (star schema)Hadoop / Big Data Ecosystem (HDFS/S3, Hive, Spark)High level of competency with SQL, including advanced conceptsBatch Data Movement – ETL (INFA experience is preferred, but having deep knowledge of concepts is mandatory)Distributed data & compute (partitioning, indexes, access patterns)Event-based application patterns & streaming data', 'Provide feedback and enhance Daman intellectual property related to data management technology deployments', 'Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions', 'Core Data Experience – Must show competency in core data & analytics concepts', 'Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging and integration. Develop overall design and determine division of labor across various architectural components', 'Responsibilities:', 'Batch Data Movement – ETL (INFA experience is preferred, but having deep knowledge of concepts is mandatory)', 'Deploy and customize Daman Standard Architecture components', 'Job Summary:', 'Assist in development of task plans including schedule and effort estimation', 'Function as the solution lead for building the data pipelines to support the development / enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into data warehouse, data marts and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging and integration. Develop overall design and determine division of labor across various architectural componentsDeploy and customize Daman Standard Architecture componentsMentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutionsProvide feedback and enhance Daman intellectual property related to data management technology deploymentsAssist in development of task plans including schedule and effort estimation', 'Hadoop / Big Data Ecosystem (HDFS/S3, Hive, Spark)', 'High level of competency with SQL, including advanced concepts', 'Required Skills;', 'Data architecture experience – designing data lakes, AWS S3 bucket management, datawarehouse modeling (star schema)', '\xa0', 'As part of Daman’s Data Engineering team, you will be architecting and delivering highly scalable, high performance data integration and transformation platforms. The solutions you will work on will include cloud, hybrid and legacy environments that will require a broad and deep stack of data engineering skills. You will be using core cloud data warehouse tools, hadoop, spark, events streaming platforms and other data management related technologies. You will also engage in requirements and solution concept development, requiring strong analytic and communication skills.', 'Event-based application patterns & streaming data']",Mid-Senior level,Contract,Finance,Financial Services,2021-03-18 14:34:51
"Software Engineer, Data Pipelines",GitHub,"Boston, MA",19 hours ago,Be among the first 25 applicants,"['', 'Preferred Qualifications:', 'You have operational experience with data pipelines and distributed systems', 'Why You Should Join:', ""Share responsibility for the availability and performance of our team's systems"", ""You're passionate about software design and have good testing skills"", 'Collaborate with teammates to design data pipelines and APIs, seeking and offering feedback along the way', 'You have experience with Kafka, Java and the JVMYou have DevOps experience\xa0', 'Build high-volume event collection, processing and storage systems', ""You're able to empathize with a diverse set of engineers"", 'Responsibilities:', ""Build high-volume event collection, processing and storage systemsWork with application engineers to build product features that use GitHub's dataCollaborate with teammates to design data pipelines and APIs, seeking and offering feedback along the wayDevelop foundational data infrastructure, enabling other teams to build data-centric featuresWork across many languages including Go, Java, Ruby and PythonShare responsibility for the availability and performance of our team's systems"", ""Work with application engineers to build product features that use GitHub's data"", 'You have DevOps experience\xa0', ""You've built software using several different programming languages"", 'Minimum Qualifications:\xa0', 'Work across many languages including Go, Java, Ruby and Python', ""You have strong written and verbal communication skillsYou've built software using several different programming languagesYou're passionate about software design and have good testing skillsYou have operational experience with data pipelines and distributed systemsYou're able to empathize with a diverse set of engineers"", 'Develop foundational data infrastructure, enabling other teams to build data-centric features', 'Who We Are:', 'You have experience with Kafka, Java and the JVM', 'You have strong written and verbal communication skills', 'Leadership Principles:']",Associate,Full-time,Engineering,Computer Software,2021-03-18 14:34:51
Data Engineer (Data Lakes),The Denzel Group,"Reading, PA",2 weeks ago,98 applicants,"['', 'Qualifications', 'Partner with others to collect and document solution requirements, data access, process automation, and performance.', 'Keywords', 'Partner with others to design, develop, deploy and maintain an enhanced data stack and data processing pipeline.', 'Experience developing with at least one programming language, such as Java, Python, etc.', 'Develop and integrate monitoring for the cloud platforms and data pipelines, to ensure data quality and availability.', 'Responsibilities', 'Establish foundation for future implementations and automation requirements for cloud environments.', '2+ years’ implementing Data Lake/Hadoop platforms.', 'This role can be largely remote, but you must be able to travel on-site roughly once a month to the Reading, PA Area.', '4+ years’ software engineering or data automation experience.', 'Develop new, comprehensive platforms.', 'Data Engineer, developer, programmer, data lake, Hadoop, Java, Python, SQL, AWS, cloud, CI/CD, Jenkins.', 'Keywords:', 'The Denzel Group, an award-winning search firm, has been chosen to work with an industry leader on their quest for a talented Data Engineer. In this role, you will work closely with Engineering and Analytics to develop a comprehensive data platform, and leverage data from ever increasing sources.', 'Develop new, comprehensive platforms.Partner with others to collect and document solution requirements, data access, process automation, and performance.Partner with others to design, develop, deploy and maintain an enhanced data stack and data processing pipeline.Establish foundation for future implementations and automation requirements for cloud environments.Develop and integrate monitoring for the cloud platforms and data pipelines, to ensure data quality and availability.', '4+ years’ software engineering or data automation experience.2+ years’ implementing Data Lake/Hadoop platforms.Experience developing with at least one programming language, such as Java, Python, etc.']",Mid-Senior level,Full-time,Information Technology,Gambling & Casinos,2021-03-18 14:34:51
Data Engineer,Equifax,"St Louis, MO",19 hours ago,Be among the first 25 applicants,"['', 'We offer excellent compensation packages with high-reaching market salaries, 401k matching, along with the works: comprehensive healthcare packages, schedule flexibility, collaborative work spaces, work from home opportunities, paid time off, and organizational growth potentialGrow at your own pace through online courses at Learning @ Equifax\xa0', 'Develop strategies, standards and best practices in the areas of data wrangling, data visualization and data integration and lead adoption throughout analytical platformsLead analysis of internal and external data assets in consultation with internal stakeholdersDevelop new processes to create metrics to track the ongoing quality across Equifax data assetsDesign new methodologies and execute investigative data analyses of moderate complexity in response to business request for internal and external customersDevelop and Collaborate extensively with Data, Analytics, and Technology leads to ensure the seamless consumption of insights generated from Equifax’s new big data analytical platform.Stay current with rapidly developing Big Data technologies landscape and share knowledge internally and with customers.\xa0 Prototype and integrate tools into Cambrian environment and lead adoption of tools through Data and Analytics.', 'Trust', 'You will be a part of a Data Engineering team that is at the center of driving enterprise level best practices on big data solutions. Through interactions with Data Stewards, Data Scientists, IT, and Analysts, the Data Engineer facilitates D&A’s cloud transformation utilizing GCP tools and data storage, and ultimately helps D&A’s transition from on-site analytics to the GCP.\xa0\xa0', '5+\xa0 years of work experience in building, loading, transforming and analyzing data within and across database platforms and other data stores is required.', 'At Equifax, we believe knowledge drives progress. As a global data, analytics and technology company, we play an essential role in the global economy by helping employers, employees, financial institutions and government agencies make critical decisions with greater confidence.\xa0', 'Decide-Execute-Ship', 'Bravery', 'We work to help create seamless and positive experiences during life’s pivotal moments: applying for jobs or a mortgage, financing an education or buying a car. Our impact is real and to accomplish our goals we focus on nurturing our people for career advancement and their learning and development, supporting our next generation of leaders, maintaining an inclusive and diverse work environment, and regularly engaging and recognizing our employees. Regardless of location or role, the individual and collective work of our employees makes a difference and we are looking for talented team players to join us as we help people live their financial best.', 'Accountability', 'Proficiency in SQL and data visualization techniques and tools (Tableau, Spotfire, Excel, etc)', 'Think and act differently', 'If this sounds like somewhere you want to work, don’t delay, apply today - we’re looking for you!', 'Collaboration', 'We offer excellent compensation packages with high-reaching market salaries, 401k matching, along with the works: comprehensive healthcare packages, schedule flexibility, collaborative work spaces, work from home opportunities, paid time off, and organizational growth potential', 'Primary Location:', 'Data and Analytics is a high-energy, fast-growing organization within Equifax charged with building and supporting best-in-industry data assets and analytical solutions. We deliver solutions and insights to enable success for our customers, drive growth for our business, and create value for our shareholders. At Workforce Solutions, divisional and enterprise data assets are combined, transformed, and delivered to our customers through many delivery channels. As a Data Engineer, you will be instrumental in designing and developing frameworks for supporting data solutions at scale with minimal friction moving from prototype to production.', 'Develop new processes to create metrics to track the ongoing quality across Equifax data assets', 'Ownership', 'Schedule:', 'Develop strategies, standards and best practices in the areas of data wrangling, data visualization and data integration and lead adoption throughout analytical platforms', 'You will become an expert in our technical environment as it relates to data delivery, design data structures and processes balancing immediate deliverables with future reusability, and evangelize best practices for data solution architecture. Being able to blend disparate technologies/platforms (BigQuery, DataFlow, GCP, Python, SAS, SQL, Hadoop, Spark, Kafka) into cohesive solutions is critical for success.', 'Design new methodologies and execute investigative data analyses of moderate complexity in response to business request for internal and external customers', 'We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.', 'Function:', 'Curiosity', 'Grow at your own pace through online courses at Learning @ Equifax\xa0', 'Lead analysis of internal and external data assets in consultation with internal stakeholders', 'Stay current with rapidly developing Big Data technologies landscape and share knowledge internally and with customers.\xa0 Prototype and integrate tools into Cambrian environment and lead adoption of tools through Data and Analytics.', '5+ years of scripting/coding experience required. Proficiency in one or more of the following languages: Python, R, SQL, SAS', 'Bachelor’s degree in Computer Science, Statistics, Economics, or equivalent quantitative field with heavy emphasis on programming required', 'AccountabilityBraveryCuriosityCollaborationThink and act differentlyTrustOwnershipDecide-Execute-Ship', '5+\xa0 years of work experience in building, loading, transforming and analyzing data within and across database platforms and other data stores is required.5+ years of scripting/coding experience required. Proficiency in one or more of the following languages: Python, R, SQL, SASProficiency in SQL and data visualization techniques and tools (Tableau, Spotfire, Excel, etc)Bachelor’s degree in Computer Science, Statistics, Economics, or equivalent quantitative field with heavy emphasis on programming required', 'Develop and Collaborate extensively with Data, Analytics, and Technology leads to ensure the seamless consumption of insights generated from Equifax’s new big data analytical platform.', 'The Data Engineer must be organized, detail-oriented, and a fast learner in order to work with all Equifax’s major data assets and a rapidly changing technology environment. The ability to work in a matrix environment to deliver complete solutions is required.']",Mid-Senior level,Full-time,Analyst,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,io173,"Boston, MA",2 days ago,102 applicants,"['', 'You will be required to work with clients, senior stakeholder and data owners and enjoy solving problems in a collaborative way.', 'Use state of the art technologies to acquire, ingest and transform big datasets', ""Partner with our clients, from data owners and users to C-level executives, to understand their needs and build impactful analytics solutionsDesign and build data pipelines to support data science projects following software engineering best practicesUse state of the art technologies to acquire, ingest and transform big datasetsMap data fields to hypothesis, curate, wrangle and prepare data to be used in advanced analytics modelsCreate and manage data environments in the cloud or on premiseEnsure information security standards are maintained at all timeContribute to cross-functional problem-solving sessions with your team and deliver presentations to colleagues and clientsBe flexible to travel to our clients' offices to deliver presentations, gather information or share knowledgeHave the opportunity to contribute to R&D and internal asset development projects"", ""Be flexible to travel to our clients' offices to deliver presentations, gather information or share knowledge"", 'Have the opportunity to contribute to R&D and internal asset development projects', 'Map data fields to hypothesis, curate, wrangle and prepare data to be used in advanced analytics models', 'You will be part of a Global Data Engineering community and you will work in cross-functional and Agile project teams alongside Project Managers, Data Scientists, other Data Engineers, Machine Learning Engineers and industry experts.', 'The ideal candidate should have experience using the following technologies:', 'On offer is the opportunity to work with a Global Top Tier Consulting organisation who are at the fore-front of the Artificial Intelligence and Data Science Markets.  Excellent compensation package is on offer for the right individual.', 'Contribute to cross-functional problem-solving sessions with your team and deliver presentations to colleagues and clients', 'Create and manage data environments in the cloud or on premise', ' Python, PySpark, SQL, Airflow, Databricks, container technologies such as Docker and Kubernetes, cloud solutions such as AWS, GCP or Azure and more', 'Duties and Responsibilities :', 'Ensure information security standards are maintained at all time', 'io173 are currently looking to recruit an experienced Data Engineer on behalf of our Global Consultancy Client.', 'Partner with our clients, from data owners and users to C-level executives, to understand their needs and build impactful analytics solutions', 'Design and build data pipelines to support data science projects following software engineering best practices', 'This role can be based in Boston, New York, Chicago or Montreal.']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Dr. Squatch,Los Angeles Metropolitan Area,,N/A,"['', 'You can get things done without perfect resources, are innovative and work with a sense of urgency', 'dbt', 'Dr. Squatch is looking for a talented Data Engineer to join our Data team. The Data Engineer will be responsible for ensuring high-quality, accurate data modeling (we use dbt), working with our outsourced data engineering team to ensure that our raw data is complete and accurate, and helping data analysts and others on the Data team transform raw data into clean, modeled data that is ready for analysis by end users. This role will have a high level of autonomy with the ability to change to the tools we use, alter the processes we maintain, and make other decisions related to data/analytics engineering that improve the capabilities and outputs of our team.', 'About You:', 'Fivetran, Stitch', 'Proficiency with a scripting language like Python (strongly preferred)', 'You work independently, ensuring work is completed on time regardless of the challenges that come up', 'You have high standards, take ownership of your work, and are invested in the outcome', 'We were recently listed as the 325th fastest growing company in the nation by Inc. Magazine', 'Expert SQL skills (required)', 'Experience working with cloud data warehouses (required)', 'Experience using dbt (strongly preferred)', 'Take ownership over our data architecture, managing, selecting, and improving the tools we use to store, load, transform, and visualize data', 'You proactively help others, stay positive and have a good sense of humor', 'The Opportunity:', 'Snowflake', 'Implement testing, validation, and documentation to flag and resolve issues with poor-quality data', 'This is a full-time role with company benefits based in Marina del Rey, California.', 'You enjoy working with data and applying the insights you find in it', 'Design, build and maintain scalable data models (in dbt) to power self-service business intelligence tools and support fast analysis by our analysts', 'Tools we currently use:', '2+ years of experience in a data/analytics engineering role', 'Optimize our data modeling layer (dbt), reducing runtime and decreasing unnecessary complexity', 'Take ownership over our data architecture, managing, selecting, and improving the tools we use to store, load, transform, and visualize dataOptimize our data modeling layer (dbt), reducing runtime and decreasing unnecessary complexityBe a champion for efficient, effective data modeling, reviewing pull requests, suggesting improvements, and helping/coaching others on the team to write better codeDesign, build and maintain scalable data models (in dbt) to power self-service business intelligence tools and support fast analysis by our analystsImplement testing, validation, and documentation to flag and resolve issues with poor-quality dataPartner with our data engineering team to improve the quality of data that we load to our warehouse via custom pipelines (e.g., data not loaded via Fivetran/Stitch)Proactively seek out and explore new technologies to advance our data capabilitiesArchitect and maintain pipelines for moving data into various third-party services, furthering our personalization capabilities', 'Play to Win', 'Responsibilities:', 'Preferred, but not required: Experience in ecommerce/direct to consumer businesses', 'Looker', 'SnowflakeFivetran, StitchdbtLooker', 'Interest in conducting reproducible analysis that is peer-reviewed, version-controlled, and well-documented', 'Qualifications:', 'Experience working with business intelligence solutions (required)', 'Partner with our data engineering team to improve the quality of data that we load to our warehouse via custom pipelines (e.g., data not loaded via Fivetran/Stitch)', 'Architect and maintain pipelines for moving data into various third-party services, furthering our personalization capabilities', 'The Company (drsquatch.com):', 'Enthusiasm for writing clean code', 'This role will report to the Director of Data & Analytics.', 'Team First Mentality', '2+ years of experience in a data/analytics engineering roleExpert SQL skills (required)Experience working with cloud data warehouses (required)Experience working with business intelligence solutions (required)Experience using dbt (strongly preferred)Proficiency with a scripting language like Python (strongly preferred)Enthusiasm for writing clean codeInterest in conducting reproducible analysis that is peer-reviewed, version-controlled, and well-documentedPreferred, but not required: Experience in ecommerce/direct to consumer businesses', 'You can get things done without perfect resources, are innovative and work with a sense of urgencyYou have high standards, take ownership of your work, and are invested in the outcomeYou proactively help others, stay positive and have a good sense of humorYou enjoy working with data and applying the insights you find in itYou work independently, ensuring work is completed on time regardless of the challenges that come up', ""Dr. Squatch is a high-growth startup changing the game in men's personal care through our all-natural products. We were recently listed as the 325th fastest growing company in the nation by Inc. Magazine and are looking to add talented and motivated people to our team! Our core values come naturally and make us a better, more whole and unique team. We are Scrappy - we get things done, we find a way, we act with urgency and we maintain a start-up mentality. We Play to Win - we have high standards, we encourage ownership of work, we are “hungry” and we invest in the outcome of our work. We have a Team First Mentality - we are humble, help others outside our own wheelhouse, stay positive and have fun. We offer a competitive salary in a growth focused & collaborative team environment. Perks include office gym and pool, snacks, unlimited PTO, and free soap. We're passionate about improving the lives of men and are looking for people who want to join us in our mission!"", 'Proactively seek out and explore new technologies to advance our data capabilities', 'Be a champion for efficient, effective data modeling, reviewing pull requests, suggesting improvements, and helping/coaching others on the team to write better code', 'We are Scrappy']",Associate,Full-time,Engineering,Consumer Goods,2021-03-18 14:34:51
Data Engineer,Atyeti Inc,"New York, United States",2 days ago,69 applicants,"['Atyeti Recognition:', '', 'Data Engineer for its team', 'Inc. 500 & 5000 Honoree Company for 2012,2013,2014,2015, 2016, and 2017Atyeti Ranks No. 270 on the 2012 Inc. 500 List2012,2016 and 2017 NJ 50 Fastest Growing Companies', '2012,2016 and 2017 NJ 50 Fastest Growing Companies', 'Excellent Python programming skills and experience delivering projects using PySpark is a must. Scala programming experience is nice to have.', 'Demonstrable experience designing and developing big data applications using Apache Spark and Airflow', 'Experience leveraging Databricks to develop Spark based applications along with very good understanding about data engineering capabilities provided by the service', '\ufeffJob Description', 'Experience using Cloud data warehouse technologies such as Snowflake is highly desired', 'Must have expertise on RDBMS solutions such as Oracle, SQL Server, Azure SQL DB etc..', 'Experience leveraging Azure Storage and Data Lake Store Gen 2 in Azure based big data projects along with\xa0integration to other Azure Data services', 'Atyeti Ranks No. 270 on the 2012 Inc. 500 List', 'in NYC ', 'Experience designing and developing Airflow DAGs, Operators etc…', 'Experience designing and developing real time data processing applications using Spark streaming, Kafka, Event Hub etc..', 'Global Investment Bank is looking for a Data Engineer for its team in NYC . Long term contract.', 'Demonstrable experience designing and developing big data applications using Apache Spark and AirflowExperience leveraging Databricks to develop Spark based applications along with very good understanding about data engineering capabilities provided by the serviceExcellent Python programming skills and experience delivering projects using PySpark is a must. Scala programming experience is nice to have.Experience designing and developing Airflow DAGs, Operators etc…Experience leveraging Azure Storage and Data Lake Store Gen 2 in Azure based big data projects along with\xa0integration to other Azure Data servicesExperience designing and developing real time data processing applications using Spark streaming, Kafka, Event Hub etc..Experience using Cloud data warehouse technologies such as Snowflake is highly desiredMust have expertise on RDBMS solutions such as Oracle, SQL Server, Azure SQL DB etc..', 'Inc. 500 & 5000 Honoree Company for 2012,2013,2014,2015, 2016, and 2017']",Associate,Contract,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer 2,PayPal,"Chandler, AZ",4 weeks ago,Be among the first 25 applicants,"['', 'Public Cloud (GCP) working knowledge is a plus', 'Hands on experience with engineering developer tools- GIT, Dockerization, Continuous Integration frameworks etc.', 'At least 2 years experience in large scale, production, server-side development on the JVM, Scala, Python and/or GO', 'Job Requirements', 'A passion for software engineering and for developing robust, scalable software systems', ' Design, coordinate and execute pilots, prototypes or proof of concepts to evaluate new technologies and frameworks', 'Experience with building RESTful API’s is a plus.', 'Hands on experience in DevOps practices – significant advantage.', 'A computer science graduate or undergraduate degree or equivalent with 4+ years of building large scalable and reliable enterprise technology platforms using Big Data open source technologies such as Hadoop, HBase, Spark, Kafka and Elastic Search / SolrAt least 2 years experience in large scale, production, server-side development on the JVM, Scala, Python and/or GOExperience with building RESTful API’s is a plus.Experience with building real time application is a plus.Hands on experience with engineering developer tools- GIT, Dockerization, Continuous Integration frameworks etc.A passion for software engineering and for developing robust, scalable software systemsPublic Cloud (GCP) working knowledge is a plusHands on experience in DevOps practices – significant advantage.Excellent verbal and written communication', 'A computer science graduate or undergraduate degree or equivalent with 4+ years of building large scalable and reliable enterprise technology platforms using Big Data open source technologies such as Hadoop, HBase, Spark, Kafka and Elastic Search / Solr', 'Job Description:', 'Experience with building real time application is a plus.', 'Job Description Summary:', 'Excellent verbal and written communication']",Not Applicable,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Engineer,Berkley Technology Services (a Berkley Company),"Wilmington, DE",1 week ago,Be among the first 25 applicants,"['', ' Assemble large, complex data sets that meet functional/non-functional business requirements. ', ' Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics. ', 'What We’ll Bring', ' A budget for continual improvement ', ' Knowledge of cloud based data warehousing products such as Snowflake', ' 1+ year of experience with object-oriented/object function scripting languages like Python. ', 'Responsibilities', ' Bachelor’s Degree ', ' A broad group of industry experts who work closely with us on everything we do ', ' 3+ years of experience with relational SQL databases. ', 'Qualifications', ' An engaged and supportive leadership team that will invest in you  Talented engineering teams to build products with  A broad group of industry experts who work closely with us on everything we do  A budget for continual improvement  Generous retirement plan  Excellent medical and dental insurance (and other health benefits) ', ' 3+ years of experience in a Data Engineer role ', ' Talented engineering teams to build products with ', ' Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery/ dataOps. ', ' 1+ year of experience working with or understanding formal ETL tools ', ' An engaged and supportive leadership team that will invest in you ', ' Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. ', 'What We’re Looking For', ' Excellent medical and dental insurance (and other health benefits) ', 'Company Details', ' Create and maintain optimal data pipeline architecture.  Assemble large, complex data sets that meet functional/non-functional business requirements.  Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery/ dataOps.  Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL.  Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.  Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. ', ' Create and maintain optimal data pipeline architecture. ', 'Skills You’ll Need', ' Bachelor’s Degree  3+ years of experience in a Data Engineer role  3+ years of experience with relational SQL databases.  1+ year of experience with object-oriented/object function scripting languages like Python.  1+ year of experience working with or understanding formal ETL tools  Knowledge of cloud based data warehousing products such as Snowflake', ' Generous retirement plan ', ' Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL. ']",Mid-Senior level,Full-time,Information Technology,Financial Services,2021-03-18 14:34:51
Data Engineer,Labcorp,"Tampa, FL",4 days ago,Be among the first 25 applicants,"['', '  Experience with Agile/Scrum methodologies and working as part of a cross-functional delivery team.  Experience with Azure Cloud service offerings such as: Azure Data Factory, Azure Databricks, Azure Functions, Azure Synapse.  Experience in the healthcare field.  ', ' Continuously fine tune and optimize data integration systems to promote stability and eliminate exceptions requiring human intervention.', 'What You Will Be Doing', ' 5+ years working in information systems development roles or proven technical skills coupled with a degree in Computer Science, Information Systems or similar field. ', ' 3+ years working in a data engineering capacity.', ' Quickly identify and resolve production issues related to data matching, ingestion merge or reporting export.', ' Experience in the healthcare field. ', ' Collaborate with application development team on database architecture and processes to support various LabCorp Employer Services internal and client facing applications.', ' Experience with Azure Cloud service offerings such as: Azure Data Factory, Azure Databricks, Azure Functions, Azure Synapse.', ' Highly proficient with SQL Server Integration Services, SQL Server Agent automation.', ' Solid understanding of flat file formats and file format conversion.', ' Ability to identify and diagnose ETL and database related issues, perform root cause analysis, and recommend corrective actions.', ' Build data systems and processes to enable easy troubleshooting and exception reconciliation by the Data Ops team.', ' Develop procedures to enhance reporting and query capabilities to improve efficiency and accuracy.', ' Ability to work with complex business requirements in developing SQL stored procedures.', ' Design and build data visualizations, interactive dashboards and report extracts to support the internal business and clients.', 'Requirements', ' Work with business and client liaisons to create standardized approaches to data sharing and integration.', ' Build automated data ingestion and ETL processes utilizing SSIS, T-SQL, SQL agent jobs, Python and Azure Cloud Services. ', '  Design and develop data report extracts, processes and comprehensive systems to support the needs of the business and functionality of web applications.  Build automated data ingestion and ETL processes utilizing SSIS, T-SQL, SQL agent jobs, Python and Azure Cloud Services.   Develop and maintain complex matching logic systems and stored procedures for merging inbound patient results data from flat files or staging tables to existing patient profile records.   Continuously fine tune and optimize data integration systems to promote stability and eliminate exceptions requiring human intervention.  Work with business and client liaisons to create standardized approaches to data sharing and integration.  Quickly identify and resolve production issues related to data matching, ingestion merge or reporting export.  Document data workflow diagrams for major services.  Build and manage monitoring systems and tests for production data quality.  Build data systems and processes to enable easy troubleshooting and exception reconciliation by the Data Ops team.  Develop procedures to enhance reporting and query capabilities to improve efficiency and accuracy.  Design and build data visualizations, interactive dashboards and report extracts to support the internal business and clients.  Collaborate with application development team on database architecture and processes to support various LabCorp Employer Services internal and client facing applications. ', ' Solid communication skills for documenting complex processes and oral communication skills in communicating with technical and non-technical employees.', ' Build and manage monitoring systems and tests for production data quality.', 'Location:', ' Exceptional skills in relational database systems - MS SQL Server and Azure SQL Server.', ' Develop and maintain complex matching logic systems and stored procedures for merging inbound patient results data from flat files or staging tables to existing patient profile records. ', '  5+ years working in information systems development roles or proven technical skills coupled with a degree in Computer Science, Information Systems or similar field.   3+ years working in a data engineering capacity.  Exceptional skills in relational database systems - MS SQL Server and Azure SQL Server.  T-SQL, dynamic SQL, scripting, reporting services and Integration Services, flat file automation (pull, post).  Highly proficient with SQL Server Integration Services, SQL Server Agent automation.  Ability to identify and diagnose ETL and database related issues, perform root cause analysis, and recommend corrective actions.  Python experience in the context of data orchestration, automation and analysis.  Data visualization design and development capability in Tableau or other tools/languages.  Ability to work with complex business requirements in developing SQL stored procedures.  Solid understanding of flat file formats and file format conversion.  Understanding of EDI file formats.  Solid communication skills for documenting complex processes and oral communication skills in communicating with technical and non-technical employees.  Data workflow mapping in Visio or other similar tool. ', 'Your Background and Experience ', ' T-SQL, dynamic SQL, scripting, reporting services and Integration Services, flat file automation (pull, post).', 'Role Description', ' Data visualization design and development capability in Tableau or other tools/languages.', 'Strongly Preferred', ' Experience with Agile/Scrum methodologies and working as part of a cross-functional delivery team.', ' Data workflow mapping in Visio or other similar tool.', ' Understanding of EDI file formats.', 'Department', ' Design and develop data report extracts, processes and comprehensive systems to support the needs of the business and functionality of web applications.', ' Document data workflow diagrams for major services.', ' Python experience in the context of data orchestration, automation and analysis.', 'Job Title']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,W. R. Berkley Corporation,"Wilmington, DE",3 weeks ago,Be among the first 25 applicants,"['', 'W. R. Berkley Corporation', 'Special Skills', ' Three years of experience as a data engineer or software developer. ', 'Berkley Regional Marketplace', 'Responsibilities', 'Experience Requirements', 'Qualifications', ' Use Airflow to construct and schedule data pipelines. ', ' Play an integral role in laying the foundation for the creation of a data-driven organization.  Work with Hive to manage, store, and retrieve data. Procure, clean, and store data from external sources.  Prepare reports, dashboards, and analysis based on data.  Pull together a variety of data sources for use in analytics.  Support the business and management with clear and insightful analyses of data.  Use data science techniques and machine learning models such as scikit-learn package.  Use Airflow to construct and schedule data pipelines.  Use big data technologies such as Hadoop and Spark.  Use Python to access and clean data.  Leverage knowledge of ETL tools and SQL. ', ' Use data science techniques and machine learning models such as scikit-learn package. ', ' Pull together a variety of data sources for use in analytics. ', ' Leverage knowledge of ETL tools and SQL. ', ' Play an integral role in laying the foundation for the creation of a data-driven organization. ', ' Master’s degree in computer science, mathematics, statistics, data science, or engineering. ', ' Education requirements: ', 'Company Details', ' Work with Hive to manage, store, and retrieve data. Procure, clean, and store data from external sources. ', ' Use big data technologies such as Hadoop and Spark. ', ' Support the business and management with clear and insightful analyses of data. ', ' Use Python to access and clean data. ', ' Prepare reports, dashboards, and analysis based on data. ']",Mid-Senior level,Full-time,Information Technology,Financial Services,2021-03-18 14:34:51
Data Engineer - Junior Level,"Community Tax, LLC","Chicago, IL",1 day ago,Be among the first 25 applicants,"['', 'Clean data and build business rules to ensure proper trigger functions within the newly implemented CRM', 'Convey your work and results to a wide variety of internal and external stakeholders - nerds and non-nerds', 'Intermediate to advanced proficiency with SQL and Python', 'Diagnose data-related bugs and ensure they are resolved in a timely manner', 'Strong relevant academic foundation with related internship, capstone or freelance gig experience', 'Comfort in a complex data environment and understanding of data structures', 'Be a passionate problem solver - breaking down problems and developing analytical insights', 'Our Mission', 'Evaluate operations for inefficiencies and identify areas where you can create, automate, and develop tools (SQL-based or otherwise)', 'A love for insights, innate curiosity, and a deep desire to find ways to create value', ""The business sense to understand why you're pulling data and whether it seems accurate...not just pulling it"", 'You like to formulate the right questions versus just seeking answers to make sure the best and smartest decisions get made', 'Support the business with ad hoc reporting', 'Continuously strive for a deeper understanding of our business drivers', 'Solid communication skills', 'Familiarity with transactional and data warehouse environments', ""Strong relevant academic foundation with related internship, capstone or freelance gig experienceEducation in Business information systems or related technical field or equivalent work experienceExperience supporting business operations in an analytics capacityA love for insights, innate curiosity, and a deep desire to find ways to create valueSolid communication skillsPassion for automationComfort in a complex data environment and understanding of data structuresIntermediate to advanced proficiency with SQL and PythonAdvanced proficiency with ExcelExperience with some flavor of data visualizationSome experience and proficiency with Power BIFamiliarity with transactional and data warehouse environmentsThe business sense to understand why you're pulling data and whether it seems accurate...not just pulling itYou like to formulate the right questions versus just seeking answers to make sure the best and smartest decisions get made"", 'Offer insight to all aspects of the organization - Sales, marketing, servicing and finance', 'Advanced proficiency with Excel', 'Experience supporting business operations in an analytics capacity', 'Some experience and proficiency with Power BI', 'Build and flesh out data models in our database for use in regular and/or automated reporting and analysis', 'What You Will Learn To Do', 'Work with a variety of data sources - extracting knowledge and actionable information from massive datasets', 'Wrangle and Scrub data through SQL and Python, to produce clean accurate data utilized in various projects.', 'Education in Business information systems or related technical field or equivalent work experience', 'Experience with some flavor of data visualization', 'Passion for automation', 'Work with a variety of data sources - extracting knowledge and actionable information from massive datasetsBuild and flesh out data models in our database for use in regular and/or automated reporting and analysisWrangle and Scrub data through SQL and Python, to produce clean accurate data utilized in various projects.Clean data and build business rules to ensure proper trigger functions within the newly implemented CRMBuild, monitor and maintain reliable data pipelines for highly available reporting or data integrations.Be a passionate problem solver - breaking down problems and developing analytical insightsConvey your work and results to a wide variety of internal and external stakeholders - nerds and non-nerdsEvaluate operations for inefficiencies and identify areas where you can create, automate, and develop tools (SQL-based or otherwise)Diagnose data-related bugs and ensure they are resolved in a timely mannerSupport the business with ad hoc reportingContinuously strive for a deeper understanding of our business driversOffer insight to all aspects of the organization - Sales, marketing, servicing and finance', 'Build, monitor and maintain reliable data pipelines for highly available reporting or data integrations.', 'What You Bring To The Table']",Associate,Internship,Information Technology,Insurance,2021-03-18 14:34:51
Data Engineer Intermediate (work from home Mid-Atlantic US resident),Geisinger,"Harrisburg, PA",2 days ago,Be among the first 25 applicants,"['', 'Collaborates and participates in the design and implementation of various projects.', 'Builds data ingestion pipelines for the Big Data Hadoop environment.Programming for a Big Data distributed computing environment using Java, Scala or similar object oriented programming languages.Programming data processing and integration algorithms on the Apache Hadoop, HBase stack.Coordinates projects and responsible for timely and accurate execution.Collaborates and participates in the design and implementation of various projects.Involves high-level participation in the design and management of a computational infrastructure for different purposes including applied medical research.Collaborates with other technology team members, clinicians and researchers on projects requiring data and analytic services.Works closely with data architects to define and execute an enterprise data architecture for complex healthcare data flows.Writes code for parallel computing.Develops new programs and responsible for moving existing code to high performance distributed systems code.Responsible to document all changes completed on the system within designated timeframes.Responsible for following department coding/programming guidelines to produce efficient routines.Provides preliminary code review, testing, debugging, and general testing instructions.', 'Provides preliminary code review, testing, debugging, and general testing instructions.', 'Responsible for following department coding/programming guidelines to produce efficient routines.', 'Collaborates with other technology team members, clinicians and researchers on projects requiring data and analytic services.', 'Develops new programs and responsible for moving existing code to high performance distributed systems code.', 'Works closely with data architects to define and execute an enterprise data architecture for complex healthcare data flows.', 'Programming for a Big Data distributed computing environment using Java, Scala or similar object oriented programming languages.', 'Writes code for parallel computing.', 'Job Summary', 'Programming data processing and integration algorithms on the Apache Hadoop, HBase stack.', 'Coordinates projects and responsible for timely and accurate execution.', 'Builds data ingestion pipelines for the Big Data Hadoop environment.', 'Job Duties', 'Involves high-level participation in the design and management of a computational infrastructure for different purposes including applied medical research.', 'Responsible to document all changes completed on the system within designated timeframes.', 'Experience']",Entry level,Full-time,Information Technology,"Health, Wellness and Fitness",2021-03-18 14:34:51
Data Engineer,Research Innovations Incorporated,"Alexandria, VA",1 day ago,Be among the first 25 applicants,"['', 'Experience with Agile Methodologies and supporting technologies enabled by Atlassian products', 'Experience in understanding and decomposing system level requirements into discrete and measurable tasks', 'Experience with other ETL and adjacent technologies such as Apache Beam, Apache Spark, Apache Kafka, Apache Storm, and/or Logstash', 'It’s also why, during the current pandemic, most of us aren’t actually in our lovely office. We’re all working fully remote where possible, with safety controls for those who do have to be on site. RII is committed to not overwhelming our healthcare system, preventing infection for those most at risk, reducing the impact on our communities, and keeping our employees working.', 'Experience with data quality and data profiling tools', 'Experience with integration methodologies and tools for Big Data applications and services', 'EVEN BETTER', 'Develop extensible and scalable solutions by collaborating with customers and the broader RII development team', 'Experience working with REST APIs', 'Experience with runtime configuration management and automated deployment (using tools such as Ansible or Puppet)', 'Experience with messaging queue implementations supporting AQMP (e.g., RabbitMQ)', 'BS in Computer Science, equivalent degree, or previous work experienceDeveloped scalable ingest/ETL pipelines using Apache NiFiExperience in developing and deploying Java software architecturesExcellent general understanding of Linux operating systems, distributed systems, microservices, and database technologiesExperience working with REST APIsExperience with runtime configuration management and automated deployment (using tools such as Ansible or Puppet)Excellent general understanding data analytics platformsExperience in understanding and decomposing system level requirements into discrete and measurable tasks', 'Research Innovations, Inc. is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender, gender identity or expression, national, origin, genetics, disability status, protected veteran status, age, or any other characteristic protected by state, federal or local law.', 'At RII, we believe that diversity in our workforce is critical to our success. We strive to hire great people from a wide variety of backgrounds, not just because it’s the right thing to do, but because it makes us stronger. We work to help your intellectual passions and creativity thrive. It’s one of our core values:\xa0Let your geek flag fly.', 'Create and maintain automated testing capabilities for our ETL framework and components', 'Establish, maintain, and enhance an extensible ETL architecture in support of several customer projects', 'We also offer all employees comprehensive benefits including: flexible work schedules, health insurance coverage, paid time off, 401k with a company match, paid parental leave, access to wellness programs and much more. You get this all from day one, and all 100% paid for by RII.', 'Developed scalable ingest/ETL pipelines using Apache NiFi', 'Experience with Python', 'WHAT YOU WILL BE DOING', 'Excellent general understanding data analytics platforms', 'Experience with Jolt transforms', 'Experience with Continuous Integration and Continuous Deployment concepts', 'Research Innovations, Inc. (RII) is breaking through the big, slow, status quo with transformative technology that fundamentally changes and improves the world. We develop cutting-edge software for all levels of the government and military. Using agile development practices and user-centered design, we create innovative software solutions for complex real-world problems.', 'Maintain awareness of the current and emerging capabilities in ETL technologies and how these apply to our customer challenges', 'BS in Computer Science, equivalent degree, or previous work experience', '\xa0It’s all part of another of our core values:\xa0Stay human.\xa0It’s why our comfortable and colorful offices include a community game room, pantry, massage chair, and an escape room, among other amenities. It’s why we have a community manager and regular community events.', 'If you are a sharp engineer with demonstrated capabilities in designing and implementing scalable solutions using extract, transform, and load technologies we want to hear from you.', 'This position requires the ability to obtain a US Secret Clearance. Active clearance not required to apply.', 'Experience with client-side development including JavaScript, HTML5 and Angular', 'Strong knowledge of installation, configuration, and maintenance of cloud computing and Big Data infrastructure to include Hadoop, Accumulo, Mongo, Kafka, Spark, Elasticsearch, Puppet, Ansible, and related technologies', 'Document use cases, solutions, & recommendations for our customers', 'Establish, maintain, and enhance an extensible ETL architecture in support of several customer projectsBe an engineering point-person for ingest capabilities and the interfaces of external systems we ingest data fromMaintain awareness of the current and emerging capabilities in ETL technologies and how these apply to our customer challengesDevelop extensible and scalable solutions by collaborating with customers and the broader RII development teamCreate and maintain automated testing capabilities for our ETL framework and componentsDocument use cases, solutions, & recommendations for our customers', 'WHAT YOU HAVE DONE', 'Experience in developing and deploying Java software architectures', 'Experience leading a team of developers', 'MS in Computer Science, equivalent degree, or work experience', 'Excellent general understanding of Linux operating systems, distributed systems, microservices, and database technologies', 'We are looking for a committed Data Engineer to join our External Systems Interfaces team. Solve unique, challenging problems for our Defense and Homeland Security customers. Help develop the ETL systems that ingest large volumes of critical, real-time data that enables Big Data analytics and provides greater situational awareness and context to our users.\xa0Get s#it done.', 'Be an engineering point-person for ingest capabilities and the interfaces of external systems we ingest data from', 'MS in Computer Science, equivalent degree, or work experienceExperience with Agile Methodologies and supporting technologies enabled by Atlassian productsExperience with other ETL and adjacent technologies such as Apache Beam, Apache Spark, Apache Kafka, Apache Storm, and/or LogstashExperience with Jolt transformsExperience with messaging queue implementations supporting AQMP (e.g., RabbitMQ)Experience with PythonExperience with Continuous Integration and Continuous Deployment conceptsExperience with data quality and data profiling toolsStrong knowledge of installation, configuration, and maintenance of cloud computing and Big Data infrastructure to include Hadoop, Accumulo, Mongo, Kafka, Spark, Elasticsearch, Puppet, Ansible, and related technologiesExperience with the BigTable family of columnar stores (HBase, Cassandra, Accumulo)Experience with integration methodologies and tools for Big Data applications and servicesExperience with client-side development including JavaScript, HTML5 and AngularExperience leading a team of developers', 'Experience with the BigTable family of columnar stores (HBase, Cassandra, Accumulo)']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Seismic,"San Diego, CA",2 days ago,58 applicants,"['', 'What You Will Be Doing', 'Evaluate stakeholder requirements for upcoming projects and assist with project scope from a technical perspective', 'Catered meals, happy hours, healthy snacks, and coffee bar', 'What You Possess', 'Competitive Medical, Dental and Vision Plans', 'Bachelor of Arts/Science degree from an accredited university in the field of computer science, MIS or comparable related technical discipline', 'Seismic Cares volunteer program', ' Provide architecture designs to store, process and publish data at every step of the data pipeline from ingestion from various data sources to designing data warehouses Play a direct role in the ETL process to model datasets to be used in enterprise-wide BI reporting, often using new data sources Evaluate stakeholder requirements for upcoming projects and assist with project scope from a technical perspective Document processes, architecture of systems/data flows, project plans, using the agile methodology Advise leaders, analysts, and developers with the intention of furthering data quality, standards, best practices and uses. ', ' Generous PTO, paid holidays, and paid sick leave Competitive Medical, Dental and Vision Plans Robust 401(k) fund options with company matching Catered meals, happy hours, healthy snacks, and coffee bar Seismic Cares volunteer program #OneSeismic culture that celebrates wins, encourages autonomy, ownership, and transparency ', 'Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities and activities may change at any time with or without notice.', 'Experience working with a variety of file formats such as JSON, CSV and Parquet required', 'DBT experience a plus', 'Play a direct role in the ETL process to model datasets to be used in enterprise-wide BI reporting, often using new data sources', 'Generous PTO, paid holidays, and paid sick leave', '1-2 years of hands-on experience with database development, modeling and governance within a RDBMS environment required.', 'Provide architecture designs to store, process and publish data at every step of the data pipeline from ingestion from various data sources to designing data warehouses', 'Advise leaders, analysts, and developers with the intention of furthering data quality, standards, best practices and uses.', 'Document processes, architecture of systems/data flows, project plans, using the agile methodology', 'Snowflake, Google BigQuery, or other data warehouse system experience required.', 'Excellent command of SQL programming skills required.', '#OneSeismic culture that celebrates wins, encourages autonomy, ownership, and transparency', ' Bachelor of Arts/Science degree from an accredited university in the field of computer science, MIS or comparable related technical discipline 1-2 years of hands-on experience with database development, modeling and governance within a RDBMS environment required. Snowflake, Google BigQuery, or other data warehouse system experience required. Excellent command of SQL programming skills required. Demonstrable understanding of coding and scripting languages - Java, Python, JavaScript, etc Experience working with a variety of file formats such as JSON, CSV and Parquet required Experience with cloud services such as aws, azure and gcp a plus. DBT experience a plus ', 'Robust 401(k) fund options with company matching', 'What We Have For You', 'Demonstrable understanding of coding and scripting languages - Java, Python, JavaScript, etc', 'Experience with cloud services such as aws, azure and gcp a plus.']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Data Engineer (Remote),CUNA Mutual Group,"Remote, OR",22 hours ago,Be among the first 25 applicants,"['', ""Bachelor's degree in computer science or related field"", 'Knowledge of how data entities and elements should be structured to assure accuracy, performance, understanding, operational, analytical, reporting, and data science efficiencies', 'Analyze new (possibly unstructured) data sources to determine what additional value they may bring and how to effectively make use of them', 'Experience with Databricks and Snowflake', 'Perform exploratory data analyses and provide guidance for use (both for marketing analysis and data science)Partner with data scientists on both model rebuilds and new model developmentAnalyze new (possibly unstructured) data sources to determine what additional value they may bring and how to effectively make use of themReview developed solutions to solve specific business problemsInvestigate, analyze and recommend data formats and structureKnowledge of how data entities and elements should be structured to assure accuracy, performance, understanding, operational, analytical, reporting, and data science efficienciesThought partner in prototyping emerging self-service technologies involving data ingestion and transformation, distributed file systems, databases and frameworksWork on cross-functional teams to design and test data-driven applications and products', 'Practical experience consuming web services', 'Communicates difficult concepts and negotiates with others to persuade a different point of view', ""Bachelor's degree in computer science or related field6-8 years of experience technology related workIn-depth knowledge of SQL, SAS, R, and Python programming languages, and extensive hands-on data engineering experience. Experience with SparkR, Hadoop, Tableau, and graph databases is helpful.Experience with Databricks and SnowflakeCapable of authoring robust, high quality, reusable code and contributing to the division's inventory of librariesExpertise in big data batch computing tools, with demonstrated experience developing distributed data processing solutionsApplied knowledge of cloud computingApplied knowledge of data modeling principles (e.g. dimensional modeling and star schemas)Strong understanding of database internals, such as indexes, binary logging, and transactionsExperience with software engineering tools and workflows (e.g. Jenkins, CI/CD, git)Practical experience consuming web servicesSolid data understanding and business acumen in the data rich industries like insurance or financial"", 'Strong understanding of database internals, such as indexes, binary logging, and transactions', 'Applied knowledge of cloud computing', 'Job Responsibilities', 'Keeps up to date on industry, competitor and market trends', 'Thought partner in prototyping emerging self-service technologies involving data ingestion and transformation, distributed file systems, databases and frameworks', 'In-depth knowledge of SQL, SAS, R, and Python programming languages, and extensive hands-on data engineering experience. Experience with SparkR, Hadoop, Tableau, and graph databases is helpful.', 'Job Requirements', 'Communicates complex ideas, anticipates potential objections and persuades others, often at senior levels, to adopt a different point of viewKeeps up to date on industry, competitor and market trendsCommunicates difficult concepts and negotiates with others to persuade a different point of viewActively solicits and listens to feedback to determine need for change or improvementThought partner for development teams in applying data modeling techniques and the usage of data modeling and repository tools in consumer data domain', 'Experience with software engineering tools and workflows (e.g. Jenkins, CI/CD, git)', 'Job Purpose', 'Actively solicits and listens to feedback to determine need for change or improvement', 'Please provide your Work Experience and Education or attach a copy of your resume. Applications received without this information may be removed from consideration.', 'Work on cross-functional teams to design and test data-driven applications and products', 'Partner with data scientists on both model rebuilds and new model development', 'Applied knowledge of data modeling principles (e.g. dimensional modeling and star schemas)', 'Investigate, analyze and recommend data formats and structure', 'Communicates complex ideas, anticipates potential objections and persuades others, often at senior levels, to adopt a different point of view', 'Solid data understanding and business acumen in the data rich industries like insurance or financial', 'Perform exploratory data analyses and provide guidance for use (both for marketing analysis and data science)', 'Thought partner for development teams in applying data modeling techniques and the usage of data modeling and repository tools in consumer data domain', 'Expertise in big data batch computing tools, with demonstrated experience developing distributed data processing solutions', ""Capable of authoring robust, high quality, reusable code and contributing to the division's inventory of libraries"", 'Review developed solutions to solve specific business problems', '6-8 years of experience technology related work']",Not Applicable,Full-time,Information Technology,Financial Services,2021-03-18 14:34:51
Data Engineer,Compri Consulting,Denver Metropolitan Area,,N/A,"['', '-Salesforce.', '-ETL / ELT processes.', '-Senior T-SQL coding.', '-MS Dynamics', '\xa0', 'Desired:', 'Client located in Lakewood, Colorado is seeking a Data Engineer for a direct hire position.\xa0Responsibilities include data architecture, data ingestion, data processing, data presentation and database administration.', '-Azure SQL Server database administration (production and development).', '-Computer Science degree.', 'Required:']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,PRI Global,United States,2 days ago,72 applicants,"['', 'Ability to develop in multiple programming and scripting languages', 'Data Engineer', 'Required Skills:', 'SQL (Oracle, MSSQL, MySQL, etc) and NoSQL (Mongo)', 'Strong understanding of data modeling, algorithms, and data transformation techniques', '3+ years of experience in Computer Engineering, Software Development', '\xa0', '100% Remote', 'Education Requirement:', 'Direct Client', 'Healthcare is Mandatory ', 'Scripting languages such as Python', 'SQL (Oracle, MSSQL, MySQL, etc) and NoSQL (Mongo)Scripting languages such as PythonStrong understanding of data modeling, algorithms, and data transformation techniques3+ years of experience in Computer Engineering, Software DevelopmentHands-on experience with ETL tools & automationHealthcare is Mandatory Ability to develop in multiple programming and scripting languages', 'Responsibilities:', 'Hands-on experience with ETL tools & automation', ""Bachelor's degree in Computer Science, Computer Engineering, Software Engineering, related field or equivalent experience""]",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Analytic Era,"Phoenix, AZ",2 days ago,110 applicants,"['', 'Gain an understanding of and continue to support data analytics policies and procedures', 'Microsoft Access experience is preferred', 'Analyze data to identify trends', 'Collaborate with business and IT Master Data Management teams to ensure tools and data support Financial requirements', 'Data Analysis experience utilizing Excel (this includes reviewing reports and data entry)', 'Microsoft SharePoint experience', 'Work with individuals at the global sites to determine reporting requirements within the scope of the QMS', 'Python', 'Using Microsoft SQL, design, develop, test, and publish standard and/or ad hoc reports as requested by the management team and provide analysis of these reports', 'Support training for divisions and functions to ensure proper use of tools and metrics related to Quality Management System', 'Our largest Financial client is seeking an Associate Data Analyst in Greater Phoenix Area. The person will support the Compliance Business Intelligence team and work alongside Program Manager,  Data Architects, and other key stakeholders to create reports and dashboards supporting Quality Management Systems.', 'College degree or higher from an accredited institution', 'Primary Responsibilities include but are not limited to the following:\xa0', 'Using Microsoft SQL, design, develop, test, and publish standard and/or ad hoc reports as requested by the management team and provide analysis of these reportsAnalyze data to identify trendsPrepare data tables for reports and generate SQL queriesExtract and manipulate large data sets including multiple data typesConduct data reviews for technical gaps and collaborate with Quality Management Systems and IT teams to resolveCollaborate with business and IT Master Data Management teams to ensure tools and data support Financial requirementsWork with individuals at the global sites to determine reporting requirements within the scope of the QMSGain an understanding of and continue to support data analytics policies and proceduresSupport training for divisions and functions to ensure proper use of tools and metrics related to Quality Management System', 'Data Analytics', 'Computer Science', 'Business Analytics', 'Information Technology', 'Prepare data tables for reports and generate SQL queries', 'Electronics', 'Must be able to work remotely and be available during core hours 9 am- 3 pm CST (Monday- Friday)', 'No experience required if related Masters Degree in ', 'Proficient with MS Office', 'Information Systems', 'Data Science and Machine Learning skills', '\xa0', 'Data Analysis experience utilizing Excel (this includes reviewing reports and data entry)Power BI/tableau ExperienceMicrosoft SharePoint experienceMicrosoft Access experience is preferredData Science and Machine Learning skillsPythonProficient with MS OfficeStrong Project Coordination and communication skillsCollege degree or higher from an accredited institutionMust be able to work remotely and be available during core hours 9 am- 3 pm CST (Monday- Friday)', 'Extract and manipulate large data sets including multiple data types', 'Preferred Qualifications\xa0', 'MSIS', 'Conduct data reviews for technical gaps and collaborate with Quality Management Systems and IT teams to resolve', 'Power BI/tableau Experience', 'Strong Project Coordination and communication skills']",Entry level,Contract,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Advanced Data Engineer (KTD),Kroger,"Cincinnati, OH",1 month ago,Be among the first 25 applicants,"['', 'Essential Job Functions', "" Bachelor's Degree in computer science, or software engineering, or related field"", 'Any experience with a variety of SQL, NoSQL and Big Data Platforms', 'Provide technical leadership to ensure clarity between ongoing projects, including direct collaboration with 84.51', 'Lead the analysis of the technology environment to detect critical deficiencies and recommend solutions for improvement', 'Present opportunities with cost/benefit analysis to leadership to shape sound architectural decisions', 'Define high-level migration plans to address the gaps between the current and future state', 'Mentor team members in data principles, patterns, processes and practices', 'Keywords', "" Bachelor's Degree in computer science, or software engineering, or related field7+ years successful and applicable hands on experience in the data development and principles including end-to-end design patterns7+ years proven track record of designing and delivering large scale, high quality operational or analytical data systems7+ years successful and applicable experience taking a lead role in building complex data solutions that have been successfully delivered to customersAny experience defining evolutionary data solutions and underlying technologiesDemonstrated written and oral communication skillsBasic understanding of network and data security architectureStrong knowledge of industry trends and industry competitionKnowledge in a minimum of two of the following technical disciplines: data warehousing, data management, analytics development, data science, application programming interfaces (APIs), data integration, cloud, servers and storage, and database management"", 'Shift(s):', 'Position Summary', 'FLSA Status: ', 'Any experience building solutions using elastic architectures (preferably Microsoft Azure and Google Cloud Platform)', 'Basic understanding of network and data security architecture', 'Knowledge in a minimum of two of the following technical disciplines: data warehousing, data management, analytics development, data science, application programming interfaces (APIs), data integration, cloud, servers and storage, and database management', 'Jobs at Kroger:', 'Demonstrated written and oral communication skills', 'Draft and review architectural diagrams, interface specifications and other design documents', 'Required Certifications/Licenses:', 'Must be able to perform the essential job functions of this position with or without reasonable accommodation', 'Promote the reuse of data assets, including the management of the data catalog for reference', 'at https://www.kroger.com/livekt ', 'Company Name:', '7+ years successful and applicable hands on experience in the data development and principles including end-to-end design patterns', 'Desired Previous Experience/Education', '7+ years successful and applicable experience taking a lead role in building complex data solutions that have been successfully delivered to customers', 'Strong knowledge of industry trends and industry competition', '7+ years proven track record of designing and delivering large scale, high quality operational or analytical data systems', 'Any experience building solutions using elastic architectures (preferably Microsoft Azure and Google Cloud Platform)Any experience with a variety of SQL, NoSQL and Big Data PlatformsAny experience with operational data science, machine learning or artificial intelligence solutionsAny experience with data science solutions or platforms', 'Line Of Business', ' See what life is like at Kroger Technology ', 'Participate in the development and communication of data strategy and roadmaps across the technology organization to support project portfolio and business strategyParticipate in, and drive, the development and communication of enterprise standards for data domains and data solutions, focusing on simplified integration and streamlined operational and analytical usesProvide technical leadership to ensure clarity between ongoing projects, including direct collaboration with 84.51Drive digital innovation by leveraging innovative new technologies and approaches to renovate, extend, and transform the existing core data assets, including SQL-based, NoSQL-based, and Cloud-based data platformsDefine high-level migration plans to address the gaps between the current and future statePresent opportunities with cost/benefit analysis to leadership to shape sound architectural decisionsLead the analysis of the technology environment to detect critical deficiencies and recommend solutions for improvementMentor team members in data principles, patterns, processes and practicesPromote the reuse of data assets, including the management of the data catalog for referenceDraft and review architectural diagrams, interface specifications and other design documentsMust be able to perform the essential job functions of this position with or without reasonable accommodation', 'Any experience defining evolutionary data solutions and underlying technologies', 'Additional Technology Information', 'Company Overview: ', 'Participate in, and drive, the development and communication of enterprise standards for data domains and data solutions, focusing on simplified integration and streamlined operational and analytical uses', 'Any experience with operational data science, machine learning or artificial intelligence solutions', 'Any experience with data science solutions or platforms', ' banner names ', 'Drive digital innovation by leveraging innovative new technologies and approaches to renovate, extend, and transform the existing core data assets, including SQL-based, NoSQL-based, and Cloud-based data platforms', 'Position Type:', 'Education Level:', 'Participate in the development and communication of data strategy and roadmaps across the technology organization to support project portfolio and business strategy', 'Minimum Position Qualifications']",Associate,Full-time,Information Technology,Food & Beverages,2021-03-18 14:34:51
Data Engineer - Personalization (Remote Eligible - Americas),Spotify,"Boston, MA",1 week ago,115 applicants,"['', 'We are a distributed workforce enabling our band members to find a work mode that is best for them!', 'Build large-scale batch and real-time data pipelines with data processing frameworks like Scalding, Scio, Storm, Spark and the Google Cloud Platform.', 'You know how to write distributed, high-volume services in Java or Scala.', 'Collaborate with other software engineers, ML experts and stakeholders, taking learning and leadership opportunities that will arise every single day.', 'You appreciate agile software processes, data-driven development, reliability, and responsible experimentation.', 'Working hours? We operate within the Eastern Standard time zone for collaboration and ask that all be located that time zone. ', 'We are a distributed workforce enabling our band members to find a work mode that is best for them!Where in the world? For this role, it can be within the Americas region in which we have a work location and is within working hours. Working hours? We operate within the Eastern Standard time zone for collaboration and ask that all be located that time zone. Prefer an office to work from home instead? Not a problem! We have plenty of options for your working preferences. Find more information about our Work From Anywhere options here .', 'Use best practices in continuous integration and delivery.', 'Work in multi-functional agile teams to continuously experiment, iterate and deliver on new product objectives!', 'Who You Are', 'Prefer an office to work from home instead? Not a problem! We have plenty of options for your working preferences. Find more information about our Work From Anywhere options here .', 'You know how to work with high volume heterogeneous data, preferably with distributed systems such as Hadoop, BigTable, and Cassandra.', ""What You'll Do"", 'Help drive optimization, testing and tooling to improve data quality.', 'Where in the world? For this role, it can be within the Americas region in which we have a work location and is within working hours. ', ""Where You'll Be"", 'You know how to work with high volume heterogeneous data, preferably with distributed systems such as Hadoop, BigTable, and Cassandra.You know how to write distributed, high-volume services in Java or Scala.You are knowledgeable about data modeling, data access, and data storage techniques.You appreciate agile software processes, data-driven development, reliability, and responsible experimentation.You understand the value of partnership within teams.We are proud to foster a workplace free from discrimination. We strongly believe that diversity of experience, perspectives, and background will lead to a better environment for our employees and a better product for our users and our creators. This is something we value deeply and we encourage everyone to come be a part of changing the way the world listens to music. ', 'We are proud to foster a workplace free from discrimination. We strongly believe that diversity of experience, perspectives, and background will lead to a better environment for our employees and a better product for our users and our creators. This is something we value deeply and we encourage everyone to come be a part of changing the way the world listens to music. ', 'Build large-scale batch and real-time data pipelines with data processing frameworks like Scalding, Scio, Storm, Spark and the Google Cloud Platform.Use best practices in continuous integration and delivery.Help drive optimization, testing and tooling to improve data quality.Collaborate with other software engineers, ML experts and stakeholders, taking learning and leadership opportunities that will arise every single day.Work in multi-functional agile teams to continuously experiment, iterate and deliver on new product objectives!', 'You understand the value of partnership within teams.', 'You are knowledgeable about data modeling, data access, and data storage techniques.']",Not Applicable,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Data Engineer (Experienced),Salesforce,"San Francisco, CA",1 day ago,Be among the first 25 applicants,"['', ' Clearly articulate pros and cons of various technologies and platforms in open source and proprietary products. Execute proof of concept on new technology and tools to help the organization pick the best tools and solutions. ', ' Communicate with product owners and analysts to clarify requirements. Craft technical solutions and assemble design artifacts (functional design documents, data flow diagrams, data models, etc.). ', 'Salesforce.org', 'Accommodations ', 'Posting Statement', ' Own the technical solution design, lead the technical architecture and implementation of data acquisition and integration projects, both batch and real time. Define the overall solution architecture needed to implement a layered data stack that ensures a high level of data quality and timely insights. ', ' Strong SQL optimization and performance tuning experience in a high volume data environment that utilizes parallel processing. Hadoop, Spark, Teradata platform experience a plus. ', ' 5+ years of experience working with ETL tools, specifically creating data driven orchestration and transformation jobs and user and project administration. A strong Python scripting knowledge including hands-on experience in building packages for ETL is required. Advanced Matillion developer is a plus. ', ' 4+ years of experience with Data Warehouse including knowledge of Stored Procedures, tasks and streams. Must be an expert in writing complex SQL queries and understand the methodologies to tune/improve query performance. ', ' Build data pipelines data processing tools and technologies in open source and proprietary products. ', ' Own the technical solution design, lead the technical architecture and implementation of data acquisition and integration projects, both batch and real time. Define the overall solution architecture needed to implement a layered data stack that ensures a high level of data quality and timely insights.  Communicate with product owners and analysts to clarify requirements. Craft technical solutions and assemble design artifacts (functional design documents, data flow diagrams, data models, etc.).  Build data pipelines data processing tools and technologies in open source and proprietary products.  Serve the team as a subject matter expert & mentor for ETL design, and other related big data and programming technologies.  Identify incomplete data, improve quality of data, and integrate data from several data sources.  Proactively identify performance & data quality problems and drive the team to remediate them. Advocate architectural and code improvements to the team to improve execution speed and reliability.  Design and develop tailored data structures in database and Hadoop.  Quickly create functioning ETL prototypes to address quickly changing business needs.  Revamp prototypes to create production-ready data flows.  Support Data Science research by designing, developing, and maintaining all parts of the Big Data pipeline for reporting, statistical and machine learning, and computational requirements.  Perform data profiling, complex sampling, statistical testing, and testing of reliability on data.  Clearly articulate pros and cons of various technologies and platforms in open source and proprietary products. Execute proof of concept on new technology and tools to help the organization pick the best tools and solutions.  Harness operational excellence & continuous improvement with a can do leadership attitude. ', ' Identify incomplete data, improve quality of data, and integrate data from several data sources. ', 'To get the best candidate experience, please consider applying for a maximum of 3 roles within 12 months to ensure you are not duplicating efforts.', ' Knowledge of data modeling techniques and high-volume ETL/ELT design. ', 'Responsibilities', ' Support Data Science research by designing, developing, and maintaining all parts of the Big Data pipeline for reporting, statistical and machine learning, and computational requirements. ', 'Job Requirements', 'Salesfore.com', ' Ability to research, analyze, interpret, and produce accurate results within reasonable turnaround times with an iterative mindset with rapid prototyping designs. ', ' Familiarity with scrum/agile project management methodologies and SDLC stages required. ', ' Strong problem solving with acute attention to detail and ability to meet tight deadlines and project plans. ', 'Job Details', 'Accommodations  - ', ' Proactively identify performance & data quality problems and drive the team to remediate them. Advocate architectural and code improvements to the team to improve execution speed and reliability. ', 'Salesforce.com', ' Design and develop tailored data structures in database and Hadoop. ', ' Harness operational excellence & continuous improvement with a can do leadership attitude. ', ' Experience with version control systems (Github, Subversion) and deployment tools (e.g. continuous integration) required. ', ' If you are currently in college/ grad school or have less than a year of experience - please check out FutureForce job opportunities at Salesforce:', ' Previous projects should display technical leadership with an emphasis on data lake, data warehouse solutions, business intelligence, big data analytics, enterprise-scale custom data products. ', ' Quickly create functioning ETL prototypes to address quickly changing business needs. ', ' Revamp prototypes to create production-ready data flows. ', ' Hands-on on Salesforce.com knowledge of product and functionality a plus. ', ' BS/MS degree in Computer Science, Engineering, Mathematics, Physics, or equivalent/related degree.  5+ years of experience working with ETL tools, specifically creating data driven orchestration and transformation jobs and user and project administration. A strong Python scripting knowledge including hands-on experience in building packages for ETL is required. Advanced Matillion developer is a plus.  4+ years of experience with Data Warehouse including knowledge of Stored Procedures, tasks and streams. Must be an expert in writing complex SQL queries and understand the methodologies to tune/improve query performance.  Previous projects should display technical leadership with an emphasis on data lake, data warehouse solutions, business intelligence, big data analytics, enterprise-scale custom data products.  Familiarity with new big data management techniques of schema on read, search analytics, graph analytics, semantic data lakes, linked data, etc.  Knowledge of data modeling techniques and high-volume ETL/ELT design.  Strong SQL optimization and performance tuning experience in a high volume data environment that utilizes parallel processing. Hadoop, Spark, Teradata platform experience a plus.  Experience with version control systems (Github, Subversion) and deployment tools (e.g. continuous integration) required.  Experience with programming languages like Java, Scala & scripting in Python, Perl, Bash.  Experience working with Public Cloud platforms like GPC, AWS, or Azure.  Familiarity with scrum/agile project management methodologies and SDLC stages required.  Hands-on on Salesforce.com knowledge of product and functionality a plus.  Ability to work effectively in an unstructured and fast-paced environment both independently and in a team setting,with a high degree of self-management with clear communication and commitment to delivery timelines.  Strong problem solving with acute attention to detail and ability to meet tight deadlines and project plans.  Ability to research, analyze, interpret, and produce accurate results within reasonable turnaround times with an iterative mindset with rapid prototyping designs. ', 'Role Description', ' Experience working with Public Cloud platforms like GPC, AWS, or Azure. ', ' BS/MS degree in Computer Science, Engineering, Mathematics, Physics, or equivalent/related degree. ', ' Familiarity with new big data management techniques of schema on read, search analytics, graph analytics, semantic data lakes, linked data, etc. ', 'Job Category', ' Experience with programming languages like Java, Scala & scripting in Python, Perl, Bash. ', 'https://www.salesforce.com/company/careers/university-recruiting/', ' Ability to work effectively in an unstructured and fast-paced environment both independently and in a team setting,with a high degree of self-management with clear communication and commitment to delivery timelines. ', ' Perform data profiling, complex sampling, statistical testing, and testing of reliability on data. ', ' Serve the team as a subject matter expert & mentor for ETL design, and other related big data and programming technologies. ']",Not Applicable,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Engineer (x2 Openings),Rev.io,"Atlanta, GA",2 days ago,Be among the first 25 applicants,"['', 'Atlassian products (JIRA, Confluence, etc.)Full Microsoft Stack (Visual Studio 2013, SQL Server 2014/2019, HTML5/JS)Python/Spark', 'Hands-on experience working with live client systems and configuring real production environments.', '1-2 years minimum of professional experience working with SQL (required)', 'Have worked with BI/Visualization tools (preferred)', '1-2 years minimum of professional experience working with SQL (required)1-2 years of experience with T-SQL (preferred)Bachelor’s Degree in Database Management, Information Technology, Computer Science, Computer Information Science, or other database and software development-focused majors (preferred)Have worked with ETL tools for data migration (preferred)Have worked with BI/Visualization tools (preferred)Strong communicator, self-driven, and ability to meet deadlinesAbility to work full-time onsite daily and work with the teamStrong debugging skills', '1-2 years of experience with T-SQL (preferred)', 'Python/Spark', 'Implement client-driven reporting and data migration needs within the Rev.io platform with exposure to user interface and database changes using SQL and ETL tools.', 'Full Microsoft Stack (Visual Studio 2013, SQL Server 2014/2019, HTML5/JS)', 'Position Description', 'Ability to work full-time onsite daily and work with the team', 'Strong debugging skills', 'Implement client-driven reporting and data migration needs within the Rev.io platform with exposure to user interface and database changes using SQL and ETL tools.Responsible for working with teammates and clients to determine innovative solutions to problems experienced when installing new customers.Involved in the database testing and will actively participate in workflows involving business intelligence and reporting.Gain exposure to large customer data sets involved in product migrations, and responsible for helping data flow into Rev.io from legacy customer systems.Hands-on experience working with live client systems and configuring real production environments.', 'Strong communicator, self-driven, and ability to meet deadlines', 'Have worked with ETL tools for data migration (preferred)', 'Gain exposure to large customer data sets involved in product migrations, and responsible for helping data flow into Rev.io from legacy customer systems.', 'Bachelor’s Degree in Database Management, Information Technology, Computer Science, Computer Information Science, or other database and software development-focused majors (preferred)', 'Involved in the database testing and will actively participate in workflows involving business intelligence and reporting.', 'Company Overview', 'Atlassian products (JIRA, Confluence, etc.)', 'Responsible for working with teammates and clients to determine innovative solutions to problems experienced when installing new customers.']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,DATAECONOMY,"Charlotte, NC",,N/A,"['', 'Ability to work in an advisory capacity to identify key technical business problems, develop and evaluate alternative solutions and make recommendations', 'Location: Charlotte, North Carolina, United States', 'Experience with software testing frameworks.', 'Perform unit tests and conduct reviews with other team members to ensure code is designed with high code coverage', 'Leverages DevOps techniques and Experience with DevOps tools - GitHub, Jira, Jenkins, Crucible for Continuous Integration, Continuous Deployment and build automation.', 'Knowledge of NoSQL, RDBMS, SQL, JSON, XML and ETL skills are must.', 'Passionate for continuous learning, experimenting, applying and contributing towards cutting edge open source technologies and software paradigms', 'Please Note: Must be US Citizen /GC/ H4 EAD / H1B Transfer Candidates', 'Job Responsibilities', 'Experience implementing Data Warehouse in Snowflake', 'Knowledge of NoSQL, RDBMS, SQL, JSON, XML and ETL skills are must.Understanding of data transformations, cleansing, and deduplications.Advanced knowledge of SQL (PSQL or TSQL).Experience developing pipelines for both Cloud and Hybrid Cloud infrastructures.Experience in AWS utilizing services such as S3, AWS CLI, and RDS.Experience using modern ETL tools like Talend and Nifi.Experience implementing Data Warehouse in SnowflakeExperience working in an Agile delivery environmentAbility to work independently and drive solutions end to end leveraging various technologies to solve data problems and develop solutions.Perform unit tests and conduct reviews with other team members to ensure code is designed with high code coveragePassionate for continuous learning, experimenting, applying and contributing towards cutting edge open source technologies and software paradigmsAbility to research and assess open source technologies and components to recommend and integrate into the design and implementationProven track record of customer satisfaction and delivery success and ability to establish and maintain appropriate relationships with business and IT stakeholdersAbility to work in an advisory capacity to identify key technical business problems, develop and evaluate alternative solutions and make recommendationsExtensive experience in all aspects of the software development life cycle', 'Ability to research and assess open source technologies and components to recommend and integrate into the design and implementation', 'Proven track record of customer satisfaction and delivery success and ability to establish and maintain appropriate relationships with business and IT stakeholders', 'Experience developing pipelines for both Cloud and Hybrid Cloud infrastructures.', 'Ability to work independently and drive solutions end to end leveraging various technologies to solve data problems and develop solutions.', 'Experience working in an Agile delivery environment', 'Hourly Billing Rate: Open - Must be competitive', 'Qualifications', 'Engage in application design and data modeling discussions also participate in developing and enforcing data security policies', 'Design, build and maintain Big Data workflows/pipelines to process billions of records in large-scale data environments with experience in end-to-end design and build process of Near-Real Time and Batch Data Pipelines.Leads code review sessions to validate adherence with development standards and benchmark application performance by capacity testing.Experience with software testing frameworks.Leverages DevOps techniques and Experience with DevOps tools - GitHub, Jira, Jenkins, Crucible for Continuous Integration, Continuous Deployment and build automation.Develop, implement and optimize streaming, data lake, and big data analytics solutionsSupport reusable framework and data governance processes by partnering with LOBs for any code/requirements remediationEngage in application design and data modeling discussions also participate in developing and enforcing data security policies', 'Understanding of data transformations, cleansing, and deduplications.', 'Extensive experience in all aspects of the software development life cycle', 'Experience using modern ETL tools like Talend and Nifi.', ""Urgent need for DATA ENGINEER for a join on our client's project."", 'Support reusable framework and data governance processes by partnering with LOBs for any code/requirements remediation', 'DATA ENGINEER', 'Mode of Employment: Full-time', '\xa0Job Responsibilities', 'Experience in AWS utilizing services such as S3, AWS CLI, and RDS.', 'Leads code review sessions to validate adherence with development standards and benchmark application performance by capacity testing.', '\xa0', 'Advanced knowledge of SQL (PSQL or TSQL).', 'If you are interested, please share your resume to jason@dataeconomy.io or call 614-734-1434', 'Develop, implement and optimize streaming, data lake, and big data analytics solutions', 'Design, build and maintain Big Data workflows/pipelines to process billions of records in large-scale data environments with experience in end-to-end design and build process of Near-Real Time and Batch Data Pipelines.']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Radancy,"Hiram, GA",1 day ago,Be among the first 25 applicants,"['', ' Build monitoring dashboards and automate data quality testing ', ' Assist with the development and review of technical and end user documentation including ETL workflows, research, and data analysis ', 'About The Job', ' Product / reporting suite experience ', ' Detail oriented and strong communicator ', ' Develop and lead the testing and fixing of new or enhanced solutions for data products and reports, including automating ETL testing ', ' Responsible for daily integrity checks, performing deployments and releases ', ' Bachelors or Masters degree in Computer Science or other related field ', ' AdTech experience preferred ', ' Radancy Data Engineering works on data services across product organizations within Radancy, and supports building a customer facing data visualization product. The Data Engineering team supports an enterprise grade recruitment platform focusing on talent acquisition and job opportunity exploration. ', ' Ingest and aggregate data from both internal and external data sources to build our world class datasets ', ' Exposure / familiarity with Google Cloud Platform / BigQuery / Amazon Redshift ', 'The Team', ' Work with Cloud Computing Platforms (GCP/AWS), Luigi, Kafka and other open-source technologies ', ' Own meaningful parts of our service, have an impact, grow with the company ', ' Radancy Data Engineering works on data services across product organizations within Radancy, and supports building a customer facing data visualization product. The Data Engineering team supports an enterprise grade recruitment platform focusing on talent acquisition and job opportunity exploration.  The team has extensive experience in ETL development, works with large scale data in real time, and cross collaborates with other engineering teams across the organization. ', 'Desired Technical Qualifications', ' Build and maintain ETL pipelines utilizing Python that connect 1st and 3rd party data  Work with Cloud Computing Platforms (GCP/AWS), Luigi, Kafka and other open-source technologies  Conduct data modeling, schema design, and SQL development  Ingest and aggregate data from both internal and external data sources to build our world class datasets  Develop and lead the testing and fixing of new or enhanced solutions for data products and reports, including automating ETL testing  Collaborate with Product Owner and domain experts to recognize and help adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation  Assist with the development and review of technical and end user documentation including ETL workflows, research, and data analysis  Work with Product team to define data collection and engineering frameworks  Build monitoring dashboards and automate data quality testing  Responsible for daily integrity checks, performing deployments and releases  Own meaningful parts of our service, have an impact, grow with the company ', ' Build and maintain ETL pipelines utilizing Python that connect 1st and 3rd party data ', ' Collaborate with Product Owner and domain experts to recognize and help adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation ', ' Work with Product team to define data collection and engineering frameworks ', ' 2+ years of Python, SQL, and ETL development ', 'Overview', 'Flexible Location (Remote Hubs): ', ' Enthusiastic about working with and exploring new data sets ', ' Exposure to front end development: HTML, JavaScript, jQuery, Angular or similar libraries ', ' Radancy', ' 2+ years of Python, SQL, and ETL development  Bachelors or Masters degree in Computer Science or other related field  Product / reporting suite experience  Familiarity with C#, .Net, Kafka, Docker  Exposure to front end development: HTML, JavaScript, jQuery, Angular or similar libraries  Exposure / familiarity with Google Cloud Platform / BigQuery / Amazon Redshift  AdTech experience preferred  Enthusiastic about working with and exploring new data sets  Detail oriented and strong communicator ', ' The team has extensive experience in ETL development, works with large scale data in real time, and cross collaborates with other engineering teams across the organization. ', ' Familiarity with C#, .Net, Kafka, Docker ', ' Conduct data modeling, schema design, and SQL development ']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Data Engineer,Shamrock Trading Corporation,"Overland Park, KS",9 hours ago,86 applicants,"['', ' Surface data integration errors to the proper teams, ensuring timely processing of new data. ', ' Provide technical consultation for the team with “infrastructure as code” best practices: building deployment processes utilizing technologies such as Terraform or AWS Cloud Formation. ', ' Work-Life Balance: Competitive PTO and paid leave options', 'Financial', ' Financial: Generous company 401(k) contributions and employee stock ownership after one year ', ' Hands-on experience building data-lake style infrastructures using streaming data set technologies (particularly with Apache Kafka) ', ' Experience data processing using Parquet and Avro ', ' Enthusiasm for working directly with customer teams (Business units and internal IT) ', 'Benefits Package', ' Experience with data processing and analytics using AWS Glue or Apache Spark  Hands-on experience building data-lake style infrastructures using streaming data set technologies (particularly with Apache Kafka)  Experience data processing using Parquet and Avro  Experience developing, maintaining, and deploying Python packages  Experience with Kafka and the Kafka Connect ecosystem.  Familiarity with data visualization techniques using tools such as Grafana, PowerBI, AWS Quick Sight, and Excel. ', 'Responsibilities', ' Experience with data processing and analytics using AWS Glue or Apache Spark ', 'Work-Life Balance', ' Wellness: Onsite gym, jogging trail and discounted membership to nearby fitness center ', 'Qualifications', ' Experience with Kafka and the Kafka Connect ecosystem. ', ' Preferred but not required qualifications include: Experience with data processing and analytics using AWS Glue or Apache Spark  Hands-on experience building data-lake style infrastructures using streaming data set technologies (particularly with Apache Kafka)  Experience data processing using Parquet and Avro  Experience developing, maintaining, and deploying Python packages  Experience with Kafka and the Kafka Connect ecosystem.  Familiarity with data visualization techniques using tools such as Grafana, PowerBI, AWS Quick Sight, and Excel. ', ' Work with Data architects to understand current data models, to build pipelines for data ingestion and transformation.  Design, build, and maintain a framework for pipeline observation and monitoring, focusing on reliability and performance of jobs.  Surface data integration errors to the proper teams, ensuring timely processing of new data.  Provide technical consultation for other team members on best practices for automation, monitoring, and deployments.  Provide technical consultation for the team with “infrastructure as code” best practices: building deployment processes utilizing technologies such as Terraform or AWS Cloud Formation. ', ' Medical: Fully-paid healthcare, dental and vision premiums for employees and eligible dependents  Financial: Generous company 401(k) contributions and employee stock ownership after one year  Wellness: Onsite gym, jogging trail and discounted membership to nearby fitness center  Work-Life Balance: Competitive PTO and paid leave options', ' Familiarity with data visualization techniques using tools such as Grafana, PowerBI, AWS Quick Sight, and Excel. ', ' Medical: Fully-paid healthcare, dental and vision premiums for employees and eligible dependents ', ' Mid/Senior level development utilizing Python: (Pandas/Numpy, Boto3, SimpleSalesforce) ', 'Medical', ' Proven experience with relational and NoSQL databases (e.g. Postgres, Redshift, MongoDB, Elasticsearch) ', ' Experience developing, maintaining, and deploying Python packages ', 'Wellness', ' Bachelor’s degree in computer science, data science or related technical field, or equivalent practical experience ', ' Work with Data architects to understand current data models, to build pipelines for data ingestion and transformation. ', ' Provide technical consultation for other team members on best practices for automation, monitoring, and deployments. ', ' Bachelor’s degree in computer science, data science or related technical field, or equivalent practical experience  Proven experience with relational and NoSQL databases (e.g. Postgres, Redshift, MongoDB, Elasticsearch)  Experience building and maintaining AWS based data pipelines: Technologies currently utilized include AWS Lambda, Docker / ECS, MSK  Mid/Senior level development utilizing Python: (Pandas/Numpy, Boto3, SimpleSalesforce)  Experience with version control (git) and peer code reviews  Enthusiasm for working directly with customer teams (Business units and internal IT)  Preferred but not required qualifications include: Experience with data processing and analytics using AWS Glue or Apache Spark  Hands-on experience building data-lake style infrastructures using streaming data set technologies (particularly with Apache Kafka)  Experience data processing using Parquet and Avro  Experience developing, maintaining, and deploying Python packages  Experience with Kafka and the Kafka Connect ecosystem.  Familiarity with data visualization techniques using tools such as Grafana, PowerBI, AWS Quick Sight, and Excel. ', ' Experience with version control (git) and peer code reviews ', 'Company Overview', ' Design, build, and maintain a framework for pipeline observation and monitoring, focusing on reliability and performance of jobs. ', ' Experience building and maintaining AWS based data pipelines: Technologies currently utilized include AWS Lambda, Docker / ECS, MSK ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Snap Inc.,"Los Angeles, CA",1 month ago,38 applicants,"['', 'Preferred Qualifications:', ' Experience cultivating a strong engineering culture in an agile environment ', ' 2+ years of technical people management experience ', ' Work closely with IT, HRIS, & People Analytics to influence the build the HR data infrastructure  Collaboration cross-functionally with other engineers and product managers to develop the best data practices in line with things we value.  Grow the technical expertise of the People Tech Team  Help cultivate agile methodologies and foster a culture of balanced tech health  Ensure accuracy and timeliness of data is maintained in the data layer  Actively participate in Data Governance efforts. ', ' Data architecture experience ', 'Knowledge, Skills & Abilities:', 'What you’ll do: Work closely with IT, HRIS, & People Analytics to influence the build the HR data infrastructure  Collaboration cross-functionally with other engineers and product managers to develop the best data practices in line with things we value.  Grow the technical expertise of the People Tech Team  Help cultivate agile methodologies and foster a culture of balanced tech health  Ensure accuracy and timeliness of data is maintained in the data layer  Actively participate in Data Governance efforts. Knowledge, Skills & Abilities: Experience cultivating a strong engineering culture in an agile environment  Proven engineering background with previous experience developing large scale systems for data engineering, including processing, storage, quality and management  In-depth knowledge of agile software processes, data-driven development, reliability, and responsible experimentation  Proven track record of collaborating with other teams on relevant topics such as data science, and software systems design ', ' Hands on experience with Google BigQuery  Experience in version control systems such as Git  Data architecture experience  Experience in ETL / Data application development  Experience working with a MapReduce or an MPP system ', 'What you’ll do:', ' Grow the technical expertise of the People Tech Team ', ' Proven track record of collaborating with other teams on relevant topics such as data science, and software systems design ', ' Proven engineering background with previous experience developing large scale systems for data engineering, including processing, storage, quality and management ', 'Knowledge, Skills & Abilities: Experience cultivating a strong engineering culture in an agile environment  Proven engineering background with previous experience developing large scale systems for data engineering, including processing, storage, quality and management  In-depth knowledge of agile software processes, data-driven development, reliability, and responsible experimentation  Proven track record of collaborating with other teams on relevant topics such as data science, and software systems design ', ' BS/BA degree in Computer Science, Math, Physics, or related field, or equivalent experience in a relevant field  5+ years working as a Software Engineering or in relevant field  5+ years experience in SQL or similar languages.  5+ years development experience in at least one object-oriented language (Python, Perl, Java, etc.)  2+ years of technical people management experience ', 'Minimum Qualifications: ', 'Preferred Qualifications: Hands on experience with Google BigQuery  Experience in version control systems such as Git  Data architecture experience  Experience in ETL / Data application development  Experience working with a MapReduce or an MPP system ', ' Experience working with a MapReduce or an MPP system ', 'What you’ll do: Work closely with IT, HRIS, & People Analytics to influence the build the HR data infrastructure  Collaboration cross-functionally with other engineers and product managers to develop the best data practices in line with things we value.  Grow the technical expertise of the People Tech Team  Help cultivate agile methodologies and foster a culture of balanced tech health  Ensure accuracy and timeliness of data is maintained in the data layer  Actively participate in Data Governance efforts. ', ' Collaboration cross-functionally with other engineers and product managers to develop the best data practices in line with things we value. ', ' In-depth knowledge of agile software processes, data-driven development, reliability, and responsible experimentation ', ' Help cultivate agile methodologies and foster a culture of balanced tech health ', ' Experience cultivating a strong engineering culture in an agile environment  Proven engineering background with previous experience developing large scale systems for data engineering, including processing, storage, quality and management  In-depth knowledge of agile software processes, data-driven development, reliability, and responsible experimentation  Proven track record of collaborating with other teams on relevant topics such as data science, and software systems design ', ' Actively participate in Data Governance efforts. ', ' 5+ years working as a Software Engineering or in relevant field ', ' Work closely with IT, HRIS, & People Analytics to influence the build the HR data infrastructure ', 'accommodations-ext@snap.com', ' Ensure accuracy and timeliness of data is maintained in the data layer ', ' 5+ years experience in SQL or similar languages. ', ' Experience in ETL / Data application development ', ' Hands on experience with Google BigQuery ', ' Experience in version control systems such as Git ', 'Minimum Qualifications:  BS/BA degree in Computer Science, Math, Physics, or related field, or equivalent experience in a relevant field  5+ years working as a Software Engineering or in relevant field  5+ years experience in SQL or similar languages.  5+ years development experience in at least one object-oriented language (Python, Perl, Java, etc.)  2+ years of technical people management experience ', ' BS/BA degree in Computer Science, Math, Physics, or related field, or equivalent experience in a relevant field ', ' 5+ years development experience in at least one object-oriented language (Python, Perl, Java, etc.) ']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Data Engineer,StockX,"Seattle, WA",5 days ago,106 applicants,"['', 'Responsibilities', 'Nice To Have', 'Qualifications']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Junior Data Engineer,AimHire,"Denver, CO",,N/A,"['', 'Free office snacks and beverages', '401k match', 'AimHire is an Equal Opportunity/Affirmative Action Employer.', 'Be a key contributor to the cloud-based applications and services using SQL, Azure Data Factory, Rest APIs, SQL Server/Synapse and Power Bi', 'Willingness to jump in and get the job done no matter how big or small', 'BS/MS in Computer Science (Or related field), code bootcamp graduate, or equivalent experience required', '1-3 years of professional software development experience', 'Deliver value to the business in a fast-paced agile environment', 'Paid parental leave', 'Embed reporting and data visualizations within the application', 'This is an exciting opportunity to quickly grow your career and offers a generous benefits package and compensation in the $90-$100K range, depending on experience.\xa0', 'We are recruiting on behalf of our client, Insurium, located in Downtown Denver. They are a recognized leader in the SaaS insurance technology market. The software delivers automation and digital transformation support for the entire commercial insurance lifecycle. CHSI’S dynamic, high energy environment is focused on collaboration and a Work Hard, Play Hard mentality. If you have an entrepreneurial spirit and the passion to work directly with customers, this is a great opportunity to put those to good use!', 'Energetic and collaborate office culture', 'Experience with databases (Postgres, SQL Server, etc)', 'Focus on continuous improvement', 'Unlimited PTO9 paid holidays401k matchComprehensive medical plan, including monthly allowance added to HSAPaid parental leaveRemote work flexibilityEnergetic and collaborate office cultureFree office snacks and beverages', 'Unlimited PTO', 'Excellent communication skills', 'Insurium', '1-2 years of experience in ReactJS', '2-3 years of experience building solutions in Power BI and Azure Synapse or a comparable business intelligence solution preferredPreferred expert level SQL knowledge with extensive ETL experience and the ability to diagnose and solve DB performance issuesBS/MS in Computer Science (Or related field), code bootcamp graduate, or equivalent experience requiredFamiliarity with agile software development methodologies 1-3 years of professional software development experience1-2 years of experience in ReactJSExperience with databases (Postgres, SQL Server, etc)Willingness to jump in and get the job done no matter how big or smallFocus on continuous improvementExcellent communication skills', '9 paid holidays', 'Comprehensive medical plan, including monthly allowance added to HSA', 'Write maintainable code and apply automated testing where applicable', 'Insurium is seeking a driven and motivated Junior Data Engineer to join their growing company. In this role, you will work as part of a truly innovative and growing team responsible for developing and advancing how we deal with all things data related. You will be a key member of the data team responsible for automating, standardizing, and optimizing every piece of our architecture that involves ETL, warehousing, reporting, and data visualization.', 'Familiarity with agile software development methodologies ', 'What You Need', 'Preferred expert level SQL knowledge with extensive ETL experience and the ability to diagnose and solve DB performance issues', 'This is NOT a fully remote position. This position is based in their office in Denver, CO, but will be temporarily remote due to COVID-19.', 'What You Will be Doing', '2-3 years of experience building solutions in Power BI and Azure Synapse or a comparable business intelligence solution preferred', 'What You Will Get', '\xa0', 'Be a key contributor to the cloud-based applications and services using SQL, Azure Data Factory, Rest APIs, SQL Server/Synapse and Power BiEmbed reporting and data visualizations within the applicationDevelop solutions that support their customers self-service reporting needs and streamline their implementation team’s workflowsWrite maintainable code and apply automated testing where applicableWork with your team to make architectural decisionsDeliver value to the business in a fast-paced agile environment', 'Develop solutions that support their customers self-service reporting needs and streamline their implementation team’s workflows', 'Remote work flexibility', 'Work with your team to make architectural decisions']",Associate,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Engineer,Apex Systems,United States,2 days ago,90 applicants,"['', 'Understand principles of data optimization', 'Apache Airflow', 'Azure Data Factory (ADF)', 'Requirements:', 'Experience with Data Ingestion', 'Apache AirflowAzure Data Factory (ADF)', '70% Data Platform Engineer and 30% Data Optimizatiion', 'Ability to build pipelines to process structured and semi-structured data.', 'Senior level experience with Databricks', 'Additional preferred but not required:', 'Hands on experience with Azure data platform stack', 'Experience with Spark designing and building end-to-end data pipelines', 'Understand the basic data engineering aspects and relational modeling', 'Experience working with Azure Delta Tables and Parquets', 'Understand ETL datawarehouse concepts and partitions', '70% Data Platform Engineer and 30% Data OptimizatiionSenior level experience with DatabricksHands on experience with Azure data platform stackExperience with Spark designing and building end-to-end data pipelinesExperience working with Azure Delta Tables and ParquetsUnderstand ETL datawarehouse concepts and partitionsExperience with Data IngestionAbility to build pipelines to process structured and semi-structured data.Understand principles of data optimizationUnderstand the basic data engineering aspects and relational modeling', 'Must Haves: Databricks, Spark and ETL']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Lockheed Martin,"Arlington, VA",2 days ago,Be among the first 25 applicants,"['', 'The Selected Candidate Will', ' Enable data pipelines to provide clean, automated, reproducible data sets for consumption by AI/ML workloads', ' Integrate data into machine learning libraries and operational frameworks into an end-to-end environment', ' Provides ongoing support, monitoring, and maintenance of deployed solutions', ' Work with AI/ML practitioners to solve complex problems and create unique solutions', ' Work through all stages of a data solution lifecycle, e.g., analyze/profile data, create conceptual, logical and physical data model designs, architect and design ETL, testing, reporting and analytics', ' Participate in on-call rotations to monitor and resolve production issues during off-hours', ' Collaborate with data architects, data scientists, data analysts, and system engineers to develop a solution across the entire data processing pipeline', ' Work with AI/ML practitioners to solve complex problems and create unique solutions Work through all stages of a data solution lifecycle, e.g., analyze/profile data, create conceptual, logical and physical data model designs, architect and design ETL, testing, reporting and analytics Responsible for collecting, ingesting, processing, storing, and virtualizing large datasets from a wide variety of data sources and stakeholders Enable data pipelines to provide clean, automated, reproducible data sets for consumption by AI/ML workloads Integrate data into machine learning libraries and operational frameworks into an end-to-end environment Maintain data systems performance by identifying and resolving production and application development problems; calculating optimum values for parameters; evaluating, integrating, and installing new releases Participate in on-call rotations to monitor and resolve production issues during off-hours Provides ongoing support, monitoring, and maintenance of deployed solutions Collaborate with data architects, data scientists, data analysts, and system engineers to develop a solution across the entire data processing pipeline Must be a US Citizen', ' Responsible for collecting, ingesting, processing, storing, and virtualizing large datasets from a wide variety of data sources and stakeholders', ' Must be a US Citizen', ' Maintain data systems performance by identifying and resolving production and application development problems; calculating optimum values for parameters; evaluating, integrating, and installing new releases']",Entry level,Full-time,Information Technology,Construction,2021-03-18 14:34:51
Junior Data Engineer,Eight Eleven Group,"Charlotte, NC",1 day ago,Be among the first 25 applicants,"['', ' Build frameworks and reusable components for data acquisition and integration going across structured and unstructured data sources Use data to find new efficiencies and decrease data duplication across the company Scan assets, pulling payment information, and inputting into corresponding applications Create data standards and patterns for others to use across the company Communicate with business and other technology teams onshore and offshore Present data stories to business leaders Monitor and analyze existing processes to identify enhancement opportunities Assist with troubleshooting issues and providing end to end technical solutions quickly and accurately Handle complex operational tasks and recommends processing and tech changes with minimal supervision Maintain knowledge of emerging technologies', '<< Return to Search Results', 'Required Skills', ' Experience building data integration and ETL components using DataStage', 'Desired Skills', ' Strong problem solving and troubleshooting skills with the ability to exercise mature judgment', ' B.S. or M.S. in Computer Science, Data Science or similar field or equivalent experience and work record 1+ years working within a SQL database 1+ years of Application Development experience with Java or a similar Object-Oriented Programming Language 1+ years utilizing Python to develop scripts 1+ years of data mining or data comparison experience Strong problem solving and troubleshooting skills with the ability to exercise mature judgment Ability to manage multiple projects without continuous direction Ability to function effectively and proficiently in an environment of ongoing change Proactive self-starter with the ability to work well under pressure, prioritize and multitask. Experience working directly with technology, business stakeholders/leadership in formulating technical requirements', ' Experience in the financial services industry, particularly within a data environment Knowledge in designing and building SQL apps on Oracle, Teradata, HBase and Hive/Impala Knowledge of Spark, Hadoop, Kafka, and Streaming Components Experience building data integration and ETL components using DataStage Application integration experience using API/Mulesoft 1+ years building Tableau dashboards and visuals to create data stories', ' Ability to manage multiple projects without continuous direction', ' Use data to find new efficiencies and decrease data duplication across the company', ' Close-knit junior level team', ' Director level exposure and mentorship', 'Remote w/ expectation to return to Charlotte, NC (University Area) Office in 2021', 'Position Summary', ' Present data stories to business leaders', ' Handle complex operational tasks and recommends processing and tech changes with minimal supervision', ' 1+ years of Application Development experience with Java or a similar Object-Oriented Programming Language', ' 1+ years of data mining or data comparison experience', ' Experience working directly with technology, business stakeholders/leadership in formulating technical requirements', ' Build frameworks and reusable components for data acquisition and integration going across structured and unstructured data sources', ' Scan assets, pulling payment information, and inputting into corresponding applications', ' Monitor and analyze existing processes to identify enhancement opportunities', ' Experience in a Fortune 100 Financial Services Firm', ' B.S. or M.S. in Computer Science, Data Science or similar field or equivalent experience and work record', ' Experience in the financial services industry, particularly within a data environment', ' Communicate with business and other technology teams onshore and offshore', ' 1+ years utilizing Python to develop scripts', ' 1+ years working within a SQL database', ' Proactive self-starter with the ability to work well under pressure, prioritize and multitask.', 'Junior Data Engineer – Chief Data Office ', ' Application integration experience using API/Mulesoft', ' Maintain knowledge of emerging technologies', ' 1+ years building Tableau dashboards and visuals to create data stories', ' Experience in a Fortune 100 Financial Services Firm Director level exposure and mentorship Close-knit junior level team', ' Create data standards and patterns for others to use across the company', ' Knowledge in designing and building SQL apps on Oracle, Teradata, HBase and Hive/Impala', 'Value Add To You', ' Ability to function effectively and proficiently in an environment of ongoing change', 'Brooksource provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, national origin, age, sex, citizenship, disability, genetic information, gender, sexual orientation, gender identity, marital status, amnesty or status as a covered veteran in accordance with applicable federal, state, and local laws.JO-2010-105912', ' Assist with troubleshooting issues and providing end to end technical solutions quickly and accurately', 'Key Responsibilities And Duties', ' Knowledge of Spark, Hadoop, Kafka, and Streaming Components']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer Lead,Synechron,"Piscataway, NJ",1 day ago,41 applicants,"['', 'Kind regards,', ' ', ' Working collaboratively with other engineers, data scientists, analytics teams, and business product owners in an agile environment', 'Responsibilities for Engineers for data platform:', 'Experience building and administering big data and real-time streaming analytics architectures in cloud environments (Preferably in AWS) leveraging technologies such as Hadoop, Spark, S3, EMR, Postgres, Redshift, Airflow, and HudiExperience architecting, building and administering large-scale distributed applicationsKnowledge of Linux operations including basic commands and shell scripting experienceFamiliarity with DevOps methodologies and Continuous Integration/Continuous Delivery within a large scale data delivery environmentSoftware development experience in least three or more of following languages: Python, Scala, Node.js (Preferably Python 3)Expertise in usage of SQL for data profiling, analysis and extraction', ' Qualifications:', ' Working collaboratively with other engineers, data scientists, analytics teams, and business product owners in an agile environment Architect, build and support the operation of our Cloud infrastructure and enterprise data platform \xa0Design robust, reusable and scalable data driven solutions and data pipeline frameworks to automate the ingestion, processing and delivery of both structured and unstructured batch and real-time streaming dataBuild data APIs and data delivery services to support critical operational processes, analytical models and machine learning applications \xa0Assist in selection and integration of data related tools, frameworks and applications required to expand our platform capabilities \xa0Understand and implement best practices in management of enterprise data, including master data, reference data, metadata, data quality and lineage', 'Email: Tanu.Sardana@synechron.com', ' \xa0Collective experience in data engineering, data analysis, data warehousing, data integration or business intelligence, in a similarly sized organization. ', 'Design robust, reusable and scalable data driven solutions and data pipeline frameworks to automate the ingestion, processing and delivery of both structured and unstructured batch and real-time streaming data', 'Synechron,\xa0one of the fastest-growing digital, business consulting & technology services providers, is a $500 million firm based in New York. Since inception in 2001, Synechron has been on a steep growth trajectory. With 8000+ professionals operating in 18 countries across the world with presence across the USA, Canada, UK, Europe, Asia, and the Middle East.', ' \xa0', 'Familiarity with DevOps methodologies and Continuous Integration/Continuous Delivery within a large scale data delivery environment', 'Tanu Sardana', 'Please do visit our website:\xa0http://www.synechron.com', 'Job Description:', ' Responsibilities for Engineers for data platform:', 'Build data APIs and data delivery services to support critical operational processes, analytical models and machine learning applications \xa0', '\xa0one of the fastest-growing digital, business consulting & technology services providers, is a $500 million firm based in New York. Since inception in 2001, Synechron has been on a steep growth trajectory. With 8000+ professionals operating in 18 countries across the world with presence across the USA, Canada, UK, Europe, Asia, and the Middle East.', 'Knowledge of Linux operations including basic commands and shell scripting experience', 'Synechron Inc. is seeking Data Engineer Lead to join our team in Harrison, NY/Piscataway, NJ.', 'Software development experience in least three or more of following languages: Python, Scala, Node.js (Preferably Python 3)', 'Synechron,', 'Qualifications:', 'Experience architecting, building and administering large-scale distributed applications', ' Architect, build and support the operation of our Cloud infrastructure and enterprise data platform \xa0', 'Assist in selection and integration of data related tools, frameworks and applications required to expand our platform capabilities \xa0', 'Expertise in usage of SQL for data profiling, analysis and extraction', 'Sr. Associate-Recruitment', '\xa0', ' Bachelor’s degree in a technical field (e.g. Comp Science, Math, Engineering) or related experience', 'Experience building and administering big data and real-time streaming analytics architectures in cloud environments (Preferably in AWS) leveraging technologies such as Hadoop, Spark, S3, EMR, Postgres, Redshift, Airflow, and Hudi', 'Phone\xa0+1 848 215-7045', 'Understand and implement best practices in management of enterprise data, including master data, reference data, metadata, data quality and lineage']",Mid-Senior level,Contract,Information Technology,Financial Services,2021-03-18 14:34:51
Data Engineer,CNA Insurance,"Chicago, IL",2 days ago,Be among the first 25 applicants,"['', ' Uses knowledge and expertise to influence the organization at every level towards best in class data science tools and techniques.  Supports projects as Data Science SME; Collaborates with stakeholders to help define data requirements. Effectively communicates with project stakeholders by presenting complex or technical concepts in business terms. Builds data pipelines and develop model features for Machine Learning projects.  Supports process automation by leading smaller scale efforts to convert existing processes to Google Cloud Platform. Aggregates disparate structured and unstructured data to create meaningful datasets that drive analytical insights. Adheres to data governance best practices by leveraging Alation to build data dictionaries for new and existing data sources. Maintains and expands capabilities for data testing, validation and quality control. Supports data modeling in Looker, our cloud-based business intelligence platform. Works independently, receiving minimal guidance; acts as a resource for colleagues with less experience by providing instruction, guidance, and advice. May actively partner with other analytical teams across the organization and/or participate in special projects. Adheres to coding and documentation best practices Responds to and fulfills ad hoc data requests. ', 'May perform additional duties as assigned.', "" Ability to recommend analytical solutions to business problems and execute. Strong SQL knowledge and proven experience working with relational databases. Experience using R, Python, SQL, and other business−related software. Experience with Linux servers on Google Cloud Platform or other cloud provider.  Practical experience with version control, preferably Git. Experience using Looker or other data visualization tools a plus. Solid knowledge of core functions of insurance companies and general insurance acumen Solid interpersonal, communication and presentation skills. Effectively interacts with all levels of CNA's internal and external business partners. Solid analytical, critical thinking and problem solving skills to effectively resolve complex situations and issues. Solid project management, organization and planning skills with the ability to manage multiple projects effectively and lead teams.  Ability to solve issues with a sense of urgency; utilizes and manages the available resources to make informed decisions and achieve superior results. "", 'Strong SQL knowledge and proven experience working with relational databases.', ""Solid interpersonal, communication and presentation skills. Effectively interacts with all levels of CNA's internal and external business partners."", "" Bachelor's Degree in Business, Economics, Mathematics, Finance, Statistics, or related field. Typically up to two years of related work experience. "", 'Experience using R, Python, SQL, and other business−related software.', ""Bachelor's Degree in Business, Economics, Mathematics, Finance, Statistics, or related field."", 'Organization', 'Job Summary', 'Essential Duties & Responsibilities', 'Uses knowledge and expertise to influence the organization at every level towards best in class data science tools and techniques. ', 'Supports data modeling in Looker, our cloud-based business intelligence platform.', 'Performs a combination of duties in accordance with departmental guidelines:', 'Responds to and fulfills ad hoc data requests.', 'Job Posting', 'Experience using Looker or other data visualization tools a plus.', 'Primary Location', 'Adheres to data governance best practices by leveraging Alation to build data dictionaries for new and existing data sources.', 'EEO Statement', 'Solid analytical, critical thinking and problem solving skills to effectively resolve complex situations and issues.', 'Works independently, receiving minimal guidance; acts as a resource for colleagues with less experience by providing instruction, guidance, and advice.', 'Practical experience with version control, preferably Git.', ' Ability to solve issues with a sense of urgency; utilizes and manages the available resources to make informed decisions and achieve superior results.', 'Other Locations', 'May actively partner with other analytical teams across the organization and/or participate in special projects.', 'Solid project management, organization and planning skills with the ability to manage multiple projects effectively and lead teams.', 'Maintains and expands capabilities for data testing, validation and quality control.', 'Experience with Linux servers on Google Cloud Platform or other cloud provider. ', 'Reporting Relationship', 'Skills, Knowledge & Abilities', 'Effectively communicates with project stakeholders by presenting complex or technical concepts in business terms.', 'Supports projects as Data Science SME; Collaborates with stakeholders to help define data requirements.', 'Supports process automation by leading smaller scale efforts to convert existing processes to Google Cloud Platform.', 'Education & Experience', 'Supervisory Position', 'Adheres to coding and documentation best practices', 'Unposting Date', 'Builds data pipelines and develop model features for Machine Learning projects. ', 'Typically up to two years of related work experience.', 'Ability to recommend analytical solutions to business problems and execute.', 'Solid knowledge of core functions of insurance companies and general insurance acumen', 'Job', 'Aggregates disparate structured and unstructured data to create meaningful datasets that drive analytical insights.']",Not Applicable,Full-time,Information Technology,Financial Services,2021-03-18 14:34:51
Data Engineer,Fearless,"Baltimore, MD",2 weeks ago,171 applicants,"['', 'Implement processes and systems to monitor data quality and volume, ensuring data is accurate and available', 'Compensation:', 'Experience working with at least one data analysis tool like Hadoop, Apache Spark, or cloud-provider equivalents', 'This position will be 100% remote during COVID.', ""What you'll be doing:"", 'This position is located in Baltimore.', 'Analyze stories and data and determines which are important to tell to provide value to our customers and will be able to share instances in which they have done this and the results', '100% coverage of the employee-only premium for HSA, HMO, or PPO plan and Employee Wellness Plan', 'Ability and willingness to work virtually and with online collaboration tools', 'Proficiency with programming languages including Python, R, SAS, and/or Java', 'Experience with Machine Learning and Deep Learning Tools and Software', 'Collaborate with data science engineers and business analysts to improve data models that solve challenges and support data-driven decision making', 'Support regular ad-hoc data and analysis needs to better understand customer behaviors. ', 'Experience in statistics, data processing, or data annotation', "" Cultural Interview - We're a people-first company, so we always start off by getting to know more about you, how you work, what your career goals are, and what you're passionate about. This is your opportunity to ask questions and get a feel for Fearless, so don't be shy! Technical Interview - This is where we get into the nitty gritty of the project. During the Technical Interview, you'll be interviewed by our Passion Coaches and/or the team's Project Lead to make sure your skills align with the project requirements. Business Interview - At this point, you've made it to the final frontier! The Business Interview is when you'll meet with Fearless leadership to dot the i's, cross the t's, and determine whether or not we'll be moving forward with the hiring process. "", 'Design and implement internal processes for automating data delivery and optimization', 'your ', 'This position will require a Public Trust Clearance that Fearless will sponsor.', 'Free parking in downtown Baltimore / public transit coverage', 'What you should know:', ' This position is located in Baltimore. This position has the flexibility to support some remote work / telecommuting. This position will be 100% remote during COVID. This position will require a Public Trust Clearance that Fearless will sponsor. ', ' Strong communication skills-both writing and orally what the data is communicating to many levels of stakeholders  Strong analytical and problem-solving skills with attention to detail Experience with Machine Learning and Deep Learning Tools and Software Experience in statistics, data processing, or data annotation Understanding of quality assurance with respect to data and models Proficiency with programming languages including Python, R, SAS, and/or Java Experience working with at least one data analysis tool like Hadoop, Apache Spark, or cloud-provider equivalents Effective organizational and time management skills with the ability to work independently, as well as with remote teams, under strict project deadlines Ability and willingness to work virtually and with online collaboration tools Ability to operate and manage work, strategically reason, and build relationships and influence others Empathy-recognizing that data has humans behind it ', 'Build and maintain secure tooling and infrastructure for scalable automated data pipelines and machine learning models that align with the business objectives', 'Effective organizational and time management skills with the ability to work independently, as well as with remote teams, under strict project deadlines', 'Ability to operate and manage work, strategically reason, and build relationships and influence others', 'Evaluate data and machine learning tools for efficacy', ""Cultural Interview - We're a people-first company, so we always start off by getting to know more about you, how you work, what your career goals are, and what you're passionate about. This is your opportunity to ask questions and get a feel for Fearless, so don't be shy!"", 'Understanding of quality assurance with respect to data and models', '3 weeks accrued PTO + 1 week sick leave + 10 federal holidays + your birthday off', ' Take initiative for their own growth through personal leadership Produce stories in Tableau (a story is a feature in Tableau that helps tell a literal story, and setting up data in tableau) Analyze stories and data and determines which are important to tell to provide value to our customers and will be able to share instances in which they have done this and the results Design and implement internal processes for automating data delivery and optimization Build and maintain secure tooling and infrastructure for scalable automated data pipelines and machine learning models that align with the business objectives Collaborate with data science engineers and business analysts to improve data models that solve challenges and support data-driven decision making Support regular ad-hoc data and analysis needs to better understand customer behaviors.  Implement processes and systems to monitor data quality and volume, ensuring data is accurate and available Perform data analysis required to troubleshoot data related issues and assist in the resolution of data issues. Evaluate data and machine learning tools for efficacy ', 'Empathy-recognizing that data has humans behind it', 'Business Interview', 'Take initiative for their own growth through personal leadership', 'Produce stories in Tableau (a story is a feature in Tableau that helps tell a literal story, and setting up data in tableau)', 'About Fearless:', 'Strong communication skills-both writing and orally what the data is communicating to many levels of stakeholders ', 'Family-friendly workplace', 'Flexible schedule', ""So, what's next?"", ""Why we're excited about you:"", ""Business Interview - At this point, you've made it to the final frontier! The Business Interview is when you'll meet with Fearless leadership to dot the i's, cross the t's, and determine whether or not we'll be moving forward with the hiring process."", 'Strong analytical and problem-solving skills with attention to detail', 'Cultural Interview', 'Safe Harbor 401(k) plan with employer contributions', 'Technical Interview', 'This position has the flexibility to support some remote work / telecommuting.', ""Technical Interview - This is where we get into the nitty gritty of the project. During the Technical Interview, you'll be interviewed by our Passion Coaches and/or the team's Project Lead to make sure your skills align with the project requirements."", ' Flexible schedule Family-friendly workplace 3 weeks accrued PTO + 1 week sick leave + 10 federal holidays + your birthday off 100% coverage of the employee-only premium for HSA, HMO, or PPO plan and Employee Wellness Plan Tech, education / training, and snack allowances Free parking in downtown Baltimore / public transit coverage Safe Harbor 401(k) plan with employer contributions ', 'Perform data analysis required to troubleshoot data related issues and assist in the resolution of data issues.', 'Why Fearless?', 'Tech, education / training, and snack allowances']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer II (Remote),American Specialty Health,United States,4 hours ago,28 applicants,"['', 'Matching 401(k) savings and retirement plan11 paid holidays annuallyPaid personal time off to be used for vacation, sick time, etc.Life, AD&D, and long-term disability insuranceEmployee discounts to local restaurants, movie theaters, fitness clubs, and theme parksPublic transportation reimbursement (up to $75 per month)Employee Assistance ProgramFlexible health care spending accountSoftware training classesMonthly seminarsTuition reimbursement', 'Stay abreast of BI industry best practices and relevant new technologies.', 'Paid personal time off to be used for vacation, sick time, etc.', 'San Diego Magazine.', 'If you are a qualified individual with a disability or a disabled veteran, you have the right to request an accommodation if you are unable or limited in your ability to use or access our career center as a result of your disability. To request an accommodation, contact our Human Resources Department at (800) 848-3555 x6702.\xa0\xa0', 'Strong knowledge of Microsoft Excel.', 'Work with SMEs (both technical and nontechnical) for knowledge transfer and documentation.', 'Since its inception in 1987, ASH has been based in San Diego and now has regional offices in Dallas and Indianapolis. Currently, ASH has over 1,500 employees. ASH subsidiaries operate in all 50 states, providing access to contracted practitioners with administrative platform for clients. ASH clients include more than 132 health plans nationwide.', 'If this current position does not fit your area of expertise, please visit us online to view our current list of available positions. www.ashcompanies.com', 'Full-time', 'Solid understanding of Inmon or Kimball methodologies.', 'Assist with BI Solutions (DBMS, SSIS, SSAS, SSRS, Tableau) development and performance tuning.', ""ASH has also been named among the 100 “Best Places to Work in Healthcare” by\xa0Modern Healthcare\xa0magazine, one of America’s 15 Fittest Companies by\xa0Men’s Fitness\xa0magazine, a National Business Group on Health Gold “Best Employers for Healthy Lifestyles” award winner, and a California Task Force on Youth and Workplace Wellness Fit Company. More locally, ASH has been named one of San Diego's best places to work by the\xa0San Diego Business Journal\xa0and\xa0San Diego Magazine."", 'Understanding of Agile development methodologies.', 'Participate in the entire lifecycle for Business Intelligence Solution Delivery.', 'Bachelor’s degree or higher in Computer Science, Computer Engineering, MIS or related field or equivalent work experience is preferred. If equivalent experience, high school diploma required.', 'Strong knowledge of data modeling.', '3 years of experience developing Business Intelligence solutions.', 'Design and develop reports/visualizations using SQL Server Reporting Services and Tableau.', 'Employee discounts to local restaurants, movie theaters, fitness clubs, and theme parks', 'Work with end-users to support adoption of team deliverables.', 'Basic understanding of security implementation in databases and BI solutions.', 'Medical, dental, and vision coverage', 'Basic understanding of dimensional data structures.', 'Health, Wellness and Fitness', 'Microsoft data-related certification preferred.', 'Employment Type', 'American Specialty Health is an Equal Opportunity/Affirmative Action Employer.\xa0\xa0', 'Qualifications', 'Remote Worker Considerations:', 'American Specialty Health Incorporated is seeking a Data Engineer II to join our Information Management department. This position will be responsible for delivering best-in-class analytical data solutions. The primary role will be developing new data assets, reporting system support, and maintenance, with opportunities to develop innovative solutions and explore new technologies. This position will have direct interaction with customers and end-users for training, demos and requirements gathering.', 'In addition to our more unique health care benefits, we provide employees with a wide array of more traditional benefits. They include:', 'Seniority LevelAssociateIndustryHealth, Wellness and FitnessEmployment TypeFull-timeJob Functions', 'Strong knowledge of data visualization practices and methods.', 'Data development, reporting system support, and maintenance.Participate in the entire lifecycle for Business Intelligence Solution Delivery.Design, build, document and manage data assets.Design and develop ETL processing solutions.Assist with BI Solutions (DBMS, SSIS, SSAS, SSRS, Tableau) development and performance tuning.Develop, test and deliver error free data.Stay abreast of BI industry best practices and relevant new technologies.Design and develop reports/visualizations using SQL Server Reporting Services and Tableau.Perform requirements gathering and analysis.Ability to work independently (as well as within a team environment) and unsupervised.Ability to work a flexible day schedule required including occasional evenings.Data modeling of enterprise data assets.Recommend and drive implementation of development techniques and methodologies.Design and develop high performance reporting assets.Work with SMEs (both technical and nontechnical) for knowledge transfer and documentation.Work with end-users to support adoption of team deliverables.Work with junior team members to develop and grow their skillsets.', 'Design and develop ETL processing solutions.', 'Perform requirements gathering and analysis.', '\xa0', 'Public transportation reimbursement (up to $75 per month)', 'San Diego Business Journal', 'The safety of our employees, both current and future, is ASH’s highest priority. At this time, most of our employees are working remotely due to the current COVID-19 pandemic. Candidates who are selected for this position will be trained remotely and must be able to work from home in a designated work area with company-provided technology equipment.', '6 years of developing solutions using the MS SQL Server BI Stack.', 'Bachelor’s degree or higher in Computer Science, Computer Engineering, MIS or related field or equivalent work experience is preferred. If equivalent experience, high school diploma required.3 years of experience developing Business Intelligence solutions.6 years of developing solutions using the MS SQL Server BI Stack.Microsoft data-related certification preferred.Strong knowledge of Microsoft Excel.Recent working knowledge of a major BI Tool (Tableau preferred).Strong technical skills; able to work with multiple platforms, integrate data with various sources.Strong problem solving and analytical skills; able to think outside of the box on a consistent basis.Willingness and ability to learn new technology and new programming languages; able to apply learnings to improve the data environment and pipelines.Strong knowledge of TSQL.Strong knowledge of data modeling.Strong knowledge of data visualization practices and methods.Basic understanding of security implementation in databases and BI solutions.Solid understanding of Inmon or Kimball methodologies.Basic understanding of dimensional data structures.Knowledge of ETL Design Patterns and Configurations.Basic understanding of multidimensional objects (cubes) and/or columnstore structures.Understanding of Agile development methodologies.', 'Matching 401(k) savings and retirement plan', 'Responsibilities', 'Work with junior team members to develop and grow their skillsets.', 'Design, build, document and manage data assets.', 'Software training classes', 'Life, AD&D, and long-term disability insurance', 'About ASH:', 'Industry', 'Tuition reimbursement', 'Strong knowledge of TSQL.', 'Data modeling of enterprise data assets.', 'Ability to work a flexible day schedule required including occasional evenings.', '11 paid holidays annually', 'Strong problem solving and analytical skills; able to think outside of the box on a consistent basis.', 'Strong technical skills; able to work with multiple platforms, integrate data with various sources.', 'Please view Equal Employment Opportunity Posters provided by OFCCP\xa0here.\xa0', 'Associate', 'Flexible health care spending account', 'Ability to work independently (as well as within a team environment) and unsupervised.', 'Basic understanding of multidimensional objects (cubes) and/or columnstore structures.', 'Develop, test and deliver error free data.', 'Recommend and drive implementation of development techniques and methodologies.', 'Willingness and ability to learn new technology and new programming languages; able to apply learnings to improve the data environment and pipelines.', 'Job Functions', 'All qualified applicants will receive consideration for employment without regard to race, color, religion, sex including sexual orientation and gender identity, national origin, disability, protected Veteran Status, or any other characteristic protected by applicable federal, state, or local law.\xa0', 'Design and develop high performance reporting assets.', 'Recent working knowledge of a major BI Tool (Tableau preferred).', 'Men’s Fitness', 'Data development, reporting system support, and maintenance.', 'This job posting is not applicable in CO', 'Seniority Level', 'Employee Assistance Program', 'American Specialty Health Incorporated (ASH) is a national health services company that provides population health management programs including prevention and wellness services, specialty health care management programs, and fitness and exercise services to health plans, insurance carriers, employer groups, and trust funds.', 'Monthly seminars', 'Knowledge of ETL Design Patterns and Configurations.', 'Modern Healthcare', 'So why not choose one of its finest companies to work for? Explore our website to find out more about our company, our values, and our open positions. Start changing lives—and your idea of “work”—today!']",Mid-Senior level,Full-time,Information Technology,"Health, Wellness and Fitness",2021-03-18 14:34:51
"Data Engineer, Analytics",Facebook,"Remote, OR",2 days ago,Be among the first 25 applicants,"['', 'Experience working with either a MapReduce or an MPP system.', '5+ years experience with object-oriented programming languages.', 'Collaborate with engineers, product managers, and data scientists to understand data needs, representing key data insights visually in a meaningful way.', 'Create and contribute to frameworks that improve the efficacy of logging data, while working with data infrastructure to triage issues and resolve.', '5+ years experience in the data warehouse space.', 'Experience analyzing data to identify gaps and inconsistencies.', 'Mentor team members by giving/receiving actionable feedback.', 'Knowledge and practical application of Python.', 'Experience working autonomously in global teams.', '5+ years experience with schema design and dimensional data modeling.', 'Responsibilities', 'Experience managing and communicating data warehouse plans to internal clients.', 'Conceptualize and own the data architecture for multiple large-scale projects, while evaluating design and operational cost-benefit tradeoffs within systems.Create and contribute to frameworks that improve the efficacy of logging data, while working with data infrastructure to triage issues and resolve.Collaborate with engineers, product managers, and data scientists to understand data needs, representing key data insights visually in a meaningful way.Define and manage SLA for all data sets in allocated areas of ownership.Determine and implement the security model based on privacy requirements, confirm safeguards are followed, address data quality issues, and evolve governance processes within allocated areas of ownership.Design, build, and launch collections of sophisticated data models and visualizations that support multiple use cases across different products or domains.Solve our most challenging data integration problems, utilizing optimal ETL patterns, frameworks, query techniques, sourcing from structured and unstructured data sources.Assist in owning existing processes running in production, optimizing complex code through advanced algorithmic concepts.Optimize pipelines, dashboards, frameworks, and systems to facilitate easier development of data artifacts.Influence product and cross-functional teams to identify data opportunities to drive impact.Mentor team members by giving/receiving actionable feedback.', '5+ years experience in custom ETL design, implementation and maintenance.', 'Influence product and cross-functional teams to identify data opportunities to drive impact.', 'Minimum Qualification', 'BS/BA in Technical Field, Computer Science or Mathematics.', 'Define and manage SLA for all data sets in allocated areas of ownership.', 'Conceptualize and own the data architecture for multiple large-scale projects, while evaluating design and operational cost-benefit tradeoffs within systems.', 'Preferred Qualification', 'Experience influencing product decisions with data.', 'Design, build, and launch collections of sophisticated data models and visualizations that support multiple use cases across different products or domains.', 'Determine and implement the security model based on privacy requirements, confirm safeguards are followed, address data quality issues, and evolve governance processes within allocated areas of ownership.', 'BS/BA in Technical Field, Computer Science or Mathematics.Experience working with either a MapReduce or an MPP system.Knowledge and practical application of Python.Experience working autonomously in global teams.Experience influencing product decisions with data.', 'Optimize pipelines, dashboards, frameworks, and systems to facilitate easier development of data artifacts.', '5+ years experience in writing SQL statements.', '5+ years experience in the data warehouse space.5+ years experience in custom ETL design, implementation and maintenance.5+ years experience with object-oriented programming languages.5+ years experience with schema design and dimensional data modeling.5+ years experience in writing SQL statements.Experience analyzing data to identify gaps and inconsistencies.Experience managing and communicating data warehouse plans to internal clients.', 'Assist in owning existing processes running in production, optimizing complex code through advanced algorithmic concepts.', 'Solve our most challenging data integration problems, utilizing optimal ETL patterns, frameworks, query techniques, sourcing from structured and unstructured data sources.']",Not Applicable,Full-time,Information Technology,Internet,2021-03-18 14:34:51
Senior Data Engineer - Remote,Futran Solutions,"Princeton, NJ",18 hours ago,Be among the first 25 applicants,"['', 'Job Summary', 'Job Functions And Responsibilities', 'Experience With Azure DevOps (pipelines) Highly Preferred']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer III,Walmart,"Bentonville, AR",6 days ago,Be among the first 25 applicants,"['', 'Data Source Identification: Supports the understanding of the priority order of requirements and service level agreements. Helps identify the most suitable source for data that is fit for purpose. Performs initial data quality checks on extracted data.', 'Data Governance: Supports the documentation of data governance processes. Supports the implementation of data governance practices.', ""Problem Formulation: Identifies possible options to address the business problems within one's discipline through analytics, big data analytics, and automation."", 'Code Development and Testing: Writes code to develop the required solution and application features by determining the appropriate programming language and leveraging business, technical, and data requirements. Creates test cases to review and validate the proposed solution design. Creates proofs of concept. Tests the code using the appropriate testing approach. Deploys software to production servers. Contributes code documentation, maintains playbooks, and provides timely progress updates.', 'Demonstrates up-to-date expertise and applies this to the development, execution, and improvement of action plans by providing expert advice and guidance to others in the application of information and best practices; supporting and aligning efforts to meet customer and business needs; and building commitment for perspectives and rationales.', 'Provides and supports the implementation of business solutions by building relationships and partnerships with key stakeholders; identifying business needs; determining and carrying out necessary processes and practices; monitoring progress and results; recognizing and capitalizing on improvement opportunities; and adapting to competing demands, organizational changes, and new responsibilities.', 'Models compliance with company policies and procedures and supports company mission, values, and standards of ethics and integrity by incorporating these into the development and implementation of business plans; using the Open Door Policy; and demonstrating and assisting others with how to apply these in executing business processes and practices.', 'Creates training documentation and trains end-users on data modeling. Oversees the tasks of less experienced programmers and stipulates system troubleshooting supports.', 'Data Strategy: Understands, articulates, and applies principles of the defined strategy to routine business problems that involve a single function.', 'Data Transformation and Integration: Extracts data from identified databases. Creates data pipelines and transform data to a structure that is relevant to the problem by selecting appropriate techniques. Develops knowledge of current data science and analytics trends.', 'Data Modeling: Analyzes complex data elements, systems, data flows, dependencies, and relationships to contribute to conceptual, physical, and logical data models. Develops the Logical Data Model and Physical Data Models including data warehouse and data mart designs. Defines relational tables, primary and foreign keys, and stored procedures to create a data model structure. Evaluates existing data models and physical databases for variances and discrepancies. Develops efficient data flows. Analyzes data-related system integration challenges and proposes appropriate solutions.', 'Applied Business Acumen: Supports the development of business cases and recommendations. Owns delivery of project activity and tasks assigned by others. Supports process updates and changes. Solves business issues.', ""Problem Formulation: Identifies possible options to address the business problems within one's discipline through analytics, big data analytics, and automation.Applied Business Acumen: Supports the development of business cases and recommendations. Owns delivery of project activity and tasks assigned by others. Supports process updates and changes. Solves business issues.Data Governance: Supports the documentation of data governance processes. Supports the implementation of data governance practices.Data Strategy: Understands, articulates, and applies principles of the defined strategy to routine business problems that involve a single function.Data Transformation and Integration: Extracts data from identified databases. Creates data pipelines and transform data to a structure that is relevant to the problem by selecting appropriate techniques. Develops knowledge of current data science and analytics trends.Data Source Identification: Supports the understanding of the priority order of requirements and service level agreements. Helps identify the most suitable source for data that is fit for purpose. Performs initial data quality checks on extracted data.Data Modeling: Analyzes complex data elements, systems, data flows, dependencies, and relationships to contribute to conceptual, physical, and logical data models. Develops the Logical Data Model and Physical Data Models including data warehouse and data mart designs. Defines relational tables, primary and foreign keys, and stored procedures to create a data model structure. Evaluates existing data models and physical databases for variances and discrepancies. Develops efficient data flows. Analyzes data-related system integration challenges and proposes appropriate solutions.Creates training documentation and trains end-users on data modeling. Oversees the tasks of less experienced programmers and stipulates system troubleshooting supports.Code Development and Testing: Writes code to develop the required solution and application features by determining the appropriate programming language and leveraging business, technical, and data requirements. Creates test cases to review and validate the proposed solution design. Creates proofs of concept. Tests the code using the appropriate testing approach. Deploys software to production servers. Contributes code documentation, maintains playbooks, and provides timely progress updates.Demonstrates up-to-date expertise and applies this to the development, execution, and improvement of action plans by providing expert advice and guidance to others in the application of information and best practices; supporting and aligning efforts to meet customer and business needs; and building commitment for perspectives and rationales.Provides and supports the implementation of business solutions by building relationships and partnerships with key stakeholders; identifying business needs; determining and carrying out necessary processes and practices; monitoring progress and results; recognizing and capitalizing on improvement opportunities; and adapting to competing demands, organizational changes, and new responsibilities.Models compliance with company policies and procedures and supports company mission, values, and standards of ethics and integrity by incorporating these into the development and implementation of business plans; using the Open Door Policy; and demonstrating and assisting others with how to apply these in executing business processes and practices."", ""Position Summary... What You'll Do..."", 'Outlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications. ', 'Outlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications. ']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer (SQL/Python),Averity,"New York, NY",,N/A,"['Who Are We?', 'Generous vacation', '$85,000 - $110,000 base salary (based on experience)Generous vacationFull Medical, Dental and Vision401(K) Matching', 'REQUIRED Experience in a technical & client-facing role 1+ yearsExperience working with Python and SQL in productionExperience involving large data sets for ETL & Data WarehousingIndependent mindset always finding ways to improve processesGood problem solver and effective communication skills', 'Good problem solver and effective communication skills', 'Full Medical, Dental and Vision', 'What Skills Do You Need?', 'We are an established Startup backed by some serious Venture Capital and are growing quickly due to bringing on board more big name clients. We are national but our head office is located here in the Flatiron area in Manhattan.', 'Independent mindset always finding ways to improve processes', 'Experience working with Python and SQL in production', 'REQUIRED Experience in a technical & client-facing role 1+ years', ""What's The Job?"", 'As a Data Engineer on our team you will be problem solving, troubleshooting and engaging regularly with Customer/Vendor teams and delivering on all technical requests. You will be working on creating ETL pipelines, ingest/export integrations, new product lines and orchestrating platform migrations. Also there is building different tools for automation and customer specific configurations.', 'Are you a Data Engineer that loves building data, writing code, implementing and building platforms? Do you also have great communication skills and can handle pressure well for customer facing responsibilities? Can you quickly deliver on customer requests? If so, then you will love this role on the team helping overcome problems and finding the solutions.', 'Experience involving large data sets for ETL & Data Warehousing', '$85,000 - $110,000 base salary (based on experience)', 'Compensation', '401(K) Matching', 'We are big proponents of diversity, and encourage diverse applicants / candidates with diverse backgrounds to apply.', '\xa0', 'We are a Customer Data Platform seeking to transform the industry. We provide ways to extract specific customer behavior data for large and well-known brands. This allows our clients to increase their marketing efforts and profitability.']",Associate,Full-time,Engineering,Staffing and Recruiting,2021-03-18 14:34:51
Data Engineer,CapTech Consulting,"Philadelphia, PA",6 days ago,Be among the first 25 applicants,"['', 'Experience tuning SQL queries to ensure performance and reliability', 'CapTech is an equal opportunity employer committed to fostering a culture of equality, inclusion and fairness — each foundational to our core values. We strive to create a diverse environment where each employee is encouraged to bring their unique ideas, backgrounds and experiences to the workplace.', 'Present programming documentation and design to team members and convey complex information in a clear and concise manner.', 'Design, develop, and implement data processing pipelines at scale', 'Participate in requirements gathering sessions with business and technical staff to distill technical requirements from business requests.', 'Competitive salary with performance-based bonus opportunities', 'Team building and social activities', 'Strong SQL development skills', 'Development experience building ETL graphs using the Ab Initio GDE, EME and Co-Operating system', 'Extract data from multiple sources, integrate disparate data into a common data model, and integrate data into a target database, application, or file using efficient programming processes.', 'Some of our technologies might include: Python, Cassandra, Spark, Java, Scala, Informatica, SQL Server, SSIS, Oracle, Kafka.', 'Partner with clients to fully understand business philosophy and IT Strategy; recommend process improvements to increase efficiency and reliability in ETL development.', 'Collaborate with Quality Assurance resources to debug code and ensure the timely delivery of products.', 'Software engineering best-practices, including version control (Git, TFS, JIRA, etc.) and test-driven development', 'Qualifications', 'Communicate with all levels of stakeholders as appropriate, including executives, data modelers, application developers, business users, and customers', 'Evaluate structured and unstructured datasets utilizing statistics, data mining, and predictive analytics to gain additional business insights', 'Competitive salary with performance-based bonus opportunitiesSingle and Family Health Insurance plans, including Dental coverageShort-Term and Long-Term disabilityMatching 401(k)Competitive Paid Time OffTraining and Certification opportunities eligible for expense reimbursementTeam building and social activitiesMentor program to help you develop your career', 'Company Description', 'Single and Family Health Insurance plans, including Dental coverage', 'Minimum of 4-5 years experience designing, developing, and testing software aligned with defined requirements', 'Short-Term and Long-Term disability', 'Write and refine code to ensure performance and reliability of data extraction and processing.', 'Development experience with Unix tools and shell scripts', 'Mentor program to help you develop your career', 'Competitive Paid Time Off', 'Development experience with at least two different programming languages (Python, Java, Scala, etc.)', 'Design, develop, document, and test advanced data systems that bring together data from disparate sources, making it available to data scientists, analysts, and other users using scripting and/or programming languages (Python, Java, Scala, etc)Evaluate structured and unstructured datasets utilizing statistics, data mining, and predictive analytics to gain additional business insightsDesign, develop, and implement data processing pipelines at scalePresent programming documentation and design to team members and convey complex information in a clear and concise manner.Extract data from multiple sources, integrate disparate data into a common data model, and integrate data into a target database, application, or file using efficient programming processes.Write and refine code to ensure performance and reliability of data extraction and processing.Communicate with all levels of stakeholders as appropriate, including executives, data modelers, application developers, business users, and customersParticipate in requirements gathering sessions with business and technical staff to distill technical requirements from business requests.Partner with clients to fully understand business philosophy and IT Strategy; recommend process improvements to increase efficiency and reliability in ETL development.Collaborate with Quality Assurance resources to debug code and ensure the timely delivery of products.Some of our technologies might include: Python, Cassandra, Spark, Java, Scala, Informatica, SQL Server, SSIS, Oracle, Kafka.', ""Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experienceDevelopment experience building ETL graphs using the Ab Initio GDE, EME and Co-Operating systemStrong SQL development skillsDevelopment experience with at least two different programming languages (Python, Java, Scala, etc.)Development experience with Unix tools and shell scriptsDevelopment experience with at least two different database platforms (Teradata, Oracle, MySQL, MS SQL, etc.)Minimum of 4-5 years experience designing, developing, and testing software aligned with defined requirementsExperience tuning SQL queries to ensure performance and reliabilitySoftware engineering best-practices, including version control (Git, TFS, JIRA, etc.) and test-driven development"", ""Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience"", 'Job Description', 'Development experience with at least two different database platforms (Teradata, Oracle, MySQL, MS SQL, etc.)', 'Matching 401(k)', 'Design, develop, document, and test advanced data systems that bring together data from disparate sources, making it available to data scientists, analysts, and other users using scripting and/or programming languages (Python, Java, Scala, etc)', 'Specific Responsibilities For The Data Engineer, Analytics Position Include', 'Training and Certification opportunities eligible for expense reimbursement']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Envestnet | Yodlee,"Sparks, MD",1 week ago,49 applicants,"['', 'Performance optimization for queries and dashboards Develop and deliver clear, compelling briefings to internal and external stakeholders on findings, recommendations, and solutions ', 'Data streaming technologies Big Data technologies including, Hadoop, Spark, Hive, Teradata, etc. ', 'Proven work experience in the following: ', 'Knowledge of CI/CD and related tools such as Gitlab, AWS CodeCommit, etc ', 'Frameworks: Hadoop, Spark, Kafka ', 'Deliver end-to-end data and analytics capabilities, including data ingest, data transformation, data science, and data visualization in collaboration with Data and Analytics stakeholder groups ', 'Design and deploy databases and data pipelines to support analytics projects Develop scalable and fault-tolerant workflows Clearly document issues, solutions, findings and recommendations to be shared internally & externally Learn and apply tools and technologies proficiently, including: ', 'Develop and communicate solutions architectures and present solutions to both business and technical stakeholders ', 'Clearly document issues, solutions, findings and recommendations to be shared internally & externally ', 'Performance optimization for queries and dashboards ', 'Data streaming technologies ', 'Test and validate data pipelines, transformations, datasets, reports, and dashboards built by team ', 'Analyze client data & systems to determine whether requirements can be met Test and validate data pipelines, transformations, datasets, reports, and dashboards built by team Develop and communicate solutions architectures and present solutions to both business and technical stakeholders Provide end user support to other data engineers and analysts ', 'Provide end user support to other data engineers and analysts ', 'AWS Solutions Architect / Developer / Data Analytics Specialty certifications, Professional certification is a plus ', 'Develop and deliver clear, compelling briefings to internal and external stakeholders on findings, recommendations, and solutions ', 'SQL, Python, PySpark. Other programming languages (R, Scala, SAS, Java, etc.) are a plus Data and analytics technologies including SQL/NoSQL/Graph databases, ETL, and BI Knowledge of CI/CD and related tools such as Gitlab, AWS CodeCommit, etc ', 'Linux command-line operations ', 'AWS services including EMR, Glue, Athena, Batch, Lambda Cloudwatch, DynamoDB, EC2, Cloudformation, IAM and EDS ', 'Develop scalable and fault-tolerant workflows ', 'Bachelor Degree in Computer Science relevant field, Masters Degree is a plus ', 'Tools/Products: Data Science Studio, Alteryx, Jupyter, Tableau, PowerBI ', 'Candidate should be able to lead the team, communicate with business, gather and interpret business requirements Experience with agile delivery methodologies using Jira or similar tools Experience working with remote teams ', 'Solid scripting skills (e.g., bash/shell scripts, Python) ', 'Learn and apply tools and technologies proficiently, including: ', 'Data and analytics technologies including SQL/NoSQL/Graph databases, ETL, and BI ', 'AWS Solutions Architect / Developer / Data Analytics Specialty certifications, Professional certification is a plus Bachelor Degree in Computer Science relevant field, Masters Degree is a plus ', 'Description', 'Networking knowledge (OSI network layers, TCP/IP, virtualization) ', ' is a plus', 'Languages: SQL (standard and DB-specific), Python, Scala, Bash ', 'Experience with agile delivery methodologies using Jira or similar tools ', 'Professional certification is a plus', 'Analyze client data & systems to determine whether requirements can be met ', 'Candidate should be able to lead the team, communicate with business, gather and interpret business requirements ', 'Experience working with remote teams ', 'Linux command-line operations Networking knowledge (OSI network layers, TCP/IP, virtualization) ', 'SQL, Python, PySpark. Other programming languages (R, Scala, SAS, Java, etc.) are a plus ', 'Solid scripting skills (e.g., bash/shell scripts, Python) Proven work experience in the following: ', 'Design and deploy databases and data pipelines to support analytics projects ', 'Expert experience in the following: ', 'plus', 'Cloud Computing: AWS ', 'Big Data technologies including, Hadoop, Spark, Hive, Teradata, etc. ', 'Frameworks: Hadoop, Spark, Kafka Cloud Computing: AWS Tools/Products: Data Science Studio, Alteryx, Jupyter, Tableau, PowerBI ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,ettain group,"Charlotte, NC",23 hours ago,103 applicants,"['', 'Preferred Qualifications:', '4 years of Hadoop experience', '6 years of experience in systems analysis, including defining technical requirements and performing high level design for complex solutions', ""Bachelor's Degree in Engineering, Computer Science, CIS, or related field"", '4 years of experience in Hadoop or any Cloud Bigdata components (specific to the Data Engineering role)', ""Bachelor's Degree in Engineering, Computer Science, CIS, or related field7 years of experience in Data Engineering, Data Warehousing/ETL, or Software Engineering6 years of experience working on project(s) involving the implementation of solutions applying development life cycles (SDLC)4 years of Hadoop experience"", 'Ingestion data from 3rd\xa0party and internal systems to support pro selling folks in stores and marketing department to manage marketing spend\xa0Internal dashboarding and integrations with 3rd\xa0party solutions to provide data to decision makers', ""Master's Degree in Computer Science, CIS, or related field"", '6 years of experience working on project(s) involving the implementation of solutions applying development life cycles (SDLC)', 'Internal dashboarding and integrations with 3rd\xa0party solutions to provide data to decision makers', '4 years of experience working with an IT Infrastructure Library (ITIL) framework', '6 years of experience writing technical documentation in a software development environment', 'Experience working with Continuous Integration/Continuous Deployment tools', '4 years of experience leading teams, with or without direct reports', '6 years of IT experience developing and implementing business systems within an organization', '6 years of experience working with source code control systems', 'Data Engineering', 'Qualifications:', 'Job Summary:', '6 years of experience working with defect or incident tracking software', '2 years of experience playing a lead role in projects (specific to the Data Engineering role)', 'Ingestion data from 3rd\xa0party and internal systems to support pro selling folks in stores and marketing department to manage marketing spend\xa0', '\xa0', '7 years of experience in Data Engineering, Data Warehousing/ETL, or Software Engineering', ""Master's Degree in Computer Science, CIS, or related field6 years of IT experience developing and implementing business systems within an organization6 years of experience working with defect or incident tracking software6 years of experience writing technical documentation in a software development environment4 years of experience working with an IT Infrastructure Library (ITIL) framework4 years of experience leading teams, with or without direct reports6 years of experience working with source code control systemsExperience working with Continuous Integration/Continuous Deployment tools6 years of experience in systems analysis, including defining technical requirements and performing high level design for complex solutionsData Engineering4 years of experience in Hadoop or any Cloud Bigdata components (specific to the Data Engineering role)2 years of experience playing a lead role in projects (specific to the Data Engineering role)Expertise in Java/Scala/Python, SQL, Scripting, Teradata, Hadoop (Sqoop, Hive, Pig, Map Reduce), Spark (Spark Streaming, MLib), Kafka or equivalent Cloud Bigdata components"", 'The primary purpose of this role is to provide consultation and technical advice on translating business requirements and functional specifications into logical program designs. This includes facilitating the implementation and maintenance of complex business and enterprise data solutions to ensure successful deployment of released applications. This role serves as a technical expert for project teams throughout the implementation and maintenance of business and enterprise Data or Platform solutions. In addition, this role personally develops and delivers modules, stable application systems, and integrated enterprise Data or Platform solutions within various computing environments.', 'Expertise in Java/Scala/Python, SQL, Scripting, Teradata, Hadoop (Sqoop, Hive, Pig, Map Reduce), Spark (Spark Streaming, MLib), Kafka or equivalent Cloud Bigdata components']",Mid-Senior level,Contract,Information Technology,Retail,2021-03-18 14:34:51
Data Engineer,Tencent,"Los Angeles, CA",3 weeks ago,186 applicants,"['·\xa02+ years’ experience in custom ETL design, implementation and maintenance.', '·\xa0Communicate, at scale, through multiple mediums: Presentations, dashboards, company-wide datasets, bots and more.', 'Responsibility:', 'Requirements:', '·\xa02+ years of SQL experience.', '·\xa0Build data expertise and own data quality for your areas.', '\xa0', 'Tencent Games is looking for Data Engineer (DE) with 2+ years’ experience:', '·\xa0Leverage data and business principles to solve large scale web, mobile and data infrastructure problems.', '·\xa02+ years of Python development experience.', '·\xa0Educate your partners: Use your data and analytics experience to ‘see what’s missing’, identifying and addressing gaps in their existing logging and processes.', '·\xa0Experience querying massive datasets using Spark, Hadoop, Presto, Hive, Impala, etc.', '·\xa02+ years of experience with workflow management engines (i.e. Airflow, Luigi, Prefect, Dagster, digdag.io, Google Cloud Composer, AWS Step Functions, Azure Data Factory, UC4, Control-M).', '·\xa0Design, build and launch extremely efficient and reliable data pipelines to move data across a number of platforms including Data Warehouse, online caches and real-time systems.']",Mid-Senior level,Full-time,Information Technology,Internet,2021-03-18 14:34:51
Data Engineer,North Highland,"Portland, OR",23 hours ago,32 applicants,"['', ' Background in Gas Utilities industry preferred. ', ' MAKE CHANGE HAPPEN. ', '  Ability to understand and translate the need of business units into effective data management solutions.   Experience in data validation and ability to analyze large datasets.   Excellent interpersonal skills, including collaboration, facilitation, and negotiation.   5+ years of experience as a Data Engineer or in a similar role   Informatica development and administration experience.   Experience with data modeling, data warehousing, and building ETL pipelines.   Experience in SQL.   Experience with data modeling, data warehouse design, and building data marts and data pipelines.   Design and implement system modifications.   Proficiency with RDBMS technology fundamentals, SQL and Query optimization.   Strong troubleshooting and diagnostic skills to determine and resolve issues.   Experience supporting, architecting, and designing requirements with respect to the flow of data between SAP and non-SAP systems preferred.   Ability to optimize the performance of enterprise business intelligence solutions by means redesigning or re-architecting data structures or data flows.   Experience working in the Microsoft Azure cloud environment.   Background in Gas Utilities industry preferred.   Understanding and knowledge of IT standards and controls, IT Service Management [ITSM, ITIL] methods  ', ' Applicants must be authorized to work in the United States without the need for visa sponsorship by North Highland. Work visa sponsorship will not be provided, now or in the future, for this position. North Highland makes change happen for organizations who dare to be different. By melding workforce, customer and operational transformation, they are one of the world’s leading consulting groups, with 65+ offices around the globe. They break new ground today so tomorrow is easier to explore.For more information, visit northhighland.com and connect with us on LinkedIn , Twitter and Facebook .North Highland is an Equal Employment Opportunity (EEO)/Affirmative Action employer. All qualified applicants will receive fair and impartial consideration without regard to race, color, sex, gender identity, religion, national origin, age, sexual orientation, disability, veteran status, or any other characteristic protected by law.', ' Experience with data modeling, data warehousing, and building ETL pipelines. ', ' 5+ years of experience as a Data Engineer or in a similar role ', 'Skills Needed', ' Ability to understand and translate the need of business units into effective data management solutions. ', ' Experience in data validation and ability to analyze large datasets. ', ' Proficiency with RDBMS technology fundamentals, SQL and Query optimization. ', 'LEAVE YOUR MARK ON A BETTER WORLD. ', ' to apply ', '  Applicants must be authorized to work in the United States without the need for visa sponsorship by North Highland. Work visa sponsorship will not be provided, now or in the future, for this position. North Highland makes change happen for organizations who dare to be different. By melding workforce, customer and operational transformation, they are one of the world’s leading consulting groups, with 65+ offices around the globe. They break new ground today so tomorrow is easier to explore.For more information, visit northhighland.com and connect with us on LinkedIn , Twitter and Facebook .North Highland is an Equal Employment Opportunity (EEO)/Affirmative Action employer. All qualified applicants will receive fair and impartial consideration without regard to race, color, sex, gender identity, religion, national origin, age, sexual orientation, disability, veteran status, or any other characteristic protected by law.', ' HERE ', ' COLLABORATE WITH AMAZING PEOPLE. ', ' Experience with data modeling, data warehouse design, and building data marts and data pipelines. ', ' Design and implement system modifications. ', ' Informatica development and administration experience. ', ' Ability to optimize the performance of enterprise business intelligence solutions by means redesigning or re-architecting data structures or data flows. ', ' Strong troubleshooting and diagnostic skills to determine and resolve issues. ', ' Experience working in the Microsoft Azure cloud environment. ', ' Experience supporting, architecting, and designing requirements with respect to the flow of data between SAP and non-SAP systems preferred. ', ' Excellent interpersonal skills, including collaboration, facilitation, and negotiation. ', ' Click ', ' Experience in SQL. ', ' Understanding and knowledge of IT standards and controls, IT Service Management [ITSM, ITIL] methods ']",Entry level,Contract,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Massachusetts General Hospital,"Boston, MA",6 days ago,103 applicants,"['', '-You may have experience with Big Data frameworks/Hadoop-based technologies (Spark, Kafka, Hive, Hbase, Sqoop, Ranger, HDFS)', '-You are fluent in Python and can pick up other languages with ease\xa0', '- Aggregate and transform raw data coming from a variety of data sources to fulfill the functional & non-functional requirements (e.g., Microsoft SQL, Apache Hive, Apache HBase, Enterprise Data Warehouse, bedside monitors (HL7), EEG recordings (waveforms), web services, and others).', '-You enjoy working cooperatively in a small collaborative team.', '-You are highly organized', '-You are fluent with data pre-processing (quality checks, formatting, joining, etc.) and can prep your own data for analysis', ""-You are able to explain/defended the analytical methodology choices you've made and the resulting findings to technical and non-technical audiences"", '-You have worked in roles where your advanced analytics skills are used regularly and you have a track record of expanding responsibilities', '-You write clear technical documentation.', '-You have experience working with large structured, semi-structured & unstructured data.\xa0', '-You may have experience in the health industry and are excited to deepen your knowledge of brain science', '-You have strong verbal and written communication abilities', 'The Massachusetts General Hospital (MGH) Neurology Department, an affiliate of Harvard Medical School and a world leader in brain research, is seeking a big data software engineer to join\xa0the Clinical Data Animation Center (CDAC). CDAC’s mission is to bring the power of medical Big Data to MGH Neurology. CDAC enables research by providing infrastructure, tools and expertise for working with medical Big Data. Our team builds the data resources, models, algorithms, and pipelines needed to bring medical Big Data to life to help patients. We are looking for a big data software engineer who can thrive as part of a cross-functional team, working alongside experts in neurology, neuroscience, statistics, time series analysis, machine learning, and computer science.', '-You are a creative problem-solver.', '-Mentoring teammates with less engineering experience and share knowledge of computer science best practices with the team.', '-Engaging in all facets of software development: business analysis, requirements gathering, functional and technical specification, infrastructure definition, data architecture design, development, implementation, testing, deployment, and support of new applications.', 'The Clinical Data Animation Center (CDAC) at Massachusetts General Hospital (MGH) and the MGH McCance Center for Brain Health and MGH Neurology Department is\xa0', '-You probably have an M.S./M.A. or Ph.D. (or equivalent experience) in a quantitative field (Math, Statistics, EE, etc.)', '-You have experience working in an Agile/Scrum environment', '- Developing and deploying software that interacts with clinical information systems, databases, and mobile devices', 'About you:\xa0', '-You are comfortable with probability and linear algebra', 'Responsibilities include:', '-Refine software development processes and best practices.', '-You have experience working in the cloud (e.g. GCP, AWS, Azure)\xa0', '-You can discuss basic statistical concepts such as p-values and interpretation of linear regression model coefficients\xa0', '-You are proficient in Python, Apache Spark, shell scripting (Bash), and SQL\xa0', '-You have excellent written and verbal communication skills and are comfortable presenting to external senior leadership-level audiences', '-You may have experience with healthcare data (HL7 messaging, DICOM, FHIR)', '-You enjoy learning about data science and prioritize keeping your knowledge fresh/up-to-date', '-You have a strong understanding of data visualization principles', '- Create and maintain related documentation on Confluence including software specifications, pipeline diagrams, dataflow diagrams, integration schemas, interoperability relationships, etc.', '- Developing software for real-time processing of medical data from with clinical devices including EEG brain monitoring devices, bedside ICU monitors, and mobile health monitoring devices.', '- Design, create, build, integrate, maintain and optimize multiple ETL data pipelines.', '-You may have experience working with signal or time-series data']",Not Applicable,Full-time,Hospital & Health Care,N/A,2021-03-18 14:34:51
Data Engineer,Kforce Inc,"Sandy, UT",1 day ago,Be among the first 25 applicants,"['', ' Minimum of 3 years of experience with ETL tool (SSIS, Informatica, etc.) ', ' Experience implementing ETL for Data Warehouse and Business Intelligence solutions ', ' Ability to read and write effective, modular, dynamic (parameterized) and robust code, establish and follow already established code standards, and ETL framework ', ' Design ETL jobs and reusable components to implement specified business requirements ', ' Experience in data streaming services', ' Experience with Reporting tools. PowerBI or Tableau preferred', 'Responsibilities', ' Create technical specifications documents and design process diagrams', ' Experience in data classification & handling around data types included in HIPAA, PCI, CCPA, and other types', ' Strong analytical and problem solving skills ', ' Troubleshoot and optimize ETL code; Interpret ETL logs, perform data validation, dissect code, understand the benefits and drawbacks of parallelism, apply best practices using change data capture, expressions, scoping of variables, commonly used transforms, event handlers and logging providers, understand and optimize the surrogate key generation and inconsistent data type handling', 'Requirements', ' Experience in RDBMS design and development, and performance tuning', ' Deep familiarity with database technologies and cloud infrastructure', ' Experience with scheduling ETL jobs using a scheduling tool ', ' Ability to work independently and as part of a team', ' Experience working with unstructured and semi structured data sets', "" Bachelor's degree in Information Systems or related discipline preferred (or equivalent work experience)"", ' Comfortable with remote, hybrid work environments', ' Develop scripts for data file processing and process integration tasks', ' Experience writing ETL jobs from Snowflake, Teradata, SQL Server, MySQL, Postgres, CSV files, etc.', ' Develop functional specifications for data acquisition, transformation and load processes', ' Industry related certifications a plus', ' Experience implementing solutions in cloud infrastructure. AWS, Azure, GCP, etc.', '  Solidify and extend existing ETL Processes and Framework   Design ETL jobs and reusable components to implement specified business requirements   Troubleshoot and optimize ETL code; Interpret ETL logs, perform data validation, dissect code, understand the benefits and drawbacks of parallelism, apply best practices using change data capture, expressions, scoping of variables, commonly used transforms, event handlers and logging providers, understand and optimize the surrogate key generation and inconsistent data type handling  Create technical specifications documents and design process diagrams  Develop functional specifications for data acquisition, transformation and load processes  Develop scripts for data file processing and process integration tasks ', ' Solidify and extend existing ETL Processes and Framework ', ""  Bachelor's degree in Information Systems or related discipline preferred (or equivalent work experience)  Industry related certifications a plus  Minimum of 3 years of experience with ETL tool (SSIS, Informatica, etc.)   Experience implementing ETL for Data Warehouse and Business Intelligence solutions   Experience in RDBMS design and development, and performance tuning  Deep familiarity with database technologies and cloud infrastructure  Experience in data classification & handling around data types included in HIPAA, PCI, CCPA, and other types  Experience with scheduling ETL jobs using a scheduling tool   Experience in data streaming services  Ability to read and write effective, modular, dynamic (parameterized) and robust code, establish and follow already established code standards, and ETL framework   Strong analytical and problem solving skills   Ability to work independently and as part of a team  Comfortable with remote, hybrid work environments  Experience writing ETL jobs from Snowflake, Teradata, SQL Server, MySQL, Postgres, CSV files, etc.  Experience implementing solutions in cloud infrastructure. AWS, Azure, GCP, etc.  Experience working with unstructured and semi structured data sets  Experience with Reporting tools. PowerBI or Tableau preferred ""]",Associate,Full-time,Information Technology,Media Production,2021-03-18 14:34:51
Data Analytics Engineer,Breakthrough,"Green Bay, WI",2 days ago,Be among the first 25 applicants,"['', 'Desired Skills and Experience:', 'Configuration of visualization tools such as Looker, Data Studio, Tableau or PowerBI.', 'ABOUT BREAKTHROUGH:', 'Streamline and integrate data management and data analytics from end to end. ', 'Experience in data ingestion and data integration strategies.', 'Ensure ongoing alignment to data management best practices while developing and implementing data policies, standards, catalogs and critical metrics.', 'WHY WORK AT BREAKTHROUGH? See our SMART, PASSIONATE, and EDGY team', 'Knowledge of data modeling concepts, SQL and data warehousing.', 'Prepare data to identify patterns and trends in data sets.', 'Contribute to the refinement, development and operationalization of the digital data management and analytics vision and strategy.', 'Transform source system data into tested and documented datasets based on business needs.', 'Own the maintenance, design and continued development of the reporting structures in close collaboration with the Data Engineering team.', 'Preparation of disparate data sources for analysis.', 'Degree in Engineering, Computer Science or similar technical field.', 'Degree in Engineering, Computer Science or similar technical field.3-5 years of experience in a similar role.Knowledge of data modeling concepts, SQL and data warehousing.Knowledge of Google BigQuery a plus.Experience in data ingestion and data integration strategies.', 'Define new data collection and analysis processes.', 'Knowledge of Google BigQuery a plus.', '3-5 years of experience in a similar role.', 'Accountable for the ongoing architecture, implementation and configuration of data analytics using dbt.', 'Responsibilities Include:', 'Accountable for the ongoing architecture, implementation and configuration of data analytics using dbt.Transform source system data into tested and documented datasets based on business needs.Build, monitor and maintain reliable data pipelines for highly available reporting or data integrations.Configuration of visualization tools such as Looker, Data Studio, Tableau or PowerBI.Preparation of disparate data sources for analysis.Prepare data to identify patterns and trends in data sets.Define new data collection and analysis processes.Streamline and integrate data management and data analytics from end to end. Own the maintenance, design and continued development of the reporting structures in close collaboration with the Data Engineering team.Contribute to the refinement, development and operationalization of the digital data management and analytics vision and strategy.Ensure ongoing alignment to data management best practices while developing and implementing data policies, standards, catalogs and critical metrics.', 'Build, monitor and maintain reliable data pipelines for highly available reporting or data integrations.']",Mid-Senior level,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Software and Data Engineer - Telecommute,Optum,"Hartford, CT",2 days ago,Be among the first 25 applicants,"['', 'Demonstrated understanding and experience using any of the following: DataStage, Mainframe, Cobol, SQL, Unix, or MicroStrategy', ""your life's best work.(sm)"", 'Ability to perform root cause analysis on external and internal processes and data to identify opportunities for improvement and answer questions', 'Ability to QA and troubleshoot data', 'Ability to set and meet deadlines', 'Primary Responsibilities', 'Workflow management/pipeline tools: Airflow and/or TWS', 'Maintain high quality documentation of data definitions, transformations, and processes to ensure data governance and security', ' Accountable for data engineering lifecycle including research and analysis, proof of concepts, architecture, design, development, test, deployment and maintenance Design, develop, implement and run cross-domain, flexible, scalable, secure, reliable and quality data solutions that transform data for meaningful analyses and analytics while ensuring operability Design, develop, implement and run data solutions that improve data efficiency, reliability and quality, and are performant by design Build processes and diagnostics tools to troubleshoot, maintain and optimize solutions and respond to customer and production issues Embrace continuous learning of engineering practices to ensure industry best practices and technology adoption, including DevOps, Cloud and Agile thinking  Maintain high quality documentation of data definitions, transformations, and processes to ensure data governance and security Assembling large, complex sets of data that meet non-functional and functional business requirements Ability to perform root cause analysis on external and internal processes and data to identify opportunities for improvement and answer questions Excellent analytic skills associated with working on complex datasets Ability to build processes that support data transformation, workload management, data structures, dependency and metadata Operational support for web application / portal as well as reporting functionality for internal and external end user base ', 'Experience in DevOps and with Agile Methodologies', ' Experience working in the healthcare industry Experience working with stakeholders including data, design, product and executive teams and assisting them with data-related technical issues Experience in DevOps and with Agile Methodologies Experience with Snowflake Building and supporting required infrastructure for optimal extraction, transformation and loading of data from various data sources Technical writing skills Ability to work in high-pressure situations ', 'Experience working in the healthcare industry', 'Ability to build processes that support data transformation, workload management, data structures, dependency and metadata', 'Analytical and problem-solving skills', 'Design, develop, implement and run cross-domain, flexible, scalable, secure, reliable and quality data solutions that transform data for meaningful analyses and analytics while ensuring operability', 'Accountable for data engineering lifecycle including research and analysis, proof of concepts, architecture, design, development, test, deployment and maintenance', 'Assembling large, complex sets of data that meet non-functional and functional business requirements', 'Building and supporting required infrastructure for optimal extraction, transformation and loading of data from various data sources', 'All Telecommuters will be required to adhere to UnitedHealth Group’s Telecommuter Policy', 'Excellent analytic skills associated with working on complex datasets', 'Experience with Datamining tools on either Platforms: Oracle or Teradata', 'Experience working with stakeholders including data, design, product and executive teams and assisting them with data-related technical issues', 'Experience of data gathering, cleansing, and transforming techniques', 'Experience with Snowflake', 'Embrace continuous learning of engineering practices to ensure industry best practices and technology adoption, including DevOps, Cloud and Agile thinking ', 'Required Qualifications', 'Operational support for web application / portal as well as reporting functionality for internal and external end user base', 'Experience with Java, Python, GitHub, Jenkins', 'Design, develop, implement and run data solutions that improve data efficiency, reliability and quality, and are performant by design', 'If you need to enter a work site for any reason, you will be required to screen for symptoms using the ProtectWell mobile app, Interactive Voice Response (i.e., entering your symptoms via phone system) or similar UnitedHealth Group-approved symptom screener. When in a UnitedHealth Group building, employees are required to wear a mask in common areas. In addition, employees must comply with any state and local masking orders', 'Ability to work in high-pressure situations', 'Technology Careers with Optum. ', 'Diversity creates a healthier atmosphere: Optum is an Equal Employment Opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law. Optum is a drug-free workplace. © 2021 Optum. All rights reserved.', ' Demonstrated understanding and experience using any of the following: DataStage, Mainframe, Cobol, SQL, Unix, or MicroStrategy Experience with Java, Python, GitHub, Jenkins Experience with Datamining tools on either Platforms: Oracle or Teradata Experience of data gathering, cleansing, and transforming techniques Workflow management/pipeline tools: Airflow and/or TWS Analytical and problem-solving skills Understanding of data warehousing and ETL techniques Ability to set and meet deadlines Ability to QA and troubleshoot data If you need to enter a work site for any reason, you will be required to screen for symptoms using the ProtectWell mobile app, Interactive Voice Response (i.e., entering your symptoms via phone system) or similar UnitedHealth Group-approved symptom screener. When in a UnitedHealth Group building, employees are required to wear a mask in common areas. In addition, employees must comply with any state and local masking orders ', 'Build processes and diagnostics tools to troubleshoot, maintain and optimize solutions and respond to customer and production issues', 'Understanding of data warehousing and ETL techniques', 'Preferred Qualifications', 'Technical writing skills']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Launch Consulting Group,"Bellevue, WA",2 days ago,48 applicants,"['', 'Excellent communication skills both written and oral ', 'Bachelor’s Degree in Computer Sciences, Analytics, Systems Eng., Statistics or related field ', 'Required Experience/Qualifications', ' 3+ years of experience in working as an data engineering or analytics team member working with cross functional teams  3+ years of SQL DW/DB development or equivalent  3+ years of experience designing solutions in Azure specializing in any of the following technologies: Azure Data Factory, Azure Data Lake, Azure SQL & Azure SQL Data Warehouse, Azure Functions  2+ years of Spark, Python and PowerShell development or equivalent  Bachelor’s Degree in Computer Sciences, Analytics, Systems Eng., Statistics or related field  Strong attention to detail and sense of urgency  Excellent communication skills both written and oral  Hands-on experience working with CICD technologies in the data engineering space ', 'Work with top-notch technical professionals developing complex systems at scale and with a focus on sustained operational excellence', 'Hands-on experience working with CICD technologies in the data engineering space', 'Responsibilities', 'Design & implement Azure Data Factory orchestration processes/tasks with a focus on data quality, master data management, and data lineage as components of the overall data pipeline ', 'We are Navigators in the Age of Transformation.', 'Please refer to job #4328', '2+ years of Spark, Python and PowerShell development or equivalent ', 'DATA ENGINEER ', 'Explore and learn the latest Azure technologies to provide new capabilities and increase efficiency', 'Participate in data governance; ensure all existing data is created in the right way, and that new data is created according to appropriate standards and with proper documentation ', 'Participate in implementation, and support of a data warehouse/data base and analytics platform utilizing Azure cloud technology ', '3+ years of experience in working as an data engineering or analytics team member working with cross functional teams ', 'Strong attention to detail and sense of urgency ', 'Read, write, and configure code for end-to-end service telemetry, alerting and self-healing capabilities ', 'Collaborate with Data Analysts & Data Scientists; for products that require reporting data or AI/ML models to ensure that datasets are in place and are used consistently internally/externally ', ' Participate in implementation, and support of a data warehouse/data base and analytics platform utilizing Azure cloud technology  Design and implement data load processes from Cloud or On Premises data sources into Azure Data Lake and subsequent Azure SQL & SQL Data Warehouse  Migrate existing processes and data from On Premises SQL Server and other environments to Azure Data Lake and ultimately Azure data warehouse or database solutions  Design & implement Azure Data Factory orchestration processes/tasks with a focus on data quality, master data management, and data lineage as components of the overall data pipeline  Explore and learn the latest Azure technologies to provide new capabilities and increase efficiency Work with top-notch technical professionals developing complex systems at scale and with a focus on sustained operational excellence Collaborate with Data Analysts & Data Scientists; for products that require reporting data or AI/ML models to ensure that datasets are in place and are used consistently internally/externally  Participate in data governance; ensure all existing data is created in the right way, and that new data is created according to appropriate standards and with proper documentation  Read, write, and configure code for end-to-end service telemetry, alerting and self-healing capabilities  Strive for continuous improvement of code quality and development practices  Help continually improve ongoing data platform, analysis and reporting processes, automating or simplifying self-service support for customers ', 'Strive for continuous improvement of code quality and development practices ', 'Design and implement data load processes from Cloud or On Premises data sources into Azure Data Lake and subsequent Azure SQL & SQL Data Warehouse ', 'Migrate existing processes and data from On Premises SQL Server and other environments to Azure Data Lake and ultimately Azure data warehouse or database solutions ', 'Help continually improve ongoing data platform, analysis and reporting processes, automating or simplifying self-service support for customers', '3+ years of SQL DW/DB development or equivalent ', '3+ years of experience designing solutions in Azure specializing in any of the following technologies: Azure Data Factory, Azure Data Lake, Azure SQL & Azure SQL Data Warehouse, Azure Functions ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Modis,"Burbank, CA",2 days ago,Be among the first 25 applicants,"['', 'Preferred Qualifications:', 'Manages Ambiguity- Operating effectively, even when things are not certain, or the way forward is not clear.', 'Title: ', 'Title: Data Engineer', 'Design conceptual, logical and physical data models, maintain data dictionary and capture metadata.', 'Engage business teams to understand requirements, document them and deliver robust and scalable solutions in the form of data models that can be leveraged for self-service analytics.', 'Experience in relevant technical languages and tools such as SQL, Python, NoSQL, Airflow, Quartz, ERWIN or equivalent.', 'Bachelor’s Degree in Computer Science, Information Systems, or other related field as well as equivalent work experience.Minimum 5 years of data engineering, data modeling or data architecture experience with a focus on multidimensional data modeling for both structured and unstructured data.Experience designing conceptual, logical and physical data models and maintaining data dictionary and capturing metadata.Experience creating and maintaining automated data pipelines, data standards, and best practices to maintain integrity and security of the data; ensure adherence to developed standards.Experience in developing and maintaining source to target mapping that includes extract requirements, derived field logic, domain values and data lineage.Experience in relevant technical languages and tools such as SQL, Python, NoSQL, Airflow, Quartz, ERWIN or equivalent.', 'Minimum Qualifications:', 'Location: ', 'Utilize ETL tools and other data pipeline automation techniques to develop and maintain source to target mapping that includes extract requirements, derived field logic, domain values and data lineage.', 'Engage business teams to understand requirements, document them and deliver robust and scalable solutions in the form of data models that can be leveraged for self-service analytics.Explore ways of modeling the Plans unstructured voice and text data into frameworks fit for analysis.Design conceptual, logical and physical data models, maintain data dictionary and capture metadata.Perform gap analysis as needed for purposes of maintaining, continuously enhancing data models and integrating KPI’s into the analytics platform as and when new KPI and business metrics are adapted by the organization.Work closely with data experts to build and maintain KPI data dictionary, metadata, data standards, and ensure adherence to the Plans Analytics Method and data standards.Work with application development team to deploy analytics data products through such ways as embedding analysis models into business applications and mobile solutions.Establish and maintain provenance, integrity and security of data used for self-service reporting, ad-hoc analysis or other levels of analysis.Utilize ETL tools and other data pipeline automation techniques to develop and maintain source to target mapping that includes extract requirements, derived field logic, domain values and data lineage.Ensure developed data models are easy to use and efficient to access data thus enable transparency of data lineage to business teams and all stakeholders', 'Expert knowledge of relational DBMS, specifically Oracle .', 'Ensure developed data models are easy to use and efficient to access data thus enable transparency of data lineage to business teams and all stakeholders', 'Location: Burbank, CA\xa0', 'Explore ways of modeling the Plans unstructured voice and text data into frameworks fit for analysis.', 'Experience in developing and maintaining source to target mapping that includes extract requirements, derived field logic, domain values and data lineage.', 'As a Data Engineer, you will design, develop, test and maintain efficient and sustainable data models to keep data accessible and ready for analysis. Working closely with Analytics Team, you will engage with business teams to understand requirements, design conceptual, logical and physical data models, and perform root-cause analysis and recommend solutions. ', 'Expert knowledge in areas of advanced data techniques including unstructured and spatial data.', 'Decision quality – Makes sound decisions, even in the absence of complete information; relies on a mixture of analysis, wisdom, experience, and judgment when making decisions; considers all relevant factors and uses appropriate decision-making criteria and principles; recognizes when a quick 80% solution will suffice.', 'This will be a hands on role utilizing Extract, Transfer & Load (ETL) tools to deliver source to target mappings, physical and logical data models and related scripts to automate and streamline data processes with the overarching goal of unlocking the value of data for the organization. This is a critical role to help build the foundation of a data-driven culture through implementation of a robust self-service data framework ', 'Bachelor’s Degree in Computer Science, Information Systems, or other related field as well as equivalent work experience.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Previous work experience in the Healthcare industry preferred.', 'Essential Job Functions: ', 'Establish and maintain provenance, integrity and security of data used for self-service reporting, ad-hoc analysis or other levels of analysis.', 'Instills trust – follows through on commitments; is seen as direct and truthful; keeps confidences; practices what he/she preaches; shows consistency between words and actions.', 'Salary:\xa0DOE', 'Knowledge Skills, & Abilities:', 'Proficient in leading business and internal team discussions to gather requirements, brainstorm and propose robust and scalable solutions; leverage business partner/unit input to enhance data models.Expert knowledge of relational DBMS, specifically Oracle .Expert knowledge in areas of advanced data techniques including unstructured and spatial data.', 'Communicates effectively – Is effective in a variety of communication settings: one-on-one, small and large groups, or among diverse styles and position levels; attentively listens to others; adjusts to fit the audience and the message; provides timely and helpful information to others across the organization; encourages the open expression of diverse ideas and opinions.', 'Salary:\xa0', 'Perform gap analysis as needed for purposes of maintaining, continuously enhancing data models and integrating KPI’s into the analytics platform as and when new KPI and business metrics are adapted by the organization.', 'Proficient in leading business and internal team discussions to gather requirements, brainstorm and propose robust and scalable solutions; leverage business partner/unit input to enhance data models.', 'Work closely with data experts to build and maintain KPI data dictionary, metadata, data standards, and ensure adherence to the Plans Analytics Method and data standards.', 'Experience designing conceptual, logical and physical data models and maintaining data dictionary and capturing metadata.', ""At Modis, we use our insight, knowledge and global resources to make exceptional connections every day. With 60 branch offices located strategically throughout North America, we are positioned perfectly to deliver the industry's top talent to each of our clients. Clients choose Modis as their workforce partner to solve staffing challenges that range from locating hard-to-find niche talent to completing quick-fill demands."", 'If interested, please apply immediately or email your updated resume to dwijen.mehta@modis.com.\xa0\xa0', 'Competencies: ', 'Work with application development team to deploy analytics data products through such ways as embedding analysis models into business applications and mobile solutions.', 'Experience creating and maintaining automated data pipelines, data standards, and best practices to maintain integrity and security of the data; ensure adherence to developed standards.', 'Customer focus - Gains insight into customer needs; identifies opportunities that benefit the customer; builds and delivers solutions that meet customer expectations; establishes and maintains effective customer relationships.Decision quality – Makes sound decisions, even in the absence of complete information; relies on a mixture of analysis, wisdom, experience, and judgment when making decisions; considers all relevant factors and uses appropriate decision-making criteria and principles; recognizes when a quick 80% solution will suffice.Communicates effectively – Is effective in a variety of communication settings: one-on-one, small and large groups, or among diverse styles and position levels; attentively listens to others; adjusts to fit the audience and the message; provides timely and helpful information to others across the organization; encourages the open expression of diverse ideas and opinions.Ensures accountability – follows through on commitments and makes sure others do the same; acts with a clear sense of ownership; takes personal responsibility for decisions, actions, and failures; establishes clear responsibilities and processes for monitoring work and measuring results; designs feedback loops into work.Instills trust – follows through on commitments; is seen as direct and truthful; keeps confidences; practices what he/she preaches; shows consistency between words and actions.Manages Ambiguity- Operating effectively, even when things are not certain, or the way forward is not clear.', 'Customer focus - Gains insight into customer needs; identifies opportunities that benefit the customer; builds and delivers solutions that meet customer expectations; establishes and maintains effective customer relationships.', '\xa0', 'Ensures accountability – follows through on commitments and makes sure others do the same; acts with a clear sense of ownership; takes personal responsibility for decisions, actions, and failures; establishes clear responsibilities and processes for monitoring work and measuring results; designs feedback loops into work.', 'Minimum 5 years of data engineering, data modeling or data architecture experience with a focus on multidimensional data modeling for both structured and unstructured data.']",Associate,Full-time,Information Technology,Entertainment,2021-03-18 14:34:51
Data Engineer,LogistiCare,"Denver, CO",,N/A,"['', 'We value our team members and realize the importance of benefits for you and your family.', 'ModivCare is an Equal Opportunity Employer.', 'Excellent interpersonal and written communication skills.', 'Pre-Tax and Post --Tax Commuter and Parking Benefits', 'POSITION QUALIFICATIONS', 'BI technologies (not required): Tableau, SAP BusinessObjects', 'Paid Time Off', 'ETL technologies: Matillion, AWS Redshift, AWS Data Migration Services, AWS Glue', 'The Data Engineer role will focus on driving the creation and execution of a business intelligence and data strategy. They will work directly with business and technology stakeholders and team member to analyze, define, design, develop and deliver solutions that provide actionable insights. They will drive and execute in an agile process to ensure consistent, frequent, quality deliverables are deployed.', 'Formulate and recommend standards for achieving maximum performance and efficiency of the BI/DW ecosystem.', 'Cloud technologies: AWS, Azure', ""Bachelor's degree from an accredited college or university with a major in Computer Science, Information Systems, Business Administration or closely related fieldEquivalent work experience is accepted."", 'ESSENTIAL FUNCTIONS', 'Equivalent work experience is accepted.', 'Short-Term and Long-Term Disability', 'This is an entry-level role and the applicant will have on-the-job training', 'Education', 'Interact with Reporting and Data Scientists to analyze and define requirements.', 'Employee Discounts (retail, hotel, food, restaurants, car rental and much more!!)', 'Medical, Dental, and Vision insurance', '401(k) Retirement Savings Plan with Company Match', 'Develop BI solutions based on an agile SDLC.', 'Skills & Experience', 'Health Care and Dependent Care Flexible Spending Accounts', 'Tuition Reimbursement', 'Salary- $75000-$125000 per year [Depending on candidate qualifications]', ""Bachelor's degree from an accredited college or university with a major in Computer Science, Information Systems, Business Administration or closely related field"", 'Technical skills -- Hands on experience in the administration and utilization of the technologies (or similar) below:', 'Formulate and recommend standards for achieving maximum performance and efficiency of the BI/DW ecosystem.Facilitate the creation and execution of an ongoing BI/DW strategy inclusive of existing and available technology.Evangelize self-service BI and visual discovery while helping to change the Excel based culture.Interact with Reporting and Data Scientists to analyze and define requirements.Develop BI solutions based on an agile SDLC.', 'This is an entry-level role and the applicant will have on-the-job trainingHealthcare or insurance experience desired but not required.Excellent time management and organizational skills.Excellent interpersonal and written communication skills.Technical skills -- Hands on experience in the administration and utilization of the technologies (or similar) below:Database technologies: DB2, Netezza, SQL Server, MySQLETL technologies: Matillion, AWS Redshift, AWS Data Migration Services, AWS GlueBI technologies (not required): Tableau, SAP BusinessObjectsCloud technologies: AWS, Azure', 'Voluntary Life Insurance (Employee/Spouse/Child)', 'ModivCare offers a comprehensive benefits package to include the following:', 'Evangelize self-service BI and visual discovery while helping to change the Excel based culture.', 'Healthcare or insurance experience desired but not required.', 'Excellent time management and organizational skills.', 'Paid Parental Leave', '\xa0', 'Employer Paid Basic Life Insurance and AD&D', 'Database technologies: DB2, Netezza, SQL Server, MySQL', 'Facilitate the creation and execution of an ongoing BI/DW strategy inclusive of existing and available technology.', 'Medical, Dental, and Vision insuranceEmployer Paid Basic Life Insurance and AD&DVoluntary Life Insurance (Employee/Spouse/Child)Health Care and Dependent Care Flexible Spending AccountsPre-Tax and Post --Tax Commuter and Parking Benefits401(k) Retirement Savings Plan with Company MatchPaid Time OffPaid Parental LeaveShort-Term and Long-Term DisabilityTuition ReimbursementEmployee Discounts (retail, hotel, food, restaurants, car rental and much more!!)']",Mid-Senior level,Full-time,Information Technology,Hospital & Health Care,2021-03-18 14:34:51
Data Integration Engineer,Peloton Interactive,"New York, NY",19 hours ago,Be among the first 25 applicants,"['', 'You are a proactive problem-solver, even in areas of uncertainty and ambiguity.', 'supply chain systems data integrations', 'Strong verbal and written communication skills', 'Communicate and collaborate effectively with technical peers and business users.', 'Provide guidance on the monitoring, troubleshooting, and resolution of data quality issues in collaboration with the appropriate stakeholders; and', ' Integration Engineer', 'Ability to work well under pressure; manage tight deadlines and situations where conflicting priorities arise.', 'Hands-on experience on Integration Platforms such as Boomi, MuleSoft; AnyPoint, Pentaho, and Informatica', 'Experience with performance tuning optimization within Boomi', 'Own the Boomi development process from requirements gathering to full implementation.', 'Minimum of 3 years experience working with integration platforms (Boomi, MuleSoft etc.).', 'Dell Boomi ', 'Job Responsibilities', 'Document and analyze current business processes and underlying systems/applications.', 'Functional experience with ERP systems (i.e., NetSuite, SAP)', 'Proven ability to implement at least 1 to 2 full integration projects involving on-premise and Cloud applications.', 'Monitor, troubleshoot, and resolve problems with integrations.', 'Basic Job Requirements', ' Bachelor’s degree Strong verbal and written communication skills Strong analytical and critical thinking skills Adapt and proactive at problem-solving and conflict resolution. Minimum of 3 years experience working with integration platforms (Boomi, MuleSoft etc.). Proven ability to implement at least 1 to 2 full integration projects involving on-premise and Cloud applications. Functional experience with ERP systems (i.e., NetSuite, SAP) Experience with REST and SOAP web services SQL language proficiency with at least one of the common database platforms such as SQL Server, Oracle, MySQL, and/or DB2 database management systems Ability to work well under pressure; manage tight deadlines and situations where conflicting priorities arise. ', 'SQL language proficiency with at least one of the common database platforms such as SQL Server, Oracle, MySQL, and/or DB2 database management systems', ' You have the ability to analyze business problems and develop both long-term sustainable solutions and also quick tactical solutions. You can articulate complex concepts in a way that is understandable to non-technical stakeholders. You have excellent analytical and critical reasoning skills. You are a proactive problem-solver, even in areas of uncertainty and ambiguity. You possess strong collaboration skills and approach problems with positive intent while driving towards resolution. ', 'Strong understanding of integration architecture options such as SoA and APIs', 'You possess strong collaboration skills and approach problems with positive intent while driving towards resolution.', 'Who You Are', 'You have the ability to analyze business problems and develop both long-term sustainable solutions and also quick tactical solutions.', 'Adapt and proactive at problem-solving and conflict resolution.', 'Lead the research, development & implementation of special projects, as needed.', 'Preferred Experience', 'About Peloton', 'You have excellent analytical and critical reasoning skills.', 'Collaborate with the development team to architect efficient and stable integrations.', ' Boomi Developer/Architect certified Experience developing applications that utilize Boomi Integration Strong understanding of integration architecture options such as SoA and APIs Experience with performance tuning optimization within Boomi Functional experience with ERP systems (i.e., NetSuite, SAP) Hands-on experience on Integration Platforms such as Boomi, MuleSoft; AnyPoint, Pentaho, and Informatica ', 'Experience with REST and SOAP web services', 'Boomi Developer/Architect certified', 'Bachelor’s degree', ' Own the Boomi development process from requirements gathering to full implementation. Monitor, troubleshoot, and resolve problems with integrations. Collaborate with the development team to architect efficient and stable integrations. Document and analyze current business processes and underlying systems/applications. Lead the research, development & implementation of special projects, as needed. Provide guidance on the monitoring, troubleshooting, and resolution of data quality issues in collaboration with the appropriate stakeholders; and Communicate and collaborate effectively with technical peers and business users. ', 'Experience developing applications that utilize Boomi Integration', 'You can articulate complex concepts in a way that is understandable to non-technical stakeholders.', 'Strong analytical and critical thinking skills']",Entry level,Full-time,Information Technology,"Health, Wellness and Fitness",2021-03-18 14:34:51
Data Engineer - Malvern,Ace Technologies,"Malvern, PA",6 days ago,Be among the first 25 applicants,"['', ' Must be customer focused and excel in coordination of problem resolution.', ' Provide rotational on-call support, responding quickly to production issues.', ' Bachelor’s degree in Computer Science, MIS or related field or equivalent experience is required.  Post graduate degree or coursework a plus.  Minimum of 9 years IT experience and at least 3 years of experience required in data warehousing, relational database management systems, multi-dimensional database management systems.  Demonstrated knowledge of at least one ETL tool such as IBM InfoSphere DataStage (or comparable ETL tool) is required. In addition to that, hands-on experience in creating ETL solutions with Python/Spark is required.  Demonstrated knowledge of Data Warehousing data population techniques for target structures such as Star Schemas, Snowflake Schemas, highly normalized data models, and file structures.  Demonstrated knowledge of source to target mappings.  Demonstrated relational database knowledge; possesses a broad understanding of the current and prospective data architecture, and database performance tuning.  Demonstrates ETL staging environment development skills and strong SQL programming skills in a DB2 environment.  Ability to establish and maintain effective working relationships with external and internal personnel.  Must be customer focused and excel in coordination of problem resolution.  Flexibility to work in a changing, fast-paced environment and work with a sense of urgency and accountability.  Experience with Agile methodologies is desirable.  Experience with programming languages such as PySpark, UNIX scripting, Java is desirable.', ' Post graduate degree or coursework a plus.', ' Demonstrated knowledge of source to target mappings.', ' Demonstrates ETL staging environment development skills and strong SQL programming skills in a DB2 environment.', ' Map source system data to data warehouse models or other file transformations.  Develop complex ETL processes following ETL standards, goals, and objectives.  Follow Change Management/Configuration Management processes for the deployment of code into test and production environments.  Provide rotational on-call support, responding quickly to production issues.  Create basic to moderate physical designs of ETL processes.', ' Bachelor’s degree in Computer Science, MIS or related field or equivalent experience is required.', 'Qualifications Of The Data Engineer', ' Ability to establish and maintain effective working relationships with external and internal personnel.', ' Minimum of 9 years IT experience and at least 3 years of experience required in data warehousing, relational database management systems, multi-dimensional database management systems.', ' Follow Change Management/Configuration Management processes for the deployment of code into test and production environments.', ' Demonstrated relational database knowledge; possesses a broad understanding of the current and prospective data architecture, and database performance tuning.', ' Create basic to moderate physical designs of ETL processes.', ' Demonstrated knowledge of at least one ETL tool such as IBM InfoSphere DataStage (or comparable ETL tool) is required. In addition to that, hands-on experience in creating ETL solutions with Python/Spark is required.', ' Map source system data to data warehouse models or other file transformations.', ' Flexibility to work in a changing, fast-paced environment and work with a sense of urgency and accountability.', ' Develop complex ETL processes following ETL standards, goals, and objectives.', ' Experience with programming languages such as PySpark, UNIX scripting, Java is desirable.', ' Demonstrated knowledge of Data Warehousing data population techniques for target structures such as Star Schemas, Snowflake Schemas, highly normalized data models, and file structures.', ' Experience with Agile methodologies is desirable.']",Entry level,Full-time,Information Technology,Computer Hardware,2021-03-18 14:34:51
Data Engineer,Human API,"Myrtle Point, OR",2 weeks ago,Be among the first 25 applicants,"['', 'Have a good sense of humor', 'A positive attitude, willingness to learn, and desire for self-improvement', ""Want to make a positive difference in people's lives"", 'Are product focused - solving the underlying product problem is more important than writing lots of code', 'By submitting your interest in this job, you agree to receive text notifications with additional steps to complete your job application. You will receive up to 6 messages from the number ""63879"". Message & data rates may apply. Please refer to our privacy policy for more information.', 'Performance and scaling for internal and external real-time data services.', 'Experience working directly on one or more of the following: database schema design, query optimization, development of ETL / analytics / reporting systems, big-data systems, large-scale text-parsing systems, stream processing, or ML pipeline engineering.', 'Your team will own', 'Incorporating appropriate use of ML techniques in the pipeline', 'You should have', ""Own problems end-to-end - we have a very ownership driven cultureAre product focused - solving the underlying product problem is more important than writing lots of codeHave a good sense of humorWant to make a positive difference in people's lives"", 'Experience with large scale distributed systems desirable', 'Bespoke data pipeline logic and processes.Performance and scaling for internal and external real-time data services.Conventional(-ish) ETL reporting and data engineering.Incorporating appropriate use of ML techniques in the pipeline', 'Bespoke data pipeline logic and processes.', 'Own problems end-to-end - we have a very ownership driven culture', 'Conventional(-ish) ETL reporting and data engineering.', 'Experience working directly on one or more of the following: database schema design, query optimization, development of ETL / analytics / reporting systems, big-data systems, large-scale text-parsing systems, stream processing, or ML pipeline engineering.Experience with large scale distributed systems desirableExperience with Databricks, Apache Spark, PrestoDB, or Kafka desirableA positive attitude, willingness to learn, and desire for self-improvement', 'Experience with Databricks, Apache Spark, PrestoDB, or Kafka desirable', 'We Really Like People Who']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Calm,San Francisco Bay Area,2 weeks ago,Over 200 applicants,"['', 'Fluent in SQL', '401K', 'Team player who gives and takes feedback in a thoughtful way, and loves to help others.', 'Over 75 Million people have downloaded the app and we are growing by 100,000 new downloads a day. The company is profitable and headquartered in San Francisco, CA.', 'Write well-tested, production ready code in Python, Go, and/or SQL', 'Calm is deeply committed to diversity, equity and inclusion, both in our hiring practices and in our experiences as a Calm employee. We strive to create a mindful and respectful environment where everyone can bring their authentic self to work, and experience a culture that is free of harassment, racism, and discrimination.\xa0', 'Test all code written and ensure production readiness before shipping', 'Experience writing production-level code in Python or Go\xa0', 'Bonus points if you have some devops related-experience working with, e.g., kubernetes, terraform, etc. Or if you have previous experience as a Data Scientist or analyst', 'Calm is proud to be an Equal Employment Opportunity employer. We do not discriminate based upon race, religion, color, national origin, sex, sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, genetic information, political views or activity, or other applicable legally protected characteristics.\xa0', 'Competitive salary and equityUnlimited PTOWe pay your medical, dental, & vision insurance premiums401KCommuter benefitsLife insurance and disability benefitsApple equipmentOpportunity to work with a product focused on making the world happier and healthierAnd much more!', 'Proactive communicator who can translate between technical and non-technical stakeholders', 'Stays up-to-date with high-potential new technologies, and can evaluate and present to the team for Calm’s use case', 'Benefits', 'Excellent sense of how to drive business impact', 'At least 2 years of data engineering experience', 'About Calm', 'Calm is also committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures. Please inform Calm’s Recruiting team if you need any assistance completing any forms or to otherwise participate in the application process.', 'Onboard onto Airflow and Redshift and assist with improvements and maintenance', 'Unlimited PTO', 'Experience working with at least one data processing tool such as Spark, Redshift/Snowflake, Airflow/Luigi, etc', 'Create automated, highly reliable data pipelines', 'Experience building and maintaining critical, reliable ETL pipelines', 'Experience working with at least one data processing tool such as Spark, Redshift/Snowflake, Airflow/Luigi, etcBasic working knowledge of other big data technologies\xa0Experience building and maintaining critical, reliable ETL pipelinesExperience writing production-level code in Python or Go\xa0Fluent in SQLProactive communicator who can translate between technical and non-technical stakeholdersExcellent sense of how to drive business impactTeam player who gives and takes feedback in a thoughtful way, and loves to help others.Work cross-functionally with our product, marketing and growth teams on complex and exciting projects that propel Calm’s businessBonus points if you have some devops related-experience working with, e.g., kubernetes, terraform, etc. Or if you have previous experience as a Data Scientist or analystAt least 2 years of data engineering experience', 'Competitive salary and equity', 'Build data integrations within our data platform and between partners\xa0', 'We pay your medical, dental, & vision insurance premiums', ""The Data Engineering team’s mission is to make Calm’s data reliable, trustworthy and easy to use.\xa0To do that, we’re building a data ecosystem that enables the entire organization to use data to make our product better, facilitate decision making and help drive business value.Data Engineers at Calm work closely with internal stakeholders, defining requirements and designing and building solutions that meet those requirements.\xa0We are a group of strong communicators, who care about our stakeholders and each other. We love to work together as a team to find simple solutions to complex problems.\xa0Our current stack includes Airflow, DBT, Tableau, Redshift, SQS, SNS, and Sagemaker all deployed on Docker and Kubernetes on AWS - but we’re always open to the right technology for the job.You can read more\xa0about our interview process,\xa0our\xa0engineering organization, and\xa0what we've worked on recently."", 'Define, design, and build data testing and quality frameworks', 'Work cross-functionally with our product, marketing and growth teams on complex and exciting projects that propel Calm’s business', 'Basic working knowledge of other big data technologies\xa0', 'Commuter benefits', 'We have a simple mission at Calm: To make the world a happier and healthier place.', 'Competencies', 'Help define and craft our data model', 'Outcomes', '\xa0', 'Life insurance and disability benefits', 'Apple equipment', 'And much more!', 'Build data integrations within our data platform and between partners\xa0Write well-tested, production ready code in Python, Go, and/or SQLImprove the efficiency, reliability, and latency of our data systemCreate automated, highly reliable data pipelinesHelp define and craft our data modelOnboard onto Airflow and Redshift and assist with improvements and maintenanceDefine, design, and build data testing and quality frameworksTest all code written and ensure production readiness before shippingWork cross-functionally with our product, marketing and growth teams on complex and exciting projects that propel Calm’s businessStays up-to-date with high-potential new technologies, and can evaluate and present to the team for Calm’s use case', 'Mission', 'Improve the efficiency, reliability, and latency of our data system', 'The heart of Calm is digital but the brand is expanding offline into a variety of products and services that bring more peace, clarity and perspective into people’s busy lives. We are building Calm into the Nike of the Mind. We believe Calm can become one of the most valuable and meaningful brands in the world.', 'Calm was co-founded by Alex Tew (Million Dollar Homepage) and Michael Acton Smith (Mind Candy, Moshi Monsters, Firebox).', 'Opportunity to work with a product focused on making the world happier and healthier']",Mid-Senior level,Full-time,Engineering,"Health, Wellness and Fitness",2021-03-18 14:34:51
Jr. Data Engineer,Optello,"Bethesda, MD",3 days ago,87 applicants,"['', 'Desired', 'Optello is proud to be an Equal Opportunity Employer', ' Python', ' We are a growing company offering business leaders an analytics platform We offer great benefits including bonus and 401(k) with 5% match', ' Base Salary: $65k - $85k 401(k) with 5% company match Full Health Benefits PTO', ' We are a growing company offering business leaders an analytics platform', ' Base Salary: $65k - $85k', 'Your Right to Work', ' Redshift', 'Please do NOT change the email subject line in any way. You must keep the JobID: linkedin : ET3-1624092 -- in the email subject line for your application to be considered.***', ' Conduct SQL development and unit testing', ' Assist with ETL troubleshooting Conduct SQL development and unit testing Perform data modeling', ' We offer great benefits including bonus and 401(k) with 5% match', ' SQL ETL', ' SQL', 'Required', ' Assist with ETL troubleshooting', 'Email Your Resume In Word To', ' AWS Redshift Python', ' Perform data modeling', ' ETL', ' 401(k) with 5% company match', ' AWS', ' Full Health Benefits', ' PTO']",Associate,Full-time,Information Technology,Construction,2021-03-18 14:34:51
Data Engineer,"KYM ADVISORS, INC.","Camp Springs, KY",2 days ago,Be among the first 25 applicants,"['', 'Desired', 'Experience with Tableau and SAS is nice-to-have', 'Experience with advanced analytics solutions (data science, machine learning, data modeling, etc.) including infrastructure (e.g., data engineering) preferred', 'Implements real time and batch processing requirements as well as graph databases in large-scale data environments.', 'Current DHS clearance or prior government agency security clearance strongly preferred', 'Vision', ' Bachelor’s Degree in Information Technology, Engineering, Computer Science, Machine Learning, Statistics or related fields 3+ years’ experience in IT, data analytics, data engineering, or related discipline Advanced working SQL knowledge with experience working with relational and NoSQL databases, Apache Spark, and Spark SQL Previous hands-on coding and scripting experience in languages such as Python and Scala Prior experience utilizing APIs to interface with Apache Spark; experience utilizing PySpark Experience working within enterprise data security management policies and procedures Due to COVID-19, certain on-site client work has been suspended until further notice, however, on-site work may be required on a case-by-case basis for this position Must be a U.S. Citizen with the ability to obtain and maintain a government suitability clearance. ', ' Medical Dental Vision Short-term Disability Long-term Disability Life and AD&D insurance 401K Plan with employer match', 'Experience working within enterprise data security management policies and procedures', 'Short-term Disability', ' Independently and as part of a project team, works to collect, build, cleanse, assemble, and refine datasets to support a variety of data analytics needs put forward by business and technical stakeholders.  Assembles large complex data sets and creates and maintains data pipelines required for optimal ETL/ELT for internal and external data sources; focuses on scalability and high performance Implements real time and batch processing requirements as well as graph databases in large-scale data environments. Works with a variety of business and technical stakeholders including executive, product, data, and information technology teams to intake requirements, provide progress updates, troubleshoot, and generally supports their needs. Forward-thinking approach with interest in highlighting internal process improvement with respect to automating manual processes, optimizing data delivery, improving infrastructure for greater scalability, etc. ', 'Due to COVID-19, certain on-site client work has been suspended until further notice, however, on-site work may be required on a case-by-case basis for this position', 'Assembles large complex data sets and creates and maintains data pipelines required for optimal ETL/ELT for internal and external data sources; focuses on scalability and high performance', 'Benefits', 'Forward-thinking approach with interest in highlighting internal process improvement with respect to automating manual processes, optimizing data delivery, improving infrastructure for greater scalability, etc.', ' Current DHS clearance or prior government agency security clearance strongly preferred Experience with Databricks as a platform strongly preferred 2+ years’ experience in machine learning (ML), artificial intelligence (AI), and/or natural language processing (NLP) preferred Experience working within an Agile and/or Scrum environment preferred Experience with Graph databases like Neo4j or GraphFrames Experience with advanced analytics solutions (data science, machine learning, data modeling, etc.) including infrastructure (e.g., data engineering) preferred Experience in an environment using leading data science and machine learning platforms (Databricks, SAS, Anaconda, Alteryx, etc.) preferred Consulting experience at large consulting firm (e.g. Booz Allen Hamilton, Deloitte, Accenture, Cap Gemini) nice-to-have Experience with Tableau and SAS is nice-to-have ', '401K Plan with employer match', 'Experience with Databricks as a platform strongly preferred', 'Consulting experience at large consulting firm (e.g. Booz Allen Hamilton, Deloitte, Accenture, Cap Gemini) nice-to-have', 'Dental', 'Requirements', 'Experience in an environment using leading data science and machine learning platforms (Databricks, SAS, Anaconda, Alteryx, etc.) preferred', '2+ years’ experience in machine learning (ML), artificial intelligence (AI), and/or natural language processing (NLP) preferred', 'Experience working within an Agile and/or Scrum environment preferred', 'Bachelor’s Degree in Information Technology, Engineering, Computer Science, Machine Learning, Statistics or related fields', 'Medical', 'Long-term Disability', 'Previous hands-on coding and scripting experience in languages such as Python and Scala', 'Data Engineer ', 'Works with a variety of business and technical stakeholders including executive, product, data, and information technology teams to intake requirements, provide progress updates, troubleshoot, and generally supports their needs.', 'Must be a U.S. Citizen with the ability to obtain and maintain a government suitability clearance.', 'Advanced working SQL knowledge with experience working with relational and NoSQL databases, Apache Spark, and Spark SQL', '3+ years’ experience in IT, data analytics, data engineering, or related discipline', 'Life and AD&D insurance', 'Independently and as part of a project team, works to collect, build, cleanse, assemble, and refine datasets to support a variety of data analytics needs put forward by business and technical stakeholders. ', 'Prior experience utilizing APIs to interface with Apache Spark; experience utilizing PySpark', 'Experience with Graph databases like Neo4j or GraphFrames']",Mid-Senior level,Full-time,Engineering,Information Technology and Services,2021-03-18 14:34:51
Senior Data Engineer,Fidelity Investments,"Merrimack, NH",1 day ago,Be among the first 25 applicants,"['', 'Sophisticated experience with PL/SQL and complex queries, views, packages etc.', 'Experience with cloud native data warehousing and data lake solutions using Redshift, Snowflake, etc.', 'Your experience in executing projects in an Agile environment.', 'Understanding of Cloud Computing and DevOps concepts including CI/CD pipelines using Git and Jenkins', 'Your sophisticated skills in data intensive application development, data integration, and data pipeline design patterns on a distributed platform.', 'Your ability to collaborate with other technical and business specialists in the team', 'Bachelor’s Degree or equivalent in a technology related field (e.g. Computer Science, Engineering, etc.) required.', 'Experience with modern Object-Oriented Programming Languages like Java, Scala, Python would be a huge plus', 'The Team', 'Basic understanding of NoSQL and BigData technologies such as e.g. Hadoop, HBase, MongoDB, Cassandra, etc.', 'Bachelor’s Degree or equivalent in a technology related field (e.g. Computer Science, Engineering, etc.) required.Strong proven understanding of Relational Databases like Oracle or PostgresSophisticated experience with PL/SQL and complex queries, views, packages etc.6-8 years of proven experience working with ETL data integration and data movement design patterns using InformaticaExperience with modern Object-Oriented Programming Languages like Java, Scala, Python would be a huge plusYour sophisticated skills in data intensive application development, data integration, and data pipeline design patterns on a distributed platform.Knowledge of batch job scheduling and dependency management using BMC Control-M for distributed systemsUnderstanding of Cloud Computing and DevOps concepts including CI/CD pipelines using Git and JenkinsBasic understanding of NoSQL and BigData technologies such as e.g. Hadoop, HBase, MongoDB, Cassandra, etc.Experience with cloud native data warehousing and data lake solutions using Redshift, Snowflake, etc.Your experience in executing projects in an Agile environment.Your ability to collaborate with other technical and business specialists in the teamYour ability to learn and experiment with new technologies and patternsYour passion to follow modern test driven and automation driven software development methodologies', 'Strong proven understanding of Relational Databases like Oracle or Postgres', 'The Role', 'The Expertise And Skills You Bring', 'Your ability to learn and experiment with new technologies and patterns', 'Your passion to follow modern test driven and automation driven software development methodologies', '6-8 years of proven experience working with ETL data integration and data movement design patterns using Informatica', 'Job Description', 'Knowledge of batch job scheduling and dependency management using BMC Control-M for distributed systems', 'Certifications']",Mid-Senior level,Full-time,Quality Assurance,Financial Services,2021-03-18 14:34:51
Software Engineer - Data Platform Engineer,Numerated,"Boston, MA",22 hours ago,39 applicants,"['', 'Bachelor’s degree in computer science, information systems or related technical field required ', 'Understand the importance of continuous integration and testing', 'Experience with distributed computation platforms such as Apache Spark, Kafka, Presto.', 'A professional attitude with strong interpersonal and communication skills at different levels - frequent video communication with remote teams required.', 'Demonstrate ownership of developed components from development through production', 'Demonstrated ability to work effectively in a fast-paced, team-oriented work environment.', 'Deliver full-stack capabilities in our modern, developer-driven tech stack: Angular / Python / Postgres / Snowflake', 'Proficiency with Angular and Python, facility with other languages desirable: Java, NodeJS, Scala, OS scripting.', 'Experience with AWS desired, particularly Glue, EMR, Lambda, Kinesis or other data-related services. Working knowledge of Linux. ', 'Participate in code reviews and contribute to automated tests', 'Experience in developing and working with data ingestion pipelines for analytics desired.', 'Bachelor’s degree in computer science, information systems or related technical field required Master’s degree in computer science, information systems or related technical field preferred', 'Contribute to the continuous improvement of your team', 'Be a member of a scrum team to deliver shippable quality code every sprintDeliver full-stack capabilities in our modern, developer-driven tech stack: Angular / Python / Postgres / SnowflakeParticipate in code reviews and contribute to automated testsUnderstand the importance of continuous integration and testingProvide ongoing maintenance, support and enhancements to existing systems and platformsContinue to grow your skill set and tool kit, continuously improving the quality of deliverables by authoring well-engineered solutions using test-first/test-driven mindsetConsistently apply best practices for design, coding standards, performance, security, delivery, and maintainabilityContribute to the continuous improvement of your teamDemonstrate ownership of developed components from development through productionExemplify the principles behind Scrum and the Agile Manifesto in all interactions', 'Be a member of a scrum team to deliver shippable quality code every sprint', ""You thrive in a fast-paced environment, and given context, you're capable of self-direction when solving difficult problems in creative ways and making a real impact to the business."", ""Significant past experience with SQL, dimensional data modeling, relational and columnar databasesExperience with AWS desired, particularly Glue, EMR, Lambda, Kinesis or other data-related services. Working knowledge of Linux. Experience in developing and working with data ingestion pipelines for analytics desired.Experience with distributed computation platforms such as Apache Spark, Kafka, Presto.Proficiency with Angular and Python, facility with other languages desirable: Java, NodeJS, Scala, OS scripting.A professional attitude with strong interpersonal and communication skills at different levels - frequent video communication with remote teams required.You're known as a creative, innovative and outside-the-box thinker, unafraid to express your ideas with other team members including those with more seniority.You thrive in a fast-paced environment, and given context, you're capable of self-direction when solving difficult problems in creative ways and making a real impact to the business.You have a passion for keeping up with the rapidly changing data technical landscape.Demonstrated ability to work effectively in a fast-paced, team-oriented work environment.Outstanding written and verbal communication skills."", 'Essential Responsibilities /', 'Outstanding written and verbal communication skills.', 'Provide ongoing maintenance, support and enhancements to existing systems and platforms', 'Work Experience Requirements /', 'Continue to grow your skill set and tool kit, continuously improving the quality of deliverables by authoring well-engineered solutions using test-first/test-driven mindset', 'Exemplify the principles behind Scrum and the Agile Manifesto in all interactions', ""You're known as a creative, innovative and outside-the-box thinker, unafraid to express your ideas with other team members including those with more seniority."", 'You have a passion for keeping up with the rapidly changing data technical landscape.', 'Master’s degree in computer science, information systems or related technical field preferred', 'Consistently apply best practices for design, coding standards, performance, security, delivery, and maintainability', 'About Numerated', '/', 'Overview ', 'Significant past experience with SQL, dimensional data modeling, relational and columnar databases', 'Education Requirements ']",Entry level,Full-time,Engineering,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Marga Consulting,Raleigh-Durham-Chapel Hill Area,3 hours ago,101 applicants,"['', '- Advanced data analysis skills', '2. Deeper analysis to build data processing framework', '4. This individual will work closely with the Product Control (PC), Financial Accounting (FA),', '- Bachelors degree or higher in an analytical area such as Computer Science, Physics, Mathematics, Statistics, Engineering or similar.', 'Over the last few years, the regulatory environment has intensified with a steady flow of new legislation. FSG has met this challenge, delivering many critical initiatives including CAD II, FSI, Basel III, CCAR and CRD IV. The continued demand for front-to-back transparency will require even greater collaboration to align data sources and establish common processes between the CRO and CFO businesses.', 'Risk clients, and Change teams to understand requirements and work with business analysts, architecture and development resources to design solutions to meet business needs. He/she will liaise with multiple IT teams to drive forward delivery.', '- 8+ years relevant professional experience in Data Engineering and Business Intelligence', '3. Be part of the thriving team to develop regulatory related functionality.', 'Together, our mission is to continually align ourselves with the business, optimizing its operating model, leveraging shared global resources and services.', '1. Building and maintaining an optimized and highly available data pipelines', '- 3+ years in with Advanced SQL (analytical functions), RDBMS, ETL, Data warehousing.', '- Strong knowledge of data warehousing concepts, including data warehouse technical architectures, infrastructure components, ETL/ ELT and reporting/analytic tools and environments, data structures, data modeling and performance tuning.', ""We are an integral part of the global IT function that underpins the Bank's corporate reputation and are responsible for implementing best in class applications capable of receiving quality data in a timely manner. Our objectives are to drive architectural simplification and reduce complexity, resulting in a stronger delivery capability."", '- Solid working experience in various forms of data infrastructure inclusive of RDBMS such as SQL, Hadoop, Unix, Oracle and OBIEE', 'Qualifications & Experience', '- Ability to effectively communicate with both business and technical teams.', '- Experience in financial services industry products and regulatory development.-']",Associate,Contract,Information Technology,Investment Banking,2021-03-18 14:34:51
100% Remote- Senior Data Engineer,Prodware Solutions,United States,2 days ago,Be among the first 25 applicants,[''],Associate,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Keller Schroeder,"Evansville, IN",2 days ago,64 applicants,"['', 'Bachelor’s Degree in Computer Science/Computer Engineering or equivalent years of software development experience required.', 'Requirements:', 'https://www.kellerschroeder.com/why-join-our-team', 'We offer a competitive compensation package, including base salary and profitability bonus, medical, dental, life and vision insurances, matching 401(k), Employee Stock Ownership Plan (ESOP), flexible spending account, vacation, holidays, paternity/maternity/adoption/foster care leave, tuition reimbursement and direct deposit.\xa0We offer flexibility and autonomy to tailor the right combination of remote, office and on-site time to get the job done and accommodate personal and client scheduling needs.', 'Experience with big data technologies, preferably Spark.', 'Experience working with teams on large technical implementations.', 'Strong analytic skills related to working with complex datasets.', 'Some experience with visualization tools such as PowerBI, Tibco Spotfire, Qlikview, Qliksense, Tableau, etc.', 'Expertise in designing, validating, and implementing multiple projects across the hybrid infrastructure (on-cloud to on-premise and vice versa).', 'Check out why Employee-Owners love to work at Keller Schroeder!', 'We look forward to receiving your resume in PDF format!', 'Keller Schroeder is an employee-owned\xa0technology company in the performance improvement business.\xa0If you are a proven performer, we would like you to take a closer look at working for us. We are in need of a Data Engineer to join our Data Strategy Solutions Group.', '3+ years of experience in business, institutional, industrial, or energy / utilities setting. Strategy development and Leadership ability.', 'Responsibilities:', 'Designing and developing scalable Bigdata systems and infrastructure.', 'Excellent verbal, meeting facilitation, presentation, and written communication skills.', 'Preferred:', 'Help and guide Data scientists to implement predictive models created by them.', 'Programming language certifications.', 'Expertise in building data pipelines, data ingestions, data integrations, data preparations, and traditional data warehouses and data marts.', 'Expertise with Data Engineering tools such as Talend, BODS and Big Data Technologies.', 'Designing and developing scalable Bigdata systems and infrastructure.Research new technologies and propose novel solutions for improving big data systems and processes. Performs big data integration design and construction activities.Help and guide Data scientists to implement predictive models created by them.Work with other Data engineers, traditional IT Analysts and Data Scientists to integrate big data ingestion and validation as the foundation of our advanced analytics platform.Performs data lifecycle management activities (data sourcing, data lineage, metadata management, business rules development, data quality, data remediation and data delivery) to curate data for the data scientist and other advanced analytics applications.Performs data integration and work with the Enterprise Architect to develop production deployment architectures for machine learning models.May recreate ML code into other languages (e.g. R to Python) for more efficient volume processing.', 'Bachelor’s Degree in Computer Science/Computer Engineering or equivalent years of software development experience required.3+ years of experience in business, institutional, industrial, or energy / utilities setting. Strategy development and Leadership ability.3+ years’ Data Engineer experienceAbility to communicate complex topics to all levels of organizations (C-level to field staff).Excellent verbal, meeting facilitation, presentation, and written communication skills.', '4 – 5 years of experience in software development life cycle concepts and demonstrated execution.', 'Demonstrated ability to accept assignments, escalate issues and achieve on-time delivery of assigned work.', 'Research new technologies and propose novel solutions for improving big data systems and processes. Performs big data integration design and construction activities.', 'Performs data lifecycle management activities (data sourcing, data lineage, metadata management, business rules development, data quality, data remediation and data delivery) to curate data for the data scientist and other advanced analytics applications.', '4 – 5 years of experience in software development life cycle concepts and demonstrated execution.Experience working with teams on large technical implementations.Experience in utility strategy and industry research a plus.Demonstrated ability to accept assignments, escalate issues and achieve on-time delivery of assigned work.Programming language certifications.Expertise with Data Engineering tools such as Talend, BODS and Big Data Technologies.Expertise in building data pipelines, data ingestions, data integrations, data preparations, and traditional data warehouses and data marts.Expertise in designing, validating, and implementing multiple projects across the hybrid infrastructure (on-cloud to on-premise and vice versa).Experience with any cloud platforms, preferably Azure.Experience with streaming technologies, preferably Kafka.Experience with big data technologies, preferably Spark.Strong experience with relational SQL and NoSQL databases.Strong analytic skills related to working with complex datasets.Some experience with visualization tools such as PowerBI, Tibco Spotfire, Qlikview, Qliksense, Tableau, etc.', 'Work with other Data engineers, traditional IT Analysts and Data Scientists to integrate big data ingestion and validation as the foundation of our advanced analytics platform.', 'Experience in utility strategy and industry research a plus.', 'Experience with streaming technologies, preferably Kafka.', 'Experience with any cloud platforms, preferably Azure.', 'Ability to communicate complex topics to all levels of organizations (C-level to field staff).', 'Performs data integration and work with the Enterprise Architect to develop production deployment architectures for machine learning models.', '3+ years’ Data Engineer experience', 'May recreate ML code into other languages (e.g. R to Python) for more efficient volume processing.', 'Keller Schroeder’s mission is to leverage technology tools and services to help our clients more successfully achieve their objectives.\xa0We are passionate about our clients and hold dear our reputation for honesty and integrity. Our company culture is focused on people and performance.\xa0', 'Strong experience with relational SQL and NoSQL databases.']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,HealthLevel,"Mountain View, CA",2 days ago,35 applicants,[''],Entry level,Full-time,Information Technology,Hospital & Health Care,2021-03-18 14:34:51
Data Engineer,Jettison,Greater Chicago Area,4 hours ago,103 applicants,"['', 'The Data Engineer will have the opportunity to work in our Chicago or New York office focusing on building out and supporting our research framework.\xa0', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Proven success developing models in a low latency trading environment', 'Required Skills & Abilities:', 'Exellent written and verbal communication skills\xa0', 'Working knowledge of DASK', 'Strong multi-tasking skills', 'Develop and optimize existing tools, used by the quants', 'A Bachelors, Masters, or PhD degree with an emphasis in Computer Science and Statistics/Math (e.g. Data Science, Physics, etc.)5+ years of professional experience as a Data EngineerExpert level skills in Python\xa0Working knowledge of DASK', 'We are seeking a candidate that is searching for an opportunity to make a real impact, deliver value, and work within an experienced team.', 'Jettison is representing a company who is searching for a Data Engineer in the Chicago Area.\xa0', 'Responsibilities:', 'Collaborate with quant researchers, and core technologists to build and maintain the central research platformDevelop and optimize existing tools, used by the quantsOngoing testing and optimization of the research pipeline by collecting and analyzing data on the utilization and efficiency of the firms distributed systems', 'Strong multi-tasking skillsExellent written and verbal communication skills\xa0', 'Collaborate with quant researchers, and core technologists to build and maintain the central research platform', '5+ years of professional experience as a Data Engineer', 'A Bachelors, Masters, or PhD degree with an emphasis in Computer Science and Statistics/Math (e.g. Data Science, Physics, etc.)', 'Expert level skills in Python\xa0', 'Ongoing testing and optimization of the research pipeline by collecting and analyzing data on the utilization and efficiency of the firms distributed systems']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer D.C.,Afiniti,"Washington, DC",2 days ago,Be among the first 25 applicants,"['', 'Expertise in MySQL or PostgreSQL', 'Proficiency in Python or any other programming language', ' Implement ETL procedures and standards using SQL/R/Talend/Python for existing and new deployments Determine specifications and implement the data pipeline according to requirements Identify issues in daily data feed and ensure data consistency and data integrity. Process, clean, and analyze data to be utilized in artificial intelligence modeling Design data models and implement effective data warehouse strategies and concepts Monitor ETL processes, perform root cause analysis on incidents and resolve data production issues Utilize advanced mathematical methods to analyze and report performance to track optimization metrics. Perform in-depth analysis on data to provide key insights to improve the call center experience. Validate models to ensure adequacy and provide recommendations for models. ', 'Knowledge of Probability/Statistics', 'Bachelors of Engineering/Sciences in Computers/Software/Analytics', 'Utilize advanced mathematical methods to analyze and report performance to track optimization metrics.', '2 - 5 years of experience in Data driven roles', 'Monitor ETL processes, perform root cause analysis on incidents and resolve data production issues', 'Perform in-depth analysis on data to provide key insights to improve the call center experience.', 'Determine specifications and implement the data pipeline according to requirements', 'Good communication skills', 'Education And Qualifications', 'Process, clean, and analyze data to be utilized in artificial intelligence modeling', 'Design data models and implement effective data warehouse strategies and concepts', ' 2 - 5 years of experience in Data driven roles Expertise in MySQL or PostgreSQL Knowledge of Probability/Statistics Proficiency in Python or any other programming language Good communication skills ', 'Identify issues in daily data feed and ensure data consistency and data integrity.', 'Implement ETL procedures and standards using SQL/R/Talend/Python for existing and new deployments', 'Validate models to ensure adequacy and provide recommendations for models.']",Entry level,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Engineer,"NBCUniversal Media, LLC","Burbank, CA",7 days ago,Be among the first 25 applicants,"['', 'Country', ' Media-focused – Strong knowledge/passion for media including broadcast TV, digital, and film Direct experience working with sources like Nielsen, Adobe Analytics, comScore, and other media/entertainment industry datasets a plus ', "" Action-oriented – You're constantly figuring out new problems and are regularly showing results with a positive attitude, always displaying ethical behavior, integrity, and building trust Strong understanding of Agile principles and best practices  You’ve dealt with ambiguity and can make quality decisions in a dynamic, fast-paced environment "", 'Notices', 'About Us', "" 5+ years of experience in a data engineering role  Direct experience with data modeling, ETL/ELT development principles, and data warehousing concepts  Knowledge of data management fundamentals and data storage principles  Experience in building data pipelines using Python/SQL or similar programming languages  Demonstratable experience in Airflow, Luigi or similar orchestration engines  General understanding of cloud data engineering design patterns and use cases  Bachelor's degree in Computer Science, Data Science, Statistics, Informatics, Information Systems or related field. "", ' Understanding of big data technology stacks (Hive / Spark etc) is a plus ', ' Strong understanding of Agile principles and best practices  You’ve dealt with ambiguity and can make quality decisions in a dynamic, fast-paced environment ', ' Build and manage relationships with supporting IT teams in order to effectively deliver work products to production ', ' General understanding of cloud data engineering design patterns and use cases ', 'Career Level', ' 5+ years of experience in a data engineering role ', ' Direct experience with data modeling, ETL/ELT development principles, and data warehousing concepts ', 'Sub-Business', 'Responsibilities', ' Experience with Snowflake, Amazon Web Services, or related cloud platforms a plus  Understanding of big data technology stacks (Hive / Spark etc) is a plus ', ' Participate in development sprints, demos, and retrospectives, as well as release and deployment ', 'City', ' Design, build, and scale data pipelines across a variety of source systems and streams (internal, third-party, as well as cloud-based), distributed/elastic environments, and downstream applications and/or self-service solutions ', ' Collaborate with business leaders, engineers, and product managers to understand data needs.  Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using cloud-native data engineering principles  Design, build, and scale data pipelines across a variety of source systems and streams (internal, third-party, as well as cloud-based), distributed/elastic environments, and downstream applications and/or self-service solutions  Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.  Implement the appropriate design patterns while optimizing performance, cost, security, and scale and end user experience  Participate in development sprints, demos, and retrospectives, as well as release and deployment  Build and manage relationships with supporting IT teams in order to effectively deliver work products to production ', ' Demonstratable experience in Airflow, Luigi or similar orchestration engines ', "" Analytical – You have experience in delivering data analytics solutions that promote data discovery Experience with Snowflake, Amazon Web Services, or related cloud platforms a plus  Understanding of big data technology stacks (Hive / Spark etc) is a plus  Media-focused – Strong knowledge/passion for media including broadcast TV, digital, and film Direct experience working with sources like Nielsen, Adobe Analytics, comScore, and other media/entertainment industry datasets a plus  Communicator – You have excellent verbal and written skills with the ability to communicate ideas effectively across all levels of the organization, both technical and non-technical  Action-oriented – You're constantly figuring out new problems and are regularly showing results with a positive attitude, always displaying ethical behavior, integrity, and building trust Strong understanding of Agile principles and best practices  You’ve dealt with ambiguity and can make quality decisions in a dynamic, fast-paced environment "", 'Qualifications', ' Analytical – You have experience in delivering data analytics solutions that promote data discovery Experience with Snowflake, Amazon Web Services, or related cloud platforms a plus  Understanding of big data technology stacks (Hive / Spark etc) is a plus ', ' Communicator – You have excellent verbal and written skills with the ability to communicate ideas effectively across all levels of the organization, both technical and non-technical ', ' Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. ', ' Desired Characteristics: ', ' Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using cloud-native data engineering principles ', ' Experience with Snowflake, Amazon Web Services, or related cloud platforms a plus ', ' Knowledge of data management fundamentals and data storage principles ', ' Experience in building data pipelines using Python/SQL or similar programming languages ', ' Implement the appropriate design patterns while optimizing performance, cost, security, and scale and end user experience ', ' You’ve dealt with ambiguity and can make quality decisions in a dynamic, fast-paced environment ', 'State/Province', 'Qualifications/Requirements', 'Multiple Locations', "" Bachelor's degree in Computer Science, Data Science, Statistics, Informatics, Information Systems or related field. "", ' Direct experience working with sources like Nielsen, Adobe Analytics, comScore, and other media/entertainment industry datasets a plus ', ' Strong understanding of Agile principles and best practices ', ' Collaborate with business leaders, engineers, and product managers to understand data needs. ']",Not Applicable,Full-time,Information Technology,Broadcast Media,2021-03-18 14:34:51
Data Engineer,Keane Soft Inc,"Dallas, TX",18 hours ago,Be among the first 25 applicants,['Skills'],Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Bloomberg LP,"New York, NY",2 weeks ago,84 applicants,"['What""s the role?', 'Work closely with partners and their teams to understand their business needs and design solutions', 'Dashboard development experience in QlikSense', 'Does this sound like you?', 'Find innovative ways to build business impact', 'Work closely with partners and their teams to understand their business needs and design solutionsPartner with stakeholders to design insightful analytics and key performance indicators that will promote efficiency/operational excellenceThink beyond specific tasks, understand the ultimate goal, and ask the right questions to drive value for the businessFind innovative ways to build business impactEffectively communicate project progress and success to partners in compelling, creative, and accessible waysAnalyze data in a reproducible environment using RStudio/Jupyter Notebook and have the technical capability to integrate different technologies (e.g. SQL and R or SQL and QlikSense) needed in the data analysis process', 'A minimum of 3 years of professional work experience in software development, data engineering, data science or information technology', 'A self-starter attitude, creative problem solving skills, and experience with diverse data processes and systems', ""You'll need to have:"", 'Partner with stakeholders to design insightful analytics and key performance indicators that will promote efficiency/operational excellence', 'Applied machine learning experience', 'Analyze data in a reproducible environment using RStudio/Jupyter Notebook and have the technical capability to integrate different technologies (e.g. SQL and R or SQL and QlikSense) needed in the data analysis process', 'An undergraduate degree or higher in Computer Science, Mathematics, or data technologyA minimum of 3 years of professional work experience in software development, data engineering, data science or information technologyA self-starter attitude, creative problem solving skills, and experience with diverse data processes and systemsApplied machine learning experienceHands-on knowledge of analytics, data modeling, and data visualizationStrong attention to detail and high degree of proven decision- makingFull spectrum of project management skills: ideation, prioritization, communication, and deliveryExcellent interpersonal skills and ability to explain technical processes and solutions to business partners and managementDashboard development experience in QlikSenseDevelopment and design of management KPI dashboards', 'Development and design of management KPI dashboards', 'Hands-on knowledge of analytics, data modeling, and data visualization', 'Excellent interpersonal skills and ability to explain technical processes and solutions to business partners and management', 'Think beyond specific tasks, understand the ultimate goal, and ask the right questions to drive value for the business', ""We'll trust you to:"", 'Effectively communicate project progress and success to partners in compelling, creative, and accessible ways', 'Who We Are:', 'Strong attention to detail and high degree of proven decision- making', 'An undergraduate degree or higher in Computer Science, Mathematics, or data technology', 'Full spectrum of project management skills: ideation, prioritization, communication, and delivery']",Not Applicable,Full-time,Information Technology,Financial Services,2021-03-18 14:34:51
Data Engineer,neteffects,Greater St. Louis,2 days ago,Be among the first 25 applicants,"['', '• Familiarity with creating and maintaining containerized application deployments with a platform like Docker', '• Experience working with scientific datasets, or a background in the application of quantitative science to business problems', '• Work with other top-level talent solving a wide range of complex and unique challenges that have real world impact', '• Be a critical senior member of a data engineering team focused on creating distributed analysis capabilities around a large variety of datasets', '• Experience with stream processing using Apache Kafka', '• Experience with: Google Cloud Platform, Apache Beam and or Google Cloud Dataflow, Google Kubernetes Engine or Kubernetes', 'No Corp to Corp', 'Bonus points for:', '• A level of comfort with Unit Testing and Test Driven Development methodologies', '100% remote position', '• Explore relevant technology stacks to find the best fit for each dataset', '• Take pride in software craftsmanship, apply a deep knowledge of algorithms and data structures to continuously improve and innovate', 'If you share our values, you should have:', '• At least 2 years experience with Go', 'o GraphConnect 2015: https://www.youtube.com/watch?v=6KEvLURBenM', 'o Google Cloud Blog: https://cloud.google.com/blog/products/containers-kubernetes/google-kubernetes-engine-clusters-can-have-up-to-15000-nodes', '• Bioinformatics experience, especially large scale storage and data mining of variant data, variant annotation, and genotype to phenotype correlation', 'Must be a US Citizen or Green Card Holder', 'o Google Cloud Next 2019: https://www.youtube.com/watch?v=fqvuyOID6v4', '• A proven ability to build and maintain cloud based infrastructure on a major cloud provider like AWS, Azure or Google Cloud Platform', '• Experience with protocol buffers and gRPC', 'What you will do is why you should join us:', '• Pursue opportunities to present our work at relevant technical conferences', 'Required skills: minimum 2 years of Go experience, data streaming using Kafka', '• Experience data modeling for large scale databases, either relational or NoSQL', '• Project your talent into relevant projects. Strength of ideas trumps position on an org chart', '• Proven experience (2 years) building and maintaining data-intensive APIs using a RESTful approach']",Mid-Senior level,Full-time,Information Technology,Staffing and Recruiting,2021-03-18 14:34:51
Data Engineer,Softchoice,"Malvern, PA",1 week ago,Be among the first 25 applicants,"['', 'Advanced development skills and strong scripting experience, for example: Python or PowerShell.', 'Apache Spark', 'Some reasons why our employees love working here: ', 'Softchoice has been certified as a Great Place to Work in the United States for several years.', 'Leading by collaboration: Working with software developers, application architects, QA engineers, and operations teams to deliver on – and exceed – client expectations. You will also team up with SME’s, practice leads and vendors to fine tune our strategy and overcome obstacles to adoption at every step.', 'Why you’ll love Softchoice:', 'Knowledge of SQL with High Availability such as Always On Availability Groups', 'Experience in the following areas are an asset: Azure Cosmos DB Azure Synapse Azure Databricks Apache Spark Data Virtualization GIT  ', 'Experience designing and implementing Digital Transformational solutions with expertise in the following areas: Solution design and data migration Security and Compliance Database tuning and optimization Big Data technologies; designing and implementing Data Lakes Integration Services: Azure Data Factory Reporting Tools such as; Power BI, SSRS, etc. Foundational cloud IaaS and PaaS technologies Oracle Python scripting  ', ' Great communication and consultative skills, with the ability to occasionally work onsite with customers and be able to present technically advanced topics to large groups of people with confidence and impact. End to end ability to chart a strategy and identify the solution, while also being able to deliver it and support the client through adoption and maturity Experience working in professional services and/or software engineering for clients in a provider to client relationship. Bonus if experience in a consulting firm. ', 'Delivering excellence: Developing, testing and managing Data cloud native and 3rd party tooling and automation technologies, delivered through code, templates and scripts to ensure top-quality client experiences, delivered with consistency and efficiency. ', 'Foundational cloud IaaS and PaaS technologies', 'Database tuning and optimization', ' Delivering excellence: Developing, testing and managing Data cloud native and 3rd party tooling and automation technologies, delivered through code, templates and scripts to ensure top-quality client experiences, delivered with consistency and efficiency.  Leading by collaboration: Working with software developers, application architects, QA engineers, and operations teams to deliver on – and exceed – client expectations. You will also team up with SME’s, practice leads and vendors to fine tune our strategy and overcome obstacles to adoption at every step. Evangelizing the New: You aren’t just deeply familiar with cutting-edge concepts and technologies. You know how to tailor your insights to customers, occasionally working onsite with customers as a consultant to explore their needs to provide a compelling vision forward. ', ' Softchoice has been recognized as a Best Workplace in Canada by the Great Place to Work Institute for 15 consecutive years. Softchoice has been certified as a Great Place to Work in the United States for several years. We offer meaningful work that drives professional development. Our team members have 2 paid volunteer days per year to give back to a cause of their choice. We offer an opportunity to build a career in the technology industry. We have raised over $3 Million through our team member run charity Softchoice Cares. You will have the opportunity to take an ownership position here at Softchoice. ', 'Prior to commencing employment:', 'What you’ll do:', 'Require an accommodation? We are ready to help:', 'Solution design and data migration', 'Evangelizing the New: You aren’t just deeply familiar with cutting-edge concepts and technologies. You know how to tailor your insights to customers, occasionally working onsite with customers as a consultant to explore their needs to provide a compelling vision forward.', 'Minimum 2 years of experience of IT Cloud Implementations', ' Azure Cosmos DB Azure Synapse Azure Databricks Apache Spark Data Virtualization GIT ', 'Bachelor’s Degree or Diploma in a relevant field or equivalent industry experience.', 'Experience working in professional services and/or software engineering for clients in a provider to client relationship. Bonus if experience in a consulting firm.', 'Reporting Tools such as; Power BI, SSRS, etc.', 'Leading by collaboration', 'Softchoice has been recognized as a Best Workplace in Canada by the Great Place to Work Institute for 15 consecutive years.', 'Expert-level understanding about Data Services', 'Python scripting', 'Azure Databricks', 'We have raised over $3 Million through our team member run charity Softchoice Cares.', 'We offer meaningful work that drives professional development.', 'The impact you will have:', 'Inclusion & Equal opportunity employment:', 'Security and Compliance', 'Big Data technologies; designing and implementing Data Lakes', 'GIT', 'Delivering excellence', 'You will have the opportunity to take an ownership position here at Softchoice.', 'Good understanding about DevOps culture, methodologies, coding and automation.', 'Azure Cosmos DB', 'Oracle', 'Data Engineer', 'End to end ability to chart a strategy and identify the solution, while also being able to deliver it and support the client through adoption and maturity', 'We offer an opportunity to build a career in the technology industry.', 'Strong knowledge of Oracle, SQL Server, Azure SQL Database and Amazon RDS', 'Our team members have 2 paid volunteer days per year to give back to a cause of their choice.', 'You will also have a wealth of professional services and consulting experience, such as:', 'Proven experience driving results with relevant technologies and methods, such as architecting and implementing public cloud data solutions on Azure.', 'Great communication and consultative skills, with the ability to occasionally work onsite with customers and be able to present technically advanced topics to large groups of people with confidence and impact.', 'Bonus points for having multiple related certifications for Azure (DP-200 & DP-201), AWS or GCP.', 'Azure Synapse', ' Expert-level understanding about Data Services Good understanding about DevOps culture, methodologies, coding and automation. Proven experience driving results with relevant technologies and methods, such as architecting and implementing public cloud data solutions on Azure. Bachelor’s Degree or Diploma in a relevant field or equivalent industry experience. Minimum 2 years of experience of IT Cloud Implementations Strong knowledge of Oracle, SQL Server, Azure SQL Database and Amazon RDS Knowledge of SQL with High Availability such as Always On Availability Groups Experience designing and implementing Digital Transformational solutions with expertise in the following areas: Solution design and data migration Security and Compliance Database tuning and optimization Big Data technologies; designing and implementing Data Lakes Integration Services: Azure Data Factory Reporting Tools such as; Power BI, SSRS, etc. Foundational cloud IaaS and PaaS technologies Oracle Python scripting   Experience in the following areas are an asset: Azure Cosmos DB Azure Synapse Azure Databricks Apache Spark Data Virtualization GIT   Advanced development skills and strong scripting experience, for example: Python or PowerShell. Bonus points for having multiple related certifications for Azure (DP-200 & DP-201), AWS or GCP. ', ' Solution design and data migration Security and Compliance Database tuning and optimization Big Data technologies; designing and implementing Data Lakes Integration Services: Azure Data Factory Reporting Tools such as; Power BI, SSRS, etc. Foundational cloud IaaS and PaaS technologies Oracle Python scripting ', 'Data Virtualization', 'Evangelizing the New', 'Our commitment to your experience:', 'Integration Services: Azure Data Factory', 'What you’ll bring to the table:']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer - Waltham,Commonwealth,"Waltham, MA",22 hours ago,Be among the first 25 applicants,"['', ' Experience with ETL tools such as SSIS and Azure Data Factory', ' Creating workflows and transformations with SQL Server Integration Services', ' Building ETL processes for data warehouse and data hub to enable business users to self-serve various data analytics  Fulfilling data requests when the data elements are not yet built in the data warehouse  Building the schema and SQL code behind our complex suite of web-based applications  Creating workflows and transformations with SQL Server Integration Services  Ensuring that data is stored efficiently and can be retrieved quickly within our databases and servers that integrate as an ecosystem to serve our clients’ needs  Troubleshooting and tuning complex SQL statements and making recommendations on indexing strategies  Working on a variety of projects and systems, from big to small and complex to simple  Collaborating with a dynamic team of web developers, business analysts, and product owners to build the best applications in the financial industry  Performing code reviews, learning from peers, and sharing your knowledge in a positive, friendly way', ' Ability to write, troubleshoot, and performance-tune stored procedures and ad-hoc scripts', 'About Commonwealth', ' Picture Yourself Here', ' Financial sector experience a plus', ' The Fine Print', 'Additional Skills And Knowledge', ' Building ETL processes for data warehouse and data hub to enable business users to self-serve various data analytics', ' Experience working with Microsoft Azure Cloud a plus', ' Solid understanding of relational modeling concepts  Ability to write, troubleshoot, and performance-tune stored procedures and ad-hoc scripts  Experience with data warehousing', ' Fulfilling data requests when the data elements are not yet built in the data warehouse', ' Core Strengths', ' Solid understanding of relational modeling concepts', ' Experience with data warehousing', ' Performing code reviews, learning from peers, and sharing your knowledge in a positive, friendly way', ' Ensuring that data is stored efficiently and can be retrieved quickly within our databases and servers that integrate as an ecosystem to serve our clients’ needs', ' Working on a variety of projects and systems, from big to small and complex to simple', ' Troubleshooting and tuning complex SQL statements and making recommendations on indexing strategies', 'Key Responsibilities', ' Experience with ETL tools such as SSIS and Azure Data Factory  5–10 years of experience  Experience working with Microsoft Azure Cloud a plus  Financial sector experience a plus', 'Overview', ' Collaborating with a dynamic team of web developers, business analysts, and product owners to build the best applications in the financial industry', ' Building the schema and SQL code behind our complex suite of web-based applications', ' 5–10 years of experience']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer - Lead Level,USAA,"San Antonio, TX",1 day ago,Be among the first 25 applicants,"['', 'Identifies and manages existing and emerging risks that stem from business activities and the job role.Ensures risks associated with business activities are effectively identified, measured, monitored, and controlled. Follows written risk and compliance policies and procedures for business activities.Design and implement complex technical solutions.Provides guidance to teams to help design, build and manage data pipelines for data structures encompassing data transformation, data models, schemas, metadata, data quality, and workload management.Participate in daily standups and lead design reviews.Breakdown business features into technical stories and approaches.Identify and re-engineer data processes to improve overall efficiency and business value.Create proof of concepts and prototypes.Implement efficient defect management, root cause analysis, and resolution processes.Ensure that team builds processes that support effective data management, metadata management, and good data quality.Mentor and coach junior / experienced engineers.Manage projects within team and work effectively with cross-functional teams.Provide guidance and work with team members to ensure successful completion of tasks', 'Minimum Requirements', 'Preferred', 'Ensure that team builds processes that support effective data management, metadata management, and good data quality.Mentor and coach junior / experienced engineers.', ""Bachelor's degree in related field of study, OR 4 additional years of related experience beyond the minimum required.And, 8 years of data management experience implementing data solutions demonstrating depth of technical understanding within a specific discipline(s)/technology(s) to include 2 years demonstrated technical leadership experience and/or leading teams.Deep experience with and contributes to multiple technologies and product lines across a company."", 'Ensures risks associated with business activities are effectively identified, measured, monitored, and controlled. ', 'Manage projects within team and work effectively with cross-functional teams.', 'Experience leading data engineering and solution design across multiple projects and functional areas', 'Design and implement complex technical solutions.', 'Ability to model and design modern data structures, SQL/NoSQL databases, Data Lakes, Cloud Data Warehouses (SnowFlake preferred)', 'Experience with designing, implementing, monitoring modern data platforms', 'Benefits', 'And, 8 years of data management experience implementing data solutions demonstrating depth of technical understanding within a specific discipline(s)/technology(s) to include 2 years demonstrated technical leadership experience and/or leading teams.', 'to the opening is 3/22/21 by 11:59 pm CST time.', 'Experience in a software engineer role, leveraging Java, Python, Scala or C++', 'About USAA', 'For Internal Candidates', 'Geographical Differential', ""Bachelor's degree in related field of study, OR 4 additional years of related experience beyond the minimum required."", 'Shift premium', 'Relocation', 'Experience with performance optimizations and best practices for scalable data models, pipelines and queries', 'Breakdown business features into technical stories and approaches.', 'Comprehensive knowledge of data management / data governance principles ', 'available', 'Implement efficient defect management, root cause analysis, and resolution processes.', 'Identifies and manages existing and emerging risks that stem from business activities and the job role.', 'Compensation', 'Participate in daily standups and lead design reviews.', 'Provide guidance and work with team members to ensure successful completion of tasks', 'USAA Total Rewards', '$$117,600 - $211,700.', 'Provides guidance to teams to help design, build and manage data pipelines for data structures encompassing data transformation, data models, schemas, metadata, data quality, and workload management.', 'Identify and re-engineer data processes to improve overall efficiency and business value.', 'Experience applying DevOps and DataOps concepts to improve quality and time to market for data and analytics products ', 'not', 'Create proof of concepts and prototypes.', 'Experience with cloud-based data offerings (Amazon AWS, Google GCP, Microsoft Azure)', 'Deep experience with and contributes to multiple technologies and product lines across a company.', 'Last day for internal candidates to apply ', 'Experience with designing, implementing, monitoring modern data platformsExperience leading data engineering and solution design across multiple projects and functional areasExperience applying DevOps and DataOps concepts to improve quality and time to market for data and analytics products Ability to model and design modern data structures, SQL/NoSQL databases, Data Lakes, Cloud Data Warehouses (SnowFlake preferred)Experience with performance optimizations and best practices for scalable data models, pipelines and queriesExperience in a software engineer role, leveraging Java, Python, Scala or C++Experience with cloud-based data offerings (Amazon AWS, Google GCP, Microsoft Azure)Comprehensive knowledge of data management / data governance principles ', 'Follows written risk and compliance policies and procedures for business activities.']",Not Applicable,Full-time,Information Technology,Financial Services,2021-03-18 14:34:51
Data Engineer,The New York Times,"New York, NY",3 days ago,188 applicants,"['', ' Run and support a production enterprise data platform ', 'The New York Times Company is an Equal Opportunity Employer and does not discriminate on the basis of an individual\'s sex, age, race, color, creed, national origin, alienage, religion, marital status, pregnancy, sexual orientation or affectional preference, gender identity and expression, disability, genetic trait or predisposition, carrier status, citizenship, veteran or military status and other personal characteristics protected by law. All applications will receive consideration for employment without regard to legally protected characteristics. The New York Times Company will consider qualified applicants, including those with criminal histories, in a manner consistent with the requirements of applicable state and local ""Fair Chance"" laws.', 'About Us', 'About The Job', ' We have frequent panel discussions and talks by a wide variety of news makers and industry leaders. ', ' Develop processes for automating, testing, and deploying your work ', ' Run and support a production enterprise data platform  Design and develop data models  Work with languages like Java, Python, Go, Bash, and SQL  Build batch and streaming data pipelines with tools such as Spark, Airflow, and cloud-based data services like Google’s BigQuery, Dataproc, and Pub/Sub  Develop processes for automating, testing, and deploying your work ', ' Make an impact by supporting our original, independent and deeply reported journalism.  We provide competitive health, dental, vision and life insurance for employees and their families  We support responsible retirement planning with a generous 401(k) company match.  We offer a generous parental-leave policy, which we recently expanded in response to employee feedback. Birth mothers receive 16 weeks fully paid; non gestational parents receive 10 weeks, also fully paid.  We are committed to career development, supported by a formal mentoring program and $8,000 annual tuition reimbursement.  We have frequent panel discussions and talks by a wide variety of news makers and industry leaders.  Join a community committed to the richness of diversity, experiences and talents in the world we cover, supported by a variety of employee resource groups. ', ' Build batch and streaming data pipelines with tools such as Spark, Airflow, and cloud-based data services like Google’s BigQuery, Dataproc, and Pub/Sub ', 'As a Data Engineer, You Will', 'Benefits And Perks', ' Make an impact by supporting our original, independent and deeply reported journalism. ', ' We are committed to career development, supported by a formal mentoring program and $8,000 annual tuition reimbursement. ', ' Design and develop data models ', ' We support responsible retirement planning with a generous 401(k) company match. ', ' We offer a generous parental-leave policy, which we recently expanded in response to employee feedback. Birth mothers receive 16 weeks fully paid; non gestational parents receive 10 weeks, also fully paid. ', ' Join a community committed to the richness of diversity, experiences and talents in the world we cover, supported by a variety of employee resource groups. ', ' We provide competitive health, dental, vision and life insurance for employees and their families ', 'Job Description', 'About You', 'This role may require limited on-call hours. An on-call schedule will be determined when you join, taking into account team size and other variables. On-call hours are unpaid, unless informed otherwise by your manager. ', ' Work with languages like Java, Python, Go, Bash, and SQL ', ' The New York Times is committed to a diverse and inclusive workforce, one that reflects the varied global community we serve. Our journalism and the products we build in the service of that journalism greatly benefit from a range of perspectives, which can only come from diversity of all types, across our ranks, at all levels of the organization. Achieving true diversity and inclusion is the right thing to do. It is also the smart thing for our business. So we strongly encourage women, veterans, people with disabilities, people of color and gender nonconforming candidates to apply. ']",Entry level,Full-time,Information Technology,Online Media,2021-03-18 14:34:51
Junior Data Engineer,Cypress HCM,Washington DC-Baltimore Area,6 days ago,161 applicants,"['', 'Designing intricate data workflow solutions and data modelsDeveloping SQL and unit testingDesigning and coding review functions for the development projectsResearching and recommending alternative procedures to resolve issuesAnalyzing trends in performance to prevent complications proactively', 'Developing SQL and unit testing', 'Responsibilities', 'Capability to work independently with a self-starting mindset and the capacity to research and find solutions', '1-2 years of developing intricate SQL queries and analyzing data, and systems integrationKnowledge of scripting languages (e.g., Java or Python)Bachelor’s degreeCapability to work independently with a self-starting mindset and the capacity to research and find solutions', '1-2 years of developing intricate SQL queries and analyzing data, and systems integration', 'Researching and recommending alternative procedures to resolve issues', 'Bachelor’s degree', 'Analyzing trends in performance to prevent complications proactively', 'Knowledge of scripting languages (e.g., Java or Python)', 'Designing and coding review functions for the development projects', 'Designing intricate data workflow solutions and data models', 'Requirements']",Associate,Full-time,Engineering,Education Management,2021-03-18 14:34:51
Data Engineer - Athlete Hub,DICK'S Sporting Goods,"Coraopolis, PA",2 days ago,Be among the first 25 applicants,"['', '3+ years of experience being close to the business and delivering value through it as part of a team.', 'Experience with message queuing, stream processing, and highly scalable ‘big data’ data stores (Kafka, Pub/Sub)', 'Work with stakeholders including the product, data and architecture teams to assist with data-related technical issues and support their data infrastructure needs.', 'At Dicks Sporting Goods, we are creating the future of sport driven by powerful data products and platforms that serve our Athletes and Teammates.', 'Experience with relational databases (Oracle, SQL Server, etc..) as well as NoSQL database technologies (MongoDB, BigTable, Cassandra, etc..)', ""Bachelor's Degree in\xa0Computer Science, Software Engineering, Information Systems or Information Technology or related field required, or equivalent experience"", 'Work as part of a team building the data ingestion, products, pipelines, and tooling supporting our Athlete Hub data product driving Marketing, Retail, E-commerce and Enterprise initiaves.', 'Hands-on experience building, managing, and automating data pipelines', 'Experience with Cloud Identity and Access Management for data on public cloud.', 'Three to five years of experience in Data Engineering, AI/ML Engineer Integration, Data Modeling', 'You would partner with marketing, retail, and e-commerce business teams to build the next-generation of Athlete data. Work alongside data science teams building AI/ML models, platform engineers creating services that support you, and software developers creating unique experiences.', 'Proficient in Linux/Unix environments', 'Any Public Cloud certification focused on Cloud Engineering', 'You understand the importance of Kafka, Snowpipe, or Kinesis for real-time needs', 'Provide proactive design, operational support, and governance for privacy and security policy for data.', 'Experience and love for Python, Spark, SQL, or other standard data scripting languagesHands-on experience building, managing, and automating data pipelinesFamiliarity and appreciation for modern public cloud data services from AWS, GCP, or Azure3+ years of experience being close to the business and delivering value through it as part of a team.Some knowledge or exposure to supporting AI/ML engineering and integrating data to model development, management and serving.A good grip of data structures, relationships, integration patterns, and algorithms.You understand the importance of Kafka, Snowpipe, or Kinesis for real-time needsSome experience applying security and privacy to how you manage data.A grasp of the importance of common data platform patterns and how they relate like Data Lake, Data Mesh, Data Catalog, Tagging, Stream Processing, etc.', 'Experience and love for Python, Spark, SQL, or other standard data scripting languages', 'Qualifications', 'Any Public Cloud certification focused on Data Engineering or Data Science', 'We are looking for a Data Engineer who wants to be a part of a team solving the ingestion, enrichment, and activation of data to drive better products, services, and experiences for our Athletes. In this role, you will work with the core of how we use Athlete data working with modern cloud and open source technology to transform the future of sport.', 'A good grip of data structures, relationships, integration patterns, and algorithms.', 'Proficient with object-oriented programming and scripting languages (Python, Java, etc..)', 'Experience with AI/ML workflow frameworks like Kubeflow, Azure AI/ML, AWS Sagemaker, etc.', 'Proficient in developing, maintaining and interacting with APIs', 'Experience with Agile Development and Agile Deployment tools and versioning using Git or similar tools\xa0', 'We are open to remote for this position.', 'A grasp of the importance of common data platform patterns and how they relate like Data Lake, Data Mesh, Data Catalog, Tagging, Stream Processing, etc.', 'Familiarity and appreciation for modern public cloud data services from AWS, GCP, or Azure', 'Some knowledge or exposure to supporting AI/ML engineering and integrating data to model development, management and serving.', 'Some experience applying security and privacy to how you manage data.', 'Experience building data pipelines on modern public cloud services like Snowflake, AWS, GCP, or Azure.', 'Proficient with SQL, Spark, and other common Query languages', '\xa0', 'Job Duties & Responsibilities', 'Work as part of a team building the data ingestion, products, pipelines, and tooling supporting our Athlete Hub data product driving Marketing, Retail, E-commerce and Enterprise initiaves.Work with stakeholders including the product, data and architecture teams to assist with data-related technical issues and support their data infrastructure needs.Provide proactive design, operational support, and governance for privacy and security policy for data.', ""Bachelor's Degree in\xa0Computer Science, Software Engineering, Information Systems or Information Technology or related field required, or equivalent experienceThree to five years of experience in Data Engineering, AI/ML Engineer Integration, Data ModelingAny Public Cloud certification focused on Data Engineering or Data ScienceAny Public Cloud certification focused on Cloud EngineeringExperience building data pipelines on modern public cloud services like Snowflake, AWS, GCP, or Azure.Experience with message queuing, stream processing, and highly scalable ‘big data’ data stores (Kafka, Pub/Sub)Proficient with SQL, Spark, and other common Query languagesExperience with Cloud Identity and Access Management for data on public cloud.Proficient with object-oriented programming and scripting languages (Python, Java, etc..)Experience with continuous integration/continuous delivery (CI/CD) pipelines (Jenkins, Concourse, Azure DevOps)Experience with relational databases (Oracle, SQL Server, etc..) as well as NoSQL database technologies (MongoDB, BigTable, Cassandra, etc..)Experience with AI/ML workflow frameworks like Kubeflow, Azure AI/ML, AWS Sagemaker, etc.Proficient in developing, maintaining and interacting with APIsExperience with Agile Development and Agile Deployment tools and versioning using Git or similar tools\xa0Proficient in Linux/Unix environments"", 'Experience with continuous integration/continuous delivery (CI/CD) pipelines (Jenkins, Concourse, Azure DevOps)', 'What you will bring:', 'This role builds new data products, pipelines, APIs, materialized views, services, and tools. They are fascinated with data and the modern ways of creating rich, scalable data products driving how we work.']",Associate,Full-time,Information Technology,Retail,2021-03-18 14:34:51
Data Engineer,Empower Professionals Inc,"Malvern, PA",1 day ago,Be among the first 25 applicants,[''],Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Mylio,"Bellevue, WA",6 days ago,74 applicants,"['', '3+ years PostgreSQL experience', '5+ years as a Data Engineer or a similar role working with data and databases.\xa03+ years PostgreSQL experienceStrong expertise in creating complex SQL queries AND design (schemas, data types, window functions, extensions, user-defined functions, conversions, etc.),Experience in data warehousing, modeling, and tuningStrong ability to work directly on developing scripts that connect that data warehouse data via APIs to Google Analytics, Salesforce, and other reporting tools such as Microsoft BIStrong expertise with ETLStrong working knowledge of\xa0Salesforce.com is preferred', 'If you are looking to be part of something where you can make a difference, help build an exciting product, and work with smart people…Mylio might be a great place for you. You’ll have responsibility, impact, and upside that you could normally only get in a high-risk, early-stage startup. We offer a collaborative, casual environment with strong benefits and a great work-life balance.', 'Only local Greater Seattle area candidates will be considered. No relocation is available. Please do not apply if you are out of the area.', 'Mylio is changing the way the world remembers!\xa0We are seeking a Data Engineer to help us grow and to help people take back control of their photos.\xa0\xa0Mylio is a category-defining product that allows you to organize the memories of a lifetime. Manage all your photos and documents on your phone, your tablet, your computer, wherever you are even with limited or no connectivity. We work across multiple platforms and devices and solve a major problem that many people still face….being overwhelmed and unorganized with their photos.\xa0', '\xa0', 'Strong ability to work directly on developing scripts that connect that data warehouse data via APIs to Google Analytics, Salesforce, and other reporting tools such as Microsoft BI', 'Experience in data warehousing, modeling, and tuning', 'Strong expertise with ETL', '5+ years as a Data Engineer or a similar role working with data and databases.\xa0', 'Strong expertise in creating complex SQL queries AND design (schemas, data types, window functions, extensions, user-defined functions, conversions, etc.),', 'NOTE: At this time, remote work from the Greater Seattle Area is acceptable. No out-of-state candidates will be considered. The ability to come into the office as needed and eventually primarily work from Bellevue is required', 'Ideal candidates will have the following:', 'Strong working knowledge of\xa0Salesforce.com is preferred']",Associate,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Engineer/Architect,The Select Group,"Raleigh, NC",2 days ago,Be among the first 25 applicants,"['', 'Nice-to-have Skills', 'Deep understanding of data orchestration, ideally with Apache Airflow Basic Python scripting experience  ', 'Help to establish thought leadership in data warehousing space and lead in open source transition', 'Coordinate software design needs between teams and management to meet development benchmarks', 'DATA ENGINEER / ARCHITECT – REMOTE ', 'MySQL database experience doing database administration and engineering work Setting up ETL pipelines and defining ETL processes  ', ' Setting up ETL pipelines and defining ETL processes ', 'Experience working in the healthcare industry', 'Experience working on open-source technologies, ideally working on Java and JavaScript platforms', ' L', ' Basic Python scripting experience ', ' Experience working on open-source technologies, ideally working on Java and JavaScript platforms ', 'Provide technical direction to data engineering team while collaborating with client partners and business analysts to manage client relationships', ' MySQL database experience doing database administration and engineering work Setting up ETL pipelines and defining ETL processes   Deep understanding of data orchestration, ideally with Apache Airflow Basic Python scripting experience   Understanding of at least one NoSQL database, such as MongoDB, DynamoDB, or AWS Redshift Working in a Cloud Environment, ideally AWS Experience working in the healthcare industry ', 'Must-have Skills', 'Daily Responsibilities', 'Setting up ETL pipelines and defining ETL processes', 'Working in a Cloud Environment, ideally AWS', 'The Principal Data Engineer will be responsible for developing the data warehouse solutions in SQL Server and Hadoop platform. The candidate will participate in all phases of the development lifecycle from initial requirements gathering and design through to coding and testing of our data warehousing solutions.', 'Understanding of at least one NoSQL database, such as MongoDB, DynamoDB, or AWS Redshift', 'Design and develop scripts, code and ETL pipelines to leverage client data', ' The Principal Data Engineer will be responsible for developing the data warehouse solutions in SQL Server and Hadoop platform. The candidate will participate in all phases of the development lifecycle from initial requirements gathering and design through to coding and testing of our data warehousing solutions. Provide technical direction to data engineering team while collaborating with client partners and business analysts to manage client relationships Coordinate software design needs between teams and management to meet development benchmarks Help to establish thought leadership in data warehousing space and lead in open source transition Design and develop scripts, code and ETL pipelines to leverage client data Participate in requirements and design Mentoring other associates in architecture, design and development best practices', 'Participate in requirements and design', 'Mentoring other associates in architecture, design and development best practices', 'Basic Python scripting experience']",Not Applicable,Full-time,Information Technology,Computer Networking,2021-03-18 14:34:51
Data Engineer (Finance),Ontario Systems,"Indianapolis, IN",1 day ago,Be among the first 25 applicants,[''],Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Sr. Data Engineer ,Onebridge,Greater Indianapolis,1 day ago,Be among the first 25 applicants,"['', 'Designs, develops and tests databases, data warehouses, data lakes, queries and views, reports, and dashboards', 'Data Engineer |\xa0Key Skills:', 'Participates in the workstream planning process including inception, technical design, development, testing, and delivery of end-to-end BI solutionsDesigns, develops and tests databases, data warehouses, data lakes, queries and views, reports, and dashboardsPerforms data conversions, imports, and exports of data within and between internal and external software systemsMerges BI platforms with enterprise systems and applicationsDocuments new and existing models, solutions, and implementations', 'Possess strong MSBI Stack including SSRS, SSIS, SSAS with advanced T-SQL', 'Participates in the workstream planning process including inception, technical design, development, testing, and delivery of end-to-end BI solutions', 'Experience creating global BI Solutions and working with Cloud BI tools such as AWS or Azure', ""7+ years' experience with advanced Data Engineering\xa0knowledgeExperience creating Tableau dashboards is required. Experience with Team Leadership and architecture development are required. Able to quickly visualize data and create dashboards using other tools such as Power BI, Qlik, and Alteryx are preferred. Possess strong MSBI Stack including SSRS, SSIS, SSAS with advanced T-SQLExpertise in tabular and multidimensional queries (DAX, MDX)Solid understanding of ETL strategies and design, (staging environments, data transformation, change data capture, slowly changing dimensions, etc.), with an eye towards delivering functional and useful solutions in a timely mannerExperience creating global BI Solutions and working with Cloud BI tools such as AWS or AzureDeeply understands bi-directional MDMLeading teams and projects to completion\xa0"", 'Merges BI platforms with enterprise systems and applications', 'Data Engineer\xa0| Core Duties:', 'Deeply understands bi-directional MDM', 'The BI Engineer will serve in a key role within our Data Practice, working with clients to develop end to end data-driven solutions. Must be highly analytical and have the proven ability to develop and reverse engineer complex solutions. Critical thinking and advanced problem-solving skills are core behaviors among the team. Strong team and project lead experience will be valuable for this role.\xa0', 'a\u202ffull-time\xa0Data\xa0Engineer', 'Leading teams and projects to completion\xa0', 'Data Factory including data pipelines, pipeline tasks, factory batches, triggers, control flows, and data flowsWrangling Data Flows, Azure Data Share, Azure DatabricksPySpark, Spark, Scala, Stream Analytics, HDInsight and Hadoop', ""7+ years' experience with advanced Data Engineering\xa0knowledge"", 'Performs data conversions, imports, and exports of data within and between internal and external software systems', 'PySpark, Spark, Scala, Stream Analytics, HDInsight and Hadoop', 'Experience with Team Leadership and architecture development are required. ', 'Solid understanding of ETL strategies and design, (staging environments, data transformation, change data capture, slowly changing dimensions, etc.), with an eye towards delivering functional and useful solutions in a timely manner', 'Data Factory including data pipelines, pipeline tasks, factory batches, triggers, control flows, and data flows', 'Onebridge is seeking a\u202ffull-time\xa0Data\xa0Engineer\xa0to join our growing Data Analytics Practice on-site in Indianapolis supporting a range of leading clients and industry verticals.', 'Experience creating Tableau dashboards is required. ', 'Wrangling Data Flows, Azure Data Share, Azure Databricks', 'Data\xa0Engineer\xa0| Quick Overview:', 'Expertise in tabular and multidimensional queries (DAX, MDX)', 'Able to quickly visualize data and create dashboards using other tools such as Power BI, Qlik, and Alteryx are preferred. ', 'Documents new and existing models, solutions, and implementations', 'Data Engineer | Desired Extras:']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer - Portland,Nectar Markets,"Portland, OR",19 hours ago,Be among the first 25 applicants,"['', 'Building/Accessing APIs', 'Ability to present information/data in highly technical terms to communicate with subject matter experts.', 'Automating reoccurring tasks through the development of production code and data-pipelines.', 'Ensuring data reliability and accuracy by building processes and controls around data compilation and reporting. (ETL /Data Governance)', 'Collaborating in the architecting and implementation of a Data Warehouse/Database(s). ', 'SQL (PostgreSQL, MS SQL Server)', 'Power BI', 'Ability to code in the following languages/packages:Python (Pandas, Plotly/Matplotlib, SQLAlchemy, and sci-kit-learn)SQL (PostgreSQL, MS SQL Server)', 'Compensation And Benefits', 'Desired but not required:VBANoSQL (MongoDB) JS (D3, Plotly/Matplotlib, and/or Leaflet)HTML and CSS/Bootstrap', 'JS (D3, Plotly/Matplotlib, and/or Leaflet)', 'Assisting in the establishment of Nectar’s Code Repository.', 'Excellent written and verbal communication', 'Reports To', 'About The Role', 'EEO Statement', 'Microsoft Excel', 'Qualifications', 'Who You Are', 'Other visualization software (with demonstrated competence)', 'PowerPoint', 'VBANoSQL (MongoDB) JS (D3, Plotly/Matplotlib, and/or Leaflet)HTML and CSS/Bootstrap', 'Who We Are', 'Python (Pandas, Plotly/Matplotlib, SQLAlchemy, and sci-kit-learn)', 'VBA', 'HTML and CSS/Bootstrap', 'Python (Pandas, Plotly/Matplotlib, SQLAlchemy, and sci-kit-learn)SQL (PostgreSQL, MS SQL Server)', 'Ability to present information/data in layman terms to reach a broader audience.', 'NoSQL (MongoDB) ', 'Munging Nectar’s data into an accessible, dependable database.Ensuring data reliability and accuracy by building processes and controls around data compilation and reporting. (ETL /Data Governance)Assisting Data Analysts in providing accurate and timely reporting to management and other stakeholders.Automating reoccurring tasks through the development of production code and data-pipelines.Assisting in the establishment of Nectar’s Code Repository.Collaborating in the architecting and implementation of a Data Warehouse/Database(s). ', 'PowerPointTableauPower BIOther visualization software (with demonstrated competence)', 'Databasing/Data Warehousing – Structures / Architecture / Governance', 'Assisting Data Analysts in providing accurate and timely reporting to management and other stakeholders.', 'Excellent written and verbal communicationMicrosoft ExcelDatabasing/Data Warehousing – Structures / Architecture / GovernanceBuilding/Accessing APIsAbility to code in the following languages/packages:Python (Pandas, Plotly/Matplotlib, SQLAlchemy, and sci-kit-learn)SQL (PostgreSQL, MS SQL Server)Desired but not required:VBANoSQL (MongoDB) JS (D3, Plotly/Matplotlib, and/or Leaflet)HTML and CSS/BootstrapExperience with at least one of the following:PowerPointTableauPower BIOther visualization software (with demonstrated competence)Ability to present information/data in layman terms to reach a broader audience.Ability to present information/data in highly technical terms to communicate with subject matter experts.', 'Munging Nectar’s data into an accessible, dependable database.', 'Experience with at least one of the following:PowerPointTableauPower BIOther visualization software (with demonstrated competence)', 'Job Title', 'Tableau']",Entry level,Full-time,Information Technology,Construction,2021-03-18 14:34:51
Senior Data Engineer,Motion Recruitment,"Fairfax County, VA",,N/A,"['', 'Strong analytical, verbal and written communications skillsMust be a critical thinker and self-starterAbility to work in fast-paced nature of a high-growth organization', 'This is an innovative and exciting health technology company uses B2B2C technology infrastructure with an emphasis on precision medicine. They are a rapidly growing company that values independent, dynamic and strategic thinkers who are ready to analyze, troubleshoot and resolve complex business and technical problems. You will work with large, complex, cloud-based scientific datasets and will help identify and answer questions that will shape scientific studies. Their adaptive platform for personalized engagement enables longitudinal data collection, engagement, and insights. The National Institutes of Health selected this company\xa0as the technology platform for the Participant Technology Center of the Precision Medicine Initiative. Their mission is to create more personalized and holistic approaches to population research and therapeutic interventions as they produce data-driven solutions that prevent, monitor, and diagnose various diseases.', 'Bachelor degree with 8+ years of experience or equivalent', 'Required Skills & Experience', 'Competitive Salary: Up to $140K/year + Benefits', 'Experience with using\xa0AWS based solutions for data management', 'Bachelor degree with 8+ years of experience or equivalentExtensive, advanced knowledge of Python 3, JavaExperience with using\xa0AWS based solutions for data managementExpertise with\xa0SQLExperience deploying data lake solutions like Hadoop, Hive, Spark, Databricks', 'Must be a critical thinker and self-starter', 'Expertise with\xa0SQL', 'Experience deploying data lake solutions like Hadoop, Hive, Spark, Databricks', 'Ability to work in fast-paced nature of a high-growth organization', 'Right now, they are looking for an experienced Data Engineer to join their team. You\xa0will work with very large, complex, cloud-based scientific datasets and help identify and answer questions that will shape scientific studies. The work that you do will contribute to improving the health of people across the nation. You will be responsible for developing, optimizing and maintaining code and data pipelines.', 'The Offer', 'Extensive, advanced knowledge of Python 3, Java', 'Applicants must be currently authorized to work in the United States on a full-time basis now and in the future.', 'Strong analytical, verbal and written communications skills', 'Desired Skills & Experience']",Mid-Senior level,Full-time,Engineering,Mental Health Care,2021-03-18 14:34:51
Data Analytics Engineer,Novanta Inc.,"Will-More, VA",23 hours ago,Be among the first 25 applicants,"['', 'Expert SQL Fluency (Well versed in CTEs and window functions)', '3+ years - Experience using Power BI, Tableau, or similar data visualization tool', 'Analyze assigned projects for data quality issues. Troubleshoot and resolve issues as they arise.', '3+ years - Experience with ETL/ELT Tools (ex. API, Informatica)', 'Required Experience, Education, Skills And Competencies', 'Design, develop and maintain scaled, automated, user-friendly systems, reports, dashboards, etc.', 'Communication', 'Primary Responsibilities', 'Job Summary', 'Draw insights from data and clearly communicate findings to stakeholders and external customers', 'Demonstrated ability in data modeling, ETL/ELT, data pipelines, EDW', 'Motivated individual with strong analytic, problem solving, and troubleshooting skills', 'None', 'Interface with business customers to gather data and metrics requirements, then driving analytic projects to solve complex challengesDraw insights from data and clearly communicate findings to stakeholders and external customersProvide exceptional customer service to stakeholders through project execution and timely delivery of solutions', 'Experienced building data warehouse infrastructure and BI tables', 'Business Unit Overview', 'Serve as a catalyst for sharing knowledge, information, and ideas throughout the company as it relates to business intelligence', 'This position can be based from a Remote home office anywhere in the U.S.', 'Automate standard report creation and sharing using tools or scripts', 'Provide exceptional customer service to stakeholders through project execution and timely delivery of solutions', 'Interface with business customers to gather data and metrics requirements, then driving analytic projects to solve complex challenges', '3+ years - Experience with MS SQL Server and Snowflake', ' Bachelor’s degree in Information Technology 3+ years - Experience with MS SQL Server and Snowflake3+ years - Experience with ETL/ELT Tools (ex. API, Informatica)3+ years - Experience using Power BI, Tableau, or similar data visualization toolExpert SQL Fluency (Well versed in CTEs and window functions)Demonstrated ability in data modeling, ETL/ELT, data pipelines, EDWExperienced building data warehouse infrastructure and BI tablesMotivated individual with strong analytic, problem solving, and troubleshooting skills', 'Design, develop and maintain scaled, automated, user-friendly systems, reports, dashboards, etc.Write complex, production-quality (i.e., accurate, performant, and maintainable) data transformation code to solve the needs of analysts, and business stakeholders (ex. MS SQL Server, Oracle, and Snowflake)Analyze assigned projects for data quality issues. Troubleshoot and resolve issues as they arise.Automate standard report creation and sharing using tools or scriptsConvert raw data into consumable information applying business logic and utilizing clean engineering workflowsEnsure that data, systems, architecture, business logic, and metrics are well-documentedSupport the acquisition of external data sets, interpreting data layouts, structures, fields, and values to incorporate new data into the core analytics databaseServe as a catalyst for sharing knowledge, information, and ideas throughout the company as it relates to business intelligence', 'Support the acquisition of external data sets, interpreting data layouts, structures, fields, and values to incorporate new data into the core analytics database', 'Less than 20%', 'Convert raw data into consumable information applying business logic and utilizing clean engineering workflows', 'Company Overview:', 'Write complex, production-quality (i.e., accurate, performant, and maintainable) data transformation code to solve the needs of analysts, and business stakeholders (ex. MS SQL Server, Oracle, and Snowflake)', 'Ensure that data, systems, architecture, business logic, and metrics are well-documented', ' Bachelor’s degree in Information Technology ']",Entry level,Full-time,Information Technology,Electrical/Electronic Manufacturing,2021-03-18 14:34:51
Data Engineer,DISYS,"Boise, ID",2 days ago,Be among the first 25 applicants,"['', 'Preferred Qualifications:', 'GitHub source code management', 'Experience:\xa0', 'Utilize critical thinking skills.', 'Talend MDM', 'TFS\xa0', 'Experience with data visualization solutions', 'Model data structures to optimize reporting and represent appropriate business rules.', 'Ability to:', 'Knowledge of:', 'MDX and T-SQL Languages ', 'Experience supporting and designing ETL processes.', 'WhereScape', 'POSITION PURPOSE:', 'Tableau visualizationWhereScapeTalend MDMGitHub source code managementTFS\xa0', 'Project management', 'Utilize critical thinking skills.Apply logical data structures to business processes.Model data structures to optimize reporting and represent appropriate business rules.Act as a subject matter expert in healthcare data', 'Experience:\xa0Five years in requirements gathering, design, development, and support of business intelligence (BI) solutions', 'Act as a subject matter expert in healthcare data', 'Education:\xa0\xa0', 'Sound decision making and problem solving.', 'Healthcare Experience', 'Apply logical data structures to business processes.', 'Java', 'QUALIFICATIONS: (Minimum qualifications\xa0required\xa0for the job)\xa0', 'required\xa0', 'Technology Stack Knowledge:', 'Data Mart design, architecture, and construction', 'DISYS is interested in every candidate but at this time our client is not sponsoring H1B and will not be engaged in C2C.', 'Knowledge, Skills and Abilities (KSAs):', 'VB.Net', 'Knowledge of data warehouse methodologies', 'Assist the existing Health Data Services design, development, implementation, documentation, and support of Business Intelligence solutions.\xa0\xa0Provide technical leadership and consulting for business users and IS professionals on the design, development, and utilization of BCI’s analytical tools, technologies, and processes.\xa0', 'Education:\xa0\xa0Bachelor’s Degree in Information Technology/Computer Science, or Computer Engineering; Master’s degree or emphasis on Data Management preferred.\xa0', 'Requirements gathering in a BI setting.Data Vault 2.0', 'Knowledge of JavaScript, Ajax, Web parts, XML (XSD/XSLT), and ASP.NETExperience supporting and designing ETL processes.VB.NetExperience with data visualization solutionsKnowledge of data warehouse methodologiesMicrosoft Certification (MCP, MCSE)Healthcare Experience', 'Self-directed while acting as an enabling, collaborative team member.\xa0', 'MDX and T-SQL Languages JavaProject managementData Mart design, architecture, and constructionStrong written and verbal communicationSound decision making and problem solving.Organization and planningHigh level of initiativeSelf-directed while acting as an enabling, collaborative team member.\xa0', 'for the job)', 'Strong written and verbal communication', 'Microsoft Certification (MCP, MCSE)', 'Organization and planning', 'Minimum qualifications\xa0', 'DISYS is looking for a 100% REMOTE Data Engineer for one of our direct clients based in Boise, Idaho.', '\xa0', 'Knowledge of JavaScript, Ajax, Web parts, XML (XSD/XSLT), and ASP.NET', 'Tableau visualization', 'High level of initiative', 'Data Vault 2.0', 'Skills:', 'Requirements gathering in a BI setting.', 'QUALIFICATIONS: (', 'Experience:\xa0Five years in requirements gathering, design, development, and support of business intelligence (BI) solutionsEducation:\xa0\xa0Bachelor’s Degree in Information Technology/Computer Science, or Computer Engineering; Master’s degree or emphasis on Data Management preferred.\xa0']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Enverus,"Conshohocken, PA",6 days ago,Be among the first 25 applicants,"['', 'Experience with workflow automation and scheduling such as Airflow/Kubeflow/Prefect. ', 'Experience with public cloud services like AWS/Azure/GCP. ', 'Build robust data science platform for feature discovery, feature serving, data versioning, data monitoring, and testing. Design flexible infrastructure and services that can grow with data science technologies and approaches. Work with data scientists to improve the efficiency of their models and bring their models to production. Actively engage with the community to learn and share new ideas that can improve existing workflows and platforms. Understanding data science concepts, experience working with data scientists and operating data science models in production services. Self-starter who can see the big picture, and self-motivated to venture beyond the obvious. Develop reusable, maintainable, and efficient production-ready code. Open to new technologies and ideas, but carefully evaluate the pros and cons before bringing these to production.', 'Actively engage with the community to learn and share new ideas that can improve existing workflows and platforms. ', 'Job Knowledge - We support your professional growth as well as encourage you to share your knowledge within the organization. ', 'Understanding data science concepts, experience working with data scientists and operating data science models in production services. ', 'Respect - We believe respect is the basis of all relationships both inside and out. ', 'Design flexible infrastructure and services that can grow with data science technologies and approaches. ', 'Experience with Docker and Kubernetes. ', 'Build robust data science platform for feature discovery, feature serving, data versioning, data monitoring, and testing. ', 'At least 3 years of hands-on software development experience. ', 'Communication - Not only accurately communicating a message but being an active listener. This is key to our team-oriented environment. ', 'Reliability - Be someone your team can rely on to finish what you start. ', 'Develop reusable, maintainable, and efficient production-ready code. ', 'Strong programming skills in Scala/Java/Python. ', 'Experience with building data science serving infrastructure and deploying data science models to production.', 'Customer Centric - always Keeping external and internal customers in mind, constantly thinking “How will this affect customers I work with?” ', 'Description', 'Work with data scientists to improve the efficiency of their models and bring their models to production. ', 'Egoless culture - We recognize as an organization that the work we are doing cannot be accomplished by a single individual. As such we value tremendously everyone’s inputs at all levels. As one team, we strive to find the right balance between providing elegant solutions and time to market. Job Knowledge - We support your professional growth as well as encourage you to share your knowledge within the organization. Quality of Work - You care about what you do. Customer Centric - always Keeping external and internal customers in mind, constantly thinking “How will this affect customers I work with?” Communication - Not only accurately communicating a message but being an active listener. This is key to our team-oriented environment. Respect - We believe respect is the basis of all relationships both inside and out. Reliability - Be someone your team can rely on to finish what you start. Integrity - Be honest and ethical in all your relationships and decisions. Collaboration and Teamwork - Champion an environment that supports effective teamwork by earning the trust and respect of those around you. ', 'At least 3 years of hands-on software development experience. Strong programming skills in Scala/Java/Python. Experience with big data technologies such as Spark/Flink/Beam. Experience with workflow automation and scheduling such as Airflow/Kubeflow/Prefect. Experience with monitoring and dashboard platforms such as Grafana/Datadog.Experience with Docker and Kubernetes. Experience with public cloud services like AWS/Azure/GCP. Experience with building data science serving infrastructure and deploying data science models to production.', 'Integrity - Be honest and ethical in all your relationships and decisions. ', 'Quality of Work - You care about what you do. ', 'Collaboration and Teamwork - Champion an environment that supports effective teamwork by earning the trust and respect of those around you. ', 'Experience with monitoring and dashboard platforms such as Grafana/Datadog.', 'Self-starter who can see the big picture, and self-motivated to venture beyond the obvious. ', 'Egoless culture - We recognize as an organization that the work we are doing cannot be accomplished by a single individual. As such we value tremendously everyone’s inputs at all levels. As one team, we strive to find the right balance between providing elegant solutions and time to market. ', 'Open to new technologies and ideas, but carefully evaluate the pros and cons before bringing these to production.', 'Experience with big data technologies such as Spark/Flink/Beam. ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Sr Data Engineer,Shortest Track,"Evanston, IL",1 day ago,Be among the first 25 applicants,"['', 'About You', 'About Shortest Track', 'About The Role']",Associate,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Data Engineer,Raymond James,"St Petersburg, FL",24 hours ago,Be among the first 25 applicants,"['', 'Raymond James Guiding Behaviors', 'Schedule', ' Work with and through others to achieve desired outcomes', ' Grow professionally and inspire others to do the same Work with and through others to achieve desired outcomes Make prompt, pragmatic choices and act with the client in mind Take ownership and hold themselves and others accountable for delivering results that matter Contribute to the continuous evolution of the firm', 'Organization', 'Travel', ' Make prompt, pragmatic choices and act with the client in mind', 'Performs Other Duties And Responsibilities As Assigned.', 'Primary Location', 'Shift', ' Take ownership and hold themselves and others accountable for delivering results that matter', ' Contribute to the continuous evolution of the firm', ' Grow professionally and inspire others to do the same', 'Job', 'Requirements', 'We Expect Our Associates At All Levels To', 'Duties']",Not Applicable,Full-time,Information Technology,Banking,2021-03-18 14:34:51
"Senior Data Engineer, Data and AI Team",Brainbase,"New York, NY",21 hours ago,Be among the first 25 applicants,"['', 'Strong skills with Python. Knowing JavaScript/TypeScript is a plus;', 'Proven experience as Data Engineer or in a similar role;Proven ability to work with transactional (OLTP) and analytical (OLAP) relational databases. General knowledge of column stores, NoSQL databases, object-based storage solutions is a plus.Extensive experience of ETL tools such as AirFlow or Luigi;Strong skills with Python. Knowing JavaScript/TypeScript is a plus;Understanding of data structures, data modeling, and algorithms;Experience working with AWS. Knowing Azure, GCP is a plus;Experience with Linux distributions;Outstanding analytics thinking and problem-solving skills;Ability to document your work in a way that it’s easy to understand by everyone necessary;Agile, open mindset, excellent communication, teamwork, and English skills', 'Proven experience as Data Engineer or in a similar role;', 'Agile, open mindset, excellent communication, teamwork, and English skills', 'Participate in Data and Software Architecture design processes and take main responsibility choosing the right data systems for particular applications and future use-cases;', 'Unlimited office snacks', 'Team lunches, happy hours, and office events', 'Brainbase is looking for a Senior Data Engineer to upgrade and design data solutions. In this role you will build databases, ETL processes, participate in data architecture design, requirements analysis, and collaboration with engineers and main stakeholders.', 'Design and build Data Infrastructure and Tooling used by Analytics Engineers, Machine Learning Engineers, and other parties interested in data.Participate in Data and Software Architecture design processes and take main responsibility choosing the right data systems for particular applications and future use-cases;Design, develop and maintain core ETL processes;Develop new tools and libraries for effective and high-quality work around data;Conduct risk assessment and requirement analysis;', 'Understanding of data structures, data modeling, and algorithms;', 'Fun, fast-growing team working on disrupting an extremely outdated industry', 'Wellness & Gym Membership Reimbursement', 'Experience working with AWS. Knowing Azure, GCP is a plus;', 'Benefits', 'Proven ability to work with transactional (OLTP) and analytical (OLAP) relational databases. General knowledge of column stores, NoSQL databases, object-based storage solutions is a plus.', 'Design, develop and maintain core ETL processes;', 'Competitive payTeam lunches, happy hours, and office eventsUnlimited office snacksFun, fast-growing team working on disrupting an extremely outdated industryHealth, Vision, and Dental InsuranceFlexible PTOWellness & Gym Membership ReimbursementStudent Loan Repayment AidContinued Learning ReimbursementDiscounted Tickets & Events', 'Conduct risk assessment and requirement analysis;', 'Brainbase is a venture-backed technology company building modern software for end-to-end intellectual property management. We help companies protect and monetize their most valuable assets in a simple, automated platform.', 'Develop new tools and libraries for effective and high-quality work around data;', 'Flexible PTO', 'Competitive pay', 'Requirements', 'You will have an opportunity to take full responsibility for all Brainbase data solutions and make strategic decisions towards business goals.', 'Description', 'You as Senior Data Engineer will:', 'Continued Learning Reimbursement', 'Extensive experience of ETL tools such as AirFlow or Luigi;', 'Discounted Tickets & Events', 'Health, Vision, and Dental Insurance', 'Brainbase has raised $12M from top venture capital firms and works with leading brands including Sanrio, BuzzFeed, MGM and more. This is an opportunity to join a fast-growing team working on transforming an extremely outdated industry.', 'Design and build Data Infrastructure and Tooling used by Analytics Engineers, Machine Learning Engineers, and other parties interested in data.', 'Experience with Linux distributions;', 'Outstanding analytics thinking and problem-solving skills;', 'Student Loan Repayment Aid', 'Ability to document your work in a way that it’s easy to understand by everyone necessary;']",Mid-Senior level,Full-time,Engineering,Internet,2021-03-18 14:34:51
Data Engineer,Securian Financial,Greater Minneapolis-St. Paul Area,1 week ago,117 applicants,"['', 'Preferred Qualifications:', 'Collaborate with business users and IT partners to understand requirements and analyze data needs.\xa0Document requirements and build data flow diagrams.', 'Data validation and cleansing', 'Ability to perform simple grasping including handwriting, paper manipulation, sorting, folding, etc. periodically throughout entire workday', 'Tuition reimbursement program. We value continuous learning at Securian!', 'employment@securian.com', 'Experience with tools and concepts related to data and analytics, such as dimensional modeling, ETL, reporting tools, data warehousing, structured and unstructured data.', 'Agile experience', 'Build and maintain data pipelines for populating data domains.', 'Securian Financial Group, Inc. does not discriminate based on race, color, creed, religion, national origin, sex, gender identity, sexual orientation, age, marital or familial status, pregnancy, disability, genetic information, political affiliation, veteran status, status in regard to public assistance, status in a local human rights commission, or any other status or condition protected by local, state or federal law. If you are a job seeker with a disability and require an accommodation to apply for one of our jobs, please contact us by telephone 651-665-5522 (voice), 711 (telecommunications relay), or by email at\xa0EmployeeRelations@securian.com', 'Create data transformations, including data cleansing and data masking.', 'Equal employment opportunity:\xa0\xa0', 'Participate in Data Governance initiatives.', 'Location: St. Paul, MN', 'Responsibilities include but not limited to:', '\ufeff', 'Experience working with business users to analyze requirements and design solutions that meet their needs.', 'Previous experience as a Data Engineer, or related field.', 'Ergonomic sit/stand desks.', 'Previous experience as a Data Engineer, or related field.4 year degree in Computer Science or related fieldExperience with tools and concepts related to data and analytics, such as dimensional modeling, ETL, reporting tools, data warehousing, structured and unstructured data.ETL experience using tools such as Informatica or StreamSetsData validation and cleansingANSI SQL Skills/ExperienceExposure and experience with traditional and modern data architecturesCloud knowledge and experience – AWS preferredExperience working with business users to analyze requirements and design solutions that meet their needs.Strong problem solving and analytical abilitiesAttention to detailSolid communication skillsWillingness to learn', '*Securian Financial Groups internal position title is\xa0Engineering Sr. Analyst', 'ANSI SQL Skills/Experience', 'Ability to utilize keyboard, mouse and computer for up to 8 hours per dayAbility to work at least 40 hours per weekAbility to utilize telephone for up to 8 hours per dayAbility to perform simple grasping including handwriting, paper manipulation, sorting, folding, etc. periodically throughout entire workday', 'Generous paid time off. We want you to take time off for whatever matters most to you!', 'XML/XSD/DTD/JSON/', 'Ability to utilize keyboard, mouse and computer for up to 8 hours per day', 'The ideal candidate is a skilled data engineer with demonstrated experience. The candidate should have cloud experience and be familiar with traditional and modern data architectures and tooling. The candidate should also be curious and enthusiastic about exploring and building data platform capabilities to achieve maximum value to the organization.', 'Play a key role in helping Securian implement the Enterprise Data Strategy by building an Enterprise Data Store, populating it with clean and well-structured data, and participating in other key initiatives in the strategy.', 'Exposure and experience with traditional and modern data architectures', 'Generous paid time off. We want you to take time off for whatever matters most to you!Tuition reimbursement program. We value continuous learning at Securian!Company-funded pension plan as well as 401k retirement plan - great resources to secure your financial future.Continuous opportunities for new challenges.Variety of health plan options as well as dental and vision plans.Discounted Metro Transit costs. All buses and the light rail stop within 3 blocks of our buildings.Paid maternity/paternity leaves.Ergonomic sit/stand desks.', '\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0', 'Securian Financial is based in downtown St. Paul for\xa0nearly 140 years. We’re committed to giving back to our community, donating 15,000+ employee volunteer hours this past year. We also provide extensive resources to our employees for professional development and growth and a number of diverse professional and social opportunities throughout the company.\xa0There’s a reason our employees have voted us as a best place to work year after year! In addition:\xa0', 'Cloud knowledge and experience – AWS preferred', 'Company-funded pension plan as well as 401k retirement plan - great resources to secure your financial future.', 'This posting is for an experienced Data Engineer.\xa0Data Engineering responsibilities will include activities such as building and maintaining data pipelines, data transformation, cleansing, and masking, and building data APIs.', 'Solid communication skills', 'Paid maternity/paternity leaves.', 'Willingness to learn', 'Spark/Python/Java development experience', 'Qualifications:', 'Collaborate with business users and IT partners to understand requirements and analyze data needs.\xa0Document requirements and build data flow diagrams.Build and maintain data pipelines for populating data domains.Create data transformations, including data cleansing and data masking.Participate in Data Governance initiatives.Play a key role in helping Securian implement the Enterprise Data Strategy by building an Enterprise Data Store, populating it with clean and well-structured data, and participating in other key initiatives in the strategy.', 'Position Summary:', 'Spark/Python/Java development experienceReport development experience - Business Objects, TableauXML/XSD/DTD/JSON/Previous experience or knowledge of Insurance/Financial Services industry.Agile experience', 'Location:', 'Continuous opportunities for new challenges.', 'Physical job requirements:', 'Strong problem solving and analytical abilities', 'Please contact us at\xa0employment@securian.com\xa0for further questions.', 'Variety of health plan options as well as dental and vision plans.', 'Ability to work at least 40 hours per week', 'The physical job requirements described above are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.', 'What you could expect from us:\xa0\xa0', 'Attention to detail', 'Previous experience or knowledge of Insurance/Financial Services industry.', 'Securian is on an exciting journey to stand up a mature data engineering discipline focused on enterprise data. One key stepping-stone is defining and implementing a data strategy and associated process and technologies to successfully leverage data as an asset.\xa0', '\xa0', 'Report development experience - Business Objects, Tableau', 'Discounted Metro Transit costs. All buses and the light rail stop within 3 blocks of our buildings.', '4 year degree in Computer Science or related field', '\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0', 'Ability to utilize telephone for up to 8 hours per day', 'EmployeeRelations@securian.com', 'ETL experience using tools such as Informatica or StreamSets']",Mid-Senior level,Full-time,Information Technology,Financial Services,2021-03-18 14:34:51
Data Engineer,AAA-The Auto Club Group,"Minneapolis, MN",1 day ago,Be among the first 25 applicants,"['', 'Preferred Qualifications:', 'Understanding of database structures, theories, principles, and practices;', 'Developing positive relationships with IT and business intelligence departments and personnel', 'Performs analysis and produces meaningful reports on business policies, processes, and performance with a focus on risk and control.', 'The Auto Club Group offers a competitive compensation and benefits package including a base salary with performance based incentives; medical/dental/vision insurance, 401(k), generous time off, a complimentary AAA Membership and much more!', '5+ years of experience in an analytics or business intelligence setting, including experience using one or more of the following analytic tools: SAD, SQL, R, Python, SPSS', 'Mathematical/statistical, reasoning, and logic skills; Understanding of statistical concepts (e.g. hypothesis testing, regression analysis)', 'The Auto Club Group (ACG) provides membership, travel, insurance and financial services offerings to approximately 9 million members and customers across 11 states and 2 U.S. territories through the AAA, Meemic and Fremont brands. ACG belongs to the national AAA federation and is the second largest AAA club in North America.\xa0', 'Work Environment:', 'Demonstrated ability to communicate, verbally and in writing, complex information to others in an understandable way', 'Dashboard design and development; building intuitive interfaces, infographics, and visualization to tell stories with data', 'Master’s degree in Mathematics, Computer Science, Statistics, or related quantitative field', 'Working within an Internal Audit department, analyzes large, diverse data sets to drive better business decisions and to identify business problems, in the context of financial, regulatory, reputational, and operational risk.', 'Extensive experience in:', 'Extensive knowledge and skills:', '3+ years in an analytics or business intelligence setting,\xa0including experience using one or more of the following analytic tools: SAS, SQL, R, Python, SPSS', 'Designing and delivering reports, visualizations, or presentations that are understandable for both technical and non-technical audiences, and explain the importance and relevance of results to business requirements;', 'Translating high level business goals into the tasks and technical specifications needed to perform meaningful data analysis;', 'Applying analytic skills to solve problems creatively; Ability to investigate and determine root causes of issues; Strong analytical problem solving, requirements facilitation, and collaboration skills; Consistently looks for better ways to achieve business results; recognizes problems as opportunities for process improvement', 'Data Analytics Lead - Audit', 'Qualifications', 'Proactively monitoring daily processes, performance strategies,and results to ensure consistent coverage over business strategies and goals', 'Familiarity with agile development methodologies', 'Description', 'Troubleshooting data issues; data reconciliation experience with consistent attention to detail', 'Mines data, performs quantitative analysis and creates clear and actionable narratives about the business.', '(21000065)', 'Insurance or Banking industry backgroundMaster’s degree in Mathematics, Computer Science, Statistics, or related quantitative field\xa0Experience:5+ years of experience in an analytics or business intelligence setting, including experience using one or more of the following analytic tools: SAD, SQL, R, Python, SPSSWorking with Internal Audit or on an audit data analytics teamDashboard design and development; building intuitive interfaces, infographics, and visualization to tell stories with dataAbility to complete advanced statistical analysisProactively monitoring daily processes, performance strategies,and results to ensure consistent coverage over business strategies and goalsFamiliarity with agile development methodologiesExperience with IBM Cognos TM1 or PeopleSoft', 'Extracting and manipulating large data sets in Oracle databases and mainframe sequential files for analysis, including integration of diverse data sources', 'Offers recommendations for new data analytic techniques and methodologies.', 'Requirements gathering and documentation;', 'Extracting and manipulating large data sets in Oracle databases and mainframe sequential files for analysis, including integration of diverse data sourcesInterpreting and understanding business strategies and goals to find emerging issues and root-causes for anomaliesComplex data analysisRequirements gathering and documentation;Troubleshooting data issues; data reconciliation experience with consistent attention to detailDesigning and delivering reports, visualizations, or presentations that are understandable for both technical and non-technical audiences, and explain the importance and relevance of results to business requirements;Translating high level business goals into the tasks and technical specifications needed to perform meaningful data analysis;Developing positive relationships with IT and business intelligence departments and personnelWorking with PC software applications (e.g., Word, Excel, Visio)\xa0\xa0Knowledge and Skills:Extensive knowledge and skills:Mathematical/statistical, reasoning, and logic skills; Understanding of statistical concepts (e.g. hypothesis testing, regression analysis)Data design principlesUnderstanding of database structures, theories, principles, and practices;Solid understanding of data capture, data mapping, and data cleansingApplying analytic skills to solve problems creatively; Ability to investigate and determine root causes of issues; Strong analytical problem solving, requirements facilitation, and collaboration skills; Consistently looks for better ways to achieve business results; recognizes problems as opportunities for process improvementSelf-motivated, flexible, organized, and able to perform with minimal supervision; Pro-actively make recommendations as issues and opportunities ariseDemonstrated ability to communicate, verbally and in writing, complex information to others in an understandable way', 'Insurance or Banking industry background', 'Data design principles', ""Bachelor's degree in Mathematics, Computer Science, Statistics, or related quantitative field\xa0"", 'Work in a temperature-controlled office environment.\xa0Occasional (less than 10% of work time) travel required.', 'Self-motivated, flexible, organized, and able to perform with minimal supervision; Pro-actively make recommendations as issues and opportunities arise', 'Solid understanding of data capture, data mapping, and data cleansing', 'Required Qualifications:', 'Data Analytics Lead - Audit\xa0-\xa0(21000065)', 'Important Note: The above statements describe the principal and essential functions, but not all functions that may be inherent in the job.\xa0This job requires the ability to perform duties contained in the job description for this position, including, but not limited to, the above requirements.\xa0Reasonable accommodations will be made for otherwise qualified applicants, as needed, to enable them to fulfill these requirements.', '\xa0\xa0', 'Interpreting and understanding business strategies and goals to find emerging issues and root-causes for anomalies', 'Working with PC software applications (e.g., Word, Excel, Visio)', 'Primary Duties and Responsibilities:', 'Knowledge and Skills:', 'Complex data analysis', '\xa0', 'Experience:', 'The Auto Club Group, and all of its affiliated companies, is an equal opportunity/affirmative action employer.\xa0All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, gender identity, sexual orientation, national origin, disability or protected veteran status.', 'Working with Internal Audit or on an audit data analytics team', 'Conducts research and special studies when needed; Performs ad hoc analysis and provides meaningful reporting to operations and the Internal Audit leadership team.', 'Ability to complete advanced statistical analysis', '-', 'Experience with IBM Cognos TM1 or PeopleSoft']",Mid-Senior level,Full-time,Information Technology,Insurance,2021-03-18 14:34:51
Data Engineer,Eliassen Group,"Atlanta, GA",2 days ago,66 applicants,"['', 'Design, develop and maintain automated data solutions based on the identification, collection, and eval of business requirements.', 'Professional experience with Java or Python', 'Azure or AWS Cloud experience', 'Design, develop and maintain automated data solutions based on the identification, collection, and eval of business requirements.Design, develop, and maintain automated data solutions data processing (Data Ingest, Data Store, Data Management)', 'Design, develop, and maintain automated data solutions data processing (Data Ingest, Data Store, Data Management)', 'For immediate consideration, please email your updated resume to Jerry King: jking@eliassen.com ', 'Experience building and managing Data Pipelines', 'Data modeling for ETL/Data Warehousing\xa0\xa0', 'Data Engineer\xa0', 'Salary: 125-135k plus Annual Bonus ', 'Benefits Offered: Medical, Dental, Vision, and 401k ', 'Responsibilities of the Data Engineer:', 'Required experience of the Data Engineer:', 'Jerry King: jking@eliassen.com ', 'Atlanta, GA\xa0', 'Are you a Data Engineer with cloud experience, looking for a new opportunity? If so, this may be an opportunity for you!', '\xa0', 'Azure or AWS Cloud experienceData modeling for ETL/Data Warehousing\xa0\xa0Professional experience with Java or PythonExperience building and managing Data Pipelines', 'Keywords: Azure, Big Data, Data Engineer, Spark,\xa0Data Warehouse, Python']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
"Jr Data Engineer - Chicago, IL or Newark, NJ",Mars,"Chicago, IL",1 week ago,61 applicants,"['', 'Chicago, IL OR Newark, NJ', 'Act as an expert technical resource for designing and building data pipelines', 'Experience with Big Data', 'Forms part of the Analytics Open Hub Global Data and Analytics team to deliver the strategy, execute plans, providing specific and deep expertise in data design, data warehousing, big data platforms and data engineering ', 'An environment where all Associates feel valued, supported and comfortable being themselves at work.', 'Post-graduate qualifications in computer science, data engineering or related ', 'Self-starter, with a demonstrated ability for personal development beyond formal training', 'What are the key responsibilities?', 'Design cloud-based data solutions to enable insight and analytics ', 'The opportunity to learn, develop and take charge of your own career. ', 'What can you expect from Mars?', 'Develop a clear understanding of the data sources available, with a hands-on orientation, and the expected uses or opportunities from the data to design and deliver the right data solutions to drive insight and scalable, sustainable value creation', 'Analyse complex business requirements to build the technical specifications for new solutions', 'Track record in building and developing successful data warehouses and data pipelines', 'Design cloud-based data solutions to enable insight and analytics Structuring data in a consumable format that allows it to be manipulated efficiently and modelled easilyLead, and develop engineering projects in collaboration with other team members (including offshore talent partners) to ensure rapid development of reliable, scalable and sustainable data solutions, using global best practices Drive agile processes and team sprints to ensure efficient use of resources Ensure right technologies are in place to support our needs - deliver engagements with up-to-date capabilities available in the industry Engage and collaborate with data and IT owners across Mars ecosystem to drive data quality, availability and governance Harvest and maintain a semantic and smart data layer and ensure holistic leadership of the data solution within the environmentWrite, test, debug and deploy codeForms part of the Analytics Open Hub Global Data and Analytics team to deliver the strategy, execute plans, providing specific and deep expertise in data design, data warehousing, big data platforms and data engineering Develop a clear understanding of the data sources available, with a hands-on orientation, and the expected uses or opportunities from the data to design and deliver the right data solutions to drive insight and scalable, sustainable value creationHands-on delivery through direct or indirect onshore/offshore data teams Works closely with data source owners and Data Science teams to shape and deliver right plansEnsure continually improving performance of solutions and ongoing nimble & lean approach Analyse complex business requirements to build the technical specifications for new solutionsAct as an expert technical resource for designing and building data pipelinesSelf-starter, with a demonstrated ability for personal development beyond formal training', 'Experience in designing and building databases, data warehouses, analytics and BI platforms, including in grocery, retail, healthcare or manufacturing ', 'BS in Data Engineering or related subject required ', 'Ensure continually improving performance of solutions and ongoing nimble & lean approach ', 'Engage and collaborate with data and IT owners across Mars ecosystem to drive data quality, availability and governance ', 'Ensure right technologies are in place to support our needs - deliver engagements with up-to-date capabilities available in the industry ', 'Works closely with data source owners and Data Science teams to shape and deliver right plans', 'An industry competitive compensation package including generous benefits (i.e. 401k, health, etc.) ', 'Jr. Data Engineer', 'What are we looking for?', 'Drive agile processes and team sprints to ensure efficient use of resources ', 'Professional qualifications in advanced data design, software, big data, etc.', 'Including use of cloud platforms like AWS or Azure, technologies across traditional and contemporary software (e.g. SQLServer, Hadoop), programming languages such as SQL, Java, C#, C++, Python, R, PySpark, etc.', 'Hands-on delivery through direct or indirect onshore/offshore data teams ', 'The opportunity to learn, develop and take charge of your own career. An industry competitive compensation package including generous benefits (i.e. 401k, health, etc.) Commitment to our Purpose and our Five Principles. An environment where all Associates feel valued, supported and comfortable being themselves at work.', 'BS in Data Engineering or related subject required Post-graduate qualifications in computer science, data engineering or related Professional qualifications in advanced data design, software, big data, etc.Technical expertise and experience in database and data engineering capabilitiesExperience in designing and building databases, data warehouses, analytics and BI platforms, including in grocery, retail, healthcare or manufacturing Experience with Big DataIncluding use of cloud platforms like AWS or Azure, technologies across traditional and contemporary software (e.g. SQLServer, Hadoop), programming languages such as SQL, Java, C#, C++, Python, R, PySpark, etc.Track record in building and developing successful data warehouses and data pipelines', 'Write, test, debug and deploy code', 'Lead, and develop engineering projects in collaboration with other team members (including offshore talent partners) to ensure rapid development of reliable, scalable and sustainable data solutions, using global best practices ', 'Harvest and maintain a semantic and smart data layer and ensure holistic leadership of the data solution within the environment', 'Structuring data in a consumable format that allows it to be manipulated efficiently and modelled easily', 'Commitment to our Purpose and our Five Principles. ', 'Technical expertise and experience in database and data engineering capabilities']",Associate,Full-time,Information Technology,Food & Beverages,2021-03-18 14:34:51
Junior Data Engineer - Everyday Health,Everyday Health Group,"New York, NY",7 days ago,87 applicants,"['', 'AWS: SQS, SNS, Lambda, DynamoDB, or similar from Google Cloud', 'You find it offensive when a system lacks a robust API', 'Exceptional communication, both with technical and non-technical stakeholders', 'Design complex queries to crunch data and produce summary and aggregate datasets', 'ETL management systems such as Apache Airflow', 'Build ETL processes, across a wide array of just about every kind of database system', 'Python, Java, JS/ECMAScript, C/C++, C#', 'At least 1 year specifically working directly with data', 'Ability to thrive in a landscape of rapid evolution and varied tasks', '1-3 years of professional software development', 'A strong tendency to be self-driven and self-motivated', 'PostgreSQL, MySQL', 'Help maintain the cloud-based Data Warehouse central to our efforts', 'What You’ll Do', 'Build and maintain various self-service data platforms and tools for internal use', 'Knowledge of modern web technologies and architecture', 'At least 1 year working with Big Data concepts and systems, preferred', 'You wish you could automate everything', 'You daydream about data schema', 'You’re more likely to get distracted by a good chart than your social media feed', '1-3 years of professional software developmentAt least 1 year specifically working directly with dataAt least 1 year working in the cloud (AWS, GCP, or Azure) preferredAt least 1 year working with Big Data concepts and systems, preferredSome exposure to non-relational or NoSQL database systemsExperience with more than one programming languageExperience with shell scripting and LinuxKnowledge of modern web technologies and architectureFoundational understanding of object-oriented programming conceptsExceptional communication, both with technical and non-technical stakeholdersExperienced collaboration skills, up/down/across a mid-sized organizationAbility to thrive in a landscape of rapid evolution and varied tasksAbility to see tasks thru from requirements gathering all the way to deploymentA strong tendency to be self-driven and self-motivated', 'Qualifications', 'Position at Everyday Health - Pregnancy & Parenting', 'Description', 'The Team', 'Facilitate and execute the collection, processing, and analysis of virtually all business dataBuild ETL processes, across a wide array of just about every kind of database systemWrite code to interact with an untold number of internal and external APIs and systemsDesign complex queries to crunch data and produce summary and aggregate datasetsCombine and correlate large datasets from multiple data sources, and analyze for integrityBuild and maintain various self-service data platforms and tools for internal useHelp maintain the cloud-based Data Warehouse central to our effortsGenerally speaking, manipulate and move a lot of data around from one place to another', 'AWS RedShift, Google BigQuery, Snowflake', 'Facilitate and execute the collection, processing, and analysis of virtually all business data', 'Experienced collaboration skills, up/down/across a mid-sized organization', 'Experience with more than one programming language', 'At least 1 year working in the cloud (AWS, GCP, or Azure) preferred', 'You think in SQL', 'Combine and correlate large datasets from multiple data sources, and analyze for integrity', 'You don’t think there’s anything you couldn’t do, when it comes to software development', 'Key Technologies', 'Some exposure to non-relational or NoSQL database systems', 'Python, Java, JS/ECMAScript, C/C++, C#PostgreSQL, MySQLAWS: SQS, SNS, Lambda, DynamoDB, or similar from Google CloudAWS RedShift, Google BigQuery, SnowflakeETL management systems such as Apache Airflow', 'Foundational understanding of object-oriented programming concepts', 'About You', 'Generally speaking, manipulate and move a lot of data around from one place to another', 'Write code to interact with an untold number of internal and external APIs and systems', 'You think in SQLYou daydream about data schemaYou wish you could automate everythingYou find it offensive when a system lacks a robust APIYou’re more likely to get distracted by a good chart than your social media feedYou don’t think there’s anything you couldn’t do, when it comes to software development', 'Ability to see tasks thru from requirements gathering all the way to deployment', 'Experience with shell scripting and Linux']",Associate,Full-time,Engineering,Marketing and Advertising,2021-03-18 14:34:51
Data Engineer,Carrot Health,"Minneapolis, MN",1 month ago,173 applicants,"['', 'Employs a strong work ethic and high standards for his/her own work and the work of others. ', ' ', 'Ability to work remotely during the COVID pandemic, and ability to  work in an office setting upon full return to work in accordance with  Carrot Health policies.', 'The Data Engineer is responsible for developing and operating a broad set of data services  and data assets as part of Carrot Health’s MarketView platform.  As a  Data Engineer at Carrot Health, you will be developing and operating  data platform services that provide extensible data components as the  backbone of the MarketView platform. As a member of the Data Engineering  team, you work collaboratively within the team as well as  cross-functionally with Product Managers, Product Engineers, and Data  Scientists while participating in the technology development lifecycle. ', 'Required Skills: ', 'This is a full-time, salaried, overtime-exempt position based in Minneapolis, MN. ', 'Develop and operate data transformation and data pipeline services  that are key to achieving the strategic product capabilities and  delivering customer value. ', 'Personal Attributes:', 'Ability to be self-directed and work independently, as well as collaboratively. ', '3+ years as a Data Engineer, Software Engineer, Backend Developer, or similar role. ', 'Demonstrated experience and competency in one or more programming languages, such as Python, R, Java, Ruby, JavaScript, or Go. ', 'Grok large and complex data sets in order to make effective engineering and product decisions. ', 'Leverage CI/CD and other automation tool sets to deploy and manage  changes, ensure quality, provide consistency, and improve scalability  across the data platform. ', 'Design and engineer databases, data schemas, and data pipelines that  meet engineering best-practices and support the required product  capabilities. ', 'Develop and operate data services to acquire and curate client data, proprietary data, commercial data, and public data. ', 'Responsibilities:', 'Bachelor’s Degree in Computer Science, Mathematics, or equivalent relevant experience. ', ' Employs a strong work ethic and high standards for his/her own work and the work of others.  Desire & ability to continually learn and innovate in new technologies.  Ability to be self-directed and work independently, as well as collaboratively.  Excellent organizational skills and the ability to handle multiple  projects at once, with superior attention to detail and follow-through.  Excellent written and oral communication skills in the English language.  Flexibility and ability to adapt quickly to changes.  Ability to work remotely during the COVID pandemic, and ability to  work in an office setting upon full return to work in accordance with  Carrot Health policies.', 'Desire & ability to continually learn and innovate in new technologies. ', 'Personal Attributes: ', 'Excellent written and oral communication skills in the English language. ', 'Develop and operate data services to monitor and manage data quality  throughout the data lifecycle and adhere to data governance objectives. ', 'Familiarity with ETL, data transformation, database, data warehouse,  and big data technologies such as Matillion, Talend, Snowflake, AWS  Redshift, MySQL, PostgreSQL, Hadoop, or Spark. ', 'Leverage unit testing, regression testing, and QA (Quality  Assurance) disciplines to minimize risk and ensure high quality work  product. ', 'Ability to work through ambiguous and nebulous problems and develop context for complex solutions. ', 'Demonstrated experience and working knowledge of engineering solutions within AWS (Amazon Web Services). ', 'Responsibilities: ', 'Flexibility and ability to adapt quickly to changes. ', 'Ability to leverage software development lifecycle capabilities  including Git version control, unit testing, regression testing, and  CI/CD pipelines. ', 'Data Engineer', 'Excellent organizational skills and the ability to handle multiple  projects at once, with superior attention to detail and follow-through. ', 'Work within an Agile development environment in order to deliver incremental value. ', ' Develop and operate data services to acquire and curate client data, proprietary data, commercial data, and public data.  Develop and operate data transformation and data pipeline services  that are key to achieving the strategic product capabilities and  delivering customer value.  Develop and operate data services to monitor and manage data quality  throughout the data lifecycle and adhere to data governance objectives.  Design and engineer databases, data schemas, and data pipelines that  meet engineering best-practices and support the required product  capabilities.  Leverage unit testing, regression testing, and QA (Quality  Assurance) disciplines to minimize risk and ensure high quality work  product.  Grok large and complex data sets in order to make effective engineering and product decisions.  Work within an Agile development environment in order to deliver incremental value.  Leverage CI/CD and other automation tool sets to deploy and manage  changes, ensure quality, provide consistency, and improve scalability  across the data platform.  Cultivate and deliver continuous improvement projects within Data  Engineering to optimize, scale, and improve quality of technology assets  and processes.  ', 'Ability to work in a fast-paced, dynamic environment and adapt new  skills and responsibilities in an entrepreneurial organization. ', 'Cultivate and deliver continuous improvement projects within Data  Engineering to optimize, scale, and improve quality of technology assets  and processes. ', 'Required Skills:', 'Demonstrated experience and robust competency with SQL. ']",Mid-Senior level,Full-time,Information Technology,"Health, Wellness and Fitness",2021-03-18 14:34:51
Data Engineer,Pear Therapeutics,"Boston, MA",3 days ago,76 applicants,"['', 'Equal Employment Opportunity', 'Build and maintain scalable and efficient data pipelines and infrastructureUnderstand and integrate large, complex, evolving data sets\xa0Effectively and efficiently transform data from source systems\xa0Create data tools for analytics and data science teamsWrangling data model modifications to accommodate source data changesIdentify, design, and implement data team process improvements and automation', 'The Pear Data Program’s mission is to deliver infrastructure, tools, and processes to provide stakeholders with accurate, reliable, integrated data and actionable insights. You will be focused on designing and developing the infrastructure, tools, and Data Warehouse to advance this mission. Your role will span all aspects of the ETL process. In addition, you’ll work with external partners to extract data from the Data Warehouse that is minimally scoped to the external entity.\xa0\xa0', 'Pear Therapeutics is the leader in prescription digital therapeutics. We aim to redefine medicine by discovering, developing, and delivering clinically validated software-based therapeutics to provide better outcomes for patients, smarter engagement and tracking tools for clinicians, and cost-effective solutions for payers. Pear has a pipeline of products and product candidates across therapeutic areas, including severe psychiatric and neurological conditions. Our first product, reSET®, treats Substance Use Disorder and was the first prescription digital therapeutic to receive marketing authorization from the FDA to treat disease. Pear’s second product, reSET-O®, for the treatment of Opioid Use Disorder, received marketing clearance from the FDA in December 2018. Pear’s third PDT, Somryst®, is the first FDA-authorized prescription digital therapeutic (PDT) for patients with chronic insomnia and the first product submitted through FDA’s traditional 510(k) pathway while simultaneously reviewed through FDA’s Software Precertification Pilot Program.', 'Deep experience with SQL and no-SQL data sourcesExperience working as a Data Engineer at multiple companies; you know what worksA strong sense of ownership\xa0Experience writing ETL scripts using python, Java, SQL, and/or other languagesExperience manipulating, processing and extracting value from large disconnected datasets', 'Pear Therapeutics is committed to equal opportunity in the terms and conditions of employment for all employees and job applicants without regard to race, color, religion or religious creed, ancestry, age, sex (including pregnancy, childbirth, breastfeeding and related medical conditions), sexual orientation, gender identity or gender expression, national origin, genetic information, qualified physical or mental disability or handicap, medical condition, qualified military or veteran status, or any other basis protected by applicable law. Pear Therapeutics also follows all applicable national, state and local laws governing nondiscrimination in employment as well as employment eligibility verification requirements of the Immigration and Nationality Act. This policy applies to all terms and conditions of employment, including hiring, placement, promotion, termination, layoff, recall, transfers, leave of absences, compensation and training.', 'A strong sense of ownership\xa0', 'Effectively and efficiently transform data from source systems\xa0', 'Responsibilities', 'Experience working as a Data Engineer at multiple companies; you know what works', 'Wrangling data model modifications to accommodate source data changes', 'Qualifications', 'Description', 'Experience writing ETL scripts using python, Java, SQL, and/or other languages', 'Build and maintain scalable and efficient data pipelines and infrastructure', 'Experience manipulating, processing and extracting value from large disconnected datasets', 'Create data tools for analytics and data science teams', 'Understand and integrate large, complex, evolving data sets\xa0', 'Deep experience with SQL and no-SQL data sources', 'About Pear Therapeutics', 'Identify, design, and implement data team process improvements and automation']",Mid-Senior level,Full-time,Engineering,Biotechnology,2021-03-18 14:34:51
Data Engineer,Porsche Cars North America,Atlanta Metropolitan Area,3 weeks ago,112 applicants,"['', 'Facilitates the development of data-related policies, processes, procedures and standards', 'Implementation of RESTful API’s supporting system integrations', 'Developing new and enhancing existing data processing (Data Ingest, Data Transformation, Data Store, Data Management, Data Quality ) components', 'Exposure in Microsoft SSIS and SQL Server', ""Design, develop and maintain reliable automated data solutions based on the identification, collection and evaluation of business requirements. Including but not limited to data models, database objects, stored procedures and viewsDeveloping new and enhancing existing data processing (Data Ingest, Data Transformation, Data Store, Data Management, Data Quality ) componentsConduct system monitoring across cloud environmentsAnalyzes and profiles large and complex data sets to discover and/or validate data quality issues and business rulesResolve technical and user issuesAutomate installation, configuration, backup, monitoring and alerting processes in SnowflakeDrive collaboration across a global, multicultural, multi-company teamImplementation of RESTful API’s supporting system integrationsActively researches philosophical and technological changes in data management and pursue an understanding of the organization's business plans and strategies. Evaluates the value of these trends to the organization and presents the information to management and other team members.Facilitates the development of data-related policies, processes, procedures and standards"", 'Design, develop and maintain reliable automated data solutions based on the identification, collection and evaluation of business requirements. Including but not limited to data models, database objects, stored procedures and views', 'Drive collaboration across a global, multicultural, multi-company team', 'Porsche is an equal opportunity employer and we take pride in our diversity. In order to provide equal employment and advancement opportunities to all individuals, employment decisions at Porsche will be based on merit, qualifications and abilities. Porsche does not discriminate in employment opportunities or practices on the basis of race, color, religion, sex, pregnancy, status as a parent, national origin, age, disability, family medical history, ancestry, medical condition, genetic information, sexual orientation, gender, gender identity, gender expression, marital status, familial status, registered domestic partner status, family and medical leave status, military status, criminal conviction history, or any other characteristic protected by federal, state or local law.\xa0', 'Position Objective:', 'Responsibilities:', '5+ years professional data engineering experience focused on batch and real time data pipelines using Spark, Python, SQL, Java', 'Critical thinking/problem solving', 'Experience with creating API’s.', 'Qualifications:', 'We are hiring a Data Engineer for a great opportunity to work with an iconic car brand. This position will be responsible for migrating our local analytics platform/EDW to our new Snowflake database, as well as the ongoing maintenance and development Snowflake and AWS. This position will require a strong background in MPP databases, analytics, cloud architecture, and the tools to implement these solutions.\xa0Additionally, this position will be expected to become an expert on Porsche business processes as it relates to serving customers, AWS, and Snowflake', ""Education:\xa0Bachelor's Degree in Computer Science, Engineering, Data Analytics or comparable experience"", 'Fundamental experience with leveraging AWS for Analytics', 'Experience in the Automotive and/or Financial Industries is a plus', 'Analyzes and profiles large and complex data sets to discover and/or validate data quality issues and business rules', 'Knowledge of best practices and IT operations in an always-up, always-available services', '8+ Years of hands-on design and development experience in data space : data processing / data transformation using ETL tools, data warehouse (data modeling, programming), RDBMS', 'Experience using JIRA and Agile Project Management software', 'Resolve technical and user issues', ""Actively researches philosophical and technological changes in data management and pursue an understanding of the organization's business plans and strategies. Evaluates the value of these trends to the organization and presents the information to management and other team members."", 'Automate installation, configuration, backup, monitoring and alerting processes in Snowflake', 'Experience with code repository solutions', 'Working knowledge of MPP systems or Snowflake a plus', '\xa0', '5+ years professional data engineering experience focused on batch and real time data pipelines using Spark, Python, SQL, Java8+ Years of hands-on design and development experience in data space : data processing / data transformation using ETL tools, data warehouse (data modeling, programming), RDBMSExperience with creating API’s.Fundamental experience with leveraging AWS for AnalyticsExposure in Microsoft SSIS and SQL ServerWorking knowledge of MPP systems or Snowflake a plusExperience using JIRA and Agile Project Management softwareExperience with code repository solutionsAble to work in a global, multicultural environmentKnowledge of best practices and IT operations in an always-up, always-available servicesCritical thinking/problem solvingExperience in the Automotive and/or Financial Industries is a plusGerman language capability a plus', 'Education:', 'Skills:', 'Able to work in a global, multicultural environment', 'Conduct system monitoring across cloud environments', 'German language capability a plus']",Mid-Senior level,Full-time,Information Technology,Automotive,2021-03-18 14:34:51
Data Engineer,Pozent Corporation,"Raritan, NJ",1 day ago,Be among the first 25 applicants,"['', 'Follow test-driven development ', ' Design SQL Server database schema  Write Azure Data Factory pipelines Build CI/CD pipelines  Follow test-driven development  Perform database administration  Write stored procedures Automate data backup and recovery ', 'Can-do attitude and strong sense of ownership ', "" A minimum of a Bachelor's Degree in EE/CS or related field from an accredited institution is required  A minimum of five (5) years of industry experience in an established company is required  Experience with data modeling, design patterns, building highly scalable and secure applications is required  Experience with MySQL or SQL Server, Docker, Unix, Java, Maven, Jenkins, Git, Azure Data Factory is required  Experience with Azure DevOps (pipelines) highly preferred  Can-do attitude and strong sense of ownership  Ability to work in an ambiguous environment and adapt to changing requirements  Strong communication skills and attention to details Team player  Candidates are strongly encouraged to send a link to their StackOverflow profile with their application."", 'Experience with Azure DevOps (pipelines) highly preferred ', 'Strong communication skills and attention to details', 'Candidates are strongly encouraged to send a link to their StackOverflow profile with their application.', 'A minimum of five (5) years of industry experience in an established company is required ', 'Build CI/CD pipelines ', 'Experience with MySQL or SQL Server, Docker, Unix, Java, Maven, Jenkins, Git, Azure Data Factory is required ', 'Team player ', 'Design SQL Server database schema ', 'Write Azure Data Factory pipelines', 'Write stored procedures Automate data backup and recovery', ""A minimum of a Bachelor's Degree in EE/CS or related field from an accredited institution is required "", 'Ability to work in an ambiguous environment and adapt to changing requirements ', 'Experience with data modeling, design patterns, building highly scalable and secure applications is required ', 'Perform database administration ']",Entry level,Contract,Information Technology,Information Technology and Services,2021-03-18 14:34:51
"Data Engineer - SQL, Azure Cloud - Austin",recruitAbility,"Austin, TX",20 hours ago,Be among the first 25 applicants,"['', ' You are a team player who enjoys working with and supporting an Application Engineering team, DBA, Infrastructure, and Project Management Office.', ' Full health, vision, dental. 401(k) plans along with a host of voluntary plans such as car insurance, legal services, and more.', ' Enjoy working in Agile as part of a scrum team and deliver high-quality products incrementally in an interactive manner.', ' Experience with other Big Data tools such as Spark, Snowflake, and Kafka', ' A solid compensation plan includes comprehensive benefits and a bonus plan', ' Write Test Driven Development based code to meet overall data quality standards as defined by the users.', ' Advanced working knowledge of SQL Server database - writing advanced SQL script, profiling, and optimization.', ' Automate the data testing processes and integrate them with monitoring systems.', ' Strong experience with NoSQL database, including Postgres Background in working with Azure Cloud Services: Data Factory, SQL database, Functions, Data Lake, Databricks, Logic Apps, and Azure Automation. Fluent in object-oriented and functional script language: Python, Scala, and C#. Advanced working knowledge of SQL Server database - writing advanced SQL script, profiling, and optimization. Working knowledge of Business Intelligence tools: Microsoft Integration Services, Reporting Services, and Analysis Services, as well as PowerBI. Experience with other Big Data tools such as Spark, Snowflake, and Kafka Preference for background in Financial Services, ideally in the Wealth Management/Independent Broker-Dealer/RIA industry Bachelors/Masters in Computer Science, MIS/Information Management, Engineering or related field', ' Award-winning, a stable leader in their market space and still growing', ' A brand new state of the art building in Southwest Austin with a basketball court, volleyball court, baseball field, walking trails, unlimited coffee, tea, and sparkling water', ' Strong experience with NoSQL database, including Postgres', ' Career path, training support, and opportunities for advancement within', ' Background in working with Azure Cloud Services: Data Factory, SQL database, Functions, Data Lake, Databricks, Logic Apps, and Azure Automation.', ' Excellent salary and comprehensive benefits package for this full-time position A world-class team of professionals, casual work environment, and rich culture Challenging projects now and on the Technology Roadmap going out several years Career path, training support, and opportunities for advancement within Award-winning, a stable leader in their market space and still growing A solid compensation plan includes comprehensive benefits and a bonus plan Full health, vision, dental. 401(k) plans along with a host of voluntary plans such as car insurance, legal services, and more. A brand new state of the art building in Southwest Austin with a basketball court, volleyball court, baseball field, walking trails, unlimited coffee, tea, and sparkling water', ' Meet with the business users, assist with data-related technical issues, and support their data infrastructure needs', 'Description', ' Fluent in object-oriented and functional script language: Python, Scala, and C#.', ' Bachelors/Masters in Computer Science, MIS/Information Management, Engineering or related field', ' Design and develop data pipelines to extract data from a wide variety of data sources using Azure, Snowflake Cloud, and cloud-native technologies. Build a data model to get actionable insights from data, operational efficiency, and other key business performance metrics. Design and manage inbound and outbound data processes and monitoring. Work with the data provider to bring in new feed into our data eco-system. Enjoy working in Agile as part of a scrum team and deliver high-quality products incrementally in an interactive manner. Write Test Driven Development based code to meet overall data quality standards as defined by the users. Automate the data testing processes and integrate them with monitoring systems. You are a team player who enjoys working with and supporting an Application Engineering team, DBA, Infrastructure, and Project Management Office. Analyze existing systems (including legacy) and data sets to help Business Analysts define the functional and non-functional requirements. Meet with the business users, assist with data-related technical issues, and support their data infrastructure needs', ' Design and develop data pipelines to extract data from a wide variety of data sources using Azure, Snowflake Cloud, and cloud-native technologies.', ' Build a data model to get actionable insights from data, operational efficiency, and other key business performance metrics.', ' A world-class team of professionals, casual work environment, and rich culture', ' Working knowledge of Business Intelligence tools: Microsoft Integration Services, Reporting Services, and Analysis Services, as well as PowerBI.', ' Design and manage inbound and outbound data processes and monitoring. Work with the data provider to bring in new feed into our data eco-system.', ' Excellent salary and comprehensive benefits package for this full-time position', ' Analyze existing systems (including legacy) and data sets to help Business Analysts define the functional and non-functional requirements.', ' Challenging projects now and on the Technology Roadmap going out several years', 'Primary Requirements For The Data Engineer', 'Primary Responsibilities Of The Data Engineer', ' Preference for background in Financial Services, ideally in the Wealth Management/Independent Broker-Dealer/RIA industry']",Entry level,Full-time,Information Technology,Staffing and Recruiting,2021-03-18 14:34:51
Data Engineer,State Farm ®,"Bloomington, IL",3 days ago,28 applicants,"['', 'Qualifications', 'Computer Science background (Bachelor degree or higher) with a minimum of 2 years of experience in an IT related field Possess strong business acumen and the technical ability to acquire, transform and interpret complex data in order to answer ad hoc questions often coming from top executivesExperience with gathering and creating analytic business requirements, researching potential data sources (both internal and external sources), designing, developing and maintaining data assetsExperience with data governance policies, including the implementation of data security strategiesExcellent communication skills and the ability to work with multiple, diverse stakeholders across business areas and leadership levelsTechnical expertise with multiple compute environments, including at least two of the following: Linux, Hadoop, Mainframe, and AWSExperience with building and maintaining data pipelinesFamiliarity with building SQL and No-SQL queriesFamiliarity with one of the following languages: Python, R, or SASKnowledge of version control and DevOps tools, such as GitLab Knowledge of work prioritization using the Agile framework', 'Responsibilities', 'Overview', 'Experience with data governance policies, including the implementation of data security strategies', 'Experience with building and maintaining data pipelines', ' Possess strong business acumen and the technical ability to acquire, transform and interpret complex data in order to answer ad hoc questions often coming from top executives', 'Excellent communication skills and the ability to work with multiple, diverse stakeholders across business areas and leadership levels', 'Familiarity with building SQL and No-SQL queries', 'Familiarity with one of the following languages: Python, R, or SAS', ' Knowledge of work prioritization using the Agile framework', 'Experience with gathering and creating analytic business requirements, researching potential data sources (both internal and external sources), designing, developing and maintaining data assets', 'Computer Science background (Bachelor degree or higher) with a minimum of 2 years of experience in an IT related field', 'Technical expertise with multiple compute environments, including at least two of the following: Linux, Hadoop, Mainframe, and AWS', 'Knowledge of version control and DevOps tools, such as GitLab']",Not Applicable,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Junior Data Engineer,VICE Media,"Brooklyn, NY",5 days ago,Over 200 applicants,"['', 'Comfortable on the command line', 'Role x You', 'VICE Guide to VICE', ' 3+ years experience in Python or other scripting language Experience with any of Airflow, Luigi, Prefect, Oozie etc. Strong SQL skills, especially in Snowflake, BigQuery, Redshift etc. Experience consuming APIs. Experience working within an AWS environment. Comfortable on the command line ', ' Media, Advertising or Consulting experience. Exposure to Salesforce, Google Analytics, Netsuite, Google Ad Manager. Experience with CI/CD pipelines Experience with analysis and deriving insights from data ', 'Experience working within an AWS environment.', 'Agencies: VICE Media Group is not partnering with agencies nor accepts unsolicited resumes and will not be responsible for any fees or expenses related to such unsolicited resumes and/or applicants.', 'Experience consuming APIs.', 'We’d Love If You Also Had These', 'Strong SQL skills, especially in Snowflake, BigQuery, Redshift etc.', 'About Vice Media Group', 'Working at VICE', 'Agencies: ', 'Qualifications', 'Experience with analysis and deriving insights from data', 'VICE x Team', 'Experience with any of Airflow, Luigi, Prefect, Oozie etc.', 'Experience with CI/CD pipelines', 'Media, Advertising or Consulting experience.', '3+ years experience in Python or other scripting language', 'Exposure to Salesforce, Google Analytics, Netsuite, Google Ad Manager.']",Associate,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Data Engineer - Analytics,EVERSANA,"New Jersey, United States",6 hours ago,Over 200 applicants,"['', 'You will be joining our team of 60+ consultants with this client', 'This is a long term consulting role with our fortune 100 client in central NJ', 'Strong IT background is critical (SQL / ETL / SDLC / Data Engineering / Cloud Solutions / APIs / Data Governance / Data Visualization / etc.)Business Domain knowledge is an asset (Digital Analytics / Precision Marketing / CRM / CDP / 1st Party Data / eCommerce / GMP / DMP / etc.)', 'No relocation, no 3rd party candidates', 'Business/Data Analyst -', 'Strong IT background is critical (SQL / ETL / SDLC / Data Engineering / Cloud Solutions / APIs / Data Governance / Data Visualization / etc.)', 'Business Domain knowledge is an asset (Digital Analytics / Precision Marketing / CRM / CDP / 1st Party Data / eCommerce / GMP / DMP / etc.)']",Mid-Senior level,Contract,Information Technology,Pharmaceuticals,2021-03-18 14:34:51
Data Engineer,"The Resource Collaborative, Inc.",New York City Metropolitan Area,3 days ago,49 applicants,"['', 'Requirements:', 'Develop frameworks or reuse/leverage on existing frameworks', 'The Data Engineer will work within BI/DW and will be responsible for Informatica Cloud Development. The person should have experience in developing End to End Data Pipelines using Informatica Cloud (Including Parameters) and have worked on using multiple types of connectors including SQL Server, Snowflake, Dynamics 365 and Azure Blob', 'Work directly with end users and understand requirements', '\ufeff', 'Identifying optimal connector', 'About the Data Engineer:', 'Our client', 'Responsibilities:', 'Understand the requirements and understand the larger the initiatives that each project would be part of and build solutions with end state in mindCreate end to end solutions for data engineering, such as:Analyzing source dataIdentifying optimal connectorDevelop frameworks or reuse/leverage on existing frameworksParameterize and AutomateWork directly with end users and understand requirements', 'Our client is an investment firm with headquarters in NYC. They are undergoing tremendous growth and are adding to their technology team...plenty of room for learning and advancement!', ""8+ years' experience in Informatica PowerCenter including Informatica Intelligent Cloud Services.8+ years' experience working with Relational Databases and Data Warehouses with ability to write, analyze and optimize complex SQL queries3+ years in Snowflake and have experience as SQL Developer and DB Administration"", 'Create end to end solutions for data engineering, such as:', ""8+ years' experience working with Relational Databases and Data Warehouses with ability to write, analyze and optimize complex SQL queries"", 'This is a Full time Role ', 'Understand the requirements and understand the larger the initiatives that each project would be part of and build solutions with end state in mind', 'Analyzing source data', '3+ years in Snowflake and have experience as SQL Developer and DB Administration', 'Parameterize and Automate', ""8+ years' experience in Informatica PowerCenter including Informatica Intelligent Cloud Services.""]",Mid-Senior level,Full-time,Information Technology,Financial Services,2021-03-18 14:34:51
Data Engineer,"McCarthy Building Companies, Inc.","Kansas City, MO",1 day ago,Be among the first 25 applicants,"['', 'Participate in code and design review to ensure alignment to standards and best practices', 'Azure DevOps', 'Mature understanding of data warehouse and data lake concepts and design (including star schemas)', 'Experience building data pipelines to ingest unstructured and streaming dataExperience preparing data for Data Science and Machine Learning use casesExperience with master data managementExperience designing reports using Power BI, Power Query, and DAX', 'Design, implement, and support ETL and ELT pipelines using Azure Data Factory pipelines and dataflows (both mapping and wrangling)', 'Extensive experience querying API endpoints', 'Extensive experience with T-SQL (queries and DDL)', 'McCarthy is proud to be an equal opportunity and affirmative action employer regardless of race, color, gender, age, sexual orientation, gender identity, religious beliefs, marital status, genetic information, national origin, disability or protected veteran status.', ""McCarthy Building Companies, Inc. is one of America's premier commercial construction companies. Our reputation for tackling the toughest building challenges starts with our focus on building high-performing teams that collaborate with clients and industry partners starting in the earliest stages of design, throughout construction and beyond project completion. With offices and employees nationwide, we specialize in a wide range of project types including healthcare, education, renewable energy, marine, water/wastewater, commercial office and retail, hospitality/entertainment and airports. Originally founded as a family business in 1864, today we are proud to be 100 percent employee owned."", 'Experience preparing data for Data Science and Machine Learning use cases', 'Experience with Agile/Scrum methodology preferred', 'Experience developing and promoting work through devops pipelines and utilizing source control, preferably GIT, and Visual Studio’s database tools', 'Research, analyze, recommend and select technical approaches for solving challenging and complex development and integration problems', 'Mature understanding of data warehouse and data lake concepts and design (including star schemas) 5+ years of data solution delivery experience with 1+ years on the Microsoft Azure platform:Azure Data Factory (including pipelines and mapping & wrangling data flows)Azure SQL (nice to have: Azure Data Warehouse/Synapse)Azure Data Lake Gen2Azure DevOpsExtensive experience with T-SQL (queries and DDL)Extensive experience querying API endpointsExperience M and Power Query data wranglingExperience developing and promoting work through devops pipelines and utilizing source control, preferably GIT, and Visual Studio’s database toolsExperience with Agile/Scrum methodology preferredExcellent analytical, conceptual, and problem-solving abilitiesEntrepreneurial attitude with a passion to deliver value for the organization and a desire to foster and develop team members\xa0', 'Experience building data pipelines to ingest unstructured and streaming data', 'Promote ADF and database objects through environments using Azure DevOps and Visual Studio', 'Excellent analytical, conceptual, and problem-solving abilities', 'Experience designing reports using Power BI, Power Query, and DAX', 'Azure Data Lake Gen2', 'Azure Data Engineer', 'Skills & Qualifications', 'McCarthy is seeking a full-time Azure Data Engineer who, as part of the Business Intelligence team, will be responsible to deliver end-to-end data pipelines and repositories leveraging both traditional tools and practices as well as modern consumption and delivery models to support a growing Business Intelligence practice.', 'Entrepreneurial attitude with a passion to deliver value for the organization and a desire to foster and develop team members\xa0', 'Azure SQL (nice to have: Azure Data Warehouse/Synapse)', 'Create and run testing protocols for data solutions', 'Experience with master data management', 'Key Responsibilities', 'Assure development work accords with best practices including security and data quality', 'Preferred:\xa0', 'Design, implement, and support modern data solutions with Azure Data Factory, Azure Data Lake, and Azure SQLDesign, implement, and support ETL and ELT pipelines using Azure Data Factory pipelines and dataflows (both mapping and wrangling)Create and run testing protocols for data solutionsPromote ADF and database objects through environments using Azure DevOps and Visual StudioAssure development work accords with best practices including security and data qualityParticipate in code and design review to ensure alignment to standards and best practicesResearch, analyze, recommend and select technical approaches for solving challenging and complex development and integration problems', 'Experience M and Power Query data wrangling', ' 5+ years of data solution delivery experience with 1+ years on the Microsoft Azure platform:Azure Data Factory (including pipelines and mapping & wrangling data flows)', 'Required:\xa0', 'Design, implement, and support modern data solutions with Azure Data Factory, Azure Data Lake, and Azure SQL']",Associate,Full-time,Information Technology,Construction,2021-03-18 14:34:51
Data Warehouse Engineer,Bright Horizons,"Newton, MA",2 days ago,42 applicants,"['', 'Develop data\xa0assets\xa0across various lines of business and functional areas of the organization based on the overall enterprise information strategyDevelop interfaces for\xa0ingesting data for enterprise applications and\xa0make data available for Business Intelligence, Analytics, and ReportingMeet with users and work with Business Analysis team, to develop requirements and needs analysis and provide technical solutions to support those needsProvide feedback on technical designs and methods to support business requirementsDocument business requirements and technical specifications in support of\xa0EDW initiativesEffectively communicate relevant project information to superiorsProvide hands-on leadership in the application development, test, and rollout of strategic business intelligence solutions.Deliver engaging, informative, well-organized presentations that are effectively tailored to the intended audience, as needed.Communicate and facilitate discussions of\xa0an\xa0approach\xa0to the intended audience to ensure that business owners are able to give complete and accurate feedbackEnsure that feedback from key business owners is fully acknowledged and incorporated into functional and technical specifications, process flow diagrams and project plans to give business leaders the confidence that they are being heard and that their needs will be met', 'Understanding of Business Intelligence and data warehouse technologies, methodologies and solutions', 'Develop data\xa0assets\xa0across various lines of business and functional areas of the organization based on the overall enterprise information strategy', 'Document business requirements and technical specifications in support of\xa0EDW initiatives', 'Meet with users and work with Business Analysis team, to develop requirements and needs analysis and provide technical solutions to support those needs', 'Experience working on diverse teams and creating a culture of inclusion and acceptance', ""The EDW, Technical Lead is responsible for the development and support of\xa0Enterprise Data Warehouse data and it's surrounding assets\xa0 supporting enterprise requirements.\xa0"", 'Communicate and facilitate discussions of\xa0an\xa0approach\xa0to the intended audience to ensure that business owners are able to give complete and accurate feedback', 'Bachelor’s Degree in Software Engineering, or related field required', 'Relevant experience with Dimensional data modeling principles (star and snowflake schemas, de-normalized data structures, slowly changing dimensions, etc.), Data integration tools and techniques such as ETL (Extract Transform Load), CDC (Change Data Capture)', 'Minimum 8\xa0 years of hands-on Enterprise Data Warehouse development and\xa0systems integrations\xa0experience, with minimum 3 years at a team leadership role.', 'Bright Horizons is famous for child care and education, but underneath beats the heart of a technology company. IT development and infrastructure is so important to our future, it’s now the fastest growing department in our company – a team on the ground floor of building all new systems. Plus, at Bright Horizons IT, you’ll be part of a creative group supporting some of the world’s best brands, connecting their employees to care and education, and making a real difference for people who need it.', 'What you will be doing in this role:', 'Provide feedback on technical designs and methods to support business requirements', 'Provide hands-on leadership in the application development, test, and rollout of strategic business intelligence solutions.', 'Bachelor’s Degree in Software Engineering, or related field requiredMinimum 8\xa0 years of hands-on Enterprise Data Warehouse development and\xa0systems integrations\xa0experience, with minimum 3 years at a team leadership role.Strong expertise in database systems and architectures, SQL, application development using various ETL tools\xa0 and frameworksUnderstanding of Business Intelligence and data warehouse technologies, methodologies and solutionsRelevant experience with Dimensional data modeling principles (star and snowflake schemas, de-normalized data structures, slowly changing dimensions, etc.), Data integration tools and techniques such as ETL (Extract Transform Load), CDC (Change Data Capture)Exposure to BI tools such as QlikSense, Tableau is\xa0desiredExperience working on diverse teams and creating a culture of inclusion and acceptance', 'Ensure that feedback from key business owners is fully acknowledged and incorporated into functional and technical specifications, process flow diagrams and project plans to give business leaders the confidence that they are being heard and that their needs will be met', 'EDW, Technical Lead', 'Develop interfaces for\xa0ingesting data for enterprise applications and\xa0make data available for Business Intelligence, Analytics, and Reporting', 'Exposure to BI tools such as QlikSense, Tableau is\xa0desired', 'What we hope you will bring to this role:', 'Deliver engaging, informative, well-organized presentations that are effectively tailored to the intended audience, as needed.', 'Effectively communicate relevant project information to superiors', 'Strong expertise in database systems and architectures, SQL, application development using various ETL tools\xa0 and frameworks']",Mid-Senior level,Full-time,Business Development,Education Management,2021-03-18 14:34:51
"Jr. Data Engineer,  Business Intelligence",Essence,New York City Metropolitan Area,2 days ago,35 applicants,"['', 'Previous experience working with data and technology', 'Become a proficient user of Google Cloud Platform and associated technologies, ensuring your work incorporates industry best practice', 'Analytically minded, enabling you to understand and overcome technically complex challenges, and to tell compelling stories with data', 'An ability to demonstrate a passion for the digital marketing ecosystem, and an understanding of the role that data plays within it', 'The Role:', 'Support the translation of user requirements and business needs into technical specifications', ':', 'Eagerness to learn and become a better programmerSome experience with programming and/or statistical languages (e.g. SQL, Python)Analytically minded, enabling you to understand and overcome technically complex challenges, and to tell compelling stories with dataStrong organizational skills and attention to detail, including the ability to manage multiple tasks in a fairly autonomous wayStrong spoken and written communication skills, ensuring your thoughts and needs are heard and understoodAn ability to demonstrate a passion for the digital marketing ecosystem, and an understanding of the role that data plays within itDelivers best results when working in a team environment, and an ability to partner effectively with people of varying degrees of technical capability', 'Experience using or building reports with business intelligence software, ideally Google DataStudio', 'Visit essenceglobal.com for more information and follow us on Twitter at @essenceglobal.', 'Assist the development of technical solutions, in line with specifications,\xa0 that collect, store and transform disparate data sources', 'Assist the development and maintenance of automated jobs that ensure required data is made available in an efficient and scalable way as possible', 'About Essence', 'An understanding of how data is tracked and exchanged in the process of digital advertising (e.g. role of ad servers and other third-party tech vendors)', 'Some of the things we’d like you to do:', 'This role forms part of a globally distributed business intelligence team, whose objective\xa0 is to ensure that one of our most important global accounts have access to the right data and insights in order to inform their marketing decisions.\xa0\xa0', 'Attend internal stakeholder meetings, presenting your solutions and providing updates on your work.', 'Required', 'Monitor automated jobs, troubleshooting data issues as-and-when they arise', 'Experience with digital marketing platforms and the data they generate, in particular Google Marketing Platform, Facebook, Twitter etc.\xa0\xa0', 'Desirable', 'Previous experience working with data and technologyExperience with digital marketing platforms and the data they generate, in particular Google Marketing Platform, Facebook, Twitter etc.\xa0\xa0An understanding of how data is tracked and exchanged in the process of digital advertising (e.g. role of ad servers and other third-party tech vendors)Experience using or building reports with business intelligence software, ideally Google DataStudioWork experience within a marketing organization, preferably at a media agency or related company (e.g. publisher, ad tech, client marketing org)', 'Eagerness to learn and become a better programmer', 'Assist the development and maintenance of data quality checks and procedures, helping account teams overcome data issues before they impact the quality of the reporting solution', 'A bit about yourself:', 'Support other members of the team responsible for “last mile” transformation and visualization of data within Google Data Studio reports and dashboards', 'What you can expect from Essence', 'As a Junior Data Engineer your primary responsibility will be to support a Senior Data Engineer to create and maintain underlying data infrastructure that provides the wider team with the data they need to provide timely, accurate and meaningful deliverables & reporting.\xa0 In doing so you will gain a foundational understanding of cloud technology and key data engineering skills and knowledge to help you build a career in this fast evolving, and in demand, industry.', 'Provide hands-on support to users of reportion solutions, helping the wider team triage and respond to user queries in a timely manner', 'Strong spoken and written communication skills, ensuring your thoughts and needs are heard and understood', 'Some experience with programming and/or statistical languages (e.g. SQL, Python)', 'Essence’s mission is to make advertising more valuable to the world.\xa0 We do this by employing the world’s very best talent to solve some of the toughest challenges of today’s digital marketing landscape.\xa0 It’s important that we hire people whose values reflect those of our own: genuine, results-focused, daring and insightful.\xa0 As an Essence employee, we promise you a workplace that invests in your career, cares for you and is fun and engaging.\xa0 We believe these factors create a workplace where you can be yourself and do amazing work.', 'The team’s primary responsibility is to maintain a global Google Cloud based reporting solution, which automates the collection and transformation of disparate marketing data into a single source of truth.\xa0 Not only does that mean running and maintaining the solution that already exists, but also continually improving it to incorporate new data sources, and to derive new insights, to support ever-evolving business demands.', 'Assist the development of technical solutions, in line with specifications,\xa0 that collect, store and transform disparate data sourcesAssist the development and maintenance of automated jobs that ensure required data is made available in an efficient and scalable way as possibleAssist the development and maintenance of data quality checks and procedures, helping account teams overcome data issues before they impact the quality of the reporting solutionSupport the translation of user requirements and business needs into technical specificationsBecome a proficient user of Google Cloud Platform and associated technologies, ensuring your work incorporates industry best practiceMonitor automated jobs, troubleshooting data issues as-and-when they ariseSupport other members of the team responsible for “last mile” transformation and visualization of data within Google Data Studio reports and dashboardsProvide hands-on support to users of reportion solutions, helping the wider team triage and respond to user queries in a timely mannerAttend internal stakeholder meetings, presenting your solutions and providing updates on your work.Support the development strong working relationships with third-party data providers that we rely on for access to necessary data', 'Support the development strong working relationships with third-party data providers that we rely on for access to necessary data', 'Strong organizational skills and attention to detail, including the ability to manage multiple tasks in a fairly autonomous way', 'Work experience within a marketing organization, preferably at a media agency or related company (e.g. publisher, ad tech, client marketing org)', ""Essence, part of GroupM, is a global data and measurement-driven media agency whose mission is to make brands more valuable to the world. Clients include Google, Flipkart, NBCUniversal, L'Oréal and the Financial Times. The agency is more than 2,000 people strong, manages $4.5B in annualized media spend, and deploys campaigns in 121 markets via 22 offices in APAC, EMEA and the Americas.\xa0"", 'Delivers best results when working in a team environment, and an ability to partner effectively with people of varying degrees of technical capability']",Associate,Full-time,Engineering,Marketing and Advertising,2021-03-18 14:34:51
Data Engineer,Automotus,United States,7 days ago,Over 200 applicants,"['', '- At least two years of professional software engineering experience, with proficiency in at least one major language (Python, Golang, Typescript preferred)', ""Our team is small but growing fast. We're currently looking for our first data engineer, who will be a critical asset in designing, building, and implementing the data pipelines that form the foundation of our product."", '- Self-directed vacation policy', '- Career development and upward mobility', 'Preferred Skills/Experience', ""- Bachelor's Degree in Computer Science or a related field"", '- Strong familiarity with SQL databases', ""- Function as a leader, contributing towards establishing a culture of excellence as Automotus' engineering team grows"", 'Automotus helps cities better understand, manage, and monetize their curbs. We use computer vision deployed at the edge to guide planning decisions, autonomously charge companies for parking by the minute, and automate enforcement, while also helping fleets save money by operating more efficiently and avoiding parking citations.', 'What We Offer', '- Health, dental, and vision coverage', 'Responsibilities', '- Strong machine learning skills: first-principles knowledge of canonical algorithms and experience using popular libraries such as Tensorflow and Pytorch', 'We encourage women and people from underrepresented groups to apply.', 'Requirements', '- Experience building and maintaining a data warehouse in production', ""Given the size of the team and the fluid nature of technical roles in an early-stage organization, we're most interested in applicants who have killer data engineering skills, but aren't opposed to getting their hands dirty elsewhere in our stack."", 'Location', '- Design and implement ML models to deliver predictive insights to our customers', '- Experience designing, implementing, and maintaining extensible and scalable ETL pipelines', '- Volunteer time off policy (VTO)', '- Experience working on IoT products', '- Competitive salary', '- Applied statistics skills such as experimental design and hypothesis testing', 'Automotus is based in Los Angeles, CA, but we operate as a fully distributed team. If the time zone where you are is within 4 hours of GMT-8, feel free to apply!', 'Overview', '- Experience working with time-series and geospatial datasets', '- Experience with data transformation tools like Spark or Hadoop', '- Experience deploying data services in the cloud (AWS preferred)', '- Architect, build, and maintain flexible & scalable data infrastructure']",Entry level,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Engineer,AllianceBernstein,"Nashville, TN",5 days ago,29 applicants,"['', ' Building business intelligence dashboards to provide data insights. ', ' Solid analytical and technical skills ', 'People of color, women, and those who identify as LGBTQ people are encouraged to apply. AB does not discriminate against any employee or applicant for employment on the basis of race, color, religion, creed, ancestry, national origin, sex, age, disability, marital status, citizenship status, sexual orientation, gender identity, military or veteran status or any other basis that is prohibited by applicable law. AB’s policies, as well as practices, seek to ensure that employment opportunities are available to all employees and applicants, based solely on job-related criteria. ', 'Qualifications, Experience, Education', ' BS in Computer Science/Engineering, Finance, Mathematics/Statistics or a related major ', ' A strong desire to document and share work done to aid in long term support ', ' Applying cloud based technologies including data lakes and data pipelines. ', ' Building visualizations using Tableau, Qlik, or PowerBI is a strong plus ', ' Experience working in the financial industry or knowledge of basic financial statement concepts ', ' Azure experience building data pipelines ', ' BS in Computer Science/Engineering, Finance, Mathematics/Statistics or a related major  5+ years programming in SQL with experience in relational schema designs and optimizing query performance  2+ years using Python or another object oriented language (C#, Java)  ETL experience is a strong plus  Working with NoSQL is a strong plus  Building visualizations using Tableau, Qlik, or PowerBI is a strong plus ', ' Must demonstrate good communication skills and be comfortable working closely with business users ', 'Describe The Role', 'Skills', ' Automating complex data loads and pipelines. ', ' ETL experience is a strong plus ', ' 2+ years using Python or another object oriented language (C#, Java) ', ' Learning the equity investment business and engaging directly with end users.  Automating complex data loads and pipelines.  Onboard alternative datasets including learning how to web scrape.  Best practices managing large data sets.  Building technical skills including SQL, Python, and PowerBI.  Applying cloud based technologies including data lakes and data pipelines. ', ' What is the professional development value of this role? ', ' Learning the equity investment business and engaging directly with end users. ', ' Building technical skills including SQL, Python, and PowerBI. ', ' Candidate must be willing to take ownership of projects and show strong client commitment ', ' Self starter as well as a good team player ', ' Automation of data ingestion supporting various sources and formats both external and internal.  Implementing a quality control framework for ensuring data consistency.  Cataloging new data sets to facilitate data discovery, lineage, and self-service.  Building business intelligence dashboards to provide data insights.  Assist with ad-hoc data and research requests from the investment team.  Provide support for overnight jobs. ', ' Best practices managing large data sets. ', ' Solid analytical and technical skills  Candidate must be willing to take ownership of projects and show strong client commitment  Must demonstrate good communication skills and be comfortable working closely with business users  Self starter as well as a good team player  A strong desire to document and share work done to aid in long term support ', ' The key job responsibilities include, but are not limited to: ', ' Provide support for overnight jobs. ', ' 5+ years programming in SQL with experience in relational schema designs and optimizing query performance ', ' Cataloging new data sets to facilitate data discovery, lineage, and self-service. ', 'This Role Provides Opportunity In The Following Areas', ' Experience working in the financial industry or knowledge of basic financial statement concepts  Azure experience building data pipelines  Experience using Airflow ', ' Onboard alternative datasets including learning how to web scrape. ', ' Automation of data ingestion supporting various sources and formats both external and internal. ', ' What makes this role unique or interesting? ', 'IT Group Description', ' Experience using Airflow ', 'Job Description', 'Special Knowledge (nice To Have, But Not Required)', ' Assist with ad-hoc data and research requests from the investment team. ', ' Working with NoSQL is a strong plus ', ' Implementing a quality control framework for ensuring data consistency. ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Rex,"Austin, TX",3 days ago,84 applicants,"['', 'Able to work with teammates at all management levels', '401(k) options ', 'SQL fluency + ETL pipeline experience', 'In Addition, We Offer The Following', 'Likes and is awesome at understanding the “what” then nailing the “how” on own', 'Unlimited PTO (Discretionary)', 'Execute an aggressive strategy to deliver key insights ', 'Medical, Dental, & Vision Benefits', 'In Year One You Will', '5+ years of industry experience in Business Intelligence, Data Scientist, or Analytics roles', 'Strong interpersonal skills', 'Strong communication skills, both oral and written', ' Founder/CEO Peter Rex in WSJ re: moving company from Seattle to Texas  Peter Rex Media Coverage - Summer 2020 2019 Video Reel ', 'Ability to work under pressure and in crisis situations', 'Peter Rex Media Coverage - Summer 2020', 'A demonstrated history of successfully executing projects and accomplishing high-level goals', 'The Ideal Candidate Has', 'Collaborate with all cross-functional leadership and management to identify areas of data capture and implement solutions to your key findings', 'Extreme character, talent, drive, and teaming ability', ' 5+ years of industry experience in Business Intelligence, Data Scientist, or Analytics roles At least 3+ years of experience in a tech start-up SQL fluency + ETL pipeline experience Has a builder mindset and is a true “data person” with a vision of what we will need to pipe in now that will prove to be incredibly valuable 10-years down the road Ability to break down high-level business problems into concrete analytical solutions Likes and is awesome at understanding the “what” then nailing the “how” on own Extreme character, talent, drive, and teaming ability Strong interpersonal skills Able to work with teammates at all management levels An entrepreneurial nature; not afraid to think outside the box A demonstrated history of successfully executing projects and accomplishing high-level goals Ability to work under pressure and in crisis situations Strong communication skills, both oral and written Willingness to relocate to Austin, Texas ', 'Leverage data insights to maximize the efficiency and optimization of our business by defining key metrics across functional verticals, reporting on KPIs/OKRs, developing optimization models, and improving processes', 'At least 3+ years of experience in a tech start-up', 'Work closely with Ops, Engineering, and Product leaders, you will significantly influence our roadmap to deliver world-class services as efficiently as possible', 'Founder/CEO Peter Rex in WSJ re: moving company from Seattle to Texas ', ' Take complete ownership of all data sources, piping, destinations, dashboards, and analytics within the current routing machine Layout an optimized data architecture, and tailor that structure for the company Execute an aggressive strategy to deliver key insights  Leverage data insights to maximize the efficiency and optimization of our business by defining key metrics across functional verticals, reporting on KPIs/OKRs, developing optimization models, and improving processes Collaborate with all cross-functional leadership and management to identify areas of data capture and implement solutions to your key findings Work closely with Ops, Engineering, and Product leaders, you will significantly influence our roadmap to deliver world-class services as efficiently as possible ', 'Layout an optimized data architecture, and tailor that structure for the company', 'Before you apply, review these resources to better understand our culture:', 'An entrepreneurial nature; not afraid to think outside the box', 'Equity', ' Medical, Dental, & Vision Benefits 401(k) options  Unlimited PTO (Discretionary) Equity', 'Has a builder mindset and is a true “data person” with a vision of what we will need to pipe in now that will prove to be incredibly valuable 10-years down the road', '2019 Video Reel', 'Take complete ownership of all data sources, piping, destinations, dashboards, and analytics within the current routing machine', 'Willingness to relocate to Austin, Texas', 'Ability to break down high-level business problems into concrete analytical solutions']",Entry level,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Engineer,Red Ventures,United States,,N/A,"['', 'Knowledge of Scala is preferred (Java, Go and Python is also acceptable)\xa0', 'Knowledge about agile software processes is a plus', ""What You'll Be Doing:"", 'Experience in Spark Structured Streaming is a plus', '2 years of experience with data processing (Hadoop, Spark), working in RDDs and DataFrames/Datasets API (with emphasis on DataFrames) to query and perform data manipulation', 'Data Engineers at Red Ventures drive business value by transferring and structuring data for production and consumption by engineers, businesses, analysts, data scientists and partners. You will join a team of highly skilled engineers who design, develop and automate high-quality, scalable solutions across the entire data lifecycle, from raw data to powerful insights and analytics. We are using Spark as our universal program on how we are transferring data across multiple platforms. If you want to join a team of collaborative, passionate and adept engineers that share your passion for high quality; this is the role for you.', '2 years of experience with any cloud computing platforms, preferably AWS (Kinesis, S3, Lambda, DynamoDB, Redshift)', ""Red Ventures is an equal opportunity employer that does not discriminate against any employee or applicant because of race, creed, color, religion, gender, sexual orientation, gender identity/expression, national origin, disability, age, genetic information, veteran status, marital status, pregnancy or any other basis protected by law. Employment at Red Ventures is based solely on a person's merit and qualifications.\xa0"", 'Founded in 2000, Red Ventures is a portfolio of growing digital businesses that bring consumers and brands together through integrated e-commerce, strategic partnerships and many proprietary brands including Bankrate, AllConnect.com and Reviews.com. Headquartered south of Charlotte, NC, Red Ventures has over 3000 employees in offices across the US, as well as London and Sao Paulo.\xa0For more information, visit www.redventures.com.', 'Linux common working knowledge, including navigating through the file system and simple bash scripting is a plus', '2 years of experience with SQL relational database (examples include Oracle, SQL Server, Postgres, MySQL, Teradata)', 'Experience in data engineering solutions best practices, including coding standards, code reviews, source control management (GitHub), build processes (CI/CD), testing and operations', 'Building large scale, real time data pipelines with Spark/Scala for Media and Technology data team, serving businesses like CNET.com, GameSpot.com, ZDNet.com, Time,Inc-NextAdvisor, etc.Working with a cross functional team of data scientists and analysts to understand business requirements leveraging our data.Design scalable solutions across distributed systems utilizing cutting edge cloud and big data technologies.', 'Working with a cross functional team of data scientists and analysts to understand business requirements leveraging our data.', 'We are committed to providing equal employment opportunities to qualified individuals with disabilities. This includes providing reasonable accommodation where appropriate. Should you require a reasonable accommodation to apply or participate in the job application or interview process, please contact accommodation@redventures.com.', 'Design scalable solutions across distributed systems utilizing cutting edge cloud and big data technologies.', 'Building large scale, real time data pipelines with Spark/Scala for Media and Technology data team, serving businesses like CNET.com, GameSpot.com, ZDNet.com, Time,Inc-NextAdvisor, etc.', '5 years of experience in software development, data engineering, business intelligence, data science or related fields with good exposure in distributed systems, distributed data processing frameworks, data warehouses, technical architectures, infrastructure components, ETL/ELT design patterns.', 'Experience with Storm or Cassandra is a plus', '5 years of experience in software development, data engineering, business intelligence, data science or related fields with good exposure in distributed systems, distributed data processing frameworks, data warehouses, technical architectures, infrastructure components, ETL/ELT design patterns.2 years of experience with SQL relational database (examples include Oracle, SQL Server, Postgres, MySQL, Teradata)2 years of experience with data processing (Hadoop, Spark), working in RDDs and DataFrames/Datasets API (with emphasis on DataFrames) to query and perform data manipulation2 years of experience with any cloud computing platforms, preferably AWS (Kinesis, S3, Lambda, DynamoDB, Redshift)Experience building large scale Spark applicationsKnowledge of Scala is preferred (Java, Go and Python is also acceptable)\xa0Experience in SparkSQL (Broadcast Joins)Experience in data engineering solutions best practices, including coding standards, code reviews, source control management (GitHub), build processes (CI/CD), testing and operationsLinux common working knowledge, including navigating through the file system and simple bash scripting is a plusExperience with Storm or Cassandra is a plusExperience in Spark Structured Streaming is a plusKnowledge about agile software processes is a plus', 'We offer competitive salaries and a comprehensive benefits program for full-time employees, including medical, dental and vision coverage, paid time off, life insurance, disability coverage, employee assistance program, 401(k) plan and a paid parental leave program.', ""What We're Looking For:"", 'Experience in SparkSQL (Broadcast Joins)', 'Who We Are:', 'Experience building large scale Spark applications', '\xa0']",Mid-Senior level,Full-time,Engineering,Internet,2021-03-18 14:34:51
Data Engineer,REI Systems,"Sterling, VA",,N/A,"['', 'Federal government experience is strongly preferred.', 'Required Skills', 'Experience with other data analytics tools such as RapidMiner and MicroStrategy', 'Support dissemination and distribution of data and reports.', 'Perform periodic check-ins with the customer unit management for status and feedback.', 'Support data analysis to study outliers and to understand the health of new programs within the customer unit.', 'Identify gaps in the system and communicate them to the solution project leads/program manager.', 'Employer will accept a suitable combination of education, training or experience', 'Expertise in one or more areas of Business Intelligence, Data Analytics, Decision Support or Business Analysis is required.', 'Experience in Hadoop, Big data related technologies ', 'Desired\xa0Skills', 'Familiarity with revision control systems such as Git/Github', 'Experience enhancing the performance of high-traffic sites', 'Experience supporting the US Patent and Trademark Office', 'Experience with Amazon Web Services (AWS)', 'Federal government experience is strongly preferred', ""Bachelor's Degree in Computer Science, Information Systems, and Statistics or related field is required."", 'Expertise in one or more of the following areas: Business Intelligence, Data Analytics, Decision Support, Business Analysis', 'Expertise in using Tableau, SQL, and Excel to build analysis to derive insights from data', 'Understand the business needs and using the knowledge of the data, prepare meaningful reports using datasets available through existing systems and datasets external to the customer unit.', 'Research and identify root cause/data issues.', 'You can be an Data Analyst anywhere, but at REI your analysis is transformational. Influence the solution!', 'Other similar professional duties maybe assigned as needed', 'Ensure data integrity between the different cross cut reports produced through the system.\xa0', 'Experience in data retrieval, manipulation, analysis, and presentation using visualizations', 'Ensure timely delivery of reports, and manage conflicting priorities and customer expectations.', '5 to 7 years of professional and proven experience in a related role', 'Experience delivering solutions using Agile delivery practices', 'Experience and/or understanding of JIRA', 'Other similar professional duties maybe assigned as neededPerform hands-on work with data analysis, validation and quality assurance, while working as a full-time on-site staff at the customer facility. Candidate must have at 5-7 years of professional and proven experience in a related role.\xa0Understand the customers’ business processes, the underlying data, and cross-cutting data sets for various systems within the customers’ business unit.\xa0Understand the business needs and using the knowledge of the data, prepare meaningful reports using datasets available through existing systems and datasets external to the customer unit.Support dissemination and distribution of data and reports.Perform data validation and quality assurance.\xa0Ensure data integrity between the different cross cut reports produced through the system.\xa0Research and identify root cause/data issues.Support data analysis to study outliers and to understand the health of new programs within the customer unit.Ensure timely delivery of reports, and manage conflicting priorities and customer expectations.Perform periodic check-ins with the customer unit management for status and feedback.Perform periodic status check-ins with the internal delivery/program manager.Identify gaps in the system and communicate them to the solution project leads/program manager.Document the unmet data needs of stakeholders for reporting purposes.Expertise in one or more areas of Business Intelligence, Data Analytics, Decision Support or Business Analysis is required.Federal government experience is strongly preferred.', 'Perform data validation and quality assurance.\xa0', 'Perform hands-on work with data analysis, validation and quality assurance, while working as a full-time on-site staff at the customer facility. Candidate must have at 5-7 years of professional and proven experience in a related role.\xa0', 'Document the unmet data needs of stakeholders for reporting purposes.', 'At REI Systems, our work makes a difference in the lives of millions of people every day. Do you want the opportunity to contribute, grow and learn every day? Enjoy working with amazing people? Being on a team that shares your values, matches your energy and truly impact people’s lives? If you are solution oriented, thrive in a fast paced, agile environment then come work for a company that was ranked\xa07th in the 2016 Washington Post Top Workplaces.', 'Proficiency in MS Excel with knowledge of Pivot Tables, VLookup, and other advanced functions', 'Message Queueing/Broker (e.g. Amazon SNS, SQS, Kafka, ActiveMQ, RabbitMQ) ', '""REI Systems is an Equal Opportunity Employer (Minority/Female/Disability/Vet)""', 'As a Sr Data Engineer, you will:', 'Perform periodic status check-ins with the internal delivery/program manager.', '7th in the 2016 Washington Post Top Workplaces.', 'Experience delivering solutions using Agile delivery practicesExperience with Amazon Web Services (AWS)Familiarity with revision control systems such as Git/GithubExperience enhancing the performance of high-traffic sitesExperience supporting the US Patent and Trademark Office', '\xa0', 'Understand the customers’ business processes, the underlying data, and cross-cutting data sets for various systems within the customers’ business unit.\xa0', 'Education:', '5 to 7 years of professional and proven experience in a related roleExpertise in using Tableau, SQL, and Excel to build analysis to derive insights from dataExperience in data retrieval, manipulation, analysis, and presentation using visualizationsProficiency in MS Excel with knowledge of Pivot Tables, VLookup, and other advanced functionsExperience in Hadoop, Big data related technologies Message Queueing/Broker (e.g. Amazon SNS, SQS, Kafka, ActiveMQ, RabbitMQ) Experience with other data analytics tools such as RapidMiner and MicroStrategyExperience and/or understanding of JIRAExpertise in one or more of the following areas: Business Intelligence, Data Analytics, Decision Support, Business AnalysisFederal government experience is strongly preferred\xa0', 'Work Authorization: US citizenship or work visa required']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Junior Data Engineer (AWS),Invesco US,"Atlanta, GA",2 weeks ago,Be among the first 25 applicants,"['', 'Strong experience with writing sophisticated programs, implementing architectures, and enabling automation in these environments', 'Health & wellbeing benefits', 'Understanding and experience working in an agile framework', 'Maintains a broad understanding of implementation, integration, and inter-connectivity issues with emerging technologies to define data strategies', '1-2 years of experience in building data pipelines in tools using Apache Airflow', 'Manage application and data integration platforms using AWS Cloud Components', 'Build robust data pipelines using modern data engineering technology stack and Cloud architecture', 'Experience with Columnar data stores - Redshift/Snowflake/Parquet', 'Exposure to machine learning services from Amazon Web Services (AWS) a plus', 'Work across teams to deliver meaningful reference architectures that outline architecture principles and standard methodologies for technology advancement', ' 1-2 years of experience in data modeling, data warehousing, and big data architectures 1-2 years of experience of Amazon cloud (AWS)- S3, EC2, RDS, Lambda, DMS, Glue, CloudWatch, SNS etc. 1-2 years of experience in building data pipelines in tools using Apache Airflow Knowledge of SQL and query optimization techniques Experience and Databases that include Oracle, MS Sql Server, Postgres, Aurora, Athena, Redshift Spectrum Experience with Columnar data stores - Redshift/Snowflake/Parquet Experience working with structured, semi structured and Unstructured data sets Proficient in application/software architecture (Definition, Business Process Modeling, etc.) Programming experience with Python or Scala Knowledge and experience of various data modeling techniques Knowledge and experience in distributed data processing using Spark or Hadoop Experience using GitHub, Bit Bucket, or other code repository solution Understanding of cloud architecture, networking, and security etc. Understanding and experience working in an agile framework Experience building traditional data pipelines with Informatica Exposure to machine learning services from Amazon Web Services (AWS) a plus Understanding of RESTful API design/build DevOps experience is a plus ', 'Partner with the Business Analytics team to optimize the cloud / redshift environment to support data sciences capability', 'Manage solution providers, define sourcing approach and manage the providers', ' Build robust data pipelines using modern data engineering technology stack and Cloud architecture Manage application and data integration platforms using AWS Cloud Components Understand the business and enable the full life cycle of development/ reporting/ integration projects: planning, design, develop, testing and rollout that confirms to Agile Standards Responsible for Data availability/enablement for business reporting within the SLA Manage solution providers, define sourcing approach and manage the providers Create and manage data, applications and technology architecture documentation and design artifacts Work across teams to deliver meaningful reference architectures that outline architecture principles and standard methodologies for technology advancement Gain adoption of architecture processes, standards and procedures Partner with the Business Analytics team to optimize the cloud / redshift environment to support data sciences capability Help maintain the code and capability environment required to evolve data-driven, analytical capabilities with the end goal of understanding customer behavior and competitive dynamics Maintains a broad understanding of implementation, integration, and inter-connectivity issues with emerging technologies to define data strategies Assist in the decision-making process related to the selection of software architecture solutions Contribute to the build of the data engineering and feature engineering capabilities required to support customer centric analytics Assist in crafting documents that ensure consistency in development across the online organization. Strong experience with writing sophisticated programs, implementing architectures, and enabling automation in these environments Consolidate, standardize, and control changes to capacity management data and metric definitions, ownership, accountability, and taxonomy to ensure alignment in understanding Collaborate with business domain experts to make strategic recommendations on data management and advocate to improve analytical capability across the organization Communicate with various business areas, partner on the formulation of technical requirements for data ingestion, verification, scheduling, etc ', 'Experience working with structured, semi structured and Unstructured data sets', 'Understanding of cloud architecture, networking, and security etc.', 'Knowledge of SQL and query optimization techniques', 'Help maintain the code and capability environment required to evolve data-driven, analytical capabilities with the end goal of understanding customer behavior and competitive dynamics', 'Gain adoption of architecture processes, standards and procedures', 'Assist in the decision-making process related to the selection of software architecture solutions', 'Responsible for Data availability/enablement for business reporting within the SLA', 'Understanding of RESTful API design/build', 'Collaborate with business domain experts to make strategic recommendations on data management and advocate to improve analytical capability across the organization', 'Knowledge and experience in distributed data processing using Spark or Hadoop', 'Communicate with various business areas, partner on the formulation of technical requirements for data ingestion, verification, scheduling, etc', 'Employee stock purchase plan', 'Your Role', 'Contribute to the build of the data engineering and feature engineering capabilities required to support customer centric analytics', 'The Department', 'Knowledge and experience of various data modeling techniques', 'You will be responsible for:', 'Flexible time off and opportunities for a flexible work schedule', 'Create and manage data, applications and technology architecture documentation and design artifacts', 'Assist in crafting documents that ensure consistency in development across the online organization.', '1-2 years of experience in data modeling, data warehousing, and big data architectures', '1-2 years of experience of Amazon cloud (AWS)- S3, EC2, RDS, Lambda, DMS, Glue, CloudWatch, SNS etc.', 'What’s in it for you?', 'DevOps experience is a plus', 'Parental Leave benefits', 'Experience and Databases that include Oracle, MS Sql Server, Postgres, Aurora, Athena, Redshift Spectrum', 'Programming experience with Python or Scala', ' Flexible time off and opportunities for a flexible work schedule 401(K) matching of 100% up to the first 6% with additional supplemental contribution Health & wellbeing benefits Parental Leave benefits Employee stock purchase plan ', 'Consolidate, standardize, and control changes to capacity management data and metric definitions, ownership, accountability, and taxonomy to ensure alignment in understanding', 'Experience building traditional data pipelines with Informatica', 'The experience you bring:', 'Experience using GitHub, Bit Bucket, or other code repository solution', '401(K) matching of 100% up to the first 6% with additional supplemental contribution', 'Understand the business and enable the full life cycle of development/ reporting/ integration projects: planning, design, develop, testing and rollout that confirms to Agile Standards', 'Proficient in application/software architecture (Definition, Business Process Modeling, etc.)']",Not Applicable,Full-time,Information Technology,Investment Management,2021-03-18 14:34:51
Data Engineer,EyeCare Partners,"Ballwin, MO",2 days ago,Be among the first 25 applicants,"['', 'Development and implementation of scripts for datahub maintenance, monitoring, performance tuning', 'A self-motivated personality with a passion for working in a fast-paced environment', 'Experience working with multiple ETL/ELT tools and cloud based data hubs', 'Work with various business and technical stakeholders and assist with data-related technical needs and issues', ' Design, build and maintain data pipelines from various source systems into Snowflake Analyze data elements from various systems, data flow, dependencies, relationships and assist in designing conceptual physical and logical data models Design, build and maintain complex data sets designed to meet various business needs in the areas of reporting, advanced analytics and ad-hoc analysis Coordinate the build and maintenance of data pipelines by third party service providers Enabling and executing data migrations across systems (e.g. SQL server to Snowflake or other cloud data platforms) Development and implementation of scripts for datahub maintenance, monitoring, performance tuning Work with data and business analysts to deploy and support a robust data quality platform Work with data and business analysts to deploy and support a robust data cataloging strategy Work with various business and technical stakeholders and assist with data-related technical needs and issues Work with data and analytics teams and drive greater value from our data and analytics investments Work closely with cross-functional teams to understand and transform business requirements into scalable and manageable solutions Present solutions and options to leadership, project teams and other stakeholders adapting style to both technical and non-technical audiences Ensures teams adhere to documented design and development patterns and standards Proactively monitor and resolve on-going production issues Work closely with various technical teams to ensure consistency, quality of solutions and knowledge sharing across the enterprise Educate organization on available and emerging tool sets Ensure adherence to the approach of self-service data solutions and enable other teams with analytics solutions delivery via ‘Data as a Service’ model ', 'Work closely with cross-functional teams to understand and transform business requirements into scalable and manageable solutions', 'Proactively monitor and resolve on-going production issues', 'Demonstrated problem solving', 'Work closely with various technical teams to ensure consistency, quality of solutions and knowledge sharing across the enterprise', 'Experience with AWS cloud services: EC2, EMR, RDS, DMS', 'Enabling and executing data migrations across systems (e.g. SQL server to Snowflake or other cloud data platforms)', 'Work with data and business analysts to deploy and support a robust data quality platform', "" Bachelor's degree in Computer Science, Information Systems or equivalent + 3 years related experience 3+ years of hands-on-experience in the design, development, and implementation of data solutions Advanced SQL knowledge with strong query writing, stored procedures skills Experience with Snowflake development and support Experience with object-oriented/object function scripting languages: Python, Java, Scala, etc. Experience with AWS cloud services: EC2, EMR, RDS, DMS Experience with relational databases such as SQL Server and object relational databases such as PostgreSQL Experience with stream-processing systems: Storm, Spark-Streaming, etc. Experience with data analysis, ETL, and workflow automation Experience working with multiple ETL/ELT tools and cloud based data hubs Demonstrated problem solving Demonstrated ability to think and work with a proactive mindset A self-motivated personality with a passion for working in a fast-paced environment"", '3+ years of hands-on-experience in the design, development, and implementation of data solutions', 'Advanced SQL knowledge with strong query writing, stored procedures skills', 'Requirements', 'Design, build and maintain data pipelines from various source systems into Snowflake', ""Bachelor's degree in Computer Science, Information Systems or equivalent + 3 years related experience"", 'Experience with object-oriented/object function scripting languages: Python, Java, Scala, etc.', 'Present solutions and options to leadership, project teams and other stakeholders adapting style to both technical and non-technical audiences', 'Position Summary:', 'Ensures teams adhere to documented design and development patterns and standards', 'Essential Responsibilities', 'Experience with Snowflake development and support', 'Coordinate the build and maintenance of data pipelines by third party service providers', 'Work with data and analytics teams and drive greater value from our data and analytics investments', 'Experience with data analysis, ETL, and workflow automation', 'Ensure adherence to the approach of self-service data solutions and enable other teams with analytics solutions delivery via ‘Data as a Service’ model', 'Design, build and maintain complex data sets designed to meet various business needs in the areas of reporting, advanced analytics and ad-hoc analysis', 'Experience with stream-processing systems: Storm, Spark-Streaming, etc.', 'Experience with relational databases such as SQL Server and object relational databases such as PostgreSQL', 'Demonstrated ability to think and work with a proactive mindset', 'Analyze data elements from various systems, data flow, dependencies, relationships and assist in designing conceptual physical and logical data models', 'Work with data and business analysts to deploy and support a robust data cataloging strategy', 'Educate organization on available and emerging tool sets', 'Data Engineer – Information Technology']",Associate,Full-time,Information Technology,Medical Practice,2021-03-18 14:34:51
Data Engineer,PlayStation,"San Diego, CA",2 weeks ago,137 applicants,"['', 'Strong verbal and written communication skills, and be able to work with others at all levels, effective at working with geographically remote and culturally diverse teams.', ' ', 'BS Degree in Engineering, Computer Science or equivalent experience', 'Mentors and inspires to effectively deliver awesome software/data solutions!', 'PRIVACY NOTICE TO SIE LLC’S JOB APPLICANTS', 'Indirect identifiers such as a government ID, your Social Security, work permit or passport #.', ' Professional or job position-related information ', ' Non-public education information , including information about your education records, such as grades and transcripts.', ' Non-public education information ', 'Strong foundation in data engineering and software design.', 'Experience with cloud-based services at enterprise scale such as AWS', 'Responsibilities', 'Direct identifiers such as your first and last name.', 'You will play a role in design, development, automation, quality, and operations for the PlayStation data platform', ' You will be part of a team building data and software solutions that enable frictionless data delivery, enabling business value to our internal business units You will be expected to build a deep understanding of PlayStation data and help data consumers drive business value from the data You will play a role in design, development, automation, quality, and operations for the PlayStation data platform Mentors and inspires to effectively deliver awesome software/data solutions! You will participate in product road-map discussions and identify key areas for improvement. ', 'Experience with AWS Big Data technologies such as Kinesis, Lambda, Firehose, Dynamo DB, Athena/Presto.', 'Contact information such as your email address, mailing address, telephone number.', 'Strong experience in advanced software development (i.e. Python, Scala, Java programming), design, and analysis.', ' Experience and working knowledge of SQL. Ability to write complex, highly performing SQL queries and optimize performance of existing queries preferred Experience with cloud-based services at enterprise scale such as AWS Experience with cloud-based data warehousing solutions such as Snowflake or Redshift Experience with AWS Big Data technologies such as Kinesis, Lambda, Firehose, Dynamo DB, Athena/Presto. ', 'Preferred:', 'Experience with cloud-based data warehousing solutions such as Snowflake or Redshift', 'Categories of personal information we collect from you', '  Identification and contact information  Direct identifiers such as your first and last name. Indirect identifiers such as a government ID, your Social Security, work permit or passport #. Contact information such as your email address, mailing address, telephone number.   Other information about you or that can be associated with you such as:  Sensitive/Protected Data. During the recruitment process, you may (voluntarily) provide us with your ethnicity, gender, military service information, or physical or mental health information, as well as your national origin and citizenship.  Professional or job position-related information , including your past professional experience, references; background verification; talent management and assessment; information regarding any conflicts of interests; and the terms and conditions of your job offer.  Non-public education information , including information about your education records, such as grades and transcripts.', 'Qualifications:', 'You will participate in product road-map discussions and identify key areas for improvement.', ' BS Degree in Engineering, Computer Science or equivalent experience Strong experience in advanced software development (i.e. Python, Scala, Java programming), design, and analysis. Strong foundation in data engineering and software design. Possess the drive and passion for quality with the ability to inspire, excite and motivate other team members. Strong verbal and written communication skills, and be able to work with others at all levels, effective at working with geographically remote and culturally diverse teams. ', ' Sensitive/Protected Data. During the recruitment process, you may (voluntarily) provide us with your ethnicity, gender, military service information, or physical or mental health information, as well as your national origin and citizenship.', ' Other information about you or that can be associated with you such as: ', ' Sensitive/Protected Data. ', ' Identification and contact information ', 'You will be part of a team building data and software solutions that enable frictionless data delivery, enabling business value to our internal business units', 'Experience and working knowledge of SQL. Ability to write complex, highly performing SQL queries and optimize performance of existing queries preferred', '  Other information about you or that can be associated with you such as:  Sensitive/Protected Data. During the recruitment process, you may (voluntarily) provide us with your ethnicity, gender, military service information, or physical or mental health information, as well as your national origin and citizenship.  Professional or job position-related information , including your past professional experience, references; background verification; talent management and assessment; information regarding any conflicts of interests; and the terms and conditions of your job offer.  Non-public education information , including information about your education records, such as grades and transcripts.', ' Professional or job position-related information , including your past professional experience, references; background verification; talent management and assessment; information regarding any conflicts of interests; and the terms and conditions of your job offer.', 'Generally, We Obtain This Information Through Our Recruiting Team', 'Possess the drive and passion for quality with the ability to inspire, excite and motivate other team members.', 'You will be expected to build a deep understanding of PlayStation data and help data consumers drive business value from the data', 'Must Have:']",Not Applicable,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Engineer IV,The Standard,United States,1 day ago,Be among the first 25 applicants,"['', '#LI-Remote', 'The ability and desire to continue learning new things. You have at least an undergrad degree in engineering and ideally a graduate degree; or, you have a combination of other education and equivalent experience', 'Demonstrated technical experience which includes\xa0fundamentals like:\xa0', 'Strong technical mentoring and collaboration skills', 'What You’ll Bring', 'Hadoop\xa0–\xa0Python,\xa0PySpark, Jupiter Notebooks, along with\xa0streaming (Kafka/ Stream Analytics)\xa0and\xa0unstructured\xa0data\xa0', 'Adaptability – quickly applying customer feedback and circumstances to team priorities\xa0', 'Excellent communication skills and professional maturity, with the ability to switch contexts between highly creative and technical topics quickly and easily', 'Drive to Win as a Team – constantly pursing exceptional results that help us win in the market\xa0', 'Collaboration – working with all team members, and across teams, to integrate perspectives and encourage contributions\xa0', 'We’re looking for\xa0an amazing\xa0Senior\xa0Data\xa0Engineer\xa0to join our\xa0enterprise team.\xa0In this\xa0key\xa0role, you’ll provide technical leadership to help us leverage and evolve our strategic data & analytics platform for actuarial\xa0digital transformation.\xa0You’ll bring your extensive background with both traditional data architectures and modern “big data” platforms to help architect, design and deliver innovative capabilities, working with and mentoring a talented, motivated team of engineers and analysts.\xa0', 'We offer a caring culture where you can make a real difference, every day.', 'Our continued success is driven by a high-performance culture. We’re looking for people who are collaborative, accountable, creative, agile and are driven by a passion for doing what’s right – across the company and in our local communities.', 'Drive to Win as a Team – constantly pursing exceptional results that help us win in the market\xa0Respect for Teammates – encouraging diverse identities, backgrounds,\xa0talent\xa0and perspectives\xa0Collaboration – working with all team members, and across teams, to integrate perspectives and encourage contributions\xa0Adaptability – quickly applying customer feedback and circumstances to team priorities\xa0', 'A strategic mindset for the importance and power of data across a spectrum of use cases and possibilities', 'A\xa0mindset that includes:\xa0', 'Azure Data Stack – API, Security, permissions,\xa0Powershell, Pester, resource-to-resource interfacing, Log Analytics, ADF or similar, Databricks, Synapse\xa0and\xa0Runbooks\xa0DevOps\xa0–\xa0Powershell, Pester, Git (source code, pull request, branching, merging)\xa0Hadoop\xa0–\xa0Python,\xa0PySpark, Jupiter Notebooks, along with\xa0streaming (Kafka/ Stream Analytics)\xa0and\xa0unstructured\xa0data\xa0', '\u200b', 'Azure Data Stack – API, Security, permissions,\xa0Powershell, Pester, resource-to-resource interfacing, Log Analytics, ADF or similar, Databricks, Synapse\xa0and\xa0Runbooks\xa0', 'What We’re Looking For', 'What You’ll Bring\xa0', 'If you want to make a positive difference and challenge your career in new ways, you’ll fit right in at The Standard. We help our customers achieve financial well-being and peace of mind through leading insurance products and services, retirement plans, individual\xa0annuities\xa0and commercial mortgages. And we’re growing, with new products, distribution channels and customer needs driving exciting technology investment. Come join us and share our passion for serving customers in a positively different way.\xa0', 'Deep knowledge of modern “big data” and Data Science technologies, with Microsoft Azure Platform experience highly preferred', 'What We’re Looking For\xa0', 'DevOps\xa0–\xa0Powershell, Pester, Git (source code, pull request, branching, merging)\xa0', '\xa0', 'Ready to reach your highest potential? Let’s work together.', 'A qualified candidate will have:\xa0', 'Respect for Teammates – encouraging diverse identities, backgrounds,\xa0talent\xa0and perspectives\xa0', 'Deep knowledge of modern “big data” and Data Science technologies, with Microsoft Azure Platform experience highly preferredA strategic mindset for the importance and power of data across a spectrum of use cases and possibilitiesStrong technical mentoring and collaboration skillsExcellent communication skills and professional maturity, with the ability to switch contexts between highly creative and technical topics quickly and easilyThe ability and desire to continue learning new things. You have at least an undergrad degree in engineering and ideally a graduate degree; or, you have a combination of other education and equivalent experience']",Mid-Senior level,Full-time,Information Technology,Insurance,2021-03-18 14:34:51
Data Engineer,Root Inc.,"Columbus, OH",3 weeks ago,41 applicants,"['', 'At least 3 years of experience in the insurance industry is strongly preferredExperience using technologies listed above is preferredSolid SQL skills. Ability to transform data without the use of an ETL tool.Experience using version control tools like GITFamiliarity with programming languages like Ruby or PythonFamiliarity with DevOps & Agile processes', 'Design solutions which help us to reach our overall goals', 'Design & develop sustainable, fast ETL processes using SQL', 'Design & develop data structures that support downstream analysis', 'Provide peer review for teammates on their change requests', 'The Team: ', 'Familiarity with DevOps & Agile processes', 'Familiarity with programming languages like Ruby or Python', 'Responsibilities', 'Experience using version control tools like GIT', 'Work with product, actuarial, and engineering teams to understand and scope new features for our environment', 'Qualifications', 'Experience using technologies listed above is preferred', 'At least 3 years of experience in the insurance industry is strongly preferred', 'Help to ensure data quality and meet data delivery SLA’s', 'Create processes to identify, prioritize, and illustrate data quality issues and remediation efforts.', 'Work with product, actuarial, and engineering teams to understand and scope new features for our environmentDesign & develop data structures that support downstream analysisDesign & develop sustainable, fast ETL processes using SQLProvide peer review for teammates on their change requestsCreate processes to identify, prioritize, and illustrate data quality issues and remediation efforts.Design solutions which help us to reach our overall goalsHelp to ensure data quality and meet data delivery SLA’s', 'Our Progress:', 'Our Tech Stack Includes', 'Solid SQL skills. Ability to transform data without the use of an ETL tool.', 'The Company:']",Entry level,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Engineer,Viasat Inc.,"Carlsbad, CA",2 days ago,Be among the first 25 applicants,"['', 'Experienced designing highly scaled systems', 'Experience maintaining SLAs', 'Familiar with Linux environment and common tools', 'A BS in Computer Science, Computer Engineering, Software Engineering, Electrical Engineering, Math, Physics or related field', 'Understanding of distributed and parallel computing architectures', 'Familiarity with RDBMS technologies (PostgreSQL, MySQL, etc.)', 'Experience building scalable infrastructure using AWS products', 'Travel up to 10%', 'Experience with Redshift, Google Big Query, and/or Presto', 'Familiar with several languages, including Python, Java, Scala', 'Requirements', '4+ years of development experience', 'Experience with SnowplowExperience with data transformation technologies like DBTExperience with Redshift, Google Big Query, and/or PrestoFamiliarity with RDBMS technologies (PostgreSQL, MySQL, etc.)Understanding of distributed and parallel computing architecturesExperience with modern deployment orchestration toolschains including CloudformationExperience maintaining SLAs4+ years of development experience', 'Experience with data transformation technologies like DBT', 'Design and build data engineering and integration solutionsDesign and build analytic tools and data models', 'Experience with modern deployment orchestration toolschains including Cloudformation', 'Experienced designing highly scaled systemsExperience building scalable infrastructure using AWS productsFamiliar with several languages, including Python, Java, ScalaFamiliar with Linux environment and common toolsA BS in Computer Science, Computer Engineering, Software Engineering, Electrical Engineering, Math, Physics or related fieldTravel up to 10%', 'Design and build data engineering and integration solutions', 'Experience with Snowplow', 'In This Position, You Will', 'Preferences', 'Design and build analytic tools and data models']",Not Applicable,Full-time,Information Technology,Defense & Space,2021-03-18 14:34:51
Data Engineer - Remote - 125K,Jefferson Frank,"Fairfax, VT",2 days ago,Be among the first 25 applicants,"['', ' SQL, (Postgres, MySQL)', ' Solid mission, VERY strong funding Never stopped hiring during COVID-19, very stable Really strong benefits', ' Kubernetes/Kubernetes tools like Helm (Docker experience is OK)', ' APIs exp a plus', 'Skills', 'Benefits', ' 135 employees', ' They are a health technology company that uses evidence-based data to solve health challenges. They come up with research-related products that can be utilized in the health care field employees', ' Up to 10% bonus for each role, to be paid out quarterly', ' Extensive Python exp', ' Experience creating Data Pipelines & Data Lakes', ' 80% health, dental & vision coverage for individual AND families', ' Up to 10% bonus for each role, to be paid out quarterly 80% health, dental & vision coverage for individual AND families 401k - 100% match up to 6%. Fully vested after first year Unlimited sick leave, 3 weeks PTO', 'About My Client', ' Unlimited sick leave, 3 weeks PTO', ' 4+ yrs exp', ' Never stopped hiring during COVID-19, very stable', ' Data experience in Hadoop, Spark, HIVE', ' AWS experience required (Redshift required)', ' Solid mission, VERY strong funding', ' Really strong benefits', ' 401k - 100% match up to 6%. Fully vested after first year', ' 135 employees HQ in Fairfax, VA. They are remote now but want the candidates in-office once they return They are a health technology company that uses evidence-based data to solve health challenges. They come up with research-related products that can be utilized in the health care field employees', ' 4+ yrs exp AWS experience required (Redshift required) Extensive Python exp Experience creating Data Pipelines & Data Lakes Data experience in Hadoop, Spark, HIVE Kubernetes/Kubernetes tools like Helm (Docker experience is OK) SQL, (Postgres, MySQL) APIs exp a plus', ' HQ in Fairfax, VA. They are remote now but want the candidates in-office once they return']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,JPMorgan Chase & Co.,"Newark, DE",4 weeks ago,Be among the first 25 applicants,"['', 'Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.', 'Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.', 'Organization', 'Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.', 'They Should Also Have Experience Using The Following Software/tools', 'About Us', ' Experience with big data tools: Hadoop, Spark, Kafka, etc. Experience with relational SQL and NoSQL databases, including Postgres and Cassandra. Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc. Experience with AWS cloud services: EC2, EMR, RDS, Redshift Experience with stream-processing systems: Storm, Spark-Streaming, etc. Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc. ', 'A successful history of manipulating, processing and extracting value from large disconnected datasets.', 'Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.', 'Strong analytic skills related to working with unstructured datasets.', 'Strong project management and organizational skills.', 'Work with data and analytics experts to strive for greater functionality in our data systems.', 'Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.', 'Build processes supporting data transformation, data structures, metadata, dependency and workload management.', 'Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.', 'Experience with AWS cloud services: EC2, EMR, RDS, Redshift', 'Experience supporting and working with cross-functional teams in a dynamic environment.', 'Advanced knowledge of application, data, and infrastructure architecture disciplines', 'BS/BA degree or equivalent experience', 'Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.', ' BS/BA degree or equivalent experience Advanced knowledge of application, data, and infrastructure architecture disciplines Understanding of software skills such as business analysis, development, maintenance, and software improvement Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies. Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics. Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Keep our data separated and secure across national boundaries through multiple data centers and AWS regions. Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader. Work with data and analytics experts to strive for greater functionality in our data systems. Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases. Experience building and optimizing ‘big data’ data pipelines, architectures and data sets. Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. Strong analytic skills related to working with unstructured datasets. Build processes supporting data transformation, data structures, metadata, dependency and workload management. A successful history of manipulating, processing and extracting value from large disconnected datasets. Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores. Strong project management and organizational skills. Experience supporting and working with cross-functional teams in a dynamic environment. ', 'Understanding of software skills such as business analysis, development, maintenance, and software improvement', 'Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.', 'Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.', 'Experience with stream-processing systems: Storm, Spark-Streaming, etc.', 'Job Description', 'Experience with big data tools: Hadoop, Spark, Kafka, etc.', 'Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.', 'Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.', 'Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.', 'Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Storable,"Illinois, United States",3 weeks ago,100 applicants,"['', 'Engage remotely! Since mid-March we transitioned to a fully remote environment and we run regular contests, trivia games and remote happy hours!', 'Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using various SQL and ‘big data’ technologies', 'About Us:', 'Assemble large, complex data sets that meet both functional and non-functional requirements', 'Create and maintain optimal data pipeline architecture', 'Get active in the community by joining one of our many quarterly offsite volunteer and community service events.', 'Create and maintain optimal data pipeline architectureAssemble large, complex data sets that meet both functional and non-functional requirementsIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using various SQL and ‘big data’ technologiesBuild analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metricsWork closely with stakeholders including the Executive, Product, Data and Design teams to design and deliver products and functionality to address analytical and functional data needsCreate data tools for analytics and data scientist and business operations team members that assist them in building and optimizing our productWork with data and analytics experts to strive for greater functionality in our data systems', 'Storable is looking for a Data Engineer to join our Data Architecture team in Austin. This team member will be responsible for expanding and optimizing our cloud native big data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing distributed systems as well as building them from the ground up.\xa0', 'Work with data and analytics experts to strive for greater functionality in our data systems', 'Computer science degree or equivalent experience3+\xa0 years experience in software development, data engineering, BI development, and / or data architectureExperience with Python, SQL, Airflow, AWS, RESTful APIs, and Tableau or other data visualization toolsConsistent track record of leading successful delivery for a large-scale project or being a key contributor on multiple projectsConsistent track record of positively influencing project direction and contributions to cross-functional and/or cross-organizational collaborations', 'Benefits and Perks:', 'Instant-Access & Flexible vacation. We trust you, so we have a ‘take what you need’ vacation policy. No waiting to use it or need for accruals.', 'Do More, Be More – Company rewards and recognition that add up and can be redeemed for once-in-a-lifetime Bucketlist experiences!', 'All applicants must be currently authorized to work in the United States on a full-time basis.', 'Must\xa0be located on:\xa0TX, KS, NC, MO, CO, PA, IL, IN', 'Create data tools for analytics and data scientist and business operations team members that assist them in building and optimizing our product', 'Computer science degree or equivalent experience', 'Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics', 'Experience with Python, SQL, Airflow, AWS, RESTful APIs, and Tableau or other data visualization tools', 'Must', 'The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.', 'We leverage our platform in combination with our over 25 years of storage industry expertise to help our thousands of storage customers achieve their tenant experience and operational efficiency objectives every single day.', 'Location:\xa0Remote', 'Consistent track record of leading successful delivery for a large-scale project or being a key contributor on multiple projects', 'What you’ll do everyday:', 'Consistent track record of positively influencing project direction and contributions to cross-functional and/or cross-organizational collaborations', 'Generous health coverage for you and your family, including short- and long-term disability coverage, 401(k) and HSA matching, company-provided life insurance, and more.Engage remotely! Since mid-March we transitioned to a fully remote environment and we run regular contests, trivia games and remote happy hours!Instant-Access & Flexible vacation. We trust you, so we have a ‘take what you need’ vacation policy. No waiting to use it or need for accruals.Do More, Be More – Company rewards and recognition that add up and can be redeemed for once-in-a-lifetime Bucketlist experiences!Get active in the community by joining one of our many quarterly offsite volunteer and community service events.Fun company events, including Halloween costume contests, ugly sweater competitions, baseball game outings, ice cream socials, food trucks, and more.', 'Storable is committed to providing equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, Storable will provide reasonable accommodations for qualified individuals with disabilities.', 'Generous health coverage for you and your family, including short- and long-term disability coverage, 401(k) and HSA matching, company-provided life insurance, and more.', '3+\xa0 years experience in software development, data engineering, BI development, and / or data architecture', 'At Storable, we believe storage operators should have one partner they can trust to help you get the results they need for their business. That’s why we’ve built the industry’s first fully integrated platform that offers facility management software, facility websites, marketing programs and services, payments, and deeply integrated tenant insurance capabilities all in one solution.\xa0', 'Fun company events, including Halloween costume contests, ugly sweater competitions, baseball game outings, ice cream socials, food trucks, and more.', 'Data Engineer', 'What you need to bring to the table:', 'Work closely with stakeholders including the Executive, Product, Data and Design teams to design and deliver products and functionality to address analytical and functional data needs', 'The Data Engineer will support our software developers, database architects, data analysts and data scientists on stakeholder initiatives and ensure delivered architecture is consistent and supportable throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products.\xa0', 'Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.']",Mid-Senior level,Full-time,Information Technology,Internet,2021-03-18 14:34:51
Data Engineer,Adobe,"Lehi, UT",6 days ago,52 applicants,"['', 'What You Need To Succeed', 'Building and maintaining an enterprise data lake', 'Experience building or managing a data lake in Azure', 'Experience partitioning data for optimal performance', 'Writing python connectors to 3rd party data APIs', 'Distributed systems (pipelines and databases)', 'Bonus Skills', 'Specialties in Python development for data pipelines and automation, including debugging, testing, and handling development and production code bases. At least 5 years experience preferred.Building and maintaining an enterprise data lakeExperience partitioning data for optimal performanceDistributed systems (pipelines and databases)Columnar technologies like ParquetExtensive SQL experience and strong background in optimizing SQL queries for transforming data. At least 5 years experience preferred.Significant experience in Linux environments, batch automation, and shell scripts', 'Significant experience in Linux environments, batch automation, and shell scripts', 'Our Company', 'The Opportunity', 'Technologies like Presto, Spark, Apache Iceberg, and DatabricksAWS technologies like EC2, S3, Glue, Etc.Experience building or managing a data lake in AzurePrevious experience in software application development in other languagesWriting python connectors to 3rd party data APIsExperience automating machine learning data pipelines and models', 'Previous experience in software application development in other languages', 'Specialties in Python development for data pipelines and automation, including debugging, testing, and handling development and production code bases. At least 5 years experience preferred.', 'Extensive SQL experience and strong background in optimizing SQL queries for transforming data. At least 5 years experience preferred.', 'Columnar technologies like Parquet', 'Technologies like Presto, Spark, Apache Iceberg, and Databricks', ""What You'll Do"", 'Experience automating machine learning data pipelines and models', 'AWS technologies like EC2, S3, Glue, Etc.']",Mid-Senior level,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Data Engineer ,"Metasys Technologies, Inc.","New York, United States",2 days ago,83 applicants,"['Location : Bristol , CT or NY', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa05+ years of experience working as an Oracle / Snowflake database developer (Oracle 11g or greater)', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0At least 5+ years with data warehouse design, development in an Oracle, Snowflake database developer (Oracle 11g or greater) or similar database technologies', ""•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience with Airflow tool and DAG's creation, Jobs Orchestration."", '\xa0', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa05+ years of working in a data warehousing / big data environment', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Database Architecture, SQL\xa0, Database performance/Optimization .', 'Duration : 6 months (extendable)', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0SQL ETL/ELT development and performance tuning', 'Position : Sr . Data Engineer ', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Must have hands on experience implementing AWS Big data lake using EMR and Spark', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Ability to develop, implement and maintain standards established by the architecture and Development teams.']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Analytics Engineer,Oxygen,"Remote, OR",2 days ago,Be among the first 25 applicants,"['', 'High EQ, Low ego. Plays well with business partners across the company. We are a small team.', 'Competitive salary based on experience.Equity from an early stage startup.Medical, Dental, Vision benefits.Free snacks, Lunch stipend (unfortunately not now during COVID SIP).PTOs and sick days.Awesome opportunities to grow yourself in different ways.', 'Own, build and manage scalable pipelines/python-based ETL tools for ingesting, processing, and delivering data.', ""You're a highly motivated self-starter with the ability to work efficiently with minimal supervision."", 'Own, build and manage scalable pipelines/python-based ETL tools for ingesting, processing, and delivering data.Own and be the driving force behind the adoption and effective use of our BI tool within Oxygen.Develop data models and schemas in our data warehouse that enable performant, intuitive analysis.Partner with teams across the organization to understand their analytics needs, create dashboards and reports that allow them to execute more effectively.Work with business leaders and cross functional partners to identify business problems, define key metrics and build reporting to monitor and measure performance along those metrics.Conduct in-depth data analyses that lead to actionable insights, owning the entire process from ideation to execution to presentation of findings to stakeholders.Become an expert on all aspects of Oxygen’s data and analytics infrastructure.Special projects as necessary.', 'Optimistic, enjoys the challenge of building a world changing FinTech start-up from the ground level.', 'Proficient writing SQL queries and with data warehousing concepts working on Redshift, Snowflake etc.', ' Nice To Have ', ' What We Offer ', 'Experience with a scripting language (preferably Python) for analysis.', 'Own and be the driving force behind the adoption and effective use of our BI tool within Oxygen.', 'Develop data models and schemas in our data warehouse that enable performant, intuitive analysis.', '2-4 years of work experience as a data analyst, data engineer, or in a highly analytical role.', ""The capacity to juggle multiple priorities effectively within a fast-paced environment is critical.You're a highly motivated self-starter with the ability to work efficiently with minimal supervision.Grasp of statistics and experience conducting rigorous data analyses for Business Operations / Marketing / Finance functions.Passion for democratizing data throughout the company and communicating insights to a broad audience with varying levels of technical expertise."", 'Passion for democratizing data throughout the company and communicating insights to a broad audience with varying levels of technical expertise.', 'Partner with teams across the organization to understand their analytics needs, create dashboards and reports that allow them to execute more effectively.', 'BA/BS in a quantitative field.2-4 years of work experience as a data analyst, data engineer, or in a highly analytical role.Proficient writing SQL queries and with data warehousing concepts working on Redshift, Snowflake etc.Familiar with A/B testing and proficient using BI tools such as Tableau, MixPanel, Looker, Amplitude, Segment, Dash etc.Experience using data processing frameworks and orchestrations tools such as Airflow, Luigi, Dataswarm, Glue, Spark, Dask etc.Experience with a scripting language (preferably Python) for analysis.Experience using git and working on AWS infrastructure.Experience at an e-commerce, B2C Saas or fintech startup a plus.Optimistic, enjoys the challenge of building a world changing FinTech start-up from the ground level.High EQ, Low ego. Plays well with business partners across the company. We are a small team.Prior startup and fintech experience preferred, but not required.', 'BA/BS in a quantitative field.', 'Experience using git and working on AWS infrastructure.', ' Who You Are ', 'Conduct in-depth data analyses that lead to actionable insights, owning the entire process from ideation to execution to presentation of findings to stakeholders.', 'Become an expert on all aspects of Oxygen’s data and analytics infrastructure.', 'Special projects as necessary.', 'PTOs and sick days.', 'Familiar with A/B testing and proficient using BI tools such as Tableau, MixPanel, Looker, Amplitude, Segment, Dash etc.', 'Prior startup and fintech experience preferred, but not required.', 'Medical, Dental, Vision benefits.', 'Experience at an e-commerce, B2C Saas or fintech startup a plus.', 'Awesome opportunities to grow yourself in different ways.', 'Experience using data processing frameworks and orchestrations tools such as Airflow, Luigi, Dataswarm, Glue, Spark, Dask etc.', 'Grasp of statistics and experience conducting rigorous data analyses for Business Operations / Marketing / Finance functions.', 'Work with business leaders and cross functional partners to identify business problems, define key metrics and build reporting to monitor and measure performance along those metrics.', 'Free snacks, Lunch stipend (unfortunately not now during COVID SIP).', 'The capacity to juggle multiple priorities effectively within a fast-paced environment is critical.', 'Equity from an early stage startup.', 'Competitive salary based on experience.']",Entry level,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Engineer,Compliant Pharmacy Alliance Cooperative,"Stoughton, WI",1 week ago,53 applicants,"['', 'What we need:', 'Azure', 'Data Engineer', 'Azure Data Engineer', 'What you’ll do:', 'This role can be a remote role, but will need to be within driving distance of Stoughton as there will be periodic needs to be onsite.\xa0', '2-3 years of Azure data experience (work as a Data Engineer with emphasis in Azure, Database Administrator in cloud-based environment, and/or ETL Developer with an emphasis in Azure)Basic understanding of Power BIFundamental knowledge of T-SQL syntax and database designBasic understanding of SSMS (SQL Server Management Studio)Knowledge of how to extract flat files (Delimited, JSON, Parquet, etc.) and transform the data into usable structuresAbility to engineer, create, and maintain Azure data pipelines', 'Knowledge of how to extract flat files (Delimited, JSON, Parquet, etc.) and transform the data into usable structures', 'Basic understanding of SSMS (SQL Server Management Studio)', 'Basic understanding of Power BI', 'Ability to engineer, create, and maintain Azure data pipelines', 'Fundamental knowledge of T-SQL syntax and database design', 'Compliant Pharmacy Alliance Cooperative in Stoughton, WI, is currently seeking an Azure Data Engineer to join their team.\xa0Working for a small, dynamic, and growing company this role is responsible for taking in raw data from multiple sources, transforming the data to a usable format and storing the data securely for company use.\xa0Transforming the data is complex, but is key to this role as we are refactoring (not doing a lift and shift) to ensure success in how we’re storing the data to ensure we’re not pulling forward unnecessary legacy methodologies.\xa0', ""The Azure Data Engineer develops, constructs, and maintains database systems and data pipeline processing systems.\xa0They are responsible for building data acquisition processes, managing data flows through distributed systems, and storing data utilizing relational databases and flat files.\xa0They also engineer and maintain accurate reporting, data, and analytical tools relative to the organization's needs."", '2-3 years of Azure data experience (work as a Data Engineer with emphasis in Azure, Database Administrator in cloud-based environment, and/or ETL Developer with an emphasis in Azure)']",Associate,Full-time,Information Technology,Pharmaceuticals,2021-03-18 14:34:51
Data Engineer,Slalom,San Diego Metropolitan Area,1 week ago,126 applicants,"['', '·\xa0\xa0\xa0\xa0\xa0\xa0Highly self-motivated and able to work independently as well as in a team environment', '·\xa0\xa0\xa0\xa0\xa0\xa0Data wrangling of heterogeneous data and explore and discover new insights', '·\xa0\xa0\xa0\xa0\xa0\xa0Consulting experience', '·\xa0\xa0\xa0\xa0\xa0\xa0Proficient in the Linux shell, including utilities such as SSH', '·\xa0\xa0\xa0\xa0\xa0\xa0Proficient in Python and/or Java', '·\xa0\xa0\xa0\xa0\xa0\xa0Proficient in SQL', 'Responsibilities', '·\xa0\xa0\xa0\xa0\xa0\xa0Familiarity with streaming data ingestion', '·\xa0\xa0\xa0\xa0\xa0\xa0Participate in development of cloud data warehouses and business intelligence solutions', '·\xa0\xa0\xa0\xa0\xa0\xa0Familiarity with implementing analytics solutions with one or more Hadoop distributions (Cloudera, Hortonworks, MapR, HDInsight, EMR)', 'As a Data Engineer, you’ll work in small teams to deliver innovative solutions on Amazon Web Services, Azure, and Google Cloud using core cloud data warehouse tools, Hadoop, Spark, Event Stream platforms, and other Big Data related technologies. In addition to building the next generation of data platforms, you’ll be working with some of the most forward-thinking organizations in data and analytics.\xa0', 'Qualifications', '·\xa0\xa0\xa0\xa0\xa0\xa0Work as part of a team to develop Cloud Data and Analytics solutions', 'Job Title: Data Engineer', '·\xa0\xa0\xa0\xa0\xa0\xa0Strong aptitude for learning new technologies and analytics techniques', '·\xa0\xa0\xa0\xa0\xa0\xa0Understanding of agile project approaches and methodologies', 'Preferred Experience', 'Slalom is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law.', '·\xa0\xa0\xa0\xa0\xa0\xa03+ years of related work experience in Data Engineering or Data Warehousing', '·\xa0Familiarity or strong desire to learn quantitative analysis techniques (e.g., predictive modeling, machine learning, segmentation, optimization, clustering, regression)', '·\xa0\xa0\xa0\xa0\xa0\xa0Gain hands-on experience with new data platforms and programming languages (e.g. Python, Hive, Spark)', '·\xa0\xa0\xa0\xa0\xa0\xa0Hands-on experience with leading commercial Cloud platforms, including AWS, Azure, and Google', '·\xa0\xa0\xa0\xa0\xa0\xa0Proficient in a source code control system, such as Git', 'Job Title', '·\xa0\xa0\xa0\xa0\xa0\xa0Proven experience with data warehousing, data ingestion, and data profiling']",Mid-Senior level,Full-time,Information Technology,Management Consulting,2021-03-18 14:34:51
Data Engineer,"ConsumerTrack, Inc.",United States,7 days ago,117 applicants,"['', 'Preferred Qualifications:', 'Ability to operate in an agile, entrepreneurial start-up environment, and prioritize.', 'Awesome medical, dental, and vision plans with heavy employer contribution', 'Our teams are working remotely 100% for the foreseeable future and have flex time. We’re in the digital media space so we’re mobile and flexible!*Option to work from an office (if you need to get away!)Tools & resources are available to keep our team connected across North America. (JIRA, Trello, Airtable, Slack, Zoom and so much more!)To keep our community of ConsumerTrackers engaged and connected, virtual team building events are held weekly and monthly.For wellness and balance, weekly virtual fitness classes such as yoga are available.To care for the local communities that we’re a part of across the U.S our team members host socially distanced philanthropic events every quarter.And most importantly, we’ve committed to consistent and transparent communication to help us all stay informed, engaged and to keep us on our path to success and #greatness.', 'Knowledge for ETL/ELT tools like Informatica, IBM DataStage, or SaaS ETL tools is a plus.', 'To keep our community of ConsumerTrackers engaged and connected, virtual team building events are held weekly and monthly.', 'Competitive salary with excellent growth opportunity; we pride ourselves in having a team that exudes leadership, high initiative, creativity, and passion.', 'For wellness and balance, weekly virtual fitness classes such as yoga are available.', 'Strong skills to write complex, highly-optimized SQL queries across large volumes of data.', 'Build processes supporting data transformation, data structures metadata, and workload management.', 'Contribution to student loan debt payments after the first year of employment', 'Functions/Responsibilities', 'We are an equal-opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law.', 'Experience with AWS infrastructure.', 'Benefits', 'Must have excellent troubleshooting and problem-solving skills.', 'The ConsumerTrack has big growth plans ahead and is looking for a rockstar Data Engineer experienced in Data warehousing and Python to join our Data Engineering team. The CTI Data Engineering team is responsible for designing and developing the Data lake, enterprise database, data warehouse, reporting solutions, and pipelines for data processing. If you are a critical thinker with a solid track record of developing data solutions and solving complex problems with SQL and Python, we want you to join our team! You will play a vital role in designing and developing our next generation data pipelines and data platform. Join the team and prototype new data product ideas and concepts!', 'Collaborate with the team to perform root cause analysis and audit internal and external data and processes to help answer specific business questions.', 'Build and maintain multiple data pipelines to ingest new data sources (API and file-based) and support products used by both external users and internal teams.Optimize by building tools to evaluate and automatically monitor data quality, develop automated scheduling, testing, and distribution of feeds.Work with our data science and product management teams to design, rapid prototype, and productize new data product ideas and capabilities.Work with the data engineering team to migrate and enhance our existing Pentaho-based ETL pipeline to a new ELT-based/SaaS Integration system.Conquer complex problems by finding new ways to solve with simple, efficient approaches with a focus on reliability, scalability, quality, and cost of our platforms.Build processes supporting data transformation, data structures metadata, and workload management.Collaborate with the team to perform root cause analysis and audit internal and external data and processes to help answer specific business questions.', 'Build and maintain multiple data pipelines to ingest new data sources (API and file-based) and support products used by both external users and internal teams.', 'Comfortable working directly with data analytics to bridge business requirements with data engineering.', 'Experience with Tableau or other reporting tools is a plus.', 'Work with the data engineering team to migrate and enhance our existing Pentaho-based ETL pipeline to a new ELT-based/SaaS Integration system.', 'Curiosity and passion for data, visualization, and solving problems.', 'Company funding for outside classes and conferences to help you improve your skills', 'Excellent communication and teamwork, and a passion for learning.', 'Work with our data science and product management teams to design, rapid prototype, and productize new data product ideas and capabilities.', 'Experience with Redshift, Snowflake, or other MPP databases is a plus.Knowledge for ETL/ELT tools like Informatica, IBM DataStage, or SaaS ETL tools is a plus.Experience with Tableau or other reporting tools is a plus.', 'Master’s Degree (or a B.S. degree with relevant industry experience) in math, statistics, computer science, or equivalent technical field.', '3+ years of professional Dimensional Data Warehousing/Data Modeling and ‘Big Data’ Experience.', 'Here’s a peek into our world at ConsumerTrack -', 'Conquer complex problems by finding new ways to solve with simple, efficient approaches with a focus on reliability, scalability, quality, and cost of our platforms.', 'Our teams are working remotely 100% for the foreseeable future and have flex time. We’re in the digital media space so we’re mobile and flexible!', 'Optimize by building tools to evaluate and automatically monitor data quality, develop automated scheduling, testing, and distribution of feeds.', 'Competitive salary with excellent growth opportunity; we pride ourselves in having a team that exudes leadership, high initiative, creativity, and passion.Awesome medical, dental, and vision plans with heavy employer contributionPaid maternity leave and paternity leave programsPaid vacation, sick days, and holidaysCompany funding for outside classes and conferences to help you improve your skillsContribution to student loan debt payments after the first year of employment401(k) -- employees can start contributing immediately. After the first year, CTI matches your contribution up to 4% of your salary', '*Option to work from an office (if you need to get away!)', 'Tools & resources are available to keep our team connected across North America. (JIRA, Trello, Airtable, Slack, Zoom and so much more!)', '401(k) -- employees can start contributing immediately. After the first year, CTI matches your contribution up to 4% of your salary', 'Paid vacation, sick days, and holidays', 'Willingness to question the validity, accuracy of data and assumptions.', 'Paid maternity leave and paternity leave programs', 'And most importantly, we’ve committed to consistent and transparent communication to help us all stay informed, engaged and to keep us on our path to success and #greatness.', ""A note about our response to COVID -19 and our new norm: The world has changed and we know it’s important to adapt and to do our part to take care of our teams in this global pandemic. Our number one priority is to have our ConsumerTrackers feel safe, balanced and connected. We’re committed to providing our teams with the best resources and tools to navigate this new virtual world that we’re living in. We've also reinvented the ways in which we recognize, celebrate, and engage with each other to keep our culture strong!"", 'ConsumerTrack™ is unique in the digital marketing and media industry - we combine marketing, digital, content and fintech. Our performance based approach increases brand awareness and generates targeted audience engagement on our internal web properties and partner sites.', 'Basic Qualifications:', 'Master’s Degree (or a B.S. degree with relevant industry experience) in math, statistics, computer science, or equivalent technical field.3+ years of professional Dimensional Data Warehousing/Data Modeling and ‘Big Data’ Experience.Strong skills to write complex, highly-optimized SQL queries across large volumes of data.Comfortable working directly with data analytics to bridge business requirements with data engineering.Experience with AWS infrastructure.Must have excellent troubleshooting and problem-solving skills.Ability to operate in an agile, entrepreneurial start-up environment, and prioritize.Excellent communication and teamwork, and a passion for learning.Curiosity and passion for data, visualization, and solving problems.Willingness to question the validity, accuracy of data and assumptions.', 'Experience with Redshift, Snowflake, or other MPP databases is a plus.', 'To care for the local communities that we’re a part of across the U.S our team members host socially distanced philanthropic events every quarter.']",Associate,Full-time,Advertising,Internet,2021-03-18 14:34:51
Data Engineer,Hinge Health,"Denver, CO",4 weeks ago,Be among the first 25 applicants,"['', 'Lead migration of the legacy data environment to cloud data services', 'Design and build the structures necessary for the Business Intelligence team to work autonomously', 'Participate in hiring and mentoring of team members', 'Monthly wellness benefit', 'Trust', 'Create tooling to automate data compliance', 'BONUS POINTS', 'Establish data governance tools and practices in a HIPAA environment', 'Programming expertise with Python, with demonstrated knowledge of software engineering best-practice development (e.g. linting, testing)', 'Ability to gather and correlate data across disparate sources and file formats', ""What We're Looking For"", 'We celebrate diversity and are committed to creating an inclusive environment for all employees.', 'Prior DBA experience', 'Excellent communication skills, both written and verbal', 'Learn-it-all (vs know-it-all): We’re always willing to learn. ', '401K match ', 'Prior experience with healthcare data (PHI/PII/HIPAA requirements)', 'FSA & HSA accounts', 'Mastery of SQL', '3 months paid parental leave', 'Professional Development budget ', 'Expertise in Go, Java, or RubyPrior DBA experiencePrior experience with healthcare data (PHI/PII/HIPAA requirements)Prior experience working with KubernetesExtensive NoSQL knowledgeBig Data technologies', 'Family & fertility benefit through Maven Clinic', 'Create the strategy, tooling, processes, and coaching that enables service and application teams to take full ownership of their data in a growing organizationLead migration of the legacy data environment to cloud data servicesCreate scalable tools and processes to propagate OLTP data to the data warehouseParticipate in hiring and mentoring of team membersEstablish data governance tools and practices in a HIPAA environmentCreate tooling to automate data complianceDesign and build the structures necessary for the Business Intelligence team to work autonomouslyAssist and coach teams to optimize poorly performing queries or overly-complicated modelsWork with SRE team establish best practices around database monitoring, alerting and availability', ""What You'll Love About Us"", 'Generous mental health stipend', 'Trust: We trust our teammates to always act in the team and company’s best interest. Hustle: We’re creative, we’re unrelenting, we find a way.Effective communication: We’re prompt and concise. Learn-it-all (vs know-it-all): We’re always willing to learn. Frugal: We don’t waste money and especially not time. ', '6 years of software engineering experience, 4+ in data engineering', 'Big Data technologies', ""What You'll Accomplish"", 'Competitive compensation with meaningful stock optionsMedical, Dental, Vision, Disability and Life Insurance (We cover 100% of your premium and 75% for your dependents) FSA & HSA accountsFamily & fertility benefit through Maven Clinic401K match 3 months paid parental leaveProfessional Development budget Monthly wellness benefitGenerous mental health stipendNoise-cancelling headphonesWork from home policyOpportunity to join a fantastically talented, diverse, and passionate team at a pivotal time in the company’s lifecycle', 'Noise-cancelling headphones', 'Hustle', 'Ability to collaborate and problem solve across teams', 'Work from home policy', 'Creation of ETL and data pipelines using code', 'Frugal: We don’t waste money and especially not time. ', 'Extensive NoSQL knowledge', 'Competitive compensation with meaningful stock options', 'Opportunity to join a fantastically talented, diverse, and passionate team at a pivotal time in the company’s lifecycle', 'Work with SRE team establish best practices around database monitoring, alerting and availability', 'Medical, Dental, Vision, Disability and Life Insurance (We cover 100% of your premium and 75% for your dependents) ', 'Create scalable tools and processes to propagate OLTP data to the data warehouse', 'Familiarity with the command line and Docker containers', 'What Shapes Our Company', 'Effective communication: We’re prompt and concise. ', 'Effective communication', 'Trust: We trust our teammates to always act in the team and company’s best interest. ', 'OLTP and data warehouse modeling best practices', 'Prior experience working with Kubernetes', 'Frugal: ', 'Create the strategy, tooling, processes, and coaching that enables service and application teams to take full ownership of their data in a growing organization', 'Hustle: We’re creative, we’re unrelenting, we find a way.', 'Learn-it-all (vs know-it-all)', 'Expertise in Go, Java, or Ruby', 'Assist and coach teams to optimize poorly performing queries or overly-complicated models', 'Mastery of SQLOLTP and data warehouse modeling best practicesProgramming expertise with Python, with demonstrated knowledge of software engineering best-practice development (e.g. linting, testing)Creation of ETL and data pipelines using codeFamiliarity with the command line and Docker containersAbility to gather and correlate data across disparate sources and file formatsAbility to collaborate and problem solve across teamsExcellent communication skills, both written and verbal6 years of software engineering experience, 4+ in data engineeringBachelor’s degree in C.S. or equivalent experience', 'Bachelor’s degree in C.S. or equivalent experience']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,ERT,"Boston, MA",1 day ago,Be among the first 25 applicants,"['', ' Tune application and query performance using profiling tools and SQL. ', ' At least 3 years of experience in Project life cycle activities on development and maintenance projects. ', ' Experience and desire to work in a Global environment ', ' Analyze and solve problems at their root, stepping back to understand the broader context. ', ' Basic knowledge of Thinking Skills', "" Bachelor's degree in a Computer Science or related field. "", ' Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for datasets. ', ' At least 2 years of experience in AWS databases including Aurora, Dynamo DB and/or RDS Oracle. ', ' Basic knowledge of Organizational Awareness', ' Implement, and support a platform providing secured access sensitive clinical data within ERT and directly to customers. ', ' Keep up to date with advances in big data technologies and run pilots to design the data architecture to scale with the increased data volume using AWS. ', ' Basic knowledge of Project Management', 'Responsibilities', ' Ability to work in team in diverse/ multiple stakeholder environment ', ' At least 3 years experience in dimensional modelling, ETL development and/or Data Warehousing. ', ' At least 4 years of database programming experience. ', ' Model data and metadata to support ad-hoc and pre-built data analysis. ', ' Strong Analytical skills ', ' At least 3 years of experience building data pipelines.  At least 3 years experience in dimensional modelling, ETL development and/or Data Warehousing.  At least 4 years of database programming experience.  At least 3 years of experience in Python, Go and/or Java.  Knowledge of data ingestion and data cataloguing experience in AWS Kinesis (or Kafka), Glue, Athena.  At least 2 years of experience in AWS databases including Aurora, Dynamo DB and/or RDS Oracle.  Strong hands on experience in SQL.  Experience in working with different data formats - json, XML, parquet  At least 3 years of experience in software development life cycle.  At least 3 years of experience in Project life cycle activities on development and maintenance projects.  At least 3 years of experience in Design and architecture review.  Ability to work in team in diverse/ multiple stakeholder environment  Strong Analytical skills  Experience and desire to work in a Global environment ', ' Interface with operational delivery teams, gathering information and delivering complete solutions. ', ' Strong hands on experience in SQL. ', 'Qualifications', ' Experience in working with different data formats - json, XML, parquet ', ' Candidate must be located within commuting distance of Boston, MA or be willing to relocate to the area. ', ' Basic knowledge of Communication', ' Keeps current with applicable Standard Operating Procedures and associative training. ', ' An ability to work in a fast-paced environment where continuous innovation is occurring and ambiguity is the norm. ', 'Other Duties And Responsibilities', ' Basic knowledge of Interpersonal Relations', ' Recognize and adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation. ', 'Experience', ' At least 3 years of experience in software development life cycle. ', "" Bachelor's degree in a Computer Science or related field.  Candidate must be located within commuting distance of Boston, MA or be willing to relocate to the area. "", ' At least 3 years of experience in Python, Go and/or Java. ', ' Triage many possible courses of action in a high-ambiguity environment, making use of both quantitative analysis and business judgment. ', 'Overview', ' Learn and understand a broad range of Amazon AWS data resources and know when, how, and which to use and which not to use. ', ' At least 3 years of experience building data pipelines. ', ' Knowledge of data ingestion and data cataloguing experience in AWS Kinesis (or Kafka), Glue, Athena. ', ' Provide subject matter expertise and advice to data consumers surrounding ERT’s data pipelines supporting ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisions. ', ' Basic knowledge of consultative/customer focus ', ' At least 3 years of experience in Design and architecture review. ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Qwinix,"Denver, CO",2 weeks ago,Be among the first 25 applicants,[''],Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer/Analyst,Leidos,"Baltimore, MD",2 days ago,Be among the first 25 applicants,"['', ' Ability to optimize and performance tune SQL queries', 'Desired Skills', 'Clearance Level Required', ' Experience working in large-scale cloud database environments is a plus.', 'Travel', 'Required Education/Experience', 'Pay Range', ' Ability to understand complex business processes to derive conceptual and logical data models.', 'External Referral Bonus', ' Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements', ' Expertise in Conceptual Data Modeling, Logical Data Modeling, Physical Data Modeling, Enterprise Data Warehouse Design, DataMart Design, Metadata, Data Quality, Master Data Management and Master Data/Data Governance using Erwin', ' Familiar with machine learning and advanced analytic application development.', ' Ability to create dashboards in Tableau or Webfocus using various sources like database, CSV files, XL spreadsheets etc.', ' Expert in designing complex and semantically rich data structures.', ' Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements Able to work independently with BA and PM to gather requirements, perform data discovery, come up with data model, work with data engineers to build data pipeline, and work with developers to answer their questions. Extensively used ETL methodologies for supporting data extraction, transformations and loading processing, in a corporate-wide-ETL Solution. Expertise in Conceptual Data Modeling, Logical Data Modeling, Physical Data Modeling, Enterprise Data Warehouse Design, DataMart Design, Metadata, Data Quality, Master Data Management and Master Data/Data Governance using Erwin Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics. Ability to understand complex business processes to derive conceptual and logical data models. Lead complex discussions and engagements that may involve multiple project teams from client. Experience designing relational data models, analytical data models and non-relational data models for OLTP and advanced analytic applications. Expert in designing complex and semantically rich data structures. Worked with OLTP/data warehouse modeling, and play key role in multiple projects and act as knowledge expert (Technical, Functional). Ability to optimize and performance tune SQL queries Good data analysis, problem solving and SQL skills.', 'Job Description:', ' Worked with OLTP/data warehouse modeling, and play key role in multiple projects and act as knowledge expert (Technical, Functional).', 'Description', ' Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.', ' Experience in PostGreSQL / Greenplum database is good to have', ' Able to work independently with BA and PM to gather requirements, perform data discovery, come up with data model, work with data engineers to build data pipeline, and work with developers to answer their questions.', ' Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards', ' Lead complex discussions and engagements that may involve multiple project teams from client.', ' Good data analysis, problem solving and SQL skills.', ' Expertise on NoSQL data modeling using Hackolade tool for databases like MongoDB, HBASE, Hive and JSON Schema.', ' Expertise on NoSQL data modeling using Hackolade tool for databases like MongoDB, HBASE, Hive and JSON Schema. Familiar with machine learning and advanced analytic application development. Experience working in large-scale cloud database environments is a plus. Ability to create dashboards in Tableau or Webfocus using various sources like database, CSV files, XL spreadsheets etc. Experience in PostGreSQL / Greenplum database is good to have Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards Excellent communication skills and ability to convey complex topics through effective documentation as well as presentation.', ' Experience designing relational data models, analytical data models and non-relational data models for OLTP and advanced analytic applications.', 'Potential For Telework', ' Extensively used ETL methodologies for supporting data extraction, transformations and loading processing, in a corporate-wide-ETL Solution.', ' Excellent communication skills and ability to convey complex topics through effective documentation as well as presentation.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Associate Data Engineer,SalesPage Technologies,"Kalamazoo, MI",3 days ago,89 applicants,"['', 'Innovating on new data processing techniques to improve accuracy, including data science techniques.', 'SalesPage provides solutions for asset managers focusing on data collection, management, aggregation, and analysis as well as industry-specific CRM capability.\xa0SalesPage functions as a hub within our client’s data architecture, serving as a center point where data is collected and distributed to other systems, such as BI platforms and Salesforce.\xa0We help asset managers enter the world of “big data” to find the best way to connect their investment products to those who will benefit from them most.', 'For over 30 years, SalesPage has been working with some of the largest and most respected asset management firms in the nation to solve business challenges with industry-leading software solutions.\xa0We\xa0are headquartered in\xa0downtown Kalamazoo, Michigan,\xa0in the Foundry Building, a state-of-the-art facility for tech companies. We’re looking for intelligent and passionate individuals to join our team and help continue our advancement towards being the most respected and in-demand distribution management partner in the industry.\xa0\xa0\xa0', '0-5 years of experience in work related to the position.', 'Familiarity using a source control tool such as Subversion or Git.', 'SalesPage provides opportunity for developers to expand and explore other skillsets as part of a client-focused team.\xa0Developers at SalesPage work as part of that team and are given a lot of independence and responsibility to fulfill their role in the team.\xa0SalesPage is an expanding company that is constantly innovating with new techniques and technologies to provide the best capability for our clients, and developers at SalesPage are responsible for that innovation.\xa0New employees will contribute to that growth and innovation.', 'Optimizing performance of existing batch processing jobs.', 'We are interested in every qualified candidate who is eligible to work in the United States. However, we are not able to sponsor visas.\xa0', 'Experience with Java or Groovy.', 'Full SDLC for data processing jobs, including ETL, on-demand volume processing, and analytics.Using and supporting technology used for implementing data processing jobs.Optimizing performance of existing batch processing jobs.Innovating on new data processing techniques to improve accuracy, including data science techniques.', 'Using and supporting technology used for implementing data processing jobs.', 'Associate Data Engineer', 'Bachelor’s degree in computer science, data science, data analytics, or a related field.0-5 years of experience in work related to the position.Familiarity with relational databases, including SQL and use of database objects such as views, stored procedures, and functions.Experience working and processing large datasets, and techniques for working with data in volume effectively.Experience with Java or Groovy.Familiarity using a source control tool such as Subversion or Git.', 'The ideal Data Engineer candidate will have the following qualifications:', 'Bachelor’s degree in computer science, data science, data analytics, or a related field.', 'Associate Data Engineers at SalesPage are responsible for:', 'We use a Java EE web-based solution to provide an application for users to manage and view data, and many Java-based data processing, data analysis, and ETL jobs. We provide both software for our clients and a SaaS solution in the AWS cloud.\xa0All developers at SalesPage are full-stack developers, familiar with web application capabilities and development, implementing business logic around client data, using a relational database, and mass data processing techniques.\xa0Data Engineers are focused on the mass data processing and relational database usage components of the stack. This is a junior-level position.', '\xa0', 'We are interested in every qualified candidate who is eligible to work in the United States. However, we are not able to sponsor visas.', 'Full SDLC for data processing jobs, including ETL, on-demand volume processing, and analytics.', 'Familiarity with relational databases, including SQL and use of database objects such as views, stored procedures, and functions.', 'About SalesPage', 'Experience working and processing large datasets, and techniques for working with data in volume effectively.']",Associate,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Engineer - Austin,Literati,"Austin, TX",2 weeks ago,Be among the first 25 applicants,"['', 'Coaching and teaching others how to do great data engineering', 'Innovating on ways to build automated pipelines and transformations to keep pace with the businesses growth', 'Working collaboratively with analytics team to build a self-service analytics infrastructure', 'Being part of the team, doing what’s needed for the company to succeed', 'Reworking existing data structures into more logical and scalable forms', 'Building data objects, procedures, and data streams in Snowflake ', 'Qualifications', 'Building new tables, schemas in the data warehouse to increase consistency and accuracy of the businesses reporting and analysis', 'Solid ability to design an enterprise data warehouse model, expert knowledge a strong plus', 'Expert knowledge in SQL, and in python and/or javascript', 'Expert knowledge in SQL, and in python and/or javascriptExtensive knowledge in Snowflake and FiveTranSolid ability to design an enterprise data warehouse model, expert knowledge a strong plusPassion towards making others better', 'You’re Good At', 'WE’RE GOOD AT:', 'Data Engineer', 'Building data pipelines from various internal and external sources using FiveTran and custom python ', 'You Will Need', 'Working collaboratively with analytics team to build a self-service analytics infrastructureBuilding new tables, schemas in the data warehouse to increase consistency and accuracy of the businesses reporting and analysisBuilding new pipelines to bulk up the data lake for greater analyticsInnovating on ways to build automated pipelines and transformations to keep pace with the businesses growth', 'Your Day-to-day Will Look Like', 'Job Description', 'Architecting great data structures in various forms (3NF, Dimensional, semi-structured)', 'Building data pipelines from various internal and external sources using FiveTran and custom python Architecting great data structures in various forms (3NF, Dimensional, semi-structured)Reworking existing data structures into more logical and scalable formsBuilding data objects, procedures, and data streams in Snowflake Coaching and teaching others how to do great data engineeringBeing part of the team, doing what’s needed for the company to succeed', 'Extensive knowledge in Snowflake and FiveTran', 'Building new pipelines to bulk up the data lake for greater analytics', 'Passion towards making others better']",Entry level,Full-time,Information Technology,Design,2021-03-18 14:34:51
Data Engineer,Kite,"San Francisco, CA",2 weeks ago,179 applicants,"['', 'Minimum of 3+ years of engineering experience', 'Become an engineering leader as the team grows, if desired', 'Strong ownership instinct and ability to deliver results', 'Write data transformation logic to scrub and join large datasets, and handle changes in source data', 'Experience building data pipelines, stream processing, and/or big-data messaging systems', ""Kite is well-funded by top investors in Silicon Valley, including the founders of PayPal, Stripe, Palantir, and Dropbox to name a few. We are looking to expand our 16-person startup with talented individuals who are interested in joining an early stage startup. The ideal candidate is excited to help guide the direction of our product and company. They will have a significant amount of ownership of critical technical components. Our team is growing rapidly and we hope you'll grow with us too!"", 'Experience building reliable and scalable systems', 'Experience building data pipelines, stream processing, and/or big-data messaging systemsBuilt ETL processes for large, disconnected, and/or un-structured datasetsTrack record of working in small teams to build new software from scratchAbility to learn and evaluate new technologies quicklyStrong ownership instinct and ability to deliver resultsPublic cloud (AWS, Azure, etc.) automation', 'Ability to learn and evaluate new technologies quickly', 'Turn technical requirements into a plan to execute, test, and deliver on time', 'Bachelor’s Degree in Computer Science or related field', ""Programmers spend too much time doing repetitive work — copying and pasting from StackOverflow, fixing simple errors, and writing boilerplate code. We're building an AI code engine that does this work for you. Programming using Kite is faster and more fun."", 'Ability to work daily from our San Francisco office (wonderful office in the Union Square area) when it is safe to do so', 'Who You Are', 'Excited to work on a small, growing team in a startup environment', 'Maintain and iteratively improve the data pipeline infrastructureWrite data transformation logic to scrub and join large datasets, and handle changes in source dataTurn technical requirements into a plan to execute, test, and deliver on timeBecome an engineering leader as the team grows, if desiredWrite clean, maintainable codeDesign and code reviews', 'Write clean, maintainable code', 'Built ETL processes for large, disconnected, and/or un-structured datasets', 'Track record of working in small teams to build new software from scratch', 'Maintain and iteratively improve the data pipeline infrastructure', ""What You've Done"", 'As a Data Engineer you will work on the data pipeline infrastructure and ETL logic at the core of our product. You will be a key part of our machine learning team by providing easy, fast, flexible and reliable access to training data.', ""What You'll Do"", 'Design and code reviews', 'Public cloud (AWS, Azure, etc.) automation', 'Bachelor’s Degree in Computer Science or related fieldMinimum of 3+ years of engineering experienceExperience building reliable and scalable systemsExcited to work on a small, growing team in a startup environmentAbility to work daily from our San Francisco office (wonderful office in the Union Square area) when it is safe to do so']",Associate,Full-time,Engineering,Computer Software,2021-03-18 14:34:51
Data Engineer,Zennify,"Sacramento, CA",1 week ago,130 applicants,"['', 'Always learning; approaches each interaction with open mind; great listener and hands-on', 'Support the successful delivery of the proposed technical solution, including data migration/integration scripts used in one time migrations, performing migrations, ongoing integrations, etc.', 'Primary Responsibilities', 'Lead and mentor the data team on the project', 'Work closely with Delivery Managers, Functional Solution Architects, Technical Architects and clients in order support the technology solution architecture through applying data architecture principlesExamine and identify database structural necessities by evaluating client operations, applications, and programmingConduct data requirements working sessions focused on data object mapping, data transformation and planning directly with the functional architect, technical architect, and client stakeholdersCollaborate with functional and technical architect(s) to help finalize the Salesforce data model necessary for successful project deliveryServe as a trusted advisor to the client and recommend solutions to improve new and existing database systemsBuilding analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition.Design and implement data migration and integration solutions and data models to store and retrieve dataSupport the successful delivery of the proposed technical solution, including data migration/integration scripts used in one time migrations, performing migrations, ongoing integrations, etc.Oversee the migration of data from legacy systems to new solutionsAssess database implementation procedures to ensure they comply with internal and external regulationsPrepare accurate database design and architectural design documentationMonitor the system performance by performing regular tests, troubleshooting and integrating new featuresPerform and/or lead necessary unit testing on all developed data scriptsWork directly with QA team to review UAT test cases in an effort to ensure accurate data is migrated and available for testingPerform and/or lead project data consultants in dry run data load testingDevelopment and refinement of production deployment documentation related to data migrations and integrationsLead and mentor the data team on the projectEnforce sound data migration and integration best practices in order to ensure the quality of deliverables and scalabilityIdentify and lead internal strategic initiatives to grow the Data Integration services offering; play an active role developing the knowledgebase and expertise of the Data teamOwn and drive data architecture solutions, technology and web flowsEducate staff members through training and individual supportMentor other ETL/Data team members and maintain best practices are followed within the teamOffer support by responding to system problems in a timely manner', 'Ability to move fast and drive business value and results', 'Prepare accurate database design and architectural design documentation', ""Bachelor’s degree in Computer Science, Information Systems,\xa0 or relevant field strongly preferred8+ years experience in developing technology solutions5+ years experience in managing external client projects from a data solution perspectiveA data engineer certification, e.g. Google’s Certified Professional Data Engineer, IBM Certified Data Engineer, CCP Data Engineer for Cloudera, MCSE/MCSE: Data Management and Analytics, is a plus.Ability to reverse engineer existing data models into a conceptual, logical data model constructsStrong conceptual and logical data modeling skillsPossesses strong knowledge of data modeling principles and best practicesExpected to be able use various data modeling tools and processes as requiredOpen minded to collaborate with various team members and able to give and handle constructive feedbackSpecializes in gathering and analyzing information and designing comprehensive solutions that are flexible and adaptable to the client's needsAbility to develop extensible and flexible data models based on long term strategy, and articulate benefit of reuseAble to design integrated data model based on understanding of business, and use of abstract conceptsExperience overseeing team membersHighly detail-oriented individual with the ability to rapidly learn and take advantage of new concepts, business models, and technologiesAbility to work independently and be a self-starterCutting edge innovator who continually studies new technologies and functionality, and is involved in projects that push the capabilities of existing technologies"", 'Monitor the system performance by performing regular tests, troubleshooting and integrating new features', ""Specializes in gathering and analyzing information and designing comprehensive solutions that are flexible and adaptable to the client's needs"", 'Experience overseeing team members', 'Serve as a trusted advisor to the client and recommend solutions to improve new and existing database systems', 'Embodies Zennify culture; a team player that everyone enjoys working with', 'Leadership Skills:', 'Conduct data requirements working sessions focused on data object mapping, data transformation and planning directly with the functional architect, technical architect, and client stakeholders', 'Significant Java development experience', 'Strong aptitude for prioritization and multitasking in a deadline-driven environment', '5+ years experience in managing external client projects from a data solution perspective', 'Ability to develop extensible and flexible data models based on long term strategy, and articulate benefit of reuse', 'Cutting edge innovator who continually studies new technologies and functionality, and is involved in projects that push the capabilities of existing technologies', '8+ years experience in developing technology solutions', 'These technologies include but are not limited to: Salesforce.com products and APIs, the Salesforce data model, Amazon Web Services (AWS), Heroku, integration/ETL technologies. You will also maintain an ongoing comprehensive understanding of data migration/integration tools, patterns and best practices.', 'Analytical and able to logically and methodically work through problems', 'Service-oriented and innately driven to produce outstanding customer satisfaction and results', 'Ability to meet travel requirements, when applicable (up to 25% travel projected)', 'Assess database implementation procedures to ensure they comply with internal and external regulations', 'Development and refinement of production deployment documentation related to data migrations and integrations', 'Experience using JIRA or similar software to manage user stories and workload', 'Ability to reverse engineer existing data models into a conceptual, logical data model constructs', 'Oversee the migration of data from legacy systems to new solutions', 'Perform and/or lead project data consultants in dry run data load testing', 'Enjoys discovering, learning about and implementing new technologies', 'Able to design integrated data model based on understanding of business, and use of abstract concepts', 'Self-aware and strategic thinker; proficient at building strong relationships', 'To succeed in this role, you should know how to examine new data system requirements and implement migration models. The ideal candidate will also have proven experience in data analysis and management, with excellent analytical and problem-solving abilities.', 'The Data Engineer is responsible for, but not limited to the following: data modeling, data migration and data integration, ETL tooling, data best practices, data movement, building and optimizing “big data” data pipelines and architectures, and the building of end to end data solutions for our customers.\xa0 Your duties may include preparing architect reports, monitoring the system, supervising system migrations, and performing root cause analysis on external and internal processes and data to identify opportunities for improvement.\xa0', 'Qualities of the Ideal Candidate', 'Confidently and effectively facilitates and presents; ably defends point-of-view; keeps audiences engaged and delivers a clear and memorable message', 'Expected to be able use various data modeling tools and processes as required', 'Zennify is looking for a qualified candidate to join their team as a Data Engineer! The Data Engineer\xa0 will work directly with our customers to not only enable them to be successful but to also help guide them through high quality data driven practices and project scope.\xa0 You will work with them and our delivery teams to develop, optimize and oversee our client’s conceptual and logical data systems.\xa0', 'Highly detail-oriented individual with the ability to rapidly learn and take advantage of new concepts, business models, and technologies', 'Significant experience with SOAP & REST API Integration, data, and security', 'Passionate about Customer Success', 'Possess a sense of urgency with strong organizational and follow-up skills\xa0', 'Significant experience with Git and standard branching strategies', 'Significant experience with writing unit tests', 'Enforce sound data migration and integration best practices in order to ensure the quality of deliverables and scalability', 'Own and drive data architecture solutions, technology and web flows', 'Work directly with QA team to review UAT test cases in an effort to ensure accurate data is migrated and available for testing', 'Data Engineer', 'Perform and/or lead necessary unit testing on all developed data scripts', 'Experience establishing data pipelines for large data sets.', 'Preferred Qualifications', 'Proficient at collaboration and working with members of a team', 'Work closely with Delivery Managers, Functional Solution Architects, Technical Architects and clients in order support the technology solution architecture through applying data architecture principles', 'Collaborate with functional and technical architect(s) to help finalize the Salesforce data model necessary for successful project delivery', 'Leads internal initiatives; actively contributes to the Community’s knowledge and resource base', 'Educate staff members through training and individual support', 'Understanding translation between logical data structures & physical database objects', 'Bachelor’s degree in Computer Science, Information Systems,\xa0 or relevant field strongly preferred', 'Design and implement data migration and integration solutions and data models to store and retrieve data', 'Hands-on experience with data architecture tools, such as Talend, Jitterbit, Capstorm, Informatica; or others such as Hyperion, Erwin, SQL Server; etc. are helpful\xa0', 'Has experience with and is currently willing to be hands on contributing developer in addition to leadership responsibilities', 'Mentor other ETL/Data team members and maintain best practices are followed within the team', 'Ability to work independently and be a self-starter', 'Passionate about Customer SuccessAlways learning; approaches each interaction with open mind; great listener and hands-onSelf-aware and strategic thinker; proficient at building strong relationshipsSpeaks and writes with clarity, brevity, and purpose; explains area of expertise clearly and confidently to others; influences and engages C-Level with authority and confidenceConfidently and effectively facilitates and presents; ably defends point-of-view; keeps audiences engaged and delivers a clear and memorable messageProficient at collaboration and working with members of a teamAbility to move fast and drive business value and resultsEmbodies Zennify culture; a team player that everyone enjoys working withLives the company’s core values; shows integrity, transparency, and reliabilityLeads internal initiatives; actively contributes to the Community’s knowledge and resource base', 'Technical Requirements', 'Open minded to collaborate with various team members and able to give and handle constructive feedback', 'Thrives in a team-based, high energy and fast-paced environmentService-oriented and innately driven to produce outstanding customer satisfaction and resultsEnjoys discovering, learning about and implementing new technologiesAnalytical and able to logically and methodically work through problemsStrong aptitude for prioritization and multitasking in a deadline-driven environmentPossess a sense of urgency with strong organizational and follow-up skills\xa0Ability to meet travel requirements, when applicable (up to 25% travel projected)', 'A data engineer certification, e.g. Google’s Certified Professional Data Engineer, IBM Certified Data Engineer, CCP Data Engineer for Cloudera, MCSE/MCSE: Data Management and Analytics, is a plus.', 'Thrives in a team-based, high energy and fast-paced environment', 'Examine and identify database structural necessities by evaluating client operations, applications, and programming', 'Experience with migrating data into Salesforce and understanding the Salesforce data model', 'Strong conceptual and logical data modeling skills', 'Possesses strong knowledge of data modeling principles and best practices', 'Hands-on experience with data architecture tools, such as Talend, Jitterbit, Capstorm, Informatica; or others such as Hyperion, Erwin, SQL Server; etc. are helpful\xa0Understanding translation between logical data structures & physical database objectsExperience with migrating data into Salesforce and understanding the Salesforce data modelExperience establishing data pipelines for large data sets.Significant Java development experienceSignificant experience with SOAP & REST API Integration, data, and securitySignificant experience with writing unit testsSignificant experience with Git and standard branching strategiesExperience using JIRA or similar software to manage user stories and workloadHas experience with and is currently willing to be hands on contributing developer in addition to leadership responsibilities', 'Speaks and writes with clarity, brevity, and purpose; explains area of expertise clearly and confidently to others; influences and engages C-Level with authority and confidence', 'Building analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition.', 'Offer support by responding to system problems in a timely manner', 'Identify and lead internal strategic initiatives to grow the Data Integration services offering; play an active role developing the knowledgebase and expertise of the Data team', 'Lives the company’s core values; shows integrity, transparency, and reliability']",Mid-Senior level,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Engineer,Martin,United States,1 week ago,Over 200 applicants,"['', 'Balance quality with shipping quicklyHigh level of autonomyEveryone’s opinion mattersFlexible work schedule', 'Participation in Employee Stock Option PlanCareer progression opportunitiesHealth Insurance401(k) Plan\xa04 weeks per year paid time offCPD encouraged', 'As a small engineering team, we’re looking for someone who with broad experience across the data engineering space that can be an asset in many areas. Key skills include:', 'Support business operations with ad hoc analysis.', 'High level of autonomy', 'About Us:\xa0', 'Flexible work schedule', 'Design, deploy, and maintain complex application software for audience measurement and analysis.', 'AdTech experience preferred, either demand or supply side', 'Responsibilities', 'Career progression opportunities', 'Balance quality with shipping quickly', 'Excellent communication skills', 'Health Insurance', 'We work with brands and agency partners to increase return on ad spend.', 'Requirements', 'Write services to expose Martin data facts via internal services.', 'We are looking for a data engineer with 3+ years experience, to work within Martin’s data management platform. The successful candidate will play a key role in driving operational simplicity and automation to improve the efficiency and functionality of Martin’s core offerings.', '401(k) Plan\xa0', 'Familiarity with JVM stack for data analysis and management, including Apache Spark and Airflow', 'Comfort with tools for building REST services to make data services available for both end users and internal users (preferably Spring experience).\xa0', 'Develop and Manage ETL pipelines for critical Martin Real Time Bidding processes.', 'Familiarity with JVM stack for data analysis and management, including Apache Spark and AirflowComfort with tools for building REST services to make data services available for both end users and internal users (preferably Spring experience).\xa0AdTech experience preferred, either demand or supply sideAble to join and contribute to the Agile development process.Excellent communication skills', 'Scope and modify infrastructure resources as needed.', 'Design, deploy, and maintain complex application software for audience measurement and analysis.Develop and Manage ETL pipelines for critical Martin Real Time Bidding processes.Write services to expose Martin data facts via internal services.Support business operations with ad hoc analysis.Scope and modify infrastructure resources as needed.', 'Everyone’s opinion matters', '4 weeks per year paid time off', 'We Offer:', 'CPD encouraged', 'Martin is a high growth ad-tech startup focused on Real Time Bidding applications. We are a strategic technology partner to sophisticated marketers who want to maximize the use of first party data in media buying and measure the true impact on sales.', 'Able to join and contribute to the Agile development process.', 'How we operate:\xa0', 'Participation in Employee Stock Option Plan']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
AWS Data Engineer,Robert Half,"Philadelphia, PA",,N/A,"['', ' Ability to foresee scalability, budget issues and make judgements', ' Excellent data modelling experience and understanding of core concepts of Big Data and has practical experience working with 100s of Terabytes to Petabytes of data', ' Experience working with CICD pipelines', 'Strong applicants will have experience with:', ' Nice to have experience in infrastructure automation using Infrastructure as Code methodology and well versed with CloudFormation or Terraform', ' Experience in developing distributed applications and dynamic workflows using serverless architecture', ' Quick Learner and Self-starter with excellent debugging skills', ' Deep understand AWS Infrastructure, networking best practices, multi-region latency and fail over/disaster recovery strategy', ' Analytical skills and experience in solving big data problems effectively using Spark/EMR', ' Effective Collaborator and Team Player Quick Learner and Self-starter with excellent debugging skills Ability to foresee scalability, budget issues and make judgements', 'Requirements', 'Description', ' Solid experience and understanding of various core AWS services such as EC2, S3, EMR/Spark, Glue, SQS, Kinesis/Firehose, DynamoDB/DAX, Step functions, ElasticSearch, Athena and Redshift.', 'Looking For Someone To Have', ' Has practical experience transferring big data in and out of AWS, transforming and computing in AWS at scale', ' Real time data streaming, batching and transformations on data in transit', ' Real time data streaming, batching and transformations on data in transit Nice to have experience in infrastructure automation using Infrastructure as Code methodology and well versed with CloudFormation or Terraform', ' Hands on programming experience using AWS SDK (Boto3), Java SDK or CLI', ' Experience with public cloud data sharing practices and security', ' Effective Collaborator and Team Player', ' Expertise with Python, PySpark, Scala, PyData, DataFrames, Jupyter Notebook and understands the fundamentals of functional programming language. Excellent data modelling experience and understanding of core concepts of Big Data and has practical experience working with 100s of Terabytes to Petabytes of data Solid experience and understanding of various core AWS services such as EC2, S3, EMR/Spark, Glue, SQS, Kinesis/Firehose, DynamoDB/DAX, Step functions, ElasticSearch, Athena and Redshift. Hands on programming experience using AWS SDK (Boto3), Java SDK or CLI Deep understand AWS Infrastructure, networking best practices, multi-region latency and fail over/disaster recovery strategy Has practical experience transferring big data in and out of AWS, transforming and computing in AWS at scale Experience in developing distributed applications and dynamic workflows using serverless architecture Analytical skills and experience in solving big data problems effectively using Spark/EMR Experience with public cloud data sharing practices and security Experience working with CICD pipelines', 'Non-Technical Experience', ' Expertise with Python, PySpark, Scala, PyData, DataFrames, Jupyter Notebook and understands the fundamentals of functional programming language.']",Entry level,Temporary,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Prospect 33,"Philadelphia, PA",1 day ago,31 applicants,"['', 'What you will do:', 'Experience consuming bespoke data sets with no control over the format of the data', 'Drive the data practice to continuously become more efficient', 'Remote work until the end of the virus then location in central Philidelphia.', 'Proficiency in a diverse set of programming languages, such as C# (.NET) and Python', 'Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community', 'Working knowledge of Docker and Kubernetes', 'Define the roadmap and drive execution', 'Must have strong coding skills and be capable of creating a new pipeline based on a new data source quickly (max 1-2 days).', 'Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and iterative delivery environment? Most importantly, do you love experimenting with new technologies and increasing your knowledge of the data space?', 'Integration of alternative data with traditional data sources', 'Ability to mentor junior data engineers', 'Demonstrated experience in large dataset design and modeling', 'Demonstrated expertise in data engineering, including but not limited to data platforms, ETL process engineering, data quality monitoring, and machine-assisted data discovery', 'Quickly build new pipelines for new data sources', 'Effectively manage expectations', 'Lead a small team responsible for providing best in class data services to multiple internal stakeholders including Investment and Quantitative Research, Investment Analysis, and Technology', 'Experience working with Redshift and SQL Server', 'Experience consuming bespoke data sets with no control over the format of the dataExcellent knowledge of tooling in the data space and the ability to hone in on the right stack for the problemDemonstrated experience in large dataset design and modelingDemonstrated expertise in data engineering, including but not limited to data platforms, ETL process engineering, data quality monitoring, and machine-assisted data discoveryFamiliarity with NoSQL technologies, such as key/value store, columnar store, document store, etc.Well-versed in the features of popular Big Data solutions, including cloud-hosted platformsProficiency in a diverse set of programming languages, such as C# (.NET) and PythonExperience working with Redshift and SQL ServerWorking knowledge of Docker and KubernetesEffectively manage expectationsAbility to mentor junior data engineersMinimum Bachelor’s degree in Computer Science or Computer EngineeringMust have strong coding skills and be capable of creating a new pipeline based on a new data source quickly (max 1-2 days).', 'Build large-scale batch and real-time data pipelines for alternative data', 'Minimum Bachelor’s degree in Computer Science or Computer Engineering', 'Familiarity with NoSQL technologies, such as key/value store, columnar store, document store, etc.', 'Qualifications/Requirements', 'Excellent knowledge of tooling in the data space and the ability to hone in on the right stack for the problem', 'Well-versed in the features of popular Big Data solutions, including cloud-hosted platforms', 'Lead a small team responsible for providing best in class data services to multiple internal stakeholders including Investment and Quantitative Research, Investment Analysis, and TechnologyBuild large-scale batch and real-time data pipelines for alternative dataQuickly build new pipelines for new data sourcesIntegration of alternative data with traditional data sourcesDefine the roadmap and drive executionDrive the data practice to continuously become more efficientShare your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community', 'We are looking for a seasoned data engineer who can work with a leading hedge fund in Philadelphia. They are constantly receiving new data sources that need to be integrated and fed to the appropriate lines of business as soon as possible. The team is expected to turn around a new pipeline for a new source within 1-2 days and needs a leader who can help not only with the low-level work but one who can make the process more efficient.']",Mid-Senior level,Full-time,Information Technology,Investment Banking,2021-03-18 14:34:51
Data Engineer Contractor - Dublin,Stafford Technology,"Dublin, OH",,N/A,[''],Associate,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,DPR Construction,"Edison, NJ",2 days ago,Be among the first 25 applicants,"['', 'Secure the movement of sensitive information in a manner consistent with company policy and management expectations', 'Control integration quality and develop ways to detect and correct anomalies with data exchange', ' DPR has been nationally recognized for its strong company culture, based on a well-defined purpose “We Exist to Build Great Things,” and four core values: integrity, enjoyment, uniqueness and ever forward. A flat, title-less organization that empowers people at all levels to make decisions, DPR ranked on FORTUNE’s “100 Best Companies to Work For” list for five consecutive years. For more information, visit http://www.dpr.com .', 'Create and maintain optimal data pipeline architecture', 'Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.', 'Enable data access, data processing, and data products by architecting, maintaining, scaling, monitoring and securing Data Warehouse, EL & ETL system, and data pipelines and BI systems', 'Ability to understand, consume and use API’s, JSON, Webservices for Data pipelines.', 'Experience in Software development.', 'Assemble large, complex data sets that meet functional / non-functional business requirements.', ' to success in this role', 'Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.', 'Define and lead API integration strategies and for the enterprise', 'Identify, design, and implement internal process improvements, automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.', 'Ability to work effectively with others who are in remote locations and varying time zones', 'Motivation to continually learn and take on added responsibilities while maintaining a positive attitude', 'Work with data and analytics experts to strive for greater functionality in our data systems.', 'Seek and Embrace Change – Continuously improve work processes rather than accepting the status quo', 'Position Summary', 'Responsibilities', 'Strong with SQL development knowledge for Relational Databases', 'Growth and Development – Know or learn what is needed to deliver results and successfully compete', 'Implement enterprise integrations that result in a scalable, flexible, and highly available solutions that perform well under high traffic', 'Excellent knowledge of EL and ELT, Datawarehousing, and cloud-based tools', 'Business and Technical Analysis skills', 'Qualifications', 'Create and maintain optimal data pipeline architectureAssemble large, complex data sets that meet functional / non-functional business requirements.Enable data access, data processing, and data products by architecting, maintaining, scaling, monitoring and securing Data Warehouse, EL & ETL system, and data pipelines and BI systemsIdentify, design, and implement internal process improvements, automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.Keep our data separated and secureCreate data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.Work with data and analytics experts to strive for greater functionality in our data systems.Define and lead API integration strategies and for the enterpriseImplement enterprise integrations that result in a scalable, flexible, and highly available solutions that perform well under high trafficSecure the movement of sensitive information in a manner consistent with company policy and management expectationsControl integration quality and develop ways to detect and correct anomalies with data exchange', 'Resourceful creative approach to problem-solving is expected', 'Key', 'Strong communication skills, with the ability to work both independently and in project teams', 'Keep our data separated and secure', 'Ability to adapt quickly to change & deep curiosity to learn new tools and technologies and apply them', 'Experience in scripting languages like Batch, Shell in Unix environment', 'Solid understanding of database engineering and design (Relational, De-normalized, Data Lakes, etc.)Knowledge of AWS and Azure platformsExperience in Software development.Ability to understand, consume and use API’s, JSON, Webservices for Data pipelines.Excellent knowledge of EL and ELT, Datawarehousing, and cloud-based toolsStrong with SQL development knowledge for Relational DatabasesBusiness and Technical Analysis skillsExperience in scripting languages like Batch, Shell in Unix environmentExperience with integration of data from multiple data sources like API’s, JSON and any other databases, Flat-files, Spreadsheets.Experience in Data Mapping, XML/JSON, and web service', 'Experience with integration of data from multiple data sources like API’s, JSON and any other databases, Flat-files, Spreadsheets.', 'Experience in Data Mapping, XML/JSON, and web service', 'Solid understanding of database engineering and design (Relational, De-normalized, Data Lakes, etc.)', 'Knowledge of AWS and Azure platforms', 'Job Description', 'Ability to adapt quickly to change & deep curiosity to learn new tools and technologies and apply themAbility to work with and collaborate across the team and work effectively with others to identify the impact on the company’s business processes, other applications, network, etc.Strong analytical and problem-solving abilities.Seek and Embrace Change – Continuously improve work processes rather than accepting the status quoGrowth and Development – Know or learn what is needed to deliver results and successfully competeAbility to work effectively with others who are in remote locations and varying time zonesResourceful creative approach to problem-solving is expectedStrong communication skills, with the ability to work both independently and in project teamsMotivation to continually learn and take on added responsibilities while maintaining a positive attitude', 'Ability to work with and collaborate across the team and work effectively with others to identify the impact on the company’s business processes, other applications, network, etc.', 'Strong analytical and problem-solving abilities.', 'Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.', 'Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.']",Entry level,Full-time,Information Technology,Construction,2021-03-18 14:34:51
Data Engineer,Talentpair,"New York, NY",,N/A,"['', 'Once you apply, we’ll send you the full job spec, all company information and a highlight of their leadership team.', 'Our CONFIDENTIAL, pre-IPO, venture-backed, growth client is hiring a Data Engineer who will tackle hard challenges everyday! Using a mix of the latest-and-greatest tech as well as proven tools to pioneer the use of data discovery, pipelining, extraction, importation, sanitization, and visualization of massive datasets to an industry that is eager to utilize an Enterprise SaaS solution in their daily workflow. You will take on tough problems in machine learning and scalable data processing using technologies like Scala, Spark, Postgres, ElasticSearch and Docker.']",Mid-Senior level,Full-time,Information Technology,Commercial Real Estate,2021-03-18 14:34:51
Data Architect/Engineer,Health IQ,United States,3 weeks ago,76 applicants,"['', '3. Casual Office Attire ', 'Health IQ believes that everyone can make an impact, and we are proud to be an equal opportunity employer committed to providing employment opportunity regardless of sex, race, creed, color, gender, religion, marital status, domestic partner status, age, national origin or ancestry, physical or mental disability, medical condition, sexual orientation, pregnancy, military or veteran status, citizenship status, and genetic information. If you require an accommodation to complete the application or the interview process, please contact talent@healthiq.com.\xa0', '1. Nutritionally Supportive Environment ', 'Data mining and machine learning', 'What you will be doing:', 'Build data pipelines and fix performance bottlenecks to visualize data real-time', 'Workflow management, orchestration and batch processing: Apache Airflow, AWS Step Functions, AWS Batch', 'Instead of the usual ping pong table, we’ve dedicated space for our employees to enjoy yoga, spin bikes, exercise equipment, and other wellness activities. We believe a healthy body is at the core of a healthy mind, so whether it’s time for a walk, using the exercise equipment, or daily meditation, we make it easy.\xa0', 'Health IQ is adding a Data Architect/Engineer to its growing platform engineering team. As a Data Architect/Engineer, you will work with clients, team members, department heads and 3rd party data providers, to develop, maintain, and enhance our data engineering capabilities in support of our data and predictive analytic offerings to the insurance marketplace.\xa0The ideal candidate demonstrates a curious analytical mind with the ability to understand business objectives, ask insightful questions, and be detail-oriented in implementation.', ""Health IQ is the internet's fastest growing online Insurance Company in the US. In the last few years, we’ve gone from 0 to $24B in coverage, 0 to 230 employees, 0 to $139MM in venture capital raised. Why is our product selling so fast? We have a data advantage. Health IQ spent 6+ years gathering the science and the proprietary data from our popular Health IQ test (taken 10.2 million times) to convince insurance carriers to give lower rates on life insurance for vegans, marathoners, triathletes, well-controlled diabetics, yogis, Crossfitters, and more. These special rates are exclusive to us saving consumers thousands of dollars each and rewarding them for living a healthy lifestyle.\xa0"", 'ELT/ETL/ETLT technologies like DBT, Fivetran, ETLeap', 'Set up of real-time data pipelines and associated tools: Hadoop, Spark, Kafka', 'Cluster management: JuypterHub, Apache Zeppelin, Docker, Kubernetes, Linux, CICD', ""Ever dream of coming to work in your casual fitness attire? Well, that's how we roll at Health IQ! Be comfortable and let your fitness fashion shine!\xa0"", 'Maintain and refactor existing schema to maximize data usability and consistency across different business functions', 'What we’re looking for:', ""We pay 100% of our employees' costs toward medical, dental, and vision insurance.\xa0"", '6. Excellent benefits ', 'Many of our employees are current or former athletes who are competitive and like to win. They motivate each other to do their personal best every day, and together we win as a team and as a company!\xa0', 'Hyper-growth means hyper opportunity for employees! While many companies hire managers externally or use a tenure system, we pride ourselves on our cultural value of meritocracy. This means that as we grow and new positions are created, we look to promote our top performers from within. Today over 80% of our managers are promoted from internal roles.\xa0', 'To make the world a healthier place, we started in our backyard. We created a health-conscious environment that allows each of our employees to reach their personal health goals. Below are a few of the employee-led programs that make working at Health IQ truly unique.\xa0', 'Join the Health Conscious Workplace of the Future', 'Build and maintain test coverage over key transforms and develop alerting systems', 'BS/MS/Ph.D.PhD in a technical field - e.g. Computer Science, Math, Statistics, Economics4+ years in Data Engineering, key experiences include:Writing in SQL, Python, R and shell scripting in a version control systemData visualization tools like Looker, Periscope, Tableau, Power BISet up of real-time data pipelines and associated tools: Hadoop, Spark, KafkaWorkflow management, orchestration and batch processing: Apache Airflow, AWS Step Functions, AWS BatchCluster management: JuypterHub, Apache Zeppelin, Docker, Kubernetes, Linux, CICDDatabase/Query performance tuningModern data warehousing solutions like Redshift, Snowflake and BigQueryVarious data structures and formats: XML, JSON, Parquet, ORC, Avro, CSVData mining and machine learningELT/ETL/ETLT technologies like DBT, Fivetran, ETLeapInsurance industry knowledge or experience with insurance data a plus', 'Insurance industry knowledge or experience with insurance data a plus', 'Writing in SQL, Python, R and shell scripting in a version control system', 'Various data structures and formats: XML, JSON, Parquet, ORC, Avro, CSV', 'Join the Health Conscious Workplace of the Future\xa0', 'Data visualization tools like Looker, Periscope, Tableau, Power BI', '4+ years in Data Engineering, key experiences include:', 'Stay up to date on industry and job-related trends and best practices, including reading relevant publications, articles, blogs, etc.', 'BS/MS/Ph.D.PhD in a technical field - e.g. Computer Science, Math, Statistics, Economics', 'Are You Ready to Join The Movement?\xa0', 'Database/Query performance tuning', 'Build tools to support common data science functions (feature extraction, experimentation, funnel metrics, etc…)', 'At the end of the day, we are a business-minded insurance company and we use analytics to measure our success and drive our business. Coming to work and working hard however is much more fun when you are surrounded by like-minded people who are motivated by the same personal goals as you. Our employees are making friends that will last a lifetime.\xa0', '4. Like-Minded Coworkers ', 'Architect data solutions to solve business problems while developing a long term data architecture roadmap', 'To learn more visit https://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdf', 'Modern data warehousing solutions like Redshift, Snowflake and BigQuery', 'Data Architect/Engineer', '\xa0', 'Architect data solutions to solve business problems while developing a long term data architecture roadmapBuild data pipelines and fix performance bottlenecks to visualize data real-timeMaintain and refactor existing schema to maximize data usability and consistency across different business functionsBuild and maintain test coverage over key transforms and develop alerting systemsBuild tools to support common data science functions (feature extraction, experimentation, funnel metrics, etc…)Stay up to date on industry and job-related trends and best practices, including reading relevant publications, articles, blogs, etc.', '5. Motivate and Compete ', '2. Optional Fitness Time ', 'Are You Ready to Join The Movement?', 'Anyone who has tried to stay healthy knows it’s hard to stick to your particular nutritional goals throughout the day with soda, chips, and sugary treats all around you. So we provide our employees with quality food and snack options, like a limitless supply of organic nuts, fruits, and veggies instead.\xa0']",Associate,Full-time,Engineering,Insurance,2021-03-18 14:34:51
Data Engineer,Idaho Central Credit Union,"Chubbuck, ID",1 week ago,77 applicants,"['', 'Experience with business intelligence visualization tools and business intelligence tools, such as, Microsoft Power BI', 'Work closely with team members and BI Analysts to turn data into critical information and knowledge that can be used to make sound business decisions.', 'Can you handle multiple projects at the same time and smile? ', 'Responsible for the full life cycle development, implementation, support, architecture and tuning of the Enterprise Data Warehouse, including Data Marts, Data Lake and data pipelines.', 'Ability to work with and communicate with all Credit Union personnel in the various departments.', 'ELT/ETL development.', '(Keywords Bank, Banking, Finance, Information Technology, Workstations)', 'Experience In', 'If you can answer yes to these questions Idaho Central Credit Union is ready for you to join our team!', 'Ability to work with other department supervisors.', 'Identify, design, and implement internal process improvements including automating manual processes, optimize data delivery and designing cloud infrastructure for greater scalability, etc..', 'Technical skill-sets needed include but not limited to SQL, ELT/ETL development, configuring and maintaining data warehousing solutions, data modeling, data lake technologies, data architecture, and BI reporting development.', 'Advocate of CI/CD methodologies and agile ways of working.', 'Willingness to work occasionally outside of normal business hours.', 'Source-code management tools such as GitHub', 'Self-motivated with the ability to prioritize, meet deadlines ,and manage changing', '2+ years’ experience working in cloud computing with Azure experience required', 'Perform tasks requiring manual dexterity (processing paperwork, filing, stapling, sorting, collating, typing, counting cash, etc.). Sit for extended periods of time. Lift 10-20 pounds of applicable supplies including but not limited to copy paper, cash drawers, marketing material, etc. Repetitive motion using wrists, hands, and fingers. Reach keyboards. Ability to operate basic office machines (calculator, computer, telephone, copy machine, fax machine, etc.).', 'Bachelor’s degree in computer science or equivalent degree is required. Five plus years of experience in a Data Engineering or similar role. Certifications in data analytics and/or data engineering a plus. A demonstrated cooperative and positive attitude toward members and other Credit Union staff. Ability to determine member needs and cross sell Credit Union services. Professional in appearance, attendance, quality, and quantity of work performed. Ability to work under pressure and conflicting situations. Must be willing to comply with the Bank Secrecy Act and USA Patriot Act as implemented by Idaho Central Credit Union.', 'Microsoft SQL Server, Oracle, and other relational and NoSQL databases.', 'Microsoft SQL Server, Oracle, and other relational and NoSQL databases.SQL developmentExperience with business intelligence visualization tools and business intelligence tools, such as, Microsoft Power BIData warehousing, Data Lake, data modeling, data pipelinesELT/ETL development.Knowing programming languages such as Java, R, Python is a plus but is not required.Source-code management tools such as GitHub2+ years’ experience working in cloud computing with Azure experience requiredKnowledge of data processing, hardware platforms, and enterprise software Technical experience with enterprise systems, databases, and user support.Advocate of CI/CD methodologies and agile ways of working.Ability to maintain confidentiality of Credit Union and member records at all times.Self-motivated with the ability to prioritize, meet deadlines ,and manage changingWillingness to work occasionally outside of normal business hours.Excellent English oral and written communication skills.Ability to work with other department supervisors.Strong analytical and problem-solving Ability to maintain confidentiality of Credit Union and member records at all times.Ability to work with and communicate with all Credit Union personnel in the various departments.', 'The above statements reflect the general details considered necessary to describe the essential functions of the job and should not be construed as a detailed description of all the work requirements that may be inherent of the job.', 'Are you compelled to initiate action and remain proactive in getting things done?', 'Design and development focus include but is not limited to dimensional data model design, ETL/ELT frameworks and processing, meta-data management, operational data integration, master data management and data quality, data auditing and profiling, and business intelligence reporting solutions.', 'Do you enjoy working with other people and finding solutions when everyone else only see problems?', 'EOE/Minorities/Females/Vet/Disability', 'Does success motivate you to want to do more and be better?', 'Do you enjoy working with other people and finding solutions when everyone else only see problems?Are you compelled to initiate action and remain proactive in getting things done?Do you take pride in the work you accomplish?Does success motivate you to want to do more and be better?Can you handle multiple projects at the same time and smile? ', 'Write technical policies, procedures, and documentation for the systems including data flow diagrams, architecture diagrams, etc.', 'Contribute to a team responsible for the design, development and implementation of mission critical business intelligence reports and applications to enterprise scale.', 'Other duties as assigned.', 'Knowing programming languages such as Java, R, Python is a plus but is not required.', 'Technologies involved but not limited to Azure Data Lake, Azure Data Factory, Azure Data-bricks, Azure Synapse, SQL Server, Power BI, Oracle database, AI/ML', 'Support the integration of enterprise application databases and real time processing into the data warehouse.', 'Ensure all data accesses, along with data at rest or in transit is secure and follows the best in class data governance standards.', 'Strong analytical and problem-solving Ability to maintain confidentiality of Credit Union and member records at all times.', 'Ensure all data sources are accurate, congruent, reliable, and secure.', 'SQL development', 'Responsible for architecting, configuring, analyzing, and maintaining all data structures used for BI analytics and reporting.', 'Define and build data integration processes to be used across the organization.', 'Excellent English oral and written communication skills.', 'Idaho Central Credit Union is a dynamic financial institution that is focused on helping our members achieve financial success. Established in 1940 we have become the largest and fastest growing financial institution in the state of Idaho. ICCU was voted for large companies the Best Place to work in Idaho. We have also been named by S&P Global Market Intelligence as the top performing credit union in the nation. We are a talent based organization looking for talented individuals to help our members achieve financial success.', 'Do you take pride in the work you accomplish?', 'Data warehousing, Data Lake, data modeling, data pipelines', 'Must be eligible for membership at Idaho Central Credit Union to obtain employment.', 'Work closely with team members and BI Analysts to turn data into critical information and knowledge that can be used to make sound business decisions.Responsible for architecting, configuring, analyzing, and maintaining all data structures used for BI analytics and reporting.Ensure all data sources are accurate, congruent, reliable, and secure.Responsible for the full life cycle development, implementation, support, architecture and tuning of the Enterprise Data Warehouse, including Data Marts, Data Lake and data pipelines.Define and build data integration processes to be used across the organization.Build conceptual and logical data models.Identify, design, and implement internal process improvements including automating manual processes, optimize data delivery and designing cloud infrastructure for greater scalability, etc..Ensure all data accesses, along with data at rest or in transit is secure and follows the best in class data governance standards.Support the integration of enterprise application databases and real time processing into the data warehouse.Technical skill-sets needed include but not limited to SQL, ELT/ETL development, configuring and maintaining data warehousing solutions, data modeling, data lake technologies, data architecture, and BI reporting development.Technologies involved but not limited to Azure Data Lake, Azure Data Factory, Azure Data-bricks, Azure Synapse, SQL Server, Power BI, Oracle database, AI/MLThis person will work closely with external professional services and third-party vendors to have a full knowledge of systems and help establish road-map for future enhancements.Write technical policies, procedures, and documentation for the systems including data flow diagrams, architecture diagrams, etc.Contribute to a team responsible for the design, development and implementation of mission critical business intelligence reports and applications to enterprise scale.Design and development focus include but is not limited to dimensional data model design, ETL/ELT frameworks and processing, meta-data management, operational data integration, master data management and data quality, data auditing and profiling, and business intelligence reporting solutions.Other duties as assigned.', 'The Data Engineer is responsible for architecting, managing, optimizing, overseeing and monitoring data retrieval, storage and distribution. This role is responsible for the technical aspects and enhancements of the data integration tools, enterprise data warehouse, data lake, and enterprise reporting tools.', 'Build conceptual and logical data models.', 'This person will work closely with external professional services and third-party vendors to have a full knowledge of systems and help establish road-map for future enhancements.', 'Ability to maintain confidentiality of Credit Union and member records at all times.', 'Knowledge of data processing, hardware platforms, and enterprise software Technical experience with enterprise systems, databases, and user support.']",Entry level,Full-time,Information Technology,Banking,2021-03-18 14:34:51
Data Engineer,Agilarc LLC,"Pittsburgh, PA",,N/A,"['', ' ', 'Data Engineer Responsibilities:', 'Ability to uncover and understand client business questions an interpret those requirements to architect and design various data solutions ', 'Remaining up-to-date with industry standards and technological advancements that will improve the quality of your outputs. ', 'An Agile Mindset of Collaboration,\xa0Continual improvement, Positive response to Change and Success for the Team.', 'Proficiency in SQL and various relational database management systems. ', 'Data Engineer Requirements:', 'Agilarc is looking for entry level data engineers that have a passion for data and a desire to solve business problems for a variety of clients across different industries.\xa0 The candidate will be teamed up with Senior Agilarc Resources as part of an agile team that works to solve data management problems accross the entire data lifecycle.\xa0 ', 'Working with senior Agilarc consultants to provide data management solutions to vlients ', 'Reformulating existing frameworks to optimize their functioning. ', ' Data Engineer Responsibilities: ', 'Preparing raw data for manipulation by business users. ', ""Bachelor's degree in Data Engineering, Big Data Analytics, Computer Engineering, Information Technology - Data Focused or related fields in Data Management "", 'Capacity to successfully manage a pipeline of duties with minimal supervision. ', 'A curiosity for data and a desire for independent learning for continuous personal develoipment. ', 'Excellent analytical and problem-solving skills. ', ' Data Engineer Requirements: ', 'Working with senior Agilarc consultants to provide data management solutions to vlients Ability to uncover and understand client business questions an interpret those requirements to architect and design various data solutions Reformulating existing frameworks to optimize their functioning. Testing such structures to ensure that they are fit for use. Preparing raw data for manipulation by business users. Remaining up-to-date with industry standards and technological advancements that will improve the quality of your outputs. ', ""Bachelor's degree in Data Engineering, Big Data Analytics, Computer Engineering, Information Technology - Data Focused or related fields in Data Management Proficiency in SQL and various relational database management systems. Excellent analytical and problem-solving skills. A curiosity for data and a desire for independent learning for continuous personal develoipment. Capacity to successfully manage a pipeline of duties with minimal supervision. An Agile Mindset of Collaboration,\xa0Continual improvement, Positive response to Change and Success for the Team."", 'Testing such structures to ensure that they are fit for use. ']",Entry level,Full-time,Consulting,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Connection,"Downers Grove, IL",2 days ago,Be among the first 25 applicants,"['', 'Experience with data visualization tools (Power BI, Tableau)', ' 5 plus years of experience and a Bachelors Degree Hi-level of competency and experience with SQL and platform specific versions (e.g.: T-SQL) Competency with web technologies including REST/SOAP APIs, Web Services, JSON, and OData Experience with data visualization tools (Power BI, Tableau) Competency with non-relational database technologies (MongoDB, CosmosDB) Experience with OLTP/OLAP database technologies (MS SQL Server, MySQL, or similar) ', 'Competency with web technologies including REST/SOAP APIs, Web Services, JSON, and OData', 'Author queries and pipelines for data extraction, movement, integration, and storage', 'Follow DevOps best practices throughout the solution development lifecycle, user acceptance testing, and production release', 'Utilize appropriate data analysis and modeling tools and languages based on the given requirements; learn new data analysis languages and tools when necessary.', 'Understand, explain, develop, and modify relational and non-relational data models; including designing, creating, and customizing along with optimizing based on system performance', 'Work directly with end users, analysts, and project managers to understand business requirements and develop technical software requirements alongside other engineers', 'Skills', 'Essential Functions', 'Responsibilities', 'Troubleshoot, diagnose, upgrade, and improve the performance of reporting and data solutions', 'Design, create and maintain solutions, extensions, and integrations for applications', 'Connection, Inc. and all of its subsidiary companies are committed to equal opportunity and proud to be affirmative action employers. All qualified applicants will receive consideration for employment, without regard to race, sex (including pregnancy), color, religion, age, national origin, ancestry, physical or mental disability status, medical condition, sexual orientation, marital status, protected veteran status, and all other characteristics protected by applicable state and federal law.', 'Additional duties and projects as assigned', 'Analyze and understand business requirements and translate into logical data models', 'Hi-level of competency and experience with SQL and platform specific versions (e.g.: T-SQL)', 'Design and build reporting solutions and dashboards from myriad disparate data sources', 'Work with technical team to prioritize technical needs of solutions and take appropriate action to fulfill', '5 plus years of experience and a Bachelors Degree', 'Competency with non-relational database technologies (MongoDB, CosmosDB)', 'Experience with OLTP/OLAP database technologies (MS SQL Server, MySQL, or similar)', ' Analyze and understand business requirements and translate into logical data models Author queries and pipelines for data extraction, movement, integration, and storage Design, create and maintain solutions, extensions, and integrations for applications Understand, explain, develop, and modify relational and non-relational data models; including designing, creating, and customizing along with optimizing based on system performance Design and build reporting solutions and dashboards from myriad disparate data sources Troubleshoot, diagnose, upgrade, and improve the performance of reporting and data solutions Work directly with end users, analysts, and project managers to understand business requirements and develop technical software requirements alongside other engineers Follow DevOps best practices throughout the solution development lifecycle, user acceptance testing, and production release Utilize appropriate data analysis and modeling tools and languages based on the given requirements; learn new data analysis languages and tools when necessary. Work with technical team to prioritize technical needs of solutions and take appropriate action to fulfill Additional duties and projects as assigned ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,VMware,"Austin, TX",3 days ago,78 applicants,"['', 'Required Skills', 'Experience: ', 'Business Summary: ', 'Preferred Skills: N/A', 'Unix/Linux/Windows operating systems.', 'Posted Date:', 'Category : ', 'SQL;', 'Subcategory: ', 'Job Role And Responsibility', 'Full Time/ Part Time: ', 'Scripting languages like Perl/Python/Shell;SQL;Unix/Linux/Windows operating systems.', 'Scripting languages like Perl/Python/Shell;']",Not Applicable,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Vida Health,"San Francisco, CA",1 week ago,53 applicants,"['', 'Monthly wellness benefit', 'New hire home office stipend', 'Monitor cost of data pipelines and data products and continuously search for ways to reduce the cost of those systems', 'About Us', 'Design and build data pipelines to replace manual tasks such as data cleaning and ingestion of data from new sources', 'Management and/or team lead experience preferred.', 'Track record of standardizing data for analysis on BI tools or delivering ML applications', 'ETL Tools such as DBT, Matillion, Snowplow, Fivetran, Stitch, and Talend', 'Flexible PTO Policy', 'Design the foundational layer of Vida Health’s data environment to make data standardized and reusable', 'Healthcare FSA Plan', 'Define and meet service-level agreements (SLAs) for data pipeline processes and ML powered APIs', 'About The Job', 'Benefits', 'Experience with cloud platforms such as AWS or Google Cloud', 'HIPAA and healthcare data such as Medical and RX claims', '401K Program', ' Has a relentless focus on delivering maximum value to the end user', 'Responsibilities', 'Has at least 5+ years of relevant work experience in Data Engineering or similar roles', 'Bonus Skills', 'Diversity is more than a commitment at Vida—it is the foundation of what we do. All qualified applicants will receive consideration for employment without regard to race, color, ancestry, religion, gender, gender identity or expression, sexual orientation, marital status, national origin, genetics, disability, age, or Veteran status. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law.', 'Weekly meetups with team members across the country through our #connectandcommit program', 'Asynchronous systems such as Kafka, RabbitMQ, PubSub, and Kinesis', 'Competitive compensation with meaningful stock optionsMedical, Dental, Vision, Disability and Life Insurance (We cover 100% of your premium and 80% for your dependents)Healthcare FSA PlanDependent Care FSA PlansCommuter and Parking Benefits 401K ProgramFlexible PTO PolicyPaid Parental Leave10 Paid Company Holidays', 'Monitor the status of data pipelines and data products and promptly inform relevant teams of errors and outages', 'Quarterly All Company Events', '10 Paid Company Holidays', 'Experience managing enterprise data exchanges, Analytics ETL, or ML Ops. ', 'Manage data pipelines that collect and transform data from multiple sources to support ML models, analysis, and reporting', 'Has an ownership mindset and is excited about monitoring and alerting on their systems', 'Commuter and Parking Benefits ', 'Qualifications', 'Work with Data Analysts and Data Scientists to design data architectures that will improve the productivity and velocity of both teams’ workflows', 'Workflow orchestration services such as Apache Airflow', 'Competitive compensation with meaningful stock options', 'Self-service BI tools such as Datastudio, Looker, Amplitude, and Tableau', 'We’re a distributed company, so you can work from most US states (We still have a HQ office in San Francisco)', 'Vida is proud to be an Equal Employment Opportunity and Affirmative Action employer.', 'Medical, Dental, Vision, Disability and Life Insurance (We cover 100% of your premium and 80% for your dependents)', 'Significant opportunities for growth and development as the business grows', 'PERKS', 'Design the foundational layer of Vida Health’s data environment to make data standardized and reusableManage data pipelines that collect and transform data from multiple sources to support ML models, analysis, and reportingManage services that make results from ML models and data analysis accessible to other services and app featuresDesign and build data pipelines to replace manual tasks such as data cleaning and ingestion of data from new sourcesWork with Data Analysts and Data Scientists to design data architectures that will improve the productivity and velocity of both teams’ workflowsMonitor cost of data pipelines and data products and continuously search for ways to reduce the cost of those systemsMonitor the status of data pipelines and data products and promptly inform relevant teams of errors and outagesDefine and meet service-level agreements (SLAs) for data pipeline processes and ML powered APIs', 'We seek to recruit, develop and retain the most talented people from a diverse candidate pool. We don’t just accept differences — we celebrate them, we support them, and we thrive on them for the benefit of our employees, our platform and those we serve. Vida is committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures.', 'Manage services that make results from ML models and data analysis accessible to other services and app features', 'Database tools such as BigQuery, Datastore, and Firestore', 'Advanced knowledge of: Data Warehouses, SQL, Python, REST APIs', 'Paid Parental Leave', 'Database tools such as BigQuery, Datastore, and FirestoreAsynchronous systems such as Kafka, RabbitMQ, PubSub, and KinesisWorkflow orchestration services such as Apache AirflowETL Tools such as DBT, Matillion, Snowplow, Fivetran, Stitch, and TalendSelf-service BI tools such as Datastudio, Looker, Amplitude, and TableauHIPAA and healthcare data such as Medical and RX claimsManagement and/or team lead experience preferred.', 'Training and leadership development programs', 'Quarterly Team Based Connection Opportunities', 'Access to a Vida Health Coach and the full Vida App', 'We’re a distributed company, so you can work from most US states (We still have a HQ office in San Francisco)Access to a Vida Health Coach and the full Vida AppNew hire home office stipendMonthly wellness benefitTraining and leadership development programsWeekly meetups with team members across the country through our #connectandcommit programQuarterly All Company EventsQuarterly Team Based Connection OpportunitiesSignificant opportunities for growth and development as the business grows', 'Our data strategy is Polychronic by Design, meaning we leverage population data insights to help members with multiple interrelated chronic conditions throughout their lifetime. ', 'Vida is authorized to do business in many, but not all, states. If you are not located in or able to work from a state where Vida is registered, you will not be eligible for employment. Please speak with your recruiter to learn more about where Vida is registered.About The JobWe are looking for a Data Engineer who can help us build, manage, and optimize Vida Health’s data pipelines within our Google Cloud Platform (GCP) environment. You would be the first Data Engineer at Vida, which is an important role because we are a very data-driven company. From measuring Vida’s impact on health care costs to correlating member activities with successful health outcomes, you’ll have a substantial impact on projects whose results can lead to changes in product design and member experience. Our data strategy is Polychronic by Design, meaning we leverage population data insights to help members with multiple interrelated chronic conditions throughout their lifetime. Because Vida is a startup, you’ll have the opportunity to work on a diverse set of projects that involve multiple parts of our data infrastructure. One of your responsibilities will be to connect all our sources of data such as claims data and app data, while making the data structured and accessible for analysis and other ETL pipelines. You’ll also work on optimizing our ETL pipelines, including our ML pipelines which deliver recommendations and predictions that influence the member and health provider’s experience on the platform.You’ll have the pleasure of working with a team who is excited about providing care to people living with chronic conditions. We also have experts in health care who are an invaluable resource for learning more about the health domain. We hope that you’ll consider embarking on this journey into polychronic health care with us.ResponsibilitiesDesign the foundational layer of Vida Health’s data environment to make data standardized and reusableManage data pipelines that collect and transform data from multiple sources to support ML models, analysis, and reportingManage services that make results from ML models and data analysis accessible to other services and app featuresDesign and build data pipelines to replace manual tasks such as data cleaning and ingestion of data from new sourcesWork with Data Analysts and Data Scientists to design data architectures that will improve the productivity and velocity of both teams’ workflowsMonitor cost of data pipelines and data products and continuously search for ways to reduce the cost of those systemsMonitor the status of data pipelines and data products and promptly inform relevant teams of errors and outagesDefine and meet service-level agreements (SLAs) for data pipeline processes and ML powered APIsQualificationsExperience with cloud platforms such as AWS or Google CloudAdvanced knowledge of: Data Warehouses, SQL, Python, REST APIsTrack record of standardizing data for analysis on BI tools or delivering ML applicationsExperience managing enterprise data exchanges, Analytics ETL, or ML Ops.  Has a relentless focus on delivering maximum value to the end userHas an ownership mindset and is excited about monitoring and alerting on their systemsHas at least 5+ years of relevant work experience in Data Engineering or similar rolesBonus SkillsDatabase tools such as BigQuery, Datastore, and FirestoreAsynchronous systems such as Kafka, RabbitMQ, PubSub, and KinesisWorkflow orchestration services such as Apache AirflowETL Tools such as DBT, Matillion, Snowplow, Fivetran, Stitch, and TalendSelf-service BI tools such as Datastudio, Looker, Amplitude, and TableauHIPAA and healthcare data such as Medical and RX claimsManagement and/or team lead experience preferred.BenefitsCompetitive compensation with meaningful stock optionsMedical, Dental, Vision, Disability and Life Insurance (We cover 100% of your premium and 80% for your dependents)Healthcare FSA PlanDependent Care FSA PlansCommuter and Parking Benefits 401K ProgramFlexible PTO PolicyPaid Parental Leave10 Paid Company HolidaysPERKSWe’re a distributed company, so you can work from most US states (We still have a HQ office in San Francisco)Access to a Vida Health Coach and the full Vida AppNew hire home office stipendMonthly wellness benefitTraining and leadership development programsWeekly meetups with team members across the country through our #connectandcommit programQuarterly All Company EventsQuarterly Team Based Connection OpportunitiesSignificant opportunities for growth and development as the business growsVida is proud to be an Equal Employment Opportunity and Affirmative Action employer.Diversity is more than a commitment at Vida—it is the foundation of what we do. All qualified applicants will receive consideration for employment without regard to race, color, ancestry, religion, gender, gender identity or expression, sexual orientation, marital status, national origin, genetics, disability, age, or Veteran status. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law.We seek to recruit, develop and retain the most talented people from a diverse candidate pool. We don’t just accept differences — we celebrate them, we support them, and we thrive on them for the benefit of our employees, our platform and those we serve. Vida is committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures.', 'Experience with cloud platforms such as AWS or Google CloudAdvanced knowledge of: Data Warehouses, SQL, Python, REST APIsTrack record of standardizing data for analysis on BI tools or delivering ML applicationsExperience managing enterprise data exchanges, Analytics ETL, or ML Ops.  Has a relentless focus on delivering maximum value to the end userHas an ownership mindset and is excited about monitoring and alerting on their systemsHas at least 5+ years of relevant work experience in Data Engineering or similar roles', 'Dependent Care FSA Plans']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer (New Grad),Orchard,"New York, NY",6 days ago,Over 200 applicants,"['', 'This is a full-time role based out of our New York City office.', 'Prior internship experience working with a team to plan, prioritize, build, and deploy code', 'Integrating with third-party data sources (MLS and county tax data are primary sources, among others) to support our home transaction platform and Data Science initiatives', 'As part of the larger engineering organization, Orchard’s Data Team performs a function that is at the core of our business: we are responsible for building and maintaining the technical infrastructure to ingest the data sources, and deploy the models that drive our decision-making and software. Part of this role will be working directly with the Data Science team in order to make their models production-ready.', 'Prioritization & value orientation: we operate with agile principles and balance long-term planning with re-evaluating priorities based on new data and assumptions', 'Orchard is radically simplifying the way people buy and sell their homes. For the average American, the home purchase and sale process takes months, creates anxiety, and is filled with uncertainty and hassle. Orchard has reimagined the end-to-end experience of buying and selling, from innovative home search tools to find the perfect home to the ability to buy a new home before selling your current one. Orchard customers manage the entire experience through a personalized online dashboard, while also getting the support of best-in-class Orchard real estate agents.\xa0', 'Why Orchard', 'Integrating with third-party data sources (MLS and county tax data are primary sources, among others) to support our home transaction platform and Data Science initiativesBuilding and maintaining ETL pipelines to support business intelligenceBuilding and maintaining model training and validation pipelines for our automated valuation model (AVM)Deploying machine learning models (our AVM) to production so that analysts can use them to value the homes we make offers onWork with the following technologies: Python3, PostgreSQL, Docker, AWS, Airflow', 'About the Role', 'Equity participationFlexible PTOUp to 18 weeks of paid family leaveEmployee discount on Orchard’s services', 'Deploying machine learning models (our AVM) to production so that analysts can use them to value the homes we make offers on', 'Low ego with appetite for feedback: we value humility and sharing + receiving feedback for growth and continuous improvement', 'Proficiency in SQL and Python. Familiarity with Postgres or Airflow is a plus.', 'What You’ll Do Here', ""We're proud to be recognized by Glassdoor, Inc. Magazine, Fast Company and Forbes on their lists of best places to work. We also have a 4.9 Glassdoor rating! Orchard is building the first one-stop-shop in real estate and we’re bringing together the most innovative professionals across real estate, business, marketing, technology and design. We also have some pretty great perks:"", 'Orchard’s engineering culture is centered around product empathy and autonomous teams with high feature ownership. Engineers take ownership of feature development end to end. This means we partner with Product Managers and Designers to solve ambiguous business problems and have a high degree of collaborative input before writing code. We strive to keep common infrastructure and dependencies simple (or only as complex as necessary), to keep the coordination costs of infrastructure deployments low, and support lean & nimble product engineering teams.\xa0', 'Equity participation', 'The team values engineers who prioritize results and testing, and we’re looking for engineers who are excited to explore our data and come up with novel ways to use it.', 'A results-oriented attitude with attention to detail: we expect engineers to own projects from planning to production deployment (and production operations!)', 'This is an exciting opportunity to join a high-growth team at the ground floor, and play an instrumental role in making the home buying and selling experience frictionless for our customers.', 'Flexible PTO', 'Headquartered in New York City and with offices throughout Texas, Colorado, Georgia, North Carolina, and Virginia, Orchard has over 300 employees and growing.\xa0We have raised over $130 million in equity financing from top-tier investors including Revolution, Firstmark, Accomplice, Navitas and Juxtapose. Our investors have also backed the likes of Pinterest, AirBnb, Shopify and Sweetgreen. Orchard is proud to be recognized as part of Glassdoor’s Best Places to Work.', ""Proficiency in SQL and Python. Familiarity with Postgres or Airflow is a plus.Prior internship experience working with a team to plan, prioritize, build, and deploy codeCurrently have, or are in the process of attaining, BS or MS in Computer Science or related fieldA results-oriented attitude with attention to detail: we expect engineers to own projects from planning to production deployment (and production operations!)Low ego with appetite for feedback: we value humility and sharing + receiving feedback for growth and continuous improvementBusiness empathy & clear communication: we work with product, design & business stakeholders collaboratively from early in the design process. We look for engineers to frame technical problems in the context of business value and be able to communicate with cross-functional teamsPrioritization & value orientation: we operate with agile principles and balance long-term planning with re-evaluating priorities based on new data and assumptionsProblem solving & ownership: we look for a willingness to take on problems in a growing organization where there's not yet a defined solution"", 'Business empathy & clear communication: we work with product, design & business stakeholders collaboratively from early in the design process. We look for engineers to frame technical problems in the context of business value and be able to communicate with cross-functional teams', 'Currently have, or are in the process of attaining, BS or MS in Computer Science or related field', 'We’d Love to Hear From You if You Have', 'We are interested in all qualified candidates who are eligible to work in the United States. However, we are not able to sponsor visas at this time.', 'About Orchard', '\ufeffOrchard is proud to be an equal opportunity employer. We provide employment opportunities without regard to age, race, color, ancestry, national origin, religion, disability, sex, gender identity or expression, sexual orientation, veteran status, or any other protected status in accordance with applicable law.', 'Employee discount on Orchard’s services', ""We're currently working from home until it's safe for employees to return to the office. We anticipate returning later this year and are excited to welcome people back to our offices and see one another in person. Until then, your interviews will all happen virtually. If there is anything we can do to make your process easier, don't hesitate to let us know!\xa0"", '\xa0', 'Building and maintaining model training and validation pipelines for our automated valuation model (AVM)', 'Up to 18 weeks of paid family leave', 'Building and maintaining ETL pipelines to support business intelligence', 'Work with the following technologies: Python3, PostgreSQL, Docker, AWS, Airflow', ""Problem solving & ownership: we look for a willingness to take on problems in a growing organization where there's not yet a defined solution""]",Entry level,Full-time,Information Technology,Real Estate,2021-03-18 14:34:51
Software Engineer - Data Systems,ATM.com,"Newport Beach, CA",1 day ago,Be among the first 25 applicants,"['', 'What we are looking for', 'Make our data actionable to Data Scientists, Data Analysts and BI use cases', 'Hands on experience with data visualization tools ', 'Comprehensive benefits', 'Proficient in Python or Java or similar programming languages', 'Fully stocked snack bar and cold brew on tap', '3+ years of experience as a data engineer ', 'Bachelor’s degree in Computer Science or equivalent experience', 'Job Summary', 'Ability to work cross functionally and translate business requirements into data needs', 'Benefits', 'What You’ll Do', 'Nice to have FinTech/AdTech experience', 'Experience working with cloud providers like AWS', 'Nice to have Startup experience, especially at an early stage company', 'Requirements', 'About ATM.com', 'Experience with tools like Spark, Kafka, AirFlow, Terraform', 'Note to Recruiters and Placement Agencies ATM does not accept unsolicited agency resumes. ATM does not pay placement fees for candidates submitted by any agency other than its approved partners.', 'Opportunity for growth and advancement', 'We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.', 'Competitive salary and stock options', 'Previous hands on experience working with large scale data systems and pipelines', ' Bachelor’s degree in Computer Science or equivalent experience 3+ years of experience as a data engineer  Previous hands on experience working with large scale data systems and pipelines Hands on experience with data visualization tools  Experience with data modeling of data from disparate source Proficient in Python or Java or similar programming languages Experience with tools like Spark, Kafka, AirFlow, Terraform Ability to work cross functionally and translate business requirements into data needs Experience working with cloud providers like AWS Nice to have FinTech/AdTech experience Nice to have Startup experience, especially at an early stage company ', 'Experience with data modeling of data from disparate source', 'Build and train predictive models, data visualization platforms', 'Design and architect our data warehouse, data pipelines and infrastructure at scale', ' Design and architect our data warehouse, data pipelines and infrastructure at scale Make our data actionable to Data Scientists, Data Analysts and BI use cases Build and train predictive models, data visualization platforms ', 'We proudly support and encourage people with military experience (active, veterans, reservists and National Guard) as well as military spouses to apply for ATM.com job opportunities.', 'Unlimited paid time off', 'Great culture', ' Competitive salary and stock options Comprehensive benefits Unlimited paid time off Opportunity for growth and advancement Great culture Fully stocked snack bar and cold brew on tap ']",Associate,Full-time,Engineering,Computer Software,2021-03-18 14:34:51
Data Engineer,NYC Health + Hospitals,New York City Metropolitan Area,,N/A,"['', 'NYC residency', 'The Test and Trace Corps is looking for a Data Engineer to join the Data, Analytics and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.', 'Department Preferences:', 'Minimum Qualifications:', 'Summary of Duties and Responsibilities:', 'The Test and Trace Corps is looking for a Data Engineer', 'Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams', 'Regulating the cleanliness of data produced from Data Engineers, Data Scientists, and Data Managers across the Data, Analytics and Product Development Team', '2. Five (5) years of progressive, responsible experience in the field of data processing, computer systems and applications.', 'General knowledge of SQL, R, Python, Excel and related data analytics toolsets', '\ufeff', 'Operations Specialty requires supervisory experience (5 years).', 'Checking and preparing data to provide to the Data Scientist and other analysts, as needed', '3+ years of\xa0experience in a data training, analytics, management, or QC role', 'Auditing and reporting on the QC of data migration and integrations throughout the Test and Trace Corps organization', 'Reporting to the Senior Data Engineer within the Data and Analytics Unit, the Data Engineer designs, evaluates and tests data structures and will be responsible for:', '3. Broad knowledge and expertise in the characteristics of computers, peripheral devices, communications systems and hardware capabilities, programming languages, E.D.P. applications, systems analysis methodology, data management and retrieval techniques; or', 'NYC Health + Hospitals is the largest public health care system in the United States. We provide essential outpatient, inpatient and home-based services to more than one million New Yorkers every year across the city’s five boroughs. Our large health system consists of ambulatory centers, acute care centers, post-acute care/long-term care, rehabilitation programs, Home Care, and Correctional Health Services. Our diverse workforce is uniquely focused on empowering New Yorkers, without exception, to live the healthiest life possible.', 'Network Services requires a telecommunications background and experience.', 'Completing complex network, statistical and programming analyses to clean and prepare data, ensuring accuracy', 'Cleaning large datasets, with responsibility for accuracy and complex analyses through programming and performing statistical analyses on large data sets to ensure integrity of data for analyses and use across the Data, Analytics and Product Development Team to support planning, reporting and development initiatives under the Test and Trace CorpsCompleting complex network, statistical and programming analyses to clean and prepare data, ensuring accuracyServing as the internal engineering consultant to solve structural data issues within the Quality Control (QC) workflow\xa0\xa0\xa0\xa0Regulating the cleanliness of data produced from Data Engineers, Data Scientists, and Data Managers across the Data, Analytics and Product Development TeamCollaborating with data staff and other departments to identify and mitigate potential data issuesAuditing and reporting on the QC of data migration and integrations throughout the Test and Trace Corps organizationChecking and preparing data to provide to the Data Scientist and other analysts, as needed', 'Serving as the internal engineering consultant to solve structural data issues within the Quality Control (QC) workflow\xa0\xa0\xa0\xa0', '4. A satisfactory equivalent combination of training, education and experience', 'Empower Every New Yorker — Without Exception — to Live the Healthiest Life Possible', '1. A Baccalaureate Degree from an accredited college or university with a major in Computer Science, Systems Engineering, applied Mathematics, Business Administration, Economics/Statistics, Telecommunications, Data Communications, or a related field of study; and', 'Ability to work autonomously, think analytically, and anticipate data issues to solve before they ariseExcellent written and verbal communication skills, with the ability to explain data systems to non-technical teamsStrong quality control abilities and exceptional attention to detailGeneral knowledge of SQL, R, Python, Excel and related data analytics toolsets3+ years of\xa0experience in a data training, analytics, management, or QC roleNYC residency', 'Cleaning large datasets, with responsibility for accuracy and complex analyses through programming and performing statistical analyses on large data sets to ensure integrity of data for analyses and use across the Data, Analytics and Product Development Team to support planning, reporting and development initiatives under the Test and Trace Corps', 'Collaborating with data staff and other departments to identify and mitigate potential data issues', 'Strong quality control abilities and exceptional attention to detail', 'Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise']",Director,Full-time,Other,Hospital & Health Care,2021-03-18 14:34:51
Data Engineer (-SK),Intel Corporation,"Folsom, CA",1 week ago,38 applicants,"['', 'A solid track record of data management showing your flawless execution and attention to detail.', 'Knowledge of data mining, machine learning, natural language processing, or information retrieval.', 'Minimum Requirements', ""Help us build out an optimal data infrastructure from the ground up. Currently, the team's primary data infrastructure is comprised of SQL servers.Work closely with BI Experts, Data Scientists, and Data Governance Analysts to ensure infrastructure meets all needs.Proficient understanding of distributed computing principles.Experience with NoSQL and SQL databases, such as HBase, Cassandra, MongoDB, MySQL, Oracle, PostgreSQL, etc.Design, build, manage and optimize data pipelines for data structures encompassing data transformation, data models, schemas, metadata, data quality, and workload management.Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, web scraping, calling APIs, write SQL queries, etc.Process unstructured data into a form suitable for analysis.Support business decisions with ad hoc analysis as needed.Monitoring data performance and modifying infrastructure as needed.Define data retention policies. Participate in standups and design reviews.Document and share knowledge through the creation of comprehensive standards.Breakdown business features into technical stories and approaches.Create proof of concepts and prototypes."", 'Posting Statement', 'Inside this Business Group', 'Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. Perform tasks such as writing scripts, web scraping, calling APIs, write SQL queries, etc.', 'Minimum 1+ years of experience in data engineering.', 'Support business decisions with ad hoc analysis as needed.', 'Monitoring data performance and modifying infrastructure as needed.', 'Experience in production support and troubleshooting.', 'Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources.', 'Knowledge of data cleaning, wrangling, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks.', 'Experience with NoSQL and SQL databases, such as HBase, Cassandra, MongoDB, MySQL, Oracle, PostgreSQL, etc.', ""Help us build out an optimal data infrastructure from the ground up. Currently, the team's primary data infrastructure is comprised of SQL servers."", 'Proficient understanding of distributed computing principles.', 'Process unstructured data into a form suitable for analysis.', 'Define data retention policies. Participate in standups and design reviews.', 'Document and share knowledge through the creation of comprehensive standards.', 'Explore new alternatives or options to solve data mining issues, and utilize a combination of industry best practices, data innovations and your experience to get the job done.', 'A passion for documenting work and sharing knowledge systematically', 'Work closely with BI Experts, Data Scientists, and Data Governance Analysts to ensure infrastructure meets all needs.', 'Qualifications', ""Bachelor's Degree or more in Computer Science or a related field."", ""Bachelor's Degree or more in Computer Science or a related field.Minimum 1+ years of experience in data engineering."", 'Breakdown business features into technical stories and approaches.', ""This position is associated with the sale of Intel's NAND memory and storage business to SK hynix"", 'Other Locations', 'A solid track record of data management showing your flawless execution and attention to detail.Programming experience, ideally in SQL, Python, Spark, Kafka or Java, and a willingness to learn new programming languages to meet goals and objectives.Knowledge of data cleaning, wrangling, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks.Knowledge of data mining, machine learning, natural language processing, or information retrieval.Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources.Explore new alternatives or options to solve data mining issues, and utilize a combination of industry best practices, data innovations and your experience to get the job done.Experience in production support and troubleshooting.A passion for documenting work and sharing knowledge systematically', 'Job Description', 'Preferred Experience And Skill', 'Create proof of concepts and prototypes.', 'Programming experience, ideally in SQL, Python, Spark, Kafka or Java, and a willingness to learn new programming languages to meet goals and objectives.', 'Design, build, manage and optimize data pipelines for data structures encompassing data transformation, data models, schemas, metadata, data quality, and workload management.', 'Data Engineer Roles And Responsibilities']",Entry level,Full-time,Information Technology,Electrical/Electronic Manufacturing,2021-03-18 14:34:51
Data Engineer,Jobot,"Champaign, IL",22 hours ago,Be among the first 25 applicants,"['', ' Python', 'A Bit About Us', ' Provide necessary support to the company’s intellectual property counsel.', 'Looking to hire 100% REMOTE Data Engineer!', 'Essential Functions', ' Python AWS, GCP and Azure Linux/UnixRedshift, MySQL or MS SQL Java, C++, or C Hadoop, Hive, Spark etc. (Big Data Technologies)React, Angular, etc. (JavaScript frameworks for Front-End development)', ' Generate deliverables upon customers’ requests and work to address any further concerns.', ' Handle data manipulation and visualization, and administration of systems securely in accordance with enterprise data security protocols and privacy policy.', 'Job Details', ' AWS, GCP and Azure', ' Java, C++, or C', ' Implement effective and efficient algorithms that are scalable, testable, and maintainable. Handle data manipulation and visualization, and administration of systems securely in accordance with enterprise data security protocols and privacy policy. Work with product management and R&D team to productize. Generate deliverables upon customers’ requests and work to address any further concerns. Automate and simplify team development, test, and operations processes. Provide necessary support to the company’s intellectual property counsel.', ' Automate and simplify team development, test, and operations processes.', ' Competitive base salary and overall compensation package Full benefits Medical, Dental, Vision Generous PTO, vacation, sick, and holidays Life Insurance coverage 401 (K) with generous company match', ' Full benefits Medical, Dental, Vision', 'React, Angular, etc. (JavaScript frameworks for Front-End development)', ' Linux/Unix', ' Hadoop, Hive, Spark etc. (Big Data Technologies)', ' Implement effective and efficient algorithms that are scalable, testable, and maintainable.', ' Life Insurance coverage', 'Redshift, MySQL or MS SQL', ' Competitive base salary and overall compensation package', ' 401 (K) with generous company match', ' Generous PTO, vacation, sick, and holidays', 'Why join us?', ' Work with product management and R&D team to productize.']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer | NY - New York,HopHR,"New York, NY",2 days ago,Be among the first 25 applicants,"['', 'Leads portions of initiatives of limited scope, with guidance and direction.', 'Proficient in SQL and experience in one of the databases', 'Collaborates with data scientists to integrate algorithms and models into automated processes.', 'Design and implement scalable, configurable and self-learning marketing campaign platform', 'Uses expertise, judgment and precedents to contribute to the resolution of moderately complex problems.', 'Experience in Spark are preferred but not required', 'Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards.', 'Participates in the design, build and management of large-scale data ETL (Extract / Transform / Load) workflows for real-time and offline analytic processing.', 'Position Summary ', ' Background Experiences ', 'Proficient in Python, Java, Scala, or C++; Experience in shell scriptsProficient in SQL and experience in one of the databasesExperience in Spark are preferred but not requiredExperience with Hadoop and Hive is a plus', 'Experience with Hadoop and Hive is a plus', 'Proficient in Python, Java, Scala, or C++; Experience in shell scripts', 'Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards.Collaborates with data scientists to integrate algorithms and models into automated processes.Design and implement scalable, configurable and self-learning marketing campaign platformUses expertise, judgment and precedents to contribute to the resolution of moderately complex problems.Leads portions of initiatives of limited scope, with guidance and direction.', ' Fundamental Components ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Staff Data Engineer,Walmart Global Tech,"Sunnyvale, CA",3 weeks ago,Over 200 applicants,"['', 'About Global Tech\xa0', 'The Customer Backbone team with Walmart Global Tech is hiring for a Staff Data Engineer role to be based with our team in Sunnyvale, CA. ', ""Lead development efforts for robust data pipelines and design efforts for working with Walmart's customer data. The ideal candidate will be a combination of solid developer with strong technical design lead."", 'We’re virtual', ""4+ years' experience in software engineering or related field"", 'Imagine working in an environment where one line of code can make life easier for hundreds of millions of people and put a smile on their face. That’s what we do at Walmart Global Tech. We’re a team of 15,000+ software engineers, data scientists and service professionals within Walmart, the world’s largest retailer, delivering innovations that improve how our customers shop and empower our 2.2 million associates. To others, innovation looks like an app, service or some code, but Walmart has always been about people. People are why we innovate, and people power our innovations. Being human-led is our true disruption.\xa0', 'Working virtually this year has helped us make quicker decisions, remove location barriers across our global team, be more flexible in our personal lives and spend less time commuting.\xa0Today, we are reimagining the tech workplace of the future by making a permanent transition to virtual work for most of our team. Of course, being together in person is an important part of our culture and shared success. We’ll collaborate in person at a regular cadence and with purpose.\xa0\xa0\xa0', 'Have robust experience with Data engineering, database engineering, business intelligence, or business analytics, ETL tools and working with large data sets in the cloud', ""Have robust experience with Data engineering, database engineering, business intelligence, or business analytics, ETL tools and working with large data sets in the cloudA Master’s degree in Computer Science or related field 4+ years' experience in software engineering or related field"", 'A Master’s degree in Computer Science or related field ', 'In this role, you will.....', ""You'll blow us away if you.....""]",Mid-Senior level,Full-time,Engineering,Computer Software,2021-03-18 14:34:51
Senior Data Engineer,Reveal Mobile,"Raleigh, NC",2 days ago,Be among the first 25 applicants,"['', 'You like being the person that others rely on.', '5+ years of experience with Scala/Java and Python', 'Competitive salary', 'Experience working with geospatial data', 'You enjoy working in an Agile environment and welcome constructive feedback', '401k matching', 'Our ideal candidate ', '3+ years working in AWS or Google Cloud', 'Flexible work schedule', ' Familiarity with event-driven systems Experience working with NoSQL data stores like ElasticSearch or MongoDB Experience working with geospatial data Experience with Kubernetes Experience with machine learning Experience working in ad tech ', '3+ years working with stream processing technologies such as Apache Kafka and Apache Storm', 'Benefits', 'You quickly learn new technologies as needed and recognize that you are engaged in timely, business-critical tasks.', '5+ years of experience with Apache Spark', 'Technical Skills', 'Bonus Skills', 'Experience working in ad tech', 'You are transparent in what you do. You discuss, document, and commit your work as needed', 'Experience working with NoSQL data stores like ElasticSearch or MongoDB', 'Unlimited PTO', 'Requirements', 'Health, dental, life and disability insurance plans', 'You take pride in your work. You are attentive to detail, but also flexible.', 'You use a combination of persistence, research, problem-solving skills, and experience to overcome obstacles', 'Experience with Kubernetes', 'You approach problems with a product development mindset', 'Experience with Git and continuous integration systems', ' 5+ years of experience with Apache Spark 5+ years of experience with Scala/Java and Python 4+ years building data pipelines 3+ years working with stream processing technologies such as Apache Kafka and Apache Storm 3+ years working in AWS or Google Cloud Experience with or desire to learn a functional programming language Experience with Git and continuous integration systems Excellent verbal and written communication skills ', 'Experience with machine learning', ' Competitive salary 401k matching Health, dental, life and disability insurance plans Unlimited PTO Flexible work schedule', 'Experience with or desire to learn a functional programming language', ' You use a combination of persistence, research, problem-solving skills, and experience to overcome obstacles You take pride in your work. You are attentive to detail, but also flexible. You are available for and responsive to questions. You are professional and collegial in your communications. You like being the person that others rely on. You quickly learn new technologies as needed and recognize that you are engaged in timely, business-critical tasks. You are transparent in what you do. You discuss, document, and commit your work as needed You enjoy working in an Agile environment and welcome constructive feedback You approach problems with a product development mindset ', '4+ years building data pipelines', 'Familiarity with event-driven systems', 'Excellent verbal and written communication skills', 'You are available for and responsive to questions. You are professional and collegial in your communications.']",Mid-Senior level,Full-time,Engineering,Information Technology and Services,2021-03-18 14:34:51
Data Engineer/Analyst,System One,"Windsor Mill, MD",2 days ago,Be among the first 25 applicants,"['', 'Expert in designing complex and semantically rich data structures.', 'Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements', 'Expertise in Conceptual Data Modeling, Logical Data Modeling, Physical Data Modeling, Enterprise Data Warehouse Design, DataMart Design, Metadata, Data Quality, Master Data Management and Master Data/Data Governance using Erwin', 'Experience designing relational data models, analytical data models and non-relational data models for OLTP and advanced analytic applications.', 'Position Description:', 'Able to work independently with BA and PM to gather requirements, perform data discovery, come up with data model, work with data engineers to build data pipeline, and work with developers to answer their questions.', 'Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.', 'Education', 'Worked with OLTP/data warehouse modeling, and play key role in multiple projects and act as knowledge expert (Technical, Functional).', 'Ability to understand complex business processes to derive conceptual and logical data models.', 'Skills Requirements:', 'Clearance', 'Ability to optimize and performance tune SQL queries', 'Location', 'Extensively used ETL methodologies for supporting data extraction, transformations and loading processing, in a corporate-wide-ETL Solution.', 'Good data analysis, problem solving and SQL skills.', 'Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirementsAble to work independently with BA and PM to gather requirements, perform data discovery, come up with data model, work with data engineers to build data pipeline, and work with developers to answer their questions.Extensively used ETL methodologies for supporting data extraction, transformations and loading processing, in a corporate-wide-ETL Solution.Expertise in Conceptual Data Modeling, Logical Data Modeling, Physical Data Modeling, Enterprise Data Warehouse Design, DataMart Design, Metadata, Data Quality, Master Data Management and Master Data/Data Governance using ErwinTechnical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.Ability to understand complex business processes to derive conceptual and logical data models.Lead complex discussions and engagements that may involve multiple project teams from client.Experience designing relational data models, analytical data models and non-relational data models for OLTP and advanced analytic applications.Expert in designing complex and semantically rich data structures.Worked with OLTP/data warehouse modeling, and play key role in multiple projects and act as knowledge expert (Technical, Functional).Ability to optimize and performance tune SQL queriesGood data analysis, problem solving and SQL skills.', 'Lead complex discussions and engagements that may involve multiple project teams from client.']",Entry level,Other,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Analytics Engineer,Northwestern Mutual,"Milwaukee, WI",1 week ago,60 applicants,"['', 'At least 1 year of professional experience in data acquisition, cleansing, feature engineering, debugging and software documentation, using languages such as SQL, Scala, Python.Experience using continuous integration and deploymentconceptsAt least 1-year experience with specific technical requirements/platforms will vary (i.e. AWS data science and analytics cloud technologies such as S3, EMR, Spark, PySpark, RDS, JupyterLab, Sagemaker, etc., data warehouses such as Redshift or Snowflake; and digital analytics tools such as Adobe Analytics and Heap) Experience navigating various types of database models and DBMS’s to create data sets for analytics and model training and development.Experience with Testing or data accuracy/quality designs', 'Experience navigating various types of database models and DBMS’s to create data sets for analytics and model training and development.', 'Data Analytics Engineer Job Description', 'This job is not covered by the existing Collective Bargaining Agreement.', 'Experience with Testing or data accuracy/quality designs', 'Apply engineering best practices in order to analyze, design, develop, deploy and support data analytics products. ', 'DevOps environment', 'Experience with user ad hoc tools: Power BI, Business Objects, Tableau', 'Experience with Agile methodologiesDevOps environmentExperience with user ad hoc tools: Power BI, Business Objects, Tableau', ""We're strong and growing."", 'At least 1 year of professional experience in data acquisition, cleansing, feature engineering, debugging and software documentation, using languages such as SQL, Scala, Python.', 'Acquire, analyze, combine, synthesize, and structure data with clear definitions and sources for analytical consumptionConsult with data consumers to identify meaningful datasets for analytical consumption.Develop data products using continuous deployment and integration practicesApply engineering best practices in order to analyze, design, develop, deploy and support data analytics products. Participate in agile story authoring, sizing, and demo sessions for product features Participate in code reviews and provide feedback to the team', 'Experience with Agile methodologies', 'Experience using continuous integration and deploymentconcepts', 'Develop data products using continuous deployment and integration practices', 'At Northwestern Mutual, we are strong, innovative and growing. We invest in our people. We care and make a positive difference.', 'Participate in agile story authoring, sizing, and demo sessions for product features ', 'At least 1-year experience with specific technical requirements/platforms will vary (i.e. AWS data science and analytics cloud technologies such as S3, EMR, Spark, PySpark, RDS, JupyterLab, Sagemaker, etc., data warehouses such as Redshift or Snowflake; and digital analytics tools such as Adobe Analytics and Heap) ', 'At Northwestern Mutual, we believe relationships are built on trust.', ""What's the role?"", 'Consult with data consumers to identify meaningful datasets for analytical consumption.', 'Qualifications', 'Grow your career with a best-in-class company that puts our client’s interests at the center of all we do. Get started now! ', ' We are an equal opportunity/affirmative action employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, gender identity or expression, sexual orientation, national origin, disability, age or status as a protected veteran, or any other characteristic protected by law.', 'Who We Are', 'Preferred Skills And Abilities', 'Participate in code reviews and provide feedback to the team', 'We care. ', 'Required Certifications', 'Responsibilities Include But Are Not Limited To', 'Role And Responsibilities', 'Acquire, analyze, combine, synthesize, and structure data with clear definitions and sources for analytical consumption', 'We invest in our people. ', 'At Northwestern Mutual, we are strong, innovative and growing. We invest in our people. We care and make a positive difference. ', 'We are an equal opportunity/affirmative action employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, gender identity or expression, sexual orientation, national origin, disability, age or status as a protected veteran, or any other characteristic protected by law.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Associate Data Engineer,Signant Health,"Blue Bell, PA",2 days ago,Be among the first 25 applicants,"['', ' Diversity and Inclusion Competencies ', ' Dedication and commitment to promote diversity, multiculturalism and inclusion in all work activities  Ability to collaborate in diverse teams to foster productive outcomes. ', 'Ability to quickly learn and apply new skills, procedures and approaches.', 'Working with Business System Analysts (BSA), gains an understanding of the business requirements for assigned tasks.', 'Working with development team, integrates implemented code and database objects into release application. Performs smoke testing for released application as required.', 'Develops and maintains application components (under supervision).', 'Working with Technical Lead, Team Lead and/or Software Development Manager to gain an understanding of design patterns, principles and standards to be followed in implementing assigned tasks.', 'Key Accountabilities/Decision Making & Influence', ' At Signant Health, accepting difference isn’t enough—we celebrate it, we support it, and we nurture it for the benefit of our team members, our clients and our community. ', 'Identifies areas of the application impacted by the resolution and works with BSA and/or Software Test Engineer to define test cases required for the resolution.', 'Knowledge, Skills & Attributes', 'Efficiently and effectively diagnoses and resolves defects.', ' Ability to collaborate in diverse teams to foster productive outcomes. ', 'Role Overview', 'Ability to complete high quality technical documentation.', 'Understands and follows coding standards.', 'Prepare all required change control documentation including updates to design and other technical documents as required.', 'Adheres to Good Clinical Practices (GCP), 21 CFR Part 11 and other regulatory requirements as required.', 'Performs all work in accordance with documented Standard Operating Procedures (SOPs), Working Instructions.', ' Dedication and commitment to promote diversity, multiculturalism and inclusion in all work activities ', ' Signant Health is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or veteran status.', 'Ability to meet established timelines.', 'Ability to work in a fast paced environment.', 'Strong verbal and written communication skills.']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Business Intelligence Data Engineer,Foresight Mental Health,"Austin, TX",18 hours ago,Be among the first 25 applicants,"['', 'Define and own organization-level data architecture for a trusted, governed, dimensionally-modeled repository of data that enables Foresight staff to quickly and reliably answer their questions.', 'Delight data consumers throughout Foresight by ensuring they have the data they need to inform decisions, where and when they need it.Define and own organization-level data architecture for a trusted, governed, dimensionally-modeled repository of data that enables Foresight staff to quickly and reliably answer their questions.Protect data sources against data quality issues: work with data producers to ensure data passes acceptance tests; design, develop and maintain data quality monitoring and assurance framework; and continuously improve the processes for developing new ones, raising the level of quality expected from our work.Improve data discovery: create data exploration processes and promote adoption of data sources across the company.Optimize business, engineering, and data processes via data architecture, engineering, testing, and operational excellence best practices.Work on various projects from a diverse set of partners as prioritized in project management software (Asana, Notion) by the Business Intelligence Director.', 'Loves solving problems so much that it’s become a personal sport', '401(k)', 'Culture', 'Optimize business, engineering, and data processes via data architecture, engineering, testing, and operational excellence best practices.', 'We stick up for each other and pick each other up when down.', 'We like to have fun while we work. ', 'Very clear communication and presentation skills, distilling complex analysis and concepts into concise business-focused takeaways for leaders', '3+ years of experience in designing and maintaining a data warehouse, like Google Big Query, over a complex network of disparate data sources.', 'Proven python skills - move and transform data from multiple distinct sources to a data store/warehouse, like Google Big Query', 'You could almost run a small BI team on your own.', 'Experience working with data visualization tools: Google Data Studio (we currently use), Tableau (we’re considering using), Looker (we’re considering using). While our data analysts create all the visualizations, it is important that our data engineers understand these tools and how they interact with the data warehouse.', 'Is supportive and constructively collaborative', 'Dental ', 'You have worked at a healthcare company or silicon valley-like startup.', 'Fully remote: this position will remain remote after COVID-19.', 'We’re Looking For Someone Who', 'Delight data consumers throughout Foresight by ensuring they have the data they need to inform decisions, where and when they need it.', 'Protect data sources against data quality issues: work with data producers to ensure data passes acceptance tests; design, develop and maintain data quality monitoring and assurance framework; and continuously improve the processes for developing new ones, raising the level of quality expected from our work.', '3+ years of experience generating data pipelines from multiple distinct data sources, in collaboration with diverse team members.', 'Bonus', 'Loves startups but doesn’t love working insane hours (we promote work-life balance)', 'Culture is our team’s greatest strength, and hence it is our top consideration when adding a new teammate. As a small but quickly growing team, we want to ensure that our existing team dynamics are nurtured. ', 'Work on various projects from a diverse set of partners as prioritized in project management software (Asana, Notion) by the Business Intelligence Director.', '3+ years of experience in data engineering, software engineering, or a closely related role', 'Unlimited PTO', ""What You'll Bring"", 'You’ve played a key role in an early stage BI team.', 'Is comfortable being scrappy and hacking things to move forward but prefers thoroughness whenever possible', 'Strong experience with Google Cloud: Big Query & AWS: Redshift, RDS', 'Creativity to engineer novel features and signals, and to push beyond current tools and approaches', 'Fully remote: this position will remain remote after COVID-19.Strong work-life balance culture.Our team has a very strong, supportive, and selfless culture. We help each other learn, grow and thrive in a way that is refreshingly atypical in companies and more akin to college camaraderie. We stick up for each other and pick each other up when down.We like to have fun while we work. We like to experiment with and break things in order to learn.We are the first to say we made a mistake or don’t know something even if we think we do. We praise each other often and freely give each other credit.When we ask “how are you,” we genuinely mean it. You get to shape an entire organization from the top down and from every angle.We’re literally making thousands of people’s lives happier and healthier every day.', 'We praise each other often and freely give each other credit.', 'When we ask “how are you,” we genuinely mean it. ', 'You get to shape an entire organization from the top down and from every angle.', 'We’re literally making thousands of people’s lives happier and healthier every day.', 'Vision Insurance', 'You actively try pushing yourself out of your comfort zone.', 'Is honest, candid and transparent', 'Perks', 'Continuing education and training ', 'Counseling, coaching, nutrition, and other wellness programs provided by our internal team of practitioners ', 'You have statistics knowledge, such as hypothesis testing, experimental design and sample size determination.', 'Is ingenious and wise but profoundly humble', 'Oh, And Of Course There’s', 'Home office setup', 'You have a working knowledge of fundamental business concepts.', 'You can quickly spot the story in the data and connect the dots across seemingly disparate phenomena.', 'You enjoy learning about the business side of healthcare.', 'Shows up with an open mind and a curiosity to learn', 'You could almost run a small BI team on your own.You’ve played a key role in an early stage BI team.You have a working knowledge of fundamental business concepts.You have statistics knowledge, such as hypothesis testing, experimental design and sample size determination.You can quickly spot the story in the data and connect the dots across seemingly disparate phenomena.You enjoy learning about the business side of healthcare.You have worked at a healthcare company or silicon valley-like startup.You actively try pushing yourself out of your comfort zone.', '3+ years of experience handling relational databases and large scale distributed systems such as Hadoop and Spark along with querying languages including SQL, Hive, SparkSQL, MySQL, SQL Tuning, data modeling best principles, OLAP, and Big Data technologies', 'Medical ', 'Improve data discovery: create data exploration processes and promote adoption of data sources across the company.', 'Experience with development best practices, including query optimization, version control, code reviews, and documentation', 'Our team has a very strong, supportive, and selfless culture. We help each other learn, grow and thrive in a way that is refreshingly atypical in companies and more akin to college camaraderie. ', 'Is flexible and adapts well to change', ""What You'll Do"", 'Strong work-life balance culture.', 'We are the first to say we made a mistake or don’t know something even if we think we do. ', '3+ years of experience in data engineering, software engineering, or a closely related role3+ years of experience handling relational databases and large scale distributed systems such as Hadoop and Spark along with querying languages including SQL, Hive, SparkSQL, MySQL, SQL Tuning, data modeling best principles, OLAP, and Big Data technologies3+ years of experience generating data pipelines from multiple distinct data sources, in collaboration with diverse team members.Experience with development best practices, including query optimization, version control, code reviews, and documentation3+ years of experience in designing and maintaining a data warehouse, like Google Big Query, over a complex network of disparate data sources.Strong experience with Google Cloud: Big Query & AWS: Redshift, RDSProven python skills - move and transform data from multiple distinct sources to a data store/warehouse, like Google Big QueryVery clear communication and presentation skills, distilling complex analysis and concepts into concise business-focused takeaways for leadersCreativity to engineer novel features and signals, and to push beyond current tools and approachesExperience working with data visualization tools: Google Data Studio (we currently use), Tableau (we’re considering using), Looker (we’re considering using). While our data analysts create all the visualizations, it is important that our data engineers understand these tools and how they interact with the data warehouse.', 'Can laugh at themselves - doesn’t take themselves too seriously', 'We like to experiment with and break things in order to learn.']",Associate,Full-time,Information Technology,Mental Health Care,2021-03-18 14:34:51
Data Engineer II,RetailMeNot,"Austin, TX",3 days ago,Be among the first 25 applicants,"['', 'Implement data system for both real-time and warehouse applications', 'We have lots of smart people to work with and learn from.', ""You have a Bachelor's degree in computer science or equivalent STEM field, or equivalent work experienceYou have 3+ years work experienceYou are skilled using Python, Linux, Docker, Git, and Amazon Web Services (or have translatable experience with similar toolsets)You have extensive SQL experience on a variety of RDBMS, and enjoy optimizing queries as well as designing efficient data modelsYou have developed scalable solutions using both SQL and NoSQL (Hadoop) databases. Working with data sets comprised of millions or billions of records is comfortableYou strive to identify simple solutions to complex problems, can identify a minimal viable product and enjoy iterative developmentYou are able to accurately estimate tasks, identify dependencies and dedicatedly solve problems to ensure commitments are metYou recognize that your success depends upon enabling your fellow team members to succeed; taking time to help others energizes youYou enjoy gathering requirements from non-technical coworkers and delivering solutions that meet their needs and exceed their expectationsYou derive satisfaction from enabling the business to succeed and delighting coworkers, not building technology for its own sakeYou have a work ethic that inspires your fellow team members to give their best"", 'You have extensive SQL experience on a variety of RDBMS, and enjoy optimizing queries as well as designing efficient data models', 'About Us', ' Competitive base & bonus packages; salary negotiable', ' Long Term Incentive Plan', ""You have a Bachelor's degree in computer science or equivalent STEM field, or equivalent work experience"", ' Very competitive benefits packages, including best-in-class parental leave', ' Cell phone & gym membership reimbursements', 'U.S. Equal Employment Opportunity/Affirmative Action Information', 'We have an open environment where engineers are given a lot of responsibility and the freedom to make a huge impact.We have lots of smart people to work with and learn from.We work on large scale challenges with a variety of technologies and believe in an ever-growing diversity of technology platforms.We believe in giving prizes, bonuses, and recognition for doing what you enjoy.', 'You are able to accurately estimate tasks, identify dependencies and dedicatedly solve problems to ensure commitments are met', 'Implement data system for both real-time and warehouse applicationsDevelop ETL processes that ensure data is accurate and available within SLAsEnhance data models by developing integrations with business partnersSeek opportunities for performance improvement and implement optimizationsCreate dashboards that provide insight into the health of data integrations, ETL processes and data sets', 'You have a work ethic that inspires your fellow team members to give their best', 'You derive satisfaction from enabling the business to succeed and delighting coworkers, not building technology for its own sake', 'We work on large scale challenges with a variety of technologies and believe in an ever-growing diversity of technology platforms.', 'You have 3+ years work experienceYou are skilled using Python, Linux, Docker, Git, and Amazon Web Services (or have translatable experience with similar toolsets)', 'You recognize that your success depends upon enabling your fellow team members to succeed; taking time to help others energizes you', 'Who You Are', 'Seek opportunities for performance improvement and implement optimizations', 'Enhance data models by developing integrations with business partners', ' Performance based rewards & recognition for your hard work and service', ' Open & flexible PTO', 'Include, But Are Not Limited To The Following', 'Develop ETL processes that ensure data is accurate and available within SLAs', 'You have developed scalable solutions using both SQL and NoSQL (Hadoop) databases. Working with data sets comprised of millions or billions of records is comfortableYou strive to identify simple solutions to complex problems, can identify a minimal viable product and enjoy iterative development', 'Who We Are', 'We have an open environment where engineers are given a lot of responsibility and the freedom to make a huge impact.', 'We believe in giving prizes, bonuses, and recognition for doing what you enjoy.', 'Some rewards do not apply to contract workers or interns.About UsRetailMeNot, Inc. is a leading savings destination bringing people and the things they lovetogether through savings with retailers, brands, restaurants and pharmacies. RetailMeNotmakes everyday life more affordable through online and in-store coupon codes, cash backoffers, and the RetailMeNot Deal Finder™ browser extension. Savings are also provided inconsumers’ mailboxes through the RetailMeNot Everyday™ direct mail package.To learn more, visit http://www.retailmenot.com/corp or follow @RetailMeNot on social media.U.S. Equal Employment Opportunity/Affirmative Action InformationAt RetailMeNot we celebrate difference. We are committed to ensuring an environment of mutual respect for every employee and proud to be an an equal employment opportunity employer who does not discriminate against any person because of race, color, creed, religion, gender, gender identity, gender expression, national origin, citizenship, age, sex, sexual orientation, pregnancy, marital status, ancestry, physical or mental disability, military or veteran status, or any other characteristic protected by law. We believe a diverse and inclusive workplace is central to our success and actively seek to recruit, develop and retain the most talented people from a diverse pool of candidates. You are being given the opportunity to provide the following information in order to help us align with federal and state Equal Employment Opportunity/Affirmative Action record keeping, reporting, and other legal requirements.', 'You enjoy gathering requirements from non-technical coworkers and delivering solutions that meet their needs and exceed their expectations', ""What You'll Do"", 'Rewards*', 'Create dashboards that provide insight into the health of data integrations, ETL processes and data sets']",Not Applicable,Full-time,Information Technology,Internet,2021-03-18 14:34:51
Data Engineer,The Hartford,"Hartford, CT",1 week ago,Be among the first 25 applicants,"['', ' Researches and evaluates alternative solutions and recommends the most efficient and cost effective solution for the systems design ', ' Data warehouse applications knowledge in insurance domain ', 'Role Expectations', ' Possesses functional knowledge and skills reflective of a competent practitioner with the ability to deliver on work of varying technical complexity ', 'Analytical approach with a strong ability to uncover and resolve problems by delivering innovative approaches and solutions.', 'Ability to develop and maintain systems according to a defined set of standards.', 'Ability to work as part of and with high performing teams.', ' Coordinate activities with cross-functional IT unit stakeholders (e.g., database, operations, telecommunications, technical support, etc.) ', ' Work within a self-organized scrum development team regarding all design and implementation ', ' Consults with functional management in the analysis of short and long-range business requirements and recommends innovations which anticipate the future impact of changing business needs ', ' Knowledge of Oracle Exadata , Unix/Linux Shell scripting, Autosys, Version Control Tools ', ' Design and develop high quality, scalable software modules for next generation analytics solution suite  Prototype high impact innovations, catering to changing business needs, by learning and leveraging new technologies (AWS Cloud, Big Data, Snowflake).  Integrate with Data Quality Services to ensure Quality data is Published to consumers.  Possesses functional knowledge and skills reflective of a competent practitioner with the ability to deliver on work of varying technical complexity  Consults with functional management in the analysis of short and long-range business requirements and recommends innovations which anticipate the future impact of changing business needs  Works closely with client management to identify and specify the complex business requirements and processes for diverse development platforms, computing environments (e.g., Cloud, host based, distributed systems, client server), software, hardware, technologies and tools  Coordinate activities with cross-functional IT unit stakeholders (e.g., database, operations, telecommunications, technical support, etc.)  Researches and evaluates alternative solutions and recommends the most efficient and cost effective solution for the systems design  Work within a self-organized scrum development team regarding all design and implementation ', ' Design and develop high quality, scalable software modules for next generation analytics solution suite ', 'Ability to set and manage own priorities effectively in a dynamic organization.', ' Bachelor degree with at least 2 years of applicable work experience or equivalent Experience in ETL / Data Integration Technologies  Knowledge of Oracle Exadata , Unix/Linux Shell scripting, Autosys, Version Control Tools  Data warehouse applications knowledge in insurance domain  Experienced in Agile Scrum and Kanban Methodologies Understanding of current and emerging IT products, services, processes and methodologies.Analytical approach with a strong ability to uncover and resolve problems by delivering innovative approaches and solutions.Ability to develop and maintain systems according to a defined set of standards.Ability to set and manage own priorities effectively in a dynamic organization.Ability to work as part of and with high performing teams.', 'Qualifications', ' Experience in ETL / Data Integration Technologies ', ' Prototype high impact innovations, catering to changing business needs, by learning and leveraging new technologies (AWS Cloud, Big Data, Snowflake). ', ' Integrate with Data Quality Services to ensure Quality data is Published to consumers. ', ' Works closely with client management to identify and specify the complex business requirements and processes for diverse development platforms, computing environments (e.g., Cloud, host based, distributed systems, client server), software, hardware, technologies and tools ', 'Understanding of current and emerging IT products, services, processes and methodologies.', ' Bachelor degree with at least 2 years of applicable work experience or equivalent', ' Experienced in Agile Scrum and Kanban Methodologies ', 'Ready to grow your career leveraging the latest DATA technologies?']",Entry level,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Engineer,Kimball Midwest,"Columbus, OH",1 day ago,Be among the first 25 applicants,"['', 'Qualifications', 'Bachelor’s degree in a computer related field or supporting work experience', 'Bachelor’s degree in a computer related field or supporting work experience2+ years of IT experience, including database and ETL / ELT design and implementationCurrent certification in related Azure concepts, or the desire and ability to lead one’s own skill development to obtain certification in the near futureProficiency is scripting languages such as T-SQL, KQL, DAX, MDX, C Sharp, R and PythonThis position requires the individual to be detail-oriented with conceptual, analytical, and problem-solving skills and the ability to make improvement recommendationsExperience with database engineering best practices utilizing Microsoft technologies, preferable in AzureWorking knowledge of source control systems for database work, preferably GIT repositoriesAbility to rapidly accumulate in-depth company and industry knowledgeSelf-motivated working ability with minimal supervision or directionMust have a “can-do” attitude. Is flexible to work outside of normal business hoursThis position requires the execution of an Employment Agreement contract between Kimball Midwest and the employee', 'Additional Information', 'Working knowledge of source control systems for database work, preferably GIT repositories', 'Ability to rapidly accumulate in-depth company and industry knowledge', 'This position requires the execution of an Employment Agreement contract between Kimball Midwest and the employee', '2+ years of IT experience, including database and ETL / ELT design and implementation', 'Proficiency is scripting languages such as T-SQL, KQL, DAX, MDX, C Sharp, R and Python', 'Current certification in related Azure concepts, or the desire and ability to lead one’s own skill development to obtain certification in the near future', 'Experience with database engineering best practices utilizing Microsoft technologies, preferable in Azure', 'Employment Agreement', 'This position requires the individual to be detail-oriented with conceptual, analytical, and problem-solving skills and the ability to make improvement recommendations', 'Must have a “can-do” attitude. Is flexible to work outside of normal business hours', 'Self-motivated working ability with minimal supervision or direction']",Entry level,Full-time,Information Technology,Construction,2021-03-18 14:34:51
Data Engineer,Amazon Web Services (AWS),"Seattle, WA",2 weeks ago,Be among the first 25 applicants,"['', ' Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets', ' 3+ years of experience as a Data Engineer or in a similar role', ' 5+ years of experience as a Data Engineer, BI Engineer, Business/Financial Analyst or Systems Analyst in a company with large, complex data sources. Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets Experience working with AWS big data technologies (EMR, Redshift, S3) Demonstrated strength in data modeling, ETL development, and data warehousing Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy Experience providing technical leadership and mentoring other engineers for best practices on data engineering Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations', ' Experience using big data technologies (Hadoop, Hive, Hbase, Spark, EMR, etc.)', ' Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy', ' Demonstrated strength in data modeling, ETL development, and data warehousing', 'Description', ' Experience with data modeling, data warehousing, and building ETL pipelines', "" Bachelor's degree in computer science, engineering, mathematics, or a related technical discipline"", ' 5+ years of experience as a Data Engineer, BI Engineer, Business/Financial Analyst or Systems Analyst in a company with large, complex data sources.', ' 4+ years of industry experience in software development, data engineering, business intelligence, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets', 'Company', ' Knowledge of data management fundamentals and data storage principles', ' Experience working with AWS big data technologies (EMR, Redshift, S3)', ' Data modeling to support machine learning model training and offline, batch inference workflows.', ' Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations', ' Experience providing technical leadership and mentoring other engineers for best practices on data engineering', 'Basic Qualifications', ' Work closely with machine learning and data scientists to scale model training, explore new data sources and feature extraction.', ' Build data pipelines to feed machine learning models for real-time and large-scale offline use cases.', ' Knowledge of distributed systems as it pertains to data storage and computing', ' Data modeling to support machine learning model training and offline, batch inference workflows. Build data pipelines to feed machine learning models for real-time and large-scale offline use cases. Work closely with machine learning and data scientists to scale model training, explore new data sources and feature extraction.', "" 3+ years of experience as a Data Engineer or in a similar role Experience with data modeling, data warehousing, and building ETL pipelines Experience in SQL Bachelor's degree in computer science, engineering, mathematics, or a related technical discipline 4+ years of industry experience in software development, data engineering, business intelligence, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets Experience using big data technologies (Hadoop, Hive, Hbase, Spark, EMR, etc.) Knowledge of data management fundamentals and data storage principles Knowledge of distributed systems as it pertains to data storage and computing"", ' Experience in SQL', 'Preferred Qualifications']",Not Applicable,Full-time,Strategy/Planning,Computer Software,2021-03-18 14:34:51
Data Engineer,PlayVS,Los Angeles Metropolitan Area,1 week ago,Over 200 applicants,"['', 'Strong verbal and written communication skills', 'Orchestrate the unity between our product data and external data sources, destinations and APIs', 'Familiarity with API integrations', 'We are looking for a passionate Data Engineer to join our team. You will be responsible for building and maintaining our data infrastructure that is foundational to our amateur esports ecosystem.', 'Ability to transform raw data into tangible insights through data visualization (Mode, Looker, Tableau)', 'Possess a growth mindset: Live and breathe experimentation, data analysis, and ruthlessly prioritize for impact', 'Design, build and maintain your vision of an optimal data pipeline architecture.', 'Committed desire to understand our customers', 'Work with the Data, Product, Operations and Executive teams to extract value from data and support their data infrastructure needs.', '4+ years of experience in Data Engineering', 'The fastest-growing High School and Collegiate sport in America isn’t basketball or football—it’s esports—and PlayVS is the official platform for High School and Collegiate esports. We offer an incredible, full-stack esport platform—game integrations, team building tools, leagues, tournaments, and schedules—and our software products tie everything together into a cohesive experience.', '4+ years of experience in Data EngineeringExtensive experience with SQL and Python scriptingExpertise with data warehousing (Redshift, BigQuery, Snowflake, Postgres), data pipelining (Airflow, AWS Data Pipeline, dbt), and data modeling.\xa0Ability to execute on ETL design, implementation, and maintenance\xa0Familiarity with API integrationsAbility to transform raw data into tangible insights through data visualization (Mode, Looker, Tableau)Passionate about PlayVS and the problem we are solvingCommitted desire to understand our customersStrong verbal and written communication skillsPossess a growth mindset: Live and breathe experimentation, data analysis, and ruthlessly prioritize for impact', 'Passionate about PlayVS and the problem we are solving', 'Expertise with data warehousing (Redshift, BigQuery, Snowflake, Postgres), data pipelining (Airflow, AWS Data Pipeline, dbt), and data modeling.\xa0', 'Deploy data models that synergizes data democratization and business operations.', ""Here's What You'll Get To Do"", 'Work with the Data, Product, Operations and Executive teams to extract value from data and support their data infrastructure needs.Design, build and maintain your vision of an optimal data pipeline architecture.Deploy data models that synergizes data democratization and business operations.Orchestrate the unity between our product data and external data sources, destinations and APIsBe the data life force of our company, continually improving the stages of our data infrastructure and managing the quality and consistency of our data', 'Extensive experience with SQL and Python scripting', 'Ability to execute on ETL design, implementation, and maintenance\xa0', '\xa0', ""Here's What We're Looking For"", 'Be the data life force of our company, continually improving the stages of our data infrastructure and managing the quality and consistency of our data']",Mid-Senior level,Full-time,Information Technology,Computer Games,2021-03-18 14:34:51
Data Engineer,Skills Pipeline,"Madison, WI",3 days ago,64 applicants,"['100% employer-funded short & long-term disability coverage', 'Collaborate with members of your team (eg, Software Developers, Customer Success) on the project goals', 'Salary paid semi-monthlyCompetitive medical, dental, and vision coverage100% employer-funded short & long-term disability coverage15 days of PTO and extra time off for holidaysRemote work throughout COVID-19 and the option to work remote two days per week once we return to the officeA pinball machine for when you really need a break', 'Experience with data analysis programming languages (i.e. Python/R, SAS, etc)', 'If you love data architecture, working with a close team and are interested in creating a more sustainable future this could be a great opportunity for you.\xa0', 'Build high-performance algorithms, predictive models, and prototypes', 'Integrate up-and-coming data management and software engineering technologies into existing data structures', 'Skills and qualifications:', 'Compile and provide data sets as requested by team members or clients', 'Execute customer onboarding verifying customer data feeds matches the uniformity of our data', 'Troubleshoot database-related application performance issues and recommend solutions', 'As a data engineer, you will play a key role in managing and improving the health and stability of their MySQL databases. You should know the ins and outs of industry standards on architecture, data modeling, security, and performance tuning, and have the ability to creatively apply solutions in collaboration with cross-functional teams to achieve business goals. The ideal candidate will excel at onboarding new customer data sets while applying their experience in data modeling and analytics to create greenhouse gas emissions and food waste dashboards for customers to track and improve their sustainability efforts.', 'Ensure that all systems meet our business requirements as well as industry', 'standards', 'Intellectual curiosity to find new and creative ways to solve data issues', 'Proven ability in executing and supporting strategies to ensure the health of enterprise database environments', 'Bonus: experience with LAMP stack development and RESTful APIs', 'Responsibilities:', 'Salary paid semi-monthly', 'Benefits + perks:\xa0', '15 days of PTO and extra time off for holidays', 'Remote work throughout COVID-19 and the option to work remote two days per week once we return to the office', 'Design, construct, install, test, and maintain data management systems', 'Bachelors degree in an applicable discipline or equivalent experience', 'Research new uses for existing data and recommend different ways to constantly improve data reliability and quality', 'On behalf of a software company Skills Pipeline is looking for a Data Engineer.', 'Design, construct, install, test, and maintain data management systemsBuild high-performance algorithms, predictive models, and prototypesEnsure that all systems meet our business requirements as well as industrystandardsTroubleshoot database-related application performance issues and recommend solutionsIntegrate up-and-coming data management and software engineering technologies into existing data structuresDevelop set processes for data modeling and analysisEmploy an array of technological languages and tools to connect systems togetherCollaborate with members of your team (eg, Software Developers, Customer Success) on the project goalsCompile and provide data sets as requested by team members or clientsResearch new uses for existing data and recommend different ways to constantly improve data reliability and qualityExecute customer onboarding verifying customer data feeds matches the uniformity of our data', 'Strong knowledge of the technical aspects of database security, enterprise backup and restore, and replication strategies', 'Develop set processes for data modeling and analysis', 'Bachelors degree in an applicable discipline or equivalent experienceProven ability in executing and supporting strategies to ensure the health of enterprise database environmentsStrong knowledge of the technical aspects of database security, enterprise backup and restore, and replication strategiesIntellectual curiosity to find new and creative ways to solve data issuesExcellent written and verbal communication, interpersonal and collaborative skillsExperience with data analysis programming languages (i.e. Python/R, SAS, etc)Bonus: experience with LAMP stack development and RESTful APIs', 'Competitive medical, dental, and vision coverage', 'Employ an array of technological languages and tools to connect systems together', 'Excellent written and verbal communication, interpersonal and collaborative skills', '\xa0', 'A pinball machine for when you really need a break']",Associate,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Engineer,GoHealth,"Charlotte, NC",2 weeks ago,137 applicants,"['', 'Strong analytical and problem solving ability with strong attention to detail and accuracy.', 'Knowledge of and experience with a version control system (such as Git, Mercurial, SVN, etc).', 'Familiarity with data warehousing platforms, such as Redshift, Snowflake, SQL Server, etc.', 'Bachelor’s Degree in computer science or equivalent experience required.', '401k program with company match', ' Open vacation policy 401k program with company match Medical, dental, vision, and life insurance benefits Flexible spending accounts Commuter and transit benefits Professional growth opportunities Casual dress code Generous employee referral bonuses Happy hours, ping-pong tournaments, and more company-sponsored events Subsidized gym memberships GoHealth is an Equal Opportunity Employer', 'Skills And Experience', 'Monitor and ensure operational stability of data pipelines.', 'Professional growth opportunities', 'Generous employee referral bonuses', 'Collaborate with the rest of the Engineering Team, subject matter experts and department leaders to understand, analyze, build and deliver new data-related processes and/or reports.', 'Happy hours, ping-pong tournaments, and more company-sponsored events', 'GoHealth is an Equal Opportunity Employer', 'Responsibilities', 'Design, develop and deploy batch and streaming data pipelines.', 'Proficiency in Java or Python programming languages.', ' Design, develop and deploy batch and streaming data pipelines. Monitor and ensure operational stability of data pipelines. Create and maintain documentation of the technical detail design, operational support and maintenance procedures for all data pipelines. Ensure data quality and compliance with development, architecture, reporting, and regulatory standards throughout entire data pipeline. Collaborate with the rest of the Engineering Team, subject matter experts and department leaders to understand, analyze, build and deliver new data-related processes and/or reports. ', ' Bachelor’s Degree in computer science or equivalent experience required. 4+ years of experience in the design and development of data pipelines and tasks. Strong analytical and problem solving ability with strong attention to detail and accuracy. Understanding of data warehousing concepts and dimensional data modeling. Hands-on experience with troubleshooting performance issues and fine tuning queries. Experience extracting data from relational and document databases. Experience consuming data over HTTP and in formats such as HTML, XML, and JSON. Knowledge of and experience with a version control system (such as Git, Mercurial, SVN, etc). Proficiency in Java or Python programming languages. Familiarity with data warehousing platforms, such as Redshift, Snowflake, SQL Server, etc. ', ""Due to the unprecedented situation of COVID-19, GoHealth has decided to protect our current and future employees by managing our business remotely. This is inclusive of interviewing, onboarding and each role day-to-day. Please consider that our roles will not be remote long-term and will return to an office setting once we're safe to do so following the guidance of local health authorities’ and the CDC."", 'Experience consuming data over HTTP and in formats such as HTML, XML, and JSON.', 'Create and maintain documentation of the technical detail design, operational support and maintenance procedures for all data pipelines.', 'Hands-on experience with troubleshooting performance issues and fine tuning queries.', 'Benefits And Perks', 'Flexible spending accounts', '4+ years of experience in the design and development of data pipelines and tasks.', 'Open vacation policy', 'Commuter and transit benefits', 'Subsidized gym memberships', 'Medical, dental, vision, and life insurance benefits', 'Ensure data quality and compliance with development, architecture, reporting, and regulatory standards throughout entire data pipeline.', 'Understanding of data warehousing concepts and dimensional data modeling.', 'Casual dress code', 'Experience extracting data from relational and document databases.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,ReviveMed,"Cambridge, MA",2 weeks ago,105 applicants,"['', 'BS, MS, or Ph.D. degree in Computer Science or related fields; 2-5 years of work experience preferred, but we will consider strong candidates with less experience.', '\xa0The Role', 'BS, MS, or Ph.D. degree in Computer Science or related fields; 2-5 years of work experience preferred, but we will consider strong candidates with less experience.Proven programming skills: We use Python along with shell scripts, R, JavaScript, HTML, and other programming languages as needed. Links to your publicly accessible programming work is a big plus (like GitHub, Bitbucket, or Gitlab).Extensive experience working with large relational and non-relational databases (SQL and NoSQL).The ability to think strategically about data architecture and build data pipelinesExperience with data handling, ETL, and schema designExperience building new solutions using Docker and the Kubernetes container orchestrationExtensive experiences with cloud computing platforms including AWS and GCPstrong analytical, oral, and writing skills', 'We are an award-winning enthusiastic team, passionate about innovative, data-driven approaches to bring the right therapeutics for the right patients, and ultimately save millions of lives.', 'Extensive experience working with large relational and non-relational databases (SQL and NoSQL).', '·\xa0\xa0\xa0\xa0\xa0\xa0Competitive compensation, health insurance, as well as share options', 'Experience building new solutions using Docker and the Kubernetes container orchestration', '\xa0Required Qualifications', 'strong analytical, oral, and writing skills', 'Proven programming skills: We use Python along with shell scripts, R, JavaScript, HTML, and other programming languages as needed. Links to your publicly accessible programming work is a big plus (like GitHub, Bitbucket, or Gitlab).', 'We are looking for an exceptional data science software engineer to join our team. We have a fast-paced, high-performance, and result-oriented culture. The preferred candidate will bring relevant experience, entrepreneurial drive, and a passion for advancing our vision.', 'The company', '·\xa0\xa0\xa0\xa0\xa0\xa0Opportunities for personal and professional growth as our company expands', '·\xa0\xa0\xa0\xa0\xa0\xa0Making an impact on the world with every line of code', 'Experience with data handling, ETL, and schema design', 'ReviveMed is an MIT spinout, artificial intelligence (AI)-driven drug discovery platform by uniquely leveraging data from small molecules or metabolites in the human body. Located in the heart of biotech innovation in Cambridge, ReviveMed uniquely overcomes the difficulties of identifying a large set of metabolites for each patient, based on technology that our team developed at MIT and published in Nature Methods. Our AI platform further combines the data from small molecules with other molecular data and translates these data into novel therapeutic insights for drug discovery. Currently, ReviveMed is collaborating with tier-one pharmaceutical companies and pursuing internal drug discovery, initially focused on metabolic diseases, including non-alcoholic fatty liver.', 'The ability to think strategically about data architecture and build data pipelines', 'Required Qualifications', '·\xa0\xa0\xa0\xa0\xa0\xa0Involvement in critical partnerships projects with pharmaceutical clients', 'Why you should join ReviveMed', '·\xa0\xa0\xa0\xa0\xa0\xa0An opportunity to learn about the applications of advanced artificial intelligence algorithms to drug discovery and development from the beginning', '\xa0', 'Extensive experiences with cloud computing platforms including AWS and GCP']",Mid-Senior level,Full-time,Information Technology,Biotechnology,2021-03-18 14:34:51
Data Infrastructure Engineer,Urgently Roadside Assistance,United States,2 days ago,Be among the first 25 applicants,"['', ""Bachelor's Degree in Computer Science or related field"", 'You have been involved in deploying data pipelines that involved collecting/streaming, storing and processing (ETL) the data for various business use cases.\xa0You have extensively worked on implementing Jenkins Pipelines\xa0You have\xa0been an integral part of a team working with structured, semi-structured and unstructured large data sets from real time/batch streaming data feeds.', 'YOUR MISSION:\xa0Your mission is to manage and optimize our cloud data platform. This means you will be responsible for working on a variety of data projects which include orchestrating our data pipelines using modern big data tools/stack and engineering our existing transactional processing systems.', 'Experience with deploying\xa0messaging and data pipeline tools like Apache Kafka, Amazon Kinesis etc.', 'Benefits = At Urgently, we have awesome benefits!\xa0We cover 100% of the cost of your dental and vision plans and we also provide short term and long term disability and life insurance to you all free of charge!\xa0We have two medical plans - a base plan that has a Health Savings account add on and a PPO option (you do have to pay for these).\xa0You’ll have 12 holidays off and unlimited paid time off..\xa0We match 100% on the first 3% you contribute to our 401(k)and then 50% of the next 2% you contribute. So, if you contribute 5% of your paycheck, we’ll match 4% of that.\xa0Free money!', 'Work with different teams to make data available for reporting and analytics.\xa0', 'Experience in writing\xa0Terraform/CloudFormation templates', '2) First 6 months:', 'Problem Solving: ', 'Proficient in Python\xa0', 'Ensure data flow and integrity between systems. Champion the flow of data across all our systems from end-to-end ensuring consistency across the chain.', 'Build deployment tools to provide blue/green and zero downtime for our services', 'Fine tune our performance with a focus on high availability and scalability.', 'Monitor our operations and security\xa0', 'Knowledge of Airflow or other workflow management systems in a distributed setup', 'Build CI/CD pipelines to deploy and monitor data pipelines\xa0Architect and build data infrastructure not available off-the-shelf.Build deployment tools to provide blue/green and zero downtime for our servicesEnsure data flow and integrity between systems. Champion the flow of data across all our systems from end-to-end ensuring consistency across the chain.', 'You should also know that we adhere to these same principles in all aspects of employment, including, but not limited to, recruitment, hiring, job assignment, compensation, promotion, access to benefits and training, discipline, and termination of employment.\xa0', 'Proficient in Jenkins CI/CD pipelines\xa0', 'Follow AWS best practices to deploy services\xa0', 'Manager = You’ll report to the Technical Manager Platform Infrastructure', 'Experience deploying and maintaining EMR, Glue, Athena, RDS', 'WHO YOU ARE:', 'Understand our platform development environment and philosophy.Understand our cloud architecture and applications’ infrastructure.Understand our engineering teams’ work culture.', 'WHAT YOU’LL BE RESPONSIBLE FOR:', 'Build CI/CD pipelines to deploy and monitor data pipelines\xa0', 'You pride yourself on working collaboratively with all of your teammates.\xa0You are transparent in your communication and proactively share what others need to be aware of.', 'Nice to Haves:', 'YOUR LEGACY:\xa0', 'Applicants with disabilities may be entitled to reasonable accommodation under the Americans with Disabilities Act and similar applicable state laws. Please let us know if you need assistance completing any forms or to otherwise participate in the application process', 'In depth and demonstrable knowledge of AWS cloud\xa0', 'Location = Great news!\xa0You have the option of working from anywhere in the U.S.!\xa0\xa0\xa0', 'Location = Great news!\xa0You have the option of working from anywhere in the U.S.!\xa0\xa0\xa0Manager = You’ll report to the Technical Manager Platform InfrastructureCompensation = Commensurate with experience for a company of Urgently’s sizeBenefits = At Urgently, we have awesome benefits!\xa0We cover 100% of the cost of your dental and vision plans and we also provide short term and long term disability and life insurance to you all free of charge!\xa0We have two medical plans - a base plan that has a Health Savings account add on and a PPO option (you do have to pay for these).\xa0You’ll have 12 holidays off and unlimited paid time off..\xa0We match 100% on the first 3% you contribute to our 401(k)and then 50% of the next 2% you contribute. So, if you contribute 5% of your paycheck, we’ll match 4% of that.\xa0Free money!', 'Architect and build data infrastructure not available off-the-shelf.', '1) First 3 months:', 'YOUR MISSION:\xa0', ""At Urgently, we don’t just accept difference - we celebrate it, we support it, and we thrive on it for the benefit of our employees, our customers and the communities where we operate. We are an equal opportunity employer which means that employment and advancement at Urgently is based on a person's merit and qualifications.\xa0"", ""Bachelor's Degree in Computer Science or related fieldIn depth and demonstrable knowledge of AWS cloud\xa0Proficient in Jenkins CI/CD pipelines\xa0Experience deploying and maintaining EMR, Glue, Athena, RDSExperience deploying and maintaining\xa0data warehousing technologies such as Amazon Redshift and Google BigQueryExperience with deploying\xa0messaging and data pipeline tools like Apache Kafka, Amazon Kinesis etc.Proficient in Python\xa0Experience in writing\xa0Terraform/CloudFormation templatesExperience using and/or implementing modern observability tooling such as Prometheus, InfluxDB, Grafana, Logstash, Kibana or Jaeger.\xa0"", 'Understand our engineering teams’ work culture.', 'Team Member: ', 'Technical Expertise:\xa0', 'Monitor table schemas (i.e. partitions, compression, distribution) to minimize costs and achieve maximize performance', 'Experience using and/or implementing modern observability tooling such as Prometheus, InfluxDB, Grafana, Logstash, Kibana or Jaeger.\xa0', 'You have been involved in deploying data pipelines that involved collecting/streaming, storing and processing (ETL) the data for various business use cases.\xa0', '4) Ongoing...\xa0', 'You have\xa0been an integral part of a team working with structured, semi-structured and unstructured large data sets from real time/batch streaming data feeds.', 'Experience deploying and maintaining\xa0data warehousing technologies such as Amazon Redshift and Google BigQuery', 'YOUR LEGACY:\xa0Your legacy will include ensuring that every Urgently team across our growing organization is able to access data from any source within minutes, none of our data is ever lost and build immutable data infrastructure.\xa0Your efforts will ensure that Urgently becomes the world’s leading mobility assistance platform.\xa0The result? \xa0You helped Urgently become the world’s leading mobility assistance company.\xa0\xa0', 'We do not discriminate against any applicant or employee on the basis of race, ethnicity, color, national origin, ancestry, citizenship, religion, creed, sex (including pregnancy, childbirth, breastfeeding or related medical conditions), sexual orientation, gender, gender identity or expression, physical or mental disability, or any other characteristic protected by federal, state or local law.\xa0', 'You have extensively worked on implementing Jenkins Pipelines\xa0', 'For more information, visit www.geturgently.com', 'Monitor table schemas (i.e. partitions, compression, distribution) to minimize costs and achieve maximize performanceFine tune our performance with a focus on high availability and scalability.Work with different teams to make data available for reporting and analytics.\xa0Monitor our operations and security\xa0Follow AWS best practices to deploy services\xa0', 'Compensation = Commensurate with experience for a company of Urgently’s size', 'THE NITTY GRITTY:', '\xa0', 'Industry Experience:\xa0', 'Understand our cloud architecture and applications’ infrastructure.', 'Team Member: \xa0', 'Understand our platform development environment and philosophy.', 'OUR FAIR HIRING PRACTICES:', '\xa0The result? ', 'You are known for proactively solving problems before they can become real problems.\xa0You are the kind of person who is constantly upgrading your skill sets and is always looking for ways to enhance the data platforms you’re working on.\xa0\xa0\xa0']",Mid-Senior level,Full-time,Information Technology,Automotive,2021-03-18 14:34:51
Data Engineer,Yum! Brands,"Irvine, CA",3 weeks ago,41 applicants,"['', 'Required Skills', ' Experience integrating data from multiple data sources and file types such as JSON, Parquet and Avro formats.', ' Build out a robust big data ingestion framework with automation, self-heal capabilities and ability to handle data drifts', ' Design and code data pipeline features and data processing jobs for collecting various data from customer, operations and financials systems', ' 3+ years of experience working in Redshift and AWS Lambda.', ' Onsite dining center (yes, you can eat KFC, Taco Bell or Pizza hut every day!)', ' 4 weeks of vacation per year plus holidays 2 paid days off per year to volunteer Onsite childcare through Bright Horizons Onsite dining center (yes, you can eat KFC, Taco Bell or Pizza hut every day!) Onsite dry cleaning, laundry services, concierge Onsite gym with fitness classes and personal trainer sessions Tuition reimbursement, education benefits and scholarship opportunities Discounts for life’s adventures (ex: theme parks, wireless plans, etc.) Generous parental leave for all new parents and adoption assistance program 401(k) with a 6% matching contribution from Yum! Brands with immediate vesting Comprehensive medical, vision and dental including prescription drug benefits and 100% preventive care Recognition based culture and unique, fun events year round Healthcare and dependent care flexible spending accounts Company paid life insurance Grow Yourself Week which is devoted to your personal development', ' Provide scalable solutions to manage large file imports', ' 4+ years of experience designing and delivering large scale, 24-7, mission-critical data pipelines and features using modern big data architectures', ' 4+ years of hands-on experience in ETL tools such as Talend & Python and SQL.', ' 4+ years of experience designing and delivering large scale, 24-7, mission-critical data pipelines and features using modern big data architectures 4+ years of hands-on experience in ETL tools such as Talend & Python and SQL. 3+ years of experience working in Redshift and AWS Lambda. Strong understanding of ETL processing with large data stores using AWS Data Lake Formation and AWS Glue Strong experience in data quality tools such as Informatica DQ or Talend DQ tools Experience in ETL and ELT workflow management Familiarity with AWS Data and Analytics technologies such as Athena, Redshift Spectrum Intermediate knowledge of BI best practices, relational structures, dimensional data modeling, structured query language (SQL) skills, data warehouse and reporting techniques BA/BS in related field, or equivalent experience Experience integrating data from multiple data sources and file types such as JSON, Parquet and Avro formats. Experience integrating data using API integration from Salesforce Experience integrating data using streaming technologies such as Kinesis Firehose, Kafka Knowledge working with git or any other version control system. Experience working in Data Ops environment', ' Work with real-time data streams & API’s from multiple internal/external sources', ' Discounts for life’s adventures (ex: theme parks, wireless plans, etc.)', ' Build visual data quality metrics in the data warehouse load processes', ' Comprehensive medical, vision and dental including prescription drug benefits and 100% preventive care', ' Experience integrating data using streaming technologies such as Kinesis Firehose, Kafka', ' Company paid life insurance', ' Familiarity with AWS Data and Analytics technologies such as Athena, Redshift Spectrum', ' Onsite gym with fitness classes and personal trainer sessions', ' Onsite dry cleaning, laundry services, concierge', ' Recognition based culture and unique, fun events year round', ' Intermediate knowledge of BI best practices, relational structures, dimensional data modeling, structured query language (SQL) skills, data warehouse and reporting techniques', ' Knowledge working with git or any other version control system.', ' 2 paid days off per year to volunteer', ' Write ETL pipelines to implement pre-defined business rules and metrics', ' Experience integrating data using API integration from Salesforce', ' Generous parental leave for all new parents and adoption assistance program', ' Healthcare and dependent care flexible spending accounts', ' Design and code data pipeline features and data processing jobs for collecting various data from customer, operations and financials systems Build out a robust big data ingestion framework with automation, self-heal capabilities and ability to handle data drifts Work with real-time data streams & API’s from multiple internal/external sources Write ETL pipelines to implement pre-defined business rules and metrics Build visual data quality metrics in the data warehouse load processes Provide scalable solutions to manage large file imports', ' Experience in ETL and ELT workflow management', ' Grow Yourself Week which is devoted to your personal development', ' 401(k) with a 6% matching contribution from Yum! Brands with immediate vesting', 'Check Out Some Of Our Great Benefits', ' BA/BS in related field, or equivalent experience', ' Strong understanding of ETL processing with large data stores using AWS Data Lake Formation and AWS Glue', ' 4 weeks of vacation per year plus holidays', ' Tuition reimbursement, education benefits and scholarship opportunities', ' Onsite childcare through Bright Horizons', ' Strong experience in data quality tools such as Informatica DQ or Talend DQ tools', ' Experience working in Data Ops environment']",Not Applicable,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Cloud Data Engineer,"KellyMitchell Group, Inc","Chicago, IL",1 day ago,Be among the first 25 applicants,"['', 'Recommend ways to improve data reliability, efficiency and quality', 'Analyzes and estimates feasibility, costs, time, and resources needed to develop, and implement enterprise datasets as needed', 'Desired Skills/Experience:', 'Experience with development and data warehouse requirements gathering, analysis and design', 'Recent experience using cloud data engineering toolsets', 'Recent experience in Azure using Azure Data Factory, Azure Databricks and Snowflake', 'Strong understanding of enterprise integration patterns (EIP) and data warehouse modeling', 'Duties:', 'Updates and creates azure pipelines to support our continuous deployment model', 'Build and maintain the data warehouse pipelines', 'Ensures physical data models align with best practice and requirements', 'Research opportunities for data acquisition and new uses for existing data', 'Build and maintain raw data pipelines from varied sources', '5 years relevant experience of development using integration platforms', 'Cloud Data Engineer', 'Collaborate with Enterprise Architecture to publish and contribute to architecture standards and roadmaps', 'Work closely with business and technical teams to deliver enterprise grade datasets that are reliable, flexible, scalable, and provide low cost of ownershipEnsures physical data models align with best practice and requirementsBuild and maintain raw data pipelines from varied sourcesBuild and maintain the data warehouse pipelinesUpdates and creates azure pipelines to support our continuous deployment modelRecommend ways to improve data reliability, efficiency and qualityAnalyzes and estimates feasibility, costs, time, and resources needed to develop, and implement enterprise datasets as neededResearch opportunities for data acquisition and new uses for existing dataRecommend ways to improve data reliability, efficiency and qualityCollaborate with Enterprise Architecture to publish and contribute to architecture standards and roadmapsAchieves and maintains relevant technical competencies and helps to foster an environment of continued growth and learning among colleagues on existing and emerging technologies', ""Bachelor's Degree in Computer Science or related field\xa05 years relevant experience of development using integration platformsRecent experience using cloud data engineering toolsetsRecent experience in Azure using Azure Data Factory, Azure Databricks and SnowflakeMinimum of 2 years experience building database tables and modelsMust be able to write TSQL for DDL and DML operations fluentlyStrong understanding of enterprise integration patterns (EIP) and data warehouse modelingExperience with development and data warehouse requirements gathering, analysis and designPossess strong business acumen and consistently demonstrates forward thinking"", 'Achieves and maintains relevant technical competencies and helps to foster an environment of continued growth and learning among colleagues on existing and emerging technologies', 'KellyMitchell matches the best IT and business talent with premier organizations nationwide. Our clients, ranging from Fortune 500 corporations to rapidly growing high-tech companies, are exceptionally served by our 1500+ IT and business consultants. Our industry is growing rapidly, and now is a great time to launch your career with the KellyMitchell team.', 'Possess strong business acumen and consistently demonstrates forward thinking', 'Must be able to write TSQL for DDL and DML operations fluently', 'Minimum of 2 years experience building database tables and models', ""Job Summary: Our client\xa0is looking for associates with a passion for excellence and the ability to exceed our customers' expectations - attributes that have been keys to our success. If that sounds like you, then our client offers the opportunity to learn, grow, and to make a difference in a friendly and welcoming environment."", ""Bachelor's Degree in Computer Science or related field\xa0"", 'Job Summary: ', 'Work closely with business and technical teams to deliver enterprise grade datasets that are reliable, flexible, scalable, and provide low cost of ownership']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Carnegie Mellon University,"Pittsburgh, PA",1 day ago,Be among the first 25 applicants,"['Make hazardous jobs safer', 'Accelerate screening of pharmaceuticals Take Control of Your Career!', 'Maintain work/life balance', 'CMake, Valgrind, and other development tools.', 'Supportive of a non-standard schedule', 'Proficient Matlab or R skills.', 'Data preparation, integration, verification.', 'Leading projects and fostering client relationships', 'Proficient C or C++ skills.', 'Select the career pathway that interests you', 'Transitioned into a robotics, program management, technical lead, or software architecture roles', 'High performance computing tools (Hadoop, AWS, Azure, etc.).', 'Front end (Data visualization, exploration, labeling).', 'Receiving mentoring from senior engineers and robotics experts', 'Professional software development processes.', 'Mixed software and hardware architecture.', 'Remove waste from farming = more food', 'MLOps tools (airflow, mlflow, etc.).', 'Staying connected with academic and recreational activities at Carnegie Mellon', 'Ingestion and integration of disparate sources.', 'Participating in the software process: design, code reviews, etc.', ""Outreach to K-12 and college-level students At NREC, we value diversity, support it, and thrive on it for the benefits of our organization, our employees and our community. Carnegie Mellon University is an Equal Opportunity Employer/Disability/Veteran.Business OperationsUnited States-Pennsylvania-PittsburghFull TimeMaster's Degree or equivalenttrue"", 'Model training infrastructure tools (tensorflow, caffe, Petuum, etc.).', 'Development in a Linux environment. Required Qualifications:', 'Proficient development skills (Python preferred).', 'Becoming an in-depth expert in a technical area', 'Taking courses at Carnegie Mellon', 'Developing, documenting, testing, and fixing software.', 'Demonstrated understanding and use of software engineering concepts, practices, and procedures.', 'Linux development experience.', 'Adapting and integrating proprietary and open source software packages and APIs.', 'Mentoring junior engineers', 'Data infrastructure tools (SQL, NoSQL, data version control, etc.).', 'Ability to participate in a multi-disciplinary team. We especially want to hear from you if you have experience or qualifications in ANY of the following areas:', 'B.S. in Computer Science, Engineering, Mathematics is required (Any more is a bonus).', 'Influence the direction of projects', 'Technical communication skills.', 'Networking interfaces and applications.', 'Supporting development and acquisition of training and inference hardware.', 'Performing consulting during off time', 'Switch between part-time and full-time as life demands NREC is at the center of the robotics ecosystem in Pittsburgh, PA. With over 60 robotics companies, Pittsburgh has become the robotics capital of the world. Geek Wire calls it Robotics Row; others call it Roboburgh. Join the leader in the most exciting time in robotics! Join the best robotics R & D group Join our talented team at NREC, an operating unit within the world-renowned Robotics Institute at Carnegie Mellon University. NREC has 20+ years of experience and is globally renowned for developing and deploying robots into many applications across multiple sectors, such as agriculture, mining, defense, energy, and manufacturing. We strive to provide solutions for real world challenges where automation and robots have greater impact on productivity and improve the safety and comfort of the labor force. Our unique expertise places us at the forefront of unmanned ground vehicle design, autonomy, sensing and perception, machine learning, machine vision, operator assistance, 3D mapping and position estimation. With over 120 robotics professionals, we can solve challenges that no other organization can. NREC also leads in educational outreach through its Robotics Academy, which builds robotics curricula and software for K-12 and college-level students. Your primary responsibilities include:', 'Developing software for machine learning and data-science applications.', 'Tools and protocols for reproducible research and data analysis.', 'Improve efficiency in industry & manufacturing', '3-5 years of relevant experience is required.', 'Cloud, high performance, and distributed computing.', 'Inference model deployment tools (Docker or other containerization, etc.).', 'Computer vision, robotics, machine learning, scientific computing, simulation, or graphics. Opportunities people at NREC have seized: As a member of NREC, you have the opportunity to take control of your career. People in similar roles have shaped their careers to suit their interests and their needs.', 'Data processing (pre-processing, augmentation, post-processing).', 'Architecting big-data pipelines.', 'Make industrial processes environmentally friendly']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Electronic Arts (EA),"Seattle, WA",2 weeks ago,115 applicants,"['', ' Design, develop, and maintain data pipelines and backend services for real-time decisioning, reporting, optimization, data collection, and related functions ', ' Create and maintain SLAs on data reporting through reports and dashboards ', ' Onboard new data to support business needs ', ' Design, develop, and maintain data pipelines and backend services for real-time decisioning, reporting, optimization, data collection, and related functions  Design efficient data structures and database schemas  Incorporate data processing and workflow management tools into pipeline design  Onboard new data to support business needs  Use tools, debugging logs, performance metrics, and other data sources to make code- and application-level improvements  Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.  Create and maintain SLAs on data reporting through reports and dashboards  Work collaboratively and communicate effectively with stakeholders  Experiment with and recommend new technologies that simplify or improve the tech stack ', ' Master degree required in a engineering discipline (i.e. Computer Science, Industrial Engineering or equivalent) ', ' Design efficient data structures and database schemas ', ' Results-driven and highly quantitative ', ' Keep our data separated and secure across national boundaries through multiple data centers and AWS regions. ', ' Experiment with and recommend new technologies that simplify or improve the tech stack ', 'Skills And Experience', ' Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.  Experience with big data tools: Hadoop, Spark, Kafka, AWS etc.  Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.  Experience with data pipeline and workflow management tools: Airflow, Azkaban, Luigi, etc.  Experience with AWS cloud services: EC2, EMR, RDS, Redshift. Or similar experience with GCP or Azure.  Experience with stream-processing systems: Storm, Spark-Streaming, etc.  Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.  Master degree required in a engineering discipline (i.e. Computer Science, Industrial Engineering or equivalent)  3+ years experience in data technologies as data engineer or related roles', 'Responsibilities', ' Experience with AWS cloud services: EC2, EMR, RDS, Redshift. Or similar experience with GCP or Azure. ', ' Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases. ', ' EA Security  ', 'Requirements', ' True passion for understanding customer behavior on-platform and in-game ', ' The Challenge Ahead ', ' Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc. ', ' Strong interpersonal and communication skills ', ' Experience with data pipeline and workflow management tools: Airflow, Azkaban, Luigi, etc. ', ' Use tools, debugging logs, performance metrics, and other data sources to make code- and application-level improvements ', ' Experience with big data tools: Hadoop, Spark, Kafka, AWS etc. ', 'EA Security ', ' Work collaboratively and communicate effectively with stakeholders ', ' Incorporate data processing and workflow management tools into pipeline design ', 'Data Engineer ', ' 3+ years experience in data technologies as data engineer or related roles', ' Able to multi-task and thrive in a dynamic, fast-paced environment ', 'Data Engineer', ' Able to multi-task and thrive in a dynamic, fast-paced environment  Strong problem solving skills and ability to collaborate within a team environment  Results-driven and highly quantitative  True passion for understanding customer behavior on-platform and in-game  Strong interpersonal and communication skills ', ' Strong problem solving skills and ability to collaborate within a team environment ', ' Experience with relational SQL and NoSQL databases, including Postgres and Cassandra. ', ' Experience with stream-processing systems: Storm, Spark-Streaming, etc. ']",Not Applicable,Full-time,Strategy/Planning,Computer Games,2021-03-18 14:34:51
Data Engineer,Blueprint Technologies,"Myrtle Point, OR",4 days ago,Be among the first 25 applicants,"['', 'Qualifications', 'Excellent organization skills and able to multi-task and detailed oriented ', ' Excellent verbal and written communication skills (must be able to write clear and concise emails for any audience, etc. ', ' At least 3-years of experience with SQL, Python and/or other data collection tools & reporting ', ' At least 5-years of experience as a software development or data engineer  At least 3-years of experience with SQL, Python and/or other data collection tools & reporting  Advanced knowledge and skills with AWS is required  Experience with Pyspark or Scala is a plus (Databricks or Spark) Excellent collaboration skills to work on a team as well as independently (be self-reliant and resourceful)Excellent organization skills and able to multi-task and detailed oriented  Excellent verbal and written communication skills (must be able to write clear and concise emails for any audience, etc. ', ' Experience with Pyspark or Scala is a plus (Databricks or Spark) ', ' FLSA - Job Classification: ', 'Excellent collaboration skills to work on a team as well as independently (be self-reliant and resourceful)', ' At least 5-years of experience as a software development or data engineer ', ' Data Engineer ', 'Location:', 'What does Blueprint do?', 'By submitting your interest in this job, you agree to receive text notifications with additional steps to complete your job application. You will receive up to 6 messages from the number ""63879"". Message & data rates may apply. Please refer to our privacy policy for more information.', 'What will I be doing?', 'Who is Blueprint?', 'Why Blueprint?', ' Advanced knowledge and skills with AWS is required ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Borrego,"San Diego, CA",2 days ago,Be among the first 25 applicants,"['Borrego is seeking a\xa0Data Engineer to join our growing Data & Analytics team!\xa0 We are building an analytics platform for business and operation intelligence. At Borrego, we want to help business users to make data-driven decisions. Our platform will allow businesses to answer ""what"", ""when"" and ""how"" questions, as well as allow them to ask ""what if"". In this role, you will design, develop, maintain and support our Enterprise Data Warehouse & BI platform within Borrego, using various data & BI tools.\xa0 This position offers a unique opportunity to make a significant impact on the entire organization in developing data tools and driving a data-driven culture.\xa0\xa0The Data Engineer will report to the Director of Applications Development.', ""Borrego's approach to compensating our employees is unique and progressive. We offer a casual community-based workplace that is upbeat and hard working. We strive for quality workmanship and place a large emphasis on customer satisfaction. We promote from within and are looking for career minded individuals, looking to advance in the growing photovoltaic industry. We offer pay commensurate with experience along with excellent benefits including:"", 'Develop collaborative relationships and work with key business owners, IT resources to gather requirements and for the efficient resolution of requests to help them observe patterns and spot anomalies', 'Working with a system at scale and with Docker/Kubernetes/Jenkins CI/CD pipeline', 'Build processes supporting data transformation, data structures, metadata, dependency and workload management', 'DESIRED SKILLS:', 'Borrego is a national leader in solar & energy storage, with strong and steady leadership and an outstanding culture of teamwork, integrity, and a commitment to continuous improvement. Unique in our industry, Borrego has steadily grown and remained profitable every year over the last decade &mdash; and we’re continuing to grow across all areas of our company.', 'Competitive base salaryPotential for bonusesComprehensive benefits package including dental, vision, health, life, and disability insuranceSelf-managed flexible work schedules and time-off policies401(k) plan with company match and immediate vestingContinuing education and professional development assistance', 'Build ad-hoc applications as needed, to support more curious data users and to provide automation as possible', 'Our mission is “to solve the world’s energy problems by accelerating the adoption of renewable energy.” To date we have developed and built more than 640 Megawatts of solar while maintaining a reputation for quality, deep staff expertise, and a focus on building long-term relationships based on trust and transparency. But being the largest private commercial solar and energy storage company in the United States is not enough. Solving the world’s energy problems means moving from Megawatts to Gigawatts, and that is what we are going to do. Our 5-year goal is to be the leading solar and energy storage provider in the United States.\xa0Be a Part of Our Mission. Join the GigaWatt Revolution!', 'Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement', 'Experience with Firestore with focus on performance', 'Potential for bonuses', 'Experience supporting and working with cross-functional teams in a dynamic environment', '5+ years building data catalog, data dictionary, and make data searchable for our stakeholders and users', 'Comprehensive benefits package including dental, vision, health, life, and disability insurance', 'Take ownership of deployment and release process', '5+ years of experience building and optimizing google BigQuery data pipelines, architectures and data sets', '401(k) plan with company match and immediate vesting', 'REQUIRED SKILLS AND EXPERIENCE:', 'ABOUT BORREGO', 'Create ETL/ELT pipelines using Python, Airflow', 'Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field', 'Strong analytic skills related to working with unstructured datasets and big query expertise', 'Keep up to date on relevant technologies and frameworks', 'Communicate technical and business topics, as appropriate, in a 360 degree fashion, when required; communicate using written, verbal and/or presentation materials as necessary', 'Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases', 'Develop, enforce, and recommend enhancements to Applications in the area of standards, methodologies, compliance, and quality assurance practices; participate in design and code walkthroughs', 'Experience with object-oriented/object function scripting languages: Python, Java, Scala, etc.', 'Work experience with Tableau', 'Competitive base salary', 'A successful history of manipulating, processing and extracting value from large disconnected datasets', 'Be a Part of Our Mission. Join the GigaWatt Revolution', 'Borrego provides equal employment opportunities to all employees and applicants without regard to race, color, religion, gender, age, sexual orientation, national origin, ancestry, disability, genetics, veteran status or any other characteristic protected by state, federal and local laws. In addition to federal law requirements, Borrego complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities.', 'Continuing education and professional development assistance', 'Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databasesExperience with object-oriented/object function scripting languages: Python, Java, Scala, etc.5+ years building data catalog, data dictionary, and make data searchable for our stakeholders and users5+ years of experience building and optimizing google BigQuery data pipelines, architectures and data setsExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvementStrong analytic skills related to working with unstructured datasets and big query expertiseExperience with Firestore with focus on performanceBuild processes supporting data transformation, data structures, metadata, dependency and workload managementA successful history of manipulating, processing and extracting value from large disconnected datasetsExperience supporting and working with cross-functional teams in a dynamic environmentGraduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field', 'COMPENSATION', 'Work experience with TableauExperience with data science tools such as Pandas, Numpy, RWorking with a system at scale and with Docker/Kubernetes/Jenkins CI/CD pipeline', 'Utilize technical and domain knowledge to develop and implement effective solutions; provide hands on mentoring to team members through all phases of the Systems Development Life Cycle (SDLC) using Agile practices', 'Data Engineer', 'OUR MISSION', 'Work in a time-constrained environment to analyze, design, develop, and deliver Enterprise Data Warehouse solutions', 'Experience with data science tools such as Pandas, Numpy, R', 'Work in a time-constrained environment to analyze, design, develop, and deliver Enterprise Data Warehouse solutionsCreate ETL/ELT pipelines using Python, AirflowWork with various Google Cloud Platform (GCP) technologies, Metadata Management tool, Data Quality (DQ) tool, Cloud-Native Microservice architecture, CI/CD, Dev/OpsBuild ad-hoc applications as needed, to support more curious data users and to provide automation as possibleWork with systems that handle sensitive data with strict SOX controls and change management processesDevelop collaborative relationships and work with key business owners, IT resources to gather requirements and for the efficient resolution of requests to help them observe patterns and spot anomaliesCommunicate technical and business topics, as appropriate, in a 360 degree fashion, when required; communicate using written, verbal and/or presentation materials as necessaryDevelop, enforce, and recommend enhancements to Applications in the area of standards, methodologies, compliance, and quality assurance practices; participate in design and code walkthroughsUtilize technical and domain knowledge to develop and implement effective solutions; provide hands on mentoring to team members through all phases of the Systems Development Life Cycle (SDLC) using Agile practicesTake ownership of deployment and release processKeep up to date on relevant technologies and frameworks', 'This is a full-time, permanent position.', '\xa0', 'POSITION RESPONSIBILITIES WILL INCLUDE, BUT ARE NOT LIMITED TO:', 'Work with systems that handle sensitive data with strict SOX controls and change management processes', 'EEO M/F/D/V', 'Self-managed flexible work schedules and time-off policies', 'Work with various Google Cloud Platform (GCP) technologies, Metadata Management tool, Data Quality (DQ) tool, Cloud-Native Microservice architecture, CI/CD, Dev/Ops']",Mid-Senior level,Full-time,Information Technology,Renewables & Environment,2021-03-18 14:34:51
Data Engineer,Alexander Technology Group,"Portsmouth, NH",1 week ago,70 applicants,"['', 'Advanced Microsoft Office skills:\xa0Excel, PowerPoint, Outlook', 'SAP experience strongly preferred', 'The Alexander Technology Group is looking for a Data Engineer for a client in the Seacoast NH Area. This will be a permanent opportunity and applicants must be local to New Hampshire. ', 'Qualifications ', 'Demonstrated professionalism in written and verbal communications', 'SSIS – SQL server Integration services', 'Undergraduate degree required – Computer Science', 'Azure DB', 'SSIS – SQL server Integration servicesSSAS – SQL server Analysis servicesAzure DBUndergraduate degree required – Computer SciencePower BI experience 1-3 yearsSAP experience strongly preferredAdvanced Microsoft Office skills:\xa0Excel, PowerPoint, OutlookDemonstrated professionalism in written and verbal communicationsAccuracy and attention to detailCommitment to meeting deadlines and continuous improvement mindset', 'If interested, please send resume to Jpolombo@alexandertg.com', 'Power BI experience 1-3 years', 'Commitment to meeting deadlines and continuous improvement mindset', 'Accuracy and attention to detail', 'SSAS – SQL server Analysis services']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer II,Microsoft,"Redmond, WA",3 days ago,47 applicants,"['', 'Qualifications', ""3+ years' experience in an Analytics or Data Engineer or Software Engineer role with focus on big data processing"", ""3+ years' experience in an Analytics or Data Engineer or Software Engineer role with focus on big data processingExperience building and optimizing “big data” data workflows including ETL, business intelligence pipelines and data visualization tools such as Power BI."", 'Experience with relational databases as well as working familiarity with a variety of big data sources in Scope, Spark, Scala and/or other big data systems.', 'Demonstrated proficiency in writing complex highly optimized queries across diverse data sets and implementing data science applications in R and Python using cloud-based tools and notebooks (Jupyter, DataBricks, etc). ', 'Responsibilities', 'Experience with relational databases as well as working familiarity with a variety of big data sources in Scope, Spark, Scala and/or other big data systems.Demonstrated proficiency in writing complex highly optimized queries across diverse data sets and implementing data science applications in R and Python using cloud-based tools and notebooks (Jupyter, DataBricks, etc). Excellent written and verbal communication skills', 'Excellent written and verbal communication skills', 'Build and maintain the team’s grading pipeline for data labelling.', 'Help the team answer business questions with telemetry datCommunicate key insights and results with stakeholders and leadership. Interpret data/insights and uses storytelling and data visualization to recommend product decisions', 'Requirements', 'Experience building and optimizing “big data” data workflows including ETL, business intelligence pipelines and data visualization tools such as Power BI.', 'Design, develop, and maintain data pipelines and back-end services for ML experimentation, investigations, reporting, data collection, and related functions.', 'Build and maintain reports and dashboards that monitor KPIs, the identity attack ecosystem and other indicators that help us understand the threat landscape and the accuracy and performance of our detections.', 'Preferred Qualifications', 'Design, develop, and maintain data pipelines and back-end services for ML experimentation, investigations, reporting, data collection, and related functions.Build and maintain reports and dashboards that monitor KPIs, the identity attack ecosystem and other indicators that help us understand the threat landscape and the accuracy and performance of our detections.Build and maintain the team’s grading pipeline for data labelling.Help the team answer business questions with telemetry datCommunicate key insights and results with stakeholders and leadership. Interpret data/insights and uses storytelling and data visualization to recommend product decisions']",Not Applicable,Full-time,Information Technology,Computer Hardware,2021-03-18 14:34:51
Data Engineer,Haystax,"McLean, VA",2 days ago,Be among the first 25 applicants,"['', 'Requirements:', 'Ability to interact well with both internal teams and external customers.', 'Haystax Technology (a subsidiary of the Fishtech Group) is a fast-moving startup working on the cutting edge of threat analytics, providing solutions to government and commercial customers interested in finding a better way to calculate, prioritize, and address their most challenging risks. From determining the likelihood that certain events are fraudulent attacks to predicting and preventing insider threats by deploying behavioral analytics in a truly unique way, Haystax is out in front of the most damaging and complex human, organizational, and economic risks.', 'Willing to dive deeper into development of customized solutions using programming language such as Python.', 'Analyze data sources and data flows working with both structured and unstructured data.', 'Key Responsibilities:', 'In this role, the Haystax Data Analytics Engineer will engage with our customers to assist with data analytics related tasks. In addition, the engineer will engage with Haystax’s Product Development and Data Analytics teams to help with innovations related to the insider threat, Haystax risk models and related security risk management solutions. In this role, the engineer will also assist with the development of cloud solutions and automation algorithms. The ideal candidate must be comfortable with elementary logic and probability and demonstrate great communication and problem-solving skill sets. This role focuses on a balance between customer facing data analytics engagements, assisting the data analytics team in model improvements and the development of new customized automated solutions.', 'Excellent written and verbal communication skills.', 'Prior knowledge of Elasticsearch DSL language is a plus.', 'Minimum 5 years of experience working to support business applications and/or application development.', 'Perform the extraction, ingestion, augmentation and aggregation of data from multiple cyber and non-cyber data sources.', 'Preferred Experience:', 'Monitor ongoing performance of platform integrations, identify issues, troubleshoot and escalate issues as needed.', 'Collaborate with the data analytics and customer success teams to prioritize business related tasks.', 'A good understanding of Unix/Linux (CentOS, RHEL, Ubuntu) command line interface including proficient with general system commands and text editors (i.e. ViM, Nano, Emacs)', 'A desire to dive deeper into full-stack development is a plus', 'Perform data analytics on data sources specifically focused on Haystax Technology insider threat model.', 'Travel – as necessary, based on customer requests and requirements (once travel resumes).', 'Working knowledge or familiarity with NoSQL database technologies such as MongoDB, Elasticsearch.', 'A good level of experience and understanding of alerts and data sets from multiple security controls is a plus.Desire understanding of AWS infrastructure – Command Line Interface (AWS CLI), S3, AWS Console.Prior knowledge of Elasticsearch DSL language is a plus.Fundamental understanding of Bayesian statistics is a plus.Willing to dive deeper into development of customized solutions using programming language such as Python.A desire to dive deeper into full-stack development is a plus', 'Work with the data analytics team to test and improve Haystax Technology models.', 'Interpret customer data, analyze and tune the results using various statistical analysis techniques and machine learning algorithms.', 'Fundamental understanding and experience with regression, clustering, XGBoost, DBSCAN and linear machine learning algorithms.', 'Experience with container technologies like Docker/Kubernetes', 'Assist in the development of cloud and automation solutions to improve ongoing tasks.', ""Bachelor's degree in data science, data analytics or computer science"", 'A good level of experience and understanding of alerts and data sets from multiple security controls is a plus.', 'Maintain day-to-day relationships with your technical counterparts at the existing projects.', 'Experience at working both independently and in a fast-paced, team-oriented environment.', 'Fundamental skills in Python development and a knowledge and experience of manipulating CSV, JSON, HTML, and XML file formats using machine learning algorithms.', 'Experience with manipulation of data from high-volume and multiple data sets.', 'Fundamental understanding of Bayesian statistics is a plus.', 'Desire understanding of AWS infrastructure – Command Line Interface (AWS CLI), S3, AWS Console.', 'Manage customer expectations and keep project deliverables on schedule.', 'High level of responsibility and commitment.', 'Perform data analytics on data sources specifically focused on Haystax Technology insider threat model.Perform the extraction, ingestion, augmentation and aggregation of data from multiple cyber and non-cyber data sources.Interpret customer data, analyze and tune the results using various statistical analysis techniques and machine learning algorithms.Analyze data sources and data flows working with both structured and unstructured data.Assist in the development of cloud and automation solutions to improve ongoing tasks.Work with the data analytics team to test and improve Haystax Technology models.Maintain day-to-day relationships with your technical counterparts at the existing projects.Monitor ongoing performance of platform integrations, identify issues, troubleshoot and escalate issues as needed.Manage customer expectations and keep project deliverables on schedule.Collaborate with the data analytics and customer success teams to prioritize business related tasks.Travel – as necessary, based on customer requests and requirements (once travel resumes).', ""Bachelor's degree in data science, data analytics or computer scienceMinimum 5 years of experience working to support business applications and/or application development.A good understanding of Unix/Linux (CentOS, RHEL, Ubuntu) command line interface including proficient with general system commands and text editors (i.e. ViM, Nano, Emacs)Experience with container technologies like Docker/KubernetesExperience with manipulation of data from high-volume and multiple data sets.Working knowledge or familiarity with NoSQL database technologies such as MongoDB, Elasticsearch.Fundamental skills in Python development and a knowledge and experience of manipulating CSV, JSON, HTML, and XML file formats using machine learning algorithms.Fundamental understanding and experience with regression, clustering, XGBoost, DBSCAN and linear machine learning algorithms.Excellent written and verbal communication skills.Ability to interact well with both internal teams and external customers.Experience at working both independently and in a fast-paced, team-oriented environment.High level of responsibility and commitment.""]",Associate,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Solutions Engineer,Epsilon,"Chicago, IL",2 weeks ago,59 applicants,"['', 'Experience with scheduling applications with complex interdependencies, preferably Airflow', 'Ability to handle complex products', '5 – 10 years of business analysis experience around database marketing technologies and data management, and technical understanding in these areas.', 'Good experience in working with geographically and culturally diverse teams', 'Bachelor’s Degree in Computer Science or equivalent degree is required.', 'Excellent written and verbal communication skills.', 'Ideal candidate can lead in the areas of solution design, code development, quality assurance, data modeling, business intelligence, cross team communication, and application maintenance.', 'What You’ll Do', 'Familiarity with complex data lake environments that span OLTP, MPP and Hadoop platforms', 'What You’ll Need', 'Bachelor’s Degree in Computer Science or equivalent degree is required.5 – 10 years of business analysis experience around database marketing technologies and data management, and technical understanding in these areas.Strong experience in Basic and Advanced SQL writing and tuning .Experience with PythonStrong understanding of Disaster Recovery and Business Continuity solutionsExperience with scheduling applications with complex interdependencies, preferably AirflowGood experience in working with geographically and culturally diverse teamsFamiliarity with complex data lake environments that span OLTP, MPP and Hadoop platformsExcellent written and verbal communication skills.Ability to handle complex productsExcellent Analytical and problem-solving skillsAbility to diagnose and troubleshoot problems quickly', 'Experience with Python', 'Airflow', 'Strong understanding of Disaster Recovery and Business Continuity solutions', 'Working closely with Engineering resources across the globe to ensure enterprise data warehouse solutions and assets are actionable, accessible and evolving in lockstep with the needs of the ever-changing business model.', 'Excellent Analytical and problem-solving skills', 'Ability to diagnose and troubleshoot problems quickly', 'Company Description', 'Great People, Deserve Great Benefits', ' Lead, design and code solutions on and off database for ensuring application access to enable data driven decision making for the company’s multi-faceted ad serving operations.', ' Lead, design and code solutions on and off database for ensuring application access to enable data driven decision making for the company’s multi-faceted ad serving operations.Working closely with Engineering resources across the globe to ensure enterprise data warehouse solutions and assets are actionable, accessible and evolving in lockstep with the needs of the ever-changing business model.Ideal candidate can lead in the areas of solution design, code development, quality assurance, data modeling, business intelligence, cross team communication, and application maintenance.', 'Job Description', 'Strong experience in Basic and Advanced SQL writing and tuning .']",Not Applicable,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Engineer,Hired Recruiters,"Austin, TX",1 day ago,Be among the first 25 applicants,"['', 'You have worked with a variety of cloud and data solutions, such as: AWS / Azure / Google Cloud, SnowFlake, SnowPlow, Kafka, Segment.io, DataBricks, Trifecta, Hadoop, Spark, Airflow', 'Design, implement, and operate stable, scalable, low cost solutions to flow data from production systems into the data warehouse and into end-user facing BI applications', 'Help lead the team to build our data infrastructure and work with emerging technologies such as Snowflake and associated cloud services', 'Implement processes and systems to monitor data quality, ensuring production data is always accurate and available', 'Experience operating a production solution which supports the business', ' You have 5+ years of experience in the field of data engineering or related engineering experience You have recent experience building large-scale production data solutions You are familiar with data driven marketing and integrating into marketing automation solutions. You have a command of various programming languages to collect and manipulate data such as Python, R, and SQL. Experience operating a production solution which supports the business You have strong solution architecture skills and a passion for building data solutions that power the future business. You have worked with a variety of cloud and data solutions, such as: AWS / Azure / Google Cloud, SnowFlake, SnowPlow, Kafka, Segment.io, DataBricks, Trifecta, Hadoop, Spark, Airflow We are looking for people that will thrive in the collaborative environment while also being strong individual contributors with a proactive mindset. ', 'You have 5+ years of experience in the field of data engineering or related engineering experience', 'You are familiar with data driven marketing and integrating into marketing automation solutions.', 'You have strong solution architecture skills and a passion for building data solutions that power the future business.', 'What You’ll Do', 'You have recent experience building large-scale production data solutions', 'Running machine learning experiments using best-in-class ML platforms', 'Who You Are', ' Help lead the team to build our data infrastructure and work with emerging technologies such as Snowflake and associated cloud services Design, implement, and operate stable, scalable, low cost solutions to flow data from production systems into the data warehouse and into end-user facing BI applications Implement processes and systems to monitor data quality, ensuring production data is always accurate and available Running machine learning experiments using best-in-class ML platforms Automate & optimize everything Leverage best in industry practices to build the next generation data ecosystem to collect, move, store and analyze data ', 'We are looking for people that will thrive in the collaborative environment while also being strong individual contributors with a proactive mindset.', 'Automate & optimize everything', 'Leverage best in industry practices to build the next generation data ecosystem to collect, move, store and analyze data', 'You have a command of various programming languages to collect and manipulate data such as Python, R, and SQL.']",Mid-Senior level,Full-time,Engineering,Computer Software,2021-03-18 14:34:51
Data Engineer (Full Time),Arthur Grand Technologies,"Plano, TX",3 days ago,Be among the first 25 applicants,"['', 'Mandatory Skills', 'Data warehousing expertise,', 'Knowledge of architecture, design and business processes', 'AWS, Cloud', 'Data profiling and modeling,', 'Ability to work collaboratively in teams and develop meaningful relationships to achieve common goals', 'Proficiency in modern programming languages', 'ETL expertise', 'Demonstrated experience with SCM tools such as GIT, Jenkins', 'Familiar with development tools such as Jenkins, Jira, Git etc. ', 'Experience in Agile software development methodology and practices with full development lifecycle from inception through implementation.', 'AWS, CloudData warehousing expertise,Data profiling and modeling,Familiar with development tools such as Jenkins, Jira, Git etc. Rest API, MVC, Unit test caseETL expertise', 'Role: Data Lake/Data Engineer,', 'Fulltime ', 'Location: Plano, TX', 'Experience in Agile software development methodology and practices with full development lifecycle from inception through implementation.Working knowledge of Object Oriented Analysis/Object Oriented Design methodologies and life cycles.Demonstrated experience with SCM tools such as GIT, JenkinsAbility to work collaboratively in teams and develop meaningful relationships to achieve common goalsKnowledge of architecture, design and business processesProficiency in modern programming languages', 'Job Description', 'Rest API, MVC, Unit test case', 'Working knowledge of Object Oriented Analysis/Object Oriented Design methodologies and life cycles.']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,CommunityCare,"Tulsa, OK",2 weeks ago,89 applicants,"['', 'Willingness to work in a high-tech, continually evolving, innovative environment.', 'KEY RESPONSIBILITIES:', 'College degree or equivalent experience required.Project management skills preferred.Willingness to work in a high-tech, continually evolving, innovative environment.', 'Experience in the development of SSIS, ETL and other standardized data management tools.', 'Strong project management and organizational skills.', 'QUALIFICATIONS:', 'Create and maintain optimal data pipeline architecture to support our next generation of products and data initiatives.', 'Build processes supporting data transformation, data structures, metadata, dependency and workload management.', 'College degree or equivalent experience required.', 'Create and maintain optimal data pipeline architecture to support our next generation of products and data initiatives.Assemble large, complex data sets that meet functional business requirements.Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability.Experience in the development of SSIS, ETL and other standardized data management tools.Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.Performs other duties as required.', 'EDUCATION/EXPERIENCE:', 'JOB SUMMARY:', 'The Data Engineer will be responsible for expanding, optimizing and monitoring our data and data pipeline architecture, as well as optimizing data flow and collection across organizational teams. The Data Engineer will support our software engineers, database architects and data analysts on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. ', 'Project management skills preferred.', 'Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.', 'Build processes supporting data transformation, data structures, metadata, dependency and workload management.Strong project management and organizational skills.Ability to work independently, handle multiple tasks and projects simultaneously.', 'Assemble large, complex data sets that meet functional business requirements.', 'Ability to work independently, handle multiple tasks and projects simultaneously.', '\xa0', 'Performs other duties as required.', 'Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability.']",Not Applicable,Full-time,Health Care Provider,Insurance,2021-03-18 14:34:51
Data Engineer,McDonald's,"Chicago, IL",2 weeks ago,131 applicants,"['', ' Strong knowledge of relational and multi-dimensional database architecture', ' Bachelor’s Degree in Computer Science, Computer Engineering, Information Systems, or similar. Or equivalent work experience.', ' Experience in ETL and data warehouse technologies old and new (Oracle, SQL Server, Redshift, S3, snowflake)', ' Bachelor’s Degree in Computer Science, Computer Engineering, Information Systems, or similar. Or equivalent work experience. Master’s degree or equivalent work experience preferred. Experience managing applications in cloud based technologies and pipelining and familiarity with core services (AWS or GCP preferred) Experience in ETL and data warehouse technologies old and new (Oracle, SQL Server, Redshift, S3, snowflake) Familiarity with modern Machine Learning techniques Experience and desire to work in a Global delivery environment is a plus Comfortable with ambiguity and willing to proactively problem solve Strong knowledge of relational and multi-dimensional database architecture Expert at scripting ETL and application related processes in Python Experience building capabilities to support data science and analytics functions Knowledge of front-end technologies as they relate to data delivery a plus Strong verbal and written communication skills, and ability to synthesize technical information for a business audience', ' Master’s degree or equivalent work experience preferred.', 'Additional Information</strongMcDonald’s is committed to providing qualified individuals with disabilities reasonable accommodations to perform the essential functions of their jobs. Additionally, if you (or another applicant of whom you are aware) require assistance accessing or reading this job posting or otherwise seek assistance in the application process, please contact recruiting.supportteam@us.mcd.comMcDonald’s provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to sex, sex stereotyping, pregnancy (including pregnancy, childbirth, and medical conditions related to pregnancy, childbirth, or breastfeeding), race, color, religion, ancestry or national origin, age, disability status, medical condition, marital status, sexual orientation, gender, gender identity, gender expression, transgender status, protected military or veteran status, citizenship status, genetic information, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.Nothing in this job posting or description should be construed as an offer or guarantee of employment.', 'Company Description', ' Experience managing applications in cloud based technologies and pipelining and familiarity with core services (AWS or GCP preferred)', ' Familiarity with modern Machine Learning techniques', 'Qualifications</strong Bachelor’s Degree in Computer Science, Computer Engineering, Information Systems, or similar. Or equivalent work experience. Master’s degree or equivalent work experience preferred. Experience managing applications in cloud based technologies and pipelining and familiarity with core services (AWS or GCP preferred) Experience in ETL and data warehouse technologies old and new (Oracle, SQL Server, Redshift, S3, snowflake) Familiarity with modern Machine Learning techniques Experience and desire to work in a Global delivery environment is a plus Comfortable with ambiguity and willing to proactively problem solve Strong knowledge of relational and multi-dimensional database architecture Expert at scripting ETL and application related processes in Python Experience building capabilities to support data science and analytics functions Knowledge of front-end technologies as they relate to data delivery a plus Strong verbal and written communication skills, and ability to synthesize technical information for a business audienceAdditional Information</strongMcDonald’s is committed to providing qualified individuals with disabilities reasonable accommodations to perform the essential functions of their jobs. Additionally, if you (or another applicant of whom you are aware) require assistance accessing or reading this job posting or otherwise seek assistance in the application process, please contact recruiting.supportteam@us.mcd.comMcDonald’s provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to sex, sex stereotyping, pregnancy (including pregnancy, childbirth, and medical conditions related to pregnancy, childbirth, or breastfeeding), race, color, religion, ancestry or national origin, age, disability status, medical condition, marital status, sexual orientation, gender, gender identity, gender expression, transgender status, protected military or veteran status, citizenship status, genetic information, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.Nothing in this job posting or description should be construed as an offer or guarantee of employment.', ' Expert at scripting ETL and application related processes in Python', ' Experience building capabilities to support data science and analytics functions', ' Strong verbal and written communication skills, and ability to synthesize technical information for a business audience', ' Experience and desire to work in a Global delivery environment is a plus', ' Comfortable with ambiguity and willing to proactively problem solve', 'Job Description</strongOur core data science organization is looking to hire a Data Engineer. The role will work closely with data scientists and ML engineers and includes designing, architecting, and building data pipelines to support business use cases. Responsibilities also include collaborating with business leaders to translate business requirements into technical, scale-able solution.Qualifications</strong Bachelor’s Degree in Computer Science, Computer Engineering, Information Systems, or similar. Or equivalent work experience. Master’s degree or equivalent work experience preferred. Experience managing applications in cloud based technologies and pipelining and familiarity with core services (AWS or GCP preferred) Experience in ETL and data warehouse technologies old and new (Oracle, SQL Server, Redshift, S3, snowflake) Familiarity with modern Machine Learning techniques Experience and desire to work in a Global delivery environment is a plus Comfortable with ambiguity and willing to proactively problem solve Strong knowledge of relational and multi-dimensional database architecture Expert at scripting ETL and application related processes in Python Experience building capabilities to support data science and analytics functions Knowledge of front-end technologies as they relate to data delivery a plus Strong verbal and written communication skills, and ability to synthesize technical information for a business audienceAdditional Information</strongMcDonald’s is committed to providing qualified individuals with disabilities reasonable accommodations to perform the essential functions of their jobs. Additionally, if you (or another applicant of whom you are aware) require assistance accessing or reading this job posting or otherwise seek assistance in the application process, please contact recruiting.supportteam@us.mcd.comMcDonald’s provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to sex, sex stereotyping, pregnancy (including pregnancy, childbirth, and medical conditions related to pregnancy, childbirth, or breastfeeding), race, color, religion, ancestry or national origin, age, disability status, medical condition, marital status, sexual orientation, gender, gender identity, gender expression, transgender status, protected military or veteran status, citizenship status, genetic information, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.Nothing in this job posting or description should be construed as an offer or guarantee of employment.', ' Knowledge of front-end technologies as they relate to data delivery a plus']",Not Applicable,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Software Engineer-Data Analytics,U.S. Bank,San Francisco Bay Area,,N/A,"['', 'Apache Spark', '·\xa0\xa0\xa0\xa0\xa0\xa0Technical expertise in the design, development, coding, testing, and debugging of software ', ""·\xa0\xa0\xa0\xa0\xa0\xa0Considers scalability, reliability and performance of systems/contexts affected when defining technical designs. Has an understanding of the team's domain, how work in this domain relates to the team's objectives and deliverables and how it contributes to overall business strategy and how technical strategy maps to this. "", ""At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors."", '·\xa0\xa0\xa0\xa0\xa0\xa0U.S. Bank is seeking a senior developer and solution architect with end-to-end design and development experience creating solutions in support of digital and marketing clickstream data collection initiatives. ', '·\xa0\xa0\xa0\xa0\xa0\xa0Familiarity with data modelling', '• Knowledge in agile methodologies and software development life cycle ', 'Javascript', 'Preferred Technical Skills', 'Tableau/Power BI', 'Basic Qualifications ', '·\xa0\xa0\xa0\xa0\xa0\xa0Design digital data collection solutions that are aligned with U.S. Bank enterprise technology strategy ', 'Skills / Experience ', 'Develop, implement, and disseminate metrics about quality, performance and outcomes for various clickstream data implementations across the enterprise', 'Effective verbal, written and presentation skills', '• Excellent verbal and written communication skills ', '• Technology skills in streaming, NoSQL, web scale distributed systems ', '·\xa0\xa0\xa0\xa0\xa0\xa0Ideal candidate has a thorough understanding of features purpose and the flows to create a mechanism for Digital data measurement. ', 'Python/R/SQL', 'Knowledge of digital products and services, financial services as well as marketing concepts and the bank’s (retail/commercial) sales and marketing programs in order to interpret technical or product concepts.', '·\xa0\xa0\xa0\xa0\xa0\xa0The ideal candidate will have deep expertise when it comes to digital data collection solutioning including alignment and optimization of third-party platforms with complex enterprise architectures. Prior experience with designing clickstream data collection solution is preferred. ', 'Qualifications ', 'Adobe Analytics or other clickstream data collection tools', '·\xa0\xa0\xa0\xa0\xa0\xa0CI/CD pipeline for end to end automation ', 'Develop, implement, and disseminate metrics about quality, performance and outcomes for various clickstream data implementations across the enterpriseKnowledge of digital products and services, financial services as well as marketing concepts and the bank’s (retail/commercial) sales and marketing programs in order to interpret technical or product concepts.Effective verbal, written and presentation skills', '·\xa0\xa0\xa0\xa0\xa0\xa0Build consensus around solutions that benefit the organization ', '·\xa0\xa0\xa0\xa0\xa0\xa0Bachelors or Masters degree in Computer Science or related field with 5+ years of experience in Technology, Financial services or Retail ', 'Be part of something big, where integrity matters and success inspires, where great people collaborate, innovate and give back, where you feel included, valued and proud. At U.S. Bank we’re looking for people who want more than just a job – they want to make a difference in the communities where they live and work. ', '·\xa0\xa0\xa0\xa0\xa0\xa0A solid foundation in computer science, with strong competencies in data structures, algorithms, and software design ', '·\xa0\xa0\xa0\xa0\xa0\xa0At least 5 years of experience designing, planning and executing at all stages in n-tier enterprise applications and platforms ', '·\xa0\xa0\xa0\xa0\xa0\xa0Manage relationships with various business leaders, architects and engineering managers ', 'Responsibilities ', '·\xa0\xa0\xa0\xa0\xa0\xa0Excellent analytical and problem-solving skills; ability to find creative solutions ', '·\xa0\xa0\xa0\xa0\xa0\xa0Able to work with teams of developers and business stakeholders to bring solution to life ', '·\xa0\xa0\xa0\xa0\xa0\xa0Has the ability to analyze the bigger picture, identifying and prioritizing with the aim to consider more than one domain within this analysis. ', 'Presto', '·\xa0\xa0\xa0\xa0\xa0\xa0Experience working with cloud platforms ', '\xa0', 'CSS', '·\xa0\xa0\xa0\xa0\xa0\xa0Knowledge of data management principles ', 'HTML', 'JavascriptApache SparkAdobe Analytics or other clickstream data collection toolsPrestoHTMLCSSPython/R/SQLTableau/Power BI', '·\xa0\xa0\xa0\xa0\xa0\xa0Is able to create clear and well thought out technical designs and considers dependencies, failure states, maintainability, testability and ease of support. ', '·\xa0\xa0\xa0\xa0\xa0\xa0Thoroughly understands the business model in relation to their current product focus area. Looks for opportunities to simplify product & technical design. ']",Mid-Senior level,Full-time,Engineering,Banking,2021-03-18 14:34:51
Data Engineer,W2O Group,"Florham Park, NJ",2 days ago,26 applicants,"['', 'Extensive, demonstrated expertise with Python. should be generally comfortable in different categories (machine learning, development, scripting, etc.)', 'Hands on experience leading enterprise-wide data engineering, warehousing and analytics projects', '5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative fieldAdvanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.Extensive, demonstrated expertise with Python. should be generally comfortable in different categories (machine learning, development, scripting, etc.)Experience in building real time streaming data ingestion and processing pipeline using Apache Beam (running on either Google Dataflow or Apache (Apex Flink, or Spark) or Kafka in an analytics or data science use caseBuild processes supporting data transformation, data structures, metadata, dependency and workload management.Strong analytic skills related to working with unstructured datasets.Hands on experience leading enterprise-wide data engineering, warehousing and analytics projectsExperience with data pipeline and workflow management tools: Airflow, Oozie etc.Ability to think strategically about business, product, and technical challenges in an enterprise environmentUnderstanding of database and analytical technologies in the industry including MPP and NoSQL databases, Data Warehouse design, PostgreSQL, BI reporting and Dashboard developmentDemonstrated industry efficiency in the fields of database, data warehousing or data sciencesCustomer facing skills with the ability to drive discussions with senior leadership regarding trade-offs, best practices and risk mitigationDesire and ability to interact with all levels of the organization', 'Own the Insights gleaned by the creation of advanced technology roadmaps. Share real world implementations and recommend new capabilities that would simplify adoption and drive greater value.', 'Strong analytic skills related to working with unstructured datasets.', 'Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.', 'Design and implement data ingestion solutions on GCP using GCP native services ', 'Responsibilities', 'Experience in building real time streaming data ingestion and processing pipeline using Apache Beam (running on either Google Dataflow or Apache (Apex Flink, or Spark) or Kafka in an analytics or data science use case', '5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field', 'Build processes supporting data transformation, data structures, metadata, dependency and workload management.', 'Innovate and engage with key technology stakeholders to create a compelling vision of a data-driven enterprise environment and the impact it will have on their teams, their projects and their outcomes.Requirements5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative fieldAdvanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.Extensive, demonstrated expertise with Python. should be generally comfortable in different categories (machine learning, development, scripting, etc.)Experience in building real time streaming data ingestion and processing pipeline using Apache Beam (running on either Google Dataflow or Apache (Apex Flink, or Spark) or Kafka in an analytics or data science use caseBuild processes supporting data transformation, data structures, metadata, dependency and workload management.Strong analytic skills related to working with unstructured datasets.Hands on experience leading enterprise-wide data engineering, warehousing and analytics projectsExperience with data pipeline and workflow management tools: Airflow, Oozie etc.Ability to think strategically about business, product, and technical challenges in an enterprise environmentUnderstanding of database and analytical technologies in the industry including MPP and NoSQL databases, Data Warehouse design, PostgreSQL, BI reporting and Dashboard developmentDemonstrated industry efficiency in the fields of database, data warehousing or data sciencesCustomer facing skills with the ability to drive discussions with senior leadership regarding trade-offs, best practices and risk mitigationDesire and ability to interact with all levels of the organizationW2O Group offers a comprehensive benefit program and perks, including flexible PTO, expanded paid leave for new parents including a 4th Trimester program that helps new parents transition back to work, and a five-week sabbatical program. Other perks include Income Protection, Retirement plans/401(k) match, and cell phone savings plans. Learn more about our great benefits and perks at: http://www.w2ogroup.com/W2O Group is an Equal Opportunity Employer. We foster an environment that embraces diversity. We are stronger with a wider range of opinions, strengths, and backgrounds to achieve our goals.W2O Group offers a comprehensive benefit program and perks, including flexible PTO, expanded paid leave for new parents including a 4th Trimester program that helps new parents transition back to work, and a five-week sabbatical program. Other perks include Income Protection, Retirement plans/401(k) match, and cell phone savings plans. Learn more about our great benefits and perks at: http://www.w2ogroup.com/W2O Group is committed to being an Equal Opportunity employer. As such, we seek motivated and qualified applicants without regard to race, color, religion, sex (including pregnancy), sexual orientation, gender identity/expression, ethnic or national origin, age, physical or mental disability, genetic information, marital information, or any other characteristic protected by federal, state, or local employment discrimination laws where W2O operates. We strive to employ, motivate, advance and reasonably accommodate any qualified employees and applicants. We believe diversity of persons and ideas forms the most comprehensive, forward-looking company.', 'Demonstrated industry efficiency in the fields of database, data warehousing or data sciences', 'Demonstrated experience with distributed computing', 'Requirements', 'Delivery of cloud architecture to support new distributed computing solutions that often span the full array of cloud services. This will include migration of existing applications and development of new applications using cloud services.', 'Customer facing skills with the ability to drive discussions with senior leadership regarding trade-offs, best practices and risk mitigation', 'Design and implement data ingestion solutions on GCP using GCP native services Demonstrated experience with distributed computingDesign and optimize data models on GCP cloud using GCP data stores such as BigQueryIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.Delivery of cloud architecture to support new distributed computing solutions that often span the full array of cloud services. This will include migration of existing applications and development of new applications using cloud services.Own the Insights gleaned by the creation of advanced technology roadmaps. Share real world implementations and recommend new capabilities that would simplify adoption and drive greater value.', 'Experience with data pipeline and workflow management tools: Airflow, Oozie etc.', 'Understanding of database and analytical technologies in the industry including MPP and NoSQL databases, Data Warehouse design, PostgreSQL, BI reporting and Dashboard development', 'Role', '4th Trimester', 'Desire and ability to interact with all levels of the organization', 'Design and optimize data models on GCP cloud using GCP data stores such as BigQuery', 'Ability to think strategically about business, product, and technical challenges in an enterprise environment', 'Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.', 'Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Data Engineer,Babylon Health,"Austin, TX",3 weeks ago,116 applicants,"['', 'Proven ability of looking at solutions unconventionally and explore opportunities and devise innovative solutions', 'Experience gathering complex business requirements and identifying data needs', 'Bachelor’s degree in computer science or related fieldProven ability of looking at solutions unconventionally and explore opportunities and devise innovative solutionsExcellent communication skills (verbal and written) and interpersonal skills and an ability to effectively communicate with both business and technical teamsExperience gathering complex business requirements and identifying data needsExperience with design and development of relational databases and data warehousesAdvanced level of proficiency in SQL developmentKnowledge and expertise with Python, Shell, Java ScriptingETL development experience with large-scale databases or big data systems such as Hive, BigQuery, AWS Redshift, Snowflake, etc.Experience using data transformation tools such as dbtExperience using data orchestration tools such as Apache Airflow or Apache BeamExperience with using a cloud platform provider (such as AWS/GCP) to develop tools and infrastructureExposure to a BI reporting tool (such as Tableau, Looker or PowerBI) with an understanding of why they are an important part of the analytics stackExperience analyzing data to identify deliverables, gaps and inconsistenciesNice to have: experience working in a start-up', 'Nice to have: experience working in a start-up', 'Experience with using a cloud platform provider (such as AWS/GCP) to develop tools and infrastructure', 'Our technology stack includes Python, dbt, Airflow and a host of Google Cloud products that run on a range of technologies (GCP/AWS, Docker, GitHub, CircleCI & Jenkins) ', 'Responsibilities', 'Work closely with the data science team to support processing data into a form suitable for machine learning models', 'Nice to have:', 'Requirements', 'Exposure to a BI reporting tool (such as Tableau, Looker or PowerBI) with an understanding of why they are an important part of the analytics stack', 'Advanced level of proficiency in SQL development', 'Experience using data orchestration tools such as Apache Airflow or Apache Beam', 'Build, test and refine data pipelines for data analytics and business intelligence (BI)Data modeling, process design and overall data pipeline architectureEnsure the data quality and consistency with monitoring support, and play an active role in establishing data governance around company KPIsWork closely with the business intelligence teams to design, build and test end-to-end solutionsWork closely with the data science team to support processing data into a form suitable for machine learning modelsChampion SSDLC (secure software development lifecycle) within analytics and data science and lead by example in building self-service, well tested solutionsChampion high engineering standards through comprehensive testing, code reviews, continuous integration and continuous deployment across the teamOur technology stack includes Python, dbt, Airflow and a host of Google Cloud products that run on a range of technologies (GCP/AWS, Docker, GitHub, CircleCI & Jenkins) ', 'Excellent communication skills (verbal and written) and interpersonal skills and an ability to effectively communicate with both business and technical teams', 'Experience analyzing data to identify deliverables, gaps and inconsistencies', 'ETL development experience with large-scale databases or big data systems such as Hive, BigQuery, AWS Redshift, Snowflake, etc.', 'Bachelor’s degree in computer science or related field', 'Knowledge and expertise with Python, Shell, Java Scripting', 'Data modeling, process design and overall data pipeline architecture', 'Work closely with the business intelligence teams to design, build and test end-to-end solutions', 'Build, test and refine data pipelines for data analytics and business intelligence (BI)', 'Champion SSDLC (secure software development lifecycle) within analytics and data science and lead by example in building self-service, well tested solutions', 'Champion high engineering standards through comprehensive testing, code reviews, continuous integration and continuous deployment across the team', 'Experience with design and development of relational databases and data warehouses', 'Experience using data transformation tools such as dbt', 'Ensure the data quality and consistency with monitoring support, and play an active role in establishing data governance around company KPIs']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Victra - Verizon Authorized Retailer,"Raleigh, NC",6 days ago,118 applicants,"['', '·\xa0\xa0\xa0\xa0\xa0\xa0Be customer oriented, and provide data and support to business partners as necessary', '·\xa0\xa0\xa0\xa0\xa0\xa0Work in dynamic self-organized agile teams to develop high-quality solutions using the best technology stack, design, and architecture.', '·\xa0\xa0\xa0\xa0\xa0\xa0High level of business acumen with clear understanding of how data and information support business objectives', '·\xa0\xa0\xa0\xa0\xa0\xa0Experience with ETL tools such as Pentaho, KNIME preferred', '·\xa0\xa0\xa0\xa0\xa0\xa0Advanced degree is preferred', '·\xa0\xa0\xa0\xa0\xa0\xa0Retail experience is preferred', 'The physical demands described here are representative of those that must be met by an individual to successfully perform the essential functions of this job including the ability to work in an office environment. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. ', 'SUPERVISORY RESPONSIBILITIES', '·\xa0\xa0\xa0\xa0\xa0\xa04+ years of experience developing with PowerBI', 'The requirements listed below are representative of the knowledge, skill, and/or ability required.\xa0Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.', '·\xa0\xa0\xa0\xa0\xa0\xa0Ability to both work collaboratively with teams and excels being a contributor', 'The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.\xa0Work environment is a typical climate-controlled cubicle office setting.\xa0The noise level in the work environment is usually quiet to moderate.', 'ESSENTIAL DUTIES AND RESPONSIBILITIES ', 'N/A', '·\xa0\xa0\xa0\xa0\xa0\xa0Define and evolve Victra’s data movement, data access, MDM, and data visualization strategies', 'WORK ENVIRONMENT ', '·\xa0\xa0\xa0\xa0\xa0\xa0Lead the development of certified Power BI Data Models and Data Sources, utilizing internal and external data (APIs)', 'While performing the duties of this job, the employee is regularly required to sit and talk or hear.\xa0The employee is occasionally required to stand; walk; use hands to finger, handle, or feel; reach with hands and arms; and stoop, kneel, crouch, or crawl.\xa0The employee must regularly lift and/or move up to 10 pounds and occasionally lift and/or move up to 25 pounds.\xa0Specific vision abilities required by this job include close vision, distance vision, color vision, and ability to adjust focus.', '·\xa0\xa0\xa0\xa0\xa0\xa0Ability to maintain focus, accuracy and composure during high expectation and high-pressure situations', 'EDUCATION and/or EXPERIENCE ', '·\xa0\xa0\xa0\xa0\xa0\xa0Proven ability to work across all functions and levels of the organization to improve financial and/or operational results', '·\xa0\xa0\xa0\xa0\xa0\xa0Participate in designing and implementing RESTful services and microservices.', '·\xa0\xa0\xa0\xa0\xa0\xa0Intermediate knowledge of MS Excel', '·\xa0\xa0\xa0\xa0\xa0\xa0Advanced level of SQL programming', 'CERTIFICATES, LICENSES, REGISTRATIONS ', '·\xa0\xa0\xa0\xa0\xa0\xa0Ability to influence decisions and actions of others without reporting authority', '·\xa0\xa0\xa0\xa0\xa0\xa0Advanced level of DAX programming', 'QUALIFICATIONS', '·\xa0\xa0\xa0\xa0\xa0\xa0Comfortable challenging business partners in a constructive way, including members of the Executive Management Team', '·\xa0\xa0\xa0\xa0\xa0\xa0Bachelor’s degree preferably in Computer Science, Management Information Systems (MIS), or Business Field with emphasis in Information Technology', ""·\xa0\xa0\xa0\xa0\xa0\xa0Interact with stakeholders to analyze, explain, design, and develop new data services and capabilities supporting the enable Victra's business to take data driven decisions."", '·\xa0\xa0\xa0\xa0\xa0\xa0Propose and roll-out improvements to culture, process, tools, technology, and architecture.', 'QUALIFICATIONS ', '·\xa0\xa0\xa0\xa0\xa0\xa0Experience working with data sourced from REST and/or SOAP APIs preferred', '·\xa0\xa0\xa0\xa0\xa0\xa0Process focused, has a continuous improvement mindset and driven to self-learning', '·\xa0\xa0\xa0\xa0\xa0\xa0Expected to challenge teammates in a constructive and professional manner', '·\xa0\xa0\xa0\xa0\xa0\xa0Invent ways to answer key business questions by leveraging existing data assets or assisting in creating new ones', '\xa0', '·\xa0\xa0\xa0\xa0\xa0\xa0Experience with Azure Data Analytics Architecture preferred', '·\xa0\xa0\xa0\xa0\xa0\xa0Intermediate knowledge of R or Python preferred', 'Do you strive to provide elegant solutions, to complex business problems? Do you want to enhance knowledge by enabling information to be understandable and accessible? Do you like being part of a team that creates, develops and enlightens? Would you like to make an impact with your contribution in meaningful ways, where it will truly make a difference? If your answer is YES to these questions, and you can imagine an exciting environment where your innovation and ideas will make a difference, our role at Victra is a great opportunity for you. ', '·\xa0\xa0\xa0\xa0\xa0\xa0Mentor and provide assistance with Power BI to partners across the organization', 'PHYSICAL DEMANDS ', '·\xa0\xa0\xa0\xa0\xa0\xa0Assist in developing best BI and data services deployment practices such as Azure Data Factories and Azure CI/CD pipelines, including development, governance and maintenance.']",Mid-Senior level,Full-time,Information Technology,Telecommunications,2021-03-18 14:34:51
Data Engineer,Homesite Insurance,"Phoenix, AZ",4 weeks ago,Be among the first 25 applicants,"['', 'Test, design, and implement process automation techniques to support efficiencies.', ' Clearly communicate complex findings to colleagues influencing outcomes and communicating the value add of the data engineering work being completed.', 'Expertise in relational databases (such as MS SQL Server, MySQL and Aurora) concepts.', 'Insurance industry experience a plus, but not required.Experience with cloud computing APIs (Amazon Web Services preferred).Experience with cloud computing services (Amazon Web Services like Lambda, S3, CloudWatch, ECS, and RDS preferred)', 'Knowledge, Skills And Competencies', 'A mastery of best practices in coding, code organization, data transformation, ETL, process automation, and APIs.', 'Compensation may vary based on the job level and your geographic work location.', 'Responsibilities', 'Foster relationships with colleagues to identify and explore new sources of data.', 'Lead efforts to design, develop and implement databases enhancement to improve efficiency and streamline the use for analytics, business analysis and data governance. Develop a mastery of the corporate data structure to be able to perform ETL operations from data tables existing in the company.Test, design, and implement process automation techniques to support efficiencies. Clearly communicate complex findings to colleagues influencing outcomes and communicating the value add of the data engineering work being completed.Foster relationships with colleagues to identify and explore new sources of data.', 'Knowledge of one version control system, preferably Git is required.', 'Motivated individual with strong analytic, problem solving, and troubleshooting skills.', 'Experience with cloud computing APIs (Amazon Web Services preferred).', 'Develop a mastery of the corporate data structure to be able to perform ETL operations from data tables existing in the company.', 'Other Job Experiences Desired', 'Lead efforts to design, develop and implement databases enhancement to improve efficiency and streamline the use for analytics, business analysis and data governance. ', 'Insurance industry experience a plus, but not required.', 'Expertise with Microsoft SQL Server (including SSIS, SSRS) including 2-5 years of experience. ', 'Bachelor’s degree in Computer Science, Computer Engineering, Information Technology/Systems, related field, or equivalent experience.', 'Ability to comprehensively understand data sources, elements and relationships in both business and technical terms.', 'A mastery of best practices in coding, code organization, data transformation, ETL, process automation, and APIs.Expertise in relational databases (such as MS SQL Server, MySQL and Aurora) concepts.Ability to comprehensively understand data sources, elements and relationships in both business and technical terms.Expertise with Microsoft SQL Server (including SSIS, SSRS) including 2-5 years of experience. Knowledge of one version control system, preferably Git is required.Bachelor’s degree in Computer Science, Computer Engineering, Information Technology/Systems, related field, or equivalent experience.Motivated individual with strong analytic, problem solving, and troubleshooting skills.', 'Experience with cloud computing services (Amazon Web Services like Lambda, S3, CloudWatch, ECS, and RDS preferred)']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,IntegriChain,Greater Philadelphia,3 weeks ago,27 applicants,"['', '3+ years of experience in at least one basic relational database platform (sql server, oracle, postgres, mysql) and languages (PL/SQL, SQL).', 'Develop, and refine both streaming and batch processing data pipeline frameworks.', 'Define, document, and maintain a data dictionary including:\xa0data definitions, data sources, business meaning and usage of information.', 'Qualifications and Competencies:', 'Knowledge of aws services and airflow is a plus.\xa0', '1+ years experience developing modern, industry standard big data frameworks with AWS or other cloud services.', 'Prototype and optimize data type checks to ensure data uniformity prior to load.', 'Knowledge of speciality pharmaceutical and\xa0retail pharmacy is a plus.', 'Duties:', 'Works with stakeholders to gather requirements on merging, de-duplicating, standardizing data.', 'Develop, support, and refine new data pipelines, data models, business logic, data schemas as code, and analytics to product specifications.', 'Identify and validate opportunities to reuse existing data and algorithms.', ""Bachelor's Degree in technical background\xa0or equivalent work experience."", 'Experience with common GitHub developer practices and paradigms.', 'Deliver modern data pipelines and create custom data extracts that meet the needs of both internal and external customers.', 'Collaborate on\xa0design and implementation of data standardization procedures.', 'Please do not send unsolicited resumes to our employees, job listings, or the recruiting team. IntegriChain is not responsible for any fees related to unsolicited resumes.\xa0', ""Bachelor's Degree in technical background\xa0or equivalent work experience.2 - 3+ years of experience building data pipelines and using ETL tools.\xa0Prefer python programming experience.3+ years of experience in at least one basic relational database platform (sql server, oracle, postgres, mysql) and languages (PL/SQL, SQL).1+ years experience developing modern, industry standard big data frameworks with AWS or other cloud services.Experience with common GitHub developer practices and paradigms.Experience working with agile methodologies and cross-functional teams.\xa0Knowledge of redshift or any other columnar database is prefered.Knowledge of aws services and airflow is a plus.\xa0Experience in building AWS data pipelines using python, S3 data lake is a plus.\xa0Knowledge of speciality pharmaceutical and\xa0retail pharmacy is a plus."", 'Maintain, improve, and develop expertise in existing production data, models, and algorithms.', '\ufeffMission:', 'Learn and utilize\xa0business data domain knowledge and its correlation to\xa0underlying data sources.', '\xa0', 'Share team responsibilities; such as contributing to development of data warehouses and productizing algorithms created by Data Science team members.', '2 - 3+ years of experience building data pipelines and using ETL tools.\xa0Prefer python programming experience.', 'Knowledge of redshift or any other columnar database is prefered.', '**Recruiting Agencies:\xa0Please do not send unsolicited resumes to our employees, job listings, or the recruiting team. IntegriChain is not responsible for any fees related to unsolicited resumes.\xa0', 'Experience in building AWS data pipelines using python, S3 data lake is a plus.\xa0', '**Recruiting Agencies:\xa0', 'Experience working with agile methodologies and cross-functional teams.\xa0', 'Develop, support, and refine new data pipelines, data models, business logic, data schemas as code, and analytics to product specifications.Prototype and optimize data type checks to ensure data uniformity prior to load.Develop, and refine both streaming and batch processing data pipeline frameworks.Maintain, improve, and develop expertise in existing production data, models, and algorithms.Learn and utilize\xa0business data domain knowledge and its correlation to\xa0underlying data sources.Define, document, and maintain a data dictionary including:\xa0data definitions, data sources, business meaning and usage of information.Identify and validate opportunities to reuse existing data and algorithms.Works with stakeholders to gather requirements on merging, de-duplicating, standardizing data.Collaborate on\xa0design and implementation of data standardization procedures.Share team responsibilities; such as contributing to development of data warehouses and productizing algorithms created by Data Science team members.']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Vivint,"Lehi, UT",2 weeks ago,57 applicants,"['', 'Experience with machine learning technologies (R, SparkML, AzureML, etc.)', 'Familiarity with one or more web technologies (Angular, React, PHP, ASP.net, etc.)', 'Paid holidays and flexible paid time away', '1+ years of ETL & data pipeline development experience ', 'Job Summary', 'Proficiency in the python scripting language. Preference given to knowledge of more (Perl, .net, etc.)', 'Onsite health clinic', 'Ability to initiate, drive, and manage projects with competing priorities', 'Who Are We', 'Must have a passion for data and helping the business turn data into information and action1+ years of data engineering experience1+ years of ETL & data pipeline development experience Ability to initiate, drive, and manage projects with competing prioritiesAbility to communicate effectively with business leaders, IT leadership, and engineers Expert in SQL, databases, and ETL development processes & tools (Cloud MPP like Snowflake or Redshift)', 'Bonus Skills', 'Employee pricing on smart home products', ' https://www.fastcompany.com/3067476/why-vivint-smart-home-is-one-of-the-most-innovative-companies-of-2 ', 'Safety', ' Medical/dental/vision/life coverage ', ' http://archive.sltrib.com/article.php?id=5360131&itype=CMSID ', 'Who You Will Work With', 'Your choice between Mac or PC', 'Proficiency in the python scripting language. Preference given to knowledge of more (Perl, .net, etc.)Familiarity with one or more web technologies (Angular, React, PHP, ASP.net, etc.)', 'Must have a passion for data and helping the business turn data into information and action', 'Onsite gym, gaming tables across our campus', 'Working Conditions', '1+ years of data engineering experience', 'Why Vivint', 'What We Stand For', 'What We’re Looking For', 'Experience with big data technologies (HDFS, Hadoop, Spark, Elastic Search, Redshift, Snowflake, etc...)Experience with Tableau or similar data visualization toolExperience with AWS or Azure data product offerings and platformExperience with machine learning technologies (R, SparkML, AzureML, etc.)', 'Paid holidays and flexible paid time awayYour choice between Mac or PCEmployee pricing on smart home productsCasual dress codeOnsite gym, gaming tables across our campusOnsite health clinic Medical/dental/vision/life coverage ', 'Expert in SQL, databases, and ETL development processes & tools (Cloud MPP like Snowflake or Redshift)', 'Job Description', 'Experience with big data technologies (HDFS, Hadoop, Spark, Elastic Search, Redshift, Snowflake, etc...)', 'Ability to communicate effectively with business leaders, IT leadership, and engineers ', 'Casual dress code', "" Find out more about what it's like to work here: "", ' https://www.vivint.com/company/careers/culture ', 'Experience with AWS or Azure data product offerings and platform', 'Experience with Tableau or similar data visualization tool', 'If you are an active Vivint employee, please apply through Workday by searching ""Find Jobs"".']",Entry level,Full-time,Information Technology,Consumer Electronics,2021-03-18 14:34:51
Data Engineer,Envision,Greater St. Louis,3 days ago,110 applicants,"['', '• Familiarity with creating and maintaining containerized application deployments with a platform like Docker', '• Experience working with scientific datasets, or a background in the application of quantitative science to business problems', '• Experience with stream processing using Apache Kafka', '• Experience with: Google Cloud Platform, Apache Beam and or Google Cloud Dataflow, Google Kubernetes Engine or Kubernetes', 'No C2C, must be our\xa0W2 employee', 'Bonus points for:', '• A level of comfort with Unit Testing and Test Driven Development methodologies', 'Data Engineer, (remote *)', '• At least 2 years experience with Go', 'You may apply on LinkedInor on our web site: http://www.envision.com/jobs/index.html#/jobs/71767', '• Bioinformatics experience, especially large scale storage and data mining of variant data, variant annotation, and genotype to phenotype correlation', '*Remote: For exceptional candidates who can show previous remote work on their resume, we will allow remote. You must be able to show where you worked remotely and outline the tools you used to work remotely.*', 'You must be able to show where you worked remotely and outline the tools you used to work remotely.', '• A proven ability to build and maintain cloud based infrastructure on a major cloud provider like AWS, Azure or Google Cloud Platform', 'Required experience:', '• Experience with protocol buffers and gRPC', '• Experience data modeling for large scale databases, either relational or NoSQL', '• Proven experience (2 years) building and maintaining data-intensive APIs using a RESTful approach']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer - Defense Analytics,McKinsey & Company,"Washington, DC",3 weeks ago,47 applicants,"['', '3-5 years of experience in a data engineering and/or architecture role', 'Development experience using Hadoop (HDFS, Hive, HBase, Impala) and/or Spark ', 'Database development on either relational databases (e.g., PostgreSQL, SQL Server) or NoSQL on distributed platforms (e.g. HBase, Cassandra)', 'Practitioner in analytics with a strong grasp of ETL, reporting tools, data governance, data warehousing, dimensional modeling and working with structured and unstructured data', 'Experience working on a collaborative Agile product team ', 'Ability to perform quantitative and statistical techniques to identify trends, patterns and correlations ', 'Ability to take creative approaches to analysis and syndicate complex ideas clearly', 'Qualifications', 'Bachelor’s or master’s degree in a technical or quantitative field (mathematics, statistics, CS, engineering, etc.) ', 'Intermediate skills with SQL as well as 1 programming language (Scala, Java, C++, Python, R)', 'Bachelor’s or master’s degree in a technical or quantitative field (mathematics, statistics, CS, engineering, etc.) 3-5 years of experience in a data engineering and/or architecture roleAbility to perform quantitative and statistical techniques to identify trends, patterns and correlations Ability to take creative approaches to analysis and syndicate complex ideas clearlyPractitioner in analytics with a strong grasp of ETL, reporting tools, data governance, data warehousing, dimensional modeling and working with structured and unstructured dataDevelopment experience using Hadoop (HDFS, Hive, HBase, Impala) and/or Spark Database development on either relational databases (e.g., PostgreSQL, SQL Server) or NoSQL on distributed platforms (e.g. HBase, Cassandra)Comfortable using the terminal and Linux/UNIX-like systemsIntermediate skills with SQL as well as 1 programming language (Scala, Java, C++, Python, R)Strong understanding of lambda architectures and stream processingExperience with developing solutions on cloud computing services (e.g. Azure, AWS, GCP) Familiarity with Kubernetes and Docker containersExposure to BI tools such as Tableau, Power BI, Looker, R ShinyExperience working on a collaborative Agile product team Experience in defense/national security is a plusSECRET or TS/SCI clearance is a plus', 'Experience with developing solutions on cloud computing services (e.g. Azure, AWS, GCP) ', ""Who You'll Work With"", 'Strong understanding of lambda architectures and stream processing', 'SECRET or TS/SCI clearance is a plus', 'Comfortable using the terminal and Linux/UNIX-like systems', 'Familiarity with Kubernetes and Docker containers', ""What You'll Do"", 'Exposure to BI tools such as Tableau, Power BI, Looker, R Shiny', 'Experience in defense/national security is a plus']",Not Applicable,Full-time,Analyst,Defense & Space,2021-03-18 14:34:51
Data Engineer,BuildOps,Los Angeles Metropolitan Area,2 weeks ago,148 applicants,"['', 'B.S., M.S. or PhD from top university in computer science, engineering, information systems, or related fields', 'Strong Excel skills including VLOOKUP, INDEX-MATCH, and string manipulation', 'Work cross-functionally with the implementation and engineering team.', 'Prior knowledge or ability to quickly learn Node.js-based GraphQL API', 'Experience working with and converting different data formats including JSON and CSV', 'Share', 'Experience with sending and pulling data via RESTful API', 'Communicate effectively with engineers, product managers, customers, partners, and other leaders.', 'Write quality API calls to deliver customer data into our product via our GraphQL API.Propose recommendations to streamline data processes.Work cross-functionally with the implementation and engineering team.Share your technical knowledge and expertise.Communicate effectively with engineers, product managers, customers, partners, and other leaders.', 'Propose recommendations to streamline data processes.', 'Strong SQL skills and ability to pull joined data exports on the fly', 'Write quality API calls to deliver customer data into our product via our GraphQL API.', 'Share your technical knowledge and expertise.', 'Strong Excel skills including VLOOKUP, INDEX-MATCH, and string manipulationStrong SQL skills and ability to pull joined data exports on the flyExperience with sending and pulling data via RESTful APIExperience working with and converting different data formats including JSON and CSVPrior knowledge or ability to quickly learn Node.js-based GraphQL APIB.S., M.S. or PhD from top university in computer science, engineering, information systems, or related fields', 'Requirements to be successful in this role:', 'Work', 'As a Data Engineer, you will:', 'If this is the kind of challenge that motivates you, please send your resume to careers@buildops.com.', 'Propose', 'Write ', 'Communicate', 'You will be joining our LA-based product team. As a pioneering member of the team, your primary focus will be to help build and support our data implementation infrastructure. We are looking for a motivated, self-starter who can work in a distributed team environment. This is a hands-on role. The day-to-day responsibilities are broad, and you will ultimately be responsible for delivering customer data into our product with efficiency and accuracy.', 'Who we are looking for:']",Entry level,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Engineer,Zaxby's Franchising LLC,"Athens, GA",3 days ago,141 applicants,"['', 'Strong ability to articulate and explain situations, issues and solutions to members of the organization not regularly involved in data management', 'Participate in requirements gathering and brainstorming sessions, and be willing to suggest and provide innovative solutions to complex business problems', 'Develop software solutions that can send and receive information via protocols such as HTTPS and SFTP', 'Essential duties may include but are not limited to the following:', 'Working knowledge of Snowflake, Azure Blob Storage, and Azure File Storage', 'Ability to join and contribute effectively to large meetings with individuals from various departments within the organization and at various levels', 'Education:\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Bachelor’s degree in Computer Science, Information Systems, Data Science, or related field', 'Create, support, and distribute reports and visualizations using applicable tools available to the Zaxby’s brand', 'Strong knowledge of relational databases and ERD developmentWorking knowledge of Snowflake, Azure Blob Storage, and Azure File StorageStrong knowledge of Power BI, and Power BI Report BuilderPreferred experience or understanding of IBM PureData Systems and IBM CognosWorking knowledge of legacy ETL tools such as SQL Server Integration Services (SSIS) and IBM DatastageWorking knowledge of cloud-based, on premise, and hybrid data and application architecturesAbility to create software solutions using the .Net Framework and Core Framework for application service developmentFundamental understanding of common structured and unstructured data storage solutionsStrong ability to articulate and explain situations, issues and solutions to members of the organization not regularly involved in data managementAbility to join and contribute effectively to large meetings with individuals from various departments within the organization and at various levelsProficient in common offerings within Office 365 product suite (Word, PowerPoint, Excel)', 'Fundamental understanding of common structured and unstructured data storage solutions', 'Implement structured and non-structured data storage solutions', 'Seek data sources that provide high value to the overall brand data ecosystem', 'Research and stay abreast of industry leading technologies that improve business intelligence and create more effective data structures', 'Understand and perform to standards within project plans set forth by leaders of the organization and the Project Management Office', 'Develop processes and procedures for acquiring data from disparate sourcesEvaluate current ETL/ELT processes for proper fit of accomplishing business needsResearch and stay abreast of industry leading technologies that improve business intelligence and create more effective data structuresEvaluate and implement best practices for Master Data ManagementImplement on premise and cloud based resources to meet any given need for data storage and transmissionDevelop software solutions that can send and receive information via protocols such as HTTPS and SFTPSupport current data storage solutions and migration processes to ensure maximum uptime and usability for the brandCreate, support, and distribute reports and visualizations using applicable tools available to the Zaxby’s brandImplement structured and non-structured data storage solutionsSeek data sources that provide high value to the overall brand data ecosystemParticipate in requirements gathering and brainstorming sessions, and be willing to suggest and provide innovative solutions to complex business problemsPartner with various groups throughout the organization to create collaborative solutions to meet business needsCollaborate with other teams within IT to create a cohesive integration of systems and dataValidate, massage and clean data from all available sources to ensure the highest degree of accuracy and usabilityUnderstand and perform to standards within project plans set forth by leaders of the organization and the Project Management Office', 'Collaborate with other teams within IT to create a cohesive integration of systems and data', 'Evaluate current ETL/ELT processes for proper fit of accomplishing business needs', 'Develop processes and procedures for acquiring data from disparate sources', 'Strong knowledge of relational databases and ERD development', 'Education', 'Preferred experience or understanding of IBM PureData Systems and IBM Cognos', 'Working knowledge of legacy ETL tools such as SQL Server Integration Services (SSIS) and IBM Datastage', 'Proficient in common offerings within Office 365 product suite (Word, PowerPoint, Excel)', 'QUALIFICATIONS', 'Implement on premise and cloud based resources to meet any given need for data storage and transmission', 'Data Engineer (Business Intelligence)', 'Support current data storage solutions and migration processes to ensure maximum uptime and usability for the brand', 'Partner with various groups throughout the organization to create collaborative solutions to meet business needs', 'Strong knowledge of Power BI, and Power BI Report Builder', 'Experience', 'Experience: \xa0\xa0\xa0\xa0\xa0\xa03+ years of experience in Data Engineering (including data structure and ETL/ELT development)', 'Ability to create software solutions using the .Net Framework and Core Framework for application service development', 'Zaxby’s Franchising LLC is an equal opportunity employer and does not discriminate in employment decisions based on any factor protected by federal, state or local law.', 'ESSENTIAL JOB FUNCTIONS', 'Working knowledge of cloud-based, on premise, and hybrid data and application architectures', 'Evaluate and implement best practices for Master Data Management', 'Validate, massage and clean data from all available sources to ensure the highest degree of accuracy and usability', '\xa0', 'The Data Engineer will be responsible for implementing, vetting, and supporting new and existing solutions for ZFL’s data ecosystem.\xa0This individual will use industry-known ETL/ELT tools along with best practices to properly build designed data structures and ETL/ELT processes that provide valuable information based upon what data is available.\xa0He or she will have a thorough understanding of data storage, transfer, and reporting technologies and their capabilities.\xa0There will be a focus on providing data that can be consumed by various reporting and business intelligence tools throughout the organization to support strategic initiatives.\xa0The candidate will need to be comfortable with engaging different areas of the business to gather requirements, evaluate circumstances, and produce results under the guidance of a data architect.', 'EDUCATION AND EXPERIENCE GUIDELINES']",Entry level,Full-time,Information Technology,Restaurants,2021-03-18 14:34:51
Data Engineer,In-Finite Search Solutions,Greater Cleveland,,N/A,"['', 'BS, MA or PhD', 'Python, C++ AI, Project Management', 'design and create AI models for large scale test plans', 'Project Description:', 'Implement processes and systems to monitor data quality, ensuring production data accuracy and assist in data analysis to troubleshoot and resolve data issuesdesign and create AI models for large scale test plansPython, C++ AI, Project Management', 'Responsibilities:', 'Integrate data from a variety of systems into refined data products available to the rest of the enterprise to support both real-time and batch processing.', 'Implement processes and systems to monitor data quality, ensuring production data accuracy and assist in data analysis to troubleshoot and resolve data issues', 'Collaborate with business and IT partners to refine data requirements and perform data analysis activities including data profiling, creation of data dictionaries, data transformation rules and integration requirements.', 'Develop and maintain data pipelines and build out new API integrations to support continuing increases in data volume and complexity.', 'Our team is looking for a data driven person who has engineered and supported critical applications in an automotive environment and is experienced in analytics, business intelligence, data warehousing, artificial intelligence and setting up data pipelines across the enterprise. This seasoned professional will need to interface with multiple client groups and IT management, providing them with data solutions, timely and accurate status updates, short-term / long-term plans, release plans, etc. in a clear consistent manner that emphasizes performance metrics, strategic planning, continuous improvement and adherence to industry best practices.', 'Review and understand data requirements for operational and analytic projects, with a special emphasis on developing scalable data solutions involving data integration, reporting, analytics and data warehousing.']",Mid-Senior level,Full-time,Analyst,Staffing and Recruiting,2021-03-18 14:34:51
Senior Data Engineer,Tempest,"New York, NY",,N/A,"['', ' ', 'Work with Product Managers to grow all aspects of web tagging and clickstream data collection ', 'Applied data warehousing techniques and architecture approaches (including dimensional modeling and alternatives such as Data Vault) ', 'Experience with JavaScript, especially as it relates to 3rd party tagging ', 'Document data objects to produce technical metadata, and our data dictionaries ', '$75 monthly wellness stipend ', 'Manage Google Tag Manager inclusive of tagging documentation, tool configuration and user administration ', 'Medical, Dental, and Vision insurance for employees, dependents, and spouses ', 'Free access to One Medical through Justworks ', 'Equity ', 'Benefits', 'Data modeling skills ', 'Work with other engineering teams to ensure quality, accessibility, and consistency of collected data across web and native platforms ', 'Experience with Tag Management systems like Google Tag Manager ', 'About You ', 'The Sr. Data Engineer will: ', 'Promote data capture on digital products and business processes ', ""Report to the Director of Data & Analytics Work with multiple partners within the organization to establish data requirements Architect the data infrastructure ) Promote data capture on digital products and business processes Develop data pipelines across different sources to collect, stage, and sync data, including between operational (3NF) and analytical (DW) data stores Document data objects to produce technical metadata, and our data dictionaries Manage data loads, data quality checks and defect resolution Work with Product Managers to grow all aspects of web tagging and clickstream data collection Manage Google Tag Manager inclusive of tagging documentation, tool configuration and user administration Develop event architecture for Tempest's web data and integrate into 3rd party platforms Work with other engineering teams to ensure quality, accessibility, and consistency of collected data across web and native platforms "", 'Develop data pipelines across different sources to collect, stage, and sync data, including between operational (3NF) and analytical (DW) data stores ', '18 days PTO 12 holidays including Juneteenth and ""Fall-Food-Weekend (our inclusive version of Thanksgiving) Medical, Dental, and Vision insurance for employees, dependents, and spouses Free access to One Medical through Justworks Healthcare and Dependent care FSAs $75 monthly wellness stipend Equity 401k No meeting Fridays A code of conduct with reporting structure', 'Architect the data infrastructure ) ', '18 days PTO ', '401k ', 'Work with multiple partners within the organization to establish data requirements ', '5+ years cloud experience (preferably with AWS, GCP, and Heroku) from a data engineering standpoint (e.g. building ETL/ELT pipelines between cloud providers) Data modeling skills Expert-level ISO SQL skills (up to SQL:2016 standard - Postgres/BigQuery experience a plus) Experience with Tag Management systems like Google Tag Manager Applied data warehousing techniques and architecture approaches (including dimensional modeling and alternatives such as Data Vault) Experience with JavaScript, especially as it relates to 3rd party tagging ', ""Develop event architecture for Tempest's web data and integrate into 3rd party platforms "", 'Manage data loads, data quality checks and defect resolution ', '5+ years cloud experience (preferably with AWS, GCP, and Heroku) from a data engineering standpoint (e.g. building ETL/ELT pipelines between cloud providers) ', 'Expert-level ISO SQL skills (up to SQL:2016 standard - Postgres/BigQuery experience a plus) ', '12 holidays including Juneteenth and ""Fall-Food-Weekend (our inclusive version of Thanksgiving) ', 'Healthcare and Dependent care FSAs ', 'No meeting Fridays ', 'About You', 'A code of conduct with reporting structure', 'Report to the Director of Data & Analytics ', 'The Sr. Data Engineer will:']",Associate,Full-time,Information Technology,"Health, Wellness and Fitness",2021-03-18 14:34:51
Data Engineer,Advisor360°,"Weston, MA",2 weeks ago,115 applicants,"['', 'Experience with .NET, Python, or PowerShell', 'Must be self-motivated, detail-oriented, and have excellent communication and interpersonal skills', 'Bachelor’s degree in computer science or equivalent work experience of 3+ years3+ years of hands-on SQL and T-SQL programming skills2+ years of experience in a highly regulated data environment such as financial services or healthcare.2+ years of experience with enterprise data management tool sets such as Collibra, Informatica, Purview or Alation.2+ years’ experience with data sharing through APIs in any scripting language - .Net and Python are a plus.Strong exploratory, problem-solving, and analytical skills as related to data quality, categorization, and transformation tasksExperience working against datasets of varying size, type, and volumeDemonstrated ability to create and deliver high quality code using software engineering best practices including coding standard, code review, source control management, build process, testing, and operations.Must be self-motivated, detail-oriented, and have excellent communication and interpersonal skillsMust demonstrate flexibility, self-direction, and a growth mindset that is open to change.', 'Additional skills and knowledge', 'Responsible for data and process improvement, identifying methods to enhance existing methods.', 'Strong exploratory, problem-solving, and analytical skills as related to data quality, categorization, and transformation tasks', 'Exposure to cloud ETL products such as Azure Data Factory or Databricks', 'Create processes to aggregate and auto categorize data categories.', 'The Data Engineer role will be part of Advisor360°’s Engineering\xa0organization, as a member of our Data Management team. The primary responsibilities of this role will be creating and technically supporting data management, quality and privacy frameworks and processes. Working across the organization, the data engineer will assist in the selection, implementation and adoption of enterprise data management tooling and integration.\xa0They will also be responsible for creating processes improve overall data quality, ensure that data management and quality requirements are represented in the overall data pipeline, and implement our data privacy framework.', 'Data modeling experienceExperience with .NET, Python, or PowerShellExposure to machine learning concepts for automated data categorizationExposure to cloud ETL products such as Azure Data Factory or DatabricksKnowledge of laws and regulations that can impact data environments and use, such as GDPR, CCPA, HIPAA, etc.', '2+ years’ experience with data sharing through APIs in any scripting language - .Net and Python are a plus.', 'Responsible for the design, development, and support of data management, quality and privacy processes, frameworks, and tools. ', '2+ years of experience with enterprise data management tool sets such as Collibra, Informatica, Purview or Alation.', '2+ years of experience in a highly regulated data environment such as financial services or healthcare.', 'Requirements', 'Data modeling experience', 'Responsible for the design, development and implementation of data quality assessment, verification, and improvement practices.', '3+ years of hands-on SQL and T-SQL programming skills', 'Must demonstrate flexibility, self-direction, and a growth mindset that is open to change.', 'Experience working against datasets of varying size, type, and volume', 'Exposure to machine learning concepts for automated data categorization', 'Knowledge of laws and regulations that can impact data environments and use, such as GDPR, CCPA, HIPAA, etc.', 'Adhere to best practices in engineering and data management', 'Bachelor’s degree in computer science or equivalent work experience of 3+ years', '\u200b', 'Responsible for the design, development, and support of data management, quality and privacy processes, frameworks, and tools. Responsible for data and process improvement, identifying methods to enhance existing methods.Responsible for the design, development and implementation of data quality assessment, verification, and improvement practices.Adhere to best practices in engineering and data managementCreate processes to aggregate and auto categorize data categories.', 'Demonstrated ability to create and deliver high quality code using software engineering best practices including coding standard, code review, source control management, build process, testing, and operations.', 'Key responsibilities']",Mid-Senior level,Full-time,Information Technology,Financial Services,2021-03-18 14:34:51
Data Engineer,inSync Staffing,"Creve Coeur, MO",1 day ago,Be among the first 25 applicants,"['', ' Be a critical senior member of a data engineering team focused on creating distributed analysis capabilities around a large variety of datasets', 'NO', 'Interview preparation ', 'What We Offer For You', 'Informational career centric seminars. ', ' Interview preparation  Resume review and recommendations  LinkedIn review  Local job market information, trends, and insights  Access to networking events  Informational career centric seminars.  ', 'Neteffects Consultant Care Program', 'If You Share Our Values, You Should Have', ' Familiarity with creating and maintaining containerized application deployments with a platform like Docker', 'Remarketing:', ' Work with other top-level talent solving a wide range of complex and unique challenges that have real world impact', ' Project your talent into relevant projects. Strength of ideas trumps position on an org chart', ' CORP TO CORP ', 'Neteffects Benefits:', ' Experience with stream processing using Apache Kafka', ' A proven ability to build and maintain cloud based infrastructure on a major cloud provider like AWS, Azure or Google Cloud Platform', ' Experience working with scientific datasets, or a background in the application of quantitative science to business problems', ' Take pride in software craftsmanship, apply a deep knowledge of algorithms and data structures to continuously improve and innovate', 'Access to networking events ', ' Experience data modeling for large scale databases, either relational or NoSQL', 'Neteffects processes visas and green cards*', ' Experience with protocol buffers and gRPC', 'Neteffects', 'LinkedIn review ', ' Proven experience (2 years) building and maintaining data-intensive APIs using a RESTful approach', ""Neteffects processes visas and green cards*Upon hire neteffects can provide AWS training via neteffects university*We’re committed to empowering each individual who works with us to take charge of their career. You won’t be alone on your journey. Our dedicated team is here to serve you as a partner in the process.What We Offer For You Interview preparation  Resume review and recommendations  LinkedIn review  Local job market information, trends, and insights  Access to networking events  Informational career centric seminars.  Neteffects Consultant Care ProgramKeeping in Touch: Whether it’s a quick call or email, coffee or lunch, our check-ins support as much or as little interaction as you’d prefer. We’re here if you need us.Events: Neteffects participates in a number of events each year and hosts some employee-only social gatherings.Remarketing: When your project is nearing completion, we’ll partner with you to get you ready to engage in your next consulting opportunity through Neteffects. From resume review, to interview preparation and offer negotiation; we’ll work hard to help you excel.Neteffects Benefits: Neteffects is about celebrating technology and the pro's that come with it. We offer a very competitive benefits package, which includes Medical, Dental, Vision, Life, 401k, and more.We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status."", 'Events:', ' A level of comfort with Unit Testing and Test Driven Development methodologies', ' At least 2 years experience with Go', 'Local job market information, trends, and insights ', 'Resume review and recommendations ', 'Data Engineer', 'Google Cloud Blog: https://cloud.google.com/blog/products/containers-kubernetes/google-kubernetes-engine-clusters-can-have-up-to-15000-nodes', ' Bioinformatics experience, especially large scale storage and data mining of variant data, variant annotation, and genotype to phenotype correlation', 'Keeping in Touch:', 'What you will do is why you should join us:', ' Pursue opportunities to present our work at relevant technical conferences', ' Explore relevant technology stacks to find the best fit for each dataset', ' Experience with: Google Cloud Platform, Apache Beam and or Google Cloud Dataflow, Google Kubernetes Engine or Kubernetes', ""Upon hire neteffects can provide AWS training via neteffects university*We’re committed to empowering each individual who works with us to take charge of their career. You won’t be alone on your journey. Our dedicated team is here to serve you as a partner in the process.What We Offer For You Interview preparation  Resume review and recommendations  LinkedIn review  Local job market information, trends, and insights  Access to networking events  Informational career centric seminars.  Neteffects Consultant Care ProgramKeeping in Touch: Whether it’s a quick call or email, coffee or lunch, our check-ins support as much or as little interaction as you’d prefer. We’re here if you need us.Events: Neteffects participates in a number of events each year and hosts some employee-only social gatherings.Remarketing: When your project is nearing completion, we’ll partner with you to get you ready to engage in your next consulting opportunity through Neteffects. From resume review, to interview preparation and offer negotiation; we’ll work hard to help you excel.Neteffects Benefits: Neteffects is about celebrating technology and the pro's that come with it. We offer a very competitive benefits package, which includes Medical, Dental, Vision, Life, 401k, and more.We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status."", 'Bonus Points For', 'Google Cloud Next 2019: https://www.youtube.com/watch?v=fqvuyOID6v4', 'GraphConnect 2015: https://www.youtube.com/watch?v=6KEvLURBenM']",Entry level,Contract,Information Technology,Construction,2021-03-18 14:34:51
Data Engineer,Pandera Systems,"Illinois, United States",2 weeks ago,95 applicants,"['', 'Pandera Systems is a highly specialized analytics and technology consulting firm with a core focus in developing data-driven solutions in the Cloud. We have strategic partnerships with some of the largest Cloud Platforms, including Premier Partner with Google Cloud Platform and Premier Partner of Snowflake. We have been providing innovative data driven solutions to some of the world’s largest companies for over 11 years.', 'Be Rewarded: A competitive salary and instant vesting on 401k are only a few of the rewards for a job well done.Be Healthy: Health, dental, and vision offered through top tier providers and unlimited sick leave to keep you feeling at the top of your game.Be Inspired: Collaborative workspace and unlimited vacation to keep your mind fresh and ready to take on the next new idea.Be Supported: A large network of industry experts, internal training platform, and external learning opportunities to grow your skills and experience.Be a Team: Team outings, happy hours, passion presentations, volunteer opportunities, meetups, etc. we are creating a community to continuously share and grow as a team.', '3-5+ years of experience in Data Engineering/BI areas with at least 2 years data engineering on GCP·', '3-5+ years of experience in Data Engineering/BI areas with at least 2 years data engineering on GCP·Experience working in GCP based Big Data deployments (Batch/Real-Time) leveraging Big Query, Big Table, Google Cloud Storage, PubSub, Data Fusion, Dataflow, Dataproc, etc.Experience developing logical data models within cloud data warehousesExperience developing and deploying ETL / ELT processes and documentation including physical data model, source to target mappings, ETL / ELT packages (Matillion, Fivetran, Spark, Google Data Fusion, etc.)Demonstrated mastery in Google BigQueryDemonstrated mastery in cloud database concepts and large-scale cloud data warehouse and lake implementationsImplement solutions for structured, semi-structured, and unstructured data sources, relational and non-relational databases.Proven ability to work with users to define requirements and business issuesExcellent analytical and troubleshooting skillsStrong written and oral communication skills', 'Advanced Java/Python coding skills', 'Primary Responsibilities:', 'MDM expertise', 'Be Supported: A large network of industry experts, internal training platform, and external learning opportunities to grow your skills and experience.', 'Extracting, Loading, Transforming, cleaning, and validating data using cloud ETL/ELT tools', 'GIT expertise', '*Authorized to work in the USA', 'Proven ability to work with users to define requirements and business issues', 'Be Rewarded: A competitive salary and instant vesting on 401k are only a few of the rewards for a job well done.', 'Infrastructure as Code experience (Terraform, Ansible, etc.)', 'Do you like data? Do you know how to take data and transform it to get meaningful data into end users’ hands? Do you enjoy taking complex problems and turning them into technical solutions that power Reporting Solutions, Advanced Analytics, and Data Sciences? We are looking for a Cloud Data Engineer to join our team of engineers, analysts, and architects focused on designing, building, and delivering data solutions that drive business decisions. As a\xa0Pandera\xa0Employee, you will have the opportunity to build world-class solutions to help our clients and partners solve challenging problems through data.', 'Excellent analytical and troubleshooting skills', '*Local Hire or candidates relocating to Ohio, Indiana, Illinois, or Michigan', 'Certified Google Cloud Data Engineer / ArchitectExperience working in an AGILE environmentAdvanced Java/Python coding skillsExperience with CI-CD pipelines for promoting big data release deployments and designing log monitoring features.GIT expertiseMDM expertiseInfrastructure as Code experience (Terraform, Ansible, etc.)Experience with Deploying a Data Governance Program', 'Experience developing logical data models within cloud data warehouses', 'Salary: Competitive rate based on qualifications', 'Type:', ""If working with the world's leading Cloud and Data Platforms is something that you are interested in, and you like to be able to have the ability to work on building next generation smart data platforms, then Pandera is the right place for you. From migration and modernization of legacy data systems to the Cloud, along with building Cloud Native solutions for our customers, the adventure will never be dull, and you will always be learning and growing with Pandera."", 'Certified Google Cloud Data Engineer / Architect', 'Experience working in an AGILE environment', 'Designing pipelines and architectures for data processing', 'Location*:', 'Salary', '*Fully remote option available ', 'Ideal Qualifications:', 'Demonstrated mastery in Google BigQuery', 'Strong written and oral communication skills', 'Experience developing and deploying ETL / ELT processes and documentation including physical data model, source to target mappings, ETL / ELT packages (Matillion, Fivetran, Spark, Google Data Fusion, etc.)', 'Be Inspired: Collaborative workspace and unlimited vacation to keep your mind fresh and ready to take on the next new idea.', 'Location*:\xa0This GCP Cloud Data Engineer position\xa0will be based in the Midwest', 'Experience working in GCP based Big Data deployments (Batch/Real-Time) leveraging Big Query, Big Table, Google Cloud Storage, PubSub, Data Fusion, Dataflow, Dataproc, etc.', 'Be a Team: Team outings, happy hours, passion presentations, volunteer opportunities, meetups, etc. we are creating a community to continuously share and grow as a team.', ""Education:\xa0Bachelor's or Master’s degree in technical discipline; Master's preferred, or equivalent years’ of experience in the field."", 'Experience with Deploying a Data Governance Program', 'Data modeling and schema design that will range across multiple business domains and industries within the cloud for large enterprise data warehouse and data lakes solutions.', 'You are a hot commodity and having you on the team would be an honor to us! Here are some of the ways we pay it forward to recognize your contribution to our vision!', 'Travel:\xa0Onsite client location(s) - overnight is based on the needs of the client.', 'Work with teams to conduct workshops to identify data sources, flows, and requirements.', 'Be Healthy: Health, dental, and vision offered through top tier providers and unlimited sick leave to keep you feeling at the top of your game.', 'Primary Responsibilities:Design and build data engineering solutions using Google Cloud Platform (GCP) services: BigQuery, DataFlow, Pub/Sub, BigTable, Data Fusion, DataProc', 'Implement solutions for structured, semi-structured, and unstructured data sources, relational and non-relational databases.', 'Conduct client workshops to help shape their future data strategy by providing future state architectures, roadmaps, and implementation plans.', 'Travel:', 'Experience with CI-CD pipelines for promoting big data release deployments and designing log monitoring features.', 'Education:', 'What do you get out of this?', 'Basic Qualifications:', 'Demonstrated mastery in cloud database concepts and large-scale cloud data warehouse and lake implementations', 'Type:\xa0Full time ', 'Partner with multiple client stakeholders including partners, business users, BI and Analytics teams.', 'Primary Responsibilities:Design and build data engineering solutions using Google Cloud Platform (GCP) services: BigQuery, DataFlow, Pub/Sub, BigTable, Data Fusion, DataProcData modeling and schema design that will range across multiple business domains and industries within the cloud for large enterprise data warehouse and data lakes solutions.Extracting, Loading, Transforming, cleaning, and validating data using cloud ETL/ELT toolsDesigning pipelines and architectures for data processingPartner with multiple client stakeholders including partners, business users, BI and Analytics teams.Work with teams to conduct workshops to identify data sources, flows, and requirements.Conduct client workshops to help shape their future data strategy by providing future state architectures, roadmaps, and implementation plans.']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Wyre,"San Francisco, CA",1 month ago,173 applicants,"['', 'Perform analysis, conceptualize, design, implement and develop solutions for critical BI components from the ground up', 'Plan and implement standards, define/code ', 'Provide timely and accurate estimates for new functionality requirements', 'Enjoy a highly fulfilling, mission-driven culture', '5+ years of developing end-to-end Business Intelligence solutions Data Warehouse, data modeling, ETL, and reporting', 'Fast-learner and ability to pivot priorities quickly', 'Experience with ETL tools like Xplenty/AWS Glue/Talend/Nifi', 'Creative problem solver ', 'Health, dental, and vision benefits for you and your family', 'Support the performance of BI systems', "" Bachelor's degree in Computer Science or a related technical field from an accredited institution Mastery in Business Intelligence and Data warehousing concepts and methodologies 5+ years of developing end-to-end Business Intelligence solutions Data Warehouse, data modeling, ETL, and reporting Experience with Cloud-based data-warehouse system Snowflake, Redshift, Azure SQL Data warehouse is a huge plus Experience with ETL tools like Xplenty/AWS Glue/Talend/Nifi Advanced SQL writing and experience in data mining (SQL, ETL, data warehouse, etc.) Skills in Shell Programming and any Object Oriented Programming language Should be able to write custom ETL programming from complex data processing and transformations using any programming language, such as PL/Sql, Python, Java, etc Fast-learner and ability to pivot priorities quickly Close attention to detail and strong organizational skills Exceptional ability to communicate across teams and departments Creative problem solver  "", 'Benefits', 'Equity options for all full-time employees', 'Responsibilities', 'Opportunity to work in a growing startup', 'Experience with Cloud-based data-warehouse system Snowflake, Redshift, Azure SQL Data warehouse is a huge plus', 'Advanced SQL writing and experience in data mining (SQL, ETL, data warehouse, etc.)', 'Unlimited paid time off to relax and recharge', 'Computer setup of your choice', 'Requirements', 'Flexible work hours', 'Perform data-flow, system, and data analysis to develop meaningful presentations of data in BI applications', ""Bachelor's degree in Computer Science or a related technical field from an accredited institution"", 'An opportunity to build the future and freedom to work wherever you want', 'Exceptional ability to communicate across teams and departments', 'Should be able to write custom ETL programming from complex data processing and transformations using any programming language, such as PL/Sql, Python, Java, etc', 'Strong verbal and written communication skills, including the ability to effectively lead and influence interactions with both business and technical teams', 'Collaborate closely with internal and external teams to understand and apply changes/modifications impacting data warehouse', 'You are an owner! We offer stock options to each of our employee', 'Fair pay, no matter where you live along with a competitive benefits package', 'Mastery in Business Intelligence and Data warehousing concepts and methodologies', ' Enjoy a highly fulfilling, mission-driven culture You are an owner! We offer stock options to each of our employee An opportunity to build the future and freedom to work wherever you want Fair pay, no matter where you live along with a competitive benefits package Health, dental, and vision benefits for you and your family Life insurance and disability benefits Equity options for all full-time employees 401(k) plan with corporate matching Computer setup of your choice Unlimited paid time off to relax and recharge Flexible work hours Opportunity to work in a growing startup', ' Perform analysis, conceptualize, design, implement and develop solutions for critical BI components from the ground up Strong verbal and written communication skills, including the ability to effectively lead and influence interactions with both business and technical teams Perform data-flow, system, and data analysis to develop meaningful presentations of data in BI applications Plan and implement standards, define/code  Collaborate closely with internal and external teams to understand and apply changes/modifications impacting data warehouse Monitor ETL processes, system audits, and performance. Proactively resolve issues as found Support the performance of BI systems Provide timely and accurate estimates for new functionality requirements ', 'Monitor ETL processes, system audits, and performance. Proactively resolve issues as found', '401(k) plan with corporate matching', 'Life insurance and disability benefits', 'Skills in Shell Programming and any Object Oriented Programming language', 'Close attention to detail and strong organizational skills']",Mid-Senior level,Full-time,Engineering,Computer Software,2021-03-18 14:34:51
Data Engineer,Real Chemistry,"New York, United States",1 week ago,58 applicants,"['', 'This Data Engineer will collaborate with executive level internal and external stakeholders. They will lead the development, delivery and implement of AI, IOT, data engineering and data analytics projects which will leverage data to develop industry leading business insights. This person will focus on solutions such as machine learning, IoT, batch/real-time data processing, data and business intelligence while ensuring enterprise wide data security and integrity.', 'Extensive, demonstrated expertise with Python. should be generally comfortable in different categories (machine learning, development, scripting, etc.)', 'Design and implement data ingestion solutions on GCP using GCP native services', 'Hands on experience leading enterprise-wide data engineering, warehousing and analytics projects', '5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative fieldAdvanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.Extensive, demonstrated expertise with Python. should be generally comfortable in different categories (machine learning, development, scripting, etc.)Experience in building real time streaming data ingestion and processing pipeline using Apache Beam (running on either Google Dataflow or Apache (Apex Flink, or Spark) or Kafka in an analytics or data science use caseBuild processes supporting data transformation, data structures, metadata, dependency and workload management.Strong analytic skills related to working with unstructured datasets.Hands on experience leading enterprise-wide data engineering, warehousing and analytics projectsExperience with data pipeline and workflow management tools: Airflow, Oozie etc.Ability to think strategically about business, product, and technical challenges in an enterprise environmentUnderstanding of database and analytical technologies in the industry including MPP and NoSQL databases, Data Warehouse design, PostgreSQL, BI reporting and Dashboard developmentDemonstrated industry efficiency in the fields of database, data warehousing or data sciencesCustomer facing skills with the ability to drive discussions with senior leadership regarding trade-offs, best practices and risk mitigationDesire and ability to interact with all levels of the organization', 'Own the Insights gleaned by the creation of advanced technology roadmaps. Share real world implementations and recommend new capabilities that would simplify adoption and drive greater value.', 'Strong analytic skills related to working with unstructured datasets.', 'Design and implement data ingestion solutions on GCP using GCP native servicesDemonstrated experience with distributed computingDesign and optimize data models on GCP cloud using GCP data stores such as BigQueryIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.Delivery of cloud architecture to support new distributed computing solutions that often span the full array of cloud services. This will include migration of existing applications and development of new applications using cloud services.Own the Insights gleaned by the creation of advanced technology roadmaps. Share real world implementations and recommend new capabilities that would simplify adoption and drive greater value.Innovate and engage with key technology stakeholders to create a compelling vision of a data-driven enterprise environment and the impact it will have on their teams, their projects and their outcomes.', 'Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.', 'Responsibilities', 'Experience in building real time streaming data ingestion and processing pipeline using Apache Beam (running on either Google Dataflow or Apache (Apex Flink, or Spark) or Kafka in an analytics or data science use case', '5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field', 'Build processes supporting data transformation, data structures, metadata, dependency and workload management.', 'Demonstrated industry efficiency in the fields of database, data warehousing or data sciences', 'Demonstrated experience with distributed computing', 'Requirements', 'Delivery of cloud architecture to support new distributed computing solutions that often span the full array of cloud services. This will include migration of existing applications and development of new applications using cloud services.', 'Customer facing skills with the ability to drive discussions with senior leadership regarding trade-offs, best practices and risk mitigation', 'This role will be that of an over-arching Data Engineering guru. The ideal candidate will connect and work closely with Data Warehousing, Data Analytics, SW Engineering and Data Sciences as well as Product and Project Management. They will be responsible for solving some of the most complex and high scale data Engineering challenges in our industry while impacting the lives of healthcare professionals and their patients. The ideal candidate must be technologically curious, driven to work with some of newest data engineering technologies on the marketplace today. They will be a visionary and a key driver in the rapid adoption and scalability of crucial data engineering, warehousing and cloud-based technologies.', 'Experience with data pipeline and workflow management tools: Airflow, Oozie etc.', 'Understanding of database and analytical technologies in the industry including MPP and NoSQL databases, Data Warehouse design, PostgreSQL, BI reporting and Dashboard development', 'Innovate and engage with key technology stakeholders to create a compelling vision of a data-driven enterprise environment and the impact it will have on their teams, their projects and their outcomes.', 'As a Data Engineer on the technology team, you’ll be responsible for building and maintaining our data warehouse environment, maintaining our internal data processes, and implementing and supporting business intelligence capabilities across multiple business divisions. You will play a key role in helping design our enterprise data architecture to support overarching data governance and related initiatives such as data architecture management, data security management, and data quality management.', 'Data Engineer', 'Desire and ability to interact with all levels of the organization', 'Role:', '\xa0', 'Design and optimize data models on GCP cloud using GCP data stores such as BigQuery', 'Recently named Best Place to Work by MM&M, The Holmes Report, PR News, PRWeek, and AdAge, Real Chemistry is an integrated marketing and communications firm powered by analytics and specializing in healthcare.\xa0We are currently seeking a talented Data Engineer to join our growing team of pharmaceutical advertising, technology and data professionals. This is a great opportunity to join a dynamic, fast-growing global agency.', 'Ability to think strategically about business, product, and technical challenges in an enterprise environment', 'Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.', 'Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.']",Associate,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Data Engineer,Vaco,Nashville Metropolitan Area,,N/A,"['', 'Bachelor’s degree in an analytical area such as MIS, Finance, or Business Analytics and 2-5 years of experience in reporting, analytics or business insights', 'Performs data analysis required to troubleshoot data related issues and assists in the resolution of data issues.', 'Experience in data visualization, including the ability to design meaningful data analyses in Tableau, PowerBI or similar data visualization software.', 'Minimum Qualifications:', 'Collaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.', 'Strong project and time management skills with experience partnering across multiple stakeholders', 'Key Responsibilities:', '**This role will offer a relocation package for the right candidate**', 'Minimum Qualifications', 'The Analytics Engineer will be responsible for expanding and optimizing our data architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data wrangler who enjoys optimizing data systems and building them from the ground up. The Analytics Engineer will support a broad range of business stakeholders on all things analytics and data initiatives, while ensuring a consistent and optimal data delivery architecture. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products, tools and data initiatives.', 'Experience working with and manipulating large and disparate data sources with tools like SQL, Alteryx, etc.', 'Bachelor’s degree in an analytical area such as MIS, Finance, or Business Analytics and 2-5 years of experience in reporting, analytics or business insights2+ years experience in relational cloud based database technologies like Snowflake, Amazon Redshift, etc.3+ years experience with Alteryx and other data blending tools.Strong understanding of data extraction and manipulation from ERP systems like SAPExperience in data visualization, including the ability to design meaningful data analyses in Tableau, PowerBI or similar data visualization software.Experience working with and manipulating large and disparate data sources with tools like SQL, Alteryx, etc.Ability to distill, cater, consult, and communicate complex analysis and insights to senior leaders, both verbally and in writingExcellent problem solving and troubleshooting skillsStrong project and time management skills with experience partnering across multiple stakeholders', 'Maintain a good knowledge of relevant analytical techniques.', '2+ years experience in relational cloud based database technologies like Snowflake, Amazon Redshift, etc.', 'Ensures existing data sources are always accurate and available for key stakeholders and business processes that depend on it.', 'Ability to distill, cater, consult, and communicate complex analysis and insights to senior leaders, both verbally and in writing', '3+ years experience with Alteryx and other data blending tools.', 'Position Summary:', 'Excellent problem solving and troubleshooting skills', '**Team is currently remote, but plan to come back into the office when safe**', 'Strong understanding of data extraction and manipulation from ERP systems like SAP', 'Design, deploy and manage data integrations and platforms from a wide variety of data sources using Snowflake, SQL, and AlteryxCollaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.Performs data analysis required to troubleshoot data related issues and assists in the resolution of data issues.Ensures existing data sources are always accurate and available for key stakeholders and business processes that depend on it.Seeks out and designs new data sources and integrations to aid in new insights and metrics to achieve business objectives, drive business growth and revenue.Maintain a good knowledge of relevant analytical techniques.', '\xa0', 'Seeks out and designs new data sources and integrations to aid in new insights and metrics to achieve business objectives, drive business growth and revenue.', 'Design, deploy and manage data integrations and platforms from a wide variety of data sources using Snowflake, SQL, and Alteryx']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,RemoteHub,"Home, KS",2 days ago,Be among the first 25 applicants,"['', ' Data Studio Job', ' Data Quality management. Test Cases etc.', ' Apply for this Data Engineer position', ' Experience designing and building scalable and robust data pipelines to enable data-driven decisions for the business.', ' Develop, implement and tune ETL processes.', ' Gathering technical requirement from customer and enable the right team to develop and implement it. Experience designing and building scalable and robust data pipelines to enable data-driven decisions for the business. Understand business processes, logical data models and relational database implementations for data analysis. Build data expertise and implement own data quality test cases for required areas. Expert knowledge of SQL and of relational database systems and concepts. Expert knowledge in Azure Data Factory, Databricks Script, Scope Scripts, Cosmos etc. Demonstrated strength in data modeling, ETL development, and data warehousing Develop, implement and tune ETL processes. Experience analyzing data to discover opportunities and address gaps. Develop and maintain data pipelines including solutions for data collection, management, and usage. Develop and implement solutions for data quality validation and continuous improvement. Drive our data platform and help evolve our technology stack and development best practices Develop and unit test assigned features to meet product requirements. Working knowledge of data quality approaches and techniques. Programming language experience (C#, Python, Scala Spark.) is a plus. Build visualizations in PowerBI to help derive meaningful insights from data. Maintain and enhance our data and computation platform up and running. Accountable for ensuring that project tasks are accomplished to schedule at acceptable level of quality; or that appropriate contingencies are implemented. Ensure documentation of all project artefacts are accurate and current. Ensure all projects are delivered successfully and according to Engagement statement of work process, methodology, and quality standards.', ' Programming language experience (C#, Python, Scala Spark.) is a plus.', ' Accountable for ensuring that project tasks are accomplished to schedule at acceptable level of quality; or that appropriate contingencies are implemented.', ' Drive our data platform and help evolve our technology stack and development best practices', ' Databricks', ' Expert knowledge of SQL and of relational database systems and concepts.', ' Understand business processes, logical data models and relational database implementations for data analysis.', ' Scope Scripts, Cosmos', ' PowerBI', ' SQL Azure, Synapse (SQL Datawarehouse)', ' Working knowledge of data quality approaches and techniques.', ' Build data expertise and implement own data quality test cases for required areas.', ' Demonstrated strength in data modeling, ETL development, and data warehousing', ' Gathering technical requirement from customer and enable the right team to develop and implement it.', ' Develop and maintain data pipelines including solutions for data collection, management, and usage.', ' Kusto', ' Azure Data Factory', ' Experience analyzing data to discover opportunities and address gaps.', ' Expert knowledge in Azure Data Factory, Databricks Script, Scope Scripts, Cosmos etc.', ' Azure Data Lake, Blob Storage', ' Build visualizations in PowerBI to help derive meaningful insights from data.', ' Develop and implement solutions for data quality validation and continuous improvement.', ' Maintain and enhance our data and computation platform up and running.', 'Solid Familiarity On The Following Tools', ' Develop and unit test assigned features to meet product requirements.', ' SQL Azure, Synapse (SQL Datawarehouse) Azure Data Factory Kusto Scope Scripts, Cosmos PowerBI Azure Data Lake, Blob Storage Databricks Data Quality management. Test Cases etc. Data Studio Job', ' Ensure documentation of all project artefacts are accurate and current.', ' Ensure all projects are delivered successfully and according to Engagement statement of work process, methodology, and quality standards.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,JUST Capital,"New York, NY",1 week ago,51 applicants,"['', 'Fully paid parental leave of up to 12 weeks.', '20 personal time off days, 5 sick days, as well as 15 holidays a year – and we encourage you to take them!', 'Best-in-class health, vision, and dental care benefits for you and your family including membership in One Medical Group for on-demand primary care and access to Health Advocate, the nation’s leading healthcare advocacy & assistance company.Respect for work-life balance, including a flexible work from home policy.20 personal time off days, 5 sick days, as well as 15 holidays a year – and we encourage you to take them!Other paid time off when the office generally closes the last week of the year.Fully paid parental leave of up to 12 weeks.401(k) retirement plan, with employer matching of 100% up to 4% of salary.Subsidized gym memberships, Citi Bike membership, and pre-tax commuter benefits.', '401(k) retirement plan, with employer matching of 100% up to 4% of salary.', ""Bachelor's degree in Math, Statistics, Computer Science, or other quantitative discipline. 3+ years of experience in enterprise and open source ETL tools such as Talend or Pentaho PDI platform2+ year experience in SQL with focus on Business Intelligence and Data warehousing3+ year experience in Python development; focus on data engineering or data science packages like numpy, pandas, scipy, and etc.Working experience in designing and orchestrating Pentaho kettle transformations and jobs flowsProven experience of Data Architecture and Data Modeling, developing and implementing metadata and master data strategies in data warehouse and mart environmentsThorough understanding of relational and dimensional database modelingExperience in ODS design, data warehouse and mart design methodologies such as Star-schema, Snowflake, designing slowly changing dimensions and fact tablesExperience in extracting data from disparate and heterogeneous data sources like mysql, Flat files, ftp, web services, XML and weblogs into target ODS and Data warehouse"", 'Best-in-class health, vision, and dental care benefits for you and your family including membership in One Medical Group for on-demand primary care and access to Health Advocate, the nation’s leading healthcare advocacy & assistance company.', 'Other paid time off when the office generally closes the last week of the year.', 'Experience in ODS design, data warehouse and mart design methodologies such as Star-schema, Snowflake, designing slowly changing dimensions and fact tables', 'Responsible for database administration and tuning.', '2+ year experience in SQL with focus on Business Intelligence and Data warehousing', '3+ year experience in Python development; focus on data engineering or data science packages like numpy, pandas, scipy, and etc.', 'Developing and maintaining programs on source systems, ETL applications, data cleansing functions, system management functions including load automation, data acquisition functions and others.', 'Proven experience of Data Architecture and Data Modeling, developing and implementing metadata and master data strategies in data warehouse and mart environments', ""Bachelor's degree in Math, Statistics, Computer Science, or other quantitative discipline. "", 'Extremely strong analytical and problem-solving skills', 'Working knowledge of Amazon workflow services', 'Design, implement, and support Data Models, ETLs that provide structured and timely access to large datasets.', 'Refine the logical design so that it can be translated into a specific data model.', 'Develop, institutionalize and drive best practice and architectural awareness.', 'Development of the data model(s) for the DW or a data source', 'Working experience in designing and orchestrating Pentaho kettle transformations and jobs flows', 'Proven track record of solving challenging problems in both academia and industry ', 'Build fault tolerant, self-healing, adaptive and highly accurate ETL platforms.', 'Experience in extracting data from disparate and heterogeneous data sources like mysql, Flat files, ftp, web services, XML and weblogs into target ODS and Data warehouse', 'Respect for work-life balance, including a flexible work from home policy.', 'Provide technical guidance, direction and leadership on unusual or complex problems.', 'Subsidized gym memberships, Citi Bike membership, and pre-tax commuter benefits.', 'Requirements (must Be Able To Demonstrate With Past Experience)', 'Thorough understanding of relational and dimensional database modeling', 'Working knowledge of Amazon workflow servicesExperience in managing big data for BI and insight analyticsExtremely strong analytical and problem-solving skills', 'Provide technical guidance, direction and leadership on unusual or complex problems.Design, implement, and support Data Models, ETLs that provide structured and timely access to large datasets.Build fault tolerant, self-healing, adaptive and highly accurate ETL platforms.Developing and maintaining programs on source systems, ETL applications, data cleansing functions, system management functions including load automation, data acquisition functions and others.Development of the data model(s) for the DW or a data sourceResponsible for database administration and tuning.Refine the logical design so that it can be translated into a specific data model.Develop, institutionalize and drive best practice and architectural awareness.', 'Experience in managing big data for BI and insight analytics', '3+ years of experience in enterprise and open source ETL tools such as Talend or Pentaho PDI platform']",Mid-Senior level,Full-time,Information Technology,Nonprofit Organization Management,2021-03-18 14:34:51
Data Engineer,Honeywell,"Atlanta, GA",3 weeks ago,43 applicants,"['', 'Identify opportunities for data acquisition', 'Basic familiarity with Machine Learning concepts', ' Bachelor’s degree 5 years of experience with at least one statistical programming language: Python, R, SAS, Julia (Python Preferred) 5 years of experience using SQL with any database 2 years of technical expertise with data models, data mining, and segmentation techniques ', 'Previous experience to work as Data Engineer', 'Category: ', 'Excellent oral, written, and presentation skills with the ability to deal tactfully, confidently, and ethically with both internal and external customers', 'Location: ', 'Hands-on experience with SQL database design', 'Build algorithms and prototypes', 'Analyze Business KPI across different functions Engineering, ISC, Marketing, Finance, and more as needed', 'You Must Have ', '2 years of technical expertise with data models, data mining, and segmentation techniques ', 'Supporting initiatives for data integrity and normalization', 'Responsibilities Include', 'Prepare data for prescriptive and predictive modeling', 'Combine raw information from different sources', 'Financial modeling ability', 'Dive deep into data to uncover trends, data quality, and data completeness.', 'Ability to operate independently and proactively and drive the business forwards', 'JOB ID: ', 'Collaborate with data scientists and architects on several projects', 'Exempt', 'Utilize statistical modelling techniques where appropriate to drive business decisions', ' Hands-on experience with SQL database design Great numerical and analytical skills Previous experience to work as Data Engineer Ability to operate independently and proactively and drive the business forwards Intermediate Business modeling skills using Excel or BI Tool Excellent oral, written, and presentation skills with the ability to deal tactfully, confidently, and ethically with both internal and external customers Time management, organization, and prioritization skills  Financial modeling ability Basic familiarity with Machine Learning concepts Advanced statistical knowledge ', '5 years of experience with at least one statistical programming language: Python, R, SAS, Julia (Python Preferred)', 'JOB ID: req263748Category: Information TechnologyLocation: 715 Peachtree Street, N.E.,Atlanta,Georgia,30308,United StatesExempt', 'We Value', 'Location: 715 Peachtree Street, N.E.,Atlanta,Georgia,30308,United States', 'Time management, organization, and prioritization skills ', 'Conduct complex data analysis and Prepare trend analysis and report results to Team', '5 years of experience using SQL with any database', 'Data Engineer ', 'Intermediate Business modeling skills using Excel or BI Tool', 'Great numerical and analytical skills', 'Build data systems and pipelines and Evaluate business needs and objectives', 'Analyze Business KPI across different functions Engineering, ISC, Marketing, Finance, and more as neededBuild data systems and pipelines and Evaluate business needs and objectivesDive deep into data to uncover trends, data quality, and data completeness.Conduct complex data analysis and Prepare trend analysis and report results to TeamUtilize statistical modelling techniques where appropriate to drive business decisionsPrepare data for prescriptive and predictive modelingBuild algorithms and prototypesCombine raw information from different sourcesExplore ways to enhance data quality and reliabilityIdentify opportunities for data acquisitionCollaborate with data scientists and architects on several projectsGood Understanding of data structures, data modeling, data mining, and segmentation techniquesSupporting initiatives for data integrity and normalization', 'Good Understanding of data structures, data modeling, data mining, and segmentation techniques', 'Advanced statistical knowledge', 'JOB ID: req263748', 'Bachelor’s degree', 'Explore ways to enhance data quality and reliability', 'Category: Information Technology']",Entry level,Full-time,Information Technology,Electrical/Electronic Manufacturing,2021-03-18 14:34:51
Data Engineer,UP Professional Solutions®,"Houston, TX",1 week ago,101 applicants,"['', 'Architect end to end data solutions including data collection and storage, data modeling, and data consumption', 'Design and implement an Enterprise Data Warehouse', 'Experience:\xa0', 'EOE M/F/Disabled/Veteran\xa0', 'Duration:\xa0About 6 months', 'Develop data-intensive applications with API’s and streaming data pipelines', 'Estimated Start Date: February 2021', 'Implement data flows connecting operational systems, BI systems, and the big data platform', 'Technical Skills Required:\xa0Python, Spark, SQL', 'Duration:', 'Productionize mathematical models and machine learning models', 'Experience:\xa0Data modelling; ETL;\xa0Knowledgeable of Machine Learning; Knowledgeable of Statistics; Knowledgeable of Data Management such as MDM (Master Data Management), Data Catalog, and Data Governance', 'Automate manual data flows for repeated use and scalability', 'Contract Opportunity', 'Software:\xa0Software Development practices such as Testing, CI/CD, Version Control', 'Assists data analysts and data scientists with query optimization, performance tuning, and data processing', 'Houston, TX', 'Identify opportunities for data improvements and presents recommendations to management', 'Prepare and transform data into a usable state for analytics', 'SM24047', 'Technical Skills Required:\xa0', 'Benefits:', 'Software:', 'Benefits:\xa0Health, Vision, 401K, Paid Time Off', 'Data Engineer', 'Document and maintain source-to-target mappings and data lineage', 'Work independently on data projects for multiple business functions', '\xa0', 'We are currently seeking a\xa0Data Engineer\xa0with 5+ years’ experience to join the Big Data and Advanced Analytics department.\xa0As part of the Data Engineering team, the\xa0Data Engineer\xa0will work closely with the Data Science team and Business functions to solve real-world oil and gas midstream problems using machine learning, data science and artificial intelligence.', 'Benefits:\xa0Health, Vision, 401K, Paid Time OffEstimated Start Date: February 2021Duration:\xa0About 6 monthsTechnical Skills Required:\xa0Python, Spark, SQLSoftware:\xa0Software Development practices such as Testing, CI/CD, Version ControlExperience:\xa0Data modelling; ETL;\xa0Knowledgeable of Machine Learning; Knowledgeable of Statistics; Knowledgeable of Data Management such as MDM (Master Data Management), Data Catalog, and Data Governance', 'Estimated Start Date: ', 'Architect end to end data solutions including data collection and storage, data modeling, and data consumptionWork independently on data projects for multiple business functionsImplement data flows connecting operational systems, BI systems, and the big data platformDesign and implement an Enterprise Data WarehouseAutomate manual data flows for repeated use and scalabilityDevelop data-intensive applications with API’s and streaming data pipelinesPrepare and transform data into a usable state for analyticsDocument and maintain source-to-target mappings and data lineageProductionize mathematical models and machine learning modelsAssists data analysts and data scientists with query optimization, performance tuning, and data processingIdentify opportunities for data improvements and presents recommendations to management']",Mid-Senior level,Full-time,Information Technology,Information Services,2021-03-18 14:34:51
Data Engineer,Callisto Media,"San Jose, CA",20 hours ago,Be among the first 25 applicants,"['', 'Other Useful Skills/experience', '2+ years professional development experience', 'Experience in OOP (e.g. C++, Python, etc.)', 'Experienced in ETL developmentUnderstanding of dimensional and normalized database models and their applications', 'Knowledge of Business Intelligence (BI) / Data Warehousing (DW) principles and software (e.g. Tableau, Looker, etc.)', 'Good verbal and written communication skills', 'Responsibilities', 'Experience with clustered and/or distributed systems', 'Fluency in SQL, preferably MySQLFamiliar with data storage mechanisms, e.g. Hadoop, NoSQL, etc.', 'Experience using GitKnowledge of JavaScriptKnowledge of Business Intelligence (BI) / Data Warehousing (DW) principles and software (e.g. Tableau, Looker, etc.)Experience with cloud-based SaaS platformsExperience with clustered and/or distributed systemsExperience developing within large codebasesExperience with web development: CGI, HTML, Javascript, ApacheExperience with REST APIs', 'Knowledge of JavaScript', 'Qualifications', 'Experience with cloud-based SaaS platforms', 'Collaborate with architects to implement data solutions that solve business problemsDesign conceptual, logical and physical data modelsImplement effective and scalable end-to-end data pipeline solutionsImport data from a multitude of external systemsBuild metrics and reports ', 'Build metrics and reports ', 'Collaborate with architects to implement data solutions that solve business problems', 'Implement effective and scalable end-to-end data pipeline solutionsImport data from a multitude of external systems', ""Bachelor's degree in Computer Science/Engineering or equivalent industry experience"", 'Strong software engineering best practices (unit testing, code reviews, design documentation) ', 'Experience using Git', 'Good knowledge of the Linux operating system, networking, and toolset (bash, ssh, vim, etc.), especially text processing commands (sed, awk, etc.)', ""Bachelor's degree in Computer Science/Engineering or equivalent industry experience2+ years professional development experienceFluency in SQL, preferably MySQLFamiliar with data storage mechanisms, e.g. Hadoop, NoSQL, etc.Experienced in ETL developmentUnderstanding of dimensional and normalized database models and their applicationsGood knowledge of the Linux operating system, networking, and toolset (bash, ssh, vim, etc.), especially text processing commands (sed, awk, etc.)Experience in OOP (e.g. C++, Python, etc.)Strong software engineering best practices (unit testing, code reviews, design documentation) Good verbal and written communication skills"", 'Experience developing within large codebasesExperience with web development: CGI, HTML, Javascript, ApacheExperience with REST APIs', 'Design conceptual, logical and physical data models']",Entry level,Full-time,Information Technology,Media Production,2021-03-18 14:34:51
Data Engineer,Tradeswell,"Baltimore, MD",2 weeks ago,80 applicants,"['', 'An passion for data, data engineering, and data scienceStrong engineering background and experience with strong understanding of Python, Spark, and Airflow, Data Lakes & Warehousing, AWS Step FunctionsExperience managing managing infrastructure for data pipelines, ETL process, and data warehousesA desire to continually grow, learn, and iterate on the product and yourself', 'What you will do:', 'Tradeswell is the operating system for real-time commerce. We’re on a mission to democratize ecommerce intelligence and empower growth for brands by making ecommerce actions more informed, more coordinated, and more profitable. We use AI to create insights and make real-time optimization decisions across the entire ecommerce value chain. We put the power of data science at the fingertips of our customers, empowering their decision-making and giving them unprecedented control.', 'Strong engineering background and experience with strong understanding of Python, Spark, and Airflow, Data Lakes & Warehousing, AWS Step Functions', 'At the core of Tradeswell is a data platform optimized for the intricacies of real-time commerce. As a data engineer you will maintain and evolve data flows that enable critical aspects of the application and data science stack. You will work closely with other engineers and product management as part of a cross-functional team solving problems for internal and external customers.', 'Experience managing managing infrastructure for data pipelines, ETL process, and data warehouses', 'About you:', '\xa0', 'Integrate with 3rd party systems to ingest and normalize data for customers', 'Integrate with 3rd party systems to ingest and normalize data for customersDesign, build, and maintain business critical data infrastructureCollaborate with other engineers, product managers, and data scientists to solve problems for internal and external customers', 'Any unsolicited resumes/candidate profiles submitted through our website or to personal email accounts of employees of Tradeswell are considered property of Tradeswell and are not subject to payment of agency fees.', 'A desire to continually grow, learn, and iterate on the product and yourself', 'Design, build, and maintain business critical data infrastructure', 'Collaborate with other engineers, product managers, and data scientists to solve problems for internal and external customers', 'About Tradeswell:', 'An passion for data, data engineering, and data science', 'The opportunity:']",Mid-Senior level,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Scientist/Engineer,Oracle,"Austin, Texas Metropolitan Area",2 weeks ago,Over 200 applicants,"['', 'Job Type', ' Identify recurring problems and bottlenecks that might be improved through upgrades to our software product, new technologies in the analytics team infrastructure, or further research into statistical methodology.', 'Organization', ' Work with application management team, and where necessary, other members of the analytics team to efficiently execute larger-scale analytic deliverables and operationalize the results;', ' Ability to write SQL queries and capability to do exploratory analysis on structured data. ', ' MS in Computer Science, Mathematics, Statistics, Physics, Economics or other STEM field;', ' 1-3 years of professional work experience in a data analytics role involving customer-facing deliverables and internal team coordination;', ' Be a team player and be willing to delve into the details of the product to make sure clients are seeing value. ', ' Efficiently execute well-defined, discrete analytic tasks based on the needs of client, as communicated by the client management team; Work with client management team and our customers to define and take ownership of scope, intermediate deliverables, and timelines around larger-scale analytic deliverables; Work with application management team, and where necessary, other members of the analytics team to efficiently execute larger-scale analytic deliverables and operationalize the results; Identify recurring problems and bottlenecks that might be improved through upgrades to our software product, new technologies in the analytics team infrastructure, or further research into statistical methodology.', ' Model evaluation and validation expertise, should be demonstrated as a consequence of having to do this in current work developing ML models. ', 'Responsibilities', ' Ability to work closely with other stakeholders in the client delivery organization for OUAI (Customer success and Implementation Engineering) ', 'Oracle is an Affirmative Action-Equal Employment Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability, protected veterans status, age, or any other characteristic protected by law.', ' MS in Computer Science, Mathematics, Statistics, Physics, Economics or other STEM field; 1-3 years of professional work experience in a data analytics role involving customer-facing deliverables and internal team coordination;', ' Prior experience with programming languages (e.g. Python) and statistical programming languages (e.g. R, SAS, SPS, etc.) strongly preferred;', ' Strong written & oral communication skills;', ' Ability to write well structured, testable Python or PySpark code to implement use cases using Machine Learning techniques in a production grade environment. ', ' Ability and willingness to be able to use OUAI’s proprietary tool to recreate or build rule-based solutions to analytics problems, mostly classification ', 'Location', 'Other Locations', ' Work with client management team and our customers to define and take ownership of scope, intermediate deliverables, and timelines around larger-scale analytic deliverables;', ' Efficiently execute well-defined, discrete analytic tasks based on the needs of client, as communicated by the client management team;', ' Prior experience working as a data scientist in role that at a minimum requires creation of ""proof of concept"" data science models. ', 'Preferred Qualifications', ' Prior experience framing and conducting analyses in a relational database environment (e.g. Oracle DB, MySQL, PostgreSQL, MSSQL) and through spreadsheet-based tools; Prior experience with programming languages (e.g. Python) and statistical programming languages (e.g. R, SAS, SPS, etc.) strongly preferred; A passion for and curiosity about new big data analytics technologies and methods; Strong written & oral communication skills; Prior experience working in complex data environments within the energy and utility industry preferred. A successful candidate will demonstrate the following qualities: Ability to write SQL queries and capability to do exploratory analysis on structured data.   Ability to write well structured, testable Python or PySpark code to implement use cases using Machine Learning techniques in a production grade environment.   Prior experience working as a data scientist in role that at a minimum requires creation of ""proof of concept"" data science models.   Ability and willingness to be able to use OUAI’s proprietary tool to recreate or build rule-based solutions to analytics problems, mostly classification   Breadth and depth across machine learning techniques: specifically Deep Learning (using TF/Keras or other frameworks), as well as other ML techniques to re-create ML equivalents of legacy rule-based classification algorithms.   Model evaluation and validation expertise, should be demonstrated as a consequence of having to do this in current work developing ML models.   Ability to work closely with other stakeholders in the client delivery organization for OUAI (Customer success and Implementation Engineering)   Be a team player and be willing to delve into the details of the product to make sure clients are seeing value. ', 'This is a remote/office based position which may be performed anywhere in the United States except for within the state of Colorado.', ' A passion for and curiosity about new big data analytics technologies and methods;', ' Breadth and depth across machine learning techniques: specifically Deep Learning (using TF/Keras or other frameworks), as well as other ML techniques to re-create ML equivalents of legacy rule-based classification algorithms. ', ' Prior experience framing and conducting analyses in a relational database environment (e.g. Oracle DB, MySQL, PostgreSQL, MSSQL) and through spreadsheet-based tools;', ' Prior experience working in complex data environments within the energy and utility industry preferred. A successful candidate will demonstrate the following qualities:', ' Oracle Utilities Analytics Insights (OUAI) Data Science & Analytics | Data Analyst ', 'Job']",Entry level,Full-time,Engineering,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,TrueData,"Los Angeles, CA",2 weeks ago,140 applicants,"['', 'Empower the business - Support the business to unearth data, transform data, access data, understand data and exploit data', '<3 data - Hypothesize, discover, collect, iterate and prove .', 'Paid Time Off (Vacation, Skick & Public Holidays)', 'Achievement Unlocked Inquisitive, Determined, Scholar - Be up for the challenge.', 'Speak the language - Be willing to create prose with at least three of Java, Kotlin, Scala, SQL, Python. Know which to use for the time & purpose.', 'Build > buy - Know the power of seeking out and putting together the right open source into a highly flexible and cohesive system instead of purchasing a solution that delivers a portion of the need, stops you from delivering the rest and ties you to your pricey investment.', 'Retirement Plan (401k, IRA)', 'Create awesome software - Use agile techniques to build software and platforms like recommendation engines, analytics engines, advertising engines.', 'Benefits', ' XP - 2+ years of programming experience and 3+ years of data pipeline experience. Academic and hobby experience counts. Speak the language - Be willing to create prose with at least three of Java, Kotlin, Scala, SQL, Python. Know which to use for the time & purpose. <3 data - Hypothesize, discover, collect, iterate and prove . Achievement Unlocked Inquisitive, Determined, Scholar - Be up for the challenge. #algorithm and #datastructures fanatic - Have gorged on distributed, parallel and probabilistic algorithms and data structures. Build > buy - Know the power of seeking out and putting together the right open source into a highly flexible and cohesive system instead of purchasing a solution that delivers a portion of the need, stops you from delivering the rest and ties you to your pricey investment. ', '#algorithm and #datastructures fanatic - Have gorged on distributed, parallel and probabilistic algorithms and data structures.', 'Requirements', 'XP - 2+ years of programming experience and 3+ years of data pipeline experience. Academic and hobby experience counts.', ' Health Care Plan (Medical, Dental & Vision) Retirement Plan (401k, IRA) Paid Time Off (Vacation, Skick & Public Holidays) Family Leave (Maternity, Paternity, etc...) Work From Home', 'Family Leave (Maternity, Paternity, etc...)', 'Prosper with us - Work with devops, analysts, data scientists, programmers, marketers, filmmakers, finance, project management, product management and leaders to amaze, surprise and delight.', ' Empower the business - Support the business to unearth data, transform data, access data, understand data and exploit data Create awesome software - Use agile techniques to build software and platforms like recommendation engines, analytics engines, advertising engines. Command data - Leverage petabytes of data, data mining, statistics, machine learning, deep learning, open source, orchestration tools, NoSQL, Apache Spark, and Apache Kafka Prosper with us - Work with devops, analysts, data scientists, programmers, marketers, filmmakers, finance, project management, product management and leaders to amaze, surprise and delight. ', 'Substantial equivalent research oriented analytical work experience may be acceptable. B.S. or M.S. in Computer Science, Mathematics, Engineering or a Physical Science from an accredited or notable institution preferred. ', 'Health Care Plan (Medical, Dental & Vision)', 'Work From Home', 'Command data - Leverage petabytes of data, data mining, statistics, machine learning, deep learning, open source, orchestration tools, NoSQL, Apache Spark, and Apache Kafka']",Mid-Senior level,Full-time,Engineering,Marketing and Advertising,2021-03-18 14:34:51
Data Engineer (Data Governance),FanDuel,Los Angeles Metropolitan Area,3 weeks ago,Be among the first 25 applicants,"['', 'THE POSITION', 'TVG', 'Define performance and quality metrics and ensure compliance with data related policies, standards, roles and responsibilities, and adoption requirements.', 'PokerStars', 'FanDuel\xa0—\xa0A game-changing real-money fantasy sports app\xa0', 'Facilitate the development and implementation of data quality standards, data protection standards and adoption requirements using software SDLC.', 'Experience with systems for data processing (Spark, Flink, Hadoop, Airflow) and storage (S3, Kafka, ElasticSearch, Dynamo, MySQL, or Postgres)', '●\xa0\xa0\xa0\xa0\xa0Mentorship and professional development resources to help you refine your game', 'THE CONTRACT', 'Design and promote standards for security operations, data telemetry and processing.', '●\xa0\xa0\xa0\xa0\xa0Opportunities to build really cool products that fans love', '●\xa0\xa0\xa0\xa0\xa0Hall of Fame benefit programs and platforms', 'Minimum of 3+ years of operational or technical experience in data stewardship and data governance implementations preferredExperience with one or more of the following: data scanners, data dictionaries, data warehouses, data pipeline builds, reporting or system extracts, metadata, business semantics, Big Data, data access using API.Experience with systems for data processing (Spark, Flink, Hadoop, Airflow) and storage (S3, Kafka, ElasticSearch, Dynamo, MySQL, or Postgres)Experience with data governance practices, business and technology issues related to management of enterprise information assets and approaches related to data protection.Solid understanding of current data governance topics.Knowledge of risk data architecture and technology solutions.', 'FanDuel Sportsbook\xa0—\xa0America’s #1 sports betting app\xa0', 'Establish an enterprise data governance framework for data policies, standards and practices both at Data Engineering and data users.Facilitate the development and implementation of data quality standards, data protection standards and adoption requirements using software SDLC.Define performance and quality metrics and ensure compliance with data related policies, standards, roles and responsibilities, and adoption requirements.Work closely with compliance and security team to be better aligned with group-wide compliance requirements.Design and promote standards for security operations, data telemetry and processing.', 'Data Engineer (Governance) will have responsibility for improving the data quality and managing the protection of sensitive data and information assets that Data Engineering team directly manages on the FanDuel Group Data Platform.', 'Establish an enterprise data governance framework for data policies, standards and practices both at Data Engineering and data users.', 'Our roster has an opening with your name on it', 'Experience with one or more of the following: data scanners, data dictionaries, data warehouses, data pipeline builds, reporting or system extracts, metadata, business semantics, Big Data, data access using API.', 'FOXBet\xa0— A world-class betting platform and affiliate of FanDuel Group\xa0', 'FOXBet\xa0', 'FanDuel Racing', 'Minimum of 3+ years of operational or technical experience in data stewardship and data governance implementations preferred', 'Knowledge of risk data architecture and technology solutions.', 'Solid understanding of current data governance topics.', 'FanDuel Racing\xa0—\xa0A horse racing app built for the average sports fan\xa0', 'Everyone on our team has a part to play', 'THE GAME PLAN', 'FanDuel Group is based in New York, with offices in California, New Jersey, Florida, Oregon and Scotland. Our brands include:', 'FanDuel Group is a world-class team of brands and products all built with one goal in mind —\xa0to give fans new and innovative ways to interact with their favorite games, sports, teams, and leagues. That’s no easy task, which is why we’re so dedicated to building a winning team. And make no mistake, we are here to win, but we believe in winning right. That means we’ll never compromise when it comes to looking out for our teammates. From our many opportunities for professional development to our generous insurance and paid leave policies, we’re committed to making sure our employees get as much out of FanDuel as we ask them to give.', 'Competitive compensation is just the beginning. As part of our team, you can expect:', 'THE STATS', 'ABOUT FANDUEL GROUP', 'Work closely with compliance and security team to be better aligned with group-wide compliance requirements.', 'TVG\xa0—\xa0The best-in-class horse racing TV/media network and betting platform\xa0', 'FanDuel Sportsbook', 'PokerStars\xa0US —\xa0The premier online poker product\xa0and affiliate of FanDuel Group', '●\xa0\xa0\xa0\xa0\xa0An exciting and fun environment committed to driving real growth', ""FanDuel Group is an equal opportunities employer. Diversity and inclusion in FanDuel means that we respect and value everyone as individuals. We don't tolerate bias, judgement or harassment.\xa0Our focus is on developing employees so that they reach their full potential."", 'FanDuel Casino & Betfair Casino\xa0—\xa0Fan-favorite online casino apps\xa0', '●\xa0\xa0\xa0\xa0\xa0Flexible vacation allowance to let you refuel', 'Experience with data governance practices, business and technology issues related to management of enterprise information assets and approaches related to data protection.', 'We treat our team right', 'FanDuel Casino & Betfair Casino', 'FanDuel', 'What we’re looking for in our next teammate']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Regions Bank,"Birmingham, AL",1 week ago,Be among the first 25 applicants,"['', 'Prior banking or financial Services experienceExperience developing solutions for the financial services industryBackground in Big Data Engineering and Advanced Data Analytics ', 'Experience with DevOps principals and CI/CD.', 'Strong ability to transform and integrate complex data from multiple sources into accessible, understandable, and usable data assets and frameworks', 'Location Details', 'Supports any team members in the development of such information delivery and aid in the automation of data products', 'Partners with Regions Technology partners to Design, Build, and Maintain the data-based structures and systems in support of Data and Analytics and Data Product use casesBuilds data pipelines to collect and arrange data and manage data storage in Regions’ big data environmentBuilds robust, testable programs for moving, transforming, and loading data using big data tools such as Spark.Coordinates design and development with Data Products Partners, Data Scientists, Data Management, Data Modelers, and other Technical partners to construct strategic and tactical data storesEnsures data is prepared, arranged and ready for each defined business use caseDesigns and deploys frameworks and micro services to serve data assets to data consumersCollaborates and aligns with technical and non-technical stakeholders to translate customer needs into Data Design requirements, and work to deliver world-class visualizations, data stories while ensuring data quality and integrityProvides consultation to all areas of the organization that plan to use data to make decisionsSupports any team members in the development of such information delivery and aid in the automation of data productsActs as trusted adviser and partner to business leads- assisting in the identification of business needs & data opportunities, understanding key drivers of performance, interpreting business case data drivers, turning data into business value, and participating in the guidance of the overall data and analytics strategy', 'Eight (8) years of data-oriented experience including experience managing data and analytics resources', 'Experience working with Hadoop ecosystem building Data Assets at an enterprise scale', 'Primary Responsibilities', 'Experience building data solutions at scaleExperience designing and building relational data structures in multiple environmentsExperience with Airflow, Argo, Luigi, or similar orchestration toolExperience with DevOps principals and CI/CD.Experience with Docker and KubernetesExperience with No-SQL databases such as HBase, Cassandra, or MongoDBExperience with streaming technologies such as Kafka, Flink, or Spark StreamingExperience working with Hadoop ecosystem building Data Assets at an enterprise scaleProven record of accomplishment of delivering operational Data solutions including Report and Model Ready Data AssetsSignificant experience working with senior executives in the use of data, reporting and visualizations to support strategic and operational decision makingStrong ability to transform and integrate complex data from multiple sources into accessible, understandable, and usable data assets and frameworksStrong technical background including database and business intelligence skills', 'Prior banking or financial Services experience', 'Partners with Regions Technology partners to Design, Build, and Maintain the data-based structures and systems in support of Data and Analytics and Data Product use cases', 'Strong technical background including database and business intelligence skills', 'Experience designing and building relational data structures in multiple environments', 'Experience developing solutions for the financial services industry', 'Provides consultation to all areas of the organization that plan to use data to make decisions', 'Acts as trusted adviser and partner to business leads- assisting in the identification of business needs & data opportunities, understanding key drivers of performance, interpreting business case data drivers, turning data into business value, and participating in the guidance of the overall data and analytics strategy', 'Target Range To:', 'Position Type', 'Background in Big Data Engineering and Advanced Data Analytics ', 'Builds data pipelines to collect and arrange data and manage data storage in Regions’ big data environment', 'Requirements', 'Collaborates and aligns with technical and non-technical stakeholders to translate customer needs into Data Design requirements, and work to deliver world-class visualizations, data stories while ensuring data quality and integrity', 'Experience with No-SQL databases such as HBase, Cassandra, or MongoDB', 'Location:', 'Target Range From:', 'Experience with streaming technologies such as Kafka, Flink, or Spark Streaming', 'Ensures data is prepared, arranged and ready for each defined business use case', 'Experience with Docker and Kubernetes', 'Skills And Competencies', 'Experience building data solutions at scale', 'Designs and deploys frameworks and micro services to serve data assets to data consumers', 'Builds robust, testable programs for moving, transforming, and loading data using big data tools such as Spark.', ""Bachelor's degree in Business or a technical related field"", 'Job Description', ""Bachelor's degree in Business or a technical related fieldEight (8) years of data-oriented experience including experience managing data and analytics resources"", 'Preferences', 'The target information listed below is the 10th and 50th percent of the market range, based on the Birmingham, AL market and level of the position', 'Coordinates design and development with Data Products Partners, Data Scientists, Data Management, Data Modelers, and other Technical partners to construct strategic and tactical data stores', 'Experience with Airflow, Argo, Luigi, or similar orchestration tool', 'Significant experience working with senior executives in the use of data, reporting and visualizations to support strategic and operational decision making', 'Proven record of accomplishment of delivering operational Data solutions including Report and Model Ready Data Assets']",Not Applicable,Full-time,Information Technology,Banking,2021-03-18 14:34:51
Data Engineer,CAP Global Consulting,"Denver, CO",1 week ago,52 applicants,"['This engineer will be assisting the lead and will be helping design the AI/ML solutions to identify insights, predictions and recommendations using company Wireless Commercial data providing value to the company business team and build analytics product offerings in the marketplace for Commercial customers.', '', 'What part or role with the candidate play in project or team?', '\xa0internal business partners to understand requirementsArchitects to put together a solutionDev teams to deliver the design', 'Dev teams to deliver the design', 'This person will be working cross functionally with teams in various geographic regions such as', '\xa0internal business partners to understand requirements', 'Architects to put together a solution']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
"Data Engineer - Remote, SF, or DEN",Quizlet,"San Francisco, CA",3 weeks ago,198 applicants,"['', 'Horizontal sharding solutions for relational data stores', 'Experience with any / all of:', 'Ensure reliability of the systems that move and transform Quizlet’s data for use in analytics and the product', 'Next Steps', 'Quizlet will not pay fees to any third-party agency or firm nor will it be responsible for any agency fees associated with unsolicited resumes.', ""Help scale our application data layer so it can handle > 100k transactions per secondBuild new streaming data infrastructure to feed machine learning and analyticsEnsure reliability of the systems that move and transform Quizlet’s data for use in analytics and the productCreate and maintain libraries, APIs, and other shared abstractions used by Quizlet's Data Science and Analytics teams or delivered as microservices.Do all of this in a way that internal customers find approachable and easy to use"", 'Systematic problem-solving approach, coupled with good communication skills and a strong sense of ownership.Empathy for the wide array of people who rely on Quizlet data platforms (analytics, product, data science… everybody, really)A reliable teammate with a strong sense of ownership.', 'Systematic problem-solving approach, coupled with good communication skills and a strong sense of ownership.', 'Experience with any / all of:Horizontal sharding solutions for relational data storesMySQL (Cloud SQL), Cloud Spanner, Memcache, RedisContainers, Kubernetes, Terraform, Helm, and supporting technologies running on Google Cloud PlatformVisualization tools like DataDog that connect metrics and eventsPython, TypeScript, GoLang, Kotlin/JavaAVRO, DBTApache Airflow, RabbitMQ, Apache Kafka', 'Meet with Hiring Manager', 'Help scale our application data layer so it can handle > 100k transactions per second', ""Create and maintain libraries, APIs, and other shared abstractions used by Quizlet's Data Science and Analytics teams or delivered as microservices."", 'Do all of this in a way that internal customers find approachable and easy to use', 'Visualization tools like DataDog that connect metrics and events', 'About Quizlet', 'Benefits And Perks', 'In Closing', 'AVRO, DBT', 'The Role', 'To All Recruiters And Placement Agencies', 'Build new streaming data infrastructure to feed machine learning and analytics', 'Meet with Potential Colleagues and Leadership', 'MySQL (Cloud SQL), Cloud Spanner, Memcache, Redis', 'Empathy for the wide array of people who rely on Quizlet data platforms (analytics, product, data science… everybody, really)', 'Required Qualifications', 'Containers, Kubernetes, Terraform, Helm, and supporting technologies running on Google Cloud Platform', 'We’ll provide you with a laptop, top-notch benefits available to all fulltime Quizleters', 'A reliable teammate with a strong sense of ownership.', 'Python, TypeScript, GoLang, Kotlin/Java', 'Apache Airflow, RabbitMQ, Apache Kafka', 'We offer 20 days of paid vacation (and we expect you to take them)', 'Collaborate with your manager and team to create a schedule that ensures a high level of productivity (creating that ideal work/life balance)', 'Meet with Recruiter', 'We strive to make everyone feel comfortable and welcome! ', 'We provide a monthly in home office stipend to employees while our teams are working remote for the COVID pandemic', 'Preferred Qualifications']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Advanced Data Engineer,Kroger Technology,Cincinnati Metropolitan Area,4 weeks ago,71 applicants,"['', 'Essential Job Functions', 'Lead the analysis of the technology environment to detect critical deficiencies and recommend solutions for improvement', 'Present opportunities with cost/benefit analysis to leadership to shape sound architectural decisions', 'Define high-level migration plans to address the gaps between the current and future state', 'Mentor team members in data principles, patterns, processes and practices', 'Basic understanding of network and data security architecture', 'Knowledge in a minimum of two of the following technical disciplines: data warehousing, data management, analytics development, data science, application programming interfaces (APIs), data integration, cloud, servers and storage, and database management', 'Demonstrated written and oral communication skills', 'Draft and review architectural diagrams, interface specifications and other design documents', 'Must be able to perform the essential job functions of this position with or without reasonable accommodation', 'The Kroger team is looking for a Data Engineer experienced in implementing data solutions in Azure. The Data Engineer will analyze, design and develop enterprise data and information architecture deliverables, focusing on data as an asset for the enterprise. The Data Engineer will also support the implementation of Infrastructure as Code (IaC) by working with teams to help engineer scalable, reliable, and resilient software running in the cloud.', 'Promote the reuse of data assets, including the management of the data catalog for reference', '7+ years successful and applicable hands on experience in the data development and principles including end-to-end design patterns', '7+ years successful and applicable experience taking a lead role in building complex data solutions that have been successfully delivered to customers', 'Strong knowledge of industry trends and industry competition', '7+ years proven track record of designing and delivering large scale, high quality operational or analytical data systems', 'Any experience defining evolutionary data solutions and underlying technologies', 'Participate in, and drive, the development and communication of enterprise standards for data domains and data solutions, focusing on simplified integration and streamlined operational and analytical uses', 'Drive digital innovation by leveraging innovative new technologies and approaches to renovate, extend, and transform the existing core data assets, including SQL-based, NoSQL-based, and Cloud-based data platforms', ""Bachelor's Degree in computer science, or software engineering, or related field"", 'Participate in the development and communication of data strategy and roadmaps across the technology organization to support project portfolio and business strategy', 'Minimum Position Qualifications']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Benchmark Analytics,"Chicago, IL",1 week ago,184 applicants,"['', 'Build a high speed, high capacity data system (multi-tenant data lake and databases)', 'Has a strong sense of process (ability to understand how steps relate to each other to achieve end results)', 'Experience working with and without AWS infrastructure/software', 'Educate and learn from internal teams on ETL data engineering best practices, relevant knowledge and specific skills', 'Design, enhance and automate end to end data platform processes;', 'What We Offer:', 'Build a high speed, high capacity data system (multi-tenant data lake and databases)Build automated tools for importing data from our transactional platformBuild dynamic, high performance system for processing millions of records dailyManage an existing python-based toolset for ETL (django+airflow) and modeling (Scikit learn and licensed software)Build tools to intergrade, deploy, monitor, and archive machine learning models developed by our data scientistsExtract and transform data from large file-based and server-based datasetsWrite, review and maintain code and configurations for performing data aggregations and manipulations.Work with leadership and peers to drive decisions and measure progress through agile and scrum processes.Analyze entire ETL processes, identify inefficiencies, suggest improvement.Support process documentation as necessary to prepare the team and company for growth/scalePrepare presentations and reports based on the results of work/analysisEducate and learn from internal teams on ETL data engineering best practices, relevant knowledge and specific skillsWork with the research, product, and engineering teams to critique and iteratively improve results', 'Collaborate with the Benchmark Research Team members across the organization that focus on improving product data quality, data pipeline efficiency and data platform performance.', 'Build and implement ETL data processes touching millions of records and spanning multiple customer systems;', 'Experience troubleshooting problems throughout the stack', 'Excellent understanding of ETL processes and relational databases', '\ufeff', 'We cover 75% of Medical (BCBS) Premiums, Dental & Vision Premiums, and offer 100% company sponsored Life Insurance as well as a 401k plan', 'A truly unique experience to work for a fast-growing, young companyThe satisfaction that comes with being part of a solution that makes a big difference in the worldThe excitement that comes with the need to endlessly innovate and transform our marketA competitive base salary and flexible PTO policyWe cover 75% of Medical (BCBS) Premiums, Dental & Vision Premiums, and offer 100% company sponsored Life Insurance as well as a 401k plan', 'Responsibilities', 'Works and communicates well with others:', 'Has at least 3 to 5 years of experience in engineering data pipeline, analytics infrastructure and integrating platforms:', 'Able to work independently with reasonable guidance from management (except in complex and non-routine situations)', 'Work with leadership and peers to drive decisions and measure progress through agile and scrum processes.', 'Thank you for your interest in our company and what we are building.', 'Prepare presentations and reports based on the results of work/analysis', 'The Candidate', 'Experience building applications for end-users and in-house analysts', 'Has the ability to successfully mange multiple concurrent tasks/projects and meet deadlines', 'The Fine Print and How to Apply:', 'Build and implement ETL data processes touching millions of records and spanning multiple customer systems;Design, enhance and automate end to end data platform processes;Collaborate with the Benchmark Research Team members across the organization that focus on improving product data quality, data pipeline efficiency and data platform performance.', 'Enjoys writing queries, looking at data, and testing assumptions:', 'The ideal candidate will be someone who:', 'Unfortunately, we are not able to sponsor employment visas at this time, so we can only accept applications from candidates who are authorized to work in the US.', 'Able to effectively communicate across multiple levels (team members, managers) in a fast-paced environment', 'A competitive base salary and flexible PTO policy', 'Has at least 3 to 5 years of experience in engineering data pipeline, analytics infrastructure and integrating platforms:Experience with building python-based data pipeline and ETL systemsExperience building applications for end-users and in-house analystsExperience with Python 3Experience working with and without AWS infrastructure/softwareExperience troubleshooting problems throughout the stackEnjoys writing queries, looking at data, and testing assumptions:Excellent understanding of ETL processes and relational databasesStrong in data engineering development technologies (e.g., SQL, Python, Django) and data senseStrong technical aptitude and technical acquisition skills (ability to work with and develop skills in technical products)Has a strong sense of process (ability to understand how steps relate to each other to achieve end results)Has the ability to successfully mange multiple concurrent tasks/projects and meet deadlinesWorks and communicates well with others:Has empathy for colleagues and customersAble to receive and respond positively to feedbackAble to work independently with reasonable guidance from management (except in complex and non-routine situations)Able to effectively communicate across multiple levels (team members, managers) in a fast-paced environment', 'Experience with Python 3', 'Extract and transform data from large file-based and server-based datasets', 'Has empathy for colleagues and customers', 'A truly unique experience to work for a fast-growing, young company', 'Thank you for your interest in our company and what we are building.Benchmark Analytics is an Equal Opportunity Employer. We value diversity of all kinds in our effort to create a stellar workforce of committed and passionate team members.Unfortunately, we are not able to sponsor employment visas at this time, so we can only accept applications from candidates who are authorized to work in the US.If you’d like to apply, please email your resume to\xa0courtney.jian@benchmarkanalytics.com.', 'Write, review and maintain code and configurations for performing data aggregations and manipulations.', 'Strong technical aptitude and technical acquisition skills (ability to work with and develop skills in technical products)', 'If you’d like to apply, please email your resume to\xa0courtney.jian@benchmarkanalytics.com.', 'If you are interested in the intersection of public sector operations, research and data analytics, we believe this is an unprecedented opportunity to experience it first-hand.', 'Manage an existing python-based toolset for ETL (django+airflow) and modeling (Scikit learn and licensed software)', 'Experience with building python-based data pipeline and ETL systems', 'The Role', 'Able to receive and respond positively to feedback', 'The satisfaction that comes with being part of a solution that makes a big difference in the world', 'We are currently seeking an experienced\xa0Data Engineer\xa0to join us to build a robust and scalable data platform as we continue to grow our predictive analytics business. ', 'Support process documentation as necessary to prepare the team and company for growth/scale', 'Work with the research, product, and engineering teams to critique and iteratively improve results', 'Build dynamic, high performance system for processing millions of records daily', 'As a Data Engineer,\xa0you will get to:', 'Strong in data engineering development technologies (e.g., SQL, Python, Django) and data sense', 'The excitement that comes with the need to endlessly innovate and transform our market', 'Benchmark Analytics is an Equal Opportunity Employer. We value diversity of all kinds in our effort to create a stellar workforce of committed and passionate team members.', 'Build tools to intergrade, deploy, monitor, and archive machine learning models developed by our data scientists', 'Build automated tools for importing data from our transactional platform', 'Analyze entire ETL processes, identify inefficiencies, suggest improvement.']",Entry level,Full-time,Information Technology,Law Enforcement,2021-03-18 14:34:51
Data Engineer,Coppei,Greater Seattle Area,1 week ago,79 applicants,"['', 'Develop and apply exceptional knowledge of common and emerging data platforms (ex: MuleSoft, SQL Server Integration Services SSIS, Talend, Alteryx, AWS Glue, Matillion, Fivetran or Azure Data Factory)', 'Degree\xa0', 'Coppei\xa0Partners is an Equal Opportunity Employer – Minority / Women / Disability / Veteran / Gender Identity / Sexual Orientation / Age\xa0Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.\xa0Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities and activities may change at any time with or without notice.\xa0', 'Bachelor’s Degree\xa0or equivalent work experience', 'Participate in solution design discussions with a Coppei project team', 'Combine expertise in data pipelines and data engineering with innovative thinking to deliver excellent client work', 'Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.\xa0', 'Utilize best practices and methodologies and apply prior experiences to deliver high-quality client workOperate across business and technical organizations\xa0\xa0Collaborate with your Coppei team, while personally owning and driving key deliverablesStay on top of emerging technologies, and partner with market-leading technology organizations to understand and leverage their products', 'Job Responsibilities', 'Design and lead implementation of data and infrastructure solutions for client teams', 'Consulting experience or having operated within a departmental or enterprise analytics shared service', 'Coppei\xa0Partners is an Equal Opportunity Employer – Minority / Women / Disability / Veteran / Gender Identity / Sexual Orientation / Age\xa0', '7+ years of experience as a data engineer, delivering end-to-end data technologies and solutions using scripting or coding', '4+ years of experience in data engineering', 'Consulting experience or having operated within a departmental or enterprise analytics shared service7+ years of experience as a data engineer, delivering end-to-end data technologies and solutions using scripting or coding4+ years working on cloud-based data solutions (ex: Azure, Google Cloud Platform GCP, or Amazon Web Services AWS)2+ years working with a cloud data warehouse or enterprise data warehouse EDW (ex: Redshift, Snowflake, Oracle, Teradata, Netezza, Vertica, Greenplum, SAP hana or Azure DW)Experience with relational database management systems RDBMS, Massively Parallel Processing MPP, or distributed data platformsExperience with solution delivery using agile or pod-based delivery methodologiesExperience with business Intelligence and visualization tools (ex: Looker, Tableau, Domo or Power\xa0BI)', '4+ years working on cloud-based data solutions (ex: Azure, Google Cloud Platform GCP, or Amazon Web Services AWS)', 'Operate across business and technical organizations\xa0\xa0', 'Experience with business Intelligence and visualization tools (ex: Looker, Tableau, Domo or Power\xa0BI)', 'Legal Disclaimers\xa0', 'Conduct assessments of a company technology stack, identifying challenges and gaps, business and technical pain points, current state and future state scenarios', 'Legal Disclaimers', 'Experience with relational database management systems RDBMS, Massively Parallel Processing MPP, or distributed data platforms', 'Stay on top of emerging technologies, and partner with market-leading technology organizations to understand and leverage their products', 'Plan and manage delivery of complex solutions', 'Data Engineering', 'Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities and activities may change at any time with or without notice.\xa0', 'Collaborate with and guide client partners on their technology stack of services', '4+ years of experience with scripting or coding (ex: SQL, Python, JavaScript, PHP, VBScript, Ruby or Java)', '2+ years working on cloud-based data solutions (ex: Azure, AWS or Google Cloud) or cloud data warehouses (ex: Snowflake, Azure DW or Redshift)', '2+ years working with a cloud data warehouse or enterprise data warehouse EDW (ex: Redshift, Snowflake, Oracle, Teradata, Netezza, Vertica, Greenplum, SAP hana or Azure DW)', 'Basic Qualifications', 'Experience with solution delivery using agile or pod-based delivery methodologies', 'The name\u202fCoppei\u202fis pronounced “copy-eye”. Our people are what make us exceptional. We are a career destination for those who love the consulting world but were ready to join a more personal firm where they would have a bigger impact and belong to a team that holds transparency, ethics, and consultant care as its highest priorities. We are seeking experienced, motivated individuals excited to join a fast-growing practice.\xa0', 'Collaborate with your Coppei team, while personally owning and driving key deliverables', 'As a Data Engineering Senior Consultant at Coppei you will partner with clients to implement industry-leading technical solutions necessary to drive vital business decisions. You will be nimble yet methodical in the face of changing business priorities to deliver high-quality, scalable, and secure data solutions. Your expertise in developing data pipelines and cloud solutions will enable transformational outcomes for our clients. No project is the same – you will have the opportunity to work across technologies and learn while developing unique solutions to a breadth of challenges. If you are a data engineer with a consulting mindset, who thrives on shaping data solutions from vision through deployment, read on!', '\xa0', '4+ years of experience in data engineering4+ years of experience with scripting or coding (ex: SQL, Python, JavaScript, PHP, VBScript, Ruby or Java)2+ years working on cloud-based data solutions (ex: Azure, AWS or Google Cloud) or cloud data warehouses (ex: Snowflake, Azure DW or Redshift)Bachelor’s Degree\xa0or equivalent work experience', 'Combine expertise in data pipelines and data engineering with innovative thinking to deliver excellent client workDesign and lead implementation of data and infrastructure solutions for client teamsCollaborate with and guide client partners on their technology stack of servicesParticipate in solution design discussions with a Coppei project teamConduct assessments of a company technology stack, identifying challenges and gaps, business and technical pain points, current state and future state scenariosPlan and manage delivery of complex solutionsDevelop and apply exceptional knowledge of common and emerging data platforms (ex: MuleSoft, SQL Server Integration Services SSIS, Talend, Alteryx, AWS Glue, Matillion, Fivetran or Azure Data Factory)', 'Consulting', 'Preferred Qualifications', 'Utilize best practices and methodologies and apply prior experiences to deliver high-quality client work']",Mid-Senior level,Full-time,Information Technology,Management Consulting,2021-03-18 14:34:51
Sr Data Engineer - Core Data team,Disney Streaming Services,"New York, NY",3 days ago,Be among the first 25 applicants,"['', 'New York City preferred', 'Additional Information', 'Proficiency with Scala or Python (preferably both)Traditional relational databases and/or distributed systems such as Hadoop/Hive, BigQuery, Redshift, SnowflakeStreaming platforms such as Flink, SparkWorkflow management tools such as AirflowHadoop (or similar) Ecosystem (MapReduce, Yarn, HDFS, Hive, Spark, Presto, Pig, HBase)Data exploration and data visualization tools such as Looker, Tableau, Chartio, Apache Superset, Plotly / Dash ', 'Preferred Education', 'Job Summary', '5+ years of relevant professional experienceExperience with terabyte or petabyte scale data systemsProficiency in Scala (or willingness to learn)', 'Hadoop (or similar) Ecosystem (MapReduce, Yarn, HDFS, Hive, Spark, Presto, Pig, HBase)', 'Traditional relational databases and/or distributed systems such as Hadoop/Hive, BigQuery, Redshift, Snowflake', 'Coach data engineers best practices and technical concepts of building large scale data platforms', 'Responsibilities', 'Proficiency with Scala or Python (preferably both)', 'Develop unit/integration tests and data validations to ensure the quality of code and data', 'Workflow management tools such as Airflow', 'Drive and maintain a culture of quality, innovation and experimentation', 'Remote possible for the right candidate who is willing to work EST hours from within the USA', 'Experience with terabyte or petabyte scale data systems', 'Technology Skills & Experience', 'Develop high performance Spark ETLs in Scala and PythonDevelop unit/integration tests and data validations to ensure the quality of code and dataDrive and maintain a culture of quality, innovation and experimentationWork in an Agile environment that focuses on collaboration and teamworkCoach data engineers best practices and technical concepts of building large scale data platforms', 'Streaming platforms such as Flink, Spark', 'Develop high performance Spark ETLs in Scala and Python', '5+ years of relevant professional experience', 'Proficiency in Scala (or willingness to learn)', 'Basic Qualifications', 'Data exploration and data visualization tools such as Looker, Tableau, Chartio, Apache Superset, Plotly / Dash ', 'Work in an Agile environment that focuses on collaboration and teamwork', 'New York City preferredRemote possible for the right candidate who is willing to work EST hours from within the USA']",Mid-Senior level,Full-time,Quality Assurance,Marketing and Advertising,2021-03-18 14:34:51
Data Engineer,NFI,"Camden, NJ",3 days ago,Be among the first 25 applicants,"['', 'Optimizing high-performing SQL for efficient data transformation', 'Demonstrated success in technical proficiency, creativity, collaboration, and independent thought', 'Working knowledge of application development and data modeling to support implementation and development of analytics', '2+ years of experience creating data visualizations', ' Data modeling and dimensional schema design Design and develop data ingestion, pipeline, processing, and transformation frameworks to automate high-volume, and real-time data delivery ', 'Collaborate with various Business, Operations, Applications and Analytics teams to ensure adherence to enterprise data standards and data architecture principles', 'Demonstrated mastery in one or more SQL variants: T-SQL or PL/SQL', 'Essential Duties & Responsibilities', 'Strong interpersonal skills to resolve issues in a professional manner, lead working groups, negotiate and create consensus', 'Warehousing or Transportation Logistics experience, a plus', 'Build the data infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources', 'Experience architecting and implementing data solutions, and working with unstructured datasets in a cloud environment, a plus', 'Experience in migrating ETL processes (not just data) from relational databases to cloud based solutions', 'Java / R/ Python, Data science/machine learning, a plus', '4+ years of experience building data pipelines and using ETL tools', 'Excellent analytic and troubleshooting skills', ' Build the data infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Create, enhance, and optimize data visualizations Identify, design, and implement internal process improvements to automate manual processes, and optimize data delivery Build processes to support data transformation, data structures, metadata, dependency and workload management. Define and maintain target data architectures and master data management strategies Collaborate with various Business, Operations, Applications and Analytics teams to ensure adherence to enterprise data standards and data architecture principles Monitor compliance of development work against established standards and principles and work to resolve discrepancies and issues Optimizing high-performing SQL for efficient data transformation Production release and operation of data services ', 'Identify, design, and implement internal process improvements to automate manual processes, and optimize data delivery', 'Define and maintain target data architectures and master data management strategies', 'Design and develop data ingestion, pipeline, processing, and transformation frameworks to automate high-volume, and real-time data delivery', 'Requirements', 'Data modeling and dimensional schema design', 'Build processes to support data transformation, data structures, metadata, dependency and workload management.', 'In-depth knowledge of Microsoft SQL Server or Oracle', 'Proficiency with data profiling and data migration', 'Bachelor’s degree in Computer Science, Engineering, or related field', 'Knowledge of real-time data acquisition and/or automation/controls', 'Demonstrated mastery in database concepts and large-scale database implementations and design', ' Bachelor’s degree in Computer Science, Engineering, or related field 4+ years of experience building data pipelines and using ETL tools 2+ years of experience leading data Engineering efforts 2+ years of experience creating data visualizations Knowledge of data management, data integration and database development technologies and processes Up-to-date specialized knowledge of data wrangling, manipulation and management Demonstrated mastery in one or more SQL variants: T-SQL or PL/SQL In-depth knowledge of Microsoft SQL Server or Oracle Demonstrated mastery in database concepts and large-scale database implementations and design Excellent analytic and troubleshooting skills Proficiency with data profiling and data migration Knowledge of real-time data acquisition and/or automation/controls Working knowledge of application development and data modeling to support implementation and development of analytics Demonstrated success in technical proficiency, creativity, collaboration, and independent thought Experience in migrating ETL processes (not just data) from relational databases to cloud based solutions Strong interpersonal skills to resolve issues in a professional manner, lead working groups, negotiate and create consensus Experience architecting and implementing data solutions, and working with unstructured datasets in a cloud environment, a plus Java / R/ Python, Data science/machine learning, a plus Warehousing or Transportation Logistics experience, a plus ', 'Production release and operation of data services', 'Data Engineer', 'Knowledge of data management, data integration and database development technologies and processes', 'Monitor compliance of development work against established standards and principles and work to resolve discrepancies and issues', '2+ years of experience leading data Engineering efforts', 'Up-to-date specialized knowledge of data wrangling, manipulation and management', 'Create, enhance, and optimize data visualizations']",Entry level,Full-time,Information Technology,Construction,2021-03-18 14:34:51
Sr Data Engineer (Remote),PRECISION SERVICES SYSTEM IT,"Cleveland, OH",21 hours ago,Be among the first 25 applicants,"['We never submit your resume without your permission, and we never give your resume to other staffing companies', 'We have dedicated HR personnel that will answer the phone when you call. THE STAFFING FIRM YOU WORK WITH MATTERS. WORK WITH PRECISION. Job ID 4344DI-2355', 'Familiarity with big data technology including PDW and Hadoop', 'We are up front about pay rates and the interviewing process, and provide you with constant feedback. Our Responsibility. We take seriously our obligations to our consultants.', 'We offer highly competitive pay rates, as well as an opportunity to participate in our group health insurance, long term disability and 401(k) plans.', ""We actually understand your skills. We'll make sure that you are right for the job, and that the job is right for you. Our Reputation for Quality. Clients favor Precision's candidates."", 'Training in lean and agile delivery methodologies Please click the Apply Now button to apply for the job. We will review your resume and call if you are qualified. Resumes will NOT be sent to clients without your approval. REFERRALS WANTED', ""We're the top provider of contract IT professionals to many of our clients Our Experience. We know how to sell our candidates to clients."", 'Experience working with a financial ERP product like Oracle, SAP or Workday Plusses', 'Familiarity with cloud data pipelines and products such as AWS, S3, Lambda, Snowflake', 'We know what it is like to be an IT consultant working in a large organization.', 'We take the time to learn about you so that we can best match you to our openings and sell you to our clients.', ""Our clients know from experience that Precision's consultants are the most qualified and the best fit for their project teams. Our Ethics. We pride ourselves on our ethical business practices."", ""We've been in business for over 20 years, and have made thousands of successful placements Our Background. Precision is run by former IT professionals."", 'We are direct vendors to Fortune 500 companies across North America', 'When required, we carefully manage H1-B and applications, and involve you at every step.', '1000 REWARD! Refer a colleague to us, and Precision will give you 1000 if we find a job for that person! (The fine print The referred candidate must be previously unknown to us. Start date must be within 6 months of referral.) THERE ARE MANY IT STAFFING FIRMS YOU CAN WORK WITH. WHY WORK WITH PRECISION? Our Clients. We work with the best.', 'We always pay on time', 'Experience building and supporting data pipelines and products using SQL, SSIS, SSAS, C, .Net']",Associate,Contract,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Centura Health,"Centennial, CO",3 days ago,Be among the first 25 applicants,"['', 'Experience with several querying languages, schema definition languages, and scripting languages', ""Bachelor's degree in engineering, physics, computer sciences, math, information systems, statistics or related field, required. Equivalent combination of education and experience may be considered in lieu of Bachelor's Degree"", ""5+ years of experience as a data engineer or in a similar roleTechnical expertise with data models, data mining, and segmentation techniquesExperience with several querying languages, schema definition languages, and scripting languagesExperience with writing and optimizing SQL queries in a business environment with large-scale, complex data setsExperience with working with large data sets, data warehouse technical architecture, infrastructure components, ETL, and reporting/analytic tools and environmentsStrong knowledge of various data warehousing methodologies and data modeling concepts. Hands on modelling experience is highly desiredBachelor's degree in engineering, physics, computer sciences, math, information systems, statistics or related field, required. Equivalent combination of education and experience may be considered in lieu of Bachelor's Degree"", 'Experience with writing and optimizing SQL queries in a business environment with large-scale, complex data sets', 'Experience with working with large data sets, data warehouse technical architecture, infrastructure components, ETL, and reporting/analytic tools and environments', 'Strong knowledge of various data warehousing methodologies and data modeling concepts. Hands on modelling experience is highly desired', '5+ years of experience as a data engineer or in a similar role', 'Technical expertise with data models, data mining, and segmentation techniques', 'What You Bring/Job Requirements']",Entry level,Full-time,Information Technology,Hospital & Health Care,2021-03-18 14:34:51
Data Engineer - Washington,DC Public Charter School Board,"Washington, DC",,N/A,"['', 'About The Dc Public Charter School Board (dc Pcsb)', 'TO APPLY', 'Competencies And Qualifications', 'Compensation', 'About The Role', 'DC PCSB is an equal opportunity employer committed to building a culturally diverse staff. We strive to foster an environment where everyone feels included. We believe that when people bring their unique identities, backgrounds, perspectives, and experiences to our community, we are able to truly achieve excellence in our work.']",Entry level,Full-time,Information Technology,Nonprofit Organization Management,2021-03-18 14:34:51
Data Engineer,Charles Schwab,"Phoenix, AZ",6 days ago,Be among the first 25 applicants,"['', ' Mentoring, motivating, and supporting the team to achieve organizational objectives and goals ', ' 2+ years of experience building data pipelines and interfaces with object oriented languages (.Net, Java, Python) ', ' 2+ years of experience working on agile teams delivering data solutions ', ' Designing, implementing, and maintaining data warehouses and near real-time data pipelines via the practical application of existing and new data engineering techniques ', ' 1+ years of experience modeling star schema data warehouses using the Kimball dimensional modeling techniques ', ' Exceptional interpersonal skills, including teamwork, communication, and negotiation', 'What You Are Good At', ' 3+ years of experience designing, building, and supporting near real-time data pipelines and analytical solutions using Hadoop, Teradata, MS SQL Server, Talend, Informatica, and/or SSIS ', ' 1+ years of experience delivering solutions on public cloud platforms (Google Cloud preferred) ', 'What You Have', ' Ensuring consistency with published development, coding and testing standards ', ' Advocating for agile practices to increase delivery throughput ', ' 3+ years of experience designing, building, and supporting near real-time data pipelines and analytical solutions using Hadoop, Teradata, MS SQL Server, Talend, Informatica, and/or SSIS  2+ years of experience working on agile teams delivering data solutions  2+ years of experience building data pipelines and interfaces with object oriented languages (.Net, Java, Python)  1+ years of experience modeling star schema data warehouses using the Kimball dimensional modeling techniques  1+ years of experience delivering solutions on public cloud platforms (Google Cloud preferred)  Basic understanding of at least one IT Management frameworks such as ITIL or COBiT  Experience writing automated unit, integration, and acceptance tests for data interfaces & data pipelines  Ability to quickly learn & become proficient with new technologies  Exceptional interpersonal skills, including teamwork, communication, and negotiation', ' Collaborating directly with business and technology stakeholders to define future-state business capabilities & requirements, and translating those into transitional and target state data architectures  Analyzing the current technology environment to detect critical deficiencies, and recommend solutions for improvement  Designing, implementing, and maintaining data warehouses and near real-time data pipelines via the practical application of existing and new data engineering techniques  Developing continuous integration and continuous deployment pipelines for data solutions that include automated unit & integration testing  Mentoring, motivating, and supporting the team to achieve organizational objectives and goals  Advocating for agile practices to increase delivery throughput  Ensuring consistency with published development, coding and testing standards ', ' Ability to quickly learn & become proficient with new technologies ', 'Your Opportunity ', ' Developing continuous integration and continuous deployment pipelines for data solutions that include automated unit & integration testing ', ' Analyzing the current technology environment to detect critical deficiencies, and recommend solutions for improvement ', ' Experience writing automated unit, integration, and acceptance tests for data interfaces & data pipelines ', ' Collaborating directly with business and technology stakeholders to define future-state business capabilities & requirements, and translating those into transitional and target state data architectures ', ' Basic understanding of at least one IT Management frameworks such as ITIL or COBiT ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Junior/Grad Data Engineer,RealTime Recruitment,"Antrim County, MI",2 weeks ago,37 applicants,"['', 'Interested?', 'SQL knowledge', ""Ideally, you'll have 1 year experience in Data Engineering"", 'What do I need to apply?', 'Opportunity to gain exposure to the latest tech stack', ""In return you'll receive: "", 'Do you want to work in a team where your voice is heard and gain exposure to the latest tech stack?', ""Ideally, you'll have 1 year experience in Data EngineeringSQL knowledgeBachelor's degree or higherThe right to work in the UK"", 'Are you a Junior Data Engineer or Graduate looking for an exciting new challenge?', 'The right to work in the UK', 'A market leading salary based on your experience', 'james.dowse@realtime.jobs', '02895 211 121', ""Bachelor's degree or higher"", 'What will you be doing?', 'Flexible working opportunities', 'Company bonus scheme', 'A market leading salary based on your experienceCompany bonus schemeFlexible working opportunitiesOpportunity to gain exposure to the latest tech stack']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer ,Bayside Solutions,"Seattle, WA",3 days ago,59 applicants,"['', 'Only W2 candidates and Candidate needs to be willing to re-lo to Seattle if and when the client goes back onsite.', 'Passionate about latest big data technologies, open source community presence is a big plus', '5+ years of experience in big data ecosystem. Example technologies include batch and stream processing (e.g. Spark, Hive, Flink, Beam), analytical engines (e.g. Presto, Druid), search platform (e.g. Solr/Lucene), tooling (e.g. Airflow, Jupyter, Superset, Tableau), and storage format (e.g. Iceberg)Excellent verbal and written communication skills, able to collaborate cross-functionally with data science, machine learning, data platform and analytics teamsCustomer-focused mindset, with emphasis on user experience and satisfactionSuperb problem-solving skills, and able to thrive in a fast-paced and dynamic environmentHands-on in designing, building, scaling, and troubleshooting solutions to big data problemsMust be self-driven, and able to provide advice and support to users to properly integrate with our data platformProgramming experience in Java, Python, Scala, or similar languagesPassionate about latest big data technologies, open source community presence is a big plusExperience with AWS, Kubernetes, Infrastructure-as-code, and data privacy & compliance is a big plus', 'Preferred - Only W2 candidates and Candidate needs to be willing to re-lo to Seattle if and when the client goes back onsite.', 'Customer-focused mindset, with emphasis on user experience and satisfaction', '\xa0Data Infrastructure team within an organization that powers analytics, experimentation and ML feature engineering. The mission of the Data Infrastructure org is to provide our engineers and data scientists a cutting edge, reliable and easy to use infrastructure for ingesting, storing, processing and interacting with data and ultimately help the teams that build data intensive applications be successful. You will work with many cross functional teams and lead the planning, execution and success of technical projects with the ultimate purpose of improving experience for customers. We are looking for engineers who want to bring their passion for infrastructure to build world class infrastructure products. Are you a passionate about building scalable, reliable, maintainable infrastructure and solving data problems at scale? Come join us and be part of the Data Infrastructure journey.', 'Description', 'Superb problem-solving skills, and able to thrive in a fast-paced and dynamic environment', 'Summary', 'Must be self-driven, and able to provide advice and support to users to properly integrate with our data platform', 'Excellent verbal and written communication skills, able to collaborate cross-functionally with data science, machine learning, data platform and analytics teams', 'Preferred ', 'Programming experience in Java, Python, Scala, or similar languages', 'Education & Experience', 'BS, MS, or PhD degree in Computer Science or equivalent', 'Hands-on in designing, building, scaling, and troubleshooting solutions to big data problems', 'You will be a key member in the client engineering team that drives user engagement with our new generation of data platform, collaborates cross-functionally with data, product management, and infrastructure teams, and provides data tools to enable ingesting, storing, processing and interacting of data with a focus in user experience. RESPONSIBILITIES INCLUDE: - Collaborate with our infrastructure users and product management to ensure success in customer engagement and onboarding of new users - Be an advocate of our tech stack, stay on top of technology advancement and explore innovation opportunities - Prototype, build, diagnose, fix, improve and automate complex issues across the entire stack to power ETL, analytics and privacy efforts across AI/ML - Advise and support other teams on proper integration of our platform, including holding regular brown bag, office hour and training sessions - Build relationships with Data Scientists, Product Managers and Software Engineers to understand data needs - Establish and fulfill SLAs for supported data tools', 'Experience with AWS, Kubernetes, Infrastructure-as-code, and data privacy & compliance is a big plus', '5+ years of experience in big data ecosystem. Example technologies include batch and stream processing (e.g. Spark, Hive, Flink, Beam), analytical engines (e.g. Presto, Druid), search platform (e.g. Solr/Lucene), tooling (e.g. Airflow, Jupyter, Superset, Tableau), and storage format (e.g. Iceberg)', 'Key Qualifications']",Associate,Contract,Engineering,Consumer Electronics,2021-03-18 14:34:51
Intern - Data Engineer,American Medical Association,"Chicago, IL",2 days ago,57 applicants,"['', 'Any knowledge/interest in implementing data management systems, ETL development, or master data management solutions is highly desirable.', 'Experience and interest in presenting analytic findings to business customers. Familiar with reporting and visualization tools, such as Tableau, Power BI, Business Objects, etc.', 'Basic analysis skills; familiar with data analysis tools and techniques, such as SAS, R, Python, SPSS, text analytics, NLP; Able to manage and integrate insights and establish monitoring around multiple internal data sources, such as AMA Masterfile, Enterprise Data Warehouse, customer database and purchased/appended data if available.', 'Familiar with SQL in the extraction and manipulation of datasets. General knowledge of transactional data processing, ETL, data warehouse, data mart, and operational reporting solutions a plus.Any experience with the following tools is desirable: Aqua Data Studio, IBM DataStage / QualityStage, Information Analyzer, Informatica Business Glossary and Powercenter.', 'What Puts You Over The Top', 'Experience and interest in deeper analysis of data subjects, potentially spanning over a timeline of several months.', 'Responsibilities', 'Be working towards a BS or MS degree in Data Science, Statistics, Analytics, Computer Science, Information Systems, or a related degree.', 'Support the overall effort to modernize and enhance AMA data management architecture. Assist in development and testing of innovative analytical approaches, including but not limited to Big Data, A.I., machine learning, text analytics, and Natural Language Processing (NLP). Assist in vendor integration, data management work flow evaluation and optimization, vendor service level adherence monitoring activities, and requirements gathering around internal data management processes as effort moves forward.', 'Some practical experience working on AWS or another cloud provider ', 'Support the Data Management group and overall Health Solutions team through insightful data analysis and the development of analytical reporting, including subject areas of data collection, data quality, business rule adherence, and optimal choice of data fitness. Tailor analytical/reporting output to be digestible given the audience through usage of visualization and dashboards. Utilize technical acumen to automate reports over time and eliminate unnecessary manual processes. Support the overall effort to modernize and enhance AMA data management architecture. Assist in development and testing of innovative analytical approaches, including but not limited to Big Data, A.I., machine learning, text analytics, and Natural Language Processing (NLP). Assist in vendor integration, data management work flow evaluation and optimization, vendor service level adherence monitoring activities, and requirements gathering around internal data management processes as effort moves forward.Respond to data analysis needs and help deliver data analysis projects through the appropriate choice of front-end error detection and correction, process control and improvement, or process design strategies. Develop testing and processes to ensure data integrity and accuracy of the data, as well as proof of concept matching exercise to vet external data sources and gauge benefit. Follow all processes and procedures and provide documentation on all work. Support the ongoing effort to master AMA data assets through the implementation of an enterprise wide physician Masterfile strategy, work closely with technology partners to leverage technology tools to the utmost and understand internal data flow and ETL activities between current AMA systems and future platforms. ', 'Requirements', 'Experience with various batch matching methodologies. Willingness to learn and work with disparate data sets of varying structures and quality and interested in creating ad hoc methodologies to facilitate matching on these data sets, often without the benefit of common keys.', 'Some practical experience working on AWS or another cloud provider Some practical experience developing with Apache Spark and/or Hive Good knowledge of SQL and experience with columnar datastoresYou are working on your Masters degree', 'Ingestion, standardization, metadata management, business rule curation, data enhancement, and statistical computation against data sources that include relational, XML, JSON, streaming, REST API, and unstructured data.', 'THE AMA IS COMMITTED TO IMPROVING THE HEALTH OF THE NATION', 'Data Science InternChicago, IL', 'Any experience with or basic understanding of newer database structures and models such as NoSQL, Hadoop, Marklogic and Cassandra, a plus. Programming skills in Python, Java highly desirable.', 'Respond to data analysis needs and help deliver data analysis projects through the appropriate choice of front-end error detection and correction, process control and improvement, or process design strategies. Develop testing and processes to ensure data integrity and accuracy of the data, as well as proof of concept matching exercise to vet external data sources and gauge benefit. Follow all processes and procedures and provide documentation on all work. ', 'May include other responsibilities as assigned. ', 'You are working on your Masters degree', 'Understanding of orchestration and scheduling tooling such as Jenkins/Airflow/Rundeck', 'Support the ongoing effort to master AMA data assets through the implementation of an enterprise wide physician Masterfile strategy, work closely with technology partners to leverage technology tools to the utmost and understand internal data flow and ETL activities between current AMA systems and future platforms. ', 'Support the Data Management group and overall Health Solutions team through insightful data analysis and the development of analytical reporting, including subject areas of data collection, data quality, business rule adherence, and optimal choice of data fitness. Tailor analytical/reporting output to be digestible given the audience through usage of visualization and dashboards. Utilize technical acumen to automate reports over time and eliminate unnecessary manual processes. ', 'Some practical experience developing with Apache Spark and/or Hive ', 'Good knowledge of SQL and experience with columnar datastores']",Internship,Internship,Information Technology,Nonprofit Organization Management,2021-03-18 14:34:51
Data Engineer,Bangor Savings Bank,"Bangor, ME",3 days ago,Be among the first 25 applicants,"['', 'Relevant technical certification(s) strongly preferred.', 'Strong knowledge of SQL development, performance tuning, index management.', 'Apply Bank standards and industry best practices to the ongoing management of the database infrastructure and related technologies.', 'Are a successful, vibrant, and innovative company.Care most about our employees, our customers and our communities.Believe every interaction is an opportunity to provide a “You Matter More” experience.Believe in autonomy & initiative taking.Are a technologically and data-driven business.Have a smart, experienced, and diverse leadership team that wants to do it right & is open to new ideas.Have a beautiful new campus in Bangor, Maine.', 'Have a high proficiency in a MS SQL environment.', 'You’ll Love Working at Bangor Savings Bank Because We… ', 'Perform data profiling of source data to identify data quality issues and anomalies, business knowledge embedded in data, gathering of natural keys, and metadata information.', 'Collaborate with project leads, business analysts, end users and third-party contacts to design, implement and test data warehouse applications.', 'Have good interpersonal skills including active listening skills and negotiation techniques.', 'Strong verbal and written communication skills.', 'We’re Excited About You Because You…', 'Have a beautiful new campus in Bangor, Maine.', 'By submitting your interest in this job, you agree to receive text notifications with additional steps to complete your job application. You will receive up to 6 messages from the number ""63879"". Message & data rates may apply. Please refer to our privacy policy for more information.', 'Experience working with multi-format files likes JSON, XML, CSV, Flat, etc.', 'Translate business requirements and data needs into solutions easily used for reporting, scorecards and dashboards.', 'Have relevant employment experience and demonstrated abilities as a Data Engineer.Have the desire and ability to maintain knowledge and skill currency within the fast-changing technological realm.', 'Interact with end users and business analysts to understand reporting/dashboard requirements.', 'Support and improve production data integration system and environment.Understand the data architecture needs and data structures in the source systems and business processes.Design data marts for business units and collaborate with development teams during the implementation.Collaborate with internal & external data consumers to understand their data needs and drive towards unifying collections of data requirements for key data elements across the organization.Document and maintain documentation related data mapping and other data design artifacts that encompass data specifications, business & transformation rules.Collaborate with vendors and internal developers in requirements gathering sessions with stakeholders to determine user needs and capture data requirements.Translate business requirements and data needs into solutions easily used for reporting, scorecards and dashboards.Apply Bank standards and industry best practices to the ongoing management of the database infrastructure and related technologies.Demonstrate ownership of database and related technologies and all issues that arise with them.Ensure the highest levels of availability and performance within BI systems and infrastructure.Perform BI Administrative functions as requested.', 'Have a strong organization system and use that system for the improvement and advancement of personal and team goals.', 'Care most about our employees, our customers and our communities.', 'Have relevant employment experience and demonstrated abilities as a Data Engineer.', 'You’re Excited About This Opportunity Because You Will…', 'Develop and perform unit, system, performance and regression testing on ETL mappings.', 'Have the vision and values of BSB!', 'Have the desire and ability to maintain knowledge and skill currency within the fast-changing technological realm.', 'Experience in one or more programming languages like Python, JavaScript, C#, Java, etc.', 'Experience working with APIs like REST APIs, SDKs and CLI tools as part of ETL provisioning.', 'Strong knowledge of relational and multi-dimensional databases.', 'Have integrity and ethics to deal with others in a straightforward, honest manner, are accountable for you own actions, maintain confidentiality, support company values, and convey news good or bad.', 'Design and develop enterprise and departmental business intelligence and data warehousing solutions.', 'Have demonstrated experience using SSIS, Talend or other similar ETL tools in a data warehouse environment.', 'About The Role', 'Understand the data architecture needs and data structures in the source systems and business processes.', 'Expert-level knowledge of modern databases and their related toolsets, reporting packages, and underlying technologies.', 'Believe in autonomy & initiative taking.', 'Design data marts for business units and collaborate with development teams during the implementation.', 'Believe every interaction is an opportunity to provide a “You Matter More” experience.', 'Are a successful, vibrant, and innovative company.', 'Experience and implementation of Data Architecture, Data Lake, Data Marts, Operational Data Store, Analytical systems & Metadata management initiatives.', 'Experience with schema design and dimensional data modeling.', 'Are a technologically and data-driven business.', 'Have a smart, experienced, and diverse leadership team that wants to do it right & is open to new ideas.', 'What You Bring To The Table Is…', 'Willingness and ability to maintain knowledge regarding relevant current and emerging technologies and industry trends and best practices.', 'Collaborate with internal & external data consumers to understand their data needs and drive towards unifying collections of data requirements for key data elements across the organization.', 'Are adaptable and flexible, aren’t afraid of change, open to new ideas, take on new challenges, handle pressure, adjust plans to meet changing needs.', 'Strong documentation skills, to include proficiency with MS-Word, MS-Excel and MS-Visio.', 'Hands-on experience with data modeling techniques, including with star schemas and contemporary ETL strategies.', 'Collaborate with project leads, business analysts, end users and third-party contacts to design, implement and test data warehouse applications.Design, build, enhance, and maintain ETL processes to make new and existing data sources.Develop and perform unit, system, performance and regression testing on ETL mappings.Design and develop enterprise and departmental business intelligence and data warehousing solutions.Interact with end users and business analysts to understand reporting/dashboard requirements.Perform data profiling of source data to identify data quality issues and anomalies, business knowledge embedded in data, gathering of natural keys, and metadata information.', 'Collaborate with vendors and internal developers in requirements gathering sessions with stakeholders to determine user needs and capture data requirements.', 'Are analytical, observe processes and trends; make recommendations for process changes that help achieve departmental and individual goals.Take initiative and independent action, operate as a pro-active self-starter, act on opportunities, and practice self-development.Are adaptable and flexible, aren’t afraid of change, open to new ideas, take on new challenges, handle pressure, adjust plans to meet changing needs.Have integrity and ethics to deal with others in a straightforward, honest manner, are accountable for you own actions, maintain confidentiality, support company values, and convey news good or bad.Have good interpersonal skills including active listening skills and negotiation techniques.Have the vision and values of BSB!Have a strong organization system and use that system for the improvement and advancement of personal and team goals.', 'A BS or MS degree in Computer Science or a related technical field or relevant work experience in the field.', 'Design, build, enhance, and maintain ETL processes to make new and existing data sources.', 'Document and maintain documentation related data mapping and other data design artifacts that encompass data specifications, business & transformation rules.', 'Take initiative and independent action, operate as a pro-active self-starter, act on opportunities, and practice self-development.', 'Perform BI Administrative functions as requested.', 'Have demonstrated experience using SSIS, Talend or other similar ETL tools in a data warehouse environment.Have a high proficiency in a MS SQL environment.', 'Are analytical, observe processes and trends; make recommendations for process changes that help achieve departmental and individual goals.', 'A BS or MS degree in Computer Science or a related technical field or relevant work experience in the field.Experience and implementation of Data Architecture, Data Lake, Data Marts, Operational Data Store, Analytical systems & Metadata management initiatives.Experience with schema design and dimensional data modeling.Experience in one or more programming languages like Python, JavaScript, C#, Java, etc.Experience working with APIs like REST APIs, SDKs and CLI tools as part of ETL provisioning.Experience working with multi-format files likes JSON, XML, CSV, Flat, etc.Relevant technical certification(s) strongly preferred.Exceptional troubleshooting abilities.Strong verbal and written communication skills.Strong documentation skills, to include proficiency with MS-Word, MS-Excel and MS-Visio.Expert-level knowledge of modern databases and their related toolsets, reporting packages, and underlying technologies.Strong knowledge of SQL development, performance tuning, index management.Hands-on experience with data modeling techniques, including with star schemas and contemporary ETL strategies.Strong knowledge of relational and multi-dimensional databases.Analytical approach to problem solving and process improvement.Willingness and ability to maintain knowledge regarding relevant current and emerging technologies and industry trends and best practices.', 'Analytical approach to problem solving and process improvement.', 'Ensure the highest levels of availability and performance within BI systems and infrastructure.', 'Exceptional troubleshooting abilities.', 'Support and improve production data integration system and environment.', 'Demonstrate ownership of database and related technologies and all issues that arise with them.']",Entry level,Full-time,Information Technology,Banking,2021-03-18 14:34:51
Data Engineer,TEKsystems,"Allentown, PA",3 days ago,Be among the first 25 applicants,"['', 'Top Skills Details', 'Description', ' Experience leading a Data Warehouse and/or Data Engineering team. Must be able to build a data lake and data warehouse strategy', ' Data engineering experience - data ingestion and building data pipelines', 'About TEKsystems', ' Cloud Analytics experience (vendor agnostic - can be AWS, GCP, Azure or Snowflake)', ' Experience leading a Data Warehouse and/or Data Engineering team. Must be able to build a data lake and data warehouse strategy Data engineering experience - data ingestion and building data pipelines Cloud Analytics experience (vendor agnostic - can be AWS, GCP, Azure or Snowflake)']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
eCom Data Engineer,PepsiCo,"San Francisco, CA",20 hours ago,Be among the first 25 applicants,"['', 'Act as a consultant to the broader business users, management, vendors, and technicians to determine technology needs and system requirements.', 'Build new technologies and algorithms to optimize any business process', 'BS or MS degree in Computer Science or a related technical field', 'Proven self-starter who can move projects forward by filling in the gaps on Agile teams, from leading a design session to doing some test automation, to mentoring a teammate struggling with a new technology', 'Create and maintain optimal data pipelines architecture.', 'Use large data sets to resolve major business and functional issues while improving data reliability, efficiency and quality', '3+ years of Python or Java development experience, Experience with multiple data technologies and concepts such as Airflow, Kafka, Hadoop, Hive, Spark, MapReduce, SQL, NoSQL, and Columnar databases', 'Experience designing, building and maintaining data processing systems', 'Knowledge of machine-learning tools and techniques', 'Accountabilities', 'Ability in managing and communicating data warehouse plans to internal clients.', 'Optimize processes implementing new technology and automation across eCommerce businesses and eCommerce functions', 'Develop technology project evaluations as well as proposal feasibility with the different eCommerce businesses', 'Relocation Eligible:', 'Auto req ID:', '3+ years of experience with schema design and dimensional data modeling', 'Develop technology project evaluations as well as proposal feasibility with the different eCommerce businessesCreate and maintain optimal data pipelines architecture.Apply theoretical expertise and innovation to create or apply new technologies to apply to the entire digital landscapeAct as a consultant to the broader business users, management, vendors, and technicians to determine technology needs and system requirements.Build new technologies and algorithms to optimize any business processUse large data sets to resolve major business and functional issues while improving data reliability, efficiency and qualityOptimize processes implementing new technology and automation across eCommerce businesses and eCommerce functions', 'BS or MS degree in Computer Science or a related technical field3+ years of Python or Java development experience, Experience with multiple data technologies and concepts such as Airflow, Kafka, Hadoop, Hive, Spark, MapReduce, SQL, NoSQL, and Columnar databases3+ years of experience with schema design and dimensional data modelingAbility in managing and communicating data warehouse plans to internal clients.Experience designing, building and maintaining data processing systemsExperience writing production code for Python or JVM-based systems, but you know a few other languages and like the right tool for the jobKnowledge of machine-learning tools and techniquesProven self-starter who can move projects forward by filling in the gaps on Agile teams, from leading a design session to doing some test automation, to mentoring a teammate struggling with a new technologyRequires Department of Transportation (DOT) certification and successful Motor Vehicle Report (MVR) review during the pre-onboarding process', 'Qualifications/Requirements', 'Job Description', 'Apply theoretical expertise and innovation to create or apply new technologies to apply to the entire digital landscape', 'Requires Department of Transportation (DOT) certification and successful Motor Vehicle Report (MVR) review during the pre-onboarding process', 'Experience writing production code for Python or JVM-based systems, but you know a few other languages and like the right tool for the job', 'Job Type:']",Mid-Senior level,Full-time,Information Technology,Consumer Goods,2021-03-18 14:34:51
Data Engineer,Meridian Technology Group,United States,2 weeks ago,132 applicants,"['', 'Full salary and benefit package including unlimited PTO', 'Any offer of employment will be conditional, based on successfully passing a Criminal Background Check.', 'Good experience with ETL / data ingestion', 'Constructing, testing and maintaining data ingestion pipelines and architecture', 'Strong, professional experience in data engineering', 'Meridian Technology Group is committed to equal employment opportunity (EEO) and non-discrimination for all employees in all job classifications and for prospective employees without regard to race, color, religion, sex, age, sexual orientation, veteran status, physical or mental disability, national origin, or any other characteristic protected by applicable federal or state law. All hiring is contingent on eligibility to work in the United States. We are unable to sponsor applicants for work visas therefore, please do not apply if you are not\xa0eligible to work without sponsorship, as sponsorship is not available at this time. ', 'Developing and maintaining a data dictionary for persistent database', 'Experience with HL7, FHIR or any Electronic Health Record (EHR) platform', 'Interesting, real-world problems to solveMaking a difference in millions of livesFull salary and benefit package including unlimited PTOOther smart, humble engineers to work with', ""What you'll be doing"", 'If you’re looking for something meaningful, working with other smart engineers, let’s talk.', 'Understanding of PHI and HIPAA compliance', 'This position is Remote.\xa0', 'Conducting database design and architecture', 'Excellent knowledge of SQL', 'Making a difference in millions of lives', 'Experience working with healthcare dataUnderstanding of PHI and HIPAA complianceExperience with HL7, FHIR or any Electronic Health Record (EHR) platformExperience with any of the following: Postgres, Docker, Kubernetes, AWS', 'Meridian Technology Group is seeking a Data\xa0Engineer. ', 'Interesting, real-world problems to solve', 'No 3rd party companies/candidates.', 'Other tech includes Scala, Python, AWS, Docker, Kubernetes, Postgres', 'This position will be responsible for establishing data quality best practices, and for building, testing and maintaining our data ingestion pipelines. Powered by an analytics and workflow engine, data is the centerpiece of our platform. We use Artificial Intelligence (AI) and Natural Language Processing (NLP) to transform structured and unstructured data into accurate, actionable insights. Establishing robust data quality assurance is a critical aspect of this role.', 'Meridian Technology Group is seeking a Data\xa0Engineer', 'Experience working with healthcare data', 'Experience with any of the following: Postgres, Docker, Kubernetes, AWS', 'Constructing, testing and maintaining data ingestion pipelines and architectureConducting database design and architectureIdentifying and implementing ways to improve data reliability, efficiency and qualityDeveloping and maintaining a data dictionary for persistent database', '\ufeffWhat’s in it for you', 'Data Engineer', 'Strong, professional experience in data engineeringExcellent knowledge of SQLProficiency with at least one programming languages (Scala, Python, Java, etc.)Good experience with ETL / data ingestion', 'Nice to have', 'What we’re looking for', 'Other smart, humble engineers to work with', 'Identifying and implementing ways to improve data reliability, efficiency and quality', 'Meridian Technology Group is committed to equal employment opportunity (EEO) and non-discrimination for all employees in all job classifications and for prospective employees without regard to race, color, religion, sex, age, sexual orientation, veteran status, physical or mental disability, national origin, or any other characteristic protected by applicable federal or state law. All hiring is contingent on eligibility to work in the United States. We are unable to sponsor applicants for work visas therefore, please do not apply if you are not\xa0eligible to work without sponsorship, as sponsorship is not available at this time.  No 3rd party companies/candidates.', '100% remote (forever) anywhere in the United States', 'Proficiency with at least one programming languages (Scala, Python, Java, etc.)']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Travelers,"Hartford, CT",4 weeks ago,Be among the first 25 applicants,"['', 'Nice to have experience with Airflow DAG for orchestration.', 'Develops and maintains relationships across the enterprise.', 'Primary Job Duties & Responsibilities', 'Education, Work Experience, & Knowledge', 'Develops process to acquire and integrate data.', 'Able to clearly and concisely communicate with the business. Demonstrates willingness, cooperation, and concern for business issues. Able to understand assigned business unit and business priorities.', 'Company Summary', 'Consultation:Shares knowledge with users on data or analytic products.', 'Minimum Qualifications', 'Employment Practices', 'Travelers is an equal opportunity employer. We value the unique abilities and talents each individual brings to our organization and recognize that we benefit in numerous ways from our differences. ', 'Performs analysis of complex (type, quality, volume) sources to determine value and use. Determines and recommends data to include in analytical projects.', 'Hands on experience using Spark/Python. Utilizing DataBricks a plus!', 'Hands-on experience in writing complex, optimized SQL queries across large datasets. ', 'Hands on experience using Spark/Python. Utilizing DataBricks a plus!Understanding of AWS Native tools like Lambda, Glue, Kinesis, Terraform scripting, EC2, IAM etc.Hands-on experience in writing complex, optimized SQL queries across large datasets. Understanding of tools/technologies like Athena, Redshift and snowflake a plusUnderstanding of Visualization tools: Tableau, Qliksense or MicrostrategyNice to have experience with Airflow DAG for orchestration.Having awareness of web development technologies and frameworks like Node Js, React Js, HTML, Java script, CSS is a plus', 'Travelers reserves the right to fill this position at a level above or below the level included in this posting.', 'Effectively contributes and communicates with the immediate team.', 'Advanced knowledge of data tools, techniques, and manipulation preferred.', 'Taking care of our customers, our communities and each other. That’s the Travelers Promise. By honoring this commitment, we have maintained our reputation as one of the best property casualty insurers in the industry for over 160 years. Join us to discover a culture that is rooted in innovation and thrives on collaboration. Imagine loving what you do and where you do it.', 'Ensures customer satisfaction through professional communication, follow-up, and responsiveness to issues.', 'Job Description Summary', 'Able to coordinate with other technical areas to achieve project/department or division goals.', 'Able to recognize, analyze, and diagnose business and data issues of advanced complexity.', 'Leads medium scale projects and coordinates aspects of larger projects with limited supervision.', 'Actively seeks opportunities to expand technical knowledge and capabilities.', 'Understanding of AWS Native tools like Lambda, Glue, Kinesis, Terraform scripting, EC2, IAM etc.', 'Job Specific Technical Skills & Competencies', 'Ability to communicate thoughts/designs/ideas in an unambiguous manner and adjusts communication based on audience.', 'Understanding of tools/technologies like Athena, Redshift and snowflake a plus', 'Develops moderate and applies complex data derivations, business transformation rules, and data requirements.', 'Target Openings', 'Understanding of Visualization tools: Tableau, Qliksense or Microstrategy', 'Provides guidance and mentorship to lower level technical employees.', 'Builds effective relationships with stakeholders.', 'Applies knowledge of current industry trends and techniques to formulate solutions within the context of assigned projects and/or enhancements.', 'Reviews unfamiliar data sources. Connects to value for business. Able to link to systems.', 'Creates moderate (technology and features) data visualization techniques to help support data exploration.', 'Independently reviews, prepares, designs and integrates complex (type, quality, volume) data correcting problems and recommends data cleansing/quality solutions to major issues.', 'Examples (but not limited to): Big data and Hadoop platforms and languages - SAS, SQL, Spark, Python, Pig, Hive, QlikView, Tableau.', 'Exhibits active and effective communication skills with team members - including active listening and effective written and verbal communication skills.', 'Having awareness of web development technologies and frameworks like Node Js, React Js, HTML, Java script, CSS is a plus', 'Advanced knowledge of data tools, techniques, and manipulation preferred.Examples (but not limited to): Big data and Hadoop platforms and languages - SAS, SQL, Spark, Python, Pig, Hive, QlikView, Tableau.Ability to communicate thoughts/designs/ideas in an unambiguous manner and adjusts communication based on audience.Exhibits active and effective communication skills with team members - including active listening and effective written and verbal communication skills.Effectively contributes and communicates with the immediate team.Able to clearly and concisely communicate with the business. Demonstrates willingness, cooperation, and concern for business issues. Able to understand assigned business unit and business priorities.Able to coordinate with other technical areas to achieve project/department or division goals.Able to recognize, analyze, and diagnose business and data issues of advanced complexity.', '4 years of relevant experience with data tools, techniques, and manipulation required.', 'If you have questions regarding the physical requirements of this role, please send us an email so we may assist you.', 'Reviews unfamiliar data sources. Connects to value for business. Able to link to systems.Develops process to acquire and integrate data.Independently reviews, prepares, designs and integrates complex (type, quality, volume) data correcting problems and recommends data cleansing/quality solutions to major issues.Works within Travelers standards, processes, and protocols.Develops moderate and applies complex data derivations, business transformation rules, and data requirements.Leads medium scale projects and coordinates aspects of larger projects with limited supervision.Performs analysis of complex (type, quality, volume) sources to determine value and use. Determines and recommends data to include in analytical projects.Creates moderate (technology and features) data visualization techniques to help support data exploration.Utilizes business knowledge to explain technical activities in business terms.Actively seeks opportunities to expand technical knowledge and capabilities.Develops and maintains relationships across the enterprise.Applies knowledge of current industry trends and techniques to formulate solutions within the context of assigned projects and/or enhancements.Ensures customer satisfaction through professional communication, follow-up, and responsiveness to issues.Consultation:Shares knowledge with users on data or analytic products.Builds effective relationships with stakeholders.Provides guidance and mentorship to lower level technical employees.', 'Works within Travelers standards, processes, and protocols.', 'Utilizes business knowledge to explain technical activities in business terms.']",Not Applicable,Full-time,Information Technology,Insurance,2021-03-18 14:34:51
Data Engineer,"Cadence Bank, N.A.","Houston, TX",2 weeks ago,162 applicants,"['Professional image with ability to form good partner relationships across functions', ""The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and line of business partners on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company's data architecture to support our next generation of products and data initiatives."", 'Demonstrated abilities in relationship management', '1 - 3 years of experience participating in developing strategic plans to realize business objectives', '5 - 7 years of experience as a Data Engineer', 'Create and maintain optimal data pipeline architecture', 'Cadence Bank is an affirmative action/equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, religion, color, national origin, sex, age, status as a protected veteran, among other things, or status as a qualified individual with disability.', 'Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.', 'TRAVEL REQUIREMENT', 'A successful history of manipulating, processing and extracting value from large disconnected datasets.', '5 - 7 years of experience as a Data Engineer3 - 5 years of experience manipulating, processing and extracting value from large disconnected datasets.1 - 3 years of experience in developing, communicating and presenting concepts to varying audiences1 - 3 years of experience participating in developing strategic plans to realize business objectivesIndustry organization participation / leadership (ISACA, EDM Council, IAPP, etc.)', 'Strong analytic skills related to working with unstructured datasets.', 'Work with data and analytics experts to strive for greater functionality in our data systems.', 'Essential Responsibilities:', 'Experience in leading process improvement initiatives', 'Behavioral Traits', 'Ability to motivate high performance, multi-discipline teams', 'Assemble and optimize large, complex data sets that meet functional / non-functional business requirements.', 'Demonstrates the capacity to manage changing priorities and ambiguity while remaining calm and controlled', '3 - 5 years of experience as a Data Engineer or Sr. Data Analyst1 - 3 years of experience manipulating, processing and extracting value from large disconnected datasets.1 - 3 years of experience in developing, communicating and presenting concepts to varying audiences1 - 3 years of experience participating in developing strategic plans to realize business objectives', 'The above statements are intended to describe the general nature and level of the work being performed by people assigned to this work. This is not an exhaustive list of all duties and responsibilities. Cadence Management reserves the right to amend and change responsibilities to meet business and organizational needs as necessary.', '0% - 25%', 'Advanced, working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.Strong analytic skills related to working with unstructured datasets.A successful history of manipulating, processing and extracting value from large disconnected datasets.Strong teamwork and interpersonal skillsExperience in leading process improvement initiativesAbility to motivate high performance, multi-discipline teamsDemonstrated competency in project executionDemonstrated abilities in relationship management', '1 - 3 years of experience in developing, communicating and presenting concepts to varying audiences', 'Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.', 'Conduct regular meetings with internal and external stakeholders to ensure clarification and meet specific requirements in a timely manner.Create and maintain optimal data pipeline architectureAssemble and optimize large, complex data sets that meet functional / non-functional business requirements.Perform root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.Keep our data separated and secure across national boundaries through multiple data centers and regions.Create data tools for analytics and business intelligence team members that assist them in building and optimizing our products into an innovative industry leader.Work with data and analytics experts to strive for greater functionality in our data systems.Gains understanding of customer needs and adapt product strategies to meet their expectationsOther duties as assigned or requested', 'MINIMUM EXPERIENCE', '1 - 3 years of experience manipulating, processing and extracting value from large disconnected datasets.', 'Advanced, working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.', 'Proven ability to quickly learn new applications, processes, and proceduresDemonstrates a meticulous attention to detailDemonstrates the capacity to manage changing priorities and ambiguity while remaining calm and controlledMaintains direction and focus through proactive planning and organized approaches to work resulting in target deadlines that are consistently metCommunicates a ""can do"" attitude and positive outlook, minimizing negative behaviorsProfessional image with ability to form good partner relationships across functionsDemonstrates initiative, resourcefulness, and independence', 'Communicates a ""can do"" attitude and positive outlook, minimizing negative behaviors', 'Position Summary:', 'Keep our data separated and secure across national boundaries through multiple data centers and regions.', 'Gains understanding of customer needs and adapt product strategies to meet their expectations', 'Maintains direction and focus through proactive planning and organized approaches to work resulting in target deadlines that are consistently met', 'Conduct regular meetings with internal and external stakeholders to ensure clarification and meet specific requirements in a timely manner.', 'PREFERRED EXPERIENCE', 'Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.', '3 - 5 years of experience as a Data Engineer or Sr. Data Analyst', 'Demonstrated competency in project execution', 'Demonstrates a meticulous attention to detail', 'Industry organization participation / leadership (ISACA, EDM Council, IAPP, etc.)', 'Proven ability to quickly learn new applications, processes, and procedures', 'Other duties as assigned or requested', 'Create data tools for analytics and business intelligence team members that assist them in building and optimizing our products into an innovative industry leader.', 'EQUIPMENT/SOFTWARE:', 'REQUIRED EDUCATION', '3 - 5 years of experience manipulating, processing and extracting value from large disconnected datasets.', 'Demonstrates initiative, resourcefulness, and independence', 'Perform root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.', '\xa0', ""Bachelor's Degree - Computer Science, Statistics, Informatics, Information Systems or another quantitative field"", 'Strong teamwork and interpersonal skills', 'KNOWLEDGE, SKILLS & ABILITIES', 'Microsoft Office including advanced Microsoft Excel and PowerBI. Experience with Snowflake, Qlik, or Datamart or similar systems. Powershell or Python scripting experience.', 'Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.']",Mid-Senior level,Full-time,Other,Banking,2021-03-18 14:34:51
Data Engineer,vHire Inc,"San Francisco, CA",3 days ago,Be among the first 25 applicants,"['', 'Required Skills']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Analyst Engineer,"FAR Consulting, Inc.",Atlanta Metropolitan Area,,N/A,"['', 'Embed analytics into web applications hosted on the cloud (AWS, GCP, Azure)', ""Bachelor's Degree in Information Systems, Computer Science or Computer Engineering or other related fields"", 'Familiarity with the scripting data and machine learning ecosystems - (Jupyter Notebooks, scikit-learn, SciPy, NumPy, pandas, Matplotlib, TensorFlow, etc.)', 'We are seeking a Data Analyst Engineer to build analysis from the ground up. The ideal candidate will embed analytics into applications and research new solutions to store and present information.', 'Professional certifications in data technologies (BI tools, cloud technologies)', 'Experience developing embedded analytics in software applications', ""Master's degree in Management Information Systems, Analytics or Data Engineering"", 'Develop analytics using power BI, Looker or Tableau', 'Develop data strategy and data analysisDevelop analytics using power BI, Looker or TableauEmbed analytics into web applications hosted on the cloud (AWS, GCP, Azure)Participate in the design, development and implementation of end-to-end Business Intelligence Solutions', 'Familiarity with cloud technologies Data warehouse (BigQuery, RedShift) / cloud storage (GS, S3) / app engine (GCP App Engine, AWS Elastic Beanstalk)', 'Essential Functions', 'Note: This is a 1099 Contract Position (REMOTE)', 'Data Visualization tools: Tableau, Power BI, Looker', 'Develop data strategy and data analysis', 'Experience with SQL via exposure to RDBMS’s such as Amazon Redshift, Google Cloud BigQuery, SQL Server, Oracle, Teradata, postgres, etc.', 'Experience with ""Big Data"" environments', 'Ability to gather and document business requirements and translate them to technical requirements', 'Analytical scripting language skills, such as Python, R, SAS', ""Master's degree in Management Information Systems, Analytics or Data EngineeringProfessional certifications in data technologies (BI tools, cloud technologies)"", 'Basic Qualifications', 'At least 2-5 years of hands-on data experience', 'Participate in the design, development and implementation of end-to-end Business Intelligence Solutions', 'Bachelor\'s Degree in Information Systems, Computer Science or Computer Engineering or other related fieldsAt least 2-5 years of hands-on data experienceExperience developing embedded analytics in software applicationsSolid interpersonal and communication skills (written and verbal) to technical and non-technical audience of wide variety of levels including client-facing senior managementAnalytical scripting language skills, such as Python, R, SASData Visualization tools: Tableau, Power BI, LookerFamiliarity with the scripting data and machine learning ecosystems - (Jupyter Notebooks, scikit-learn, SciPy, NumPy, pandas, Matplotlib, TensorFlow, etc.)Experience with wide array of analytical approaches (correlation analysis, predictive and explanatory modeling, data mining, unsupervised clustering, analysis of unstructured data)Experience with SQL via exposure to RDBMS’s such as Amazon Redshift, Google Cloud BigQuery, SQL Server, Oracle, Teradata, postgres, etc.Familiarity with cloud technologies Data warehouse (BigQuery, RedShift) / cloud storage (GS, S3) / app engine (GCP App Engine, AWS Elastic Beanstalk)Experience with ""Big Data"" environmentsAbility to gather and document business requirements and translate them to technical requirements', 'Experience with wide array of analytical approaches (correlation analysis, predictive and explanatory modeling, data mining, unsupervised clustering, analysis of unstructured data)', 'Solid interpersonal and communication skills (written and verbal) to technical and non-technical audience of wide variety of levels including client-facing senior management', 'Preferred Qualifications', 'As a Data Scientist, you will be part of the technology team responsible for data modeling, application development, technical product assistance and tuning to meet performance and functional requirements. The Data Analyst Engineer will play a key role in building state-of-the-art business intelligence solutions that deliver business insights and support data-driven decision making.']",Entry level,Contract,Information Technology,Management Consulting,2021-03-18 14:34:51
Data Engineer,Neudesic,"Houston, TX",1 week ago,Over 200 applicants,"['Exposure to Azure Synapse a plus', 'Knowledge of object-oriented programming languages and techniques', 'Team\xa0', 'Data warehousing: experience in Microsoft traditional data warehousing and BI\xa0', 'Knowledge Foundation', 'Strong understanding of data structures and algorithms', 'Passionate', 'Experience working with data API’s a plus', 'Our Predictive Enterprise Capability is comprised of some of the most respected and well-known architects as well as brilliant new developers and designers. Together, our teams of professionals have delivered some of the most innovative and leading edge implementations of Business Intelligence, Data Warehousing and Big Data solutions for the business-to-business as well as business-to-consumer space.', 'Experience with database systems (SQL and/or NoSQL)Data warehousing: experience in Microsoft traditional data warehousing and BI\xa0ETL tools: Azure Data Factory and Databricks a plusProgramming skills in Python, R, SQL and/or Scala a plusExperience working with data API’s a plusExposure to Azure Cloud services and big data processing solutions a plusExposure to Azure Synapse a plusExperience with Cloud Security a plus', 'Desire to learn new technologies and languages required.', 'Neudesic is an Equal Employment Opportunity Employer', 'Excellent problem solving and troubleshooting skills.', 'Able to convey information concisely and clearly with great technical documentation skills.', 'Integrity.', 'ETL tools: Azure Data Factory and Databricks a plus', 'Experience in\xa0Database development, Data Modeling, architecture, and storage', 'Programming skills in Python, R, SQL and/or Scala a plus', 'Bachelor’s degree (or higher) in Computer Science or related field with 3+ years’ experience required.', 'Role Profile', 'Please note - this position is located in Houston, Texas.', ""the core,\xa0Innovative\xa0by nature, committed to a\xa0Team\xa0and conduct themselves with\xa0Integrity.\xa0If these attributes mean something to you - we'd like to hear from you."", 'Experience with Cloud Security a plus', 'Required and Preferred Technical and Professional Expertise', 'About Neudesic', 'Passion for technology drives us, but it’s innovation that defines us.\xa0From design to development and support to management, Neudesic offers decades of experience, proven frameworks and a disciplined approach to quickly deliver reliable, quality solutions that help our customers go to market faster.', 'All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.', 'Knowledge of Agile Software Development methodologies and Azure DevOps including Git and GitHub', 'Experience in\xa0Database development, Data Modeling, architecture, and storageStrong understanding of data structures and algorithmsSolid understanding of concurrency and concurrent programming techniquesKnowledge of functional programing languages and techniquesKnowledge of object-oriented programming languages and techniquesKnowledge of Agile Software Development methodologies and Azure DevOps including Git and GitHubData Analysis Knowledge: Understanding how data is collected, analyzed and utilized', 'Exposure to Azure Cloud services and big data processing solutions a plus', 'Knowledge of statistical analysis and machine learning is a plus.', 'Excellent oral and written communication skills with a keen sense of customer service.', '\xa0', 'Solid understanding of concurrency and concurrent programming techniques', 'What sets us apart from the rest, is an amazing collection of people who live and lead with our core values. We believe that everyone should be\xa0Passionate\xa0about what they do,\xa0Disciplined\xa0to', 'Foundation Technical Skills', 'Experience with database systems (SQL and/or NoSQL)', 'Disciplined\xa0', 'Able to work closely and effectively with peer developers and work on several active projects simultaneously.', 'Knowledge of functional programing languages and techniques', 'Data Analysis Knowledge: Understanding how data is collected, analyzed and utilized', 'Innovative', '.\xa0', 'Bachelor’s degree (or higher) in Computer Science or related field with 3+ years’ experience required.Desire to learn new technologies and languages required.Excellent oral and written communication skills with a keen sense of customer service.Able to convey information concisely and clearly with great technical documentation skills.Excellent problem solving and troubleshooting skills.Able to work closely and effectively with peer developers and work on several active projects simultaneously.Knowledge of statistical analysis and machine learning is a plus.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer-Azure,"Vizient, Inc","Irving, TX",1 day ago,Be among the first 25 applicants,"['', ' Play an active role in story breakup and refinement sessions. ', ' Experience with cloud platform technologies such as Microsoft Azure or other PaaS technologies is preferred. ', 'Equal Opportunity Employer: Females/Minorities/Veterans/Individuals with Disabilities', ' Data analysis, data modeling, and data integration using Azure technologies like Azure Data Factory (ADF) is required. ', ' Design, develop, enhance, code, test, deliver and debug software independently.  Implement larger, more complex, or new stories for your product.  Play an active role in story breakup and refinement sessions.  Drive and lead story level architecture/design sessions.  Participate in feature level architecture/design sessions.  Recommend actions to improve procedures and standards.  Stay up to date on technical trends and emerging technology. ', ' Experience working with enterprise solution delivery in a large-scale distributed software design environment is preferred. ', ' Participate in feature level architecture/design sessions. ', ' Experience with PowerBI, Microstrategy, Java, C#, .NET, Typescript, JavaScript and Angular development is preferred. ', ' 2 or more years of experience in a software development or software engineer or data engineer role is required. ', ' Stay up to date on technical trends and emerging technology. ', 'Responsibilities', ' Hands on experience with Azure Synapse, Azure Stream Analytics, Azure Event Hubs, Azure Event Grid, Databricks, ADLS Gen 2, & Logic Apps is strongly preferred. ', ' Strong analytical and conceptual thinking along with database design skills is preferred. ', 'Qualifications', ' Relevant degree preferred.  2 or more years of experience in a software development or software engineer or data engineer role is required.  Data analysis, data modeling, and data integration using Azure technologies like Azure Data Factory (ADF) is required.  Strong aptitude and experience in writing and troubleshooting SQL and T-SQL is preferred.  Experience with Azure SQL and cloud data solutions within a data warehouse environment, using multiple data sources, data lake, etc. is strongly preferred.  Strong analytical and conceptual thinking along with database design skills is preferred.  Hands on experience with Azure Synapse, Azure Stream Analytics, Azure Event Hubs, Azure Event Grid, Databricks, ADLS Gen 2, & Logic Apps is strongly preferred.  Experience working with enterprise solution delivery in a large-scale distributed software design environment is preferred.  Experience working in an Agile based development environment, using Agile concepts such as Continuous Integration, TDD, and Paired Programming is preferred.  Experience with cloud platform technologies such as Microsoft Azure or other PaaS technologies is preferred.  Experience with Octopus deploy, and Jenkins is preferred.  Experience with PowerBI, Microstrategy, Java, C#, .NET, Typescript, JavaScript and Angular development is preferred.  Microsoft Azure certifications in database and business intelligence technologies is preferred. ', 'Summary', ' Experience with Octopus deploy, and Jenkins is preferred. ', ' Design, develop, enhance, code, test, deliver and debug software independently. ', ' Strong aptitude and experience in writing and troubleshooting SQL and T-SQL is preferred. ', ' Implement larger, more complex, or new stories for your product. ', ' Drive and lead story level architecture/design sessions. ', ' Experience working in an Agile based development environment, using Agile concepts such as Continuous Integration, TDD, and Paired Programming is preferred. ', ' Relevant degree preferred. ', ' Recommend actions to improve procedures and standards. ', ' Microsoft Azure certifications in database and business intelligence technologies is preferred. ', ' Experience with Azure SQL and cloud data solutions within a data warehouse environment, using multiple data sources, data lake, etc. is strongly preferred. ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,DataKitchen,Greater Boston,4 weeks ago,Over 200 applicants,"['', 'DataKitchen has a culture of trust and transparency. As a Data Engineer at DataKitchen, you will own and take responsibility for the work you do. We pride ourselves on being collaborative amongst the team and with our customers. We are always asking “Is there a better way we can do this?,” refactoring and building on what we have done as a team.', 'Self-manage and lead client projects', 'Develop and maintain data pipelines utilizing our DataOps software', 'Work in an agile working environment: weekly sprints, daily scrums', 'Solid experience with at least one object-oriented programming language, preferably Python', 'Knowledge of Commercial Pharmaceutical / Life Sciences data sets (NPP, DDD, Xponent, Plantrak, SPP, Symphony, IQVIA)', '5+ years of hands-on data engineering experience', 'DataKitchen has an excellent opportunity for a US based remote Data Engineer or Customer Success professional to be part of an exciting, growing company delivering a cloud-based DataOps Solution! We have several roles available for all levels of experience and leadership.  We are leading the DataOps movement, making it possible for enterprise data teams to turn data into true business value. DataKitchen was honored as 2019 Gartner Cool Vendor and a CRN Big Data “Start-Up to Watch” in 2020. Our company is profitable, rapidly growing and stock will be part of the package.', 'We follow agile development practices while embracing our errors and falling forward. We hire the best and brightest and give everyone the opportunity to contribute to the growth of the company.', 'Desired Tools and Experience:', 'MS in a quantitative field (Computer Science, preferred)', 'Write Amazon Redshift SQL and leverage other AWS products (S3, Lambda, Glacier) and technologies to transform raw data into analytic assets\xa0', 'Day to Day:', 'Develop and maintain data pipelines utilizing our DataOps softwareWrite Amazon Redshift SQL and leverage other AWS products (S3, Lambda, Glacier) and technologies to transform raw data into analytic assets\xa0Self-manage and lead client projectsRun with open-ended requirements to mockup features for clients and then iterate and improve them over timeCommunicate directly with customers; maintain transparency through Jira ticketing and Confluence documentationWork in an agile working environment: weekly sprints, daily scrums', '5+ years of hands-on data engineering experienceMS in a quantitative field (Computer Science, preferred)Advanced knowledge of cloud database design, warehousing (AWS, Snowflake, Google Cloud, etc.)Solid experience with at least one object-oriented programming language, preferably PythonStrength communicating technical details, at both a high level and very detailedKnowledge of Commercial Pharmaceutical / Life Sciences data sets (NPP, DDD, Xponent, Plantrak, SPP, Symphony, IQVIA)', 'Run with open-ended requirements to mockup features for clients and then iterate and improve them over time', 'Strength communicating technical details, at both a high level and very detailed', 'Advanced knowledge of cloud database design, warehousing (AWS, Snowflake, Google Cloud, etc.)', 'Why You Should Work at DataKitchen', 'Communicate directly with customers; maintain transparency through Jira ticketing and Confluence documentation']",Mid-Senior level,Full-time,Engineering,Computer Software,2021-03-18 14:34:51
Data Services Engineer (Healthcare Data- Payment Integrity),ClarisHealth,United States,1 week ago,Be among the first 25 applicants,"['', 'Work with the team to brainstorm new ideas', 'The\xa0Data Services Engineer\xa0is responsible for researching, developing, and implementing analytics solutions for our health care clients. Solutions may be in specific areas such as contractual reimbursement, coordination of benefits, behavioral health, or third-party liability. Solutions may also be general in nature or focused on a system solution. The Data Services Engineer also provides ETL support to our Operations team by in-taking and loading data.', 'Healthcare data knowledge required', 'ClarisHealth is an Equal Opportunity Employer. Anyone needing accommodation to complete the interview process should notify the Director, People & Management.', 'Analyze, interpret and summarize large data sets', 'Learn medical billing terminology', 'Work with C# consoles to make edits for ETL processes', 'Measures of Success', 'Bachelor’s degree or equivalent work experience', 'Bachelor’s degree or equivalent work experienceSQL (T-SQL, MySQL, or Postgres) experienceHealthcare data knowledge requiredExcellent communication and organizational skillsStrong analytical skills', 'EOE including Disability/Veterans/', 'Responsibilities', 'Work with operations to identify areas of focus for data analysisResearch, develop and test queries and data solutionsAnalyze, map and load data to SQL, PostgreSQL, or Mongo databases as part of client implementationAnalyze, interpret and summarize large data setsIdentify new areas of focus for payer cost containmentWork with C# consoles to make edits for ETL processesCreate complex SQL statements to find claims identified for a refund based on specsWork with the team to brainstorm new ideasLearn medical billing terminology', 'Excellent communication and organizational skills', 'Data Services Engineer\xa0', 'ClarisHealth is the answer to the health plan industry’s siloed solutions and traditional models for identification and overpayment recovery services. Founded in 2013, we provide health plans and payers with total visibility into payment integrity operations through our advanced cost containment technology Pareo®. Pareo enables health plans to maximize avoidance and recoveries at the most optimized cost for a 10x return on their software investment. Currently, nearly 33 million lives are served by our total payment integrity platform.', 'Create complex SQL statements to find claims identified for a refund based on specs', 'Research, develop and test queries and data solutions', 'Requirements', 'Analyze, map and load data to SQL, PostgreSQL, or Mongo databases as part of client implementation', 'A successful candidate will have a healthcare technology background, be able to write complex SQL statements, have an aptitude for learning new ways of coding, and a desire to write optimized SQL for faster run times. The candidate should always have a desire to learn something new and be a team player.', 'Identify new areas of focus for payer cost containment', 'The essential functions include, but are not limited to the following:', 'ClarisHealth embraces a working culture of transparency and innovation, termed internally as “Got Your Back”. Our strong Cultures program operates on 5 core values: Innovation, Compassion, Integrity, Communication, and Accountability. For more information on our culture and employment opportunities, please visit us at https://www.clarishealth.com/careers/.', 'Work with operations to identify areas of focus for data analysis', 'SQL (T-SQL, MySQL, or Postgres) experience', 'Strong analytical skills']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
"Data Engineer, Web Solutions Engineer, Business Technology, YouTube",YouTube,"Mountain View, CA",1 week ago,80 applicants,"['', 'Working experience with protocol buffers.', 'About The Job', 'Responsibilities', 'Experience with basic Unix environment operations.', 'Experience working with systems for collaborative code reviews, version control, continuous integration and automated testing.', 'Own and drive complex technical projects from the planning stage through to execution.', 'Design, develop, deploy and maintain scalable solutions for data processing, feed generation using C++ and SQL.', 'Oversee data and data pipeline operations including data center migration, infrastructure migration, resource management, data pipeline configurations, and troubleshooting.', ' SQL, Python, and/or Java experience. Working experience with protocol buffers. Experience with creating system/product designs and leading them to launch. Experience working with systems for collaborative code reviews, version control, continuous integration and automated testing. Basic understanding and passion for massive data processing in parallel in a distributed network computing environment. Passion for data pipeline/workflow design, development and support. ', "" Bachelor's degree in Computer Science or related technical field or equivalent practical experience. 2 years of software development experience in C++. Experience with basic Unix environment operations. Experience in one or more of the following: Relational/non-relational databases, SQL data pipelines, Web application development, backend system design. "", 'Write and review technical documents and review code in compiled or scripted languages.', 'Experience in one or more of the following: Relational/non-relational databases, SQL data pipelines, Web application development, backend system design.', 'Basic understanding and passion for massive data processing in parallel in a distributed network computing environment.', 'SQL, Python, and/or Java experience.', 'Passion for data pipeline/workflow design, development and support.', 'Experience with creating system/product designs and leading them to launch.', ' Design, develop, deploy and maintain scalable solutions for data processing, feed generation using C++ and SQL. Oversee data and data pipeline operations including data center migration, infrastructure migration, resource management, data pipeline configurations, and troubleshooting. Write and review technical documents and review code in compiled or scripted languages. Own and drive complex technical projects from the planning stage through to execution. Collaborate with other Web Solutions Engineers, Product Managers, and Data Analysts on data schema design, and business logic definition. ', ""Bachelor's degree in Computer Science or related technical field or equivalent practical experience."", '2 years of software development experience in C++.', 'Collaborate with other Web Solutions Engineers, Product Managers, and Data Analysts on data schema design, and business logic definition.']",Not Applicable,Full-time,Information Technology,Information Services,2021-03-18 14:34:51
Data Engineer (Remote) - Advanced Technology Group,Cognizant,"Overland Park, KS",2 days ago,Be among the first 25 applicants,"['', 'Work with project management to provide regular status updates to internal and external stakeholders.', 'Key Experience', 'Work collaboratively with a project and client team in an agile environment to support our cloud initiatives', 'Knowledge of Quote to Cash process (CPQ, Order Management, Billing, and CRM) a plus', '4-year degree in a technical or business field of study preferred', 'Willingness and ability to travel up to 50% (subject to change at any time based off project demands)', 'Leverage system and process analysis skills to produce data strategies that create a seamless and uninterrupted experience for our clients.', ' Participate in discovery sessions with project teams to understand data requirements needed to support solution implementations. Work collaboratively with a project and client team in an agile environment to support our cloud initiatives Leverage system and process analysis skills to produce data strategies that create a seamless and uninterrupted experience for our clients. Collaborate with clients to generate data mapping documents between target and source systems. Leverage industry-leading tools to extract, transform and load data into source systems in an automated fashion to support data migrations. Perform data profiling tasks to collect statistics, trends, impacts, and summaries that lead to a structured and successful implementation. Provide key insights based on data analytics to aid the client in drawing conclusions as to the business value and impact of their data and best approaches for migrating and transforming data to support new system architectures. Work with project management to provide regular status updates to internal and external stakeholders. ', ' A minimum of 1 years’ experience in a data related position  Experience with ETL (Extract-Transform-Load) concepts and platforms Experience with at least one database platform such as Microsoft SQL Server, MySQL, Oracle, etc Experience with scripting languages such as JavaScript or Python Experience working in cloud transformation projects within CRM, CPQ, and/or finance systems Experience working in an Agile (Scrum) environment desired ', 'Excellent verbal and written communication skills. ', 'Experience working in an Agile (Scrum) environment desired', 'Experience working in cloud transformation projects within CRM, CPQ, and/or finance systems', 'About Cognizant', 'Experience with scripting languages such as JavaScript or Python', ' 4-year degree in a technical or business field of study preferred Knowledge of Quote to Cash process (CPQ, Order Management, Billing, and CRM) a plus ', 'Position Overview', ' Bachelor’s degree and interest in learning common database tools, ETL products, and working with data in relational databases. Ability to effectively manage time and prioritize tasks Ability to work with ambiguity Capability to take on tasks with little direction and produce high-value deliverables in a fast-paced environment Excellent verbal and written communication skills.  Ability to work with technical and non-technical users, other departments, and clients Organization skills - ability to work in a highly dynamic environment with shifting priorities ', 'Education', 'Participate in discovery sessions with project teams to understand data requirements needed to support solution implementations.', 'Ability to work with technical and non-technical users, other departments, and clients', 'Ability to effectively manage time and prioritize tasks', 'Location and Travel:', 'Collaborate with clients to generate data mapping documents between target and source systems.', 'Desired locations are ATG Delivery Centers, located in Missoula, MT, Cincinnati, OH, Kansas City, MO or St. Louis, MO, however for senior experienced professionals working virtual may be an option.', 'Preferred Knowledge And Skills', 'Key Responsibilities', 'Relevant Technologies', ' Willingness and ability to travel up to 50% (subject to change at any time based off project demands) Desired locations are ATG Delivery Centers, located in Missoula, MT, Cincinnati, OH, Kansas City, MO or St. Louis, MO, however for senior experienced professionals working virtual may be an option. ', 'Experience with at least one database platform such as Microsoft SQL Server, MySQL, Oracle, etc', 'A minimum of 1 years’ experience in a data related position ', 'Perform data profiling tasks to collect statistics, trends, impacts, and summaries that lead to a structured and successful implementation.', 'Provide key insights based on data analytics to aid the client in drawing conclusions as to the business value and impact of their data and best approaches for migrating and transforming data to support new system architectures.', 'Ability to work with ambiguity', 'Leverage industry-leading tools to extract, transform and load data into source systems in an automated fashion to support data migrations.', 'Organization skills - ability to work in a highly dynamic environment with shifting priorities', 'Capability to take on tasks with little direction and produce high-value deliverables in a fast-paced environment', 'Bachelor’s degree and interest in learning common database tools, ETL products, and working with data in relational databases.', 'Experience with ETL (Extract-Transform-Load) concepts and platforms']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Airtable,"San Francisco, CA",1 week ago,55 applicants,"['', 'Who You Are', 'Work closely with the rest of engineering, as well as other stakeholders from our growth, sales, marketing, and product teams, to understand the data needs of the business and produce systems that enable better product and growth decision-making.', 'Ensure that our business-critical data is accurate and correct.', ""What You'll Do"", 'You communicate with clarity and precision in written form; experience communicating with graphs and plots, or at least understanding how to enable other people to do this, is a big bonus.', ""Work on the data collection pipeline across the entire stack, from client-side event logging to ETL. Systems you'll touch may include data warehousing using MPP databases (e.g. RedShift or Vertica), workflow systems (e.g. Airflow or Luigi), streaming data processors (Kinesis, Kafka, etc.), and distributed data processing systems (Spark, Hadoop, etc.)."", 'You may have experience administering modern large-scale data management systems such as ELK.', ""You're passionate and thoughtful about building systems that enhance human understanding."", "" Help architect, build, and scale our initial data engineering platform, with an eye on security and privacy. Work closely with the rest of engineering, as well as other stakeholders from our growth, sales, marketing, and product teams, to understand the data needs of the business and produce systems that enable better product and growth decision-making. Work on the data collection pipeline across the entire stack, from client-side event logging to ETL. Systems you'll touch may include data warehousing using MPP databases (e.g. RedShift or Vertica), workflow systems (e.g. Airflow or Luigi), streaming data processors (Kinesis, Kafka, etc.), and distributed data processing systems (Spark, Hadoop, etc.). Ensure that our business-critical data is accurate and correct. "", "" You're passionate and thoughtful about building systems that enhance human understanding. You have professional experience working with modern data storage and processing technologies, and you've wrangled enough data to understand how often the complex systems that produce data can go wrong. You can write clear, correct code in at least one programming language, and are willing to become effective in others as needed to get your job done. You communicate with clarity and precision in written form; experience communicating with graphs and plots, or at least understanding how to enable other people to do this, is a big bonus. You may have experience administering modern large-scale data management systems such as ELK."", 'You can write clear, correct code in at least one programming language, and are willing to become effective in others as needed to get your job done.', 'Help architect, build, and scale our initial data engineering platform, with an eye on security and privacy.', ""You have professional experience working with modern data storage and processing technologies, and you've wrangled enough data to understand how often the complex systems that produce data can go wrong.""]",Associate,Full-time,Engineering,Computer Software,2021-03-18 14:34:51
Data Engineer,Brace,"New York, NY",1 week ago,52 applicants,"['', 'Bonus Points', ""Where We're Going"", 'JSON', 'REST', '4+ years of professional experience in data engineering, business intelligence or data science', ' Amazon Web Services (AWS) Amazon Glue Amazon Athena Airflow ', 'Experience analyzing a wide variety of data: structured and unstructured to drive system designs and product implementations', 'Proficiency in at least one common data engineering language like Python', 'Familiarity with software development technologies such as:', 'Responsibilities', 'Experience with predictive modeling using statistical techniques and/or machine learning', 'Own setup and rollout of our BI/reporting solution (Looker) including data modeling, metric definitions, and database performance optimization (Postgres)', 'Influence the direction Brace should take in establishing a data warehouse and potential client-facing data products', 'Proficiency in SQL, especially PostgreSQL', 'Comfortable working in a small team, and driving it forward', 'Experience with any of the following technologies:', 'Amazon Web Services (AWS)', 'Airflow', 'Requirements', 'Experience with Amazon Redshift, Snowflake or similar analytics database', 'Experience with modern Java (8+)', ""What We're Doing"", ' 4+ years of professional experience in data engineering, business intelligence or data science Proficiency in SQL, especially PostgreSQL Experience with Amazon Redshift, Snowflake or similar analytics database Proficiency in at least one common data engineering language like Python Comfortable working in a small team, and driving it forward Experience with ETL/ELT tools and the problems and concerns in such systems Experience analyzing a wide variety of data: structured and unstructured to drive system designs and product implementations Good understanding of cloud computing, infrastructure, database scale & performance concepts Familiarity with software development technologies such as:', 'Work cross-functionally with Customer Success, Finance and Product to understand business needs and translate them into scalable data solutions', 'Role', 'Experience with ETL/ELT tools and the problems and concerns in such systems', ' Own setup and rollout of our BI/reporting solution (Looker) including data modeling, metric definitions, and database performance optimization (Postgres) Create and own scalable core data objects and transformations that streamline analytical workflows - including the build out of robust data pipelines and back-end systems. Understand the structure of the database backing our applications and suggest improvements Influence the direction Brace should take in establishing a data warehouse and potential client-facing data products Work cross-functionally with Customer Success, Finance and Product to understand business needs and translate them into scalable data solutions Develop and maintain organized documentation of work ', 'Who We Are', ' Experience with any of the following technologies:', 'Git', 'Amazon Athena', 'Data Engineer', ' Git REST JSON ', 'Create and own scalable core data objects and transformations that streamline analytical workflows - including the build out of robust data pipelines and back-end systems.', 'Develop and maintain organized documentation of work', 'Understand the structure of the database backing our applications and suggest improvements', 'Good understanding of cloud computing, infrastructure, database scale & performance concepts', 'Amazon Glue']",Entry level,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Engineer,Avenue Code,"Brazil, IN",2 weeks ago,Be among the first 25 applicants,"['', 'Trabalhar em time de Scrum global.', 'Trabalhar com as mais novas tecnologias de Big Data', 'Experiência com Azure', 'Flexibilidade de horário;', 'Nice To Have', 'Curso Superior em Ciência da Computação ou afins', ' Fomos premiados como uma das melhores empresas para se trabalhar (prêmio GPTW – 2015, 2016 e 2020); Atuação com modernas tecnologias; Flexibilidade de horário; Desenvolvimento do inglês (língua oficial da empresa) ', 'Fomos premiados como uma das melhores empresas para se trabalhar (prêmio GPTW – 2015, 2016 e 2020);', 'Experiência na criação de testes', ' Curso Superior em Ciência da Computação ou afins Experiência com Python, Spark, SQL, Hadoop e Cloud Inglês avançado ', ' Experiência com Hive Experiência com Linux e Bash scripting Experiência com Azure Experiência com Jenkins e Gradle Experiência na criação de testes ', 'More reasons to be an Avenue Coder?', 'Experiência com Hive', 'Experiência com Linux e Bash scripting', 'Inglês avançado', 'Criar pipelines de dados para viabilizar um sistema de previsão de demanda. ', 'Desenvolvimento do inglês (língua oficial da empresa)', 'Required Qualifications', ' Propor pipelines de dados que atendam às demandas de um time de Cientista de Dados Criar pipelines de dados para viabilizar um sistema de previsão de demanda.  Trabalhar em time de Scrum global. Trabalhar com as mais novas tecnologias de Big Data ', 'Experiência com Python, Spark, SQL, Hadoop e Cloud', 'Atuação com modernas tecnologias;', 'Opportunities', 'Propor pipelines de dados que atendam às demandas de um time de Cientista de Dados', 'Experiência com Jenkins e Gradle']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Nextuple Inc,United States,7 days ago,Over 200 applicants,"['', 'Experience with SQL and NoSQL databases, such as MongoDB, Cassandra, HBase.', 'At least 2-4 years relevant working experience', 'Apply state of the art research techniques to derive insights and spot trends', 'If you possess the skills and are passionate about joining a growing team with phenomenal potential, we would love to chat with you.\xa0\xa0Please send us your resume via careers@nextuple.com', 'Work with multiple stakeholders to facilitate preprocessing, analysis of data.', 'Data mining using state-of-the-art methodsWork with multiple stakeholders to facilitate preprocessing, analysis of data.Apply state of the art research techniques to derive insights and spot trendsUsing machine learning techniques to correlate customer behavior to historical dataDeveloping testing strategiesAd-hoc analysis and presenting results', 'Experience with data visualisation tools', 'Masters or PhD degree in computer science, operations research, statistics, mathematics, or equivalent fields', 'Nextuple is helping retailers deliver perfect post purchase experiences to customers using AI/ML tools that are integrated with our products. We are looking for a Data-ML Engineer who can help us with building data ingestion pipelines, constructing\xa0 ML infrastructure, building research models and developing A/B testing procedures for ML models. This role is a full time opportunity based in the US or Canada.\xa0', 'Responsibilities', 'Job Requirements', 'Solid programming skills in SQL, Java, Python and Scala', 'Ability to consume research papers and convert them into quick proof of concepts\xa0', 'Excellent applied statistics skills, such as distributions, statistical testing, regression, etc.', 'Experience in machine learning libraries such as Tensorflow, Keras, PyTorch and Scikit-learn', 'At least 2-4 years relevant working experienceMasters or PhD degree in computer science, operations research, statistics, mathematics, or equivalent fieldsSolid theoretical foundations and industry experience in large scale data analysis/platforms, machine learning, sales forecasting.Experience in machine learning libraries such as Tensorflow, Keras, PyTorch and Scikit-learnSolid programming skills in SQL, Java, Python and ScalaAbility to consume research papers and convert them into quick proof of concepts\xa0Passionate about technology, demonstrates ability to generate new ideas and innovations; excellent in self-learning, problem analysis and solving; works independently and is proactiveGood interpersonal and communication skills, including the ability to describe the logic and complex models to stakeholders.', 'Good interpersonal and communication skills, including the ability to describe the logic and complex models to stakeholders.', 'Skills and Qualifications', 'Data mining using state-of-the-art methods', 'Experience in large scale data processing, preferably with SparkKnowledge of multi processing and cluster programming techniques.Excellent understanding of common data science toolkitsExperience with data visualisation toolsExperience with SQL and NoSQL databases, such as MongoDB, Cassandra, HBase.Excellent applied statistics skills, such as distributions, statistical testing, regression, etc.', 'Excellent understanding of common data science toolkits', 'Using machine learning techniques to correlate customer behavior to historical data', 'Experience in large scale data processing, preferably with Spark', 'Passionate about technology, demonstrates ability to generate new ideas and innovations; excellent in self-learning, problem analysis and solving; works independently and is proactive', 'Knowledge of multi processing and cluster programming techniques.', 'Solid theoretical foundations and industry experience in large scale data analysis/platforms, machine learning, sales forecasting.', 'Developing testing strategies', 'Ad-hoc analysis and presenting results']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer ,Fluence,"Arlington, VA",2 weeks ago,136 applicants,"['', 'Create and optimize Fluence data pipelines.', 'Engineer and support analytics and machine learning tooling.', 'Working on transforming a fundamental part of our society is exciting and fulfilling. It requires creativity, diversity of ideas and backgrounds, and building trust to effect change and move with speed. We respect our coworkers and customers. We listen to what others have to say, and we are inclusive.', 'Communication skills to explain recommendations from complex data analysis to teammates, colleagues, and leadership.\xa0', 'Experience building and optimizing data pipelines, both with bare metal and cloud services.\xa0', 'engineer and support analytics and machine learning tooling.', 'Increase data visibility and automation.', 'Fluence IS AN EQUAL OPPORTUNITY EMPLOYER and fully subscribes to the principles of Equal Employment Opportunity to ensure that all applicants and employees are considered for hire, promotion, and job status without regard to race, color, religion, sex, national origin, age, disability, veteran status, sexual orientation, marital or familial status.', 'Location: Arlington, VA (preferred) or continental US', 'Our Corporate Culture', 'o\xa0\xa0Batched and real-time data stream-processing systems.\xa0', 'Leading ', 'GET IN TOUCH', 'In this role, you will engineer and support analytics and machine learning tooling. You’ll create data tools for the Fluence team, including business analysts and data scientists, that help them discover insights across our organization. You’ll work with data and analytics experts to create industry leading functionality.\xa0', 'Engineer and support analytics and machine learning tooling. You’ll create data tools for the Fluence team, including business analysts and data scientists, that help them discover insights across our organization. You’ll work with data and analytics experts to create industry leading functionality.\xa0', 'o\xa0\xa0Relational and non-relational databases, including expertise in Postgres, MariaDB, Snowflake, or Presto/Athena.\xa0', 'Fluence is defined by its unwavering commitment to safety, quality, and integrity. We take personal ownership in what we do, and we prioritize the development of trust in our relationships. We firmly believe in having honest, forthcoming, respectful, and fair communications. \xa0We are curious, adaptable, and self-critical; we use these traits to meet our team’s and customers’ needs. We always want to be working on the issues with the greatest impact.\xa0Succeeding in our mission requires creativity of thought, diversity of backgrounds, and an environment of safety and trust. These aspects of our culture enable us to effect change and move with speed. Workspace respect, balance, and inclusivity is foundational to our success, and our experiences at work are an essential part of what we create.\xa0', 'Advanced working knowledge of SQL and experience working with relational databases, query authoring, optimization and familiarity with a variety of databases.\xa0', 'Fluence, a Siemens and AES company, is the global market leader in energy storage technology solutions and services, combining the agility of a technology company with the expertise, vision and financial backing of two well-established and respected industry giants. Building on the pioneering work of AES Energy Storage and Siemens energy storage, our goal is to create a more sustainable future by transforming the way we power our world. Providing design, delivery and integration, Fluence offers proven energy storage technology solutions that address the diverse needs and challenges of customers in a rapidly transforming energy landscape. ', 'Fluence currently has more than 2.4 gigawatts of projects in operation or awarded across 24 countries and territories worldwide. We topped the Navigant Research utility-scale energy storage leaderboard in 2018 and were named one of Fast Company’s Most Innovative Companies in 2019. In 2020, our sixth-generation Tech Stack won Commercial Technology of the Year at the 22nd annual S&P Global Platts Global Energy Awards.', 'Fun', 'Here at Fluence, we strive to continuously improve, be intellectually curious and be adaptive to our customers and employee’s needs. Collaboration is key, both in our partnerships with our customers, and with each other. Fluence prioritizes the most critical efforts that allow for the greatest impact. ', 'Advanced working knowledge of SQL and experience working with relational databases, query authoring, optimization and familiarity with a variety of databases.\xa0Experience building and optimizing data pipelines, both with bare metal and cloud services.\xa0Knowledge needed manage and process large amounts of time series data, including real-time processing, analytics, machine learning data set creation, and retrieval systems for archived data.\xa0Awareness of modern clustering, containerization, services, and serverless architectures.\xa0Cooperation skills to work collaboratively in a fast-paced entrepreneurial environment, while also taking ownership of responsibilities and pursuing them diligently.\xa0Communication skills to explain recommendations from complex data analysis to teammates, colleagues, and leadership.\xa0Belief that the work we do must always improve lives by lowering the cost of electricity, improving the resilience of the electric system, and enhancing grid sustainability.\xa0Knowledge of basic Linux system administration concepts—working with the command line and operating system resource management.\xa0A degree in Computer Science, Information Systems or another quantitative field with relevant experience.\xa0You should also have experience with the following technology:\xa0', 'Create and optimize Fluence data pipelines. This involves assembling large, complex data sets that meet our customers’ diverse and evolving requirements. You’ll create and maintain the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of sources using SQL and other query languages. You’ll configure data collection and aggregation pipelines in both bare metal Linux and AWS environments and design procedures for maintaining these pipelines.\xa0', 'Knowledge needed manage and process large amounts of time series data, including real-time processing, analytics, machine learning data set creation, and retrieval systems for archived data.\xa0', 'Awareness of modern clustering, containerization, services, and serverless architectures.\xa0', 'A degree in Computer Science, Information Systems or another quantitative field with relevant experience.\xa0', 'Responsible', 'Knowledge of basic Linux system administration concepts—working with the command line and operating system resource management.\xa0', 'Do others come to you for your subject matter expertise? Are you excited by the challenge of working in a start-up atmosphere with a purpose? ', 'Please send your resume and cover letter to careers@fluenceenergy.com.', 'Cooperation skills to work collaboratively in a fast-paced entrepreneurial environment, while also taking ownership of responsibilities and pursuing them diligently.\xa0', 'Succeeding in our mission requires creativity of thought, diversity of backgrounds, and an environment of safety and trust. These aspects of our culture enable us to effect change and move with speed. Workspace respect, balance, and inclusivity is foundational to our success, and our experiences at work are an essential part of what we create.\xa0', 'Belief that the work we do must always improve lives by lowering the cost of electricity, improving the resilience of the electric system, and enhancing grid sustainability.\xa0', 'Agile', 'o\xa0\xa0Standard industry data formats such as CSV, Parquet, Avro, etc.\xa0', 'Fluence is defined by its unwavering commitment to safety, quality, and integrity. We take personal ownership in what we do, and we prioritize the development of trust in our relationships. We firmly believe in having honest, forthcoming, respectful, and fair communications. \xa0', 'Increase data visibility and automation. You’ll identify, design, and implement internal process improvements: enhance data access and visibility, automate manual processes, optimize data delivery and design for scalability.\xa0', 'Fluence is seeking a Data Engineer to help implement and monitor large-scale data systems that will help create more sustainable global energy systems.', 'Data Engineer', 'As an ideal candidate you have:\xa0\xa0', 'You should also have experience with the following technology:\xa0', 'ABOUT FLUENCE', 'In this role, you will engineer and support analytics and machine learning tooling. You’ll create data tools for the Fluence team, including business analysts and data scientists, that help them discover insights across our organization. You’ll work with data and analytics experts to create industry leading functionality.\xa0Create and optimize Fluence data pipelines. This involves assembling large, complex data sets that meet our customers’ diverse and evolving requirements. You’ll create and maintain the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of sources using SQL and other query languages. You’ll configure data collection and aggregation pipelines in both bare metal Linux and AWS environments and design procedures for maintaining these pipelines.\xa0Increase data visibility and automation. You’ll identify, design, and implement internal process improvements: enhance data access and visibility, automate manual processes, optimize data delivery and design for scalability.\xa0Engineer and support analytics and machine learning tooling. You’ll create data tools for the Fluence team, including business analysts and data scientists, that help them discover insights across our organization. You’ll work with data and analytics experts to create industry leading functionality.\xa0', 'o\xa0\xa0Automation and scripting using Python (required).\xa0', 'We are curious, adaptable, and self-critical; we use these traits to meet our team’s and customers’ needs. We always want to be working on the issues with the greatest impact.\xa0', 'o\xa0\xa0AWS cloud services such as EC2, S3, Lambda, and Fargate.\xa0']",Mid-Senior level,Full-time,Information Technology,Renewables & Environment,2021-03-18 14:34:51
Data Engineer (Python),TalentPartners,United States,,N/A,"[' Experience with Apache Airflow or Google Composer', ' Experience with Atlassian products Jira and Confluence', ' Bachelor’s Degree in Computer Science or a related discipline', ' ', ' Detail-oriented and document all the work', ' Nice to have:', 'Nice to have:', 'Experience with version control systems (Git and Bitbucket)', ' knowledge of Application Programming Interfaces', ' Strong proficiency in Python\xa0with an emphasis in building data pipelines', ' Ability to work with others from diverse skill-sets and backgrounds', ' 5+ years of applicable engineering experience', ' Ability to write complex SQL to perform common types of analysis and aggregations\xa0', 'Required Qualifications:', ' Experience with Docker containerization']",Mid-Senior level,Full-time,Information Technology,Entertainment,2021-03-18 14:34:51
Data Engineer,Jerry.ai,"Boston, MA",3 weeks ago,96 applicants,"['', '2+ years of data engineering experience within a rigorous engineering environment', 'Rockstar teammates. You will be working with a strong team with prior work experience at Amazon, Microsoft, NVIDIA, Alibaba, etc.', 'Consistently evolve data model & data schema based on business and engineering needs', 'Locations ', 'Experience with BI software (preferably Metabase or Tableau).', 'Experience with deploying and maintaining data infrastructure in the cloud (experience with AWS preferred).', ' Owner of the core company data pipeline, responsible for scaling up data processing flow to meet the rapid data growth Consistently evolve data model & data schema based on business and engineering needs Implement systems tracking data quality and consistency Develop tools supporting self-service data pipeline management (ETL) SQL and MapReduce job tuning to improve data processing performance ', 'Responsibilities', 'Owner of the core company data pipeline, responsible for scaling up data processing flow to meet the rapid data growth', 'Proficient in SQL, specially with Postgres dialect.', 'Meritocracy - we promote based on performance, not tenure', 'Expertise in Python for developing and maintaining data pipeline code.', 'Experience with Apache Spark and PySpark library (experience with AWS extension of PySpark is a plus).', 'Toronto', 'About The Role', 'Requirements', 'Boston', 'About Jerry.ai', 'Palo Alto', 'Experience with Hadoop (or similar) Ecosystem.', ' 2+ years of data engineering experience within a rigorous engineering environment Proficient in SQL, specially with Postgres dialect. Expertise in Python for developing and maintaining data pipeline code. Experience with Apache Spark and PySpark library (experience with AWS extension of PySpark is a plus). Experience with BI software (preferably Metabase or Tableau). Experience with Hadoop (or similar) Ecosystem. Experience with deploying and maintaining data infrastructure in the cloud (experience with AWS preferred). Comfortable working directly with data analytics to bridge business requirements with data engineering ', 'Start-up energy working with a brilliant and passionate team', ' Toronto Boston Palo Alto', 'We’d love to hear from you if you like ', 'Comfortable working directly with data analytics to bridge business requirements with data engineering', 'Exponential growth (5 straight quarters of 50-100%+ quarter over quarter growth)', 'SQL and MapReduce job tuning to improve data processing performance', ' Start-up energy working with a brilliant and passionate team Exponential growth (5 straight quarters of 50-100%+ quarter over quarter growth) Flat structure and access to senior leadership for continuous mentorship Meritocracy - we promote based on performance, not tenure Rockstar teammates. You will be working with a strong team with prior work experience at Amazon, Microsoft, NVIDIA, Alibaba, etc. ', 'Flat structure and access to senior leadership for continuous mentorship', 'Implement systems tracking data quality and consistency', 'Develop tools supporting self-service data pipeline management (ETL)', 'personal concierge for your car and home']",Associate,Full-time,Engineering,Marketing and Advertising,2021-03-18 14:34:51
"Data Engineer, Experimentation",Dow Jones,"New York, NY",2 days ago,Be among the first 25 applicants,"['', 'Skills', 'Strategic expertise in creating and maintaining data architecture', 'Assists in necessary software investigation, analysis and evaluation to determine solution feasibility.', 'Develops their productivity skills by learning source control, editors, the build system, and other tools as well as testing best practices', ' Assists in necessary software investigation, analysis and evaluation to determine solution feasibility.Develops their productivity skills by learning source control, editors, the build system, and other tools as well as testing best practicesDevelops knowledge of a single component of our architectureDevelops module specifications and supports data design.Participates in business analysis, systems analysis/consulting, and systems design.Makes use of application development standards for designing, building and maintaining applications, applications components, and common services including the use of standard languages and tools.', 'Responsibilities', 'Participates in business analysis, systems analysis/consulting, and systems design.', 'Current Stack: Python, AWS Redshift, AWS S3, CloudWatch, EMR Spark, Airflow, Okta', 'Develops knowledge of a single component of our architecture', 'Makes use of application development standards for designing, building and maintaining applications, applications components, and common services including the use of standard languages and tools.', 'Job Description', 'About Us', 'Ability to work with numerous technologies across a technology stack.', 'Develops module specifications and supports data design.', ' Ability to work with numerous technologies across a technology stack.Current Stack: Python, AWS Redshift, AWS S3, CloudWatch, EMR Spark, Airflow, OktaStrategic expertise in creating and maintaining data architecture']",Not Applicable,Full-time,Information Technology,Online Media,2021-03-18 14:34:51
Data Engineer,East West Bank,"Pasadena, CA",3 weeks ago,196 applicants,"['', '·\xa0\xa0\xa0\xa0\xa0\xa0Can-do attitude, self-motivated and strong work ethic.', 'o\xa0\xa0Banking operations', '·\xa0\xa0\xa0\xa0\xa0\xa0Strong computing skills with at least one of the following: C#, Java, Python, R, T-SQL with a strong interest to learn.', '·\xa0\xa0\xa0\xa0\xa0\xa0Able to work under pressure while managing competing demands and tight deadlines.', '·\xa0\xa0\xa0\xa0\xa0\xa0Maintain an understanding of business operations, operational risks and regulatory requirements ', '·\xa0\xa0\xa0\xa0\xa0\xa0Collaborate with team members and business units to Deliver Data-Oriented solutions to the enterprise.', 'o\xa0\xa0Loans', 'o\xa0\xa0Deposits', '·\xa0\xa0\xa0\xa0\xa0\xa0Create reports, processes and tools to monitor key risk indicators for business units across the organization.', '·\xa0\xa0\xa0\xa0\xa0\xa0Translate, cleanse and normalize large datasets.', '·\xa0\xa0\xa0\xa0\xa0\xa0Define new data collection and analysis processes', '·\xa0\xa0\xa0\xa0\xa0\xa0Bachelor’s degree required in Computer Science, Information Technology, Management Information Systems or Business Management.', '·\xa0\xa0\xa0\xa0\xa0\xa0Design, implement, deploy and maintain Data solutions.\xa0These solutions are written in T-SQL, Python, C#, Java and R.', 'Responsibilities', '·\xa0\xa0\xa0\xa0\xa0\xa0Extract and collect large data sets from various sources and formats', '·\xa0\xa0\xa0\xa0\xa0\xa0Interpret and analyze results from data extractions to identifying patterns and trends', '·\xa0\xa0\xa0\xa0\xa0\xa0Self-driven to identify areas of improvement.', '·\xa0\xa0\xa0\xa0\xa0\xa0Analytical and problem solving skills including troubleshooting.', '·\xa0\xa0\xa0\xa0\xa0\xa0Test and validate that key assumptions, data sources, and procedures utilized in measuring and monitoring risk and internal controls can be relied upon on an ongoing basis; and, in the case of transaction testing, to assess that controls are working as intended.', 'Qualifications', '·\xa0\xa0\xa0\xa0\xa0\xa0Quantitative, analytical, process oriented and troubleshooting skills', '·\xa0\xa0\xa0\xa0\xa0\xa0Assuring the integrity of data, including data extraction, storage, manipulation, processing and analysis', '·\xa0\xa0\xa0\xa0\xa0\xa0Build reports in SSRS, Power BI, Tableau, Excel and other visualization tools.', '·\xa0\xa0\xa0\xa0\xa0\xa0Evaluate internal controls and identify deficiencies through the testing of data source, systems, and processes', 'East West Bank is seeking a Data Engineer.\xa0The Data Engineer works with banking data and business units to propose, design, and deliver business solutions. The data engineer will collect and analyze data wherever it resides, including in spreadsheets, files, databases, and APIs. The data engineer will create reports, dashboards, and other visualizations and they will develop applications that collect data from users to enrich enterprise data sets to improve their completeness and accuracy.\xa0\xa0', '·\xa0\xa0\xa0\xa0\xa0\xa0Collect and document business requirements.\xa0Maintain functional and technical artifacts including design documents, data mappings, architecture, data models, and dictionaries.', '·\xa0\xa0\xa0\xa0\xa0\xa0Report results back to management and relevant team members', '·\xa0\xa0\xa0\xa0\xa0\xa0Build high-performance algorithms, predictive models, and prototypes.', '·\xa0\xa0\xa0\xa0\xa0\xa0Product knowledge in ', '·\xa0\xa0\xa0\xa0\xa0\xa0Well organized with meticulous attention to detail.', '·\xa0\xa0\xa0\xa0\xa0\xa0Anticipate changes in the internal and external environment and adapt the testing program accordingly ', '·\xa0\xa0\xa0\xa0\xa0\xa0Must be team-oriented with experience working on interdepartmental team projects.', '·\xa0\xa0\xa0\xa0\xa0\xa0Provides recommendations to streamline tasks and create a more efficient working environment', '·\xa0\xa0\xa0\xa0\xa0\xa0Master’s degree in Mathematics, Statistics, Computer Science, Data Science or relevant.', '·\xa0\xa0\xa0\xa0\xa0\xa0Proficiency with Excel.', '·\xa0\xa0\xa0\xa0\xa0\xa0Test adherence with Bank’s policies and controls, as well as regulatory requirements']",Associate,Full-time,Information Technology,Banking,2021-03-18 14:34:51
Data Engineer (Remote),KAR Global,United States,6 days ago,Be among the first 25 applicants,"['', 'This candidate should be a self-starter who is interested in learning new systems/environments and passionate about developing quality supportable data service solutions for internal and external customers. We highly value natural curiosity about data and technology that drives results through quality, repeatable, and long-term sustainable database and code development. The candidate should be highly dynamic and excited by opportunities to learn many different products and data domains and how they drive business outcomes and value for our customers.', 'Experience with Informatica (preferred)', 'Collaborates with source systems data stewards, Data owners and technical personnel for data governance and resolves any data quality or technical issues related to data ingestion.', 'Experience with Python, Docker, and data warehouse environments (preferred).', 'We’re a technology company\xa0delivering next generation tools to accelerate and simplify remarketing.', 'Experience with Kinesis/Kafka (preferred).', 'We’re an analytics company\xa0leveraging data to inform and empower our customers with clear, actionable insights.', 'Experience with Informatica MDM Hub configurations - Data modeling & Data Mappings (Landing, staging and Base Objects), Data validation, Match and Merge rules, Active VOS, SIF Framework, and MDM User Exits (preferred)', 'What You Will Be Doing: ', 'Design, build, and delivery of enterprise data services solutions for MDM on the DaaS Platform.', 'About Our Team: ', 'Experience working with master data management in automotive or business to business customer domains (preferred)', 'KAR Global powers the world’s most trusted automotive marketplaces through innovation, technology and people. Our end-to-end platform serves the remarketing needs of the world’s largest OEMs, dealers, fleet operators, rental companies and financial institutions.', 'Contribute to the Data Engineering and MDM team delivery framework including building of re-usable code, implementing industry best practices, and maintain a common delivery framework.', 'Work with product, data science, analytics, and engineering teams to learn project data needs and define project scope.', 'And we’re an auction company\xa0powering the world’s most advanced and integrated mobile, digital and physical auction marketplaces.', 'Monitoring, maintenance, documentation, and incident resolution of scheduled production data jobs supporting internal and external customers data needs.', 'Experience with Postgres, Elastic Search, AWS EMR, and AWS ECS (preferred).', 'Experience planning and designing maintainable data schemas (required).', 'Bachelor’s degree in Business, Computer Science, Management Information Systems or equvalent (required).5+ years experience Postgres or Oracle SQL development including functions, stored procedures, and indexing or equivalent (required).Experience in production data management in high availability product delivery ODS / RDBMS or equivalent (required).Experience planning and designing maintainable data schemas (required).Experience with Python, Docker, and data warehouse environments (preferred).Experience using Github / Jenkins (CI/CD) / Artifactory / PyPy or comparable delivery stacks (preferred).Experience with Postgres, Elastic Search, AWS EMR, and AWS ECS (preferred).Experience with AWS Redshift, MPP, or Dynamo DB (preferred).Experience with Informatica (preferred)Experience with Kinesis/Kafka (preferred).Experience working with large enterprise data lakes / Snowflake (preferred).Experience with Informatica MDM Hub configurations - Data modeling & Data Mappings (Landing, staging and Base Objects), Data validation, Match and Merge rules, Active VOS, SIF Framework, and MDM User Exits (preferred)Experience working with master data management in automotive or business to business customer domains (preferred)', 'Experience in production data management in high availability product delivery ODS / RDBMS or equivalent (required).', 'As part of a small, passionate and accomplished team of experts, you will work the full spectrum of Master Data Management (MDM). This is a high-impact, high-visibility team and is responsible for ensuring all master data is accurate, complete, and consistent across the entire enterprise. This team supports current new product development projects and performs ongoing guidance throughout the master data lifecycle.', 'KAR Global Data-as-a-Service team (DAAS) is looking to expand our data team as we continue to grow our data platform in support of a mission of digital transformation in automotive wholesale markets.\xa0 The data engineering team is responsible for the ingestion and persistence of data supporting an array of data products supporting KAR Global’s automotive wholesale business.', 'Location', 'Responsibilities include:', 'Members of the data engineering team participate daily in ceremonies of Agile sprint to help design, plan, build, test, develop, and support KAR Global MDM data products and platforms consisting of Python ETL pipelines, Informatica applications, Postgres, Redshift, Dynamo DB, Oracle, and Snowflake databases. Our team works in a shared services delivery model supporting seven lines of business including front-end customer-facing products, B2B portals, mobile applications, business analytics, and data science initiatives.', 'Bachelor’s degree in Business, Computer Science, Management Information Systems or equvalent (required).', 'We’re a technology company\xa0delivering next generation tools to accelerate and simplify remarketing.We’re an analytics company\xa0leveraging data to inform and empower our customers with clear, actionable insights.And we’re an auction company\xa0powering the world’s most advanced and integrated mobile, digital and physical auction marketplaces.', 'Experience using Github / Jenkins (CI/CD) / Artifactory / PyPy or comparable delivery stacks (preferred).', 'Location - Remote, North America', 'About Our Candidate: ', 'Building and delivery of Python/Docker feed framework data pipeline jobs and services.', 'Who We Are:', 'What You Need to Be Successful: ', 'Work with product, data science, analytics, and engineering teams to learn project data needs and define project scope.Design, build, and delivery of enterprise data services solutions for MDM on the DaaS Platform.Building and delivery of Python/Docker feed framework data pipeline jobs and services.Contribute to the Data Engineering and MDM team delivery framework including building of re-usable code, implementing industry best practices, and maintain a common delivery framework.Monitoring, maintenance, documentation, and incident resolution of scheduled production data jobs supporting internal and external customers data needs.Collaborates with source systems data stewards, Data owners and technical personnel for data governance and resolves any data quality or technical issues related to data ingestion.', 'Experience with AWS Redshift, MPP, or Dynamo DB (preferred).', 'Experience working with large enterprise data lakes / Snowflake (preferred).', '5+ years experience Postgres or Oracle SQL development including functions, stored procedures, and indexing or equivalent (required).', 'The DRIVIN, a KAR Global brand, is comprised of a team that is passionate about the intersection of data, technology and cars. As the Chicago-based innovation and data science hub for KAR, this team of data scientists and analysts creates new products to help power the company’s physical, online, and digital/mobile automotive auction marketplaces. DRIVIN was founded in 2015 and joined the KAR family of companies in April 2017. Since then, DRIVIN has expanded its capabilities across the organization, shifting from a transactional marketplace to a full-service data and analytics platform. The DRIVIN data engine is unmatched, fueling powerful insights and recommendations that help KAR’s customers optimize risk, price and automotive inventory.']",Mid-Senior level,Full-time,Information Technology,Information Services,2021-03-18 14:34:51
Data Engineer,ServiceNow,"Santa Clara, CA",1 week ago,159 applicants,"['', 'You will provide insights and deep analysis being sought by users/business stakeholdersWork with Cross-Functional Analytics team members to curate and assimilate insightsGrow into being SME on business functionsGather business requirements from stakeholders on various analytics initiativesAnalyze requirements, determine optimal solutions and determine gap from the current state, dependencies and ways to mitigate risksDevelop business requirements documentation, process workflow diagrams, functional specifications, user acceptance test scripts and other supporting documentation for Business Intelligence and Analytics initiativesAssist stakeholders with data analysis, design data models & develop DB Views, procs, models in SAP HANA to meet the business needDevelop dashboard and report prototypes and mockups with respect to the UX/UI Best Practices and have impactful UI DesignCommunicate status regularly with stakeholdersDefine required data integration requirements between various systems and work with extended team to get them createdCollaborate with India Development Center BI team to translate business requirements and get appropriate data solutions developed to meet the business needPartner with Global BI team to help implement solutions for end-user adoption', 'Develop dashboard and report prototypes and mockups with respect to the UX/UI Best Practices and have impactful UI Design', 'Gather business requirements from stakeholders on various analytics initiatives', 'Partner with Global BI team to help implement solutions for end-user adoption', 'Assist stakeholders with data analysis, design data models & develop DB Views, procs, models in SAP HANA to meet the business need', 'Grow into being SME on business functions', 'Bachelor’s Degree in Information System, Analytics, Business Intelligence or related field required', 'Capable of using Microsoft Project, Excel, Word, PowerPoint, and Visio or similar products', 'Work with Cross-Functional Analytics team members to curate and assimilate insights', 'Must have good communication, presentation, and documentation skills', 'Define required data integration requirements between various systems and work with extended team to get them created', 'What You Get To Do In This Role', 'Analyze requirements, determine optimal solutions and determine gap from the current state, dependencies and ways to mitigate risks', 'Expertise in database design & development, writing optimized queries, handling Facts, dimension data effectively', 'You will provide insights and deep analysis being sought by users/business stakeholders', 'Working knowledge Tableau, Power BI is a plus', 'Ability to analyze data coming from myriad data sources, mine and analyze and derive value from it to improve business SQL and other computer programs (Python, R is preferred)', 'Job Title: Data EngineerLocation: Santa Clara, CA', 'Company', 'Communicate status regularly with stakeholders', 'Business process design, project management, and/or Agile SDLC experience a plus', 'Strong analytical and problem-solving ability and be able to dive into technical details and design analytics solutions', 'Ability to visualize the results in the previous step by putting together simple and easily consumable dashboards using reporting tools', '3-5 years of documented experience in writing strong SQL, PLSQL in data warehouse technologies (Hana, Snowflake, or any modern database).', 'Collaborate with India Development Center BI team to translate business requirements and get appropriate data solutions developed to meet the business need', 'Develop business requirements documentation, process workflow diagrams, functional specifications, user acceptance test scripts and other supporting documentation for Business Intelligence and Analytics initiatives', 'Bachelor’s Degree in Information System, Analytics, Business Intelligence or related field required3-5 years of documented experience in writing strong SQL, PLSQL in data warehouse technologies (Hana, Snowflake, or any modern database).Ability to analyze data coming from myriad data sources, mine and analyze and derive value from it to improve business SQL and other computer programs (Python, R is preferred)Ability to visualize the results in the previous step by putting together simple and easily consumable dashboards using reporting toolsWorking knowledge Tableau, Power BI is a plusStrong analytical and problem-solving ability and be able to dive into technical details and design analytics solutionsExpertise in database design & development, writing optimized queries, handling Facts, dimension data effectivelyMust have good communication, presentation, and documentation skillsCapable of using Microsoft Project, Excel, Word, PowerPoint, and Visio or similar productsBusiness process design, project management, and/or Agile SDLC experience a plus']",Entry level,Full-time,Project Management,Information Technology and Services,2021-03-18 14:34:51
"Data Engineer, Marketing",Apollo GraphQL,United States,3 weeks ago,43 applicants,"['', ""You'll build a data platform that will consist of data from critical systems, product usage data, and external data sourcesYou’ll create data pipelines from systems like Salesforce CRM, Marketo, and Google Analytics to be stored in our marketing data warehouse with an emphasis on scalability and efficiencyYou'll leverage a Customer Data Platform to create connected data processes from multiple data providers and enrichment services to create complete and informative records in our CRMYou'll deliver insights from models like propensity to buy, churn, or next best offer to drive business value and support customer experience use casesYou'll own and manage our company identity graph"", 'Bonus if you have experience in a software engineering role or are comfortable coding', 'California residents applying for positions at Apollo can see our privacy policy\xa0here', ""What you'll do"", 'You’ll create data pipelines from systems like Salesforce CRM, Marketo, and Google Analytics to be stored in our marketing data warehouse with an emphasis on scalability and efficiency', ""You're passionate about data quality!"", ""If you’re a builder and want to work for a company on an incredibly exciting trajectory, we'd love to hear from you."", 'It’s not just our technology that we pride ourselves upon, it’s also our company culture of kindness and transparency. Check out our Glassdoor page if want to learn more about how we work together.', 'Apollo is proud to be an equal opportunity workplace dedicated to pursuing and hiring a talented and diverse workforce.', 'You are an architect and a builder and love helping to bring the marketing data platform vision to life', ""You'll own and manage our company identity graph"", ""You're able to collaborate successfully with a diverse set of stakeholders (from technical engineers to less technical sales and marketing stakeholders) to connect your work to business value"", 'About you', ""You're passionate about data quality!You are an architect and a builder and love helping to bring the marketing data platform vision to lifeYou're able to collaborate successfully with a diverse set of stakeholders (from technical engineers to less technical sales and marketing stakeholders) to connect your work to business valueYou're energized when you're able to help people make the most of their dataYou've got experience working in a cloud platform like GCPYou value a positive company culture and something you’ll help us grow and preserveBonus if you have experience in a software engineering role or are comfortable coding"", 'You value a positive company culture and something you’ll help us grow and preserve', 'here', 'Other Info', ""You're energized when you're able to help people make the most of their data"", ""You'll build a data platform that will consist of data from critical systems, product usage data, and external data sources"", ""A little about us - we're Apollo, an Andreessen Horowitz funded company in hyper growth mode. We’re the data graph company building the next generation of technology that will replace APIs."", ""You'll deliver insights from models like propensity to buy, churn, or next best offer to drive business value and support customer experience use cases"", 'You can do this work from anywhere workable within North American time zones.', ""This is a rare opportunity to pioneer and build and scale a marketing data platform that has wide reaching impact for our company and our customers. You'll help craft a data warehouse strategy as well as map architecture for connecting data within Apollo’s CDP."", ""You'll leverage a Customer Data Platform to create connected data processes from multiple data providers and enrichment services to create complete and informative records in our CRM"", ""You've got experience working in a cloud platform like GCP""]",Mid-Senior level,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Engineer,Skillz Inc.,"Denver, CO",2 days ago,36 applicants,"['', 'Familiarity with Kinesis, Lamda', 'Building data integration toolkit for backend services', 'Who We’re Looking For', 'Build new systems to provide real-time streaming analytics and event processing pipeline based on fast data architecture', 'Familiarity with Alooma, Snowflakes', ' Familiarity with Agile engineering practices 1+ years experience on Kubernete, Helm chart 1+ years of experience with Spark, Scala and/or Akka 1+ years of experience with Spark Streaming, Storm, Flink, or other Stream Processing technologies 1+ years of experience working with Kafka or similar data pipeline backbone 1+ years of experience with Unix/Linux systems with scripting experience in Shell, Perl or Python 1+ years’ experience with NoSQL implementation (ElasticSearch, Cassandra, etc. a plus) At least 1 year of experience with Unix/Linux systems with scripting experience Familiarity with Alooma, Snowflakes Familiarity with Kinesis, Lamda Prior experience in gaming Prior experience in finance ', '1+ years of experience with Spark Streaming, Storm, Flink, or other Stream Processing technologies', '1+ years experience on Kubernete, Helm chart', 'What You’ll Do', 'Bonus', 'Build enterprise grade data lake to support both business analytical needs and next generation data infrastructure', 'Find better ways to move massive amounts of data from a variety of sources to formats consumable by reporting systems and people', '1+ years of experience with Unix/Linux systems with scripting experience in Shell, Perl or Python', ' At least 1+ years of experience in Scala/Java or Python programming AWS data products (Data pipelines, Athena, Pinpoint, S3, etc) Experience deploying data infrastructure Experience with recognized industry patterns, methodologies, and techniques ', 'At least 1+ years of experience in Scala/Java or Python programming', '1+ years of experience working with Kafka or similar data pipeline backbone', 'Prior experience in finance', '1+ years’ experience with NoSQL implementation (ElasticSearch, Cassandra, etc. a plus)', 'Familiarity with Agile engineering practices', '1+ years of experience with Spark, Scala and/or Akka', 'About Skillz', 'Prior experience in gaming', 'Improve monitoring and alarms that impact data integrity replication lag', 'Support our data science team in deploying new algorithms for matchmaking, fraud and cheat detection', 'AWS data products (Data pipelines, Athena, Pinpoint, S3, etc)', 'Basic Qualifications', 'Experience with recognized industry patterns, methodologies, and techniques', 'Support our product development team in creating new events to measure/track', 'At least 1 year of experience with Unix/Linux systems with scripting experience', 'Experience deploying data infrastructure', 'Your Skillz:', ' Build new systems to provide real-time streaming analytics and event processing pipeline based on fast data architecture Build enterprise grade data lake to support both business analytical needs and next generation data infrastructure Building data integration toolkit for backend services Support our data science team in deploying new algorithms for matchmaking, fraud and cheat detection Find better ways to move massive amounts of data from a variety of sources to formats consumable by reporting systems and people Improve monitoring and alarms that impact data integrity replication lag Support our product development team in creating new events to measure/track ']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Data Engineer/Developer,JPI,"Washington, DC",3 days ago,49 applicants,"['', 'Support continuous process automation for data ingest', '\u200bWork closely with software engineers and architects to extract, transform, and standardize data to prepare for ingest into target sourcesDesign and develop data services and/or pipelines as part of an Agile/Scrum teamSupport continuous process automation for data ingestWork with program management and engineers to implement and document complex and evolving requirementsPerform multiple tasks simultaneously and successful perform under changing requirements and deadlinesHelp cultivate an environment that promotes customer service excellence, innovation, collaboration, and teamworkThe person shall apply established engineering and management principles to specifications and documentation of systems developed', 'Good understanding of development lifecycle of projects with PostgreSQL database, Azure Cloud platform and Power BI and Tableau Visualization Tools', '\u200b\u200bBasic Requirements:', 'Experience with DHS and knowledge of DHS standards a plusDemonstrated experience translating business and technical requirements into comprehensive data strategies and analytic solutionsDemonstrated ability to communicate across all levels of the organization and communicate technical terms to non-technical audiences with an impeccable attention to detail', 'Experience with DHS and knowledge of DHS standards a plus', 'Hands on experience with software upgrade and deployment documentation', 'JPI is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status.', 'Requirements\u200b\u200b', 'Demonstrated experience translating business and technical requirements into comprehensive data strategies and analytic solutions', 'Experience in Data Warehouse/Data Mart Development Life Cycle using Dimensional modeling of STAR, SNOWFLAKE schema, Fact and Dimension tables', 'Help cultivate an environment that promotes customer service excellence, innovation, collaboration, and teamwork', 'Experience writing and modifying ETL design documentation, Test case documentation and standard operating procedures (SOP) documentation', 'Proficiency with Python, R, and SQL languages, as well as various command line interfaces (Linux, AWS, Git Bash, etc.)', 'Responsibilities\u200b', 'Proficiency developing data extraction, transformation, and loading (ETL) processes, and performing test and validation steps', 'Experience handling multiple tasks, changing priorities, and timely action', 'Must be able to obtain a DHS Public Trust', 'Experience with developing data pipelines from many sources for structure and unstructured data sets in a variety of formats', 'Mid-level expertise in developing and managing data technologies, technical operations, reusable data services, and related tools and technologies', 'Experienced in optimizing database querying, data manipulation and population using SQL and PL/SQL in Oracle, MySQL, PostgreSQL databases', 'Perform multiple tasks simultaneously and successful perform under changing requirements and deadlines', 'Familiarity with Hive, Hadoop, Kylin, and other big data analytic tools', 'Demonstrated ability to communicate across all levels of the organization and communicate technical terms to non-technical audiences with an impeccable attention to detail', 'Must be a US CitizenMust be able to obtain a DHS Public Trust7+ years of IT experience including experience in design, management, and solutioning of large, complex data sets and models.Experience handling multiple tasks, changing priorities, and timely actionExperience with developing data pipelines from many sources for structure and unstructured data sets in a variety of formatsProficiency developing data extraction, transformation, and loading (ETL) processes, and performing test and validation stepsProficiency with Python, R, and SQL languages, as well as various command line interfaces (Linux, AWS, Git Bash, etc.)Demonstrated experience translating business and technical requirements into comprehensive data strategies and analytic solutionsMid-level expertise in developing and managing data technologies, technical operations, reusable data services, and related tools and technologiesDemonstrated ability to adequately plan and meet delivery objectives and maintain adequate service levels in a highly dynamic, complex environmentsExperienced in optimizing database querying, data manipulation and population using SQL and PL/SQL in Oracle, MySQL, PostgreSQL databasesHands on experience with software upgrade and deployment documentationExperience in Data Warehouse/Data Mart Development Life Cycle using Dimensional modeling of STAR, SNOWFLAKE schema, Fact and Dimension tablesGood understanding of development lifecycle of projects with PostgreSQL database, Azure Cloud platform and Power BI and Tableau Visualization ToolsExperience writing and modifying ETL design documentation, Test case documentation and standard operating procedures (SOP) documentationExperience in developing Shell scripts on UNIXFamiliarity with Hive, Hadoop, Kylin, and other big data analytic tools', '\u200bWork closely with software engineers and architects to extract, transform, and standardize data to prepare for ingest into target sources', 'The person shall apply established engineering and management principles to specifications and documentation of systems developed', 'JPI is seeking a Data Engineer to support a rapidly growing Data Analytics and Business Intelligence platform focused on providing solutions that empower our Federal customers with the tools and capabilities needed to turn data into actionable insights. The ideal candidate is a critical thinker and motivated, perpetual learner and is excited to gain exposure and build skillsets across a range of technologies while solving some of our clients’ toughest challenges. As a Data Engineer, you will be integral to data operations for the development and integration of multiple data types across a range of data sets and sources.', 'Demonstrated ability to adequately plan and meet delivery objectives and maintain adequate service levels in a highly dynamic, complex environments', 'You will be responsible for the day-to-day operations of systems that depend on data, ensuring data is properly processed and securely transferred to its appropriate location, in a timely manner.\xa0Processing data will include managing, manipulating, storing and parsing data in a data pipeline for variety of target sources. You will also support maintenance of applications and tools that reside on these systems such as upgrades, patches, and configuration changes. The work is performed in a multidisciplinary team environment using agile methodologies. The candidate we seek must be highly motivated and enthusiastic about implementing new technologies and learning about new data in a small team environment where deadlines are important.', '\xa0', 'Work with program management and engineers to implement and document complex and evolving requirements', 'Design and develop data services and/or pipelines as part of an Agile/Scrum team', 'Experience in developing Shell scripts on UNIX', 'Must be a US Citizen', 'Additional/Desired Requirements:', '7+ years of IT experience including experience in design, management, and solutioning of large, complex data sets and models.']",Associate,Full-time,Project Management,Management Consulting,2021-03-18 14:34:51
Senior Data Engineer,Appriss Retail,"Oregon, United States",1 day ago,Be among the first 25 applicants,"['', ' Ability to explore multiple technologies at once including but not limited to Python, Ruby, NoSQL and various AWS services', ' Proficient developing in SQL, ideally PostgreSQL on relational database systems', ' 3-5 years total software and relational database development experience.  2 years with a strong demonstrated ability to develop and maintain ETL solutions, ideally using Python and various Application Programming Interfaces (API)  2 of those years with a strong demonstrated ability to leverage and maintain data models in a data warehouse or data lake  Proficient developing in SQL, ideally PostgreSQL on relational database systems  Experience with containerization and container orchestration technologies such as Docker, Swarm, or Kubernetes  Ability to work in an Agile environment include comfort using JIRA  Self-motivated with a thirst to work in a dynamic and fast-pace environment leveraging modern technologies.  Understand how developer contributions are used to achieve individual goals and organizational Objectives and Key Results  Ability to explore multiple technologies at once including but not limited to Python, Ruby, NoSQL and various AWS services  Excellent written and verbal communication skills', ' You are someone willing and able to share skills in areas of strength and comfortable working with others or doing research into new technologies to complete team objectives in areas where you may lack experience.', ' Design, develop and debug ETL components utilizing AWS services  Meet the delivery expectations of the Agile Project Management methodology (1-week Data Engineering Sprint cycles)  Develop clear and concise documentation regarding technical solutions and willingness to share knowledge with teammates via “Lunch and Learns”.  Work with internal and external customers to prove requirements have been met.  Work with team to improve processes and procedures.  Work with team to make sure that 24/7 support coverage is available for On Call Rotation in areas of primary responsibility.  Other duties as assigned.', ' Excellent written and verbal communication skills', ' Other duties as assigned.', 'Helpful / Preferred', ' Experience with containerization and container orchestration technologies such as Docker, Swarm, or Kubernetes', ' Experience with Data Visualization', ' Exposure to a big data environment where scalability is a prime concept', ' You are passionate about ETL, Data Modeling, Business Intelligence and Python Development.', 'Knowledge, Skills, Abilities, Experience, Or Characteristics', ' 3-5 years total software and relational database development experience.', ' Meet the delivery expectations of the Agile Project Management methodology (1-week Data Engineering Sprint cycles)', 'Minimum Requirements -Education', ' Ability to work in an Agile environment include comfort using JIRA', ' Experience integrating AWS services with the boto3 Software Development Kit  Exposure to a big data environment where scalability is a prime concept  Exposure to maintaining software on Linux based environments  Experience with Tableau, Matillion, Kubernetes  Experience utilizing Web Services, APIs and SDKs  Experience with Data Visualization', ' OR Bachelor’s Degree in any field and equivalent experience.', ' 2 years with a strong demonstrated ability to develop and maintain ETL solutions, ideally using Python and various Application Programming Interfaces (API)', ' Disclaimer', ' You are someone who demonstrates the ability to communicate with both technical and non-technical professionals in an accurate and kind manner.', ' You are someone able to understand the needs of intra-company teams that depend on your work and strive to exceed expectations.', ' Develop clear and concise documentation regarding technical solutions and willingness to share knowledge with teammates via “Lunch and Learns”.', ' Self-motivated with a thirst to work in a dynamic and fast-pace environment leveraging modern technologies.', 'Physical And Mental Requirements', 'Other', 'Summary', ' Experience integrating AWS services with the boto3 Software Development Kit', 'Functions And Responsibilities', ' Experience utilizing Web Services, APIs and SDKs', ' Bachelor’s Degree in a computer-related field  OR Bachelor’s Degree in any field and equivalent experience.', ' You are passionate about ETL, Data Modeling, Business Intelligence and Python Development.  You are someone who will contribute to the team’s success by completing tasks assigned on time, helping others when needed and asking for help when needed.  You are someone who demonstrates the ability to communicate with both technical and non-technical professionals in an accurate and kind manner.  You are someone who can identify missing requirements and roadblocks to task completion and is able to perform research in technologies that enable the completion of team tasks.  You are someone willing and able to share skills in areas of strength and comfortable working with others or doing research into new technologies to complete team objectives in areas where you may lack experience.  You are someone able to understand the needs of intra-company teams that depend on your work and strive to exceed expectations.', ' Exposure to maintaining software on Linux based environments', ' Bachelor’s Degree in a computer-related field', ' Work with team to make sure that 24/7 support coverage is available for On Call Rotation in areas of primary responsibility.', ' 2 of those years with a strong demonstrated ability to leverage and maintain data models in a data warehouse or data lake', ' You are someone who can identify missing requirements and roadblocks to task completion and is able to perform research in technologies that enable the completion of team tasks.', ' Experience with Tableau, Matillion, Kubernetes', 'About You', ' You are someone who will contribute to the team’s success by completing tasks assigned on time, helping others when needed and asking for help when needed.', ' Work with internal and external customers to prove requirements have been met.', ' Design, develop and debug ETL components utilizing AWS services', ' Work with team to improve processes and procedures.', ' Understand how developer contributions are used to achieve individual goals and organizational Objectives and Key Results']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Staff Data Engineer,Sam's Club,"Sunnyvale, CA",2 weeks ago,155 applicants,"['', 'You’ll sweep us off our feet if:', 'You\xa0have a proven track record coding\xa0with at least one programming language (Java, Scala and/or Python) to write data pipeline and data processing layers\xa0', 'Benefits & Perks:\xa0', 'Architecting, Designing, building, testing and deploying cutting edge solutions at scale, impacting millions of customers worldwide drive value from dataInteracting with Sam’s Club engineering teams across geographies to leverage expertise and contribute to the tech community.Engaging with Product Management and Business to drive the agenda, setting your priorities and delivering awesome product features to keep platform ahead of market scenarios.Identifying the right open source tools to deliver product features by performing research, POC/Pilot and/or interacting with various open source forumsDeveloping and/or making a Contribution to adding features that enable the adoption of data across Sam’s ClubDeploying and monitoring products on Cloud platformsDeveloping and implementing the best-in-class monitoring processes to enable data applications to meet SLAsGuiding the team technically for end to end solution lifecycle', 'Being human-led is our true disruption.', 'You’re\xa0an\xa0inquisitive,\xa0out-of-the box thinker\xa0who’s\xa0continually on the lookout for\xa0opportunities\xa0to\xa0improve\xa0and innovate\xa0data\xa0systems and\xa0analytic\xa0solutions\xa0', 'Guiding the team technically for end to end solution lifecycle', 'Developing and/or making a Contribution to adding features that enable the adoption of data across Sam’s Club', 'You’ll make an impact by:\xa0', 'The above information has been designed to indicate the general nature and level of work performed in the role.\u202f It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications required of employees assigned to this job. The full Job Description can be made available as part of the hiring process.\xa0', 'We’re virtual', 'Sam’s Club is on a mission for Savings Made Simple. A division of Wal-Mart Stores Inc., we are the membership warehouse club solution for small business and everyday living. Since 1983, we’ve worked to provide our members quality products at incredible values. At Sam’s Club, everything we do is to serve our members. Whether it’s offering them quality products at an incredible value or remodeling our clubs to improve their shopping experience, we keep our members at the heart of everything we do.\xa0', 'You evangelize an extremely high standard of code quality, system reliability, and performance\xa0', 'Developing and implementing the best-in-class monitoring processes to enable data applications to meet SLAs', 'Benefits & Perks:', 'You’re\xa0an\xa0inquisitive,\xa0out-of-the box thinker\xa0who’s\xa0continually on the lookout for\xa0opportunities\xa0to\xa0improve\xa0and innovate\xa0data\xa0systems and\xa0analytic\xa0solutions\xa0You have consistently high standards,\xa0your passion for quality is inherent in everything that you do\xa0You are a collaborative connector who builds bridges between teams and functions\xa0You evangelize an extremely high standard of code quality, system reliability, and performance\xa0You\xa0have a proven track record coding\xa0with at least one programming language (Java, Scala and/or Python) to write data pipeline and data processing layers\xa0You’re\xa0experienced\xa0in\xa0computing platforms\xa0and\xa0companion\xa0tools (e.g., Azure/GCP,\xa0SQL and NoSQL)\xa0You’re\xa0skilled\xa0in data modeling &\xa0data migration\xa0protocols\xa0You have 3+ years of streaming expertise and leading teams building real time streaming pipeline, Spark streaming a plusYou are knowledgeable in Data Productization, Data Lineage and Metadata Management.', 'Job Description:', 'Equal Opportunity Employer\xa0', 'You are a collaborative connector who builds bridges between teams and functions\xa0', 'Walmart, Inc. is an Equal Opportunity Employer – By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing diversity- unique styles, experiences, identities, ideas and opinions – while being inclusive of all people.\xa0', 'You have consistently high standards,\xa0your passion for quality is inherent in everything that you do\xa0', 'About Global Tech', 'You’ll make an impact by:', ""Do you have boundless energy and passion for engineering data used to solve dynamic problems that will shape the future of retail?\xa0With the sheer scale of Sam’s\xa0Club Technologies environment\xa0comes the biggest of big data sets.\xa0As a\xa0Sam's Club\xa0Staff Data Engineer,\xa0you will dig\xa0into our\xa0mammoth\xa0scale of\xa0data to help\xa0unleash\xa0the power of retail\xa0data science by\xa0imagining, developing, and maintaining data pipelines\xa0that our Data Scientists and Analysts can rely on.\xa0\xa0You will be responsible for contributing to an orchestration layer of complex data transformations, refining raw data from source into targeted, valuable data assets for consumption in a governed way.\u202fYou will\xa0partner\xa0with\xa0Data Scientists, Analysts, other engineers\xa0and business stakeholders to\xa0solve complex\xa0and exciting\xa0challenges\xa0so that\xa0we can\xa0build out capabilities that\xa0evolve the\xa0retail business model\xa0while\xa0making\xa0a\xa0positive\xa0impact\xa0on\xa0our\xa0customers’\xa0lives.\xa0"", 'You’re\xa0skilled\xa0in data modeling &\xa0data migration\xa0protocols\xa0', 'You have 3+ years of streaming expertise and leading teams building real time streaming pipeline, Spark streaming a plus', 'Interacting with Sam’s Club engineering teams across geographies to leverage expertise and contribute to the tech community.', 'You are knowledgeable in Data Productization, Data Lineage and Metadata Management.', '(You must work in the Sunnyvale, Dallas OR Bentonville area)', 'You’ll sweep us off our feet if:\xa0', 'Imagine working in an environment where one line of code can make life easier for hundreds of millions of people and put a smile on their face. That’s what we do at Walmart Global Tech. We’re a team of 15,000+ software engineers, data scientists and service professionals within Walmart, the world’s largest retailer, delivering innovations that improve how our customers shop and empower our 2.2 million associates. To others, innovation looks like an app, service or some code, but Walmart has always been about people. People are why we innovate, and people power our innovations. Being human-led is our true disruption.', 'Identifying the right open source tools to deliver product features by performing research, POC/Pilot and/or interacting with various open source forums', 'Deploying and monitoring products on Cloud platforms', 'Equal Opportunity Employer', 'Working virtually this year has helped us make quicker decisions, remove location barriers across our global team, be more flexible in our personal lives and spend less time commuting.\xa0Today, we are reimagining the tech workplace of the future by making a permanent transition to virtual work for most of our team. Of course, being together in person is an important part of our culture and shared success. We’ll collaborate in person at a regular cadence and with purpose.\xa0', 'Engaging with Product Management and Business to drive the agenda, setting your priorities and delivering awesome product features to keep platform ahead of market scenarios.', 'Architecting, Designing, building, testing and deploying cutting edge solutions at scale, impacting millions of customers worldwide drive value from data', 'Beyond competitive pay, you can receive incentive awards for your performance. Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO, multiple health plans, and much more.\xa0', 'You’re\xa0experienced\xa0in\xa0computing platforms\xa0and\xa0companion\xa0tools (e.g., Azure/GCP,\xa0SQL and NoSQL)\xa0', ' It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications required of employees assigned to this job. The full Job Description can be made available as part of the hiring process.', 'The above information has been designed to indicate the general nature and level of work performed in the role.']",Mid-Senior level,Full-time,Information Technology,Retail,2021-03-18 14:34:51
 Data Engineer/Architect ,Steven Cardwell Recruitment & Engineering Search Firm Inc.,"Georgia, United States",,N/A,"['Experience in Machine learning model deployment in live environment', '3+ years of experience as a data engineer within a modeling environment including parsing JSON data', 'Assemble large and complex data sets that meet both functional and non-functional business needs', '2+ years of experience in working with at least one NoSQL system like MongoDB.', 'Development with Big Data, IoT data, SQL, AWS and built productionized solutions', 'A competitive salary and comprehensive benefits package, including medical, dental, vision, maternity support program, discounted virtual physician visits, voluntary medical benefits (Critical Illness, Hospital Care, and Accidental Injury), FSA, HSA,\xa0life insurance, short term and long term disability, cash matching 401(k) plan, employee assistance program (EAP), pet insurance, employee discount program, generous vacation and 11 paid holidays.', 'Key Responsibilities:', 'As the Data Engineer/Architect, you will be part of the team that delivers customer solutions and outcomes in the industry.\xa0The successful candidate must have experience with successfully designing, building, and configuring the architecture for an IoT program.\xa0The Data Engineer/Architect will spend a significant amount of time working with cloud-based components and replacing and/or upgrading components like the database, data conditioning, visualization, analytics, and math engine’s in the IoT system architecture.', ""Bachelor's degree in Computer Science, Information Systems, Applied Mathematics or equivalent work experienceExperience in Machine learning model deployment in live environmentDesigned, built, and configured the architecture for an IoT program; successfully completed multiple iterations of changes and/or upgrades to the systemDevelopment with Big Data, IoT data, SQL, AWS and built productionized solutionsExperience building and optimizing big data pipelines architectures and data sets.3+ years of experience as a data engineer within a modeling environment including parsing JSON data3+ years of data & analytics experience inside industrial industry is preferred2+ years of experience in working with at least one NoSQL system like MongoDB."", 'Ensure the long-term data storage, processing, infrastructure, speed and quality of the data-based solutions in the marketplace', 'Work with stakeholders on data-related technical issues and their data needs, and maintain a data dictionary of all the data elements and all of the different variations based on equipment data tags', '3+ years of data & analytics experience inside industrial industry is preferred', 'Create and maintain optimal data pipeline (AWS construct) architectures in the AWS cloud environment', 'Job Details', 'Education & Experience:', 'Create data tools for analytics and data science team members that will assist them in building and optimizing the product portfolio', 'Data Engineer/Architect ', 'Experience building and optimizing big data pipelines architectures and data sets.', ""Bachelor's degree in Computer Science, Information Systems, Applied Mathematics or equivalent work experience"", 'Identify, design and implement internal process improvements such as automating manual data processes, optimizing data delivery and scalability', 'Create analytics tools that utilize the data pipeline to provide actionable insights, operational efficiencies and other key performance metrics/indicators (KPM/KPI)', 'Designed, built, and configured the architecture for an IoT program; successfully completed multiple iterations of changes and/or upgrades to the system', 'Build and maintain the infrastructure that is required to transfer and hold data from a wide variety of data sources', 'Work closely with the Product Manager and internal team members to ensure all the data is optimized for next step or process.', 'Create and maintain optimal data pipeline (AWS construct) architectures in the AWS cloud environmentWork closely with the Product Manager and internal team members to ensure all the data is optimized for next step or process.Assemble large and complex data sets that meet both functional and non-functional business needsIdentify, design and implement internal process improvements such as automating manual data processes, optimizing data delivery and scalabilityEnsure the long-term data storage, processing, infrastructure, speed and quality of the data-based solutions in the marketplaceBuild and maintain the infrastructure that is required to transfer and hold data from a wide variety of data sourcesCreate analytics tools that utilize the data pipeline to provide actionable insights, operational efficiencies and other key performance metrics/indicators (KPM/KPI)Create data tools for analytics and data science team members that will assist them in building and optimizing the product portfolioWork with stakeholders on data-related technical issues and their data needs, and maintain a data dictionary of all the data elements and all of the different variations based on equipment data tags', 'Location:\xa0Georgia & Wisconsin (This is not a remote role)']",Mid-Senior level,Full-time,Engineering,Staffing and Recruiting,2021-03-18 14:34:51
Data Engineer,iFit,United States,3 days ago,108 applicants,"['', 'A free beverage center and snack bar to keep you hydrated and fueled throughout the day.', 'Build data pipelines and data models that will empower engineers and analysts to make data-driven decisions', '2-6 years of experience', 'Access to cutting-edge technology and hardware for work and fitness.', 'Working in beautiful Cache Valley with access to an exceptional outdoor lifestyle, a university campus nearby, and the chance to test the fitness products we create.Highly competitive compensation.Full benefits package (Medical, HSA, FSA, Dental, Vision and Life insurance)\xa0401(k) with company match.A PTO policy that ensures you are able to find a happy work-to-life balance.Access to cutting-edge technology and hardware for work and fitness.Collaborative workspace and environment.A free beverage center and snack bar to keep you hydrated and fueled throughout the day.', 'Deliver the highest standard in data integrity', '**Not all perks are applicable to all positions and/or locations**', 'Build data pipelines and data models that will empower engineers and analysts to make data-driven decisionsBuild data models to deliver insightful analytics\xa0Deliver the highest standard in data integrityStrong analytical skills with the ability to analyze and project sales, subscriber, and engagement data.', 'Scikit learn ', 'Build data models to deliver insightful analytics\xa0', 'Tensorflow', 'Machine learning experience ', 'Very strong SQL\xa0skillsData Modeling\xa0experience Python\xa0experience 2-6 years of experience', 'REQUIRED SKILLS ', 'Working in beautiful Cache Valley with access to an exceptional outdoor lifestyle, a university campus nearby, and the chance to test the fitness products we create.', ""We are in the process of building out our data warehouse, so there is a lot to learn all the way from ingestion to sharing data with our internal customers. We are using leading-edge tech: snowflake, _dbt, looker, and several ingestion tools like fivetran. Once this phase is over we'll get more into the data science aspect of it: generating machine learning models to improve our product offering."", 'Python\xa0experience ', 'Highly competitive compensation.', 'Strong analytical skills with the ability to analyze and project sales, subscriber, and engagement data.', 'Very strong SQL\xa0skills', 'A PTO policy that ensures you are able to find a happy work-to-life balance.', 'NICE TO HAVE', 'Full benefits package (Medical, HSA, FSA, Dental, Vision and Life insurance)\xa0', 'Data Modeling\xa0experience ', 'Pytorch\xa0', 'As a Data Engineer, you will take on big data challenges in order to deliver insightful analytics. You will build data pipelines and data models that will empower engineers and analysts to make data-driven decisions and deliver a deep understanding of the business.\xa0', 'COMPENSATION AND BENEFITS', 'Collaborative workspace and environment.', '\xa0', 'Machine learning experience Pytorch\xa0TensorflowScikit learn ', 'JOB RESPONSIBILITIES', '401(k) with company match.']",Associate,Full-time,Information Technology,"Health, Wellness and Fitness",2021-03-18 14:34:51
Data Engineer/Analyst,"ALTA IT Services, LLC","Baltimore, MD",1 day ago,Be among the first 25 applicants,"['', 'Expert in designing complex and semantically rich data structures.', 'Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements', 'Expertise in Conceptual Data Modeling, Logical Data Modeling, Physical Data Modeling, Enterprise Data Warehouse Design, DataMart Design, Metadata, Data Quality, Master Data Management and Master Data/Data Governance using Erwin', 'Experience designing relational data models, analytical data models and non-relational data models for OLTP and advanced analytic applications.', 'Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.', 'Able to work independently with BA and PM to gather requirements, perform data discovery, come up with data model, work with data engineers to build data pipeline, and work with developers to answer their questions.', 'Skills Requirements', ' Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements Able to work independently with BA and PM to gather requirements, perform data discovery, come up with data model, work with data engineers to build data pipeline, and work with developers to answer their questions. Extensively used ETL methodologies for supporting data extraction, transformations and loading processing, in a corporate-wide-ETL Solution. Expertise in Conceptual Data Modeling, Logical Data Modeling, Physical Data Modeling, Enterprise Data Warehouse Design, DataMart Design, Metadata, Data Quality, Master Data Management and Master Data/Data Governance using Erwin Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics. Ability to understand complex business processes to derive conceptual and logical data models. Lead complex discussions and engagements that may involve multiple project teams from client. Experience designing relational data models, analytical data models and non-relational data models for OLTP and advanced analytic applications. Expert in designing complex and semantically rich data structures. Worked with OLTP/data warehouse modeling, and play key role in multiple projects and act as knowledge expert (Technical, Functional). Ability to optimize and performance tune SQL queries Good data analysis, problem solving and SQL skills.', 'Education', 'Position Description', 'Worked with OLTP/data warehouse modeling, and play key role in multiple projects and act as knowledge expert (Technical, Functional).', 'Ability to understand complex business processes to derive conceptual and logical data models.', 'Clearance', 'Ability to optimize and performance tune SQL queries', 'Location', 'Extensively used ETL methodologies for supporting data extraction, transformations and loading processing, in a corporate-wide-ETL Solution.', 'Good data analysis, problem solving and SQL skills.', 'Lead complex discussions and engagements that may involve multiple project teams from client.']",Entry level,Contract,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer/Analyst - Milford Mill,"Centurion Consulting Group, LLC","Milford Mill, MD",22 hours ago,Be among the first 25 applicants,"['', 'Desired Skills', 'Able to work independently with BA and PM to gather requirements, perform data discovery, come up with data model, work with data engineers to build data pipeline, and work with developers to answer their questions. ', 'Familiar with machine learning and advanced analytic application development. ', 'Centurion is hiring immediately for a Data Engineer/Analyst to support a multi-year Federal program in Baltimore, MD. ', 'Extensively used ETL methodologies for supporting data extraction, transformations and loading processing, in a corporate-wide-ETL Solution. ', 'Good data analysis, problem solving and SQL skills. ', 'Experience working in large-scale cloud database environments is a plus. ', ' Expertise on NoSQL data modeling using Hackolade tool for databases like MongoDB, HBASE, Hive and JSON Schema.  Familiar with machine learning and advanced analytic application development.  Experience working in large-scale cloud database environments is a plus.  Ability to create dashboards in Tableau or Webfocus using various sources like database, CSV files, XL spreadsheets etc.  Experience in PostgreSQL / Greenplum database is good to have  Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards  Excellent communication skills and ability to convey complex topics through effective documentation as well as presentation.  ', 'Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements ', 'Experience in PostgreSQL / Greenplum database is good to have ', 'Bachelors +5 minimum ', 'Ability to optimize and performance tune SQL queries. ', 'Key Required Skills', 'Expertise in Conceptual Data Modeling, Logical Data Modeling, Physical Data Modeling, Enterprise Data Warehouse Design, DataMart Design, Metadata, Data Quality, Master Data Management and Master Data/Data Governance using Erwin ', ' Bachelors +5 minimum  ', 'Required', 'Education', 'Data Engineer/Analyst', 'Worked with OLTP/data warehouse modeling, and play key role in multiple projects and act as knowledge expert (Technical, Functional). ', 'Ability to create dashboards in Tableau or Webfocus using various sources like database, CSV files, XL spreadsheets etc. ', 'Windsor Mill, MD', 'Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics. ', 'Expertise on NoSQL data modeling using Hackolade tool for databases like MongoDB, HBASE, Hive and JSON Schema. ', 'Position Details', 'Expert in designing complex and semantically rich data structures. ', 'Ability to understand complex business processes to derive conceptual and logical data models. ', 'Experience designing relational data models, analytical data models and non-relational data models for OLTP and advanced analytic applications. ', ' Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements  Able to work independently with BA and PM to gather requirements, perform data discovery, come up with data model, work with data engineers to build data pipeline, and work with developers to answer their questions.  Extensively used ETL methodologies for supporting data extraction, transformations and loading processing, in a corporate-wide-ETL Solution.  Expertise in Conceptual Data Modeling, Logical Data Modeling, Physical Data Modeling, Enterprise Data Warehouse Design, DataMart Design, Metadata, Data Quality, Master Data Management and Master Data/Data Governance using Erwin  Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.  Ability to understand complex business processes to derive conceptual and logical data models.  Lead complex discussions and engagements that may involve multiple project teams from client.  Experience designing relational data models, analytical data models and non-relational data models for OLTP and advanced analytic applications.  Expert in designing complex and semantically rich data structures.  Worked with OLTP/data warehouse modeling, and play key role in multiple projects and act as knowledge expert (Technical, Functional).  Ability to optimize and performance tune SQL queries.  Good data analysis, problem solving and SQL skills.  ', 'Lead complex discussions and engagements that may involve multiple project teams from client. ', 'Data Engineer/Analyst Windsor Mill, MD', 'Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards ', 'Excellent communication skills and ability to convey complex topics through effective documentation as well as presentation. ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer II,Voltaiq,"California, United States",2 days ago,Be among the first 25 applicants,"['', ' Experience in Python programming language. ', ' Experience in Data modeling ', 'Our Stack', ' Ensure data confidentiality, integrity, and availability for our customers ', ' Understand the evolving needs of the customer-facing product and data science teams, and how these will be served by the data platform ', ' Unit testing and integration testing ', ' Experience with Pandas. ', 'Preferred Skills & Qualifications: ', ' Experience in designing, building, testing, and deploying ETL data pipelines. ', ' Excellent teamwork skills ', ' Development process and agile methodologies ', ' 3+ years of working experience in related fields. ', 'Data Engineer II', 'Responsibilities', ""  B.Sc. or higher in Computer Science or a related field or comparable experience.   3+ years of working experience in related fields.   Experience in designing, building, testing, and deploying ETL data pipelines.   Experience in Python programming language.   Experience working with SQL, specifically postgres' flavor of SQL.   Experience with Pandas.   Desire to learn new technologies.   Strong computer science fundamentals.   Familiarity with Apache Spark   Experience in Data modeling   Excellent teamwork skills   Excellent communication skills.  "", 'Voltaiq is an equal opportunity employer and is committed to achieving a diverse workforce through application of its equal opportunity and nondiscrimination policy, in all aspects of employment.', ' B.Sc. or higher in Computer Science or a related field or comparable experience. ', '  Help design, build, and test scalable data processing pipelines and ETL processes   Help design, build, and test data access services and tools for analytics   Write scripts to parse and transform battery data coming from databases, text files, and binary files   Troubleshoot emergent customer data pipeline issues   Communicate data platform architecture to other members of the Engineering team   Understand the evolving needs of the customer-facing product and data science teams, and how these will be served by the data platform   Provide visibility into the structure, state and performance of the data platform   Ensure data confidentiality, integrity, and availability for our customers  ', ' Object-Oriented and Functional programming concepts ', 'Required Skills & Qualifications', ' Communicate data platform architecture to other members of the Engineering team ', ' Excellent communication skills. ', ' Distributed systems ', ' Help design, build, and test scalable data processing pipelines and ETL processes ', ' Strong computer science fundamentals. ', ' Familiarity with Apache Spark ', ' Experience working with large scale, data-intensive web applications. ', ' Experience with python ORM, preferably Django ORM ', 'The Role', 'Competitive salary plus equity and full benefits. Our office is located in Berkeley, CA.', ' Desire to learn new technologies. ', ' Troubleshoot emergent customer data pipeline issues ', '  Distributed systems   Object-Oriented and Functional programming concepts   Unit testing and integration testing   Development process and agile methodologies   Experience working with large scale, data-intensive web applications.   Experience with python ORM, preferably Django ORM  ', ' Help design, build, and test data access services and tools for analytics ', ' Provide visibility into the structure, state and performance of the data platform ', "" Experience working with SQL, specifically postgres' flavor of SQL. "", ' Write scripts to parse and transform battery data coming from databases, text files, and binary files ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Zycron,"Farmers Branch, TX",2 weeks ago,118 applicants,"['', 'Zycron, a Brand of BG Staffing, Inc. (NYSE American: BGSF), is one of the largest IT solutions firms headquartered in Tennessee. We provide client-specific solutions from staffing to outsourcing across all industries, with extensive experience in health care, energy and utilities, and state and local government. To learn more about our services visit\xa0www.zycron.com', 'Create and support data processing solutions using a wide array of technologies across Google, Amazon, and Azure cloud environments.', 'Experience with big data tools: Hadoop, Spark, Kafka, etc. is a plus.', 'Participate in the data management life cycle process from requirements through production support.', '3+ years of total data engineering experience.', 'Build, manage, and optimize data pipelines for data structures encompassing data transformation, data models, schemas, metadata, data quality, and workload management.', 'Participate in Agile ceremonies like standup, grooming and retrospectives.', 'Develop, test, and deploy solutions for data warehousing.', 'Essential Duties & Responsibilities', 'Experience exposing data as a service using RESTful APIs is preferred.', 'Experience with at least one of Google Cloud, AWS Cloud, or Azure Cloud preferred.', 'Zycron is currently seeking a\xa0Data Engineer\xa0for our client in the Dallas, TX area.\xa0This is a direct hire/full time position with generous salary, bonus and benefits.\xa0The client is not able to sponsor visas at this time.\xa0No CTC.', 'Data Engineer\xa0', 'A strong desire for continuous growth and learning.', 'Job ID Number:', 'Experience with Google Big Query or SQL Server is a preferred.', 'Minimum Qualifications (Knowledge, Skills, and Abilities)', 'Prepare design documents, create unit tests, apply version control, and perform related operational duties.', 'Create and support data processing solutions using a wide array of technologies across Google, Amazon, and Azure cloud environments.Develop, test, and deploy solutions for data warehousing.Participate in the data management life cycle process from requirements through production support.Create technical documentation.Participate in Agile ceremonies like standup, grooming and retrospectives.Prepare design documents, create unit tests, apply version control, and perform related operational duties.Build, manage, and optimize data pipelines for data structures encompassing data transformation, data models, schemas, metadata, data quality, and workload management.Troubleshoot and own defects identified by the QA team and customers.Work closely with business analysts and product owners to understand requirements.', '2 years experience using\xa0SQL to\xa0query.', 'Work closely with business analysts and product owners to understand requirements.', 'Data Engineer', 'Job ID Number:\xa0117375 (Please reference in call or email)', 'The\xa0Data Engineer\xa0will\xa0deliver quality data intelligence solutions to the organization, assist our analytics team with drawing insights, and participate in creating our data platform. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up.', '3 years combined experience with SQL and/or No SQL databases.', '1 year experience working with a version control system..', '\xa0', 'Create technical documentation.', '2 years combined experience with object-oriented scripting languages such as C#, Python, Go, or Java.', 'Troubleshoot and own defects identified by the QA team and customers.', ""Only candidates with backgrounds who match our client's requested experience will be contacted. Do not take this as a poor reflection on your experience, just a decision for the specific needs of our client's project/job. We look forward to working with you."", '3+ years of total data engineering experience.3 years combined experience with SQL and/or No SQL databases.2 years combined experience with object-oriented scripting languages such as C#, Python, Go, or Java.2 years experience using\xa0SQL to\xa0query.1 year experience working with a version control system..Experience exposing data as a service using RESTful APIs is preferred.Experience with Google Big Query or SQL Server is a preferred.Experience with big data tools: Hadoop, Spark, Kafka, etc. is a plus.Experience with at least one of Google Cloud, AWS Cloud, or Azure Cloud preferred.A strong desire for continuous growth and learning.']",Mid-Senior level,Full-time,Information Technology,Apparel & Fashion,2021-03-18 14:34:51
Data Science Engineer,WorkBoard Inc.,"Austin, TX",20 hours ago,Be among the first 25 applicants,"['', ""Within One Month, You'll"", 'Leverage your mastery of business, communication, and technical skills to consult with experts in most successful enterprises in the business world. ', ' Bring at least 5 years of combined experience in Business Intelligence, Data Science and Data Warehousing Software development expertise in Python. Expertise in SQL and experience with large relational database systems. Experience creating an Events framework to enable better data analytics Experience with Data Visualization standard methodologies Experience working with the Product team to define and create Product Success metrics and engagement drivers. BS in Computer Science, Engineering or related technical or equivalent experience Have worked at a fast growing SaaS organization where you’ve demonstrated personal accountability and willingness to go above and beyond the job description. You are genuine, warm, positive, empathetic, and engaging with a passion for technology, and customer success. ', 'Humble ', ' Fully understand the Schema for our entire Relational Database Understand and be a point of contact for our event logging pipeline and the business logic associated with the events Ability to have client-facing conversations on our published metrics and help them understand the product logic Maintain / Enhance our internal Dashboard in Python and Sigma Work with App Engineering, Product and Customer Success teams to roll out in-product data focused features Dig deep into data to find levers to increase adoption and success and work cross functionally to drive awareness of these levers ', 'Fully understand the Schema of our core product tables', 'happy ', 'THE WORKBOARD STORY', 'honest ~ ', 'THE TEAM', 'Experience working with the Product team to define and create Product Success metrics and engagement drivers.', 'Have a deep understanding of the problem space we are building the platform for, and how are various offerings are interconnected to provide a comprehensive business solution', 'Demonstrate 100% understanding of WorkBoard’s underlying data structure and it’s mapping to our UI Objects', 'Work with App Engineering, Product and Customer Success teams to roll out in-product data focused features', ' Become a certified OKR Coach and WorkBoard Expert! Demonstrate 100% understanding of WorkBoard’s underlying data structure and it’s mapping to our UI Objects Have action plans in place to achieve your Key Results! ', '401K with employer matching', ""Within Three Months, You'll"", 'Understand our user engagement and product analytics for 3 areas better than anyone', 'Understand our current Analytics events structure and framework', 'OUR VALUES - WE LIVE BY THE 4 Hs', ""Within Six Months, You'll"", 'Ability to have client-facing conversations on our published metrics and help them understand the product logic', 'Have worked at a fast growing SaaS organization where you’ve demonstrated personal accountability and willingness to go above and beyond the job description.', 'Understand and be a point of contact for our event logging pipeline and the business logic associated with the events', 'Paid holidays', 'Experience creating an Events framework to enable better data analytics', 'THE OPPORTUNITY', 'Hungry ', 'Have action plans in place to achieve your Key Results!', 'WorkBoard', 'Maintain / Enhance our internal Dashboard in Python and Sigma', 'Dig deep into data to find levers to increase adoption and success and work cross functionally to drive awareness of these levers', 'COMING IN', 'Become a certified OKR Coach and WorkBoard Expert!', 'Health insurance ', 'Bring at least 5 years of combined experience in Business Intelligence, Data Science and Data Warehousing', 'a Few Of Our Awesome Benefits', 'Experience with Data Visualization standard methodologies', 'Software development expertise in Python.', 'Expertise in SQL and experience with large relational database systems.', 'You are genuine, warm, positive, empathetic, and engaging with a passion for technology, and customer success.', 'Be part of the foundational team with great people, who have an entrepreneurial mindset and bring their absolute best every day.', 'Rich, deep data set to draw insights from and influence internal and product and engineering leadership', 'Provide customer feedback and work with the engineering team to translate the feedback into product features. ', 'BS in Computer Science, Engineering or related technical or equivalent experience', 'Flexible PTO & sick days', 'And much more!', ' Rich, deep data set to draw insights from and influence internal and product and engineering leadership Be part of the foundational team with great people, who have an entrepreneurial mindset and bring their absolute best every day. Leverage your mastery of business, communication, and technical skills to consult with experts in most successful enterprises in the business world.  Provide customer feedback and work with the engineering team to translate the feedback into product features.  ', 'Fully understand the Schema for our entire Relational Database', ' Have a deep understanding of the problem space we are building the platform for, and how are various offerings are interconnected to provide a comprehensive business solution Fully understand the Schema of our core product tables Understand our current Analytics events structure and framework Understand our user engagement and product analytics for 3 areas better than anyone ', ' Flexible PTO & sick days Paid holidays Health insurance  401K with employer matching Quarterly All-Hands Meetings And much more! ', 'Quarterly All-Hands Meetings']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Guild Mortgage,"San Diego, CA",3 days ago,Be among the first 25 applicants,"['', 'Experience collecting structured, semi-structured and unstructured data in various popular formats and sourced from internal core systems as well as 3rd\xa0partner providers such as Google Analytics, Facebook Insights, Zillow, CoreLogic, MLS Data, Public Records, and Property Data', 'Physical: Work is primarily sedentary; mobility in an office setting.', 'Strong analytical skills to build processes supporting data transformation, data structures, metadata, dependency and workload management.', 'Audio/Visual:', 'Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.', 'Develop processes and tools to monitor and analyze data warehouse performance and data accuracy.', ""Bachelor's degree, BS in Statistics, Computer Science, Data Science or related quantitative field is required, along with a minimum of five years’ experience in Data Engineer related role(s) and at least three of those years spent in a senior technical level role(s) required."", 'Audio/Visual: Ability to accurately interpret sounds and associated meanings at a volume consistent with interpersonal conversation. Regularly required to accurately perceive, distinguish and interpret information received visually and through audio; e.g., words, numbers and other data broadcasted aloud/viewed on a screen, as well as print and other media.', 'Assemble large, complex data sets that meet functional / non-functional business requirements.', 'Environmental:\xa0Office environment – no substantial exposure to adverse environmental conditions.', 'Focus on the continual improvement of policies, procedures, and processes falling under scope of authority.', 'Create data tools for data scientist and analytics team members that assist them in building and optimizing data analysis and reporting.', 'Expert at creating data integrations using Extract, Transform and Load (ETL) tools and modern data pipes.', 'Highly proficient with data loading, processing and data warehouse design techniques: star or snowflake schema designs, etc.', ""Bachelor's degree, BS in Statistics, Computer Science, Data Science or related quantitative field is required, along with a minimum of five years’ experience in Data Engineer related role(s) and at least three of those years spent in a senior technical level role(s) required.Highly proficient in building and optimizing Big Data pipelines, architecture and data sets.Highly proficient with data loading, processing and data warehouse design techniques: star or snowflake schema designs, etc.Proven experience with data lake and warehouse best practices and leading products in the marketplace.Expert at creating data integrations using Extract, Transform and Load (ETL) tools and modern data pipes.Strong knowledge of Kafka, Sparks, ESB and/or other messaging queues, real-time data integration & stream processing technologies.Proven experience with querying and analyzing data from SQL relational databases such as AuroraDB, IBM DB2, MemSQL, MS SQL Server, MySQL, Postgres, Redshift or Snowflake, and NoSQL Databases such as Cassandra, DynamoDB, Redis, or MongoDBExperience with data science languages such as R, and Python as well as general purpose languages such as Java, Scala, C# or JavaScript.Experience collecting structured, semi-structured and unstructured data in various popular formats and sourced from internal core systems as well as 3rd\xa0partner providers such as Google Analytics, Facebook Insights, Zillow, CoreLogic, MLS Data, Public Records, and Property DataExperience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.Strong analytical skills to build processes supporting data transformation, data structures, metadata, dependency and workload management.Strong data governance skills to ensure highest quality of data is made available for data analysis and accuracy in reporting.Problem solver with an ability to work as a team towards a solution.Ability to prioritize multiple tasks in a deadline-driven environment, strong sense of urgency and responsiveness.Strong detail orientation and highly organized with proven ability to lead effectively and drive results in a matrixed management environment.Ability to think critically, including the ability to evaluate facts and data to draw conclusions, determine the downstream impact of decisions and associated risks.Excellent verbal and written communication skills plus demonstrate strong leadership capabilities.Strong interpersonal and team building skills.Self-starter with the demonstrated ability to learn/adapt to new technologies and techniques.Ethical, with a commitment to company values."", 'Keep our Personally Identifiable Information (PII) data separated and secured through table splitting, data masking, access control restrictions and redundancy (high availability across multiple zones).', 'Contribute to the team of data professionals by using expertise to answer questions and sharing repeatable design patterns with less experienced teammates, enhancing skillsets and competencies of team members, and sharing technical knowledge throughout the team.', 'Self-starter with the demonstrated ability to learn/adapt to new technologies and techniques.', 'Drive strong communications, partnerships, and stakeholder management with senior leaders, functional managers and staff.', 'Essential Functions', 'Excellent verbal and written communication skills plus demonstrate strong leadership capabilities.', 'Position Summary', 'Stay abreast of latest technology trends and participate in high-level decisions impacting the direction of Information Technology function.', 'Consistently monitor and model platform usage, database sizes, compute resources and third-party costs to ensure that the data team’s spend is as cost-efficient as possible.', 'The Data Engineer is a key technical position and plays an important role in the organization by performing a number of activities related to the company’s Information Technology functions.\xa0The role is primarily responsible, under limited supervision, for executing the data strategy and analysis efforts within the company’s enterprise data platform by expanding and optimizing data and data pipeline architecture, as well as optimizing data flow and collection from cross functional areas. Responsibilities include supporting software developer, database architects, data analysts and data scientists on various data initiatives as well as ensuring optimal data delivery architecture is maintained as needs grow.', 'Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.', 'Environmental:\xa0', 'Ability to think critically, including the ability to evaluate facts and data to draw conclusions, determine the downstream impact of decisions and associated risks.', 'Guild offers a pleasant work environment, competitive compensation and excellent benefits package; including medical, dental, vision, life insurance, AD&D, LTD and 401(k) with employer match.', 'Participate in stakeholder reviews, and design sessions.', 'Improve the quality of data used for analysis by assessing the accuracy of new data sources and the effectiveness of new data gathering techniques.', 'Partner with key stakeholders, including the business unit leaders, Product, Data & Technology teams, to assist with their data-related technical issues and support their data infrastructure needs by collecting data from primary sources and optimizing the data architecture, improving quality of sourced data, and ensuring consistent delivery of data to key stakeholders in a timely manner.', 'Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and leading ELT tools.', 'Provide data, reports, and information to management as needed.', 'Highly proficient in building and optimizing Big Data pipelines, architecture and data sets.', 'Requirements', 'Champion data transformation to new ways of working and generating insights.', 'Proven experience with data lake and warehouse best practices and leading products in the marketplace.', 'Qualifications', 'Problem solver with an ability to work as a team towards a solution.', 'Ability to prioritize multiple tasks in a deadline-driven environment, strong sense of urgency and responsiveness.', 'Strong interpersonal and team building skills.', 'Strong data governance skills to ensure highest quality of data is made available for data analysis and accuracy in reporting.', 'Physical: ', 'Guild Mortgage Company is an Equal Opportunity Employer.', 'Strong knowledge of Kafka, Sparks, ESB and/or other messaging queues, real-time data integration & stream processing technologies.', 'Manual Dexterity:', 'Guild Mortgage Company, closing loans and opening doors since 1960. As a mortgage banking firm we are dedicated to serving the home owner/buyer. Our goal is to provide affordable home financing for our customers, utilizing the best terms available while providing a level of professionalism and service unsurpassed in the lending industry.', 'Use expertise to resolve high level issues that cannot be solved by teammates.', 'Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.', 'Perform other duties as assigned.', 'Ethical, with a commitment to company values.', 'Strong detail orientation and highly organized with proven ability to lead effectively and drive results in a matrixed management environment.', 'Partner with key stakeholders, including the business unit leaders, Product, Data & Technology teams, to assist with their data-related technical issues and support their data infrastructure needs by collecting data from primary sources and optimizing the data architecture, improving quality of sourced data, and ensuring consistent delivery of data to key stakeholders in a timely manner.Extract, Load & Transform (ELT) data from various primary data sources (internal & 3rd\xa0party) into the data lake to create optimal data pipeline architecture.Assemble large, complex data sets that meet functional / non-functional business requirements.Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and leading ELT tools.Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.Keep our Personally Identifiable Information (PII) data separated and secured through table splitting, data masking, access control restrictions and redundancy (high availability across multiple zones).Create data tools for data scientist and analytics team members that assist them in building and optimizing data analysis and reporting.Improve the quality of data used for analysis by assessing the accuracy of new data sources and the effectiveness of new data gathering techniques.Develop processes and tools to monitor and analyze data warehouse performance and data accuracy.Contribute to the team of data professionals by using expertise to answer questions and sharing repeatable design patterns with less experienced teammates, enhancing skillsets and competencies of team members, and sharing technical knowledge throughout the team.Participate in stakeholder reviews, and design sessions.Provide data, reports, and information to management as needed.Identify, track, and monitor trends and avoidable technology-related errors; work across functions to develop complex solutions, improvements, and stop-gaps.Focus on the continual improvement of policies, procedures, and processes falling under scope of authority.Use expertise to resolve high level issues that cannot be solved by teammates.Stay abreast of latest technology trends and participate in high-level decisions impacting the direction of Information Technology function.Consistently monitor and model platform usage, database sizes, compute resources and third-party costs to ensure that the data team’s spend is as cost-efficient as possible.Champion data transformation to new ways of working and generating insights.Drive strong communications, partnerships, and stakeholder management with senior leaders, functional managers and staff.Perform other duties as assigned.', 'Travel: Infrequent based on company events and/or relevant conferences or training', 'Identify, track, and monitor trends and avoidable technology-related errors; work across functions to develop complex solutions, improvements, and stop-gaps.', 'Experience with data science languages such as R, and Python as well as general purpose languages such as Java, Scala, C# or JavaScript.', 'Extract, Load & Transform (ELT) data from various primary data sources (internal & 3rd\xa0party) into the data lake to create optimal data pipeline architecture.', 'Travel:', '\xa0', 'Proven experience with querying and analyzing data from SQL relational databases such as AuroraDB, IBM DB2, MemSQL, MS SQL Server, MySQL, Postgres, Redshift or Snowflake, and NoSQL Databases such as Cassandra, DynamoDB, Redis, or MongoDB', 'Manual Dexterity:\xa0Frequent use of computer keyboard and mouse.', 'Guild Mortgage Company', 'Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.']",Mid-Senior level,Full-time,Information Technology,Financial Services,2021-03-18 14:34:51
Data Engineer,The Motley Fool,"Alexandria, VA",2 weeks ago,85 applicants,"['', 'Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.Experience building and optimizing ‘Big Data’ data pipelines, architectures, and data sets.Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.Strong analytical skills and detailed oriented.Build processes supporting data transformation, data structures, metadata, dependency and workload management.A successful history of manipulating, processing and extracting value from large disconnected datasets.Strong project management and organizational skills.Experience supporting and working with cross-functional teams in a dynamic environment.Ability to work with stakeholders to translate business requirements into technical requirements. ', 'Experience with serverless technologies like AWS Lambda a plus.', 'Ability to work with stakeholders to translate business requirements into technical requirements. ', 'Primary Responsibilities', 'They Should Also Have Experience Using The Following Software/tools', 'Assemble large, complex data sets that meet functional / non-functional business requirements.', 'Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.', 'A successful history of manipulating, processing and extracting value from large disconnected datasets.', 'Experience with relational SQL databases.Experience with some cloud services like Azure and AWS.Experience with object-oriented/object function scripting languages like Python.Experience with streaming tools like Kafka/Kinesis and Spark Structured Streaming a plus.Experience with serverless technologies like AWS Lambda a plus.Experience working with or understanding formal ETL tools like SSIS a plus. ', 'Experience working with or understanding formal ETL tools like SSIS a plus. ', 'Experience building and optimizing ‘Big Data’ data pipelines, architectures, and data sets.', 'Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL.', 'Experience with relational SQL databases.', 'Experience with streaming tools like Kafka/Kinesis and Spark Structured Streaming a plus.', 'Strong analytical skills and detailed oriented.', 'Strong project management and organizational skills.', 'Work with data and analytics experts to strive for greater functionality in our data systems.', 'Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.', 'Build processes supporting data transformation, data structures, metadata, dependency and workload management.', 'Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.', 'Description', 'Experience supporting and working with cross-functional teams in a dynamic environment.', 'Create and maintain optimal data pipeline architecture.Assemble large, complex data sets that meet functional / non-functional business requirements.Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, and re-designing infrastructure for greater scalability.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL.Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.Work with data and analytics experts to strive for greater functionality in our data systems.', 'Create and maintain optimal data pipeline architecture.', 'Experience with some cloud services like Azure and AWS.', 'Experience with object-oriented/object function scripting languages like Python.', 'Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, and re-designing infrastructure for greater scalability.', 'Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.', 'Preferred Qualifications', 'Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.']",Entry level,Full-time,Information Technology,Online Media,2021-03-18 14:34:51
Data engineer,Perficient,"Wilmington, DE",1 week ago,Be among the first 25 applicants,"['', 'Job Overview', ' Experts of the data and its application by users  Understands data landscape and environments: sources, elements, update freq, completeness, stewards/contacts, platforms  Manages ETL: use programming and tools for data ingestion, configure pipelines, apply transformations and decoding, integrate and fuse data, move and securely deliver  Translates business requirements to build repeatable, sustainable, efficient, coded processes that can be productionized by Software Engineers and readily modified by other Data Engineers  Creates POC processes in Dev/QA and works with Software Engineers to productionize and align best coding and process practices that maximize efficiency, speed, stability, system resources and capabilities  Leverages frameworks in place with big data tools: Hadoop, Spark, Python, Kafka, etc.  Experience with relational SQL and NoSQL databases  Awareness of and compliance with: data privacy, security, legal and contractual guidelines  Incorporates data quality and privacy checks/alerts to minimize bad data being consumed by end users, models and dashboards, and to protect customer data; revises checks for new data issues  Maintains feedback loop with Data Stewards on data issues, standards, fit for use (Data Stewardship is a subset of data engineering which would include responsibilities like data curation)  Validates data products and pipelines are functioning as expected following system or application upgrades, source changes, etc.  Responsible for data architecture including sources, table structures, physical models  Works closely with Architects to align systems, tools and applications being utilized with business use case and performance requirements  Communicates with end users to set expectations and ensure alignment around data accuracy, completeness, timeliness and consistency  Provides data product support and maintenance  Establishes, tracks and monitors KPIs related to specific data products and deliverables ', ' Develop open source platform components using Spark, Scala, Java, Oozie, Hive and other components ', ' Responsible for data architecture including sources, table structures, physical models ', ' Will you now, or in the future, require sponsorship for employment visa status (e.g. H-1B visa status)?', ' Troubleshoot deployment to various environments and provide test support. ', ' Manages ETL: use programming and tools for data ingestion, configure pipelines, apply transformations and decoding, integrate and fuse data, move and securely deliver ', ' Work with data engineering team to define and develop data ingestion, validation, transformation and data engineering code.  Develop open source platform components using Spark, Scala, Java, Oozie, Hive and other components  Document code artifacts and participate in developing user documentation and run books  Troubleshoot deployment to various environments and provide test support.  Participate in design sessions, demos and prototype sessions, testing and training workshops with business users and other IT associates ', ' Participate in design sessions, demos and prototype sessions, testing and training workshops with business users and other IT associates ', 'Responsibilities', ' Establishes, tracks and monitors KPIs related to specific data products and deliverables ', ' Maintains feedback loop with Data Stewards on data issues, standards, fit for use (Data Stewardship is a subset of data engineering which would include responsibilities like data curation) ', ' Translates business requirements to build repeatable, sustainable, efficient, coded processes that can be productionized by Software Engineers and readily modified by other Data Engineers ', ' Understands data landscape and environments: sources, elements, update freq, completeness, stewards/contacts, platforms ', ' Are you legally authorized to work in the United States?', ' Document code artifacts and participate in developing user documentation and run books ', ' Validates data products and pipelines are functioning as expected following system or application upgrades, source changes, etc. ', 'Qualifications', ' Creates POC processes in Dev/QA and works with Software Engineers to productionize and align best coding and process practices that maximize efficiency, speed, stability, system resources and capabilities ', ' Incorporates data quality and privacy checks/alerts to minimize bad data being consumed by end users, models and dashboards, and to protect customer data; revises checks for new data issues ', ' Experts of the data and its application by users ', ' Provides data product support and maintenance ', ' Work with data engineering team to define and develop data ingestion, validation, transformation and data engineering code. ', ' Leverages frameworks in place with big data tools: Hadoop, Spark, Python, Kafka, etc. ', 'More About Perficient', 'Preferred Skills And Education', ' Works closely with Architects to align systems, tools and applications being utilized with business use case and performance requirements ', 'Data Engineer with Spark exp.', 'Overview', ' Experience with relational SQL and NoSQL databases ', ' Awareness of and compliance with: data privacy, security, legal and contractual guidelines ', ' Communicates with end users to set expectations and ensure alignment around data accuracy, completeness, timeliness and consistency ', ' Disclaimer: ']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer/REMOTE,The Bachrach Group,New York City Metropolitan Area,1 week ago,89 applicants,"['', 'Goal of this role is to review through trillions of datapoints, and petabytes of data to help make it faster, more cost effective, and stable.', 'Prior experience in an AWS environment', 'Do you love data??? Are you looking for an opportunity to join an innovative, well established small business that is all about data? Then this is the opportunity for you to jump right in and help oversee one of the largest data marketplaces!', 'MUST HAVE TECH SKILLS:', 'Php', 'Ideal person will love working autonomously, spending most of their time with all their data!!', 'Competitive compensation', 'node.js', 'PLUS SKILLS:', 'SQL', 'Prior experience must be in working with node.js', 'Golang']",Mid-Senior level,Full-time,Engineering,Computer Software,2021-03-18 14:34:51
Data Engineer,Compass,"Seattle, WA",3 weeks ago,Over 200 applicants,"['', 'Design, build and launch core data models into production', '2+ years experience in the data modeling/ETL/schema design', 'Experience working with databricks is a plus', 'Responsibilities', '2+ years experience writing SQL', '2+ years experience in fields such as data engineering, business intelligence, or analytics', 'About This Role', 'Partner with Engineering, Product and/or the Business to understand data needs and drive innovative solutions', 'Define and manage SLA for datasets in areas of ownership', 'Advocate for process improvements to drive our data platform forward with a metrics-driven, continuous improvement approach', ' Partner with Engineering, Product and/or the Business to understand data needs and drive innovative solutions Design, build and launch core data models into production Define and manage SLA for datasets in areas of ownership Advocate for process improvements to drive our data platform forward with a metrics-driven, continuous improvement approach ', '2+ years of experience with an object-oriented programming languages (e.g Python) is a plus', '2+ years working with Spark or an MPP system ', 'Experience with business intelligence tools such as Looker or Tableau is a plus', 'What We Look For', ' 2+ years experience in fields such as data engineering, business intelligence, or analytics 2+ years experience writing SQL 2+ years experience in the data modeling/ETL/schema design 2+ years working with Spark or an MPP system  2+ years of experience with an object-oriented programming languages (e.g Python) is a plus Experience with business intelligence tools such as Looker or Tableau is a plus Experience working with databricks is a plus Knowledge of, or prior experience with, real estate data integrations is a plus ', 'Knowledge of, or prior experience with, real estate data integrations is a plus']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Data Engineer,TuneCore,"Vinegar Hill, NY",5 days ago,27 applicants,"['', 'Candidates Should', 'Job Description']",Entry level,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Engineer,Harnham,"New York, United States",3 days ago,80 applicants,"['', 'A client in the Apparel industry is looking for a Data Engineer, in this role, you will be helping manage all the engineering pieces around Data (e.g. feature pipelines, deployment, monitoring, etc)', 'Batch streaming is a must-have, streaming is a nice to have', '5 years of relevant professional experienceBatch streaming is a must-have, streaming is a nice to havePyspark, bigdata hadoop, databricksExperience in using CI/CD pipelineExperience leveraging open sources big data processing frameworks, such as Apache Spark, Hadoop and streaming technologies such as Kafka', 'Pyspark, bigdata hadoop, databricks', 'Remote', '6-month w Extension', '5 years of relevant professional experience', 'Experience in using CI/CD pipeline', 'Data Engineer ', 'Experience leveraging open sources big data processing frameworks, such as Apache Spark, Hadoop and streaming technologies such as Kafka', '$80-90/hr']",Mid-Senior level,Contract,Engineering,Staffing and Recruiting,2021-03-18 14:34:51
Data Engineer (remote),Spider Strategies,"Arlington, VA",5 days ago,145 applicants,"['Data Engineer (remote)', 'Details', 'Spider Strategies has an opening for a talented, experienced Data Engineer to join our government contracting team.', 'Job Description', 'This position requires a thorough understanding of databases, SQL, primary keys, relational integrity, bash/unix scripting, SSH, (S)FTP, and how to manipulate raw data sets (Excel, CSV, XML, JSON, etc.).', 'You will support our customer as a Data Engineer and technical consultant on a strategic dashboard and business intelligence platform, focusing on manipulation and presentation of data as per client requirements.', ' Must have a DOD SECRET (or interim) clearance and prior experience working with the US Army.**Job DescriptionYou will support our customer as a Data Engineer and technical consultant on a strategic dashboard and business intelligence platform, focusing on manipulation and presentation of data as per client requirements.This position involves working with data source owners to obtain data from systems of record. You will clean and transform data, writing and maintaining scripts in the process. You will build, maintain, and link datasets in the customer BI platform. You will also assist with building dashboards and reports, allowing for the most informed decisions to be made by senior leaders.This is a full-time remote work position.DetailsThis position requires a thorough understanding of databases, SQL, primary keys, relational integrity, bash/unix scripting, SSH, (S)FTP, and how to manipulate raw data sets (Excel, CSV, XML, JSON, etc.).The preferred candidate has an understanding of Military force structure and Army-related regulations/publications.Experience with ETL and business intelligence/reporting tools is helpful but not required.Experience with programming languages like JavaScript or Python is helpful but not required.Excellent written and oral communication skills are a must in order to prepare concise recommendations, reports, and briefings to senior-level agency officials.Capability to plan, organize, and complete studies and analyses related to assigned actions.Spider Strategies is a leading provider of performance management software and consulting and supports some of the world\'s largest companies and U.S. Federal Agencies. In addition to rewarding work, working at Spider Strategies is rewarding. Spider Strategies was recently named one of the Top 10 ""Best Places to Work"" in the Washington, DC area by the Washington Business Journal -- this is Spider\'s 4th time to receive this recognition.', 'This is a full-time remote work position.', 'This position involves working with data source owners to obtain data from systems of record. You will clean and transform data, writing and maintaining scripts in the process. You will build, maintain, and link datasets in the customer BI platform. You will also assist with building dashboards and reports, allowing for the most informed decisions to be made by senior leaders.', 'Experience with ETL and business intelligence/reporting tools is helpful but not required.', 'Experience with programming languages like JavaScript or Python is helpful but not required.', 'Capability to plan, organize, and complete studies and analyses related to assigned actions.', 'Excellent written and oral communication skills are a must in order to prepare concise recommendations, reports, and briefings to senior-level agency officials.', 'The preferred candidate has an understanding of Military force structure and Army-related regulations/publications.', 'Looking for an exciting, challenging opportunity to take your Data Engineer career to the next step? Seeking a fast-paced, government contracting environment where skill and motivation are rewarded, teamwork and a can-do attitude are expected, and a work-life balance is valued? Then we have the perfect position for you!']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Sr. Data Engineer,Blue River Technology,"Sunnyvale, CA",1 day ago,Be among the first 25 applicants,"['', "" Expert Python developer Bachelor's Degree in Computer Science, Math, Physics or other quantitative field 5 years professional software development experience Adept query optimization in NoSQL and SQL databases such as Postgres, Mongo, Athena, and InfluxDB Experience developing microservices and cloud native applications Familiarity with common data visualization tools: e.g, Apache Superset, Tableau, Looker Self motivated, independent thinker, and excellent communicator"", ' Expert Python developer', "" Bachelor's Degree in Computer Science, Math, Physics or other quantitative field"", ' Familiarity with common data visualization tools: e.g, Apache Superset, Tableau, Looker', ' Adept query optimization in NoSQL and SQL databases such as Postgres, Mongo, Athena, and InfluxDB', ' Experience with Kubeflow or TFX', ' Experience with production ML platform', ' Build solutions to satisfy the data management, search, and data versioning needs of Blue River Connect our ecosystem by building integrations across multiple existing and new tools Implement and optimize data model, queries, and ETLs for complicated text, image, and sensor data Design service level interfaces for our ML and Data Platform Work with product, data scientists, and roboticists to build visualization tools and dashboards', 'Required Skills & Qualifications', ' 5 years professional software development experience', ' Self motivated, independent thinker, and excellent communicator', ' Connect our ecosystem by building integrations across multiple existing and new tools', ' Experience developing within the Jupyter ecosystem is a plus', 'Role Responsibilities', ' Experience developing microservices and cloud native applications', ' Experience developing within the Jupyter ecosystem is a plus Experience with production ML platform Experience with Kubeflow or TFX Familiarity with ML Frameworks: TensorFlow, PyTorch.', ' Work with product, data scientists, and roboticists to build visualization tools and dashboards', ' Design service level interfaces for our ML and Data Platform', ' Implement and optimize data model, queries, and ETLs for complicated text, image, and sensor data', ' Familiarity with ML Frameworks: TensorFlow, PyTorch.', ' Build solutions to satisfy the data management, search, and data versioning needs of Blue River', 'Preferred Qualifications']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer/Analyst,"Software Tech Enterprises, Inc.","Baltimore, MD",21 hours ago,Be among the first 25 applicants,"['', 'Required Skills', 'Work Location: ', 'Experience with data lake, big data, AWS and cloud-based databases, MPP database technologies such as Greenplum and PostgreSQL is a plus familiar with data replication methodology.', 'Software Tech Enterprises, Inc. (STE) is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: STE is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at STE are based on business needs, job requirements, and individual qualifications, without regard to race, color, religion or belief, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. STE will not tolerate discrimination or harassment based on any of these characteristics. STE encourages applicants of all ages.', 'Strong interpersonal skills with ability to collaborate with others effectively and efficiently.', 'Key Required Skills: ', 'Experience in BI related fields with progressively increased level of responsibility in data analysis, data modeling, BI development, database development and SQL experiences.', 'Good understanding with BI development life cycle, data architect, metadata management, data marts and related methodologies.', 'Excellent oral and written communication skills are mandatory', 'Education', 'Software Tech Enterprises', 'Able to multi-task under tight deadlines.', 'Excellent troubleshooting and problem-solving skills Outstanding SQL skills Proficient with Erwin Data Modeler tools to build data models.', 'Data Engineer/Analyst. ', 'Detailed Skills Requirement ', 'To learn more about Software Tech Enterprises, visit us at ', 'Experience in analyzing and building reports / dashboards using Tableau is advantageous Proficient in discussing the project architecture with solution architects and Enterprise architect teams.', 'Possesses a strong passion for data analytics.', 'STE', 'Experience in BI related fields with progressively increased level of responsibility in data analysis, data modeling, BI development, database development and SQL experiences.Good understanding with BI development life cycle, data architect, metadata management, data marts and related methodologies.Experience in data modeling for operational and data warehousingExperience with data lake, big data, AWS and cloud-based databases, MPP database technologies such as Greenplum and PostgreSQL is a plus familiar with data replication methodology.Experience in analyzing and building reports / dashboards using Tableau is advantageous Proficient in discussing the project architecture with solution architects and Enterprise architect teams.Possesses a strong passion for data analytics.Excellent troubleshooting and problem-solving skills Outstanding SQL skills Proficient with Erwin Data Modeler tools to build data models.Ability to create model from scratch and modify existing complex models as per the Enterprise standards.Able to multi-task under tight deadlines.Ability to work with multiple teams and users for more than one project, at the same time.Strong interpersonal skills with ability to collaborate with others effectively and efficiently.Excellent oral and written communication skills are mandatory', 'Software Tech Enterprises (STE)', 'Experience in data modeling for operational and data warehousing', 'Ability to create model from scratch and modify existing complex models as per the Enterprise standards.', 'http://www.software-tec.com', 'Ability to work with multiple teams and users for more than one project, at the same time.']",Entry level,Full-time,Information Technology,Capital Markets,2021-03-18 14:34:51
Data Engineer,Oak Street Health,"Chicago, IL",2 days ago,Be among the first 25 applicants,"['', 'Daily production support for Enterprise Data Warehouse including ETL/ELT jobs.', 'Design, Develop, and unit test new or existing ETL/Data Integration solutions to meet business requirements.', 'Develop workflows in the cloud environment using Cloud base architecture (Azure).', 'Develop dataflows and processes for the Data Warehouse using SQL Server.', 'Proven experience in real time data integration role with expert level SQL', 'Knowledge in DevOps practices and tools is a plus', 'Proven experience integrating enterprise software using ETL modules/Data Engineering tools', 'Deliver increased productivity and effectiveness through rapid delivery of high-quality applications.', 'Design and Develop data integration/engineering workflows on big data technologies and platforms', 'Design, Develop, and unit test new or existing ETL/Data Integration solutions to meet business requirements.Daily production support for Enterprise Data Warehouse including ETL/ELT jobs.Design and Develop data integration/engineering workflows on big data technologies and platformsDevelop workflows in the cloud environment using Cloud base architecture (Azure).Develop dataflows and processes for the Data Warehouse using SQL Server.Develop workflows for real time data integration.Develop Data integration workflows using Web services in XML, JSON, flat file format, RESTWork with stakeholders including the Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.Deliver increased productivity and effectiveness through rapid delivery of high-quality applications.', 'Experience with Big Data technology a plus', 'Headquarters (the “Treehouse”) located in the heart of Downtown, close to many public transit options and great restaurants ', 'Ability to integrate data from Web services in XML, JSON, flat file format, REST', 'Oak Street Health Offers Our Coworkers The Opportunity To Be At The Forefront Of a Revolution In Healthcare, As Well As', 'Working knowledge of ETL change detection solutions such as change data capture (CDC)', 'Description', 'Collaborative and energetic cultureHigh levels of responsibility and rapid advancementHeadquarters (the “Treehouse”) located in the heart of Downtown, close to many public transit options and great restaurants Competitive benefits; including paid vacation/sick time, generous 401K match with immediate vesting, as well as health benefits', 'Collaborative and energetic culture', 'High levels of responsibility and rapid advancement', 'Develop Data integration workflows using Web services in XML, JSON, flat file format, REST', 'Competitive benefits; including paid vacation/sick time, generous 401K match with immediate vesting, as well as health benefits', 'Experience Bachelor’s degree in Computer Science, Engineering, or related field from an accredited university', 'Experience Bachelor’s degree in Computer Science, Engineering, or related field from an accredited universityProven experience in real time data integration role with expert level SQLWorking knowledge of ETL change detection solutions such as change data capture (CDC)Knowledge of Apache Kafka, MicroservicesProven experience integrating enterprise software using ETL modules/Data Engineering toolsKnowledge of data architecture, structures and principles with the ability to critique data and system designsAbility to integrate data from Web services in XML, JSON, flat file format, RESTExperience with Big Data technology a plusKnowledge in DevOps practices and tools is a plus', 'Knowledge of Apache Kafka, Microservices', 'Develop workflows for real time data integration.', 'Knowledge of data architecture, structures and principles with the ability to critique data and system designs', 'We’re Looking For Motivated, Experienced Developers With', 'Work with stakeholders including the Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.']",Entry level,Full-time,Information Technology,Financial Services,2021-03-18 14:34:51
Data Engineer - Job ID 12852,Infor,United States,1 week ago,44 applicants,"['', 'S', 'Real-world experience with cloud-based server monitoring data and workflows strongly desired.', 'Hands-on experience with one major cloud platform is preferred.', 'About Infor', 'Bachelors (or foreign equivalent) in Computer Science, Electrical Engineering or a related quantitative discipline from an accredited university.Demonstrated real-world experience in building and orchestrating big data pipelines of structured and unstructured data sets.Experience building data-centric analytics solutions.Understanding of basic data science models: Linear regression, Logistic regression, SVM, K-mean, Decision trees, etc.Good data modeling and problem-solving skills.Proficiency in development using a JVM-based language. Scala is preferred.Hands-on experience with frameworks in the Big Data ecosystem. Spark, Delta Lake, and Neo4j are preferred.A strong collaborative mindset with great interpersonal skills to help solve complex business problems.', 'Familiarity with AI/ML techniques such as boosted models, tree-based models, neural networks, etc.', 'Learn and adopt new technologies fast.', 'Build reusable, scalable, and reliable big data pipelinesSupport all facets of machine learning based software deploymentsAdd high-value data analytics capabilities to existing Infor products.Timely deliver projects and products.Learn and adopt new technologies fast.Be a team player.', ':', 'Add high-value data analytics capabilities to existing Infor products.', 'A Day In The Life Typically Includes', 'Location: Remote', 'Advanced proficiency in Apache Spark and Scala.', 'I', 'Position Summary', 'What Will Put You Ahead?', 'Proficiency in development using a JVM-based language. Scala is preferred.', 'Hands-on experience with frameworks in the Big Data ecosystem. Spark, Delta Lake, and Neo4j are preferred.', 'A proven desire to continue learning new technologies and techniques.', 'Demonstrated real-world experience in building and orchestrating big data pipelines of structured and unstructured data sets.', 'Bachelors (or foreign equivalent) in Computer Science, Electrical Engineering or a related quantitative discipline from an accredited university.', 'Good data modeling and problem-solving skills.', 'Infor Values', 'What You Will Need', 'Advanced proficiency in Apache Spark and Scala.Real-world experience with cloud-based server monitoring data and workflows strongly desired.Familiarity with AI/ML techniques such as boosted models, tree-based models, neural networks, etc.Demonstrated real-world experience in mathematical modeling, data science methodologies, and coding.Hands-on experience with one major cloud platform is preferred.Excellent communication skills (e.g., the ability to communicate effectively and efficiently with a broad range of audiences.)A proven desire to continue learning new technologies and techniques.', 'A strong collaborative mindset with great interpersonal skills to help solve complex business problems.', 'Basic Qualifications', 'Experience building data-centric analytics solutions.', 'Support all facets of machine learning based software deployments', 'Be a team player.', 'Data Engineer', 'Watch to find out!', 'De', 'Understanding of basic data science models: Linear regression, Logistic regression, SVM, K-mean, Decision trees, etc.', 'Build reusable, scalable, and reliable big data pipelines', 'Excellent communication skills (e.g., the ability to communicate effectively and efficiently with a broad range of audiences.)', 'Timely deliver projects and products.', 'A', 'Demonstrated real-world experience in mathematical modeling, data science methodologies, and coding.', 'Preferred Qualifications', 'Who is Infor? ']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Macquarie Group,"Philadelphia, PA",1 month ago,Be among the first 25 applicants,"['', ' Our commitment to Diversity and Inclusion ', 'About The Corporate Operations Group', 'Data Engineer', 'Work type:', 'Division:', 'Category:', 'Group:', 'Location:', 'Opening Date:', 'Recruiter:', 'About Macquarie', 'Job no:']",Not Applicable,Full-time,Information Technology,Banking,2021-03-18 14:34:51
Data Engineer - Marketing Applications,Squarespace,"New York, NY",1 week ago,50 applicants,"['', ' Experience with end-to-end ETL work using Python and SQL Experience making large datasets accessible Experience with compute frameworks and job orchestration systems A sincere commitment to creating readable, reliable, and well tested code BS in Computer Science, or related or equivalent industry experience 3+ years of industry experience  ', 'Free lunch and snacks at all offices', 'About Squarespace', ' Experience with end-to-end ETL work using Python and SQL Experience making large datasets accessible Experience with compute frameworks and job orchestration systems A sincere commitment to creating readable, reliable, and well tested code BS in Computer Science, or related or equivalent industry experience 3+ years of industry experience ', 'Build end-to-end data pipelines, consuming from a variety of sources', 'Build processes to make pipeline implementations faster, simpler, and less error-prone for product engineers', 'Responsibilities', 'Equity plan for all employees', 'Up to 20 weeks of paid family leave', 'Retirement benefits with employer match', 'Education reimbursement', 'Experience making large datasets accessible', 'Qualifications', ' Health insurance with 100% premium covered for you and your dependent children Flexible vacation & paid time off Up to 20 weeks of paid family leave Equity plan for all employees Retirement benefits with employer match Fertility and adoption benefits Free lunch and snacks at all offices Education reimbursement Dog-friendly workplace in New York office Commuter benefit in the form of reduced tax (Ireland) and pretax (US) ', '  Experience with end-to-end ETL work using Python and SQL Experience making large datasets accessible Experience with compute frameworks and job orchestration systems A sincere commitment to creating readable, reliable, and well tested code BS in Computer Science, or related or equivalent industry experience 3+ years of industry experience   ', 'Maintain monitoring and test coverage to ensure reliability of our data sets', 'Flexible vacation & paid time off', 'Work with data consumers to identify self-service opportunities, and make sure they have access to the data they need to make decisions.', 'Health insurance with 100% premium covered for you and your dependent children', 'Today, more than a million people around the globe use Squarespace to share different perspectives and experiences with the world. Not only do we embrace and celebrate the diversity of our customer base, but we also strive for the same in our employees. At Squarespace, we are committed to equal employment opportunity regardless of race, color, ethnicity, ancestry, religion, national origin, gender, sex, gender identity or expression, sexual orientation, age, citizenship, marital or parental status, disability, veteran status, or other class protected by applicable law. We are proud to be an equal opportunity workplace.', 'A sincere commitment to creating readable, reliable, and well tested code', 'Benefits & Perks', 'Commuter benefit in the form of reduced tax (Ireland) and pretax (US)', ' Build end-to-end data pipelines, consuming from a variety of sources Work with data consumers to identify self-service opportunities, and make sure they have access to the data they need to make decisions. Help guide the team towards data best practices, and mentor teammates. Build processes to make pipeline implementations faster, simpler, and less error-prone for product engineers Maintain monitoring and test coverage to ensure reliability of our data sets ', 'Experience with compute frameworks and job orchestration systems', 'Dog-friendly workplace in New York office', 'Experience with end-to-end ETL work using Python and SQL', 'BS in Computer Science, or related or equivalent industry experience', '3+ years of industry experience', 'Fertility and adoption benefits', 'Help guide the team towards data best practices, and mentor teammates.']",Mid-Senior level,Full-time,Advertising,Computer Software,2021-03-18 14:34:51
Data Engineer,Princess Polly,United States,2 weeks ago,53 applicants,"['', 'Proficiency with a scripting language like Python (strongly preferred)', '#PrincessPolly #PursueYourPassion #PrincessPollyCareers #SomethingDifferent\xa0', '·\xa0\xa0Amazing Employee Discount Program (40%)', ""Princess Polly is an Equal Opportunity Employer (EOE) . We're committed to a diverse and inclusive workplace and encourage applicants from all walks of life. Come join us, different makes us better"", 'While already setting the pace in the industry and maintaining our market-leading position, in late 2018 we set up our Los Angeles office and tripled sales for the region within the first 12 months. Now a team of over 230 people globally, we are looking for the best and brightest to grow the Princess Polly brand in our US office located in West Hollywood!\xa0', 'Expert SQL skills (required)', 'Experience using dbt (strongly preferred)', 'Experience working with cloud data warehouses (required)', 'Take ownership over our data architecture, managing, selecting, and improving the tools we use to store, load, transform, and visualize data', '·\xa0\xa0Company Paid Life, Short Term Disability, Long Term Disability, & Employee Assistance Plans', '·\xa0\xa015 Vacation Days + 10 Sick Days + 10 Holidays', 'Implement testing, validation, and documentation to flag and resolve issues with poor-quality data', 'Partner with our analysts to improve the quality of data that we load to our warehouse via custom pipelines (e.g. via Fivetran)', 'Experience in ecommerce/direct to consumer businesses', 'Design, build and maintain scalable data models (in dbt) to power self-service business intelligence tools and support fast analysis by our analysts', 'We recognize that asking you to give 100% of yourself on a daily basis, requires us to show you the love and offer a package that can only be described as best in class within the retail space today!', '2+ years of experience in a data/analytics engineering role', '·\xa0\xa0Positive Company Culture that Celebrates both Personal & Company Milestones', ""We are always on the hunt for incredibly talented individuals like YOU to join the Polly team. We’re customer-obsessed, it’s what drives us to always stay relevant. Our ideal you, is driven, a diva for on-trend fashion, is not just all talk about how quickly you can go but someone who can walk/run/sprint when needed, as we are a fast-paced business! If you can identify with our brand and vision don't waste another moment, submit your application today!"", '·\xa0\xa0401(k) Program (100% Match Up to 5% of Pay)', 'Optimize our data modeling layer (dbt), reducing runtime and decreasing unnecessary complexity', 'If you need assistance or accommodation during the hiring process due to a disability, please contact us at accomodation@exceleratebrands.com. Please note that we do not respond to application inquiries or resume submissions via this email address.', 'Responsibilities:', 'Interest in conducting reproducible analysis that is peer-reviewed, version-controlled, and well-documented', 'Qualifications:', 'What are you waiting for? Come experience something different and amazing in the retail space!', 'Established as an online force in the Australian retail scene, Princess Polly has delivered fashion-forward, trend-driven apparel and accessories for over 15 years. We are obsessed with creating the best customer experience available online and are committed to ensuring a speedy delivery so our customers can wear their picks this weekend!\xa0', '·\xa0\xa0Company Sponsored Medical (HMO & PPO Options), Dental, & Vision Plans', 'Princess Polly is looking for a talented Data Engineer to join our Data team. He/She will be responsible for ensuring high-quality, accurate data modeling (we use dbt), ensuring that our raw data is complete and accurate, and helping data analysts and others on the Data team transform raw data into clean, modeled data that is ready for analysis by end users. This role will have a high level of autonomy with the ability to alter the processes we maintain, and make other decisions related to data/analytics engineering that improve the capabilities and outputs of our team. This role will report to the Head of of Data & Analytics.', 'Enthusiasm for writing clean code', '·\xa0\xa0Individual & Team Based Leadership Development Programs', '\xa0', 'Proactively seek out and explore new technologies to advance our data capabilities', 'Aside from the amazing array of tangible benefits and perks, Princess Polly offers you the chance to make a daily impact on a global business. You have the opportunity to pursue your passion and plan your own future as part of our team!', 'Be a champion for efficient, effective data modeling, reviewing pull requests, suggesting improvements, and helping/coaching others on the team to write better code']",Mid-Senior level,Full-time,Information Technology,Retail,2021-03-18 14:34:51
Data Engineer - Predictive Analytics,Interactive Resources - iR,"Austin, TX",24 hours ago,26 applicants,"['', 'RESPONSIBILITIES:', 'Meet with the business users, assist with data-related technical issues, and support their data infrastructure needs.', 'Good understanding in object-oriented and functional script language: Python, Scala, and C#.', 'Strong experience with NoSQL database, including PostgresBackground in working with Azure Cloud Services: Data Factory, SQL database, Functions, Data Lake, Databricks, Logic Apps, and Azure Automation.Good understanding in object-oriented and functional script language: Python, Scala, and C#.Proficient with SQL Server database - writing advanced SQL script, profiling, and optimization.Working knowledge of BI tools: MS Integration Services, Reporting Services, Analysis Services, and PowerBI.Experience with Big Data tools such as; Spark, Snowflake, and Kafka', 'Build a data model to get actionable insights from data, operational efficiency, and other key business performance metrics.', 'Proficient with SQL Server database - writing advanced SQL script, profiling, and optimization.', 'Design and manage inbound and outbound data processes and monitoring. Work with the data provider to bring in new feed into our data eco-system.', 'Working knowledge of BI tools: MS Integration Services, Reporting Services, Analysis Services, and PowerBI.', 'EDUCATION:', 'Experience with Big Data tools such as; Spark, Snowflake, and Kafka', 'Strong experience with NoSQL database, including Postgres', 'Automate the data testing processes and integrate them with monitoring systems.', 'Bachelors/Masters in Computer Science, MIS/Information Management, Engineering or related field', 'Background in working with Azure Cloud Services: Data Factory, SQL database, Functions, Data Lake, Databricks, Logic Apps, and Azure Automation.', 'Data Engineer –  Fulltime - Austin TX', 'KNOWLEDGE:', 'Design and develop data pipelines to extract data from a wide variety of data sources.', 'Write TDD based code to meet overall data quality standards as defined by the users.', 'Design and develop data pipelines to extract data from a wide variety of data sources.Build a data model to get actionable insights from data, operational efficiency, and other key business performance metrics.Design and manage inbound and outbound data processes and monitoring. Work with the data provider to bring in new feed into our data eco-system.Enjoy working in Agile as part of a scrum team and deliver high quality product incrementally in an interactive manner.Write TDD based code to meet overall data quality standards as defined by the users.Automate the data testing processes and integrate them with monitoring systems.Analyze existing systems and data sets to help Business Analysts define the functional and non-functional requirements.Meet with the business users, assist with data-related technical issues, and support their data infrastructure needs.', '\xa0', 'Analyze existing systems and data sets to help Business Analysts define the functional and non-functional requirements.', 'Enjoy working in Agile as part of a scrum team and deliver high quality product incrementally in an interactive manner.']",Mid-Senior level,Full-time,Information Technology,Staffing and Recruiting,2021-03-18 14:34:51
Senior Data Engineer,Mastech Digital,"Austin, MN",1 day ago,35 applicants,"['Role Description: ', 'Education Level\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0', 'Role:\xa0Senior Data Engineer', 'Bachelor’s Degree + 10 years of experience', ' for our direct client. We value our professionals, providing comprehensive benefits and the opportunity for growth. This is a Contract-To-Hire position and the client is looking for someone to start immediately.', ' ', 'Equal Employment Opportunity', 'Mastech Digital provides digital and mainstream technology staff as well as Digital Transformation Services for all American Corporations. We are currently seeking a\xa0Senior Data Engineer', 'direct ', '·Experience in designing and developing ETLs with tools like Informatica, Microsoft SSIS (SQL Server Integration Services), Oracle Data Integrator (ODI), Python.', 'Recruiter Phone: 877.884.8834 (Ext: 2147)/C: +1 203.678.9801', 'Recruiter Name: Divya Mishra', 'Contract-To-Hire ', 'Email ID: divya.mishra@mastechdigital.com', '· Excellent written and verbal communication skills.', '·Experience with reading and writing SQL. ', 'Mastech Digital ', 'Primary Skills: Data warehouse with Oracle PL/SQL', 'Note: W2 only, No C2C', 'Experience: Minimum 10+ years', ""· A bachelor's degree in Computer Science, MIS, or related area and significant experience with business intelligence design and development.\xa0"", '#Mastech1', 'Description', 'Local Preferred ', 'Senior Data Engineer', '·Experience designing and developing within a business intelligence/reporting tool like OBIEE (Oracle Business Intelligence Enterprise Edition) or OTBI (Oracle Transactional Business Intelligence). ', 'Location:\xa0Austin, MN(Remote\xa0till COVID)', '·Experience  in  engineering within a data warehouse or related experience with dimensional data modeling.', 'Duration: Contract-To-Hire', 'Mandatory']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Peptilogics,"Pittsburgh, PA",3 weeks ago,95 applicants,"['', 'Peptilogics: Leaders in Scalable, Rational Design', 'Experience with high-performance computing (HPC) and public clouds (e.g., AWS, GCP, Azure).', 'Identify and implement data access tools and APIs.', 'Recommend and implement ways to improve data quality and efficiency.', 'Contribute requirements specifications and feedback to the software platform team.', 'Diversity & Inclusion: Our Foundation for Innovation', 'At Peptilogics, diversity, inclusion and equality are embedded in our DNA.\xa0Together, regardless of gender, race, ethnicity, national origin, age, sexual orientation or identity, education or disability, the Peptilogics team envisions a workplace where all employees feel valued and respected. We continue to build an inclusive culture that encourages, supports, and celebrates the diverse voices of our team members.\xa0', 'M.S. in computer science, bioinformatics, or related discipline.', 'Excellent communication and interpersonal skills.', 'Job duties and responsibilities:', 'Peptilogics is committed to serving as a model of diversity and inclusion for the entire biotech and biopharmaceutical industry and to maintaining an inclusive environment with equitable treatment for all. It fuels our innovation, reinforces our vision, and more closely connects us to the communities we serve.', 'Peptilogics is a clinical-stage biotechnology company that designs and develops novel peptide therapeutics. The company leverages machine learning, automation, and peptide synthesis to build platforms for science at scale. With its foundations in drug development and engineering, the company’s platforms vertically integrate proprietary hardware, software, bioinformatics, chemistry, and molecular biology to advance basic research, target validation, and clinical trials.', ""Acquire relevant data from public and proprietary sources and integrate it with Peptilogics' data systems.Recommend and implement ways to improve data quality and efficiency.Identify, implement, maintain, and test data architecture to maximize the value of large, heterogeneous data collections and to maximize their performance.Identify and implement data access tools and APIs.Identify and implement tools for data visualization.Contribute requirements specifications and feedback to the software platform team.Communicate methods and results to stakeholders representing diverse expertise and backgrounds."", '\ufeff', 'M.S. in computer science, bioinformatics, or related discipline.Experience with, and strong working knowledge of, diverse database technologies including graph (e.g., Dgraph, Neo4j), document (e.g., MongoDB), and relational (e.g., MariaDB MySQL).Experience working with bioinformatic data.Experience with high-performance computing (HPC) and public clouds (e.g., AWS, GCP, Azure).Experience with Linux, Python, and API design and implementation.Excellent communication and interpersonal skills.', 'About the Position', 'Identify and implement tools for data visualization.', 'Identify, implement, maintain, and test data architecture to maximize the value of large, heterogeneous data collections and to maximize their performance.', 'Education and experience:', ""Acquire relevant data from public and proprietary sources and integrate it with Peptilogics' data systems."", 'Experience with Linux, Python, and API design and implementation.', 'Experience with, and strong working knowledge of, diverse database technologies including graph (e.g., Dgraph, Neo4j), document (e.g., MongoDB), and relational (e.g., MariaDB MySQL).', '\xa0', 'Experience working with bioinformatic data.', 'This Data Engineer position contributes to research and development to advance discovery and development of peptide therapeutics. The position operates in close collaboration with experts in machine learning, computational biology, peptide science, and software platform development.', 'Communicate methods and results to stakeholders representing diverse expertise and backgrounds.']",Not Applicable,Full-time,Engineering,Biotechnology,2021-03-18 14:34:51
Data Engineer,Versique Search & Consulting,"Minneapolis, MN",1 week ago,26 applicants,"['', 'Required Skills & Experience', ' A successful history of manipulating, processing and extracting value from large disconnected datasets. ', ' Create and maintain optimal data pipeline architecture  Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.  Assemble large, complex data sets that meet functional / non-functional business requirements.  Build the infrastructure required for optimal ETL of data from a wide variety of data sources using SQL and AWS technologies.  Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.  Implements processes and systems to monitor data quality, ensuring production data is accurate and available for key stakeholders and business processes that depend on it.  Work with stakeholders including the Executive, Product, and Data Science teams to assist with data-related technical issues and support their data infrastructure needs.  Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.  Work with data and analytics experts to strive for greater functionality in our data systems. ', ' Experience building and optimizing data pipelines, architectures and data sets. ', ' Create and maintain optimal data pipeline architecture ', ' Build the infrastructure required for optimal ETL of data from a wide variety of data sources using SQL and AWS technologies. ', ' 5+ years of experience in a Data Engineer role. ', ' Implements processes and systems to monitor data quality, ensuring production data is accurate and available for key stakeholders and business processes that depend on it. ', ' Strong analytic skills related to working with unstructured datasets. ', 'About Versique', ' Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. ', ' Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics. ', ' Working knowledge of message queuing, stream processing, and highly scalable data stores. ', ' Assemble large, complex data sets that meet functional / non-functional business requirements. ', 'Data Engineer ', ' Build processes supporting data transformation, data structures, metadata, dependency and workload management. ', ' Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. ', ' Bachelor’s Degree, an undergraduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field ', ' Experience supporting and working with cross-functional teams in a dynamic environment. ', ' Bachelor’s Degree, an undergraduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field  5+ years of experience in a Data Engineer role.  Advanced working SQL knowledge and experience working with relational databases, query authoring as well as working familiarity with a variety of databases.  Experience building and optimizing data pipelines, architectures and data sets.  Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.  Strong analytic skills related to working with unstructured datasets.  Build processes supporting data transformation, data structures, metadata, dependency and workload management.  A successful history of manipulating, processing and extracting value from large disconnected datasets.  Working knowledge of message queuing, stream processing, and highly scalable data stores.  Strong project management and organizational skills.  Experience supporting and working with cross-functional teams in a dynamic environment. ', ' Strong project management and organizational skills. ', ' Work with data and analytics experts to strive for greater functionality in our data systems. ', 'Job Description', ' Work with stakeholders including the Executive, Product, and Data Science teams to assist with data-related technical issues and support their data infrastructure needs. ', ' Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader. ', ' Advanced working SQL knowledge and experience working with relational databases, query authoring as well as working familiarity with a variety of databases. ', 'Role & Responsibilities']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,DAT Freight & Analytics,"Denver, CO",18 hours ago,Be among the first 25 applicants,"['', 'The Skills You’ll Need', 'Strong SQL Proficiency with Snowflake including analytic queries and other complex use cases', 'DAT Software and Analytics is a next-generation SaaS technology company that has been at the leading edge of innovation in transportation supply chain logistics for 43 years. We continue to transform the industry year over year, by deploying a suite of software solutions to millions of customers every day - customers who depend on DAT for the most relevant data and most accurate insights to help them make smarter business decisions and run their companies more profitably. We operate the largest marketplace of its kind in North America, with 226 million freight posts in 2020, and a database of $110 billion of annual global shipment market transaction data. DAT is based in Beaverton, OR, with offices in Colorado, Missouri, Texas, and Bangalore, India.', 'Experience with WhereScape RED ELT Software', 'Experience with Snowflake cloud data warehouse administration.\xa0', 'Working either independently on projects or in collaborative teams depending upon the needs of the effort', 'All referrals and résumés are managed exclusively through the Human Resources Department.', 'Understanding of Oracle, SQL Server, Mongo and other database systemsScripting experience with Powershell and PythonExperience with WhereScape RED ELT SoftwareUnderstanding of modern data warehouse architectures such as data vault', 'Experience with SQL Query Tuning, data investigation, and optimization', 'DAT Software & Analytics is seeking a Data Engineer to join our Data and Engineering team in Denver, Colorado.', 'DAT Software and Analytics embraces the value of a diverse workforce, and believes it is a core strength of our company that we encourage those values in every DAT employee, at every level of our organization, regardless of tenure or rank. We provide equal employment opportunities (EEO) to all employees and applicants without regard to race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, disability, genetic information, marital status, amnesty, or status as a covered veteran in accordance with applicable federal, state, and local laws.', 'a Data Engineer', 'What You’ll Do', 'Working closely with product teams and data scientists to determine how to capture, persist, and maintain data', 'Bonus Skills', 'For additional information, see www.DAT.com/company', 'Deployment pipeline and infrastructure as code and deployment pipeline experience on a cloud platform such as AWS', 'Working closely with product teams and data scientists to determine how to capture, persist, and maintain dataBuilding distributed, scalable, and reliable data pipelines that ingest and process data at scale with ever diminishing time processing windowsLeverage AWS to deliver efficient, cloud-native solutionsIdentifying opportunities in our data landscape where we need data enrichmentIdentifying and make recommendations on data storage solutions using alternate\xa0technologiesAnalyzing\xa0and recommending alternate persistence solutions\xa0Serving as a resource for the Data Sciences team by partnering with them to make production quality and supportable solutions\xa0Gathering requirements from analysts and the larger business community to optimize, unify, and simplify data according to business needsWorking either independently on projects or in collaborative teams depending upon the needs of the effort', 'DAT Software and Analytics will not consider unsolicited résumés from vendors including search firms, fee-based referral services, and/or recruitment agencies.', 'Understanding of modern data warehouse architectures such as data vault', 'Scripting experience with Powershell and Python', 'About DAT', 'Experience building large data stores, modern data warehouse, or advanced data processing solutions', 'DAT Software and Analytics offers competitive compensation and an excellent benefit package that includes medical, dental, and vision coverage, flexible savings accounts, 401K, Life and AD&D insurance, a comprehensive Paid Leave program, and a Tuition Reimbursement program.', 'Identifying opportunities in our data landscape where we need data enrichment', 'Identifying and make recommendations on data storage solutions using alternate\xa0technologies', 'Serving as a resource for the Data Sciences team by partnering with them to make production quality and supportable solutions\xa0', 'Exceptional ELT/ETL background with data transformation and operational experience\xa0\xa0Strong SQL Proficiency with Snowflake including analytic queries and other complex use casesExperience with SQL Query Tuning, data investigation, and optimizationExperience with Snowflake cloud data warehouse administration.\xa0Experience modeling star schemas and other summary objects based on business requirementsExperience building large data stores, modern data warehouse, or advanced data processing solutionsExperience with AWS tools and infrastructure.\xa0Particularly in regards to s3 storage, data pipelines, and serverless AWS compute services (e.g. Lambdas)Deployment pipeline and infrastructure as code and deployment pipeline experience on a cloud platform such as AWS', 'Analyzing\xa0and recommending alternate persistence solutions\xa0', 'Leverage AWS to deliver efficient, cloud-native solutions', 'Exceptional ELT/ETL background with data transformation and operational experience\xa0\xa0', '\xa0\xa0', 'Experience with AWS tools and infrastructure.\xa0Particularly in regards to s3 storage, data pipelines, and serverless AWS compute services (e.g. Lambdas)', 'Understanding of Oracle, SQL Server, Mongo and other database systems', 'The range of responsibilities of the Data Engineer are broad and range from traditional ETL & SQL solutions to building cloud infrastructure and serverless data processing solutions (e.g. AWS Lambdas) for sophisticated machine learning models. In this role you will be part of a supportive, collaborative, and diverse team responsible for exploring, making recommendations, and implementing enterprise data solutions primarily in our snowflake data warehouse.', '\xa0', 'Gathering requirements from analysts and the larger business community to optimize, unify, and simplify data according to business needs', 'DAT’s industry dominant network of carriers, brokers and shippers makes our data an immensely valuable asset to the industry.\xa0', 'Experience modeling star schemas and other summary objects based on business requirements', 'Building distributed, scalable, and reliable data pipelines that ingest and process data at scale with ever diminishing time processing windows']",Mid-Senior level,Full-time,Engineering,Computer Software,2021-03-18 14:34:51
"Data Engineer, Ads Measurement","Reddit, Inc.",Utica-Rome Area,1 week ago,37 applicants,"['', 'What We Can Expect From You', 'Build ETLs and data aggregations that can improve our understanding of ad performance.', 'Investigate discrepancies between 1st party and 3rd party mobile conversion data and identify gaps and product changes to improve our conversion coverage', 'Experience with dashboarding tools and data visualization', 'Build a norms database that tracks the performance of various advertising campaigns. A complete understanding of campaign parameters and associated performance will help us make recommendations that can improve advertiser ROI.', 'Responsibilities', 'Bachelor’s degree or above in a quantitative major (e.g., mathematics, statistics, computer science). ', ' Bachelor’s degree or above in a quantitative major (e.g., mathematics, statistics, computer science).  Proficiency with Python programming language including common libraries (Pandas, NumPy, Scikit-learn, Plotly etc) and visualizations Knowledge and experience with big data tools (SQL, Big Query, Spark, Hadoop) Experience with dashboarding tools and data visualization Experience with workflow scheduling (such as Airflow) and data pipelining  Familiarity with building experimentation platforms is a plus Knowledge of statistics is a plus 2+ years of experience in Data Engineer, Data Warehouse Engineer or Data Scientist roles', '2+ years of experience in Data Engineer, Data Warehouse Engineer or Data Scientist roles', 'Knowledge and experience with big data tools (SQL, Big Query, Spark, Hadoop)', 'Knowledge of statistics is a plus', 'You will take over the existing Marketing data pipelines and enhance them to improve data quality, accessibility and reliability.', 'Assist with running and evaluating multi-cell experiments against counterfactuals and wrangle with data issues and biases. Test out various statistical approaches to deal with data insufficiency and under-powered tests.', 'Develop scalable experimentation solutions through 3rd party data ingestions, integrations with measurement and data partners, automated scripts for experiment set up validation and metric pipelines for lift measurement.', 'Partner with Data Scientists, Measurement Researchers and leads in building measurement solutions such as conversion lift studies, brand lift studies, brand insights tools etc', ' Build robust data infrastructure that not only supports the current Measurement initiatives, but also serves as a data platform on which future Measurement solutions will depend upon. You will take over the existing Marketing data pipelines and enhance them to improve data quality, accessibility and reliability. Build a norms database that tracks the performance of various advertising campaigns. A complete understanding of campaign parameters and associated performance will help us make recommendations that can improve advertiser ROI. Develop scalable experimentation solutions through 3rd party data ingestions, integrations with measurement and data partners, automated scripts for experiment set up validation and metric pipelines for lift measurement. Partner with Data Scientists, Measurement Researchers and leads in building measurement solutions such as conversion lift studies, brand lift studies, brand insights tools etc Investigate discrepancies between 1st party and 3rd party mobile conversion data and identify gaps and product changes to improve our conversion coverage Assist with running and evaluating multi-cell experiments against counterfactuals and wrangle with data issues and biases. Test out various statistical approaches to deal with data insufficiency and under-powered tests. Build tools that can assist Marketing and Sales teams with creating powerful marketing narratives for advertisers. Build ETLs and data aggregations that can improve our understanding of ad performance. ', 'Proficiency with Python programming language including common libraries (Pandas, NumPy, Scikit-learn, Plotly etc) and visualizations', 'Familiarity with building experimentation platforms is a plus', 'Build robust data infrastructure that not only supports the current Measurement initiatives, but also serves as a data platform on which future Measurement solutions will depend upon.', 'Experience with workflow scheduling (such as Airflow) and data pipelining ', 'Build tools that can assist Marketing and Sales teams with creating powerful marketing narratives for advertisers.']",Not Applicable,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Jr. Data Engineer,Qlarion,"Reston, VA",5 days ago,72 applicants,"['', ' B.S. in Computer Science or Systems Engineering preferred. Other 4-year degrees with relevant experience will be considered.', ' Experience with data science a plus.', ' Experience with data visualization a plus.', ' Experience with SQL a must.', ' Experience in data analysis and problem solving required.', ' Experience with Python/R a must.', ' Strong oral and written communication skills desired.', ' Ability to work independently and in a team environment.']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Codoxo,Atlanta Metropolitan Area,2 weeks ago,Over 200 applicants,"['', 'Required technical skills:\xa0', '-\tExpert knowledge in Python\xa0', 'Do you want to help make healthcare more effective and affordable for everyone? That’s our mission at Codoxo. The U.S. spends more on healthcare than any other country in the world, but not all of the $3.8 trillion goes to real patient care. A significant portion, up to 10% or $380 billion, is lost to fraud, waste, and abuse.', 'Codoxo’s patented artificial intelligence technology helps healthcare companies and agencies identify and act quickly to control costs. Codoxo now has six AI-powered applications that help every department across health insurance payers proactively bring down costs and reduce fraud, waste, and abuse – so more dollars to toward patient care.', '-\tExpert knowledge in SQL', 'Beneficial technical skills:\xa0', '-\tAWS Glue, RDS, S3, Aurora, Data Lake\xa0', '-\tProven experience processing billions of records\xa0', 'Job Description\xa0', 'Other Requirements/Preferences:\xa0', '-\tRelational database (PostgreSQL) programming', 'PLEASE NOTE BEFORE APPLYING: THIS ROLE IS LOCATED IN ATLANTA, GA AND THE CHOSEN CANDIDATE MUST BE LOCATED OR WILLING TO RELOCATE HERE AS WELL. IN ADDITION, CODOXO IS NOT ABLE TO OFFER SPONSORSHIP OR ACCOMMODATE ANY CANDIDATES THAT ARE CURRENTLY BEING SPONSORED NOW OR IN THE FUTURE', ""-\tBachelor or master's degree in Computer Science, Engineering or related field\xa0"", '-\tApache Spark\xa0', '-\tExperience processing medical claims or related health care data\xa0', '-\tAt least 2 years of experience in the software industry\xa0', '\xa0', '-\tCloud experience (AWS)\xa0', 'We are seeking a talented Data Engineer with experience in data analytics, building large reservoirs of data, and performing efficient queries. We have built a web-based application that is supported by a large dataset of healthcare data, on which we must frequently perform large queries very efficiently and return results in real-time to the user. The role of the data engineer is to bridge the data scientist with the developers and the ingestion of data. Specifically, one of the major tasks is to build and execute new data pipelines on the cloud (AWS).\xa0', '-\tAuthorization to work in the USA\xa0']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Sr Data Engineer,"Lowe's Companies, Inc.","Charlotte, NC",23 hours ago,Be among the first 25 applicants,"['', ' Has solid grasp of software design patterns and approaches; understands application level software architecture; makes technical trade-off decisions at application level', ' 3 years of experience working with an IT Infrastructure Library (ITIL) framework', ' Participates and coaches others in all software development end-to-end product lifecycle phases by applying and sharing an in-depth understanding of complex company and industry methodologies, policies, standards, and controls', ' Automates and simplifies team development, test, and operations processes; develops detailed architecture plans for large scale enterprise architecture projects and drives the plans to fruition', 'Job Summary', ' 5 years of experience working with defect or incident tracking software', 'Minimum Qualifications', ' 5 years of experience writing technical documentation in a software development environment', ' 5 years of experience in systems analysis, including defining technical requirements and performing high level design for complex solutions', ' Translates complex cross-functional business requirements and functional specifications into logical program designs, modules, stable application systems, and data solutions; partners with Product Team to understand business needs and functional specifications Collaborates with cross-functional teams to ensure specifications are converted into flexible, scalable, and maintainable solution designs; evaluates project deliverables to ensure they meet specifications and architectural standards Guides development teams in the design and build of complex Data or Platform solutions and ensures that teams are in alignment with the architecture blueprint, standards, target state architecture, and strategies Coordinates, executes, and participates in component integration (CIT) scenarios, systems integration testing (SIT), and user acceptance testing (UAT) to identify application errors and to ensure quality software deployment Participates and coaches others in all software development end-to-end product lifecycle phases by applying and sharing an in-depth understanding of complex company and industry methodologies, policies, standards, and controls Has solid grasp of software design patterns and approaches; understands application level software architecture; makes technical trade-off decisions at application level Automates and simplifies team development, test, and operations processes; develops detailed architecture plans for large scale enterprise architecture projects and drives the plans to fruition Solves complex architecture/design and business problems; solutions are extensible; works to simplify, optimize, remove bottlenecks, etc. Provides mentoring and guidance to more junior level engineers; may provide feedback and direction on specific engineering tasks', "" Bachelor\\'s degree in Engineering, Computer Science, CIS, or related field (or equivalent work experience in a related field)"", ' 5 years of experience in Data, BI or Platform Engineering, Data Warehousing/ETL, or Software Engineering', ' Experience working with Continuous Integration/Continuous Deployment tools', ' Solves complex architecture/design and business problems; solutions are extensible; works to simplify, optimize, remove bottlenecks, etc.', ' 5 years of experience working with source code control systems', ' Guides development teams in the design and build of complex Data or Platform solutions and ensures that teams are in alignment with the architecture blueprint, standards, target state architecture, and strategies', ' Master’s degree in Computer Science, CIS, or related field 5 years of IT experience developing and implementing business systems within an organization 5 years of experience working with defect or incident tracking software 5 years of experience writing technical documentation in a software development environment 3 years of experience working with an IT Infrastructure Library (ITIL) framework 3 years of experience leading teams, with or without direct reports 5 years of experience working with source code control systems Experience working with Continuous Integration/Continuous Deployment tools 5 years of experience in systems analysis, including defining technical requirements and performing high level design for complex solutions', 'About Lowe’s In The Community', 'About Lowe’s', ' Coordinates, executes, and participates in component integration (CIT) scenarios, systems integration testing (SIT), and user acceptance testing (UAT) to identify application errors and to ensure quality software deployment', "" Bachelor\\'s degree in Engineering, Computer Science, CIS, or related field (or equivalent work experience in a related field) 5 years of experience in Data, BI or Platform Engineering, Data Warehousing/ETL, or Software Engineering 4 years of experience working on project(s) involving the implementation of solutions applying development life cycles (SDLC)"", ' Provides mentoring and guidance to more junior level engineers; may provide feedback and direction on specific engineering tasks', ' 3 years of experience leading teams, with or without direct reports', ' Master’s degree in Computer Science, CIS, or related field', 'Key Responsibilities', ' 4 years of experience working on project(s) involving the implementation of solutions applying development life cycles (SDLC)', ' Collaborates with cross-functional teams to ensure specifications are converted into flexible, scalable, and maintainable solution designs; evaluates project deliverables to ensure they meet specifications and architectural standards', ' 5 years of IT experience developing and implementing business systems within an organization', ' Translates complex cross-functional business requirements and functional specifications into logical program designs, modules, stable application systems, and data solutions; partners with Product Team to understand business needs and functional specifications', 'Preferred Qualifications']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Cloud Engineer,"Flash Technology Group, LLC","Annapolis Junction, MD",1 day ago,Be among the first 25 applicants,"['', '2 years experience configuring system monitoring tools', 'Life and Disability Insurance', 'Etc.', '2 years experience standing up new Linux servers', 'Paid Time Off (Vacation, Sick, and Holiday time)', 'Tuition Reimbursement (can be used for professional licenses and certificates)', 'Desired: experience with NiFi, Spark, Splunk, Oracle, MariaDB, Prometheus, Grafana', 'Must have a current\xa0TS/SCI Security Clearance with active polygraph\xa0to be considered for the position.', 'Required: 5 years experience with Python development', 'Dental Insurance', 'TS/SCI Security Clearance with active polygraph', 'Experience with Cloud architectures (AWS, Azure), automation, and microservices architectures', 'Some of the benefits offered to Flash employees include:', 'Vision Insurance', 'Experience with Ansible, Docker, Kubernetes and k8s', 'DESCRIPTION:', '2 years experience with Linux system administration', 'At Flash Technology Group, we think of our employees as family. We strive to foster a work-life balance as we maintain our commitments to our customers. We like to hire professionals in our market regions (Maryland, San Antonio, TX, Virginia, Washington, DC)—so we can build relationships with each other. We understand the value of our team mates, and our goal is to nurture their success and professional growth. We consider it as important as our own. And we offer the flexibility, responsiveness, and accessibility you won’t find in a big multi-tiered corporation. (Think laid back, not buttoned up.)', 'Medical Insurance (with HSA and HRA options)', 'Referral bonuses', '401(k) with 6% company match - VESTS IMMEDIATELY', 'Complimentary AURA Identity Theft Protection', '\xa0', 'This position will support the modernization and transition of security and insider threat platforms.\xa0']",Not Applicable,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Neuberger Berman,"Chicago, IL",3 hours ago,Be among the first 25 applicants,"['', 'Integrate new software tools for data analysis into the existing tool-set', 'Set up strong foundational procedures, guidelines, and standards for data analytics and processing', 'Work closely with business stakeholders to understand their analytics, and construct efficient and scalable algorithms to implement themSet up strong foundational procedures, guidelines, and standards for data analytics and processingIntegrate new software tools for data analysis into the existing tool-setBuild automated pipelines for developing, testing, and deploying data analytics applicationsConduct ad-hoc analysis and present results in a clear mannerProcess, clean, and verify the integrity of data used for analysis', 'Experience with Azure and Snowflake is a plus.', 'Bachelor degree or equivalent in Computer Science, Data Science, or Engineering5 years of experience of MS SQL Server with knowledge of OLTP and Dimensional Modeling design and development. Familiar with MS SQL Server performance optimization techniques.Experience with T-SQL and various of MS SQL 2012+ advance programming features.Experience with Tableau \xa0reporting and analytical toolsExperience working with financial data sets a plusExperience with data integration and workflow tools.\xa0 \xa0Hands-on with large data pre-processing (ETL), and data cleansing.Experience with DevOps tool-sets such as Confluence, JIRA, and Git.Experience working within an Agile software development framework with strongly disciplined approach to software developmentExperience with Azure and Snowflake is a plus.A team-player who is eager to learn with strong analytical and communications skills', 'Experience with DevOps tool-sets such as Confluence, JIRA, and Git.', 'Experience working with financial data sets a plus', '5 years of experience of MS SQL Server with knowledge of OLTP and Dimensional Modeling design and development. Familiar with MS SQL Server performance optimization techniques.', 'Neuberger Berman is an equal opportunity/affirmative action employer. The Firm and its affiliates do not discriminate in employment because of race, creed, national origin, religion, age, color, sex, marital status, sexual orientation, gender identity, disability, citizenship status or protected veteran status, or any other characteristic protected by local, state, or federal laws, rules, or regulations.', 'Neuberger Berman is an equal opportunity/affirmative action employer. The Firm and its affiliates do not discriminate in employment because of race, creed, national origin, religion, age, color, sex, marital status, sexual orientation, gender identity, disability, citizenship status or protected veteran status, or any other characteristic protected by local, state, or federal laws, rules, or regulations.\xa0If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact onlineaccommodations@nb.com.', 'We are looking for a hands-on data developer / engineer with strong MS SQL Server experiences. The candidate requires knowledge of RDBMS modeling and development experiences as well as some data science technologies.', 'If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact onlineaccommodations@nb.com.', 'Summary:', 'Responsibilities:', 'Process, clean, and verify the integrity of data used for analysis', 'Experience with T-SQL and various of MS SQL 2012+ advance programming features.', 'Bachelor degree or equivalent in Computer Science, Data Science, or Engineering', 'Qualifications:', 'Experience with Tableau \xa0reporting and analytical tools', 'A team-player who is eager to learn with strong analytical and communications skills', 'Build automated pipelines for developing, testing, and deploying data analytics applications', 'Experience with data integration and workflow tools.\xa0 \xa0Hands-on with large data pre-processing (ETL), and data cleansing.', 'Conduct ad-hoc analysis and present results in a clear manner', 'Work closely with business stakeholders to understand their analytics, and construct efficient and scalable algorithms to implement them', 'Experience working within an Agile software development framework with strongly disciplined approach to software development']",Associate,Full-time,Finance,Financial Services,2021-03-18 14:34:51
Data Engineer,StaffQuest LLC,"Santa Clara, CA",1 week ago,108 applicants,"['', 'Strong professional experience creating production RESTful web servicesProfessional Amazon Web Services (AWS) experiencePython expertiseStrong SQL experience and database performanceStrong critical thinking and analytical skillsOutgoing personality and the willingness to take initiative to complete challenging workExcellent communication (verbal and written) skills', 'Professional Amazon Web Services (AWS) experience', 'StaffQuest is searching for a Data Engineer with Python and cloud experience that is passionate about machine learning and AI. This is a Full Time (FTE) opportunity with a small, high growth SaaS team supporting the healthcare industry. You will be responsible for collaborating with a cross functional team comprised of highly skilled designers / front end developers, product managers and data scientists. The back end data engineer will take accountability and ownership of the code base and produce design documents as well as automation friendly code. You will also perform design and code reviews, resolve production issues and ensure service is running smoothly at all times. We are seeking candidates with a passion for technology that will have a positive impact on the team and are natural leaders willing to share their knowledge base for the betterment of the team.', 'Python expertise', 'Strong critical thinking and analytical skills', 'Strong SQL experience and database performance', 'Outgoing personality and the willingness to take initiative to complete challenging work', 'Excellent communication (verbal and written) skills', 'Strong professional experience creating production RESTful web services', 'Requirements']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,OROLIA,"Rochester, NY",3 weeks ago,Be among the first 25 applicants,"['', 'Education/Experience Requirements', 'Ability to have fun! ', 'Proficient in analyzing data via SQL', 'Visualization experience using SSRS, Tableau or PowerBIProficient in writing SQL, including writing multi-table joins and calculationsProficient in analyzing data via SQLSkilled in various scripting/coding languagesStrong ability to create standard design documentatio nsuch as data flow diagrams and data dictionariesAbility to think critically with strong attention to detail while conducting analysis of dataStrong communication skills with the ability to convey complex concepts to non-technical employeesAptitude to quickly learn new things and adapt new skills Ability to act as a resource to othersAbility to have fun! ', 'Strong communication skills with the ability to convey complex concepts to non-technical employees', 'About Orolia', ' Thank You ', 'Ability to think critically with strong attention to detail while conducting analysis of data', 'By submitting your interest in this job, you agree to receive text notifications with additional steps to complete your job application. You will receive up to 6 messages from the number ""63879"". Message & data rates may apply. Please refer to our privacy policy for more information.', 'Gather and confirm user requirements for various projects', 'Ability to act as a resource to others', 'Manage and support middleware application(s) for transforming data ', 'Design, develop and manage data structures including databases and data warehousesCreate visualizations of data to meet our business needsLead efforts to standardize, improve, and implement enterprise data processes Assist with defining standards for data handling including database design standards, data clensing, validation and error handlingManage and support middleware application(s) for transforming data Script data integrationsGather and confirm user requirements for various projectsAccurately process financial and operational data to create valuable business information', 'Skilled in various scripting/coding languages', 'Apply for this Position', 'Lead efforts to standardize, improve, and implement enterprise data processes ', 'Accurately process financial and operational data to create valuable business information', 'Script data integrations', 'Aptitude to quickly learn new things and adapt new skills ', 'Assist with defining standards for data handling including database design standards, data clensing, validation and error handling', 'Design, develop and manage data structures including databases and data warehouses', 'Visualization experience using SSRS, Tableau or PowerBI', ""Knowledge, Skills & Abilities We're Looking For"", 'Create visualizations of data to meet our business needs', 'Proficient in writing SQL, including writing multi-table joins and calculations', 'Strong ability to create standard design documentatio nsuch as data flow diagrams and data dictionaries']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,"Munich Reinsurance America, Inc.","Princeton, NJ",1 week ago,77 applicants,"['1+ years of hands-on R or Python experience is highly preferred', ""Bachelor's degree (MS preferred) in Computer Science, Statistics, Math or equivalent combination of education and experience"", 'Drive and dedication, as well as creativity and hands-on attitude', 'Demonstrated ability to experiment with and learn new technologies', 'Independently perform the extraction, cleaning, transformation and loading of various data file formats, databases and cloud/web sources', 'At Munich Re US, we see Diversity and Inclusion as a solution to the challenges and opportunities all around us. Our goal is to foster an inclusive culture and build a workforce that reflects the customers we serve and the communities in which we live and work. We strive to provide a workplace where all of our colleagues feel respected, valued and empowered to achieve their very best every day. We recruit and develop talent with a focus on providing our customers the most innovative products and services. We are an equal opportunity employer. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.Qualifications / Job Requirements: ', 'Excellent analytical, problem solving and organizational skills.', 'Strong oral and written communication and interpersonal skills', 'Develop expert knowledge of data and analytics infrastructure within Munich Re', 'Experience with data visualization tools such as Power BI', ""Bachelor's degree (MS preferred) in Computer Science, Statistics, Math or equivalent combination of education and experience1-3 years of experience with statistical modeling, data modeling, data intake, and data-curation proceduresExperience in insurance industry (preferred experience working with insurance auto/mobility data)Expert SQL skills including experience with distributed and spatial queries1+ years of hands-on R or Python experience is highly preferredExperience in data pipeline development using Databricks and Azure Data FactoryExperience with data visualization tools such as Power BIDrive and dedication, as well as creativity and hands-on attitudeCuriosity in searching for new solutions outside of traditional approachesDemonstrated ability to experiment with and learn new technologiesStrong oral and written communication and interpersonal skillsExcellent analytical, problem solving and organizational skills."", 'Experience in insurance industry (preferred experience working with insurance auto/mobility data)', 'Experience in data pipeline development using Databricks and Azure Data Factory', 'Execute on projects to provide relevant datasets in support of business initiatives', '1-3 years of experience with statistical modeling, data modeling, data intake, and data-curation procedures', 'Build data pipelines for the automation of data processes', 'Munich Re America Services (MRAS) is a shared service organization that delivers services to all Munich Re US P&C Companies and other group entities.Job DescriptionAccess to increasing volumes of data, computing power, and data analytics capabilities is transforming the insurance industry and has an impact on all parts of the insurance value chain. Data is an important asset to Munich Re and there is an increasing need for data to develop and strengthen our business. The Munich Re America Services Data and Analytics team provides support across our businesses to achieve these goals. Our Data Engineering team supports the extraction, architecting, modeling, transformation, and interpretation of data across a range of business initiatives. The team works closely with our underwriting, actuarial, analytics, IT, and claims teams to create data assets and to automate and develop end-user tools that support data-driven decision-making. We are looking for a Data Engineer to join our team with a specific focus on supporting our work in the mobility space which is a growing area of emphasis where data is critical to success. This position will be responsible for executing on projects focusing on the understanding, extraction, cleaning, and transformation of data. This will include working with a variety of complex data sources, at various granular levels to build high quality datasets for analysis as well as building pipelines to automate processes and develop data architecture designs that support future growth. Through this work you will develop an understanding of the business models across a range of Munich Re companies and how data is being used to enable new underwriting, product development, and claims solutions. As the scope is broad and the business environment is dynamic, we are seeking someone with the creativity, drive, and dedication to initiate and execute new solutions. Key Responsibilities of this position include: ', 'Work with various complex databases containing data from multiple sources, at various levels of granularity to deliver high-quality data', 'Curiosity in searching for new solutions outside of traditional approaches', 'Expert SQL skills including experience with distributed and spatial queries', 'Execute on projects to provide relevant datasets in support of business initiativesIndependently perform the extraction, cleaning, transformation and loading of various data file formats, databases and cloud/web sourcesWork with various complex databases containing data from multiple sources, at various levels of granularity to deliver high-quality dataBuild data pipelines for the automation of data processesDrive data architecture design decisions considering future growthDevelop expert knowledge of data and analytics infrastructure within Munich Re', 'Drive data architecture design decisions considering future growth']",Mid-Senior level,Full-time,Information Technology,Insurance,2021-03-18 14:34:51
Data Engineer,BioSpace,"Pasadena, CA",3 days ago,Be among the first 25 applicants,"['', 'Expert in building and managing scalable relational databases, preferably in the life sciences space', 'Qualifications Include', 'Manage and improve our data lake of millions of fluorescence microscopy images', 'Experience with cloud computing services, preferably AWS (EMR, Redshift)', 'Work with our data scientists to incorporate our image processing workflow into the data pipeline', 'Build and manage our databases of billions to trillions of chemical structures, intensities, affinities, and data from other biological assays', 'Company Overview:', 'Experience and Qualifications: ', 'The Core Responsibilities Of This Job Will Be', ' Expert in engineering big data pipelines using modern technologies and cloud infrastructures Expert in building and managing scalable relational databases, preferably in the life sciences space Experience with cloud computing services, preferably AWS (EMR, Redshift) Experience with high-end distributed data processing environments (Spark, Hadoop, etc.) Proficiency in Linux environment, experience with database languages (e.g., SQL) and experience with version control practices and tools (Git) Experience with pipeline/workflow managers (Luigi, Airflow, Nextflow, etc.) Highly proficient in Python and the PyData stack (numpy, pandas, scipy, dask, etc.) ', 'Position Summary:', 'Expert in engineering big data pipelines using modern technologies and cloud infrastructures', 'Design and architect a data warehouse to support downstream analytics', 'Experience with high-end distributed data processing environments (Spark, Hadoop, etc.)', 'Experience with pipeline/workflow managers (Luigi, Airflow, Nextflow, etc.)', 'Highly proficient in Python and the PyData stack (numpy, pandas, scipy, dask, etc.)', 'Proficiency in Linux environment, experience with database languages (e.g., SQL) and experience with version control practices and tools (Git)']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer - Marketing Operations,Delta Defense LLC,"West Bend, WI",20 hours ago,Be among the first 25 applicants,"['', 'Partner with external agency on custom scripting and validation needs content targeting (personalization) campaigns and A/B split tests', 'Collaborate with Data Strategy, BI, and marketing channel leaders on data definition, planning KPIs, and/or setting up Google Analytics audiences as needed', 'Demonstrates the Core Values of Delta Defense, LLC', 'Ability to work both independently and collaboratively to meet deadlines', 'Preferred', 'Assist with mobile application performance monitoring, technical enhancements such as in-app triggers, segmentation queries and analytics.', 'Position Summary', 'Understanding of digital marketing channels and objectives', 'Execute and implement changes in Google Tag Manager as directed by Data Strategy to assist with Google Analytics measurementProgram, execute, and facilitate QA of website client side manipulations in Google Optimize 360Collaborate with Data Strategy, BI, and marketing channel leaders on data definition, planning KPIs, and/or setting up Google Analytics audiences as neededPartner with external agency on custom scripting and validation needs content targeting (personalization) campaigns and A/B split testsPartner with data strategy & I.T. product owners to maintain data integrity, tracking accuracy, and validation.Assist with site optimization planning to maximize site engagement, lead capture, and conversionAssist with mobile application performance monitoring, technical enhancements such as in-app triggers, segmentation queries and analytics.', 'Previous experience in a B2C membership organization', 'Web Analytics, A/B split testing tools, and tag management experience, preferably Google Analytics, DataStudio, regular expressions, Google Optimize, and GTM', 'Program, execute, and facilitate QA of website client side manipulations in Google Optimize 360', 'Requirements', 'Bachelor’s degree in marketing, statistics, finance, web development, or similar field', 'Experience with data visualization tools and advertising platforms (Domo, Segment, Salesforce Marketing Cloud, Google Ads, Facebook Advertising, Unbounce, etc)', 'Experience with mobile applicationsPrevious experience in a B2C membership organizationExperience with data visualization tools and advertising platforms (Domo, Segment, Salesforce Marketing Cloud, Google Ads, Facebook Advertising, Unbounce, etc)Understanding of statistical concepts (statistical significance, confidence intervals, etc)', '3+ years of experience in front-end web development, analytics, digital marketing, or a blend', 'Experience manipulating and writing code with front-end website languages (HTML, CSS, Javascript, etc) and experience with content management systems (Wordpress preferred)', 'Understanding of statistical concepts (statistical significance, confidence intervals, etc)', 'Execute and implement changes in Google Tag Manager as directed by Data Strategy to assist with Google Analytics measurement', 'Experience with mobile applications', 'Bachelor’s degree in marketing, statistics, finance, web development, or similar field3+ years of experience in front-end web development, analytics, digital marketing, or a blendWeb Analytics, A/B split testing tools, and tag management experience, preferably Google Analytics, DataStudio, regular expressions, Google Optimize, and GTMExperience manipulating and writing code with front-end website languages (HTML, CSS, Javascript, etc) and experience with content management systems (Wordpress preferred)Understanding of digital marketing channels and objectivesAbility to think creatively, problem-solve, and change direction oftenAbility to work both independently and collaboratively to meet deadlinesDemonstrates the Core Values of Delta Defense, LLC', 'Duties/Responsibilities', 'Partner with data strategy & I.T. product owners to maintain data integrity, tracking accuracy, and validation.', 'Ability to think creatively, problem-solve, and change direction often', 'Assist with site optimization planning to maximize site engagement, lead capture, and conversion']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Data Engineer I,Consulting Solutions,"Fort Worth, TX",,N/A,"['', 'This is a great opportunity to get real industry experience with a planned track to more senior roles in an established, enterprise environment. ', 'Proficient in understanding of data mapping and lineage strategies', 'Applied experience in Agile, SAFe, Scrum or Lean Six Sigma', 'Working in the Scaled Agile Framework, the Data Engineer I will use new and established automation and integration tools, the Enterprise Information Management Strategy (EIMS), frameworks, critical thinking, and problem-solving skills to partner with the First Command lines of business to rationalize data needs that are critical to our data transformation.\xa0', 'Develop and implement automated solutions to optimize efficiency and quality of First Command enterprise data and business processes;', 'BS preferred; MBA or MS or equivalent a plus.', 'Develop data pipeline automation and integration capabilities to implement the data lifecycle, considering how data is created, transformed, stored, archived, analyzed and shared in the organization;Collaborate with business stakeholders to develop an understanding of their business requirements and operational processes;Implement and act on the recommendations, outcomes and designs from Data Governance and Enterprise Architecture;Contribute to process for making data architecture decisionsComplete data exploration and profiling by leveraging data analysis, design and presentation tools;Contribute to the design, development, and maintenance of ongoing automation, integration, services, etc. to provide the right data to the right place at the right time;Develop and implement automated solutions to optimize efficiency and quality of First Command enterprise data and business processes;Partner with Information Technology and business unit groups to develop and implement comprehensive automation and business critical data which will be leveraged for solutions that provide consistent, clean and integrated data which enables business intelligence;Implement methods to include unstructured and big data, such as social media, emails, pictures, videos, voice and sensor data, client surveys and feedback.', 'Proficient in understanding of data management practices, data architecture principles, and data governance process', 'Proficient in transforming manual processes to industry standards and automation', 'Excellent written communication and presentation skills', 'US Citizens and Green Card holders will only be considered as this is for W2 basis only. All other visa types are not eligible for this position. Also, messages from third party vendors will be ignored. ', 'Responsibilities', 'Proficient in SQL', 'The Data Engineer I designs, builds and implements data integration and data services across the enterprise; specially in our Microsoft Azure cloud environment.\xa0As part of the Enterprise Data & Analytics Team, the Data Engineer I directly contributes to modernizing our data integration practices based on the Enterprise Data and Analytics strategy, Enterprise Architecture and Data Governance outcomes. ', 'Implement methods to include unstructured and big data, such as social media, emails, pictures, videos, voice and sensor data, client surveys and feedback.', 'Familiarity with data science and analytics tools such as SPSS, SAS, Tableau, PowerBI', 'Ability to quickly adapt to changing priorities and generating innovative solutions', '0-2 years of applied experience in data integration, ETL, and data management or comparable positions that handle large/complex data sets, developing automation, and fostering business partner relationships', 'Education', 'Partner with Information Technology and business unit groups to develop and implement comprehensive automation and business critical data which will be leveraged for solutions that provide consistent, clean and integrated data which enables business intelligence;', 'Responsibilities:', 'Collaborate with business stakeholders to develop an understanding of their business requirements and operational processes;', 'Complete data exploration and profiling by leveraging data analysis, design and presentation tools;', 'Financial services industry experience or other highly regulated industry experience a plus', '0-2 years of applied experience in data integration, ETL, and data management or comparable positions that handle large/complex data sets, developing automation, and fostering business partner relationshipsProficient in Databricks and/or Microsoft Azure Data Factory; SSIS and Informatica are a plusProficient in SQLExcellent written communication and presentation skillsProficient in transforming manual processes to industry standards and automationProficient in understanding of data mapping and lineage strategiesProficient in understanding in conceptual, logical and physical data designProficient in understanding of data management practices, data architecture principles, and data governance processAbility to quickly adapt to changing priorities and generating innovative solutions', 'Proficient in understanding in conceptual, logical and physical data design', 'Contribute to process for making data architecture decisions', 'Implement and act on the recommendations, outcomes and designs from Data Governance and Enterprise Architecture;', 'Required Qualifications', 'Financial services industry experience or other highly regulated industry experience a plusCertifications related to Data Integration a plusApplied experience in Agile, SAFe, Scrum or Lean Six SigmaFamiliarity with analytics solutions and data science capabilitiesFamiliarity with object-oriented programming and the software development processFamiliarity with data science and analytics tools such as SPSS, SAS, Tableau, PowerBI', 'Familiarity with analytics solutions and data science capabilities', 'Contribute to the design, development, and maintenance of ongoing automation, integration, services, etc. to provide the right data to the right place at the right time;', 'Certifications related to Data Integration a plus', 'Develop data pipeline automation and integration capabilities to implement the data lifecycle, considering how data is created, transformed, stored, archived, analyzed and shared in the organization;', 'Preferred Qualifications', 'Proficient in Databricks and/or Microsoft Azure Data Factory; SSIS and Informatica are a plus', 'Familiarity with object-oriented programming and the software development process']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
SQL Developer/Data Engineer,ELLKAY,"Elmwood Park, NJ",1 week ago,93 applicants,"['', '• Experience in HL7,CCDA preferred.', 'Interested applicants should submit a letter of interest with salary requirements and resume.', 'For more information on our company, visit www.ELLKAY.com.\xa0', '•\xa0Healthcare Domain knowledge preferred.', 'ELLKAY\xa0is a nationwide leader in healthcare connectivity, providing innovative, customizable solutions and unparalleled services for over a decade. We empower diagnostic laboratories, PM/EMR vendors, ACO and HIE companies, hospitals, and other healthcare organizations with cutting-edge technologies and solutions that improve their bottom lines.', '• Experience in database design and structure, with an emphasis on scalability.', '• Identify data elements from various database as requested by client.', ""Our 'Client-first' focus has made ELLKAY one of the most respected healthcare IT companies in the nation. We value our clients and believe that strong relationships are the foundation for a strong company, and we're dedicated to providing connectivity to the healthcare industry."", ""• A minimum of bachelor's or higher in Computer Science, Information Systems, or equivalent degree or strong industry experience."", '• A strong desire to develop new and innovative ways to improve our data storage and processing.', 'This is a full-time, onsite position at our HQ\xa0located in Elmwood Park, NJ. Remote work may be\xa0available.\xa0', '\ufeffAdditional information', 'Nice-to-Haves:', '• A minimum 5 years of professional experience in information technology.', 'ELLKAY was founded over a decade ago on the values of innovation, efficiency, and service created in a collaborative work culture. As we have grown, we are proud to still possess the same energy and passion for what we do. We strive to provide exceptional customer experiences to our clients, which begins with first employing amazing people. ELLKAY is proud to maintain a high-quality, innovative, and diverse workforce.', '• Programming experience required in any object oriented programming languages like C#, Java etc.', 'Qualifications', 'We deal with medical data and we take our work very seriously, but not ourselves. If you’re a smart, hard-working, dedicated individual who thrives in a laidback, friendly work environment, ELLKAY may be the place for you. We’re committed to attracting good people who are passionate about the work they do.', '• Extract data from Client’s machines and restoring databases.', '• A minimum 3 years of professional experience as SQL Developer or Data Engineer', '• Excellent SQL skills, with experience in building and interpreting complex queries.', 'ELLKAY', '• Working with Several Different Database Management Systems.', 'ELLKAY LLC is a Smoke-Free Workplace.\xa0', '• Contribute to knowledge management activities and promote best practices for project execution.', 'Job Description', '• Work with our clients Subject Matter Experts to obtain a greater understating of the business needs and goals.', '• Prepare and perform data analysis and transformations to align data to business rules.', '• Strong SQL skills (SQL Server, Oracle, MySQL, Postgres etc.).', '• Be very proficient with SQL Server.', 'AA/EOE.']",Associate,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Engineer (Remote),Last Call Media,"Remote, OR",1 week ago,Be among the first 25 applicants,"['', 'All Of Us At LCM Pride Ourselves On Being', 'Application Process', 'Familiarity with DevOps principles, such as GitOps and infrastructure as code.', 'An obvious drive to grow and learn from the highly-skilled team around you.', 'Able to empathize, which helps us understand the needs of the client, the customer, the product, and the team at LCM. ', 'Highly communicative', 'Infrastructure as code tools, especially Terraform.', 'Writing ETLs, particularly in Python for Apache Airflow or AWS Step functions.', 'What You’ll Bring', 'Experience and/or a desire to work remotely.', 'Technical Architecture experience.', 'Ability to work efficiently, sometimes under tight deadlines', 'Provide guidance on the collection of web analytics data (eg: Implementation advice for GTM).', 'What You’ll Do', 'Experienced with communicating directly with clients', 'Able to work independently ', 'Able to empathize, which helps us understand the needs of the client, the customer, the product, and the team at LCM. Highly communicativeAble to work independently Comfortable asking for helpExperienced with communicating directly with clientsEager and motivated to learn new conceptsA team player in a collaborative environment A fast learner', 'Last Call Media', 'Work with team members to optimize and extend an existing data warehouse.Gather new data collection requirements and implement or change ETL processes.Optimize existing ETL processes.Provide guidance on feasibility and advisability of upcoming data collection projects.Provide guidance on the collection of web analytics data (eg: Implementation advice for GTM).', 'Gather new data collection requirements and implement or change ETL processes.', 'Comfortable asking for help', 'A fast learner', 'Utilizing Google Analytics/Tag Manager to collect web analytics data.', 'Provide guidance on feasibility and advisability of upcoming data collection projects.', 'Building and optimizing data warehouses, especially Postgres databases.', 'A team player in a collaborative environment ', 'comprehensive benefits', 'Building and maintaining AWS infrastructure, including ECS containers and Lambda functions.', 'General comfort with Linux environments.Familiarity with DevOps principles, such as GitOps and infrastructure as code.Technical Architecture experience.Javascript experience.', 'A proven track record of building robust web data pipelines. 2+ years of similar experienceAn obvious drive to grow and learn from the highly-skilled team around you.Experience and/or a desire to work remotely.Ability to work efficiently, sometimes under tight deadlines', 'The Role', 'Javascript experience.', 'It’d Be Nice If You Also Had', 'Extracting data from Google Analytics/BigQuery, JSON files, and various web APIs.Utilizing Google Analytics/Tag Manager to collect web analytics data.Writing ETLs, particularly in Python for Apache Airflow or AWS Step functions.Building and maintaining AWS infrastructure, including ECS containers and Lambda functions.Building and optimizing data warehouses, especially Postgres databases.Infrastructure as code tools, especially Terraform.', 'Work with team members to optimize and extend an existing data warehouse.', '2+ years of similar experience', 'Extracting data from Google Analytics/BigQuery, JSON files, and various web APIs.', 'General comfort with Linux environments.', 'Eager and motivated to learn new concepts', 'A proven track record of building robust web data pipelines. ', 'Optimize existing ETL processes.']",Mid-Senior level,Full-time,Engineering,Computer Software,2021-03-18 14:34:51
Data Engineer – Payment Integration,SageSure,United States,2 weeks ago,59 applicants,"['', 'Ability to develop strong relationships and support resources across company functions to achieve results', 'Analyze structural requirements to support new payment processing functionality from software teams', ""What you'll be doing:"", 'Coordinate with operational and technical teams to solution-design future enhancements', 'Expert level proficiency in SQL', 'Bachelor’s in related field preferred', 'Familiar with payment processing architecture using multiple vendors and payment methods including credit/debit, ACH, and EBTMaintain end-to-end workflows for existing payment integration processes with vendors, processors, and banksMaintain and enhance existing month-end commission calculation and reporting systemTroubleshoot issues with existing integration partners as they ariseCoordinate with operational and technical teams to solution-design future enhancementsEnhance current Data Management payment processing operations to improve efficiency and strengthen stakeholder relationshipsAnalyze structural requirements to support new payment processing functionality from software teams', 'Gain insights into customer pain points, challenges, and needs for payment processing and integration', 'Troubleshoot issues with existing integration partners as they arise', 'Maintain end-to-end workflows for existing payment integration processes with vendors, processors, and banks', 'SageSure is an innovation-focused insurance and technology company specializing in underserved property markets. As the leading homeowners insurtech organization in the U.S. measured by premium and profitability, we offer more than 40 competitively priced insurance products on behalf of our carrier partners, serving 350,000+ policyholders. SageSure partners with a growing network of insurance agents and brokers in 14 coastal states. Since our founding in 2005, we have been modernizing property insurance through our market-leading online quoting and binding platform and our sophisticated modeling and scoring technology.', 'Excellent customer service and communication skills', 'Migrate processes from legacy systems to new solutions', 'Strong understanding of payment processing using 3rd party processing vendors', 'Familiar with payment processing architecture using multiple vendors and payment methods including credit/debit, ACH, and EBT', 'Insurance industry experience preferred', 'Review and manage a repository of documentation related to payment processing protocols while troubleshooting common issues, frameworks, technique, and opportunities for documentation improvement', 'Strong project manager experience', 'SageSure is looking for a resourceful Data Engineer – Payment Integration to develop and maintain software which executes payment processing to various partners. Our engineers move extremely fast, while solving interesting and challenging problems. The ideal candidate will be responsible for supporting and improving payments services and enabling a great payments experience. A successful candidate will thrive in a fast-paced environment, develop strong relationships across the company, and possess excellent communication skills.', 'Thrive in a fast-paced environment', '5+ years of experience with payment APIsExpert level proficiency in SQLStrong understanding of payment processing using 3rd party processing vendorsInsurance industry experience preferredAbility to develop strong relationships and support resources across company functions to achieve resultsExcellent customer service and communication skillsStrong project manager experienceMotivate and encourage others to actionThrive in a fast-paced environmentBachelor’s in related field preferred', 'Gain insights into customer pain points, challenges, and needs for payment processing and integrationEvaluate business value opportunities and deliver quality solutionsReview and manage a repository of documentation related to payment processing protocols while troubleshooting common issues, frameworks, technique, and opportunities for documentation improvementMigrate processes from legacy systems to new solutions', 'If shaping the future of an industry sounds exciting, you’ll love working at SageSure! We have the passion and drive of an insurtech startup with the experience and stability of an insurance company. We provide generous health benefits and perks, including a matching 401k plan, tuition reimbursement, wellness allowance, paid volunteer time off and more. We have eight office locations—Jersey City, NJ; Atlanta, GA; Chicago, IL; Tallahassee, FL; Cincinnati, OH; Houston, TX; Mt. Laurel, NJ; and Cheshire, CT—as well as numerous remote employees across the country. Come join our team of energetic, innovative problem solvers!', 'Evaluate business value opportunities and deliver quality solutions', '5+ years of experience with payment APIs', 'Maintain and enhance existing month-end commission calculation and reporting system', ""We're looking for someone who has:"", 'Support Responsibilities', '\xa0', 'Enhance current Data Management payment processing operations to improve efficiency and strengthen stakeholder relationships', 'Motivate and encourage others to action']",Associate,Full-time,Accounting/Auditing,Insurance,2021-03-18 14:34:51
Data Engineer (Healthcare Solutions),Komodo Health,"Chicago, IL",2 weeks ago,38 applicants,"['', 'Curiosity piqued? Learn more about us!', 'Cholangiocarcinoma Foundation & Komodo Health partner to fight cancer', 'Demonstrably deep experience with Python', 'We Breathe Life Into Data!', 'Leveraged relevant AWS services EC2, Lambdas, CloudFormation, etc, along with configuration management to ensure consistent deployment of infrastructural components', 'Developed high-performance, reliable data pipelines orchestrated via Airflow, using SQL and Python in Snowflake and/or Spark, to create and maintain relevant client datasets', 'Demonstrably deep experience with relevant ‘big data’ processing either via Spark or through a modern MPP database like Snowflake, ideally with experience in both', 'Met client SLAs for delivery, quality, and currency of data in Sentinel environments', 'What You Bring To Komodo', 'Understand and design for non-functional concerns such as performance, cost optimization, maintainability and developer experience.', ' Developed high-performance, reliable data pipelines orchestrated via Airflow, using SQL and Python in Snowflake and/or Spark, to create and maintain relevant client datasets Met client SLAs for delivery, quality, and currency of data in Sentinel environments Updated internal CI/CD tooling and patterns to ensure consistency with modern best practices Leveraged relevant AWS services EC2, Lambdas, CloudFormation, etc, along with configuration management to ensure consistent deployment of infrastructural components Partnered with product managers and Customer Success team members to understand the variety of internal and external needs for tooling and configuration and helped to define usable requirements Ensured non-functional requirements are met, such as around developer experience and maintainability of the code base Set a high technical standard overall and be a mentoring resource for others on the team and in the larger organization ', 'Expertise with industry standard distributed systems (ie. Spark), data pipeline tools (ie. Airflow) and/or databases (ie. Snowflake, PostgreSQL). Capable of quickly building expertise on an as-need basis on new tech stack.', ' Komodo Health Acquires Mavens, Series D funding led by ICONIQ Growth Wealthfront: The 2021 Career Launching Companies List In Conversation with Dr. Arif Nathoo, Komodo Health’s CEO Komodo Health’s ‘Meet a Dragon’ series Komodo’s Values that Drive our Culture Cholangiocarcinoma Foundation & Komodo Health partner to fight cancer', 'Komodo’s Values that Drive our Culture', 'Looking back on your first 12 months at Komodo Health, you will have…', 'Demonstrably deep experience with CI/CD tools and practices in a containerized AWS environment, from deployment pipelines (Jenkins, etc), infrastructure definition (Terraform, Cloudformation, etc), and configuration management via Docker', 'Ensured non-functional requirements are met, such as around developer experience and maintainability of the code base', 'Set a high technical standard overall and be a mentoring resource for others on the team and in the larger organization', 'Significant experience optimizing data retrieval processes supporting API output, ideally within a low query volume / high data volume environment.', ' Expertise with industry standard distributed systems (ie. Spark), data pipeline tools (ie. Airflow) and/or databases (ie. Snowflake, PostgreSQL). Capable of quickly building expertise on an as-need basis on new tech stack. Significant experience optimizing data retrieval processes supporting API output, ideally within a low query volume / high data volume environment. Demonstrably deep experience with Python Demonstrably deep experience with relevant ‘big data’ processing either via Spark or through a modern MPP database like Snowflake, ideally with experience in both Demonstrably deep experience with CI/CD tools and practices in a containerized AWS environment, from deployment pipelines (Jenkins, etc), infrastructure definition (Terraform, Cloudformation, etc), and configuration management via Docker Understand and design for non-functional concerns such as performance, cost optimization, maintainability and developer experience. Strong communication with engineers, product managers, and salespeople. ', 'Updated internal CI/CD tooling and patterns to ensure consistency with modern best practices', 'Komodo Health Acquires Mavens, Series D funding led by ICONIQ Growth', 'Wealthfront: The 2021 Career Launching Companies List', 'In Conversation with Dr. Arif Nathoo, Komodo Health’s CEO', 'Partnered with product managers and Customer Success team members to understand the variety of internal and external needs for tooling and configuration and helped to define usable requirements', 'Strong communication with engineers, product managers, and salespeople.', 'Komodo Health’s ‘Meet a Dragon’ series', 'The Opportunity at Komodo Health']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Amiga Informatics,"Northridge, CA",3 days ago,Be among the first 25 applicants,[''],Entry level,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Engineer,Systematix,United States,1 week ago,186 applicants,"['', 'We are Systematix and we are currently looking for a\xa0Data and Machine Learning Engineer\xa0with strong data pipeline skills to fill a contract role with one of our key clientele.', 'ABOUT THE PROJECT:', 'jobs@systematix.com.', 'Excellent analytical, problem solving and decision making skills.', 'Proven technical skills and a team player, with strong collaboration skills.', 'Specifically, your responsibilities in this role are:', 'Data and Machine Learning Engineer', 'Organizing, cleaning, and analyzing disparate data sets and find patterns and attributes that could be used to build machine learning models', 'Systematix, we pride ourselves in taking care of our consultant partnerships - by doing the little things that matter - like taking the time to get to know you, knowing when an opportunity is the perfect fit, informing you every step of the way, and building the foundation for long term relationships. We do the same with out clients, concerning ourselves more with the project than the position, so that we have the information you need to ensure an opportunity is right for you. We only work with people who enjoy making a difference.', 'Proficiency in Python software development and one or more programming tools (Pandas, Numpy...)', 'Integrating access management tools which allow data access to be configured at the user level', 'Designing and generating data flows as well as expanding and optimizing our current data pipeline architecture', 'At Systematix, we bring people and projects together!', 'Proven experience working in a cross-functional team of technical and non-technical personnel', 'If you are interested in finding out more please contact us or submit your resume to\xa0', 'ABOUT THE ROLE:', 'ABOUT THE REQUIRED SKILLS:', 'Written and verbal technical communication skills with an ability to present complex technical information in a clear and concise manner to a variety of audiences', 'Collaborating with subject matter experts and software engineers to define and implement suitable data platforms to support exploratory work as well as deliver data science products to business users', 'Experience with data Integration for Business Intelligence tools, such as Power BI, preferred', 'This is a 6 month remote contract position.', 'APPLY NOW:', 'Degree in Computer Science or Engineering.', 'If you are interested in finding out more please contact us or submit your resume to\xa0jobs@systematix.com.', 'Generating proofs-of-concept with exploratory data sets and models to demonstrate business value and define development roadmaps for future work', 'Implementing mock-up end-user interface for reporting and data visualization', 'ABOUT SYSTEMATIX:', 'Experience with NLP/text analysis, preferred', 'Experience in creating and maintaining relational databases, SQL/NoSQL, as well as key-value stores', 'Degree in Computer Science or Engineering.Proficiency in Python software development and one or more programming tools (Pandas, Numpy...)Experience in acquiring, cleaning, and structuring data from multiple sources, ETL processes, data imputationExperience in creating and maintaining relational databases, SQL/NoSQL, as well as key-value storesExperience with building data or machine learning pipelineWritten and verbal technical communication skills with an ability to present complex technical information in a clear and concise manner to a variety of audiencesExperience with NLP/text analysis, preferredExperience with data Integration for Business Intelligence tools, such as Power BI, preferredExperience with AWS technology stack, preferredExcellent analytical, problem solving and decision making skills.Proven technical skills and a team player, with strong collaboration skills.Proven experience working in a cross-functional team of technical and non-technical personnel', 'Experience in acquiring, cleaning, and structuring data from multiple sources, ETL processes, data imputation', 'Experience with building data or machine learning pipeline', 'Our client is one of the world’s leading Life Sciences organizations, with offices worldwide.\xa0Due to its ever-expanding technology footprint, providing best in class software solutions for the equipment they design and manufacture, the future of its development technology roadmap needs a guiding hand.', 'This is a contract position, where you will be a significant contributor to their product development team, working in tandem with their development group.\xa0As Data Engineer, you will be tasked with the design and development of data pipeline.', 'Organizing, cleaning, and analyzing disparate data sets and find patterns and attributes that could be used to build machine learning modelsGenerating proofs-of-concept with exploratory data sets and models to demonstrate business value and define development roadmaps for future workDesigning and generating data flows as well as expanding and optimizing our current data pipeline architectureImplementing mock-up end-user interface for reporting and data visualizationCollaborating with subject matter experts and software engineers to define and implement suitable data platforms to support exploratory work as well as deliver data science products to business usersIntegrating access management tools which allow data access to be configured at the user level', 'Experience with AWS technology stack, preferred']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,"Coalition, Inc.","Austin, TX",4 weeks ago,56 applicants,"['', 'Bonus Points', 'In-depth knowledge of AWS or other cloud-hosted platforms relevant to data engineering', 'Excellent oral and written communications skills at all levels', 'Why Coalition?', 'Implement risk models for various insurance products', 'About Us', 'Responsibilities', 'Expert-level knowledge of SQL, Python, R, or similar language used for data engineering', 'Bachelor’s degree in Computer Science or a related field preferred', 'Evaluate, recommend, and implement data pipelines for a variety of data sources used at Coalition', 'Deep understanding of ETL pipelines, statistical modeling, data analytics, and large scale data streaming', 'Requirements', '3+ years working with large disparate data sets', 'A proven track record of successfully automating business value from data insights', 'Explore new data sources and develop insights into existing data sources that improve business efficiency', 'Prior experience with insurance or network security technologies', 'Coalition Engineering', 'Recent press releases:', 'Experience with at least one big data search tool, such as Elastic', 'Experience with data visualization technologies', 'Deliver production-quality software implementations for ETL and streaming pipelines']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer Tech Lead (open to remote),Mission Lane,"Richmond, VA",3 weeks ago,67 applicants,"['', 'You Get Bonus Points For', ""Mission Laners are learners and here's what we know for certain:"", '5+ years of experience with SQL and databases such as Snowflake, Redshift, Postgres', ' Bachelor’s degree in Computer Science, Computer Engineering or related experience 5+ years of experience as a Data Engineer including building ETL/data pipelines as well as data visualization and BI solutions 5+ years of experience working in a fast-paced environment; continuous deployment, test-driven development, agile methodologies 5+ years experience using SQL and advanced SQL techniques 5+ years experience building robust, highly available, and scalable services 5+ years experience building and deploying services in the cloud Demonstrated track record of scaling data warehouse solution, meeting SLAs, and expertise in performance analysis Demonstrated strategic thinking and ability to anticipate the downstream costs of critical engineering decisions ', 'Are you passionate about making a meaningful impact?', 'Author, review and approve data requirements and designs of the data warehousing and data engineering domain including ETL/data movement and pipelines, business intelligence, and analytics', '5+ years experience using SQL and advanced SQL techniques', 'As The Principal Data Engineer (Tech Lead), You Will', ':', 'Build data pipelines leveraging internal and external data sources to meet business objectives', 'What does this mean exactly?', '5+ years experience with database architecture, design and modeling and working with a variety of data warehousing systems', 'fair and clear ', '5+ years of experience working in a fast-paced environment; continuous deployment, test-driven development, agile methodologies', '. ', 'Identify and develop governance controls for systems within the data warehousing and data engineering domains and encourage the team to develop controls as well', 'Mission Lane is an Equal Opportunity Employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex, race, color, age, national origin, religion, physical and mental disability, genetic information, marital status, sexual orientation, gender identity/assignment, citizenship, pregnancy or maternity, protected veteran status, or any other status prohibited by applicable national, federal, state or local law.Mission Lane is not currently sponsoring new applicant employment authorization for this position. And please, no third party recruiters.', 'Work directly with Product Owners and end-users to develop solutions in a highly collaborative and agile environment', 'Assist in the definition of data management processes related to data governance, stewardship, data quality, training, data retention, etc.', ' 5+ years experience building consumer-facing products 5+ years of experience with SQL and databases such as Snowflake, Redshift, Postgres 5+ years experience working with ETL tools such as Matillion, Informatica, Talend, Alooma, or comparable experience with Python, Java, Scala, Hadoop or Spark 5+ years of experience with Business Intelligence and Visualization tools such as Chartio, Tableau, etc. 5 + years of experience working with automated build and continuous integration systems 5+ years experience with database architecture, design and modeling and working with a variety of data warehousing systems ', 'Assist in defining and implementing standards and best practices within the data engineering domain that are communicated and leveraged by all data team members and other users of the platform and tools. Standards and best practices include datasets, tables, columns, scripts, variables, etc.', 'Work with business units to understand business objectives and how to leverage our data platform and tools to achieve those objectives', 'Are you purpose-driven, performance-oriented, and principles-led?', 'Demonstrated track record of scaling data warehouse solution, meeting SLAs, and expertise in performance analysis', '5+ years experience building robust, highly available, and scalable services', 'Demonstrated strategic thinking and ability to anticipate the downstream costs of critical engineering decisions', 'Quickly', '5+ years of experience with Business Intelligence and Visualization tools such as Chartio, Tableau, etc.', '5+ years experience building consumer-facing products', 'Define how systems should be maintained, including tools to be used for monitoring, thresholds, and runbooks and actively work on refining on-call practices', 'candidates don’t need good credit to apply.', '5+ years experience working with ETL tools such as Matillion, Informatica, Talend, Alooma, or comparable experience with Python, Java, Scala, Hadoop or Spark', 'Mission Lane is not currently sponsoring new applicant employment authorization for this position. And please, no third party recruiters.', '5+ years of experience as a Data Engineer including building ETL/data pipelines as well as data visualization and BI solutions', ' And we have only just begun.', ' Author, review and approve data requirements and designs of the data warehousing and data engineering domain including ETL/data movement and pipelines, business intelligence, and analytics Build data pipelines leveraging internal and external data sources to meet business objectives Provide in architecting data solutions that will capture, manage, process and serve small to large data at scale Identify and develop governance controls for systems within the data warehousing and data engineering domains and encourage the team to develop controls as well Assist in defining and implementing standards and best practices within the data engineering domain that are communicated and leveraged by all data team members and other users of the platform and tools. Standards and best practices include datasets, tables, columns, scripts, variables, etc. Assist in the definition of data management processes related to data governance, stewardship, data quality, training, data retention, etc. Define how systems should be maintained, including tools to be used for monitoring, thresholds, and runbooks and actively work on refining on-call practices Ensure that testing is performed and documented to ensure the quality of the work being delivered Work directly with Product Owners and end-users to develop solutions in a highly collaborative and agile environment Work with business units to understand business objectives and how to leverage our data platform and tools to achieve those objectives ', '5 + years of experience working with automated build and continuous integration systems', 'Enter Mission Lane', 'fee harvesters ', '5+ years experience building and deploying services in the cloud', 'To set you up for success in this role from day one, Mission Lane is looking for candidates who have', 'Bachelor’s degree in Computer Science, Computer Engineering or related experience', 'Mission Lane is an Equal Opportunity Employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex, race, color, age, national origin, religion, physical and mental disability, genetic information, marital status, sexual orientation, gender identity/assignment, citizenship, pregnancy or maternity, protected veteran status, or any other status prohibited by applicable national, federal, state or local law.', 'Ensure that testing is performed and documented to ensure the quality of the work being delivered', 'Provide in architecting data solutions that will capture, manage, process and serve small to large data at scale']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Shift4 Payments,United States,1 week ago,Be among the first 25 applicants,"['', 'We are looking for a Data Engineer to join our Information Technology Group. This position will engage in an Agile-based SDLC to complete data layer requirements (includes but not limited to table creation, stored procedure creation and updates, ETL processes) as well as acquire a deep understanding of business processes and flows to assist with reporting / business intelligence tasks. The Data Engineer will work in a team comprised of front / backend developers, business analysts, QA engineers, and will follow the Project Manager’s lead for sprint goals and deliverables.', '3+ years of experience with SQL Server.', 'Experience with an Agile SDLC a plus.', 'Bachelor of Science degree and/or relevant work experience', 'Working with stakeholders and Technology Group to fine tune outputs to ensure all requirements are met for reports', 'Validating data, performing cross reference checking between source data and outputs', 'Working closely on related issues with internal business units', 'Refactoring existing code based on existing DBs and modifying it for newer models', 'Basic VB & C# experience a plus.', ""Bachelor's degree in Computer Science or equivalent work experience3+ years of experience with SQL Server.Experience working with API build-outs and data layer requirements Experience with SSIS and SSRS essential. SSAS preferred but not required.Thorough understanding of SDLC and how it pertains to the database.Experience with Jira and ConfluenceExcellent written communication skills, particularly in the realm of technical documentationBasic VB & C# experience a plus.Experience with an Agile SDLC a plus.Ability to communicate technical issues to all levels"", 'Ability to communicate technical issues to all levels', 'Responsibilities:', 'We are looking for individuals that are extremely self-sufficient, available to work flexible hours, & hold themselves to the highest standards of professionalism. We will be evaluating candidates based on how they interview, prior experiences, functional knowledge, and references.', 'Excellent written communication skills, particularly in the realm of technical documentation', 'Shift4 Payments provides\xa0equal employment opportunities\xa0(EEO) to all employees and applicants for\xa0employment\xa0without regard to race, color, religion, sex, national origin, age, disability or genetics.', 'Experience with SSIS and SSRS essential. SSAS preferred but not required.', 'Qualifications:', 'Job Summary:', 'without regard to race, color, religion, sex, national origin, age, disability or genetics.', ""Bachelor's degree in Computer Science or equivalent work experience"", 'Shift4 Payments is the leader in secure payment processing solutions. The company’s groundbreaking technologies help power the top software providers in numerous verticals, including hospitality, retail, F&B, e-commerce, lodging, gaming, and many more. Shift4’s family of software brands includes Harbortouch, Restaurant Manager, POSitouch, and Future POS — with additional integrations to 300+ POS/PMS systems across every industry. With an expansive global footprint that includes eight offices across the U.S. and Europe and over 8,000 sales partners, the company securely processes more than a billion transactions annually for nearly 200,000 businesses, representing over $100 billion in payments each year. For additional information, visit www.shift4.com.', 'Thorough understanding of SDLC and how it pertains to the database.', 'Helping design, develop, debug and optimize stored procedures and views producing data suitable for reporting purposes', 'Data Engineer', 'Ensuring code standards are followed on all code', 'Experience working with API build-outs and data layer requirements ', 'Helping design, develop, debug and optimize stored procedures and views producing data suitable for reporting purposesEnsuring code standards are followed on all codeRefactoring existing code based on existing DBs and modifying it for newer modelsValidating data, performing cross reference checking between source data and outputsWorking with stakeholders and Technology Group to fine tune outputs to ensure all requirements are met for reportsWorking closely on related issues with internal business units', '\xa0', 'Shift4 Payments provides\xa0equal employment opportunities\xa0(EEO) to all employees and applicants for\xa0employment', 'Education:', 'Company Background:', 'Experience with Jira and Confluence']",Mid-Senior level,Full-time,Information Technology,Financial Services,2021-03-18 14:34:51
People Data Engineer,Micron Technology,"Boise, ID",6 hours ago,Be among the first 25 applicants,"['', 'Understanding of MS SQL server database, SQL queries and stored procedures', 'Oversee and test reports, applications, and websites regularly to ensure consistency.', 'Bachelor’s Degree in Computer Science, Web Design, and/or Information Systems or equivalent experience required.', 'IIS, FTP, Notepad++, MS Outlook, Excel', 'Fixes technical problems.', 'About Us', 'Analyze business needs to understand the problem.', 'Identify the most effective way for users to interact with a website.', 'Partner with the People Organization’s Service and Program Owners to ensure information and logic is up-to-date and accurate.', 'Plan, develop, and lead custom Business Intelligence solutions that complement existing functionality.', 'Perform day-to-day maintenance on HR’s custom solutions.', 'A Successful Candidate Will', 'Designing Responsive Web Design User Interface (UI) or Web API design', 'Analyze web and other software solution options.', 'Education', 'Communicate complex data structures, calculations, and metrics in simple terms.', 'Micron Benefits', 'Be familiar with existing 3rd-party HR systems, specifically Workday (with Prism), SAP, SuccessFactors and/or Visier. Prior experience with Advanced Workday Reports is preferred.Plan, develop, and lead custom Business Intelligence solutions that complement existing functionality.Analyze business needs to understand the problem.Communicate complex data structures, calculations, and metrics in simple terms.Analyze web and other software solution options.Define development strategy and requirements based on business objectives and user needs.Drive development of innovative solutions for content presentation and user experience.Identify the most effective way for users to interact with a website.Ensure integrity and quality for source code by identifying and fixing bugs.Perform day-to-day maintenance on HR’s custom solutions.Partner with the People Organization’s Service and Program Owners to ensure information and logic is up-to-date and accurate.Fixes technical problems.Oversee and test reports, applications, and websites regularly to ensure consistency.Evaluate solution alternatives to integrate data from various sources for reporting and dashboards.', 'Drive development of innovative solutions for content presentation and user experience.', 'Define development strategy and requirements based on business objectives and user needs.', 'Evaluate solution alternatives to integrate data from various sources for reporting and dashboards.', 'Designing reports and dashboards using tools such as Microsoft Power BI, Tableau, and/or Qlik Sense.', 'Experience', 'Knowledge in Web Design principles and design layout', '5+ years of experience in Business Intelligence, software engineering, and/or system architecture', 'Ensure integrity and quality for source code by identifying and fixing bugs.', 'Micron’s vision is to transform how the world uses information to enrich life for all. ', 'About Micron Technology, Inc.', 'Be familiar with existing 3rd-party HR systems, specifically Workday (with Prism), SAP, SuccessFactors and/or Visier. Prior experience with Advanced Workday Reports is preferred.', 'Microsoft Visual Studio or Report Builder', 'Job Description', '5+ years of experience in Business Intelligence, software engineering, and/or system architectureAt least 3-5 years solid hands-on experience of the following.Designing reports and dashboards using tools such as Microsoft Power BI, Tableau, and/or Qlik Sense.Designing Responsive Web Design User Interface (UI) or Web API designKnowledge in Web Design principles and design layoutMicrosoft Visual Studio or Report BuilderUnderstanding of MS SQL server database, SQL queries and stored proceduresIIS, FTP, Notepad++, MS Outlook, Excel', 'At least 3-5 years solid hands-on experience of the following.']",Mid-Senior level,Full-time,Human Resources,Computer Hardware,2021-03-18 14:34:51
Data Engineer,ThoughtSpot,"Sunnyvale, CA",7 days ago,28 applicants,"['', ' 2+ years of experience in data science, data engineering or analytics roles Strong communication skills and ability to manage and lead cross-functional initiatives Experience designing ETL pipelines using tools like Alteryx, Talend, Fivetran Understanding of data models, data architecture, and experience building data pipelines Experience working with Airflow, Python, PySpark, Docker preferred Working experience with scripting languages like Python Experience working with Snowflake or other Data Warehouses Advanced knowledge of Postgres relational databases ', 'Work hands-on in the data, using Snowflake, ThoughtSpot, Python and other tools to store, extract, analyze, and visualize data', 'Strategically advance our analytics efforts by surfacing challenges and opportunities to leadership and recommending solutions', '2+ years of experience in data science, data engineering or analytics roles', 'Strong communication skills and ability to manage and lead cross-functional initiatives', ' Work hands-on in the data, using Snowflake, ThoughtSpot, Python and other tools to store, extract, analyze, and visualize data Develop ETL pipelines from data sources like Mixpanel, Salesforce, Pardot, and Google Analytics Provide insight into sales and marketing performance through attribution and forecast analysis Act as a critical partner and collaborate closely with sales, marketing, finance, support, and other business teams to arm them with the data and deep insights they need to make great decisions Collaborate with stakeholders across the organization to establish the highest business priorities and opportunities by measuring expected reach and impact for operations focused teams Strategically advance our analytics efforts by surfacing challenges and opportunities to leadership and recommending solutions ', 'Required Skills/Qualifications', 'Act as a critical partner and collaborate closely with sales, marketing, finance, support, and other business teams to arm them with the data and deep insights they need to make great decisions', 'Responsibilities', 'Experience working with Airflow, Python, PySpark, Docker preferred', 'Collaborate with stakeholders across the organization to establish the highest business priorities and opportunities by measuring expected reach and impact for operations focused teams', 'Working experience with scripting languages like Python', 'Experience designing ETL pipelines using tools like Alteryx, Talend, Fivetran', 'Advanced knowledge of Postgres relational databases', 'Develop ETL pipelines from data sources like Mixpanel, Salesforce, Pardot, and Google Analytics', 'This role is not a remote only opportunity, once the Stay In Place order is lifted, the candidate will be expected to come into the office as the needed for the role.', 'Experience working with Snowflake or other Data Warehouses', 'About ThoughtSpot', 'Provide insight into sales and marketing performance through attribution and forecast analysis', 'For more information please visit thoughtspot.com.', 'Understanding of data models, data architecture, and experience building data pipelines']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Senior Data Engineer,Insticator,United States,2 days ago,Be among the first 25 applicants,"['', 'Transparent & communicative, patient', 'The Insticator Values', 'Collaborative mindset and great teamwork skills', 'Creative confidenceCollaborative mindset and great teamwork skillsSkilled at receiving feedback, as well as providing itEntrepreneurial & adaptable; great learning skillsTransparent & communicative, patientCurious, research-minded, data-informed', 'Sleeves Up\xa0- At Insticator we provide the autonomy and creativity needed to own your role, iterate where needed and drive impact on a massive scale.', 'Advanced working SQL knowledge and working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.', 'Skilled at receiving feedback, as well as providing it', '100% Viewability\xa0- Insticator is passionate about open feedback at all levels of the company. This allows us to fail fast, create in real time and build an open company culture.', 'Paid Time Off', 'Experience with data pipeline and workflow management tools: Matiliion, Azkaban, Luigi, Airflow, etc', 'Sleeves Up', 'Build the infrastructure required for optimal Extraction, Transformation, and Loading (ETL) of data from a wide variety of data sources with SQL-centric ETL paradigmIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Support development of analytics tools that utilize the data pipeline to provide actionable insights into conversion, customer acquisition, operational efficiency and other key business performance metricsWork with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needsCreate system standards to maintain the integrity and security of all databases.Monitor, track and identify data quality issues and gaps and apply the necessary action plans to remediate issues.', '5+ years of hands-on experience with creating and maintaining optimal data pipeline architecture and data warehouse, assembling large, complex data sets that meet requirements in a business environment\xa0', 'Annual Performance Bonus', 'Create system standards to maintain the integrity and security of all databases.', 'Unconditional Empathy\xa0- Our customers are real people with real business needs, and we are here to listen and tackle accordingly. If we care and respect each other, there is no challenge we can’t overcome.', 'About the Role', 'Flexible work schedule', 'Experience building and optimizing ‘big data’ data pipelines, architectures and datasets. (Apache Hadoop, MapReduce, Hive, Spark, Kafka, etc.)', 'Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs', 'Benefits', 'Insticator is the global leader in increasing engagement for Publishers through interactive content and community-building. Our suite of engagement products empowers Publishers and Users alike to amplify their voices and express their opinions in safe, interactive environments. From our human-moderated Commenting Unit that facilitates healthy, respectful discourse, to our Content Engagement Unit that enables audiences to share their opinions and interact with content that speaks directly to them, Insticator reaches over 350 million consumers monthly across its vast network of premium publishing partners including Ancestry, WebMD, Fox Sports, RealClear Media Group, Newsmax, and more.\xa0', 'Sleeves Up\xa0- At Insticator we provide the autonomy and creativity needed to own your role, iterate where needed and drive impact on a massive scale.100% Viewability\xa0- Insticator is passionate about open feedback at all levels of the company. This allows us to fail fast, create in real time and build an open company culture.Be Defiantly Great\xa0- We are defiant, that’s in our lifeblood, we accomplish what other people think are impossible. Challenging the status quo is our lifeblood.Unconditional Empathy\xa0- Our customers are real people with real business needs, and we are here to listen and tackle accordingly. If we care and respect each other, there is no challenge we can’t overcome.', 'We sponsor H1B Visas and Green Cards', 'Entrepreneurial & adaptable; great learning skills', 'Be Defiantly Great\xa0- We are defiant, that’s in our lifeblood, we accomplish what other people think are impossible. Challenging the status quo is our lifeblood.', 'Excellent social skills with proven ability to overcome objections and form trusting relationships with external clients and internal stakeholders', 'Strong Experience with AWS cloud services: EC2, EMR, RDS, Redshift', 'Competitive Salary', 'Experience with NoSQL databases, including MongoDB and ElasticSearch.', '100% Viewability', 'Build processes supporting data transformation, data structures, metadata, schema design, dependency and workload management for structured and unstructured datasets', 'Build pipeline for data visualization tools (Looker, Tableau, Sisense, etc.)', 'Qualifications', 'Professional Competencies', 'Monitor, track and identify data quality issues and gaps and apply the necessary action plans to remediate issues.', 'Experience in building systems using Python, R, Tensorflow, Linux/Unix Bash Scripts. Code must be fault tolerant/resistant', 'Understand the principles of ad serving, analytics, programmatic, RTB / DSPs / SSPs / DMPs', '5+ years of hands-on experience with creating and maintaining optimal data pipeline architecture and data warehouse, assembling large, complex data sets that meet requirements in a business environment\xa0Advanced working SQL knowledge and working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.Experience with NoSQL databases, including MongoDB and ElasticSearch.Experience building and optimizing ‘big data’ data pipelines, architectures and datasets. (Apache Hadoop, MapReduce, Hive, Spark, Kafka, etc.)Build processes supporting data transformation, data structures, metadata, schema design, dependency and workload management for structured and unstructured datasetsProven successful record of manipulating, processing and extracting value from large disconnected datasetsHands-on experience in end-to-end data product developing & implementing cycle and working with cross-functional teams in a dynamic environment.Build pipeline for data visualization tools (Looker, Tableau, Sisense, etc.)Experience in building systems using Python, R, Tensorflow, Linux/Unix Bash Scripts. Code must be fault tolerant/resistantStrong Experience with AWS cloud services: EC2, EMR, RDS, RedshiftExperience with SnowflakeStrong communication skills with a proven ability to discuss data, infrastructure, and analytics with technical & non-technical across the organization\xa0', 'About the Company', 'Cultural Competencies', 'Stock Options (so you have ownership in the company and benefit as it grows)', 'Preferred Experience', 'Support development of analytics tools that utilize the data pipeline to provide actionable insights into conversion, customer acquisition, operational efficiency and other key business performance metrics', 'Creative confidence', 'Responsibilities and Duties', 'Experience with Snowflake', 'Unconditional Empathy', 'Health, Dental and Vision Insurance (location dependent)', 'Working knowledge of AdTech and audience Data Management Platform (DMP)Understand the principles of ad serving, analytics, programmatic, RTB / DSPs / SSPs / DMPsExperience with data pipeline and workflow management tools: Matiliion, Azkaban, Luigi, Airflow, etcExperience with stream-processing systems: Storm, Spark-Streaming, etc.Excellent social skills with proven ability to overcome objections and form trusting relationships with external clients and internal stakeholders', 'Strong communication skills with a proven ability to discuss data, infrastructure, and analytics with technical & non-technical across the organization\xa0', 'Working knowledge of AdTech and audience Data Management Platform (DMP)', '401k (only in USA)', 'Experience with stream-processing systems: Storm, Spark-Streaming, etc.', 'This is a remote position that can be based anywhere however the right candidate must be able to work US Eastern business hours. The Senior Data Engineer will report to our CTO and be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or re-designing our company’s data architecture to support data scientists in building our next generation of products into an innovative industry leader.', 'We recruit, promote, and reward based off of our four core values:', 'Build the infrastructure required for optimal Extraction, Transformation, and Loading (ETL) of data from a wide variety of data sources with SQL-centric ETL paradigm', '\xa0', 'Curious, research-minded, data-informed', 'We offer a diverse package and the chance to grow financially with the company, including:', 'Competitive SalaryHealth, Dental and Vision Insurance (location dependent)Annual Performance BonusPaid Time OffStock Options (so you have ownership in the company and benefit as it grows)Flexible work schedule401k (only in USA)We sponsor H1B Visas and Green Cards', 'Hands-on experience in end-to-end data product developing & implementing cycle and working with cross-functional teams in a dynamic environment.', 'Proven successful record of manipulating, processing and extracting value from large disconnected datasets', 'Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.', 'Be Defiantly Great']",Mid-Senior level,Full-time,Engineering,Internet,2021-03-18 14:34:51
Data Engineer,McGraw Hill,"Boston, MA",2 days ago,57 applicants,"['', 'Experience with agile engineering practices.', 'Work effectively with Technical Product Management and SCRUM masters to meaningfully contribute to our agile team.', 'Strong understanding of data modeling concepts, including schema development, validation, and evolution (normalized and denormalized).', 'Data Engineer, ', 'Build the Future', 'Having coding experience with a modern development language (Scala, Python, Java).', 'What can you bring to the role?', 'Your impact on the team', 'Identify gaps and proactively improve system service level agreements.', 'Provide technical knowledge sharing and coaching to engineers on the development team.', 'Experience with Apache Spark.', 'Other Locations', 'Data Integration team ', 'What can you expect from the position?', 'At least 5 years of experience with ETL data processing concepts with full implementation cycle experience in enterprise data marts, including advanced SQL development skills.', 'At least 3 years of Architecture and Optimization experience for database systems technologies with a focus on data marts and data warehouses.', 'Contribute to complex solution designs, hands-on software development goals, new tool and framework creation, and code reviews.', 'Experience with performance tuning and scaling production databases.']",Entry level,Full-time,Quality Assurance,Education Management,2021-03-18 14:34:51
Data Engineer,Verizon,"Jacksonville, FL",3 weeks ago,25 applicants,"['', 'Experience working in a network operations center environment.', 'Bachelor’s degree or four or more years of work experience.', 'Equal Employment Opportunity', 'Explore suitable options and designs for specific analytical solutions.', 'Experience knitting disperate data sources together', 'Master’s degree in Computer Science, Engineering, Statistics, IT, or related field.', 'Diversity and Inclusion at Verizon', 'When you join Verizon', 'Experience on Big Data platforms (i.e., Hadoop, Map/Reduce, Spark, HBase, CouchDB, Hive, etc.)', 'Gather requirements, assess gaps, and build roadmaps and architectures to help the analytics driven organization achieve its goals.Work closely with Data Analysts to ensure data quality and availability for analytical modelling.Explore suitable options and designs for specific analytical solutions.Define extract, load, and transform (ELT) based on jointly defined requirements.Prepare, clean, and massage data for use in modeling and prototypesIdentify gaps and implement solutions for data security, quality, and automation of processes.Support maintenance, bug fixes and, performance analysis along data pipeline.', 'What You’ll Be Doing...', 'Define extract, load, and transform (ELT) based on jointly defined requirements.', 'Four or more years of experience building data pipelines', 'Even Better If You Have', 'Work closely with Data Analysts to ensure data quality and availability for analytical modelling.', 'Identify gaps and implement solutions for data security, quality, and automation of processes.', 'Bachelor’s degree or four or more years of work experience.Four or more years of experience as a data engineerFour or more years of experience finding, cleaning, and preparing data for use by Data ScientistsExperience knitting disperate data sources togetherFour or more years of experience building data pipelinesExperience using SQL (i.e., PL/SQL or T-SQL with RDBMSs like Teradata, MS SQL Server, Oracle, etc.)Experience in data engineering, databases, and data warehouses.Strong experience with data engineering in Python.Ability to travel occasionally', ""You'll Need To Have"", 'Support maintenance, bug fixes and, performance analysis along data pipeline.', 'Gather requirements, assess gaps, and build roadmaps and architectures to help the analytics driven organization achieve its goals.', 'Prepare, clean, and massage data for use in modeling and prototypes', 'Four or more years of experience finding, cleaning, and preparing data for use by Data Scientists', 'Experience as an open source Contributor.', 'Master’s degree in Computer Science, Engineering, Statistics, IT, or related field.Experience with Scala, Julia, R, Python or other machine learning programming languageExperience on Big Data platforms (i.e., Hadoop, Map/Reduce, Spark, HBase, CouchDB, Hive, etc.)Strong analytical and problem-solving skills.Experience working in a network operations center environment.Experience as an open source Contributor.', 'diversity and inclusion', 'Experience with Scala, Julia, R, Python or other machine learning programming language', 'Experience in data engineering, databases, and data warehouses.', 'Experience using SQL (i.e., PL/SQL or T-SQL with RDBMSs like Teradata, MS SQL Server, Oracle, etc.)', 'Ability to travel occasionally', 'Strong experience with data engineering in Python.', 'Four or more years of experience as a data engineer', 'What we’re looking for...', 'Strong analytical and problem-solving skills.']",Mid-Senior level,Full-time,Information Technology,Telecommunications,2021-03-18 14:34:51
Data Engineer (Cloud Big Data),WarnerMedia,"Atlanta, GA",1 week ago,109 applicants,"['', 'Knowledge of Luigi, AWS Glue, AWS Athena, and Apache Spark.', ' ', 'The Daily', 'Create and maintain transformations to summarize/aggregate data and load it so users can consume this data using various BI/Analytics tools', 'Contribute to the project planning process by estimating tasks and deliverables', 'Knowledge and working experience with AWS such as S3, Lambda, EC2, Kinesis, etc.', 'Experience with Apache Airflow or similar job scheduling application.', '*** If you are interested in the position above, please apply to the following link on our career site!', '*** If you are interested in the position above, please apply to the following link on our career site! https://www.warnermediacareers.com/job/atlanta/software-application-developer/1174/18558626', 'Develop and maintain standards for administration and operation including the scheduling, running, monitoring, logging, management of errors, recovery from failures, and validation of outputs', 'Specific application experience with media, Web Analytics and consumer data systems a plus', 'The Extra Credit', 'BS or higher in Computer Science, Mathematics, MIS, business, or equivalent education / training / experience', 'Work closely with Revenue Analytics team members to understand user requirements', 'The Team', '45 years ago we changed the face of television, and we continue that today by building and delivering next-generation entertainment and technology solutions across the globe. Our innovations impact advertising, data management, information security, content creation and delivery, business operations, broadcasting and ultimately, the fan experience.', 'Basic understanding of statistics', 'Create solutions to transform data from various sources and load it into Snowflake to create a data lakeCreate and maintain transformations to summarize/aggregate data and load it so users can consume this data using various BI/Analytics toolsDevelop and maintain standards for administration and operation including the scheduling, running, monitoring, logging, management of errors, recovery from failures, and validation of outputsContribute to the project planning process by estimating tasks and deliverablesWork closely with Revenue Analytics team members to understand user requirementsBe at the cutting edge of utilizing data about consumers in the media industry to improve audience experience', 'Experience working with REST and SOAP APIs', 'Experience with Apache Airflow or similar job scheduling application.Knowledge and working experience with AWS such as S3, Lambda, EC2, Kinesis, etc.Knowledge of Luigi, AWS Glue, AWS Athena, and Apache Spark.Experience with ""big data"" platforms such as Snowflake, Hadoop, Hive, Presto or cloud-based tools such as Amazon Redshift, Google BigQueryExperience working with REST and SOAP APIsSpecific application experience with media, Web Analytics and consumer data systems a plusExperience with BI tools such as Looker and Tableau', 'Minimum 3-4 years solid experience writing Object-oriented programs in PythonMinimum 2-3 years solid experience writing SQLExperience in working with Linux environments and Unix Shell ScriptingUnderstanding of ETL processes and designBasic understanding of statisticsBS or higher in Computer Science, Mathematics, MIS, business, or equivalent education / training / experience', 'Working under direction of Data Architects and Technical Managers, this position is responsible for building data pipelines into and out of a data lake using state of the art technologies. This includes the ingestion and transformation of data that provides critical information to the enterprise and business partners. This position can be in the Atlanta or New York City office.', 'Be at the cutting edge of utilizing data about consumers in the media industry to improve audience experience', 'WarnerMedia is a leading media and entertainment company that creates and distributes premium and popular content from a diverse array of talented storytellers and journalists to global audiences through its consumer brands including: HBO, HBO Max, Warner Bros., TNT, TBS, truTV, CNN, DC Entertainment, New Line, Cartoon Network, Adult Swim, Turner Classic Movies and others.', 'Experience with BI tools such as Looker and Tableau', 'Minimum 3-4 years solid experience writing Object-oriented programs in Python', 'Experience with ""big data"" platforms such as Snowflake, Hadoop, Hive, Presto or cloud-based tools such as Amazon Redshift, Google BigQuery', 'The Job', 'Create solutions to transform data from various sources and load it into Snowflake to create a data lake', 'As a technologist at WarnerMedia, you will work at the intersection of art and science. You’ll work for brands that inform and entertain the world including [adult swim], Bleacher Report, Boomerang, Cartoon Network, CNN, ELEAGUE, Great Big Story, HLN, iStreamPlanet, TBS, Turner Classic Movies (TCM), TNT, truTV and Turner Sports- which includes the NBA, NCAA March Madness, Major League Baseball and the UEFA Champions League. You’ll be part of a company that enables community and belonging by creating content that connects with fans when, how and where they want it.', 'Understanding of ETL processes and design', ' The Extra Credit', 'Minimum 2-3 years solid experience writing SQL', 'Company Overview', 'The Essentials', 'Experience in working with Linux environments and Unix Shell Scripting']",Mid-Senior level,Full-time,Information Technology,Entertainment,2021-03-18 14:34:51
Data Software Engineer,Roku Inc.,"San Jose, CA",3 days ago,64 applicants,"['', 'Participate in architecture discussions, influence product roadmap, and take ownership and responsibility over new projects.', 'Strong SQL skills.', 'What You Will Do', 'Experience with AWS is a plus.', 'BS in Computer Science; MS in Computer Science preferred.', ' Strong SQL skills. Proficiency in at least one scripting language. Proficiency in at least one object-oriented language is desired. Experience with AWS is a plus. Collaborate with cross-functional teams of developers, QA and operations to execute deliverables. 5+ years professional experience as a data or software engineer. BS in Computer Science; MS in Computer Science preferred.', 'Low level systems debugging, performance measurement & optimization on large production clusters.', 'Proficiency in at least one object-oriented language is desired.', 'Maintain and support existing platforms and evolve to newer technology stacks and architectures.', 'Scratch-build a highly scalable, available, fault-tolerant data processing systems using AWS technologies, HDFS, YARN, Map-Reduce, Hive, Kafka, Spark, and other big data technologies. These systems should handle batch and real-time data processing over 10s of terabytes of data ingested every day and petabyte-sized data warehouse.', ""What You've Done And What You Bring"", 'Proficiency in at least one scripting language.', '5+ years professional experience as a data or software engineer.', 'Collaborate with cross-functional teams of developers, QA and operations to execute deliverables.', ' Scratch-build a highly scalable, available, fault-tolerant data processing systems using AWS technologies, HDFS, YARN, Map-Reduce, Hive, Kafka, Spark, and other big data technologies. These systems should handle batch and real-time data processing over 10s of terabytes of data ingested every day and petabyte-sized data warehouse. Low level systems debugging, performance measurement & optimization on large production clusters. Participate in architecture discussions, influence product roadmap, and take ownership and responsibility over new projects. Maintain and support existing platforms and evolve to newer technology stacks and architectures. ']",Not Applicable,Full-time,Engineering,Computer Software,2021-03-18 14:34:51
Associate Data Engineer,EAB,"Richmond, VA",3 days ago,59 applicants,"['', 'Medical, dental, and vision insurance; dependents and domestic partners eligible', 'Phase Back to Work program for employees returning from parental leave', 'Experience developing ETL processes', 'Support operations by identifying, researching and resolving performance and production issues', 'Dynamic growth opportunities with merit-based promotion philosophy', 'Primary Responsibilities', 'Strong written and oral communication skills ', 'Experience with AWS infrastructure (Lambda, Python, Aurora DB)', 'Bachelor’s or Master’s degree in Computer Science or Computer Engineering', 'Demonstrated mastery in database concepts and large-scale database implementations and design patterns', 'Proven ability to work with users to define requirements and business issues', 'Coordinate work with external teams to ensure a smooth development process', ' Medical, dental, and vision insurance; dependents and domestic partners eligible 401(k) retirement plan with company match 20+ days of PTO annually, in addition to paid firm holidays Daytime leave policy for community service or fitness activities (up to 10 hours a month each) Paid parental leave for birthing or non-birthing parents Phase Back to Work program for employees returning from parental leave Infertility treatment coverage and adoption or surrogacy assistance Wellness programs including gym discounts and incentives to promote healthy living Dynamic growth opportunities with merit-based promotion philosophy Benefits kick in day one, see the full details here.  ', 'Benefits', '401(k) retirement plan with company match', 'Paid parental leave for birthing or non-birthing parents', 'Excellent analytic and troubleshooting skills', 'Infertility treatment coverage and adoption or surrogacy assistance', 'Experience working in an AGILE environment', 'Benefits kick in day one, see the full details here. ', 'Responsible for data modeling and schema design that will range across multiple business domains within higher education', '20+ days of PTO annually, in addition to paid firm holidays', 'Ideal Qualifications', 'Familiar with Snowflake database', 'The Role In Brief', 'GIT expertise ', ' Responsible for data modeling and schema design that will range across multiple business domains within higher education Partner with multiple stakeholders including clients, new product development, BI engineers to develop scalable standard schemas Work with clients to research and conduct business information flow studies Codify high-performing SQL for efficient data transformation Coordinate work with external teams to ensure a smooth development process Support operations by identifying, researching and resolving performance and production issues ', 'Experience developing logical data models within a data warehouse', 'Experience developing commercial software products', 'Demonstrated mastery in one or more SQL variants: PostgreSQL, MySQL, Oracle, SQL Server, or DB2', 'Codify high-performing SQL for efficient data transformation', ' Bachelor’s or Master’s degree in Computer Science or Computer Engineering Experience working with relational or multi-dimensional databases Experience developing logical data models within a data warehouse Experience developing ETL processes Demonstrated mastery in one or more SQL variants: PostgreSQL, MySQL, Oracle, SQL Server, or DB2 Demonstrated mastery in database concepts and large-scale database implementations and design patterns Proven ability to work with users to define requirements and business issues Excellent analytic and troubleshooting skills Strong written and oral communication skills  ', 'Daytime leave policy for community service or fitness activities (up to 10 hours a month each)', 'Experience working with relational or multi-dimensional databases', 'Basic Qualifications', ' Experience working in an AGILE environment Experience developing commercial software products Experience with AWS infrastructure (Lambda, Python, Aurora DB) Familiar with Snowflake database GIT expertise  ', 'About EAB', 'Wellness programs including gym discounts and incentives to promote healthy living', 'Partner with multiple stakeholders including clients, new product development, BI engineers to develop scalable standard schemas', 'Work with clients to research and conduct business information flow studies']",Entry level,Full-time,Education,Education Management,2021-03-18 14:34:51
Data Engineer,Talkspace - Online Therapy,"New York, NY",3 weeks ago,Over 200 applicants,"['', 'You will be responsible for data that is used for business, product, marketing, and broad company analytics', 'About This Role', 'You’ve worked at a data analytics company', 'Benefits', 'Both learn from and teach your fellow engineers by providing constructive feedback', 'Participate in the technical design of new features', 'You will own the entire data life cycle, including the project management, onboarding and implementation of any new business intelligence and/or data analytics tools ', 'Python expert', 'Requirements', ' You’ve worked at a data analytics company You have health tech or behavioral health experience ', 'Description', 'SQL expert ', 'At ease with ambiguity', 'You will work closely with data analysts and scientists and develop a deep understanding of our product roadmap, marketing trends, and revenue targets ', "" Must have a Bachelor’s or Master's degree in Computer Science 2+ years of experience in a Data Engineering position Experience with Business Intelligence tools, preferably Looker SQL expert  Python expert At ease with ambiguity Experience (or strong interest) in working in a fast-paced environment "", 'You have health tech or behavioral health experience', 'Experience with Business Intelligence tools, preferably Looker', ""Must have a Bachelor’s or Master's degree in Computer Science"", '2+ years of experience in a Data Engineering position', 'Experience (or strong interest) in working in a fast-paced environment', ' You will be responsible for data that is used for business, product, marketing, and broad company analytics You will own the entire data life cycle, including the project management, onboarding and implementation of any new business intelligence and/or data analytics tools  You will work closely with data analysts and scientists and develop a deep understanding of our product roadmap, marketing trends, and revenue targets  Participate in the technical design of new features Both learn from and teach your fellow engineers by providing constructive feedback Collaborate with small teams of talented engineers, product managers, and designers to build the best platform for psychotherapy ', 'Collaborate with small teams of talented engineers, product managers, and designers to build the best platform for psychotherapy']",Not Applicable,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
No SQL Data Engineer,Illumination Works,"Beavercreek, OH",2 weeks ago,33 applicants,"['', '·\xa0\xa0\xa0\xa0\xa0\xa0Short and Long Term Disability Insurance', '·\xa0\xa0\xa0\xa0\xa0\xa0Ongoing training, education and industry partnerships that allow you to be up to speed on the latest technologies and processes.', 'Experience with big data technologies', 'Work with internal teams to plan and schedule application upgrades', 'Manage the operations for both development and production databases', 'The NoSQL Data Engineer will work with large unstructured files within Couchbase.', 'If you are interested in joining our team and being part of an outstanding group of IT professionals providing innovative solutions to a wide variety of clients, and feel you meet the prior requirements, please contact us!', '·\xa0\xa0\xa0\xa0\xa0\xa0Market Competitive Salary\xa0\xa0\xa0\xa0', 'Create and configure monitors to establish the health of the databases ', 'Illumination Works, LLC is committed to hiring and retaining a diverse workforce. We are an Equal Opportunity Employer, making decisions without regard to race, color, religion, sexual orientation, gender identity or national origin, age, veteran status, disability, or any other protected class. U.S. Citizenship is required for most positions.', 'ILW\xa0is a motivated, fast-growing Solution Enablement Company. We provide expert level consulting, design, and development services that deliver results for our clients.\xa0We look for the best of the best, those who are attracted to a challenging and rewarding career experience, not just a job.\xa0Our ideal candidate exhibits passion, patience and perseverance with an entrepreneurial mindset. We have recruited and retained some of the best technical and professional talent in the industry.', 'Additional experience that is beneficial for this position:', 'Work with customers to help determine and deploy the best datastore technology and data model strategy for their needs', 'Experience with ETL tools, with an emphasis on NoSQL datasets', '·\xa0\xa0\xa0\xa0\xa0\xa0Comprehensive Medical, Dental, Vision and Life Insurance Plans', '·\xa0\xa0\xa0\xa0\xa0\xa0401K', '5+ years professional experienceNoSQL database development experience:Experience with NoSQL technologies, such as Couchbase, MongoDB and Hadoop ecosystem toolsAbility to troubleshoot NoSQL issues and performance bottlenecksExperience with ETL tools, with an emphasis on NoSQL datasetsLinux experience requiredExperience with structured databases, such as PostgreSQL, MySQL, SQL Server, DB2Experience with big data technologiesManage the operations for both development and production databasesCreate and configure monitors to establish the health of the databases Design, implement, maintain and automate the appropriate backup and recovery architecture as requiredWork with internal teams to plan and schedule application upgradesWork with customers to help determine and deploy the best datastore technology and data model strategy for their needsStrong communications skillsAbility to self-manage and self-startB.S. degree in Computer Science, Information Technology, or equivalent fieldAbility to obtain security clearance (must be US citizen)', '5+ years professional experience', '·\xa0\xa0\xa0\xa0\xa0\xa0Generous PTO Package', 'Do you have what it takes?', 'Experience with NoSQL technologies, such as Couchbase, MongoDB and Hadoop ecosystem tools', 'Why Choose Us?', 'Summary:', 'Consulting experience', 'Ability to troubleshoot NoSQL issues and performance bottlenecks', '·\xa0\xa0\xa0\xa0\xa0\xa0Fun & Engaging Culture', 'Experience with structured databases, such as PostgreSQL, MySQL, SQL Server, DB2', 'As a company, we invest in our employees in all aspects of your life. We understand that the health of yourself and families are very important; along with your time here at ILW. Listed below you will find soExe of the top benefits and perks if you choose to be a part of our team.', 'Experience structuring data to support data science needs', 'Linux experience required', 'Design, implement, maintain and automate the appropriate backup and recovery architecture as required', 'Data architecture experienceElasticsearch experienceExperience structuring data to support data science needsConsulting experience', 'Strong communications skills', 'B.S. degree in Computer Science, Information Technology, or equivalent field', 'Elasticsearch experience', 'Data architecture experience', '\xa0', '·\xa0\xa0\xa0\xa0\xa0\xa0M.S. degree in Computer Science, Information Technology, or equivalent field', 'NoSQL database development experience:', 'Ability to self-manage and self-start', 'Ability to obtain security clearance (must be US citizen)']",Mid-Senior level,Full-time,Engineering,Information Technology and Services,2021-03-18 14:34:51
Senior Data Engineer,Discover Financial Services,"Springfield, IL",2 days ago,Be among the first 25 applicants,"['Discover. A brighter future.', '', 'Be pro-active and diligent in identifying and communicating design and development issues.', 'Utilize multiple development languages/tools such as Python, Spark to build prototypes and evaluate results for effectiveness and feasibility.Operationalize open source data-analytic tools for enterprise use.Develop real-time data ingestion and stream-analytic solutions leveraging technologies such as Kafka, Apache Spark, Python and AWS based solutions.', 'Work heavily within the Cloud ecosystem and migrate data from Teradata to AWS based platform.', 'Develop real-time data ingestion and stream-analytic solutions leveraging technologies such as Kafka, Apache Spark, Python and AWS based solutions.', 'Experience in migrating ETL processes (not just data) from relational warehouse Databases to AWS based solutions.', 'Utilize multiple development languages/tools such as Python, Spark to build prototypes and evaluate results for effectiveness and feasibility.', 'Offer system support as part of a support rotation with other team members.', 'Optimize the performance of ETL processes and scripts by working with other technical staff as needed', 'Minimum Qualifications', 'Very strong verbal & written communication skills', 'Ensure proper data governance policies are followed by implementing or validating Data Lineage, Quality checks, classification, etc.Designs and develops data ingestion frameworks, real time processing solutions, and data processing/transformation frameworks leveraging open source tools.Deploys application code and analytical models using CI/CD tools and techniques and provides support for deployed data applications and analytical models.Provides senior level technical consulting to peer data engineers during design and development for highly complex and critical data projects', 'Custom Data pipeline development (Cloud and locally hosted)', '4+ years of work experience in Data Platform Administration/Engineering, or related', 'Experience within the Financial industry', 'Design, develop, test, and implement data-driven solutions to meet business requirements', 'Operationalize open source data-analytic tools for enterprise use.', 'Responsibilities', 'Deep knowledge and very strong in SQL and Relational Databases', 'Designs and develops data ingestion frameworks, real time processing solutions, and data processing/transformation frameworks leveraging open source tools.', 'Develop data driven solutions utilizing current and next generation technologies to meet evolving business needs.Design, develop, test, and implement data-driven solutions to meet business requirements', 'Hands on experience with Amazon Web Services (AWS) based solutions such as Lambda, Dynamo dB, Snowflake and S3.', 'Contribute to determining programming approach, tools, and techniques that best meet the business requirements.', 'Willingness to continuously learn & share learnings with others', 'Ability to work in a fast-paced, rapidly changing environment', 'Develop data driven solutions utilizing current and next generation technologies to meet evolving business needs.', 'Desired Qualifications', 'Ensure proper data governance policies are followed by implementing or validating Data Lineage, Quality checks, classification, etc.', ' you', 'Experience in building & utilizing tools and frameworks within the Big Data ecosystem including Kafka, Spark, and NoSQL.', 'Bachelors degree in Information Technology, or related field', 'Ability to quickly identify an opportunity and recommend possible technical solutions.', 'Develop application systems that comply with the standard system development methodology and concepts for design, programming, backup, and recovery to deliver solutions that have superior performance and integrity.', '2+ years of work experience in Data Platform Administration/Engineering, or related', 'Bachelors degree in Information Technology, or related field2+ years of work experience in Data Platform Administration/Engineering, or related', 'Custom Data pipeline development (Cloud and locally hosted)Work heavily within the Cloud ecosystem and migrate data from Teradata to AWS based platform.Provide support for deployed data applications and analytical models by being a trusted advisor to Data Scientists and other data consumers by identifying data problems and guiding issue resolution with partner Data Engineers and source data providers.Provide subject matter expertise in the analysis, preparation of specifications and plans for the development of data processes.Optimize the performance of ETL processes and scripts by working with other technical staff as needed', 'Knowledge of Data Warehouse technology (Unix/Teradata/Ab Initio/Python/Spark/Snowflake/No SQL)', '4+ years of work experience in Data Platform Administration/Engineering, or relatedHands on experience with Amazon Web Services (AWS) based solutions such as Lambda, Dynamo dB, Snowflake and S3.Knowledge of Data Warehouse technology (Unix/Teradata/Ab Initio/Python/Spark/Snowflake/No SQL)Experience in migrating ETL processes (not just data) from relational warehouse Databases to AWS based solutions.Experience in building & utilizing tools and frameworks within the Big Data ecosystem including Kafka, Spark, and NoSQL.Deep knowledge and very strong in SQL and Relational DatabasesKnowledge of Data Warehouse technology (Unix/Teradata/Ab Initio)Willingness to continuously learn & share learnings with othersAbility to work in a fast-paced, rapidly changing environmentVery strong verbal & written communication skillsExperience within the Financial industry', 'Provide support for deployed data applications and analytical models by being a trusted advisor to Data Scientists and other data consumers by identifying data problems and guiding issue resolution with partner Data Engineers and source data providers.', 'Knowledge of Data Warehouse technology (Unix/Teradata/Ab Initio)', 'Provide business analysis and develop ETL code and scripting to meet all technical specifications and business requirements according to the established designs.', 'Understand and follow the PDP process to develop, deploy and deliver the solutions.', 'Deploys application code and analytical models using CI/CD tools and techniques and provides support for deployed data applications and analytical models.', 'Job Description', 'Provide subject matter expertise in the analysis, preparation of specifications and plans for the development of data processes.', 'Provides senior level technical consulting to peer data engineers during design and development for highly complex and critical data projects', 'Develop application systems that comply with the standard system development methodology and concepts for design, programming, backup, and recovery to deliver solutions that have superior performance and integrity.Contribute to determining programming approach, tools, and techniques that best meet the business requirements.Understand and follow the PDP process to develop, deploy and deliver the solutions.Be pro-active and diligent in identifying and communicating design and development issues.Provide business analysis and develop ETL code and scripting to meet all technical specifications and business requirements according to the established designs.Offer system support as part of a support rotation with other team members.']",Associate,Full-time,Information Technology,Financial Services,2021-03-18 14:34:51
Data Engineer,The Scotts Miracle-Gro Company,"Marysville, OH",2 weeks ago,54 applicants,"['', 'Our commitment to diversity and inclusion includes employee resource groups: Scotts Women’s Network, Scotts Black Employee Network, Scotts Veterans Network, GroPride Network, and Scotts Young Professionals', '5+ years of SQL and No-SQL experience', 'Process oriented with great documentation skills', 'Ability to meet with the business and technical teams to provide understandable technical direction that demonstrates understanding of and alignment with business objectives.', 'Complex problem solving & analysis', 'Assemble large, complex data sets that meet functional / non-functional business requirements.', 'Implements processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it.', 'First day of hire, 401K match (up to 7.5%) and discounted stock purchasing program (15% discount) ', 'Create and maintain optimal data pipeline architecture including new API integrations to support continuing increases in data volume and complexity.Implements processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it.Assemble large, complex data sets that meet functional / non-functional business requirements.Work with business stakeholders to assist with data-related technical issues and support their data infrastructure needs.Actively engage with key internal and external technology partners.Analyze the impact of any new requirements on the existing solutionEstimation of efforts for issue-resolution and change requests. ', 'Marysville, OH', '13 paid holidays and generous vacation policy ', 'Onsite wellness center which includes: 24,000 sq. ft. fitness center, Walgreens pharmacy and Doctor’s office ', '5+ years of developing and maintaining data pipelines', 'A place on an engaged team of passionate colleagues in a growing and dynamic industryOffer extremely competitive benefits including: Health, Dental and Vision coverage First day of hire, 401K match (up to 7.5%) and discounted stock purchasing program (15% discount) 13 paid holidays and generous vacation policy Onsite wellness center which includes: 24,000 sq. ft. fitness center, Walgreens pharmacy and Doctor’s office Nutrition reimbursement program (up to $200 per associate and per spouse) Beautiful campus and corporate offices designed like a log cabin offering free coffee, chef run cafe and the best crushed ice! Our commitment to diversity and inclusion includes employee resource groups: Scotts Women’s Network, Scotts Black Employee Network, Scotts Veterans Network, GroPride Network, and Scotts Young Professionals', 'What you’ll need to be successful:', 'Work with business stakeholders to assist with data-related technical issues and support their data infrastructure needs.', 'Experience with AWS platform with emphasis on Amazon S3, Glue, Athena, EMR, Redshift, and Lake Formation ', 'What you’ll do in this role:', 'Estimation of efforts for issue-resolution and change requests. ', 'Create and maintain optimal data pipeline architecture including new API integrations to support continuing increases in data volume and complexity.', 'Ability in managing and communicating data warehouse plans to internal clients', 'Nutrition reimbursement program (up to $200 per associate and per spouse) ', 'Non-Technical Skills:', 'What we do for you (just to list a few cool ones)', '5+ years of developing and maintaining data pipelinesProcess oriented with great documentation skillsExcellent problem solving and troubleshooting skillsExperience with Apache AirflowExperience with AWS platform with emphasis on Amazon S3, Glue, Athena, EMR, Redshift, and Lake Formation Experience with data cleansing, cataloging, and validationExperience with Neo4j Graph Databases, MuleSoft, and Graphql a plus', 'Computer Science, Computer Applications, and Information Systems degree or the equivalent combination of training, education, or experience', 'Offer extremely competitive benefits including: Health, Dental and Vision coverage ', '5+ years of Python or Java development experience', 'Complex problem solving & analysisAbility to formulate an in-depth technical strategy meeting the needs of supported business functionsAbility to meet with the business and technical teams to provide understandable technical direction that demonstrates understanding of and alignment with business objectives.Relationship building with technical and non-technical personnel', '5+ years of experience with schema design and dimensional data modeling', 'Relationship building with technical and non-technical personnel', 'Experience with Neo4j Graph Databases, MuleSoft, and Graphql a plus', 'Actively engage with key internal and external technology partners.', 'Experience with Apache Airflow', 'Beautiful campus and corporate offices designed like a log cabin offering free coffee, chef run cafe and the best crushed ice! ', 'Excellent problem solving and troubleshooting skills', 'Data Engineer ', 'We are looking for a Data Engineer who will join our Business Intelligence team in Marysville, OH.', 'What we do for you (just to list a few cool ones):', 'Experience with data cleansing, cataloging, and validation', 'Analyze the impact of any new requirements on the existing solution', 'Ability to formulate an in-depth technical strategy meeting the needs of supported business functions', 'Computer Science, Computer Applications, and Information Systems degree or the equivalent combination of training, education, or experience5+ years of Python or Java development experience5+ years of SQL and No-SQL experience5+ years of experience with schema design and dimensional data modelingAbility in managing and communicating data warehouse plans to internal clients', 'A place on an engaged team of passionate colleagues in a growing and dynamic industry', 'Business Intelligence ', 'The Sr. Analyst, Data Engineer role will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.', 'This position will be based at The Scotts Miracle-Gro world headquarters in Marysville, a suburb of Columbus, OH.\xa0 Not familiar with Columbus? Visit columbusregion.com/columbus-2020/', 'Education/Training:']",Associate,Full-time,Research,Consumer Goods,2021-03-18 14:34:51
"Data Engineer, Data Platform",MasterClass,"San Francisco, CA",3 weeks ago,81 applicants,"['', 'Attain SLA’s for data sets and processes running in production', '2+ years of experience in Data Engineering and Data Warehousing', 'Build strong cross-functional partnerships with Data Scientists, Analysts, Product Managers and Software Engineers to understand data needs and implement on those needs', 'Experience integrating with external systems/vendor APIs for ingestion of datasets in data platform', 'Proactively execute implementations of our data engineering/data warehouse initiatives that holds video streaming, subscription, enterprise, CRM, and financial datasets and systems', 'Maintain data quality; use best practices for standard naming conventions and coding practices to ensure consistency of data models and tracking', "" 2+ years of experience in Data Engineering and Data Warehousing Bachelor's degree in a quantitative field, e.g. Computer Science, Math, Physics Advanced proficiency with SQL, Python, Postgres, REST/GraphQL Experience in working within cloud based Data warehouse/Data platform systems, developing ETL/ELT pipelines and usage of RDBMS/NoSQL/other types of databases Experience integrating with external systems/vendor APIs for ingestion of datasets in data platform Experience integrating and development in distributed/RT systems  Strong communication skills, with the ability to execute projects proactively and accurately Work full-time in our San Francisco office "", 'Experience in working within cloud based Data warehouse/Data platform systems, developing ETL/ELT pipelines and usage of RDBMS/NoSQL/other types of databases', 'At MasterClass, we believe we put our best work forward when our employees bring together ideas that are diverse in thought. We are proud to be an equal opportunity workplace and are committed to equal employment opportunity regardless of race, color, religion, national origin, age, sex, marital status, ancestry, physical or mental disability, genetic information, veteran status, gender identity or expression, sexual orientation, or any other characteristic protected by applicable federal, state or local law. In addition, MasterClass will provide reasonable accommodations for qualified individuals with disabilities. If you have a disability or special need, we would like to know how we can better accommodate you.', 'exceptional Data Engineer ', 'Understand business needs, build data models, develop scalable and reliable solutions', 'Requirements', 'Who We Are', 'Experience integrating and development in distributed/RT systems ', ' Proactively execute implementations of our data engineering/data warehouse initiatives that holds video streaming, subscription, enterprise, CRM, and financial datasets and systems Understand business needs, build data models, develop scalable and reliable solutions Maintain data quality; use best practices for standard naming conventions and coding practices to ensure consistency of data models and tracking Attain SLA’s for data sets and processes running in production Continuously improve our various tools within data infrastructure Design and develop scalable implementations  Build strong cross-functional partnerships with Data Scientists, Analysts, Product Managers and Software Engineers to understand data needs and implement on those needs ', 'Strong communication skills, with the ability to execute projects proactively and accurately', 'Responsibilities Of The Role', ""Bachelor's degree in a quantitative field, e.g. Computer Science, Math, Physics"", 'Work full-time in our San Francisco office', 'Continuously improve our various tools within data infrastructure', 'What We Are Looking For', 'Advanced proficiency with SQL, Python, Postgres, REST/GraphQL', 'Design and develop scalable implementations ']",Associate,Full-time,Engineering,E-Learning,2021-03-18 14:34:51
Data Engineer - Monitoring Solutions,Nokia,"Dallas, TX",3 weeks ago,Be among the first 25 applicants,"['', '2+ years of experience with schema design and dimensional data modeling', 'Key Responsibilities / Functions', 'Experience/Knowledge of building, distrubuting and running containers', 'Schedule', 'Good communication and writing skills to facilitate productive collaboration with other team members and business units;', 'Ability in managing and communicating data warehouse plans to internal clients', 'Primary Location', '2+ years of Python or Java development experience', 'Experience designing and customizing databases', 'Come create the technology to connect the world.', ' Extensive experience using Python/Java including a strong grasp of object-oriented programming (OOP) fundamentals Extensive experience analyzing data using SQL  Experience designing and customizing databases Experience/Knowledge of building, distrubuting and running containers ', 'About Nokia IT', 'Extensive experience using Python/Java including a strong grasp of object-oriented programming (OOP) fundamentals', 'Other Locations', '2+ years of SQL experience (NoSQL experience is a plus)', 'Strong knowledge of project management principles and concepts;', 'Extensive experience analyzing data using SQL ', 'Experience with predictive modeling and dissemination of research results;', 'Experience solving problems with an emphasis on product development', 'Job Description', ' 2+ years of Python or Java development experience 2+ years of SQL experience (NoSQL experience is a plus) 2+ years of experience with schema design and dimensional data modeling Ability in managing and communicating data warehouse plans to internal clients 3+ years of relevant experience such as implementing statistical analysis, developing cloud-based data-lake / data warehouses, managing data science projects, developing APIs, developing machine learning models, creating advanced data visualizations. Good communication and writing skills to facilitate productive collaboration with other team members and business units; Strong knowledge of project management principles and concepts; Experience solving problems with an emphasis on product development Experience with predictive modeling and dissemination of research results; ', '3+ years of relevant experience such as implementing statistical analysis, developing cloud-based data-lake / data warehouses, managing data science projects, developing APIs, developing machine learning models, creating advanced data visualizations.', 'Job', 'Required Minimum Qualifications (Education, Technical Skills/Knowledge)']",Not Applicable,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Insight Global,"Atlanta, GA",,N/A,"['', '•Cloud based development (Azure preferred)', 'At Insight Global, our goal is to connect our consultants with our customers and empower everyone to develop personally, professionally, and financially; so they can be the light to the world around them.', '•Build analytics tools that utilize the data pipeline to provide actionable insights into consultant & customer recommendations, operational efficiency, and other key business performance metrics.', 'Minimum Requirements: ', '•3+ years professional experience in Spark using Python and associated libraries', '•Good communication and collaboration skills, and the ability to work with business and IT stakeholders', '•Assemble large, complex data sets to meet the data needs for IG’s data science, mobile application, and business reporting capabilities', '•Spark programming, Databricks experience preferred', '•Python', '•3+ years of professional data engineering experience', 'Insight Global is hiring a Spark Data Engineer to build the data pipelines and processing needed to deliver on the company’s data science and mobile initiatives. This engineer will join a team that fills a key role in the continued growth of both Insight Global and our internal data team. If you are excited about the prospect of building things that help thousands of people find jobs and the hope and joy which comes along with that mission, this is the role and company for you.', '•2+ years professional experience with SQL and RDBMS development', '•2+ years professional experience designing and developing with a cloud service (Azure preferred)\xa0', '•SQL ', 'Job Overview:', '•Work with the IG Data Engineering team to develop IG’s data pipeline and processing capabilities in a Databricks and Azure environment', 'Specific Duties & Job Responsibilities: ', '•Work with stakeholders and other teams to assist with data-related technical issues and support their data needs.', '•Hands on experience designing and developing ETL / ELT processes and pipelines', '•Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.', 'Required Skills:']",Associate,Full-time,Information Technology,Staffing and Recruiting,2021-03-18 14:34:51
Jr. Data Engineer,EXOS,"Remote, OR",4 weeks ago,Be among the first 25 applicants,"['', ' Hands-on experience in big data, information retrieval, data mining or machine learning.  Reflective, independent and eager learner (e.g., learns from mistakes, asks good questions, able to generate creative solutions to problems with minimal guidance).  Detail-oriented with an ability to effectively multi-task in a deadline-driven atmosphere.  High levels of integrity, autonomy, self-motivation and ability to work well in a team.  Experience in health, fitness, wellness, etc. ', ' Identify, design and implement internal process improvements, including automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. ', ' Experience in health, fitness, wellness, etc. ', 'KNOWLEDGE, SKILLS, AND ABILITIES', ' Find trends in data sets and develop algorithms to help make raw data more useful to the enterprise. ', ' Be a trusted technical advisor to customers and solve complex data challenges. ', ' Know how to work with high volume heterogeneous data, preferably with distributed systems on the AWS platform. ', ' Collaborate with data analysts to understand data requirements necessary for building new data pipelines, and optimize existing data streams, so that analysts can seamlessly connect to data through Tableau and/or Google Data Studio, among other tools. ', ' Inspire and lead others with your work ethic, business results, intrapersonal skills and willingness to see success based on team accomplishments vs. your individual achievements. ', 'Job Description: ', ' Optimize the existing data warehouse which will create a single version of the truth and standardize data into coherent formats so that it can be queried by users by leveraging query languages, as well as generating API calls to gather third-party vendor data. ', ' A successful history of manipulating, processing and extracting value from large disconnected datasets. ', ' EEO is the Law ', ' Experience with data processing software and algorithms. ', ' Build large-scale batch and real-time data pipelines with data processing frameworks like Spark and AWS managed services.  Use best practices in continuous integration and delivery.  Help drive optimization, testing and tooling to improve data quality and our ability to use data to make product decisions.  Collaborate with other software engineers, ML engineers and stakeholders, taking learning and leadership opportunities that will arise every single day.  Collaborate with the analytics team to support their BI tools and initiatives to deliver the reliability, speed, and scalability of a data platform they’ll love working with.  Work in multi-functional agile teams to continuously experiment, iterate and deliver on new product and infrastructure objectives.  Optimize the existing data warehouse which will create a single version of the truth and standardize data into coherent formats so that it can be queried by users by leveraging query languages, as well as generating API calls to gather third-party vendor data.  Create and maintain pertinent datasets aligned to business needs, including gathering client report requirements to execute to highest possible standards.  Ensure data structures, reports and any other data-related item meets standards and guidelines outlined by EXOS Data Governance.  Find trends in data sets and develop algorithms to help make raw data more useful to the enterprise.  Identify, design and implement internal process improvements, including automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.  Collaborate with data analysts to understand data requirements necessary for building new data pipelines, and optimize existing data streams, so that analysts can seamlessly connect to data through Tableau and/or Google Data Studio, among other tools.  Work within multidisciplinary teams to identify client needs, define critical success indicators, identify and maintain data flows that fuel operations, and supply data.  Keep abreast of new data storage, delivery, analysis, visualization, reporting techniques and software to develop more powerful data infrastructure.  Be a trusted technical advisor to customers and solve complex data challenges.  Inspire and lead others with your work ethic, business results, intrapersonal skills and willingness to see success based on team accomplishments vs. your individual achievements. ', ' Experience contributing to client-facing projects, troubleshooting technical issues, working with cross-functional stakeholders. ', ' Hands-on experience in big data, information retrieval, data mining or machine learning. ', ' Experience in writing software in one or more languages: Java, Python, Go. Experience in SQL. ', ' EEO is the Law  EEO is the Law Supplement', ' High levels of integrity, autonomy, self-motivation and ability to work well in a team. ', ' Excellent verbal and written communication skills. ', ' Work within multidisciplinary teams to identify client needs, define critical success indicators, identify and maintain data flows that fuel operations, and supply data. ', ' Superior understanding of database query languages and substantial knowledge in analytical approaches. ', ' Care a lot about fostering a diverse culture that includes everyone and supports them being their authentic self. We strongly believe that diversity of experience, perspectives, and background will lead to a better environment for our employees and a better product for our customers. ', ' Bachelor’s degree in a technology-related field (data science, computer science, software engineering, etc.), with 1-3 years’ data engineering experience. ', ' Use best practices in continuous integration and delivery. ', ' Build large-scale batch and real-time data pipelines with data processing frameworks like Spark and AWS managed services. ', '!', ' Keep abreast of new data storage, delivery, analysis, visualization, reporting techniques and software to develop more powerful data infrastructure. ', ' Understand the value of partnership within and across teams. ', ' Detail-oriented with an ability to effectively multi-task in a deadline-driven atmosphere. ', ' Know how to write distributed, high-volume services in Go, Java, Scala, or Python leveraging AWS managed services. ', ' Want to own the software you write in production. ', ' Help drive optimization, testing and tooling to improve data quality and our ability to use data to make product decisions. ', ' Bachelor’s degree in a technology-related field (data science, computer science, software engineering, etc.), with 1-3 years’ data engineering experience.  Experience with data processing software and algorithms.  Experience in writing software in one or more languages: Java, Python, Go. Experience in SQL.  Experience contributing to client-facing projects, troubleshooting technical issues, working with cross-functional stakeholders.  Experience in working with/on data warehouses, including data warehouse technical architectures, infrastructure components, ETL/ ELT and reporting/analytic tools and environments, data structures.  Excellent verbal and written communication skills. ', ' Experience in working with/on data warehouses, including data warehouse technical architectures, infrastructure components, ETL/ ELT and reporting/analytic tools and environments, data structures. ', ' Work in multi-functional agile teams to continuously experiment, iterate and deliver on new product and infrastructure objectives. ', ' EEO is the Law Supplement', ' Expertise with data mining and visualization techniques, ability to apply context to data, and strong ability to communicate the data (verbal and written). ', 'Required Qualifications', 'You May Request Reasonable Accommodation, In Writing, By Reaching Out To Our People Operations Department At', 'PREFERRED QUALIFICATIONS', ' Reflective, independent and eager learner (e.g., learns from mistakes, asks good questions, able to generate creative solutions to problems with minimal guidance). ', ' Knowledgeable about data modeling, data access, and data storage techniques. ', 'Learn More Here', ' Create and maintain pertinent datasets aligned to business needs, including gathering client report requirements to execute to highest possible standards. ', ""If you like wild growth and working with happy, enthusiastic over-achievers, you'll enjoy your career with us"", 'Day-to-day Responsibilities', ' Appreciate agile software processes, data-driven development, reliability, and responsible experimentation. ', 'We are an equal opportunity employer', ' Experience building the infrastructure required for optimal ETL of data from a wide variety of data sources. ', ' Collaborate with the analytics team to support their BI tools and initiatives to deliver the reliability, speed, and scalability of a data platform they’ll love working with. ', ' Know how to work with high volume heterogeneous data, preferably with distributed systems on the AWS platform.  Know how to write distributed, high-volume services in Go, Java, Scala, or Python leveraging AWS managed services.  Knowledgeable about data modeling, data access, and data storage techniques.  Appreciate agile software processes, data-driven development, reliability, and responsible experimentation.  Want to own the software you write in production.  Understand the value of partnership within and across teams.  Care a lot about fostering a diverse culture that includes everyone and supports them being their authentic self. We strongly believe that diversity of experience, perspectives, and background will lead to a better environment for our employees and a better product for our customers.  Experience building the infrastructure required for optimal ETL of data from a wide variety of data sources.  A successful history of manipulating, processing and extracting value from large disconnected datasets.  Superior understanding of database query languages and substantial knowledge in analytical approaches.  Expertise with data mining and visualization techniques, ability to apply context to data, and strong ability to communicate the data (verbal and written). ', ' Collaborate with other software engineers, ML engineers and stakeholders, taking learning and leadership opportunities that will arise every single day. ', ' Ensure data structures, reports and any other data-related item meets standards and guidelines outlined by EXOS Data Governance. ']",Associate,Full-time,Information Technology,Nonprofit Organization Management,2021-03-18 14:34:51
Data Engineer,Capital Group,"San Antonio, TX",3 weeks ago,30 applicants,"['', '  Excellent technical knowledge of performance tuning and query optimization across large data sets, and exposure to bottlenecks at the storage, network or compute layers   Working experience with data cataloging and enablement through APIs   Excellent understanding of traditional RDBMS   You have strengths in leadership, interpersonal, and problem solving skills with the ability to continually learn new concepts and technologies and effectively apply them  ', 'Travel required', ""  You have a bachelor's degree in Computer Science, Engineering or a related technical field   You have 3-5 years of experience delivering data solutions, including cloud technologies like AWS   You write and optimize advanced SQL queries with large-scale, complex datasets   You have experience with integration of data from multiple sources and a knowledge of ETL/ELT technologies and tools like Informatica and AWS solutions   You have a solid background with data analysis and modelling   You have coding proficiency in at least one modern language such as Python, R and Spark   You have experience in cloud-first design, preferably AWS or Azure   You're well-versed in Machine Learning and data mining  "", ""  You have a bachelor's degree in Computer Science, Engineering or a related technical field   You have 3-5 years of experience delivering data solutions, including cloud technologies like AWS   You write and optimize advanced SQL queries with large-scale, complex datasets   You have experience with integration of data from multiple sources and a knowledge of ETL/ELT technologies and tools like Informatica and AWS solutions   You have a solid background with data analysis and modelling   You have coding proficiency in at least one modern language such as Python, R and Spark   You have experience in cloud-first design, preferably AWS or Azure   You're well-versed in Machine Learning and data mining   "", ""COVID-19 HIRING Our recruiting and onboarding activities are virtual during the pandemic and we've transitioned to a work-from-home environment until further notice. We are offering generous work-from-home benefits to improve our associate's ability to work remotely. "", '""I can succeed as a Data Engineer at Capital Group.""', '   Excellent technical knowledge of performance tuning and query optimization across large data sets, and exposure to bottlenecks at the storage, network or compute layers   Working experience with data cataloging and enablement through APIs   Excellent understanding of traditional RDBMS   You have strengths in leadership, interpersonal, and problem solving skills with the ability to continually learn new concepts and technologies and effectively apply them    ', ' Enjoy generous time-away and health benefits from day one, with the opportunity for flexible work options ', ' You have coding proficiency in at least one modern language such as Python, R and Spark ', ' You have a solid background with data analysis and modelling ', 'Nearest Major Market ', ' You write and optimize advanced SQL queries with large-scale, complex datasets ', 'Other location(s)', ' You have 3-5 years of experience delivering data solutions, including cloud technologies like AWS ', '""I can be myself at work.""', ' Receive 2-for-1 matching gifts for your charitable contributions and the opportunity to secure annual grants for the organizations you love ', ' Excellent understanding of traditional RDBMS ', "" You have a bachelor's degree in Computer Science, Engineering or a related technical field "", '""I can lead a full life.""', 'Location', ""   You have a bachelor's degree in Computer Science, Engineering or a related technical field   You have 3-5 years of experience delivering data solutions, including cloud technologies like AWS   You write and optimize advanced SQL queries with large-scale, complex datasets   You have experience with integration of data from multiple sources and a knowledge of ETL/ELT technologies and tools like Informatica and AWS solutions   You have a solid background with data analysis and modelling   You have coding proficiency in at least one modern language such as Python, R and Spark   You have experience in cloud-first design, preferably AWS or Azure   You're well-versed in Machine Learning and data mining    "", '""I can influence my income.""', 'Complimentary Experience', 'Req ID', ' You have experience in cloud-first design, preferably AWS or Azure ', 'Relocation benefits offered', ' You have strengths in leadership, interpersonal, and problem solving skills with the ability to continually learn new concepts and technologies and effectively apply them ', '""I can apply in less than 4 minutes.""', ' Excellent technical knowledge of performance tuning and query optimization across large data sets, and exposure to bottlenecks at the storage, network or compute layers ', 'Job Segment ', '""I can learn more about Capital Group."" ', ' You have experience with integration of data from multiple sources and a knowledge of ETL/ELT technologies and tools like Informatica and AWS solutions ', '  Enjoy generous time-away and health benefits from day one, with the opportunity for flexible work options   Receive 2-for-1 matching gifts for your charitable contributions and the opportunity to secure annual grants for the organizations you love   Access on-demand professional development resources that allow you to hone existing skills and learn new ones  ', ' Working experience with data cataloging and enablement through APIs ', "" You're well-versed in Machine Learning and data mining "", ' Access on-demand professional development resources that allow you to hone existing skills and learn new ones ', 'I am the person Capital Group is looking for.""', '  Excellent technical knowledge of performance tuning and query optimization across large data sets, and exposure to bottlenecks at the storage, network or compute layers   Working experience with data cataloging and enablement through APIs   Excellent understanding of traditional RDBMS   You have strengths in leadership, interpersonal, and problem solving skills with the ability to continually learn new concepts and technologies and effectively apply them   ']",Not Applicable,Full-time,Information Technology,Financial Services,2021-03-18 14:34:51
Data Engineer III,Expedia Group,"Austin, TX",2 days ago,Be among the first 25 applicants,"['', 'Competitive health and insurance benefits', 'Competitive salary', 'Technologies We Use', 'Professional development experience in Java/Python/Scala', 'Develop fast, scalable, highly available, and reliable property status software that will control the enabled status for all of Expedia’s propertiesYou will scale our services to tens of thousands of requests per secondUse real-time data to understand performance and ensure system scalabilityDockerize our apps and services for cloud deploymentYou will develop property status funnel features that will drive our business through real-time feedback loopsScale our public API’s to give other partners the ability to leverage new Expedia Group servicesScale our private API’s to allow enhanced Expedia UI experiencesSimplify our core property status workflow to enhance both our travelers’ & suppliers’ experience', 'Experience building data pipelines with data from event streams, on distributed data systems (AWS/Hadoop)', 'You will develop property status funnel features that will drive our business through real-time feedback loops', 'Work in an agile environment with product management and operations', 'You will scale our services to tens of thousands of requests per second', 'Responsibilities', 'Develop fast, scalable, highly available, and reliable property status software that will control the enabled status for all of Expedia’s properties', 'Why Join Us', 'Simplify our core property status workflow to enhance both our travelers’ & suppliers’ experience', 'Opportunities to showcase your work on our tech blog and internal & external conferences', 'Backend development building applications from concept to completion', 'About Vrbo', 'Electronic, adjustable stand-up desk', 'Competitive health and insurance benefitsCompetitive salaryAnnual target bonus or commissionParental leave for up to 20 weeks (dependent on eligibility)Paid vacation and sick timeEmployee Stock Purchase ProgramFree snacks and beveragesFrequent company update talks with our leadership teamFree listing on Vrbo.comElectronic, adjustable stand-up deskDiscounted Metro & Rail passCasual dress', 'Who You Are', 'Create and maintain quality software using premier tools: Git, Splunk, Datadog, New Relic, etc.', 'Annual target bonus or commission', 'Free listing on Vrbo.com', 'You will commit to vigilantly rewriting, refactoring, and perfecting code', 'Frequent company update talks with our leadership team', 'Parental leave for up to 20 weeks (dependent on eligibility)', 'Casual dress', ""Bachelor’s + 7 years or Master's + 5 years in Computer Science or Engineering or related experience ."", 'Paid vacation and sick time', ""Bachelor’s + 7 years or Master's + 5 years in Computer Science or Engineering or related experience .Experience building data pipelines with data from event streams, on distributed data systems (AWS/Hadoop)Batch and/or stream processing experience using Spark, K-Streams, KafkaExperience building low-latency data product APIsProfessional development experience in Java/Python/Scala"", 'Experience building low-latency data product APIs', 'Scale our public API’s to give other partners the ability to leverage new Expedia Group services', 'Employee Stock Purchase Program', 'Dockerize our apps and services for cloud deployment', 'Benefits & Perks', 'Backend development building applications from concept to completionYou will commit to vigilantly rewriting, refactoring, and perfecting codeDedicated to delivering tested and optimized high performance code for a distributed SOA environmentDevelop quality scalable, tested and reliable applications using industry standard methodologiesWork in an agile environment with product management and operationsCreate and maintain quality software using premier tools: Git, Splunk, Datadog, New Relic, etc.Participate in resolution of production issues and lead efforts toward solutionsOpportunities to showcase your work on our tech blog and internal & external conferences', 'Scale our private API’s to allow enhanced Expedia UI experiences', 'Develop quality scalable, tested and reliable applications using industry standard methodologies', 'Discounted Metro & Rail pass', 'Use real-time data to understand performance and ensure system scalability', ""What You'll Do"", 'Participate in resolution of production issues and lead efforts toward solutions', 'Free snacks and beverages', 'Java 8, Python, Scala, Spark, K-Streams, Hadoop, Elasticsearch, Jetty, and Linux', 'Batch and/or stream processing experience using Spark, K-Streams, Kafka', 'Dedicated to delivering tested and optimized high performance code for a distributed SOA environment']",Mid-Senior level,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Engineer,TargetCW,San Francisco Bay Area,,N/A,"['', 'Maintain a high level of trust in these data sets: keep existing data sources fresh against changing requirements, definitions, and data quality issues.\xa0', 'Experience with best practices for development including query optimization, version control, code reviews, and documentation', 'Improve data discovery and literacy: Create documentation on data relationships and dependencies', 'Familiarity with AWS services such as Redshift and S3', 'Estimated Duration:\xa0', 'Data Engineer (W2 ONLY)', 'For this role, we’re looking for a data engineer to join the Creator Analytics team: Responsibilities', 'Strong proficiency in SQL\xa0', '3+ years of experience in data engineeringStrong proficiency in using one of the script languages, such as PythonStrong proficiency in SQL\xa0Familiarity with AWS services such as Redshift and S3Experience using cloud services to automate the data pipelinesExperience with building data warehouses and dimensional modelingExperience with best practices for development including query optimization, version control, code reviews, and documentationExperience with Salesforce backendsFluency with a Git/GitHub version control workflow\xa0Good to have: experience with Tableau', ""We are the\xa0world's leading live streaming platform for gamers and the things we love. We make it possible to watch, play and chat with millions of other fans from around the world. We are looking for a Data Engineer to join our team ASAP! If you have 3+ years of experience in Data Engineering along with Python and SQL, we want to speak to you!"", 'Good to have: experience with Tableau', 'Estimated Duration:\xa09 months with a possibility of extending up to 18+ months ', 'Requirements', '**MUST BE US CITIZEN, NO C2C**', 'Full Time', 'Strong proficiency in using one of the script languages, such as Python', 'Experience with Salesforce backends', 'Improve reliability and performance: Conduct unit, integration, and system tests on our data sources in order to validate data against source systems, and continuously optimize performance in order to improve query speed and reduce cost.', 'Build ETL pipelines between our Salesforce instance and our data warehouseMaintain a high level of trust in these data sets: keep existing data sources fresh against changing requirements, definitions, and data quality issues.\xa0Improve reliability and performance: Conduct unit, integration, and system tests on our data sources in order to validate data against source systems, and continuously optimize performance in order to improve query speed and reduce cost.Improve data discovery and literacy: Create documentation on data relationships and dependencies', '3+ years of experience in data engineering', 'Build ETL pipelines between our Salesforce instance and our data warehouse', '$70-75+/hr DOE', 'Fluency with a Git/GitHub version control workflow\xa0', 'Remote', 'Experience with building data warehouses and dimensional modeling', '\xa0', 'Experience using cloud services to automate the data pipelines', 'PLEASE SUBMIT YOUR RESUME TO BE CONSIDERED!']",Mid-Senior level,Full-time,Engineering,Computer Games,2021-03-18 14:34:51
Data Engineer,Big Cloud,San Francisco Bay Area,,N/A,"['', 'This exciting technology company with over 30 million users, is currently seeking to recruit a Data Engineer.', 'Experience in building and deploying ETL data pipelines (Spark, ElasticSearch)', 'The company are about to launch their product globally and you will contribute by developing the new data products to help during this exciting period of growth.', 'Architectural mindset and the ability to look at problems holistically', 'As the Data Engineer, you’ll develop new backend systems for natural language, search, recommendation, computer vision and user behaviour applications.', 'Production level coding skills in Scala, Clojure or Python', 'Good communication skills', 'You will have…', 'Experience with cloud platforms (AWS, GCP)', 'You’ll design, develop and deploy data processing and streaming platforms and data pipelines, capable of training algorithms with multi-millions of daily data points.', 'A minimum of 3-5 years experience in building and deploying production software', 'A minimum of 3-5 years experience in building and deploying production softwareProduction level coding skills in Scala, Clojure or PythonExperience in building and deploying ETL data pipelines (Spark, ElasticSearch)Experience with cloud platforms (AWS, GCP)Good communication skillsArchitectural mindset and the ability to look at problems holistically', 'Are you an experienced Data Engineer? Do you have experience supporting data science/machine learning projects?']",Mid-Senior level,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Engineer,General Mills,"Minneapolis, MN",6 days ago,Be among the first 25 applicants,"['', 'Act as a key technical leader within General Mills', 'Ability to research, plan, organize, lead, and implement new processes or technology', 'Experience with agile techniques or methods', 'Python, Scala or Java development experienceFamiliarity with Kafka Familiarity with the Linux operating systemExperience with agile techniques or methods', 'Collaboratively troubleshoot technical and performance issues in the big data ecosystem', 'Generate and implement your own ideas on how to improve the operational and strategic health of big data ecosystem', 'Bachelor’s Degree; Computer Science, MIS, or Engineering preferred', 'Minimum 2 years of IT experience, 3+ preferredCloud data experience', 'Effective analytical and technical skills.', 'Act as a key technical leader within General MillsDesign, create, code, and support a variety of ETL solutions (potentially including but not limited to: Talend Studio, Python, Scala, Kafka, SAP Data Services, or others)Generate and implement your own ideas on how to improve the operational and strategic health of big data ecosystemParticipate in the evaluation, implementation and deployment of emerging tools & process in the big data space.Partner with business analysts and solutions architects to deliver business initiatives.Collaboratively troubleshoot technical and performance issues in the big data ecosystem', 'Minimum Qualifications', 'Strong understanding of Hadoop fundamentals', 'Development experience using Hive and/or Spark', 'Python, Scala or Java development experience', 'Process mindset with experience creating, documenting and implementing standard processes', 'Familiarity with Kafka ', 'Design, create, code, and support a variety of ETL solutions (potentially including but not limited to: Talend Studio, Python, Scala, Kafka, SAP Data Services, or others)', 'Familiarity with the Linux operating system', 'Participate in the evaluation, implementation and deployment of emerging tools & process in the big data space.', 'Ability to work in a team environment', 'Effective verbal and written communication and influencing skills.', 'Database development experience using Oracle, SQL Server, SAP BW or SAP HANA', 'Key Responsibilities', 'Partner with business analysts and solutions architects to deliver business initiatives.', 'Bachelor’s Degree; Computer Science, MIS, or Engineering preferredMinimum 2 years of IT experience, 3+ preferredCloud data experienceStrong understanding of Hadoop fundamentalsDatabase development experience using Oracle, SQL Server, SAP BW or SAP HANAJob Scheduling experienceProcess mindset with experience creating, documenting and implementing standard processesDevelopment experience using Hive and/or SparkEffective verbal and written communication and influencing skills.Effective analytical and technical skills.Ability to work in a team environmentAbility to research, plan, organize, lead, and implement new processes or technology', 'Overview', 'Job Scheduling experience', 'Company Overview', 'Preferred Qualifications']",Entry level,Full-time,Information Technology,Consumer Goods,2021-03-18 14:34:51
Data Engineer,Chase International Partners,"Boston, MA",2 weeks ago,111 applicants,"['', 'The Data Engineer will be responsible for building and maintaining data management processes for our cloud-based Investment Data Analytics Platform, a strategic asset that drives quantitative-driven research and investment decision-making.\xa0 ', 'Hired staff will design and build data validations, transformations, normalizations, reports and extracts, and integration processes that deliver unstructured and structured content to our cloud-based Data Lake and SQL Warehouse.\xa0\xa0 ', 'Ability to use analytical skills to translate business ideas into technology solutions', 'Understanding of DevOps and CI/CD practices and tools', 'Experience working with distributed external resources and vendor teams (onshore and offshore)', 'Job Skills and Abilities', 'The staff will partner with business users, senior architects, and infrastructure engineers to form complete end-to end-solutions.\xa0 They thrive in an evolving fast-paced environment, and bring a work style marked by high energy, flexibility, quick learning, and collaboration.', 'In-depth SQL, plus Python, R and/or similar analytical languages', 'Comfort and confidence interacting with business users', 'Mix of education and experience with SQL Server, SQL Server Integration Services, and Azure Data Services (Data Factory, Databricks)', 'Clear communicator of technical details both verbally and in writing', 'Track record and skill sets demonstrated on high-intensity, high-complexity projects', 'High energy and collaborative style; flexibility and willingness to take on a broad range of engineering and support assignments', 'My client, a leader within the alternative investment space is growing. We have seen them grow in 4 years from 300 staff to 650 plus and $30 bil AUM to over $70 bil AUM. They have been hiring all year and still growing, an amazing company to join.', 'Data modeling and design understanding, including conceptual, logical, and physical modeling', 'Mix of education and experience with SQL Server, SQL Server Integration Services, and Azure Data Services (Data Factory, Databricks)In-depth SQL, plus Python, R and/or similar analytical languagesData modeling and design understanding, including conceptual, logical, and physical modelingUnderstanding of DevOps and CI/CD practices and toolsHigh energy and collaborative style; flexibility and willingness to take on a broad range of engineering and support assignmentsTrack record and skill sets demonstrated on high-intensity, high-complexity projectsComfort and confidence interacting with business usersAbility to use analytical skills to translate business ideas into technology solutionsExperience in Agile delivery frameworkExperience working with distributed external resources and vendor teams (onshore and offshore)Clear communicator of technical details both verbally and in writing', 'Experience in Agile delivery framework']",Mid-Senior level,Full-time,Information Technology,Venture Capital & Private Equity,2021-03-18 14:34:51
Data Analytics Engineer,Vouch Insurance,San Francisco Bay Area,1 week ago,76 applicants,"['', 'About You:', '3+ years experience developing ETL workflows as a data analytics engineer, data engineer, or data analyst', ""We're doing this by making insurance fast, responsive, and focused on our customer -high growth and innovative companies. Instead of printed PDF applications and week-long waits, Vouch is building new technology to solve real problems, writing policies that actually cover relevant startup scenarios, and designing simple experiences in an otherwise frustrating industry."", 'Experience building business reporting processes (e.g. for finance, sales, or business operations)', 'Clearly documenting data models with source, description and field definitions for better collaboration, maintainability and usability.', '\ufeffNice to Have:', 'Exposure to and passion for early-stage startups and/or high growth environments', 'Vouch is looking for a Data Analytics Engineer to join our team. We’re looking for someone who lives and breathes data and gets excited about data warehouses, building novel data sets, and maintaining data quality. You will have the opportunity to quickly contribute critical to Vouch’s data assets and help to shape fundamental aspects of our data-driven culture.\xa0', 'Expert in SQL, capable in Python, and experience with Business Intelligence tools such as Looker, Mode Analytics or Tableau', 'Establishing engineering best practices and methodologies to ensure data transformations and computations are accurate, efficient, and tested.', '3+ years experience developing ETL workflows as a data analytics engineer, data engineer, or data analystExpert in SQL, capable in Python, and experience with Business Intelligence tools such as Looker, Mode Analytics or TableauExperience building business reporting processes (e.g. for finance, sales, or business operations)Uses software development best practices with a focus on testing, reliability and maintainabilityExperience working with cloud-based data warehouses like Snowflake, Redshift, or BigQueryExperience with data transformation tooling (dbt is preferred), data pipeline services like Stitch and Fivetran, and orchestration tools like AirflowYou can effectively talk (and listen) to engineers, designers, executives, and other stakeholders.', 'You will be responsible for:', 'Uses software development best practices with a focus on testing, reliability and maintainability', 'You can effectively talk (and listen) to engineers, designers, executives, and other stakeholders.', 'Experience building executive level slide decks\xa0Exposure to and passion for early-stage startups and/or high growth environmentsA background in insurance or other regulated categories\xa0\xa0', 'The Job:\xa0', ""Insurance... sounds slow, old-fashioned, and unexciting. Exactly. Insurance is broken, and it's failing fast-moving, innovative startups."", 'About Vouch:', 'A background in insurance or other regulated categories\xa0\xa0', 'Experience with data transformation tooling (dbt is preferred), data pipeline services like Stitch and Fivetran, and orchestration tools like Airflow', 'Experience working with cloud-based data warehouses like Snowflake, Redshift, or BigQuery', '\xa0', 'Vouch believes in putting our people first and building a diverse team is at the front of everything that we do. We welcome people from different backgrounds, experiences, and perspectives. We are an equal opportunity employer and celebrate the diversity of our growing team.\xa0', 'Working with dbt, Snowflake, Airflow, Python, Stitch, and git, and we welcome new ideas.\xa0', 'Working with our sales, marketing, product, and insurance teams to design and build data sets to drive Vouch’s business', 'Vouch is a new, technology-first insurance company, backed with $100M in funding from world-class investors. Like Stripe for payments or Brex for credit cards, Vouch is creating the go-to business insurance for high growth companies.', 'Providing clean, transformed data that is ready to be piped to our product, CRM, Finance, Underwriting, and other relevant systems', 'Experience building executive level slide decks\xa0', 'Working with our sales, marketing, product, and insurance teams to design and build data sets to drive Vouch’s businessSetting up and maintaining timely and reliable ingestion of external data sources via our data loading platforms.Providing clean, transformed data that is ready to be piped to our product, CRM, Finance, Underwriting, and other relevant systemsClearly documenting data models with source, description and field definitions for better collaboration, maintainability and usability.Establishing engineering best practices and methodologies to ensure data transformations and computations are accurate, efficient, and tested.Working with dbt, Snowflake, Airflow, Python, Stitch, and git, and we welcome new ideas.\xa0', 'Setting up and maintaining timely and reliable ingestion of external data sources via our data loading platforms.']",Associate,Full-time,Engineering,Insurance,2021-03-18 14:34:51
Cloud Data Engineer - Data & Analytics Platform,Moody's Analytics,"New York, NY",2 days ago,Be among the first 25 applicants,"['', 'EEO Policy', ' Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.  Experience building and optimizing ‘big data’ data pipelines, architectures and data sets. Previous AWS data lake experience is highly preferred.  Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.  Strong analytic skills related to working with unstructured datasets.  Build processes supporting data transformation, data structures, metadata, dependency and workload management.  A successful history of manipulating, processing and extracting value from large disconnected datasets.  Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.  Strong project management and organizational skills.  Experience supporting and working with cross-functional teams in a dynamic environment.  We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools: Experience with big data tools: Hadoop, Spark, Presto, Kafka, etc.  Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.  Experience with data pipeline and workflow management tools: Airflow, Azkaban, Luigi, etc.  Experience with AWS cloud services: EC2, S3, EMR, RDS, Athena, and Glue (To name a few)  Experience with dataops using CI/CD is highly desirable  Experience with stream-processing systems: Storm, Spark-Streaming, etc.  Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc. ', ""Working at Moody's"", 'LOB/Department', 'Entity', 'Regular/Temporary', 'Experience Level', ' Keep our data separated and secure across national boundaries through multiple data centers and AWS regions. ', ' A successful history of manipulating, processing and extracting value from large disconnected datasets. ', ' Experience with AWS cloud services: EC2, S3, EMR, RDS, Athena, and Glue (To name a few) ', 'Line of Business', ' Experience with big data tools: Hadoop, Spark, Presto, Kafka, etc. ', 'Role/Responsibilities', ' Create and maintain optimal data pipeline architecture,  Assemble large, complex data sets that meet functional / non-functional business requirements.  Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.  Ensuring the data quality, and data throughout all the data lake Tiers  Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Presto, Spark within AWS platform  Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.  Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.  Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.  Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader. ', 'Securities Trading Policy (STP)', ' Experience with big data tools: Hadoop, Spark, Presto, Kafka, etc.  Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.  Experience with data pipeline and workflow management tools: Airflow, Azkaban, Luigi, etc.  Experience with AWS cloud services: EC2, S3, EMR, RDS, Athena, and Glue (To name a few)  Experience with dataops using CI/CD is highly desirable  Experience with stream-processing systems: Storm, Spark-Streaming, etc. ', ' Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases. ', 'City', ' Experience with dataops using CI/CD is highly desirable ', 'Qualifications', ' Ensuring the data quality, and data throughout all the data lake Tiers ', ' Strong analytic skills related to working with unstructured datasets. ', ' We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools: Experience with big data tools: Hadoop, Spark, Presto, Kafka, etc.  Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.  Experience with data pipeline and workflow management tools: Airflow, Azkaban, Luigi, etc.  Experience with AWS cloud services: EC2, S3, EMR, RDS, Athena, and Glue (To name a few)  Experience with dataops using CI/CD is highly desirable  Experience with stream-processing systems: Storm, Spark-Streaming, etc. ', ' Create and maintain optimal data pipeline architecture, ', ' Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics. ', ' Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. ', ' Experience with data pipeline and workflow management tools: Airflow, Azkaban, Luigi, etc. ', ' Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc. ', ' Assemble large, complex data sets that meet functional / non-functional business requirements. ', ' Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. ', ' Build processes supporting data transformation, data structures, metadata, dependency and workload management. ', ' Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Presto, Spark within AWS platform ', ' Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. ', ' Experience building and optimizing ‘big data’ data pipelines, architectures and data sets. Previous AWS data lake experience is highly preferred. ', ' Experience supporting and working with cross-functional teams in a dynamic environment. ', ' Strong project management and organizational skills. ', ' Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader. ', ' Experience with relational SQL and NoSQL databases, including Postgres and Cassandra. ', ' Experience with stream-processing systems: Storm, Spark-Streaming, etc. ', 'Job Category', 'Job Sub Category', ' Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores. ', 'Job Req ID']",Not Applicable,Full-time,Information Technology,Financial Services,2021-03-18 14:34:51
Data Engineer I,Thermo Fisher Scientific,"Frederick, MD",4 weeks ago,25 applicants,"['', 'Proficiency in using query languages such as SQL', 'Good scripting and programming skills', 'Strong communication skills and ability to work effectively across a matrix organization', '2-4 years of data engineering experience', 'Ability to interact professionally with a diverse group including VPs, directors, managers, subject matter experts and end-users.', 'Direct interaction with the business to understand and analyze business problems, derive insights and recommend solutions', 'Must be a business partner, not merely a technical expert – this position plays an active role providing actionable insight into the Clinical Trials Division', 'Extending company’s data with third party sources of information when needed', '2-4 years of data engineering experienceData-oriented personalityGreat communication skillsExperience with data visualization tools, such as PowerBI, Tableau or CognosProficiency in using query languages such as SQLExperience with NoSQL databasesGood applied statistics skills, such as distributions, statistical testing, regression, etc.Experience with common data science toolkitsGood scripting and programming skillsAbility to partner with management at all levels and to lead major projects and initiativesStrong communication skills and ability to work effectively across a matrix organizationBA/BS degree in finance, mathematics, computer science preferred or equivalent work experience ', 'Strong analytical skills – and ability to use those skills to influence and drive change', 'Minimum Requirements/Qualifications', 'Position Summary', 'Direct interaction with the business to understand and analyze business problems, derive insights and recommend solutionsWorking independently, or with functional analysts, to create reports, data sets and mechanisms to provide visibility to business dataExecuting ad-hoc analysis and presenting results in a clear mannerExtending company’s data with third party sources of information when neededCollaborate with IT and business partners to ensure data quality, integrity, and accuracy across the global CTD data storesEstablish good working relationships with peers in other divisions and explore joint system and process improvement opportunities.Maintain MS SQL Server and Oracle database environments (security, tables, views, packages, SQL Agent jobs, SSAS database, Integration Services)Develop/Maintain and Administer PowerBI dashboards, gateways and associated infrastructure', 'Good applied statistics skills, such as distributions, statistical testing, regression, etc.', 'Global experience', 'Develop/Maintain and Administer PowerBI dashboards, gateways and associated infrastructure', 'Executing ad-hoc analysis and presenting results in a clear manner', 'BA/BS degree in finance, mathematics, computer science preferred or equivalent work experience ', 'Collaborate with IT and business partners to ensure data quality, integrity, and accuracy across the global CTD data stores', 'Experience with data visualization tools, such as PowerBI, Tableau or Cognos', 'Maintain MS SQL Server and Oracle database environments (security, tables, views, packages, SQL Agent jobs, SSAS database, Integration Services)', 'Experience with common data science toolkits', '10% travel requirement', 'Key Success Factors', 'Great communication skills', 'Data-oriented personality', 'Strong organizational and communication skills, and proven ability to adapt style to different situations and peopleMust be a business partner, not merely a technical expert – this position plays an active role providing actionable insight into the Clinical Trials DivisionStrong analytical skills – and ability to use those skills to influence and drive changeExcellent interpersonal and communication skills (both verbal and written).Ability to interact professionally with a diverse group including VPs, directors, managers, subject matter experts and end-users.Self-motivated; bias for actionGlobal experience10% travel requirement', 'Key Responsibilities', 'Establish good working relationships with peers in other divisions and explore joint system and process improvement opportunities.', 'Strong organizational and communication skills, and proven ability to adapt style to different situations and people', 'Experience with NoSQL databases', 'Self-motivated; bias for action', 'Ability to partner with management at all levels and to lead major projects and initiatives', 'Excellent interpersonal and communication skills (both verbal and written).', 'Working independently, or with functional analysts, to create reports, data sets and mechanisms to provide visibility to business data']",Not Applicable,Full-time,Other,Pharmaceuticals,2021-03-18 14:34:51
Data Engineer,"The Squires Group, Inc.","Arlington, VA",3 weeks ago,114 applicants,"['', ' #Di', ' Bachelor’s Degree AND 3 years of professional experience (or 3 additional years in lieu of a degree) ', ' Per our Federal Government Contract, candidates must be U.S. Citizens with an Active or Interim Secret Clearance ', ' Data Warehouse ETL Testing ', ' Work-Life Balance - We work hard; we work smart and have quality time for family and ""life.""', ' Bachelor’s Degree AND 3 years of professional experience (or 3 additional years in lieu of a degree)  3+ years of experience with the following: Informatica Data Quality  Data Warehouse Tools  Data Audit and Profiling  Data Warehouse ETL Testing  Informatica Axon (preferred)  Informatica Enterprise Data Catalog (preferred)  Well versed in the fundamentals of building ETLs and data models  Solid AWS experience is a plus  Per our Federal Government Contract, candidates must be U.S. Citizens with an Active or Interim Secret Clearance ', ' Our Commitment to You - We offer competitive pay, multi-year projects, and a list of exciting clients.', ' Check out our Referral Program! ', ' Solid AWS experience is a plus ', 'Secret Clearance', 'Arlington, VA', ' We Care About You – We help you meet your career goals and continuously support your efforts in the field.', ' Golden Rule - We treat our consultants the way we want to be treated: with integrity, professionalism, and trust.', ' Overview ', ' 3+ years of experience with the following: Informatica Data Quality  Data Warehouse Tools  Data Audit and Profiling  Data Warehouse ETL Testing  Informatica Axon (preferred)  Informatica Enterprise Data Catalog (preferred) ', 'U.S. Citizens with an Active or Interim Secret Clearance.', 'Qualifications', 'U.S. Citizens with an Active or Interim ', '4 Reasons To Join The Squires Group, Inc.', ' The Squires Group, Inc. is an Equal Opportunity/Affirmative Action Employer. ', ' Data Audit and Profiling ', ' Informatica Enterprise Data Catalog (preferred) ', ' Informatica Data Quality ', ' Well versed in the fundamentals of building ETLs and data models ', ' Informatica Axon (preferred) ', 'Data Engineer', ' Data Warehouse Tools ', 'Overview ', ' Informatica Data Quality  Data Warehouse Tools  Data Audit and Profiling  Data Warehouse ETL Testing  Informatica Axon (preferred)  Informatica Enterprise Data Catalog (preferred) ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,mParticle,United States,2 weeks ago,95 applicants,"['', 'Build, maintain, and document automated ETL pipelines', '1+ years of proven success working in backend of large-scale software development', '1+ years of proven success working in backend of large-scale software developmentBS/MS in Computer Science or related fieldExpertise in SQL-like languages and toolsAbility to learn quickly and display solid analytical/engineering thinkingExperience in building scalable and distributed data pipelines for analytics processes and/or training machine learning modelsAble to design and develop quality cloud-based systems and operate them in an automated fashionDemonstrable experience in taking projects from spec to releaseWorking knowledge of Druid, Fivetran, AWS (Redshift), Looker, Spark, Luigi/Airflow, etc..', 'Expertise in SQL-like languages and tools', 'Able to design and develop quality cloud-based systems and operate them in an automated fashion', '\ufeffDesired Experience', 'Employment opportunities are available to all applicants without regard to race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.', 'Experience in building scalable and distributed data pipelines for analytics processes and/or training machine learning models', 'Work with business stakeholders and product managers to define product requirementsArchitect, implement, and support scalable/reliable data pipelines and data applications in a cost efficient wayBuild, maintain, and document automated ETL pipelinesContinuously monitor and optimize the pipelines and data schemasBuild automated alerting to improve efficiency of our team’s operations, including but not limited to time series forecasting, anomaly detection, text classifications, etc.', 'Responsibilities', 'Here at mParticle we embrace the differences that make us unique. We are dedicated to building an inclusive environment that fosters respect and celebrates an array of backgrounds and perspectives.', 'Founded in 2013, mParticle is the leading customer data platform that unlocks the full power of data for businesses. The company empowers brands to accelerate their growth strategy to keep pace with their customers by providing the most advanced data platform for web and apps across all devices in the marketplace. A trusted partner among renowned brands such as Airbnb, Foursquare, Hulu, King, and Spotify among many others, the mParticle platform has grown to manage over 1 billion mobile users each month, capturing over $5 billion in ecommerce transactions and processes over 250 billion API calls. Recognized as one of Crain’s 100 Best Places to Work in New York City and named to Gartner’s “Cool Vendors in Mobile App Development” list, mParticle has 45 employees and is headquartered in New York City with offices in San Francisco, Florida, Seattle and London.', '*At this time, mParticle is unable to sponsor visas for this role, unfortunately.', 'Build automated alerting to improve efficiency of our team’s operations, including but not limited to time series forecasting, anomaly detection, text classifications, etc.', 'Architect, implement, and support scalable/reliable data pipelines and data applications in a cost efficient way', 'About mParticle', 'Work with business stakeholders and product managers to define product requirements', 'BS/MS in Computer Science or related field', 'Continuously monitor and optimize the pipelines and data schemas', 'Working knowledge of Druid, Fivetran, AWS (Redshift), Looker, Spark, Luigi/Airflow, etc..', ""We're looking for a talented and technically well-rounded person who loves to tackle complex problems and is passionate about building scalable and reliable data pipelines and applications, e.g., BI reporting, data transformations/integrations, machine learning, etc."", 'Ability to learn quickly and display solid analytical/engineering thinking', 'Demonstrable experience in taking projects from spec to release', 'Here at mParticle, everyone is equal. We\xa0believe strongly in our values\xa0and are looking for someone who demonstrates empathy and sincerity to all roles and teammates. Our clients include marketing and engineering functions for some of the largest apps in the world and our platform processes nearly one-third of the world’s smartphone traffic.', 'We are seeking someone who wants to be a contributor in a small, dynamic work environment, loves a challenge, and has a strong balance of technical and people skills. As a Data Engineer, you will be a member of our backend engineering team and collaborate to help specify, design, and develop data pipelines/applications meeting company and product requirements. You will help evaluate and choose service protocols and architectures, write high quality, maintainable code in a fast-paced startup environment with tight schedules, and be fully responsible for ensuring quality and proper deployment of the written software.']",Mid-Senior level,Full-time,Engineering,Internet,2021-03-18 14:34:51
Data Engineer,PrecisionGx,"Middle Smithfield, PA",1 month ago,99 applicants,"['', ' Big data technology Spark 2.4, Presto Cloud technology AWS Glue, Lambda, S3 Industry experience in health insurance, specifically in payment integrity is a big plus ', 'Benefits', 'Some Other Nice To Haves Are Experience With', ' Health', 'Cloud technology AWS Glue, Lambda, S3', 'Languages Python 3.4+, SQL (any dialect)', ' Travel', 'Big data technology Spark 2.4, Presto', 'Requirements', 'Industry experience in health insurance, specifically in payment integrity is a big plus']",Associate,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Principal Data Engineer,Powerlytics Inc.,Greater Philadelphia,2 days ago,33 applicants,"['', 'Additional Requirements:', 'Passion for building trustworthy, reliable, stable, and fast data products that serve customer needs.Software engineering experience and discipline in design, test, source code management and CI/CD practices.Experience in data modeling and developing SQL database solutions.Deep understanding of key algorithms and tools for developing high efficiency data processing systems.Experience in in Financial Service sectors such as Banking, Insurance or Wealth Management is a plus.', 'Compensation:', '6+ years of deep experience with large scale distributed big data systems, pipelines, and data processing.', 'Passion for building trustworthy, reliable, stable, and fast data products that serve customer needs.', 'As part of a small team, you will own significant responsibility in crafting, developing, and maintaining our large-scale ETL pipelines, storage, and processing services. The team is looking for a self-driven data engineer to help design and build data pipelines that allow our team to develop the high quality, scalable data assets. In addition to the design and implementation of this infrastructure, you will be responsible for communicating with data scientists and other team members to determine the most effective models to improve data access, promote econometrics research, and eventually ship groundbreaking features to our customers.', ""BS or MS in Computer Science, Engineering or equivalent. Master's degree preferred."", 'Practical hands-on programming and engineering experience in R, Python, Java, and Scala. Strong R experience is required.', 'Experience in stream data processing and real time analytics of data generated from user interaction with applications is a plus.', 'Experience in in Financial Service sectors such as Banking, Insurance or Wealth Management is a plus.', 'Willing to take ownership of pipeline and can communicate concisely and persuasively to a varied audience including data provider, engineering, and analysts.', 'Summary:', 'Proven experience using distributed computer frameworks on Hadoop, Spark, Cassandra, Kafka, AirFlow, distributed SQL, and NoSQL query engines.', 'Sponsorship: This role does not offer visa sponsorship', 'Aptitude to independently learn new technologies.', 'Education & Experience: ', 'Prior experience with modern web services architectures, cloud platforms, preferably AWS and Databricks.', 'Key Qualifications:', 'Excellent verbal and written communication skills is required.', '6+ years of deep experience with large scale distributed big data systems, pipelines, and data processing.Practical hands-on programming and engineering experience in R, Python, Java, and Scala. Strong R experience is required.Proven experience using distributed computer frameworks on Hadoop, Spark, Cassandra, Kafka, AirFlow, distributed SQL, and NoSQL query engines.Able to setup large scale data pipeline and data monitoring system to make sure overall pipeline is healthy.Willing to take ownership of pipeline and can communicate concisely and persuasively to a varied audience including data provider, engineering, and analysts.Ability to identify, prioritize, and answer the most critical areas where analytics and modeling will have a material impact.Experience in stream data processing and real time analytics of data generated from user interaction with applications is a plus.Prior experience with modern web services architectures, cloud platforms, preferably AWS and Databricks.Understanding of design and development of large scale, high throughput and low latency applications is a plus.Aptitude to independently learn new technologies.Excellent verbal and written communication skills is required.', 'Imagine what you could do here. Powerlytics, is a cutting edge, venture backed company with a portfolio of products and solutions underpinned by predictive analytics powered by proprietary databases of the anonymized tax returns of all households (150 million+) and for-profit businesses (30 million+) in the U.S. Powerlytics’ clients include top 5 banks, insurance companies, asset managers as well as alternative lenders, marketing firms and global consulting firms, among others. Powerlytics team is looking for a highly qualified Principal Data Engineer experienced with big data and large-scale analytics systems for our team’s substantial data analytics needs. This is an outstanding opportunity to join a focused team and work collaboratively to make a significant impact on our organization. You will be contributing to a large-scale data platform and providing end-to-end analytics solutions to transform rich data at Powerlytics scale into actionable insights. Your data sets will help us determine important features, monitor data product launches and understand data usage and quality in detail. We are looking for Data Engineers who love to build end to end analytics solutions for their customers and can produce high quality data artifacts at scale. The ideal candidate has experience implementing extendable frameworks on top of Spark and understands the inner workings of Spark execution.', 'Principal Data Engineer', 'Ability to identify, prioritize, and answer the most critical areas where analytics and modeling will have a material impact.', 'Description:', '\xa0', 'Position offers a very competitive compensation package consisting of a base salary, bonus and equity as well as a benefits package.', 'If interested and qualified, please send a message with your resume to briefly tell us why you are interested in this role and how you meet the requirements listed above. Please include the number of years of Spark programming experience in an industry setting you bring as well.\xa0Please also confirm that you are legally authorized to work in the US and do not require a work visa, now or in the future.\xa0Please send to employment@powerlytics.com', 'Able to setup large scale data pipeline and data monitoring system to make sure overall pipeline is healthy.', 'Understanding of design and development of large scale, high throughput and low latency applications is a plus.', 'Software engineering experience and discipline in design, test, source code management and CI/CD practices.', 'To Apply:', 'Experience in data modeling and developing SQL database solutions.', 'Deep understanding of key algorithms and tools for developing high efficiency data processing systems.', 'Location: Remote (should be able to work in EST time zone with occasional Post COVID travel to company headquarters in Philadelphia, PA area)']",Associate,Full-time,Information Technology,Information Services,2021-03-18 14:34:51
Lead Data Engineer,System Soft Technologies,"North Carolina, United States",24 hours ago,Be among the first 25 applicants,"['', 'Cloud big data components', '4 years of experience in Hadoop or any Cloud Bigdata components (specific to the Data Engineering role)', '7+ years in software engineering', 'Managing RDMS', '6 years of experience in systems analysis, including defining technical requirements and performing ', 'Lead Data Engineer ', 'Working incident tracking software', 'Python', 'Cloudera a plus', '(Spark Streaming, MLib), Kafka or equivalent Cloud Bigdata components', 'ETL', '6+ implementing SDLC', '4 years of experience working with an IT Infrastructure Library (ITIL) framework', 'Needs to have Lead experience', '6 years of experience writing technical documentation in a software development environment', 'high level design for complex solutions', 'Responsibilities:', 'Experience working with Continuous Integration/Continuous Deployment tools', 'Scala', '4 years of experience leading teams, with or without direct reports', 'Java', '6 years of IT experience developing and implementing business systems within an organization', '6 years of IT experience developing and implementing business systems within an organization6 years of experience working with defect or incident tracking software6 years of experience writing technical documentation in a software development environment4 years of experience working with an IT Infrastructure Library (ITIL) framework4 years of experience leading teams, with or without direct reports6 years of experience working with source code control systemsExperience working with Continuous Integration/Continuous Deployment tools6 years of experience in systems analysis, including defining technical requirements and performing high level design for complex solutionsData Engineering4 years of experience in Hadoop or any Cloud Bigdata components (specific to the Data Engineering role)2 years of experience playing a lead role in projects (specific to the Data Engineering role)Expertise in Java/Scala/Python, SQL, Scripting, Teradata, Hadoop (Sqoop, Hive, Pig, Map Reduce), Spark (Spark Streaming, MLib), Kafka or equivalent Cloud Bigdata components', '6 years of experience working with source code control systems', 'Data Engineering', 'Expertise in Java/Scala/Python, SQL, Scripting, Teradata, Hadoop (Sqoop, Hive, Pig, Map Reduce), Spark ', 'Job Summary:', 'Spark a plus', 'Hadoop', 'Must have “ LEAD’ experience ', '6 years of experience working with defect or incident tracking software', 'Working with business requirements', 'API Experience is Plus', 'Kafka a plus', '2 years of experience playing a lead role in projects (specific to the Data Engineering role)', 'Experience and Qualification:', 'Experience managing Data in Hadoop EnvironmentWorking with business requirements7+ years in software engineering6+ implementing SDLCWorking incident tracking softwarePast experience leading teamsBitbucketHadoopNeeds to have Lead experienceJavaScalaPythonSqoopSpark a plusKafka a plusCloud big data componentsETLManaging RDMSAPI Experience is PlusCloudera a plus', '\xa0', 'Sqoop', 'Bitbucket', 'Experience managing Data in Hadoop Environment', 'Past experience leading teams']",Mid-Senior level,Contract,Business Development,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Bill.com,"San Jose, CA",1 week ago,46 applicants,"['', 'Dedicated – To each other and the customer', 'Own the data expertise and data quality for the pipelines.', 'You are a creative thinker and a strong problem solver with meticulous attention to detail and can tackle loosely defined problems.', 'Experience with specific AWS technologies (such as S3, EMR, and Kinesis) is a plus.', 'Experience with at least one programming language such as Python or Java is required.', 'You have an analytical mindset and have a passion for solving business problems using data.', 'Passionate – Love what you do', 'You have a strong background in distributed data processing and software engineering and can build high-quality, scalable data products.', 'You are passionate about data and the design and development of systems that enable data driven decisions.', 'Experience with multiple data technologies and concepts such as Airflow, Kafka, Hadoop, Hive, Spark, MapReduce, SQL, NoSQL, and Columnar databases.', 'Our Applicant Privacy Notice describes how Bill.com treats the personal information it receives from applicants.', 'You thrive on the opportunity to collaborate with cross functional teams to create customer and business value.', 'Qualifications', 'Automate and improve existing data processes for quicker turnaround and high productivity.', 'Extensive knowledge of data engineering tools, technologies and approaches.', 'BS/MS in Computer Science or equivalent is required.', 'Fun – Celebrate the moments', 'Identify shared data needs across the company and build efficient and scalable pipelines to meet various needs.', 'About Bill.com', 'Design and implement data infrastructure and processing workflows required to support Data Science, Machine Learning, BI and Analytics in AWS.Build robust, efficient and reliable data pipelines consisting of diverse data sources.Design and develop real time streaming and batch processing pipeline solutions.Own the data expertise and data quality for the pipelines.Identify shared data needs across the company and build efficient and scalable pipelines to meet various needs.Work effectively using scrum with multiple team members to deliver analytical solutions to the business functions.Have a high sense of urgency to deliver projects as well as troubleshoot data pipelines/queries etc.Automate and improve existing data processes for quicker turnaround and high productivity.', 'Humble – No ego', 'You have the capability to synthesize business requirements and construct the technical design.', 'Bill.com Culture', 'Expertise in SQL and data analysis is required.', 'Design and develop real time streaming and batch processing pipeline solutions.', 'Design and implement data infrastructure and processing workflows required to support Data Science, Machine Learning, BI and Analytics in AWS.', 'You have strong knowledge of data architecture, data modeling, and data infrastructure ecosystems.', 'You have the ability to initiate and drive projects to completion with minimal guidance in a dynamic environment.', '2+ years data engineering experience in large scale data warehouse or datalake environment.', 'Demonstrated ability to build complex, scalable systems with high quality.', 'What You Will Do', 'You are passionate about data and the design and development of systems that enable data driven decisions.You thrive on the opportunity to collaborate with cross functional teams to create customer and business value.You have the capability to synthesize business requirements and construct the technical design.You have a strong background in distributed data processing and software engineering and can build high-quality, scalable data products.You have strong knowledge of data architecture, data modeling, and data infrastructure ecosystems.You have an analytical mindset and have a passion for solving business problems using data.You are a creative thinker and a strong problem solver with meticulous attention to detail and can tackle loosely defined problems.You have excellent written and verbal communication skills with an ability to communicate in a clear, collaborative and open-minded manner and effective manner with both technical and non-technical peers.You have the ability to initiate and drive projects to completion with minimal guidance in a dynamic environment.', 'Build robust, efficient and reliable data pipelines consisting of diverse data sources.', 'About You', 'Authentic – We are who we are', 'Experience with containerized workloads, Kubernetes and infrastructure-as-code principles a big plus.', 'Work effectively using scrum with multiple team members to deliver analytical solutions to the business functions.', '2+ years data engineering experience in large scale data warehouse or datalake environment.BS/MS in Computer Science or equivalent is required.Extensive knowledge of data engineering tools, technologies and approaches.Demonstrated ability to build complex, scalable systems with high quality.Experience with multiple data technologies and concepts such as Airflow, Kafka, Hadoop, Hive, Spark, MapReduce, SQL, NoSQL, and Columnar databases.Expertise in SQL and data analysis is required.Experience with at least one programming language such as Python or Java is required.Experience with containerized workloads, Kubernetes and infrastructure-as-code principles a big plus.Experience with specific AWS technologies (such as S3, EMR, and Kinesis) is a plus.', 'You have excellent written and verbal communication skills with an ability to communicate in a clear, collaborative and open-minded manner and effective manner with both technical and non-technical peers.', 'Have a high sense of urgency to deliver projects as well as troubleshoot data pipelines/queries etc.']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,E-Solutions,"Atlanta, GA",21 hours ago,Be among the first 25 applicants,"['', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0 Knowledge and understanding of testing frameworks, followed by experience in writing and designing automated test cases.', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0 Good knowledge and understanding the principles of CICD followed by experience in using any of such tools like Jenkins, Team city , google cloud build.', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0 Experience in building end to end data solutions starting from requirements to pushing the code to production', 'Key-Skills Required -', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0 Experience in using any of the version control tools like GIT , GITLAB or any other similar code repositories.', 'Frameworks/Toolkits:', 'Preferred Qualifications :', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0 Experience or understanding of complex pipeline architecture designs that handles heavy amounts of incoming and outgoing traffic.', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0 Experience in working with Big data technologies\xa0 like HDFS, Hive , Pig , Airflow.', 'Languages:', 'Nice To Have :', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0 Certified with any of the cloud platforms ( GCP , AWS , Azure ).', '\xa08+ Years', 'Basic Must Have Qualifications :', 'Must-have:\xa0Spark or Hadoop or any distributed data processing framework', 'Data Engineer with Cloud/Python/Java:', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0 Experience in working with any of the cloud platforms ( preferably : Google Cloud )', 'Location:\xa0Atlanta, GA', 'Experience:\xa08+ Years', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0 Experience in high level programming languages such as Python , Java.', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0 Proficiency with databases and SQL expertise is required', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0 Experience in using of any of the visualization tools like Tableau , Data Studio.', 'Must-have: SQL, Java\xa08 or above, Python', 'Data Engineer', 'Position Title:\xa0Data Engineer', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0 3+ yrs of dedicated Data engineering experience.', 'Disclaimer:\xa0E-Solutions provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, disability, genetic information, marital status, amnesty, or status as a covered veteran in accordance with applicable federal, state and local laws. We especially invite women, minorities, veterans, and individuals with disabilities to apply. EEO/AA/M/F/Vet/Disability.', 'Must have cloud experience - Google Cloud is preferred', 'Note - He has to do some coding on editor with SQL & python at the time of Interview.', 'Atlanta, GA', 'Nice to have: Spring Boot, REST API design']",Entry level,Other,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Nesco Resource,United States,2 weeks ago,84 applicants,"['', 'Minimum of 5 years of experience with NoSQL database, including PostgresMUST HAVE prior experience migrating to Azure from a legacy system (SSIS to Azure is fine)Minimum 3 years of experience\xa0working with Azure Cloud Services: Data Factory, SQL database, Functions, Data Lake, Databricks, Logic Apps, and Azure Automation.Fluent with the following scripting languages: Python, Scala, and C#Advanced knowledge with SQL Server Database - including writing advanced SQL scripts, profiling and optimization.Working knowledge of BI Tools: MS Integration Services, Reporting Services, and Analysis Services, and Power BIPrior experience working with other Big Data tools such as Spark, Snowflake, and Kafka', 'Prior experience working with other Big Data tools such as Spark, Snowflake, and Kafka', 'Duties/Responsibilities:', 'Minimum 3 years of experience\xa0working with Azure Cloud Services: Data Factory, SQL database, Functions, Data Lake, Databricks, Logic Apps, and Azure Automation.', ' Data Engineer', 'Nesco Resource is hiring a Data Engineer - Azure for a growing company in Austin. We are looking for a passionate Data Engineer to join a growing Data and Analytics team whose mission is to modernize their platform to the next-generation cloud platform. You will be responsible for expanding, optimizing, and improving overall data quality and set up next-generation data orchestration using modern cloud tools and technologies. \xa0This will be a remote role but at some point this year you will\xa0be going back into the Austin office.This is a direct hire/perm role that includes a great benefits package and a bonus plan. Would you be available? ', 'Fluent with the following scripting languages: Python, Scala, and C#', 'Minimum of 5 years of experience with NoSQL database, including Postgres', 'Working knowledge of BI Tools: MS Integration Services, Reporting Services, and Analysis Services, and Power BI', 'Knowledge/Skills:', 'Advanced knowledge with SQL Server Database - including writing advanced SQL scripts, profiling and optimization.', ' Duties/Responsibilities: ', 'Data Engineer - Azure', 'Design/develop data pipelines to extract data from multiple data sources using Azure, Snowflake Cloud, and other cloud-native technologies', 'Thank you for your time and consideration!', 'Design/develop data pipelines to extract data from multiple data sources using Azure, Snowflake Cloud, and other cloud-native technologiesBuild out a data model to gain actionable insights from data, operational efficiency, and other key business performance metrics.Design, manage, & monitor inbound and outbound data processesAutomate the data testing processes and integrate them with monitoring systemsWork with and support\xa0an Application Engineering team, DBA, Infrastructure, and Project Management Office.Analyze existing legacy systems and data sets to help Business Analysts define the functional/non-functional requirements.', 'This is a direct hire/perm role that includes a great benefits package and a bonus plan. ', 'Knowledge/Skills: ', 'MUST HAVE prior experience migrating to Azure from a legacy system (SSIS to Azure is fine)', 'Data Engineer', 'Analyze existing legacy systems and data sets to help Business Analysts define the functional/non-functional requirements.', 'Work with and support\xa0an Application Engineering team, DBA, Infrastructure, and Project Management Office.', 'Build out a data model to gain actionable insights from data, operational efficiency, and other key business performance metrics.', 'Automate the data testing processes and integrate them with monitoring systems', 'Design, manage, & monitor inbound and outbound data processes']",Mid-Senior level,Full-time,Information Technology,Insurance,2021-03-18 14:34:51
Associate Data Engineer,KPMG US,"New York, NY",4 weeks ago,38 applicants,"['', ""Rapidly architect, design, prototype, and implement architectures to tackle the Big Data needs for a variety of Fortune 1000 corporations and other major organizationsWork in cross-disciplinary teams with KPMG industry experts to understand client needs and ingest data sources such as documents, financial data, and operational dataResearch, experiment, and utilize leading Big Data methodologies, such as Hadoop, Spark, Redshift, Netezza, SAP HANA, and Microsoft AzureImplement and test data processing pipelines, and data mining / data science algorithms on a variety of hosted settings, such as AWS, Azure, client technology stacks, and KPMG's own clustersTranslate advanced business analytics problems into technical approaches that yield actionable recommendationsDevelop skills in business requirement capture and translation, hypothesis-driven consulting, work stream and project management, and client relationship development"", 'Work in cross-disciplinary teams with KPMG industry experts to understand client needs and ingest data sources such as documents, financial data, and operational data', ""A minimum of one year of data analytics experience with a an internal strategy/analytics group, or similar environmentBachelor's degree in a technical field from an accredited college or university (master's or MBA degree preferred) with expertise in programming languages and a working knowledge of topics such as Data Exploration, Profiling, Quality, Transformation and MiningProficient in designing efficient and robust ETL/ELT workflows, schedulers, and event-based triggersAbility to quickly learn, adapt, and implement Open Source technologies and desire to learn new skills and techniquesAbility to work independently with limited supervision as well as contribute to team efforts and ability to support multiple projects simultaneously and work in a fast-paced environmentAbility to travel up to eighty percent of the time; Applicants must be currently authorized to work in the United States without the need for visa sponsorship now or in the future"", 'Proficient in designing efficient and robust ETL/ELT workflows, schedulers, and event-based triggers', 'Responsibilities', 'Ability to travel up to eighty percent of the time; Applicants must be currently authorized to work in the United States without the need for visa sponsorship now or in the future', 'A minimum of one year of data analytics experience with a an internal strategy/analytics group, or similar environment', 'Requisition Number:', 'Qualifications', ""Implement and test data processing pipelines, and data mining / data science algorithms on a variety of hosted settings, such as AWS, Azure, client technology stacks, and KPMG's own clusters"", 'Description', 'Translate advanced business analytics problems into technical approaches that yield actionable recommendations', 'Develop skills in business requirement capture and translation, hypothesis-driven consulting, work stream and project management, and client relationship development', 'Rapidly architect, design, prototype, and implement architectures to tackle the Big Data needs for a variety of Fortune 1000 corporations and other major organizations', 'Research, experiment, and utilize leading Big Data methodologies, such as Hadoop, Spark, Redshift, Netezza, SAP HANA, and Microsoft Azure', ""Bachelor's degree in a technical field from an accredited college or university (master's or MBA degree preferred) with expertise in programming languages and a working knowledge of topics such as Data Exploration, Profiling, Quality, Transformation and Mining"", 'Ability to quickly learn, adapt, and implement Open Source technologies and desire to learn new skills and techniques', 'Ability to work independently with limited supervision as well as contribute to team efforts and ability to support multiple projects simultaneously and work in a fast-paced environment']",Mid-Senior level,Full-time,General Business,Management Consulting,2021-03-18 14:34:51
Data Engineer,EdgeLink,"Portland, Oregon Metropolitan Area",,N/A,"['', 'Energetic – we move fast', 'Polyglot – work comfortably with many technologies, tools, languages.Smart – no problem is too hardPositive – we’ve got a great culture and we’re keeping it that wayEnergetic – we move fastTwo to five years of experience in a similar role', 'Expand or improve enterprise data model', 'Polyglot – work comfortably with many technologies, tools, languages.', 'Smart – no problem is too hard', 'Build ETL or API-based integrations between systems to ensure clear bi-directional data synchronization.', 'Positive – we’ve got a great culture and we’re keeping it that way', 'technologies include: SQL, TSQL, Azure SQL Services, Data Factories, Data Lakes, PowerBI, Salesforce, Apex, Python, SSIS, SSRS.\xa0We are open to new technologies if you have ideas to improve the tool stack.', 'Build reports and ad hoc data queries', 'Characteristics', 'If you’re interested in having a huge impact on the company and how it uses data, and you have the skills and energy, please apply.\xa0We have great benefits for a small company: health, dental, vision, 401k with matching.', 'Identify areas of improvement to achieve data quality', 'Two to five years of experience in a similar role', 'Projects for the Data Engineer', 'EdgeLink is searching for a Data Engineer that can connect disparate data sources and deliver actionable data. The role is a mix of data steward, API programmer, ETL expert, data analyst, report writer and researcher.\xa0Soft skills including communication and helping people understand data will also be critical.\xa0The position will be remote initially with the expectation to join in the Lake Oswego office when appropriate. ', 'Evaluate enterprise datasets for quality and accuracy', 'Amaze the company with insights from data', 'Amaze the company with insights from dataBuild ETL or API-based integrations between systems to ensure clear bi-directional data synchronization.Build business intelligence and data visualization dashboards for new data setsExpand or improve enterprise data modelBuild reports and ad hoc data queriesEvaluate enterprise datasets for quality and accuracyDetermine root cause for data quality errors and make recommendations for long-term solutionsIdentify areas of improvement to achieve data quality', 'Now is an exciting time to join our fast growth client. They are a rapidly growing, two year old, startup with friendly people, strong benefits and a world-improving mission. The CTO, CEO and entire company is committed to high quality data and systems. This company is interconnecting multiple data sources to build something our industry has never seen.\xa0', 'Determine root cause for data quality errors and make recommendations for long-term solutions', 'Build business intelligence and data visualization dashboards for new data sets']",Mid-Senior level,Full-time,Information Technology,Financial Services,2021-03-18 14:34:51
Data Engineer,"Fetch Rewards, Inc.","Chicago, IL",5 days ago,Be among the first 25 applicants,"['', 'At least 3 years of relevant full-time work experience', 'Familiarity with open source software and dependency management', 'ETL process, data pipeline, and/or micro-service development experience', 'Experience with relational (SQL), non-relational (NoSQL), and/or object data stores (e.g., Snowflake, MongoDB, S3, HDFS, Postgres, Redis, DynamoDB)', 'REQUIRED: ', 'Solid SQL skills', 'REQUIRED:', ' REQUIRED: Python programming skills Solid SQL skills Familiarity with Unix systems, shell scripting, and Git Experience with relational (SQL), non-relational (NoSQL), and/or object data stores (e.g., Snowflake, MongoDB, S3, HDFS, Postgres, Redis, DynamoDB) Interest in building and experimenting with different tools and tech, and sharing your learnings with the broader organization The desire to work with other teams in the organization (e.g., Development, Business Intelligence, Data Science) to build tools and solutions that support and help manage data within the Fetch ecosystem Bachelor’s degree in Computer Science (or equivalent) At least 3 years of relevant full-time work experience ', 'Fetch Rewards is an equal employment opportunity employer.', 'Excellent written and verbal communication skills', 'Familiarity with Unix systems, shell scripting, and Git', 'Familiarity with messaging and asynchronous technologies (e.g., SQS, Kinesis, RabbitMQ, Kafka)', 'Experience with visualization tools (e.g., Tableau)', "" Excellent written and verbal communication skills Familiarity with open source software and dependency management ETL process, data pipeline, and/or micro-service development experience Cloud engineering and DevOps skills (e.g., AWS, CloudFormation, Docker) Familiarity with messaging and asynchronous technologies (e.g., SQS, Kinesis, RabbitMQ, Kafka) Big data development skills (e.g., Spark, Hadoop, MPP DW) Experience with visualization tools (e.g., Tableau) Love of Dogs! . . . Or just tolerance. We're a very canine-friendly workplace"", 'Who We Are', 'Bachelor’s degree in Computer Science (or equivalent)', 'The Role', 'Why Join the Fetch Family?', 'Cloud engineering and DevOps skills (e.g., AWS, CloudFormation, Docker)', 'Big data development skills (e.g., Spark, Hadoop, MPP DW)', 'The desire to work with other teams in the organization (e.g., Development, Business Intelligence, Data Science) to build tools and solutions that support and help manage data within the Fetch ecosystem', 'Interest in building and experimenting with different tools and tech, and sharing your learnings with the broader organization', ""Love of Dogs! . . . Or just tolerance. We're a very canine-friendly workplace"", 'REQUIRED: Python programming skills', 'Bonus Points For']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Data Engineer,Sagent,United States,1 week ago,Be among the first 25 applicants,"['', 'Position is remote.', '•\xa0Bachelor’s degree in Computer Science or equivalent education in a related discipline is required.\xa0\xa0\xa0', ""2+ years’ experience in construction and implementation of data workflows in Azure Data Factory (ADF). This includes having implemented at least one system using ADF into production.7+ years' experience in working on data-integration projects, including operational datastores, data marts, and data warehouses.7+ years’ experience in construction, implementation,\xa0unit testing, and production support of ETL/ELT solutions using tools such as SSIS, Informatica, Talend, or native database code.7+ years in working collaboratively and communicating comfortably on multi-discipline development teams.7+ years’ experience in working with terabyte-sized\xa0databases in engines such as Snowflake, Azure SQL, SQL Server, or Oracle. This includes the demonstrated ability to design and construct performant load processes, perform query optimization, and understand database and database design fundamentals that impact functionality and performance.7+ years’ experience in construction and maintenance of stored procedures.Experience in implementation and deployment of code\xa0within a Continuous Integration/Continuous Deployment (CI/CD) discipline and repository, such as Azure DevOPS or TFS.Experience in working with data sourced from both relational and non-relational (e.g. mainframe) systems.Strong English language written and verbal skills."", 'Additional Skills/Knowledge:\xa0\xa0', '•\xa0Work on a development team to help plan, implement and support enterprise applications.\xa0', 'About the Role', '•\xa0Collaborate with application architect and business analysts to understand requirements.', 'This is one of the most exciting times to join our team because of a recent joint venture agreement between our previous parent company Fiserv and private equity firm Warburg Pincus. With this agreement, we are backed by both the legacy of a Fortune 500 company that has been voted one of the World’s Most Admired Companies and the growth expertise of a leading investment group.\xa0\xa0\xa0', 'Sagent Lending Technologies is seeking a Software Development Engineer to help delight our borrowers by providing top notch client support. The team is building our next-generation borrower facing portals and servicing systems. The systems will be used by lenders and servicers in the consumer and mortgage lending markets in the United States. As a Software Development Engineer, you will be working on a delivery team using modern technologies, tools and frameworks to develop advanced, enterprise business components mainframe and Microsoft platforms.\xa0\xa0\xa0', '•\xa0Document and communicate technical designs, standards and processes as needed.', 'About our Business', 'Job Related Experience:\xa0\xa0\xa0', 'Experience in implementation and deployment of code\xa0within a Continuous Integration/Continuous Deployment (CI/CD) discipline and repository, such as Azure DevOPS or TFS.', 'About our Business:\xa0\xa0\xa0', '7+ years’ experience in working with terabyte-sized\xa0databases in engines such as Snowflake, Azure SQL, SQL Server, or Oracle. This includes the demonstrated ability to design and construct performant load processes, perform query optimization, and understand database and database design fundamentals that impact functionality and performance.', 'About the Role:\xa0', 'Experience in working with data sourced from both relational and non-relational (e.g. mainframe) systems.', 'Extensive experience with full-lifecycle development (i.e. research, design, coding, testing, debugging, etc.)\xa0', 'Essential Job Responsibilities', 'Strong English language written and verbal skills.', 'Required Qualifications:\xa0\xa0\xa0\xa0', ""7+ years' experience in working on data-integration projects, including operational datastores, data marts, and data warehouses."", 'About our Business:', 'Required Qualifications:\xa0\xa0', '•\xa0Continuously enhance skills by learning and applying relevant technologies and patterns.\xa0', 'Essential Job Responsibilities:\xa0\xa0', '•\xa0Troubleshoot and resolve software bugs and deployment issues', '•\xa0Understanding of development methodologies', '•\xa0Perform unit and integration testing to ensure application quality.\xa0', 'Essential Job Responsibilities:\xa0\xa0\xa0\xa0', '7+ years in working collaboratively and communicating comfortably on multi-discipline development teams.', '•\xa0Work with others to fine tune application performance.\xa0', 'Job Related Experience:', 'Required Qualifications:', '2+ years’ experience in construction and implementation of data workflows in Azure Data Factory (ADF). This includes having implemented at least one system using ADF into production.', 'Additional Skills/Knowledge', '7+ years’ experience in construction and maintenance of stored procedures.', 'About the Role:\xa0\xa0\xa0', '•\xa0Minimum of 7 years’ experience as a data engineer\xa0', 'Job Related Experience:\xa0', '•\xa0Design, develop and test new system capabilities.', 'For every individual and family, one of their biggest life purchasing decisions involve buying a house. We are the company working behind the scenes with our clients to create industry leading technology that ensures the experience is secure and easy for consumers. Each year, we are proud to help millions of people realize their home ownership dreams. Our clients, who are Blue Chip lenders and high-growth brand name disruptors trust us to help them address and anticipate the ever-evolving lending landscape. Over the years, we earned the industry’s choice for scalable performance and innovative technology that connects lenders with the people that they serve\xa0\xa0\xa0\xa0', '7+ years’ experience in construction, implementation,\xa0unit testing, and production support of ETL/ELT solutions using tools such as SSIS, Informatica, Talend, or native database code.']",Associate,Full-time,Information Technology,Financial Services,2021-03-18 14:34:51
Data Engineer/Wrangler,Kaiser Permanente,"Greenwood Village, CO",4 weeks ago,Be among the first 25 applicants,"['', ""Two (2) years of work experience in a role requiring interaction with senior leadership (e.g., Director level and above)Three (3) years experience in a leadership role of a large matrixed organization.Three (3) years experience working with IT vendors.Four (4) years experience working on project(s) involving the implementation of software development life cycle(s) (SDLC).Three (3) years experience writing technical documentation in a software testing or quality assurance environment.Four (4) years experience working with integration technologies (e.g., web services, MQ).Two (2) years experience working in and defining the requirements for virtual testing environments.Two (2) years experience working in and defining the requirements for healthcare testing environments.Two (2) years experience testing and defining the requirements testing business system applications.Two (2) years experience working with and defining the requirements for data mart/data warehousing.Master's degree in Computer Science, CIS, or related field.Quality Assurance Institute (QAI) or American Society for Quality (ASQ) or similar certification."", 'Quality Assurance Institute (QAI) or American Society for Quality (ASQ) or similar certification.', 'Conducts or oversees business-specific projects by applying deep expertise in subject area; promoting adherence to all procedures and policies; developing work plans to meet business priorities and deadlines; determining and carrying out processes and methodologies; coordinating and delegating resources to accomplish organizational goals; partnering internally and externally to make effective business decisions; solving complex problems; escalating issues or risks, as appropriate; monitoring progress and results; recognizing and capitalizing on improvement opportunities; evaluating recommendations made; and influencing the completion of project tasks by others.Practices self-leadership and promotes learning in others by building relationships with cross-functional stakeholders; communicating information and providing advice to drive projects forward; influencing team members within assigned unit; listening and responding to, seeking, and addressing performance feedback; adapting to competing demands and new responsibilities; providing feedback to others, including upward feedback to leadership and mentoring junior team members; creating and executing plans to capitalize on strengths and improve opportunity areas; and adapting to and learning from change, difficulties, and feedback.Leads the development of quality assurance (QA) test project strategies, methodologies, and standard processes for large-scale, complex IT initiatives spanning all QA domains by analyzing business and technology requirements to ensure testability and traceability, and determining testing scope and approach.Oversees and addresses critical issues, dependencies, and risks related to testing.Ensures quality assurance projects are appropriately staffed, work plans are followed, and milestones are met.Reviews and signs off on testing scope and approach, and partners with cross-functional IT and business stakeholders to review and approve the overall testing approach.Manages the development test scenarios and execution of test cases across all testing phases (e.g., SIT, Performance, UAT, Automation, Production, Validation).Ensures quality metrics are tracked across testing phases (e.g., SIT, Performance, UAT, Automation, Production, Validation).Manages the review and validation of testable business processes, test data, and test environment requirements.Develops quality assurance (QA) project plans for moderately to highly complex projects by identifying project scope, work plans, schedules, milestones, and critical paths, and ensuring proper staffing.Develops guidelines and best practices to ensure test plans and timelines are aligned with project/program milestones.Defines and ensures adherence with entry and exit criteria according to Kaiser testing standards.Reviews project status and milestones reports, provides justification for and first-level authorization for exceptions and waivers, and meets with IT and business stakeholders to address contingency plans, as appropriate.Generates scheduled reports (e.g., test execution, defects, ad hoc reports) and provides daily test execution metrics to IT teams and management, as appropriate.Initiates and evaluates required business process improvements in order to achieve business results and appropriate solutions for customers.', 'Ensures quality metrics are tracked across testing phases (e.g., SIT, Performance, UAT, Automation, Production, Validation).', 'Three (3) years experience writing technical documentation in a software testing or quality assurance environment.', 'Four (4) years experience working with integration technologies (e.g., web services, MQ).', ""Bachelor's degree in Computer Science, CIS, or related field and Minimum eight (8) years experience in SQA, software testing or related field. Additional equivalent work experience may be substituted for the degree requirement."", 'Minimum Qualifications', ""Minimum two (2) years SQA experience working across multiple IT domains or business units in a corporate setting.Minimum two (2) years in a leadership role working with technical teams.Bachelor's degree in Computer Science, CIS, or related field and Minimum eight (8) years experience in SQA, software testing or related field. Additional equivalent work experience may be substituted for the degree requirement."", 'Leads the development of quality assurance (QA) test project strategies, methodologies, and standard processes for large-scale, complex IT initiatives spanning all QA domains by analyzing business and technology requirements to ensure testability and traceability, and determining testing scope and approach.', 'Generates scheduled reports (e.g., test execution, defects, ad hoc reports) and provides daily test execution metrics to IT teams and management, as appropriate.', 'Minimum two (2) years in a leadership role working with technical teams.', 'Manages the review and validation of testable business processes, test data, and test environment requirements.', 'Three (3) years experience working with IT vendors.', 'Practices self-leadership and promotes learning in others by building relationships with cross-functional stakeholders; communicating information and providing advice to drive projects forward; influencing team members within assigned unit; listening and responding to, seeking, and addressing performance feedback; adapting to competing demands and new responsibilities; providing feedback to others, including upward feedback to leadership and mentoring junior team members; creating and executing plans to capitalize on strengths and improve opportunity areas; and adapting to and learning from change, difficulties, and feedback.', 'Two (2) years of work experience in a role requiring interaction with senior leadership (e.g., Director level and above)', 'Description', 'Initiates and evaluates required business process improvements in order to achieve business results and appropriate solutions for customers.', 'Two (2) years experience testing and defining the requirements testing business system applications.', 'Four (4) years experience working on project(s) involving the implementation of software development life cycle(s) (SDLC).', ""Master's degree in Computer Science, CIS, or related field."", 'Ensures quality assurance projects are appropriately staffed, work plans are followed, and milestones are met.', 'Three (3) years experience in a leadership role of a large matrixed organization.', 'Two (2) years experience working in and defining the requirements for healthcare testing environments.', 'Essential Responsibilities', 'Two (2) years experience working with and defining the requirements for data mart/data warehousing.', 'Develops quality assurance (QA) project plans for moderately to highly complex projects by identifying project scope, work plans, schedules, milestones, and critical paths, and ensuring proper staffing.', 'Conducts or oversees business-specific projects by applying deep expertise in subject area; promoting adherence to all procedures and policies; developing work plans to meet business priorities and deadlines; determining and carrying out processes and methodologies; coordinating and delegating resources to accomplish organizational goals; partnering internally and externally to make effective business decisions; solving complex problems; escalating issues or risks, as appropriate; monitoring progress and results; recognizing and capitalizing on improvement opportunities; evaluating recommendations made; and influencing the completion of project tasks by others.', 'Oversees and addresses critical issues, dependencies, and risks related to testing.', 'Reviews and signs off on testing scope and approach, and partners with cross-functional IT and business stakeholders to review and approve the overall testing approach.', 'Manages the development test scenarios and execution of test cases across all testing phases (e.g., SIT, Performance, UAT, Automation, Production, Validation).', 'Two (2) years experience working in and defining the requirements for virtual testing environments.', 'Develops guidelines and best practices to ensure test plans and timelines are aligned with project/program milestones.', 'Reviews project status and milestones reports, provides justification for and first-level authorization for exceptions and waivers, and meets with IT and business stakeholders to address contingency plans, as appropriate.', 'Minimum two (2) years SQA experience working across multiple IT domains or business units in a corporate setting.', 'Preferred Qualifications', 'Defines and ensures adherence with entry and exit criteria according to Kaiser testing standards.']",Entry level,Full-time,Information Technology,Financial Services,2021-03-18 14:34:51
Data Engineer - Data Automation,Addepar,"Los Angeles, CA",5 days ago,45 applicants,"['', 'Team with Services, Product, and Engineering Leaders to define best practices for the flow of data into and out of Addepar.', 'Develop subject matter expertise in the Addepar product as well as the tools upon which Addepar relies for data management.', ' Automating business processes and integrating with cloud based services via APIs. Consultative approach in designing and building best in class data solutions to optimize business processes with an eye toward eliminating redundant data maintenance. Synthesize business and product requirements to design and deliver integrated data solutions in an agile way. Team with Services, Product, and Engineering Leaders to define best practices for the flow of data into and out of Addepar. Develop subject matter expertise in the Addepar product as well as the tools upon which Addepar relies for data management. Develop and maintain Python SDK and related tooling to enable users to more efficiently integrate data with Addepar. Taking a full stack engineering approach to solving core data problems. ', 'Solutions oriented, with exceptional analytical and problem solving skills', 'Nice to Have', 'Experience building ETL pipelines that support data collection, transformation, validation, and ingestion', 'In depth knowledge of Python or Java ', 'Synthesize business and product requirements to design and deliver integrated data solutions in an agile way.', 'Responsibilities', 'Taking a full stack engineering approach to solving core data problems.', 'Experience with large-scale / streaming data processing tools including Kafka, Spark, and Flink ', 'Consultative approach in designing and building best in class data solutions to optimize business processes with an eye toward eliminating redundant data maintenance.', ' In depth knowledge of Python or Java  Experience with large-scale / streaming data processing tools including Kafka, Spark, and Flink  Experience with frontend application development 2+ years of professional data engineering experience in analytics, data operations, or related roles ', 'Automating business processes and integrating with cloud based services via APIs.', 'Requirements', 'Experience with frontend application development', ' BS/MS in Computer Science, Statistics, Mathematics, or other quantitative field Experience building ETL pipelines that support data collection, transformation, validation, and ingestion Solutions oriented, with exceptional analytical and problem solving skills Exceptional communication skills Familiarity with writing, debugging, and optimizing SQL queries ', '2+ years of professional data engineering experience in analytics, data operations, or related roles', 'BS/MS in Computer Science, Statistics, Mathematics, or other quantitative field', 'Exceptional communication skills', 'Develop and maintain Python SDK and related tooling to enable users to more efficiently integrate data with Addepar.', 'Familiarity with writing, debugging, and optimizing SQL queries']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
SQL Data Engineer,Sequoia Consulting Group,"New York, NY",5 days ago,Be among the first 25 applicants,"['', 'Design and implement effective database solutions and models to store and retrieve company data ', 'Compensation & Benefits', 'Examine and identify database structural necessities by evaluating client operations, applications, and programming ', 'Required Skills & Experience', 'Knowledge of medical, dental, LTD/STD/life insurance concepts and data Understanding of various data extraction and transformation techniques Working familiarity with Salesforce (SFDC) Knowledge of Mulesoft is a bonus Familiar with data visualization standard methodologies\u202f ', 'Monitor the system performance by performing regular tests, fixing, and integrating new features ', 'Extensive hands-on experience working with SQL and Python for the purposes of data modeling and ETL ', 'Advanced working knowledge and ability to write complex SQL queries in MySQL, Snowflake, and Salesforce (SFDC) environments ', 'Soft Skills', 'Advanced working knowledge and ability to write complex SQL queries in MySQL, Snowflake, and Salesforce (SFDC) environments Extensive hands-on experience working with SQL and Python for the purposes of data modeling and ETL Strong familiarity with Kimball, OLAP, and EDW data design methodologies, especially for healthcare and benefits datasets 3+ years’ experience in data modeling and architecting, ETL, data engineering, or BI fields with concentration on data transformations and data modeling ', 'A champion of quality, able to QA and vouch for the integrity of the report output ', 'Design conceptual and logical data models and flowcharts ', 'What Does the Role Entail?', 'Lead all aspects of the migration of data from legacy systems to new solutions ', 'Install and organize information systems to guarantee company functionality ', 'Working familiarity with Salesforce (SFDC) ', 'Understanding of various data extraction and transformation techniques ', 'SQL', 'Knowledge of Mulesoft is a bonus ', '3+ years’ experience in data modeling and architecting, ETL, data engineering, or BI fields with concentration on data transformations and data modeling ', 'Assess fitness-for-purpose of existing data model and processes Design conceptual and logical data models and flowcharts Design and implement effective database solutions and models to store and retrieve company data Development of reporting solutions to meet the operational and executive needs of the platform Examine and identify database structural necessities by evaluating client operations, applications, and programming Optimize new and current database systems Assess database implementation procedures to ensure they follow internal and external regulations Install and organize information systems to guarantee company functionality Prepare accurate database design and architecture reports for management and executive teams Lead all aspects of the migration of data from legacy systems to new solutions Monitor the system performance by performing regular tests, fixing, and integrating new features Recommend solutions to improve new and existing database systems ', 'Assess fitness-for-purpose of existing data model and processes ', 'Maintaining business partner engagement and setting expectations ', 'Recommend solutions to improve new and existing database systems ', 'Translate business requirements into specifications that will be used to implement the required user-friendly environments, reports and dashboards, built from potentially multiple data sources ', 'Ability to succeed in a dynamic, Agile environment Strong prioritization and time-management skills Dedication to team goals that include support of live 24/7 production systems A consummate collaborator, able to establish good relationships with technical, product, and business owners A champion of quality, able to QA and vouch for the integrity of the report output Maintaining business partner engagement and setting expectations Assessing current processes and recommending changes as needed Documenting and communicating technical specifications to ensure that proper and optimized techniques, queries, data standards, and final outputs are understood and incorporated into data and analytics processes Participate in business analysis activities to gather required reporting and dashboard requirements Translate business requirements into specifications that will be used to implement the required user-friendly environments, reports and dashboards, built from potentially multiple data sources ', 'Development of reporting solutions to meet the operational and executive needs of the platform ', 'Knowledge of medical, dental, LTD/STD/life insurance concepts and data ', 'Prepare accurate database design and architecture reports for management and executive teams ', 'Useful Skills And Experience', 'Data Engineer ', 'A consummate collaborator, able to establish good relationships with technical, product, and business owners ', 'Participate in business analysis activities to gather required reporting and dashboard requirements ', 'Sequoia’s Culture – Our most important asset', 'Strong familiarity with Kimball, OLAP, and EDW data design methodologies, especially for healthcare and benefits datasets ', 'Assessing current processes and recommending changes as needed ', ""What You'll Do"", 'Familiar with data visualization standard methodologies\u202f ', 'Optimize new and current database systems ', 'Ability to succeed in a dynamic, Agile environment ', 'Assess database implementation procedures to ensure they follow internal and external regulations ', 'Strong prioritization and time-management skills ', 'Documenting and communicating technical specifications to ensure that proper and optimized techniques, queries, data standards, and final outputs are understood and incorporated into data and analytics processes ', 'Dedication to team goals that include support of live 24/7 production systems ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer - USA,InterWorks,Oklahoma City Metropolitan Area,1 month ago,45 applicants,"['', 'Solve data-acquisition, integration and management problems', 'AWS / Microsoft Azure', 'Adaptability and flexibility in changing situations', 'Matillion, Fivetran, Airflow, DBT or other ETL tools', 'Experience with software engineering practicesExperience with modern data-engineering practices and frameworksExperience with integration from API sourcesMatillion, Fivetran, Airflow, DBT or other ETL toolsAWS / Microsoft AzureSnowflake / Amazon Redshift / Google BigQuery / Azure Synapse ', 'Snowflake / Amazon Redshift / Google BigQuery / Azure Synapse ', 'Excellent verbal and written communication', 'Experience with integration from API sources', 'What You’ll Do', 'What You’ll Need', 'Experience with software engineering practices', 'Strong problem-solving skills', 'What We’d Like You to Have', 'Collaborate closely with users to understand their unique needs and support them with the best solutions', 'Create ETL processes based on client needs while managing client expectations', 'Business acumen', 'Why InterWorks', 'Strong ETL proficiency using GUI-based tools or code-based patterns', 'Passion for delivering compelling solutions that exceed client expectations', 'Tackle diverse projects that range in duration from a few days to a few months for clients ranging from local businesses to the Fortune 500Work with disparate data sources (relational databases, flat files, Excel, HDFS/Big Data systems, high-performance analytical databases, etc.) to unify client dataCollaborate closely with users to understand their unique needs and support them with the best solutionsSolve data-acquisition, integration and management problemsCreate ETL processes based on client needs while managing client expectations', 'Programming (Python, Java, C#, PHP, etc.)', 'Experience with modern data-engineering practices and frameworks', 'Must-Haves', 'Excellent SQL fluency', 'Excellent SQL fluencyProgramming (Python, Java, C#, PHP, etc.)Strong ETL proficiency using GUI-based tools or code-based patternsUnderstanding of data-modeling principlesExcellent verbal and written communicationBusiness acumenStrong problem-solving skillsAdaptability and flexibility in changing situationsPassion for delivering compelling solutions that exceed client expectations', 'Work with disparate data sources (relational databases, flat files, Excel, HDFS/Big Data systems, high-performance analytical databases, etc.) to unify client data', 'Tackle diverse projects that range in duration from a few days to a few months for clients ranging from local businesses to the Fortune 500', 'Understanding of data-modeling principles']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,IFP Independent Financial Partners,"Tampa, FL",1 week ago,Be among the first 25 applicants,"['', 'Qualifications', 'Experience managing data and reporting in Salesforce and AWS', 'Responsibilities', 'Reporting and data analysis using various platforms, including AWS and SalesforceCommunicating outcomes and insights to various stakeholders to aid strategic decisions for improvementsIdentify and drive scalable solutions for building and automating reports, dashboards, and infrastructure to monitor key performance metrics and workforce strategySupport software engineering team in feature development and research', 'Experience managing data and reporting in Salesforce and AWSExperience in a quantitative and/or analytical role using data wrangling and automation tools such as Python, Salesforce and SQLEffective communication skills and the ability to understand complex processes and apply creative problem solving to find efficiencies and improvements', 'Communicating outcomes and insights to various stakeholders to aid strategic decisions for improvements', 'Effective communication skills and the ability to understand complex processes and apply creative problem solving to find efficiencies and improvements', 'Job Description', 'Company', 'Support software engineering team in feature development and research', 'Identify and drive scalable solutions for building and automating reports, dashboards, and infrastructure to monitor key performance metrics and workforce strategy', 'Reporting and data analysis using various platforms, including AWS and Salesforce', 'Experience in a quantitative and/or analytical role using data wrangling and automation tools such as Python, Salesforce and SQL']",Entry level,Full-time,Information Technology,Insurance,2021-03-18 14:34:51
Data Engineer,RemyCorporation,"Massachusetts, United States",3 weeks ago,139 applicants,"['', 'Must (requirement) have experience in either the Consumer Goods or Retail Industry.', 'Really looking for someone who is curious and always learning new technologies, not just a doer but a thinker! If this is you please reach out.', 'Please call or email me 303-990-2320 rachel@remycorp.com', 'I have both Sr. and Jr. level positions. Must have experience with Azure Data Factory, Snowflake, Python scripting, building pipelines from scratch, Best Practices around building pipelines and strong documentation skills.', 'This is a perm/ remote opportunity, the client will provide sponsorship if needed. Please no 3rd party calls.']",Mid-Senior level,Full-time,Information Technology,Consumer Goods,2021-03-18 14:34:51
Data Engineer,"Cardiovascular Systems, Inc.",Greater Minneapolis-St. Paul Area,1 week ago,35 applicants,"['we save lives', 'Knowledge, Skills and Abilities Required for Successful Job Performance:\xa0', ""Bachelor's degree in a technical discipline or equivalent experience\xa0\xa0Demonstrated 3-5 years of development experience in data warehouse or equivalent data integration and analytics domain\xa0\xa0Demonstrated hands-on proficiency with\xa0data pipeline and processing tools\xa0(SQL, Python)\xa0\xa0Demonstrated proficiency with data extraction techniques – querying data from internal and external sources through a variety of interfaces and protocols\xa0\xa0Demonstrated proficiency with data transformation techniques – joining data from multiple independent systems\xa0\xa0Demonstrated proficiency with\xa0business\xa0data\xa0source tools\xa0(Tableau)\xa0\xa0Able to work within a defined development and production released process that involves varying levels of rigor for design validation, based on business risk\xa0\xa0Able to work independently with minimal guidance to meet commitments on assigned tasks\xa0\xa0Able to collaborate effectively with other employees to meet team objectives\xa0\xa0Able to address and resolve interpersonal conflicts in a professional manner\xa0\xa0Able to communicate clearly through both written technical documentation as well as written business analytics user-oriented communication\xa0\xa0Able to communicate clearly and confidently in small group interactions with a variety of types of participants and interpersonal styles\xa0\xa0"", 'Must be able to work at a computer for 8 hours per day\xa0\xa0', 'something greater than ourselves', 'Demonstrated proficiency with\xa0business\xa0data\xa0source tools\xa0(Tableau)\xa0\xa0', 'Preferred', 'think big and move fast', 'Knowledge, Skills and Abilities Required for Successful Job Performance:', 'Develop, test, and maintain data\xa0pipelines\xa0(Python,\xa0SQL,\xa0Web APIs) to incorporate new or changing sources of internal and external data into the CSI data\xa0platform and data warehouse\xa0', 'Demonstrated 3-5 years of development experience in data warehouse or equivalent data integration and analytics domain\xa0\xa0', 'Develop, test, and maintain data\xa0processing flows\xa0(SQL) to integrate sources of data into the conformed dimensional structure of the CSI data warehouse\xa0\xa0', 'Able to communicate clearly through both written technical documentation as well as written business analytics user-oriented communication\xa0\xa0', 'Experience\xa0working with modern enterprise data platforms (e.g., Snowflake, Azure Data Lake, cloud-based\xa0pipeline\xa0and transformation tools)\xa0in\xa0a\xa0mid-to-large company environment\xa0', 'community', 'Job Summary:\xa0', 'Demonstrated proficiency with data extraction techniques – querying data from internal and external sources through a variety of interfaces and protocols\xa0\xa0', 'Develop, test, and maintain\xa0Tableau\xa0data sources for business reporting and analytics usage\xa0\xa0', 'Able to communicate clearly and confidently in small group interactions with a variety of types of participants and interpersonal styles\xa0\xa0', 'Demonstrated proficiency with data transformation techniques – joining data from multiple independent systems\xa0\xa0', 'Must be able to work\xa0occasionally\xa0after hours\xa0and\xa0weekends\xa0when required\xa0to support major system updates during non-business hours\xa0\xa0', 'impact can be felt company-wide', 'Understand, adhere to, and contribute to the development of SDLC standards and practices for the CSI data\xa0platform\xa0', 'Must be fluent in English, capable of clear written and verbal communication and comprehension\xa0', 'Required', 'Specifications/Other\xa0\xa0', 'Putting a new spin on your career', 'Preferred\xa0', 'Required\xa0', 'Experience supporting a variety of data analysis types – descriptive, diagnostic, predictive, prescriptive – by a variety of disciplines (e.g., marketing, sales, finance, operations), using a variety of data sources (e.g., ERP, CRM, HRIS, external market data)\xa0\xa0', ""Bachelor's degree in a technical discipline or equivalent experience\xa0\xa0"", 'The Data Engineer will be a member of the Business Intelligence team, and will work under the technical guidance of a\xa0Senior Data Engineer, to develop, test, deploy, document, and maintain the CSI data warehouse and data sources for CSI business analytics users.\xa0', 'Essential Duties and Responsibilities:\xa0', 'Must be able to work at a computer for 8 hours per day\xa0\xa0Must be able to work\xa0occasionally\xa0after hours\xa0and\xa0weekends\xa0when required\xa0to support major system updates during non-business hours\xa0\xa0Must be able to respond and address system failures during non-business hours to avoid impact to business users\xa0\xa0Must be fluent in English, capable of clear written and verbal communication and comprehension\xa0', 'Perform other related duties and responsibilities, on occasion, as assigned\xa0\xa0', 'Job Summary:', 'Working at CSI means being a part of\xa0something greater than ourselves. We\xa0think big and move fast. From creating revolutionary technologies, to working with world class physicians and teams, we don’t simply work,\xa0we save lives. We have big company drive and\xa0small company vibe\xa0where your daily\xa0impact can be felt company-wide. If you are curious, ambitious and dedicated, you will love our\xa0community.\xa0Grow\xa0with us!', 'Able to work independently with minimal guidance to meet commitments on assigned tasks\xa0\xa0', 'Must be able to respond and address system failures during non-business hours to avoid impact to business users\xa0\xa0', 'Able to address and resolve interpersonal conflicts in a professional manner\xa0\xa0', 'Grow', 'Develop, test, and maintain data\xa0pipelines\xa0(Python,\xa0SQL,\xa0Web APIs) to incorporate new or changing sources of internal and external data into the CSI data\xa0platform and data warehouse\xa0Develop, test, and maintain data\xa0processing flows\xa0(SQL) to integrate sources of data into the conformed dimensional structure of the CSI data warehouse\xa0\xa0Develop, test, and maintain\xa0Tableau\xa0data sources for business reporting and analytics usage\xa0\xa0Support Tableau dashboard and report development by business analysts using rapid prototype and production data sources\xa0\xa0Understand, adhere to, and contribute to the development of SDLC standards and practices for the CSI data\xa0platform\xa0Create and maintain technical\xa0and business\xa0documentation for data\xa0pipelines, processing flows, and data sources\xa0Support business analytics users with user-focused documentation\xa0\xa0Perform other related duties and responsibilities, on occasion, as assigned\xa0\xa0', 'Demonstrated hands-on proficiency with\xa0data pipeline and processing tools\xa0(SQL, Python)\xa0\xa0', 'Able to work within a defined development and production released process that involves varying levels of rigor for design validation, based on business risk\xa0\xa0', 'Essential Duties and Responsibilities:', 'Specifications/Other\xa0', 'Able to collaborate effectively with other employees to meet team objectives\xa0\xa0', 'small company vibe', 'Experience\xa0working with modern enterprise data platforms (e.g., Snowflake, Azure Data Lake, cloud-based\xa0pipeline\xa0and transformation tools)\xa0in\xa0a\xa0mid-to-large company environment\xa0Experience supporting a variety of data analysis types – descriptive, diagnostic, predictive, prescriptive – by a variety of disciplines (e.g., marketing, sales, finance, operations), using a variety of data sources (e.g., ERP, CRM, HRIS, external market data)\xa0\xa0', 'Support Tableau dashboard and report development by business analysts using rapid prototype and production data sources\xa0\xa0', 'Create and maintain technical\xa0and business\xa0documentation for data\xa0pipelines, processing flows, and data sources\xa0', 'Support business analytics users with user-focused documentation\xa0\xa0']",Not Applicable,Full-time,Analyst,Biotechnology,2021-03-18 14:34:51
Data Engineer,Sense,"Cambridge, MA",4 weeks ago,101 applicants,"['', 'A wide range of difficult and interesting problems to be solved.', 'Work with the data science team to develop active learning tools and UIs that will facilitate the development of statistical models.', '50 on Fire - BostInno', 'Best Consumer AI Technology - AI Dev World', 'Best Startups in Cambridge - Tech Tribune', 'Work with a small team of experienced entrepreneurs creating revolutionary technology.', 'Why Sense', 'Benefits', 'Excited about high-volume, real-time data and solving the challenges it poses.', ' 5+ years professional experience as a data or backend engineer. Solid knowledge of computer science. Experience programming in Python. Knowledge of C/C++ or the willingness to learn is a plus. Experience with relational databases such as MySQL. Experience with AWS services is a plus. Must be authorized to work in the U.S. ', 'Design and implement new data pipelines and databases as needed.', 'Experience with AWS services is a plus.', 'Experience programming in Python.', '""One of the world\'s top 100 AI companies"" - VentureBeat', 'Knowledge of C/C++ or the willingness to learn is a plus.', ' Passionate about the energy sector and climate change. Excited about high-volume, real-time data and solving the challenges it poses. Motivated to learn new tools and software to develop performant and scalable systems. ', 'As the first Data Engineer on our Data Science team, collaborate in building the infrastructure and architecture for data generation.', ' As the first Data Engineer on our Data Science team, collaborate in building the infrastructure and architecture for data generation. Work with the data science team to develop active learning tools and UIs that will facilitate the development of statistical models. Analyze, maintain, and improve our existing data systems and infrastructure. Design and implement new data pipelines and databases as needed. ', '5+ years professional experience as a data or backend engineer.', 'Flexible hours and WFH policy.', 'Requirements', 'Have a big impact at a VC-backed consumer startup that\'s doing big things Best Startups in Cambridge - Tech Tribune ""One of the world\'s top 100 AI companies"" - VentureBeat Clean Tech Company of the Year - New England Venture Capital Association 50 on Fire - BostInno Top 100 - Red Herring Best Consumer AI Technology - AI Dev World Global Cleantech 100  ', 'Must be authorized to work in the U.S.', 'Great opportunity to gain experience at a consumer smart home startup.', 'Solid knowledge of computer science.', 'Global Cleantech 100', 'Motivated to learn new tools and software to develop performant and scalable systems.', ' Be a part of building something that will make a difference in the world. Have a big impact at a VC-backed consumer startup that\'s doing big things Best Startups in Cambridge - Tech Tribune ""One of the world\'s top 100 AI companies"" - VentureBeat Clean Tech Company of the Year - New England Venture Capital Association 50 on Fire - BostInno Top 100 - Red Herring Best Consumer AI Technology - AI Dev World Global Cleantech 100   Work with a small team of experienced entrepreneurs creating revolutionary technology. A wide range of difficult and interesting problems to be solved. Great opportunity to gain experience at a consumer smart home startup. Competitive compensation and generous healthcare benefits. Flexible hours and WFH policy. Paid parental leave. Though our company is fully remote due to the COVID-19 pandemic, we have a great office in Central Square in Cambridge, MA right by the Red Line. (re-opening July 2021 at the earliest due to COVID-19)', 'Experience with relational databases such as MySQL.', ' Best Startups in Cambridge - Tech Tribune ""One of the world\'s top 100 AI companies"" - VentureBeat Clean Tech Company of the Year - New England Venture Capital Association 50 on Fire - BostInno Top 100 - Red Herring Best Consumer AI Technology - AI Dev World Global Cleantech 100 ', 'Paid parental leave.', 'Data Engineer', 'Passionate about the energy sector and climate change.', 'Competitive compensation and generous healthcare benefits.', 'Clean Tech Company of the Year - New England Venture Capital Association', 'Analyze, maintain, and improve our existing data systems and infrastructure.', 'Be a part of building something that will make a difference in the world.', 'Job Description', 'Top 100 - Red Herring', 'Though our company is fully remote due to the COVID-19 pandemic, we have a great office in Central Square in Cambridge, MA right by the Red Line. (re-opening July 2021 at the earliest due to COVID-19)']",Not Applicable,Full-time,Engineering,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,The Adecco Group,"Mountain View, CA",3 days ago,60 applicants,"['', 'Desired', 'Project Overview', 'Proficiency in C++/Java/Json ', '\xa012\xa0months', 'Integrate solutions into backend data processing pipelines/servers.', 'Develop decoding mechanisms for extracting information from a standard mapping format and converting it into a format for use in internal Google systems.\xa0', 'Equal Opportunity Employer Minorities/Women/Veterans/Disabled', 'Mountain View, CA.', 'BS / MS in Computer Science or equivalent 6-10 years of relevant experience ', 'Responsible for developing new mapping data ingestion processes under the direction and mentorship of an FTE engineer.\xa0', 'Position Description', 'Develop decoding mechanisms for extracting information from a standard mapping format and converting it into a format for use in internal Google systems.\xa0Integrate solutions into backend data processing pipelines/servers.Clearly communicate updates and questions to stakeholders verbally, over email, or via the internal work management system. Write, review, and update technical documentation.', 'Clearly communicate updates and questions to stakeholders verbally, over email, or via the internal work management system. Write, review, and update technical documentation.', 'Experience with backend software design, development, and testing', 'Skill/Experience/Education', 'Preferable to have experience in big data pipelines (ex: parallel computing) and tools', 'Overall Responsibilities', 'BS / MS in Computer Science or equivalent 6-10 years of relevant experience Proficiency in C++/Java/Json Experience with backend software design, development, and testing', 'Software', 'Top 3 Daily Responsibilities', 'The Company will consider for employment qualified applicants with arrest and conviction records', 'Adecco Group is currently assisting a local client, the World’s Largest Search Engine, in their search to find a\xa0Software Engineer II.\xa0This is part of an exciting fast-growing project within the Search Engine. This is a vendor contract opportunity for\xa012\xa0months\xa0and will sit in\xa0Mountain View, CA.', 'Engineer II', 'Design, develop and debug software solutions aligned with business requirements:\xa0 Integrate maps collected from vendor partners in a standardized format for ingestion.', 'This is a W2 only opportunity.', 'Mandatory']",Associate,Contract,Marketing,Human Resources,2021-03-18 14:34:51
Data Science Engineer,Stripe,"San Francisco, CA",4 weeks ago,119 applicants,"['', 'Our stack spans tools in Scala, Python, R, Javascript, React, SQL ', 'Work on our centralized experimentation platform to pipeline experiment metrics and compute descriptive statistics', 'You Should Include These In Your Application', 'LinkedIn profile', 'We’re Looking For Someone Who Has', 'Help the Data Science team apply and generalize statistical and econometric models on large datasets', 'Resume', 'An inquisitive nature in diving into data inconsistencies to pinpoint issues', 'Identify data needs for business and product teams, understand their specific requirements for metrics and analysis, and build efficient and scalable data pipelines to enable data-driven decisions across Stripe', 'Prior experience with writing and debugging data pipelines using a distributed data framework (Hadoop/Spark/Pig etc…)', 'Improve our data visualization tooling and platform at Stripe to help the team create dynamic tools and reporting', '3+ Years of experience in a Data Engineering or Data Science role, with a focus on building data pipelines or conducting data intensive analysis. ', ' 3+ Years of experience in a Data Engineering or Data Science role, with a focus on building data pipelines or conducting data intensive analysis.  A strong engineering background and are interested in data Prior experience with writing and debugging data pipelines using a distributed data framework (Hadoop/Spark/Pig etc…) An inquisitive nature in diving into data inconsistencies to pinpoint issues Knowledge of a scientific computing language (such as R or Python) and SQL The ability to communicate cross-functionally, derive requirements and architect shared datasets ', 'Build data pipelines that track our marketing funnel from visits to onboarding to active usage of Stripe', 'A strong engineering background and are interested in data', 'The ability to communicate cross-functionally, derive requirements and architect shared datasets', ' Identify data needs for business and product teams, understand their specific requirements for metrics and analysis, and build efficient and scalable data pipelines to enable data-driven decisions across Stripe Design, develop, and own data pipelines and models that power internal analytics for product and business teams  Help the Data Science team apply and generalize statistical and econometric models on large datasets Drive the collection of new data and the refinement of existing data sources, develop relationships with production engineering teams to manage our data structures as the Stripe product evolves  Develop strong subject matter expertise and manage the SLAs for those data pipelines  ', 'Knowledge of a scientific computing language (such as R or Python) and SQL', 'You Will', 'Develop unified user data schemas and tables that provide a complete view of the business across our various products such as Stripe Connect, Atlas, or Sigma', 'Drive the collection of new data and the refinement of existing data sources, develop relationships with production engineering teams to manage our data structures as the Stripe product evolves ', 'Design, develop, and own data pipelines and models that power internal analytics for product and business teams ', 'Some Things You Might Work On', 'Develop strong subject matter expertise and manage the SLAs for those data pipelines ', ' Resume LinkedIn profile', ' Develop unified user data schemas and tables that provide a complete view of the business across our various products such as Stripe Connect, Atlas, or Sigma Build data pipelines that track our marketing funnel from visits to onboarding to active usage of Stripe Work on our centralized experimentation platform to pipeline experiment metrics and compute descriptive statistics Improve our data visualization tooling and platform at Stripe to help the team create dynamic tools and reporting Our stack spans tools in Scala, Python, R, Javascript, React, SQL  ']",Mid-Senior level,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Engineer,Snap Finance,"Salt Lake City, UT",4 weeks ago,46 applicants,"['', 'Understanding of ETL functionality and fluent in integrations into a broader technology ecosystem with data lakes and data warehouses.', ' Generous paid time off Competitive medical, dental & vision coverage 401K with company match Company-paid life insurance + supplemental options Company-paid short-term disability Long term disability and legal coverage Pet insurance, free snacks, and fun events A value-based culture where growth opportunities are endless ', 'Familiar with ETL.', 'Generous paid time off', 'Proven experience in Cloud technologies (Storage, Databases, Caching, Modeling)', 'Familiar with API implementations (REST and SOAP)', 'Excellent communication skills and the ability to work with a team', 'Ensures data security.', ""Why You'll Love It Here…"", ' Understanding of transactional databases, stored procedures, and functions. Data modeling, transformation, and ingestion. Understands and creates data flows that align with the business requirements. Ensures data security. Knowledge of Big Data technologies, such as Apache Hadoop. Familiar with ETL. Focused on performance and optimization. Deep understanding of data governance. Oversee technologies, tools, and techniques used within the team Ensure that all Engineers within the team understand and follow best practices towards data. Work closely with development managers, architects, and team members when necessary to create cohesive software Gain enough business knowledge to be able to actively participate in specifying data solutions for business needs Show flexibility to respond to the changing needs of the business. Data abstraction (nice to have) ', 'Data abstraction (nice to have)', 'Pet insurance, free snacks, and fun events', 'A value-based culture where growth opportunities are endless', 'Knowledge of Big Data technologies, such as Apache Hadoop.', 'Knowledge in Agile Development Methodologies such as Scrum or Kanban', 'You...', 'BS/MS degree in Computer Science, Engineering or a related subject', 'Up to date on the latest software innovations and committed to continually build on existing knowledge and skills', 'Advanced English level B2+ or above', 'The Job...', 'Strong Data Skillset (SQL, Postgres, JSON, No-SQL DBs)', 'Competitive medical, dental & vision coverage', 'More…', 'Show flexibility to respond to the changing needs of the business.', 'Data modeling, transformation, and ingestion.', ' 3+ years of experience in data engineering Strong Data Skillset (SQL, Postgres, JSON, No-SQL DBs) Understanding of ETL functionality and fluent in integrations into a broader technology ecosystem with data lakes and data warehouses. Proven experience in Cloud technologies (Storage, Databases, Caching, Modeling) Knowledge of various programming languages (such Java, C#) and familiar with OOP. Familiar with API implementations (REST and SOAP) Excellent communication skills and the ability to work with a team Up to date on the latest software innovations and committed to continually build on existing knowledge and skills Knowledge in Agile Development Methodologies such as Scrum or Kanban BS/MS degree in Computer Science, Engineering or a related subject Advanced English level B2+ or above ', 'Oversee technologies, tools, and techniques used within the team', '3+ years of experience in data engineering', '401K with company match', 'Company-paid short-term disability', 'Long term disability and legal coverage', 'Focused on performance and optimization.', 'Work closely with development managers, architects, and team members when necessary to create cohesive software', 'Understands and creates data flows that align with the business requirements.', 'Ensure that all Engineers within the team understand and follow best practices towards data.', 'Knowledge of various programming languages (such Java, C#) and familiar with OOP.', 'Company-paid life insurance + supplemental options', 'Understanding of transactional databases, stored procedures, and functions.', 'Gain enough business knowledge to be able to actively participate in specifying data solutions for business needs', 'Deep understanding of data governance.']",Entry level,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Lead Data Engineer ,BICP,"Portland, OR",7 days ago,Be among the first 25 applicants,"['', 'Experience developing with scripting languages such as Shell and Python', 'Key Skills:', 'If you’re looking to join an organization where there is tremendous growth opportunity, that operates with transparency and with a highly collaborative approach then BICP could be a great place to accelerate your career. We offer excellent compensation and these roles will be 100% remote for for foreseeable future.', '3+ years of experience with data engineering with emphasis on data analytics and reporting', 'Experience developing solutions in SnowflakeExperience with workload automation tools such as Airflow, Autosys.Kubernetes, Lambda, Spark Streaming', 'Nice to Have:', 'Kubernetes, Lambda, Spark Streaming', 'Expert experience with SQL and Relational database engineering (Oracle, SQL Server, Teradata)— expert-level SQL abilities', 'BICP, a market leader in BI, Advanced Analytics and Innovation Platforms, is looking to hire two (2) Lead Data Engineers to support new cloud transformation initiatives at our long standing Fortune 100 Retail client in Beaverton, OR. We currently have a team on-site supporting multiple groups with their drive to deploy NextGen platforms and applications capable of powering real time analytics. Ideal candidate will have 3+ years Data Engineering experience in large enterprise environments dealing with massive volumes of data. Consultant will have prior experience supporting cloud based, analytics-driven transformation efforts from legacy to NextGen platforms. Sr. Data Engineer will be expected to Develop and support data solutions in support of reporting and analytics requirements; Engage with product owner, technology lead, report developers, product analysts, and business partners to understand capability requirements and develop data solutions based on product backlog priorities. Additional key skills and qualifications below:', 'Strong experience developing with PySpark, preferably leveraging AWS EMR managed service', '3+ years of experience with data engineering with emphasis on data analytics and reportingExperience developing with scripting languages such as Shell and PythonStrong experience developing with PySpark, preferably leveraging AWS EMR managed serviceExpert experience with SQL and Relational database engineering (Oracle, SQL Server, Teradata)— expert-level SQL abilitiesExperience with agile delivery methodologies– Scrum, SAFe, Extreme ProgrammingExperience working with source-code management tools such as GitHub and JenkinsAbility to partner with business and technology team members, to understand business requirements and translate those into value-add technology solutions', 'Experience developing solutions in Snowflake', 'Experience working with source-code management tools such as GitHub and Jenkins', 'BICP is a leading-edge consulting firm focused on delivering innovative and transformative BI&A, Advanced Analytics and Big Data solutions to our customers.\xa0With deep experience across a diverse ecosystem of NextGen platforms, Analytics and Data Science applications we have the required product ambiguity and expertise to deliver truly best in breed solutions customized to our client’s specific business needs.', 'Ability to partner with business and technology team members, to understand business requirements and translate those into value-add technology solutions', 'About BICP', 'Experience with agile delivery methodologies– Scrum, SAFe, Extreme Programming', 'Experience with workload automation tools such as Airflow, Autosys.']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Cloud Data Engineer,NextPhase.ai,"San Francisco, CA",23 hours ago,180 applicants,"['', 'The person we’re looking for shares our passion about reinventing the data platform and thrives in a very dynamic environment. That means having the flexibility and willingness to jump in and get done what needs to be done to make NextPhase and our customers successful. It means keeping up to date on the ever-evolving technologies for data and analytics to be an authoritative resource for NextPhase, our partners and customers. And it means working collaboratively with a broad range of people both inside and outside the organization. ', '• Competitive salaries', '• Experience in Snowflake performance tuning, capacity planning, handling cloud spend and utilization is a plus', 'We are looking for a world-class technical Data Engineer(s) to be part of a team that is responsible for the technical execution of Snowflake migration at customer sites. The DE will work directly with customers to deploy NextPhase Cloud Data solutions. This person must be a self-starter and used to a fast-paced, fun, energetic environment and comfortable with dealing with the rapidly evolving data market. This person must also have a passion to solve unique world-changing technology problems. The Data Engineer will be responsible for providing the technical expertise and guidance that make NextPhase’s customers successful. ', '• Flexible Family Leave', '• PTO', 'Snowflake', '• Annual Bonus depending on company and individual performance', 'As a Data Engineer, You Will: ', '• 401k', 'Job Description:', 'Location: SF CA', '• Ability to articulate and implement Snowflake standard methodologies and find inventive way of implementing them', '• Lead all aspects of database security both at infrastructure and application level', 'Equal Opportunity\xa0', '• Referral Bonus (up to $2500) when you refer colleagues who are with us for 3 months\xa0hello@nextphase.ai ', '• Medical, Dental and Vision Insurance', 'Duration: Fulltime', 'NextPhase.ai is an equal opportunity employer.\xa0All qualified applicants will receive consideration for employment without regard to race, religion, color national origin, sex, age, status as a protected veteran, or status as a qualified individual with disability.', 'Job Title: Cloud Data Engineer', '• NextPhase U (Learning Certifications, Leadership reimbursed by NextPhase)', '• Strong Object-oriented programming experience in Python/Java or Scala', '• Ability to identify system level bottlenecks in the database and recommend solutions', '• Lead Cross regional data replication and be an expert at database recovery features', '• Experience with one or more Data platforms (e.g.: Teradata, Hadoop, Oracle, SQL Server, DB2)', 'NextPhase.ai\xa0is evolving the way companies utilize data. Our mission is to assist our clients in harnessing the power to turn data into insights - that drive growth. \xa0Our experts help organizations to get started unlocking their data for actionable insights. Some of the Clients in our portfolio that we have successfully partnered with include – Apple, Google, Ellie Mae, First Republic Bank, US Bank, Wells Fargo, Apttus, Equinix, Genentech, Fisher Investments, Pandora and Twitter. ', '• Strong expertise in AWS/Azure and/or GCP specifically in compute, storage and security services', 'Equal Opportunity', '\xa0', 'About NextPhase.ai', '• Ability to define and automate DBA functions', 'Our Benefits:', '• 8+ years’ experience in relational cloud-based database technologies like Snowflake', '• Analyze production workloads and develop strategies to run Snowflake database with scale and efficiency', '• Exposure to writing Shell/Perl scripts in LINUX, Power shell in Windows a plus']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,"Rubio's Restaurants, Inc.","Carlsbad, CA",7 days ago,Be among the first 25 applicants,"['', 'Develop complex interactive/dynamic dashboards with various BI tools.', 'Tableau BI tool. Ability to create data sources, dashboards and worksheets.', 'Strong interpersonal and communication skills both verbal and written.', 'Kubernetes: 1-2 years (Preferred)', ""Bachelor's degree or 7 years’ experience in related disciplines.Experience with OLO online ordering platform, Punchh Loyalty Platform and Patronix Loyalty Platforms a Plus."", 'Working knowledge of Microsoft Visual Studio.', 'Experience with OLO online ordering platform, Punchh Loyalty Platform and Patronix Loyalty Platforms a Plus.', 'Casual dress code, jeans are ok here.', 'Advanced knowledge of MS SQL Server and SQL script writing.Tableau BI tool. Ability to create data sources, dashboards and worksheets.Snowflake. Knowledge of SnowSQL and solid understanding of how to push and pull data.Working knowledge Kubernetes container orchestration, Job Deployment, and Stored Procedure and Trigger development preferred.Working knowledge of MS SQL Server Reporting Services (SSRS).Strong analytical and problem-solving skills.Strong interpersonal and communication skills both verbal and written.Ability to multitask, effectively manage time and meet daily, monthly and annual deadlines.Ability to work effectively with all level employees, and vendors.Working knowledge of Microsoft Visual Studio.', 'Collaborate with Marketing team to implement Google Analytics strategy for collecting consumer behavior data that will be integrated into Tableau dashboards.', 'Carlsbad Corporate Office / Remote', 'Work with Senior Software Engineers to develop and QA ETL applications.', 'Effective reading comprehension reviewing functional and technical specifications.', 'Advanced knowledge of MS SQL Server and SQL script writing.', 'Docker: 1-2 years (Preferred)', 'Responsible for preparing analysis as well as presenting findings and recommendations. This will require working with third party vendors from time to time troubleshooting data discrepancies.', 'Ability to determine and resolve application bugs and feature extensions in a timely manner with reliable quality.', 'Snowflake. Knowledge of SnowSQL and solid understanding of how to push and pull data.', 'Lead development efforts for new loyalty data modeling and analytics initiatives.', 'A 9/80 schedule option (meaning every other Friday off!)Medical, Dental and Vision plans401K retirement savings plans with a matchFlexible in times for a strong work life balance.Casual dress code, jeans are ok here.', 'Working knowledge of MS SQL Server Reporting Services (SSRS).', 'SQL Programming: 5+ years (Required)Tableau 2+ years (Required)Snowflake 1+ years (Preferred)C syntax programming language 5+ years (Preferred)Kubernetes: 1-2 years (Preferred)Docker: 1-2 years (Preferred)', 'Medical, Dental and Vision plans', 'SQL Programming: 5+ years (Required)', 'Responsibilities:', 'Ability to multitask, effectively manage time and meet daily, monthly and annual deadlines.', ""Bachelor's degree or 7 years’ experience in related disciplines."", 'Improve existing data governance, analysis, and reporting processes to drive efficiency.', 'Qualifications', 'Salary Range:', 'Demonstrate initiative with respect to learning new data modeling technologies.', 'C syntax programming language 5+ years (Preferred)', 'Lead development efforts for new loyalty data modeling and analytics initiatives.Responsible for preparing analysis as well as presenting findings and recommendations. This will require working with third party vendors from time to time troubleshooting data discrepancies.Ability to determine and resolve application bugs and feature extensions in a timely manner with reliable quality.Effective reading comprehension reviewing functional and technical specifications.Develop complex interactive/dynamic dashboards with various BI tools.Work with Senior Software Engineers to develop and QA ETL applications.Demonstrate initiative with respect to learning new data modeling technologies.Improve existing data governance, analysis, and reporting processes to drive efficiency.Management of multiple high-profile projects.Collaborate with Marketing team to implement Google Analytics strategy for collecting consumer behavior data that will be integrated into Tableau dashboards.', '401K retirement savings plans with a match', 'A 9/80 schedule option (meaning every other Friday off!)', 'Tableau 2+ years (Required)', 'Management of multiple high-profile projects.', 'Working knowledge Kubernetes container orchestration, Job Deployment, and Stored Procedure and Trigger development preferred.', 'Ability to work effectively with all level employees, and vendors.', ""Rubio's Offers:"", 'Flexible in times for a strong work life balance.', 'Snowflake 1+ years (Preferred)', 'Strong analytical and problem-solving skills.']",Entry level,Full-time,Information Technology,Food Production,2021-03-18 14:34:51
Data Engineer,Spring EQ LLC,"Philadelphia, PA",3 weeks ago,Be among the first 25 applicants,"['Who Are We?', 'Experience with the following databases: PostgreSQL, DynamoDB, Microsoft SQL Server', 'Strive to make every customer interaction a great one', 'Communication - This position will gather and interpret requirements from business stakeholders to build new applications or add functionality to existing applications, and work with additional members of the Technology team to architect and build solutions', 'Innovate, innovate, innovate', 'Communication', 'Integrations', 'Time is money - At Spring EQ we recognize the importance of working with a sense of urgency. Ideal candidates will possess the ability to thrive in a fast-paced, team-oriented environment', 'The ideal candidate will enthusiastically contribution to a culture of innovation and continuous improvement', 'Design & Implementation -', 'Integrations - This position will be responsible for the Integration with databases such as Postgres, SQL Server and DynamoDB and leverage APIs to pull and post data to other systems.', 'Design & Implementation - This position will be responsible for design and implementation of low-latency, high-availability, performant, and secure data processing applications which will power our backend services and front-end reporting.', 'Experience with Amazon Web Services (AWS)', 'At Spring EQ, we;', 'Experience with OLAP and the building and maintenance of data marts utilizing star or snowflake schemas', 'Basic understanding of columnar databases and data architecture', ""Strive to make every customer interaction a great oneRecognize behind every loan is a person or family trusting us to handle what may be a once in a lifetime transactionWork hard and have fun to get the job doneInteract on a first name basis and recognize each team member's unique valueEncourage ways for our team members to learn, develop, diversify and grow with Spring EQPromote our team members so they can share their knowledge with othersInnovate, innovate, innovateCreate and embrace the latest technologySimplify constantly, challenging every process we use to better accomplish our goalsBuild upon a strong company culture and foster an environment of togetherness, support, and accountability."", 'Strong Python or related-language skills', 'Familiarity with streaming and queuing technologies such as Kafka, Kinesis and SQS', 'Strong understanding on SQL and data transformation principles', 'Create and embrace the latest technology', 'Ability to multitask, problem solve and efficiently manage time', 'Spring EQ was founded to help homeowners unlock the value of their home by providing visibility to it’s worth and provide quick access to that equity. Our foundation expanded to include direct mortgage lending in 2019 as interest rates reached historic low levels which set off an unprecedented round of refinancing and home buying opportunities for our borrowers. By surrounding ourselves with some of the brightest, hard-working minds in the industry, we constantly improve the loan experience for our customers.', 'Data Quality and Optimization ', 'Encourage ways for our team members to learn, develop, diversify and grow with Spring EQ', 'Promote our team members so they can share their knowledge with others', 'Simplify constantly, challenging every process we use to better accomplish our goals', 'Spring EQ is hiring a Data Engineer to join our growing IT team.\xa0', 'Build upon a strong company culture and foster an environment of togetherness, support, and accountability.', 'Spring EQ is an Equal Opportunity Employer.', 'Understanding of real-time and batch data processing and how to build both types of systems', 'Design & Implementation - This position will be responsible for design and implementation of low-latency, high-availability, performant, and secure data processing applications which will power our backend services and front-end reporting.Integrations - This position will be responsible for the Integration with databases such as Postgres, SQL Server and DynamoDB and leverage APIs to pull and post data to other systems.Data Quality and Optimization - This role will identify and respond to data quality and performance issues as well as drive solutions to improve data issues over timeCommunication - This position will gather and interpret requirements from business stakeholders to build new applications or add functionality to existing applications, and work with additional members of the Technology team to architect and build solutions', 'Data Quality and Optimization - This role will identify and respond to data quality and performance issues as well as drive solutions to improve data issues over time', 'The ideal candidate will have a self-starter mentality with the ability to solve open ended business challenges', 'Strong understanding on SQL and data transformation principlesStrong Python or related-language skillsUnderstanding of real-time and batch data processing and how to build both types of systemsExperience with OLAP and the building and maintenance of data marts utilizing star or snowflake schemasExperience with the following databases: PostgreSQL, DynamoDB, Microsoft SQL ServerBasic understanding of columnar databases and data architectureExperience with Amazon Web Services (AWS)Familiarity with streaming and queuing technologies such as Kafka, Kinesis and SQS', ""Interact on a first name basis and recognize each team member's unique value"", '\xa0', 'Ability to multitask, problem solve and efficiently manage timeThe ideal candidate will have a self-starter mentality with the ability to solve open ended business challengesThe ideal candidate will enthusiastically contribution to a culture of innovation and continuous improvementTime is money - At Spring EQ we recognize the importance of working with a sense of urgency. Ideal candidates will possess the ability to thrive in a fast-paced, team-oriented environmentSpring EQ values personal excellence, integrity and accountability - we need candidates who demonstrate these qualities in their everyday lives', 'Spring EQ values personal excellence, integrity and accountability - we need candidates who demonstrate these qualities in their everyday lives', 'Recognize behind every loan is a person or family trusting us to handle what may be a once in a lifetime transaction', 'Work hard and have fun to get the job done']",Entry level,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Engineer,Koddi,United States,3 weeks ago,70 applicants,"['', 'Getting the experience right, together.\xa0', 'You have experience working with a large multi-contributor Git repository', 'WHAT YOU’LL DO', 'Koddi is a technology company that was born in 2013 from an opportunity to innovate in the\xa0 adtech space. Our award-winning SaaS platform provides a robust network for brands to connect with consumers and drive revenue through native sponsored placements, metasearch, and programmatic media campaigns.', 'WHO WE ARE', 'You have experience mentoring teammates', 'Recommend and implement improvements to data processes and warehouses that improve supportability, usability, and scalability', 'OUR MISSION AND PROMISE', 'You adapt well working with emerging technologies', 'You have strong coding fundamentals with experience in C, C++, GoLang, or Java\xa0', 'You thrive in ever-changing environments', 'As a Senior Data Engineer at Koddi, you will work with our technology team to build and maintain our suite of data pipelines, stores, and databases that power sophisticated adtech products used by many of the world’s largest advertisers. We are looking for smart and hardworking individuals who love to build world-class software. The right candidates will be creative thinkers who can design and deploy professional applications using the newest technologies to solve real business problems.', 'Work within robust data systems and develop custom solutions while consulting with external customers', ""You have strong coding fundamentals with experience in C, C++, GoLang, or Java\xa0You have experience working with a large multi-contributor Git repositoryYou have experience working with SQL and No-SQL databases (SQL Server, Postgres, Redis, Aerospike)You’ve run large scale applications in AWSYou adapt well working with emerging technologiesYou have a strong technical base and innovative mindsetYou have experience mentoring teammatesYou thrive in ever-changing environmentsYou communicate well with coworkers of all levelsYou have a Bachelor's or Master’s degree in Computer Science with at least five years working experience on an enterprise-level application"", 'Optimize and refactor existing code', 'WHO YOU ARE', 'You communicate well with coworkers of all levels', 'Work within robust data systems and develop custom solutions while consulting with external customersDesign, deliver and implement data engineering solutions for optimized downstream impact, with end-to-end input and oversight of data engineering projectsRecommend and implement improvements to data processes and warehouses that improve supportability, usability, and scalabilityOptimize and refactor existing codeImprove the efficiency, scalability, and reliability of applications', 'Passionate about development in leading technologies? Looking to become a major player on a diverse team in your next career adventure? Koddi Data Engineers drive innovation by embracing challenges and leveraging decision science to solve complex problems in adtech. ', 'Improve the efficiency, scalability, and reliability of applications', 'You have experience working with SQL and No-SQL databases (SQL Server, Postgres, Redis, Aerospike)', ""You have a Bachelor's or Master’s degree in Computer Science with at least five years working experience on an enterprise-level application"", 'You’ve run large scale applications in AWS', 'Based in Fort Worth, Texas, we’ve grown to become a diverse global team. Ranked by Forbes, Deloitte, and Inc. magazine as one of the fastest-growing companies in the nation, we’re growing rapidly and looking for innovative problem solvers to join our team. ', 'Design, deliver and implement data engineering solutions for optimized downstream impact, with end-to-end input and oversight of data engineering projects', 'You have a strong technical base and innovative mindset']",Mid-Senior level,Full-time,Information Technology,Internet,2021-03-18 14:34:51
Data Engineer (Remote),Automox,United States,1 week ago,78 applicants,"['', 'THE FUN STUFF', 'Founded in 2015,\xa0Automox is coming off its fourth quarter of record growth that has seen its platform become the most recommended solution in endpoint security and the preferred endpoint management solution for over 1,500 customers across 30 countries.\xa0With an increasing number of operating systems, servers, hardware, and applications that need to be maintained, updated, configured, and patched on a regular basis, IT ops teams are feeling fatigued and vulnerable. Automox is building a company and team to tackle this problem for millions of endpoints.\xa0', 'Have an opinion, test your ideas, work with talented yet humble people, have fun, get stuff done', '-Time off: We have a flexible PTO policy with an additional 9 paid holidays.', 'Benefits:\xa0', 'Comfortable working in a remote/distributed work environment requiring excellent verbal, written, and asynchronous communication skills', 'Ability to assemble large, complex data sets that satisfy the needs of both internal and external customers', 'Work on projects that are critical to Automox’s mission and have high visibility across the company', 'OVERVIEW', 'Familiarity with cloud environments and technologies (AWS preferred)', 'Equity:', 'Build, enable, and maintain high-quality, reliable data infrastructure', 'Track record of creating and maintaining data pipelinesExpertise with deploying and maintaining distributed data processing frameworks, database technologies, and streaming/messaging systemsAbility to assemble large, complex data sets that satisfy the needs of both internal and external customersBuild analytics tools as part of a data platform to enable data access, analytics workflows, and actionable insights into customer’s IT environments and the company’s operational efficiencyFamiliarity with cloud environments and technologies (AWS preferred)Experience with both relational SQL and NoSQL databasesProficiency with and willingness to learn programming languages such as Golang, Python, Java, ScalaExperience partnering with business intelligence and analytics teams\xa0Comfortable working in a remote/distributed work environment requiring excellent verbal, written, and asynchronous communication skillsKnowledge of data science principles, statistics, or machine learning can be beneficial but should not be considered a required skill set', 'Benefits', 'Build analytics tools as part of a data platform to enable data access, analytics workflows, and actionable insights into customer’s IT environments and the company’s operational efficiency', 'Bonus', 'The mission of the Data Platform & Analytics (DPA) team is to ensure customers have the data and tools they need to make important and timely decisions. The role of the Data Engineer is to build the infrastructure that makes this possible.\xa0', 'Salary', '-$105,000 - $150,000/year', '-Stock options\xa0\xa0', 'Bonus:', 'LOCATION', 'Experience partnering with business intelligence and analytics teams\xa0', 'Salary:', 'Remote : USA the world is changing so are we, Automox has moved to a fully distributed company and is open to hiring across the US.\xa0', '-Perks: Monthly internet and wellness stipend, money to set up your home office, and no commute.', 'Expertise with deploying and maintaining distributed data processing frameworks, database technologies, and streaming/messaging systems', 'Knowledge of data science principles, statistics, or machine learning can be beneficial but should not be considered a required skill set', 'Partner with data science, engineering, and product teams to deliver new capabilities to customers', 'Automox aims to be an employer of choice and we know that means offering a comprehensive compensation package to support our employees. Our packages include base salary, bonus, equity, and benefits for all full time permanent employees.', 'Experience with both relational SQL and NoSQL databases', '-All employees are part of our Company bonus plan. Our bonus is a mix of company performance and individual contributions.', 'We are committed to an inclusive and diverse Automox. Automox is an equal opportunity employer. We do not discriminate based on race, color, ethnicity, ancestry, national origin, religion, sex, gender, gender identity, gender expression, sexual orientation, age, disability, veteran status, genetic information, marital status, or any legally protected status.', 'Track record of creating and maintaining data pipelines', '-Parental Benefits: Adoption benefits, Parental leave', 'Leverage modern engineering practices and tools in a cloud-native SaaS environment', 'Work on projects that are critical to Automox’s mission and have high visibility across the companyBuild, enable, and maintain high-quality, reliable data infrastructurePartner with data science, engineering, and product teams to deliver new capabilities to customersLeverage modern engineering practices and tools in a cloud-native SaaS environmentHave an opinion, test your ideas, work with talented yet humble people, have fun, get stuff done', 'WHY AUTOMOX\xa0\xa0', 'We are on a mission to enable every IT Admin to automate the fundamental tasks that keep their corporation secure. This mission can only be accomplished with a culture embodies entrepreneurialism, accountability and providing our employees with the clear direction and freedom to do their best work. We don’t measure excellence based on how but on the what. Each employee has a value and contribution to the success of Automox. We look forward to working with you and seeing the success you will bring on our journey.\xa0\xa0', 'Equity', '-Healthcare options through Cigna and Guardian including Medical, Dental, Vision, Basic Life insurance, Voluntary Life Insurance, Basic STD & LTD, HSA, FSA, 401(k) and more. Automox has a generous employer contribution towards all health plans with low premiums for all employees.\xa0\xa0', ""-Our salary ranges are based on national averages and are determined based on the level of the position we are hiring for. The ranges are wide to leave room for variability in a candidate's skills, experience, and location all which impact where someone might come in on the range.\xa0"", 'TOTAL COMPENSATION', 'Proficiency with and willingness to learn programming languages such as Golang, Python, Java, Scala', 'KEY SKILLS AND ATTRIBUTES']",Associate,Full-time,Engineering,Computer & Network Security,2021-03-18 14:34:51
Data Engineer,Pie Insurance,"Denver, CO",1 month ago,79 applicants,"['', 'Our team is looking for an experienced data engineer.\xa0We expect you’ll have spent at least 3 years in the data warehouse and/or data analytics space. Of course, you’ll also need certain skills and abilities to do the work.', 'Pie Insurance Named a Top Colorado Company 2020', 'Strong experience in ETL/ELT platforms is strongly preferred.', 'The Right Stuff', 'Trustpilot', 'As a data engineer with Pie, you will work with our data architect to Pie-oneer our data environment.\xa0This individual will be a key member that will work directly with our data architect to define the future state of our data architecture. This role will work in data architecture, data analytics, ETL development, and data reporting.', 'Our Achievements', 'Pie Insurance is an equal opportunity employer.\xa0We do not discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity, marital status, age, disability, national or ethnic origin, military service status, citizenship, or other protected characteristic.', 'Experience in data warehouse and/or data analytics. Qualified candidates may also come from a strong database skill-set involved in analytics architecture.', 'Strong experience in writing complex SQL queries.', 'Compensation Range for position: $113,000 - $145,000', 'Big Data and Business Intelligence exposure would help in the success of this role.', 'Success in this position will be establishing how data comes into and flows through the Pie insurance platform. This data will be used to help our organization quote customers based on data on best policy and prices for their workers compensation insurance.\xa0', 'Glassdoor', 'We value and want to support our team members, and are proud to offer a comprehensive compensation package which includes the following:', ""How You'll Do It"", 'We are an insurtech company where smart people can see the impact of their work as we tackle meaningful problems together. We think it’s fun to disrupt an industry that has been slow to change. But we aren’t shaking things up for the sake of change, we’re here to solve big problems using technology and an innovative approach to improve how small business owners access insurance. Like our small business clients, we are a diverse team of builders, dreamers, and entrepreneurs, so at the heart of every decision we make is the idea that if it doesn’t serve our clients, it doesn’t serve us. We hire passionate people who like to work hard, yet we also know that life exists outside the office. Small businesses are the backbone of the economy; talented team members are the backbone of Pie. We are pie-oneering a whole new approach to insurance.', 'As a Data Engineer at Pie Insurance, you’ll be a member of the team responsible for transforming the commercial insurance market by delivering best-in-class data architecture solutions and driving more accurate data-driven decision making.\xa0\xa0', '3-5 years working in data as an engineer. Building data solutions for a company who uses data as a primary part of their business.', '3-5 years working in data as an engineer. Building data solutions for a company who uses data as a primary part of their business.Experience in data warehouse and/or data analytics. Qualified candidates may also come from a strong database skill-set involved in analytics architecture.Strong experience in writing complex SQL queries.Strong experience in ETL/ELT platforms is strongly preferred.Exposure to one major SQL RDBMS or analytics database. (Snowflake, Redshift, MySQL, Postgres, Oracle, SQL Server, etc.)Big Data and Business Intelligence exposure would help in the success of this role.', 'Exposure to one major SQL RDBMS or analytics database. (Snowflake, Redshift, MySQL, Postgres, Oracle, SQL Server, etc.)', 'Our goal is to make all aspects of working with us as easy as Pie! That includes our offer process.\xa0When we have identified talent that is a good fit for Pie, we work hard to present an equitable and fair offer. We look at your knowledge, skills and experience that you bring, along with your compensation expectations and align that with our company equity processes to determine our offer ranges.\xa0', '\xa0', 'Pie Insurance Raises $45M Series B\xa0', 'Other Benefits:\xa0Each year Pie reviews Company performance and may grant discretionary bonuses to eligible team members.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Coupa Software,"Reno, NV",1 week ago,Be among the first 25 applicants,"['', '3. Strive For Excellence', 'Develop and maintain star schemas and assist Data Analysts with the maintenance and development of BI-serving models', 'Requirements: ', 'Experience with Workato and Fivetran or other integration (Mulesoft, Informatica) tools required', 'Please be advised, inquiries or resumes from recruiters will not be accepted.', 'Data stores infrastructure management: ensure that data lake, scheduled jobs, query and exploration tool and connections with visualization tools are meeting business requirements while remaining cost-effective', 'Fully utilize the resources available in our data stores infrastructure to optimize operations and maximize data security and reliability', '2. Focus On Results', 'Coding proficiency in at least one modern programming language (Python, Ruby, Java, etc.)', 'Experience interacting with APIs, utilizing SDKs, and other tools', '3+ years of technical hands-on experience with Data Warehouses such as AWS RDS PostgreSQL, Snowflake, AWS Redshift etc.Working experience with AWS resources strongly preferred (e.g. S3, Glue, Athena, Lambda, RDS, EC2, Spark, CloudFormation)Coding proficiency in at least one modern programming language (Python, Ruby, Java, etc.)Proven track record of sharing outcomes through written communication, including an ability to effectively communicate with both business and technical teamsStrong interpersonal skills and the ability to communicate complex technology solutions to leadership, gain alignment, and drive progressExperience with Workato and Fivetran or other integration (Mulesoft, Informatica) tools requiredAdvanced knowledge of PostGres SQLExperience interacting with APIs, utilizing SDKs, and other tools', 'Collaborate with Data Architecture IT team to internally serve Coupa business units with the highest quality, best-in-breed solutions for BI infrastructures, applications integrations, task automation and any relevant data automation needsBuild and maintain the infrastructure required for extraction, loading, transformation and storage of data from multiple data sources using custom scripted as well as SaaS technologiesBuild processes supporting data transformation, data structures, metadata, dependencies and model managementDevelop and maintain star schemas and assist Data Analysts with the maintenance and development of BI-serving modelsData stores infrastructure management: ensure that data lake, scheduled jobs, query and exploration tool and connections with visualization tools are meeting business requirements while remaining cost-effectiveManage and maintain the connections between different data consuming tools and data storesCreate and maintain tools to monitor data quality and integrityPerform periodic health checks and audits on all data stores, ETLs, ELTs and data consumption connectionsFully utilize the resources available in our data stores infrastructure to optimize operations and maximize data security and reliability', '1. Ensure Customer Success', 'Proven track record of sharing outcomes through written communication, including an ability to effectively communicate with both business and technical teams', 'Perform periodic health checks and audits on all data stores, ETLs, ELTs and data consumption connections', 'Manage and maintain the connections between different data consuming tools and data stores', 'Advanced knowledge of PostGres SQL', 'Working experience with AWS resources strongly preferred (e.g. S3, Glue, Athena, Lambda, RDS, EC2, Spark, CloudFormation)', 'Collaborate with Data Architecture IT team to internally serve Coupa business units with the highest quality, best-in-breed solutions for BI infrastructures, applications integrations, task automation and any relevant data automation needs', 'Strong interpersonal skills and the ability to communicate complex technology solutions to leadership, gain alignment, and drive progress', 'Responsibilities: ', 'Create and maintain tools to monitor data quality and integrity', 'Build processes supporting data transformation, data structures, metadata, dependencies and model management', '3+ years of technical hands-on experience with Data Warehouses such as AWS RDS PostgreSQL, Snowflake, AWS Redshift etc.', 'Build and maintain the infrastructure required for extraction, loading, transformation and storage of data from multiple data sources using custom scripted as well as SaaS technologies']",Not Applicable,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Engineer,"Docker, Inc","Washington, DC",3 weeks ago,50 applicants,"['', ' Implement, document, oversee and evolve the Snowflake and ETL infrastructure Maintain the integrity of data within our data pipeline and warehouse Ensure quality of data and completeness of event logging across Docker codebase Integrate data from 3rd party services via ETL tools and custom pipelines Develop ETL jobs and tests to process, validate, transport, collate, aggregate, and distribute data Transform raw event logs into higher-order tables to make existing analysis easier and new analysis possible Creating automated reporting of weekly and monthly metrics and ROI for the executive management team and board Integrate emerging methodology, technology, and version control practices that best fit the team.  Design, build and automate business metrics into self-serve dashboards via Looker  Collaborate with product managers and analysts throughout the company to deliver reliable data that powers actionable insights Champion a data-informed mindset within our culture ', 'Champion a data-informed mindset within our culture', 'Strong verbal and written communication skills', 'At least 6 months of experience with Looker and LookML ', 'Familiarity with data warehousing concepts including data model design and query optimization strategies', 'Creating production-ready ETL scripts with Python and SQL', 'Experience using data collection platforms such as Segment, RudderStack, Fivetran etc. ', 'Responsibilities', 'Data Engineer (Remote)', 'Implement, document, oversee and evolve the Snowflake and ETL infrastructure', 'Develop ETL jobs and tests to process, validate, transport, collate, aggregate, and distribute data', 'Experience using data analysis and/or statistics to inform decisions ', 'Experience designing and deploying high-performance systems with reliable monitoring and logging practices', 'Maintain the integrity of data within our data pipeline and warehouse', 'Familiarity with at least one cloud ecosystem: AWS/Azure Infrastructure/Google Cloud', 'Qualifications', 'Integrate data from 3rd party services via ETL tools and custom pipelines', 'Transform raw event logs into higher-order tables to make existing analysis easier and new analysis possible', 'Design, build and automate business metrics into self-serve dashboards via Looker ', ' BS/MS in Computer Science, Math, Physics, or other technical fields At least 6 months of experience with Looker and LookML  Proficiency in building data pipelines leveraging Docker and orchestration frameworks such as Jenkins/Airflow/Luigi Experience designing and deploying high-performance systems with reliable monitoring and logging practices Proficiency working with a Data Warehouse platform (e.g. Snowflake, Redshift, BigQuery, AWS Athena, or Spectrum) Experience using data collection platforms such as Segment, RudderStack, Fivetran etc.  Familiarity with at least one cloud ecosystem: AWS/Azure Infrastructure/Google Cloud Experience of working in an agile environment and using tools such as JIRA/Asana/Trello  ', ' 2+ years of relevant industry experience Familiarity with data warehousing concepts including data model design and query optimization strategies Experience using and maintaining BI visualization tools such as (but not limited to) Looker, Tableau, or Power BI.  Creating production-ready ETL scripts with Python and SQL Experience with version control systems such as Github, Gitlab, Bitbucket etc.  Experience automating business and reporting processes Experience using data analysis and/or statistics to inform decisions  Strong verbal and written communication skills ', 'Experience automating business and reporting processes', 'Collaborate with product managers and analysts throughout the company to deliver reliable data that powers actionable insights', 'Experience using and maintaining BI visualization tools such as (but not limited to) Looker, Tableau, or Power BI. ', 'Experience with version control systems such as Github, Gitlab, Bitbucket etc. ', 'Proficiency in building data pipelines leveraging Docker and orchestration frameworks such as Jenkins/Airflow/Luigi', 'BS/MS in Computer Science, Math, Physics, or other technical fields', 'Ensure quality of data and completeness of event logging across Docker codebase', 'Integrate emerging methodology, technology, and version control practices that best fit the team. ', '2+ years of relevant industry experience', 'Experience of working in an agile environment and using tools such as JIRA/Asana/Trello ', 'Creating automated reporting of weekly and monthly metrics and ROI for the executive management team and board', 'Proficiency working with a Data Warehouse platform (e.g. Snowflake, Redshift, BigQuery, AWS Athena, or Spectrum)', 'Preferred Qualifications']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Cervello,"Addison, TX",2 weeks ago,91 applicants,"['', 'Create and manage data environments in the Cloud', 'Strong development background with experience in at least two scripting, object oriented or functional programming language, etc. SQL, Python, Java, Scala, C#, R', 'Plan and execute secure, good practice data integration strategies and approaches', 'Data Warehousing experience, building operational ETL data pipelines across a number of sources, and constructing relational and dimensional data models', 'This is a fantastic opportunity to be involved in end-to-end data management for cutting edge Advanced Analytics and Data Science', 'A deep personal motivation to always produce outstanding work for your clients and colleagues', 'Excel in team collaboration and working with others from diverse skill-sets and backgrounds', 'ABOUT US: OUR WORKPLACE IS FUN AND FAST-PACED:', 'Acquire, ingest, and process data from multiple sources and systems into Big Data platforms', 'Have a strong understanding of Information Security principles to ensure compliant handling and management of client data', 'Experience working on lively projects and a consulting setting, often working on different and multiple projects at the same time', 'Experience working on projects within the cloud ideally AWS or Azure', 'Equal Employment Opportunity and Nondiscrimination', 'Experience with NLP, Machine Learning, etc. is a plus', 'Plan and execute secure, good practice data integration strategies and approachesAcquire, ingest, and process data from multiple sources and systems into Big Data platformsCreate and manage data environments in the CloudCollaborate with our business analysts and data scientists to map data fields to hypotheses and curate, wrangle, and prepare data for use in their advanced analytical modelsHave a strong understanding of Information Security principles to ensure compliant handling and management of client dataThis is a fantastic opportunity to be involved in end-to-end data management for cutting edge Advanced Analytics and Data Science', 'Collaborate with our business analysts and data scientists to map data fields to hypotheses and curate, wrangle, and prepare data for use in their advanced analytical models', 'Experience on client-facing projects, including working in close-knit teamsExperience and interest in Big Data technologies (Hadoop / Spark / NoSQL DBs)Experience or familiarity with real-time ingestion and streaming frameworks is a plusExperience and desire to work with open source and branded open source frameworksExperience working on projects within the cloud ideally AWS or AzureExperience with NLP, Machine Learning, etc. is a plusExperience working on lively projects and a consulting setting, often working on different and multiple projects at the same timeStrong development background with experience in at least two scripting, object oriented or functional programming language, etc. SQL, Python, Java, Scala, C#, RData Warehousing experience, building operational ETL data pipelines across a number of sources, and constructing relational and dimensional data modelsExcellent interpersonal skills when interacting with clients in a clear, timely, and professional manner.A deep personal motivation to always produce outstanding work for your clients and colleaguesExcel in team collaboration and working with others from diverse skill-sets and backgrounds', 'Qualifications', 'Summary', 'Experience and desire to work with open source and branded open source frameworks', 'Excellent interpersonal skills when interacting with clients in a clear, timely, and professional manner.', 'Experience and interest in Big Data technologies (Hadoop / Spark / NoSQL DBs)', 'Experience or familiarity with real-time ingestion and streaming frameworks is a plus', 'Experience on client-facing projects, including working in close-knit teams']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Staff Data Engineer,Codecademy,"Remote, OR",2 days ago,Be among the first 25 applicants,"['', 'Familiarity with the database technologies we use in production: MongoDB, PostgreSQL or similar NoSQL / SQL stores', 'Build scalable data infrastructure solutions for both internal and external use cases.', 'Hands-on experience building and maintaining large scale ETL systems.', ' Designing data models to meet critical product and business requirements SQL and data warehousing skills -- able to write clean and efficient queries. Hands-on experience building and maintaining large scale ETL systems. Deep understanding of database design and data structures. Experience with MPP columnar databases such as Redshift, Greenplum, Vertica and SQL + NoSQL data stores Fluency in one of the following languages: Python, Scala, Ruby, Go. Experience working with cloud-based data platforms (we use AWS). Ability to make pragmatic engineering decisions, write extensive tests and create documentation Strong project management skills; a proven ability to gather and translate requirements from stakeholders across functions and teams into tangible results. ', 'What Will Make You Stand Out', 'Experience with big data processing technologies such as Apache Spark.', 'Designing data models to meet critical product and business requirements', 'Experience (or interest in learning to) productionizing machine learning models.', 'Integrate new data sources into our existing data architecture.', 'Design and optimize new and existing data pipelines and streams.', 'Building anomaly detection systems that will help detect real-time data issues and improving internal tools.', 'Experience working with cloud-based data platforms (we use AWS).', ' Assess our current data infrastructure and needs to devise a strategic roadmap Partner closely with the Data Science team and other key stakeholders to determine organizational and specifically product needs. Build scalable data infrastructure solutions for both internal and external use cases. Design and optimize new and existing data pipelines and streams. Integrate new data sources into our existing data architecture. Collaborate with a cross-functional team of software engineers and data scientists. ', 'Experience with MPP columnar databases such as Redshift, Greenplum, Vertica and SQL + NoSQL data stores', 'Ability to make pragmatic engineering decisions, write extensive tests and create documentation', 'Deep understanding of database design and data structures.', 'Fluency in one of the following languages: Python, Scala, Ruby, Go.', ""What You'll Need"", ' Building anomaly detection systems that will help detect real-time data issues and improving internal tools. Experience with tools in our current warehousing stack: Apache Airflow, Redshift, Segment, Kinesis, S3, Looker. Familiarity with the database technologies we use in production: MongoDB, PostgreSQL or similar NoSQL / SQL stores Comfort with containerization technologies: Docker, Kubernetes, etc. Experience with big data processing technologies such as Apache Spark. Experience (or interest in learning to) productionizing machine learning models. ', ""What You'll Do"", 'Partner closely with the Data Science team and other key stakeholders to determine organizational and specifically product needs.', 'Strong project management skills; a proven ability to gather and translate requirements from stakeholders across functions and teams into tangible results.', 'Collaborate with a cross-functional team of software engineers and data scientists.', 'Assess our current data infrastructure and needs to devise a strategic roadmap', 'SQL and data warehousing skills -- able to write clean and efficient queries.', 'Experience with tools in our current warehousing stack: Apache Airflow, Redshift, Segment, Kinesis, S3, Looker.', 'Comfort with containerization technologies: Docker, Kubernetes, etc.']",Associate,Full-time,Information Technology,E-Learning,2021-03-18 14:34:51
Data Engineer(Big Data),Venhar,"Plano, TX",1 day ago,Be among the first 25 applicants,[],Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Ledgent Technology,Miami-Fort Lauderdale Area,,N/A,"['', '•\tData mining large data files and analyzing, forecasting, and trending data to support various teams.\xa0', '•\tFamiliarity with Access (preferred)', '•\tBachelor’s degree in Mathematics, Statistics, Information Technology, Finance, Economics, or a related field of study (required)', '•\tDevelop complex analysis requiring the integration of multiple data sources.', '•\t5+ years of data analysis and business analysis experience (required).\xa0', 'What you will be doing:', '•\tPrepare feasibility analysis, business process assessments, gap analysis, cost-benefit analysis, and industry best practice research. Write reports to identify areas of improvement or growth.', '•\tAssemble large, complex data sets and evaluate data integrity.\xa0', '•\tExperience writing SQL queries (required)', '**The ideal candidate will reside in Florida but not required**\xa0', 'What you should bring to the table:', '•\tAmple experience analyzing data and establishing business rules and requirements (required).', '•\tWork with individuals from various departments to ensure the requirements of their requests are understood and the outcome presented meets their expectations.', '•\tAn understanding of relational database structures, theories, principles, and practices (required).', '•\tAdvanced proficiency in Excel that includes pivot tables, macros, v/h lookups (required).', '•\tCollaborate with management team to implement process and system changes.', '•\tProficiency with statistical and analytical software (required).', '•\tIdentify, design, and implement new internal process improvements.\xa0', '•\tSearch for opportunities to automate manual processes and develop on-demand tools for           descriptive data.', '•\tDevelop instrumentation to track progress against key business objectives.', '•\tVerify the relevance and accuracy of data.', 'This position requires more than just analysis. Candidates will have to take a SQL Assessment to ensure their expertise in SQL. Degree is required.\xa0', '•\tStandardize data to consistent format/structure.', '•\tRegularly interpret, summarize and present data outcomes to senior leadership.\xa0', '•\tExtract large data records from internal systems through data mining, queries, data analysis methods, building and implementing models, using/creating algorithms, and creating/running simulations.', '•\tMonitor, analyze, and report variances from planned and forecasted figures.']",Mid-Senior level,Full-time,Information Technology,Staffing and Recruiting,2021-03-18 14:34:51
Cloud Data Engineer,Info Services,United States,2 days ago,Be among the first 25 applicants,"['', '\uf0b7 Have a technology toolbox – Hands on experience with newer technologies relevant', '\uf0b7 Creative thinking and motivated self-starter', 'other groups jealous', '\uf0b7 This candidate will work on building tools and a framework for data', 'C2CImmigration Status AllowedAny StatusNotes3 positions open for this role.', 'strategic use and development of technology to power the DTCI business, deliver world-class', 'Preferred', 'DTCI Tech is charged with developing a unified data and analytics platform that supports', 'stream processing specialists, API developers, our DevOps team, and', 'services.', '\uf0b7 Experience working in AWS cloud environment.', 'landed data, and landed data with conformed data; Tools such as Springboot, Java, Kafka', 'technology and distribution organization for world-class content. DTCI is comprised of', '\uf0b7 Build software across our entire cutting-edge analytics platform, including', '\uf0b7 Excellent communication skills and ability to interact with all levels of end users and', '\uf0b7 Proficiency with agile development methodologies shipping features every two', 'experience with developing in a cloud native environment with many different', 'Summarize job responsibilities, core deliverables and major duties. What is required for the position to exist?\xa0\xa0', 'technical resources', 'and Yarn/Kubernetes.', 'source contributions you are proud to share.Interview TypeWebexLocation (City, State)Bristol, CT – New York, NY – Seattle, WA – Burbank, CARequirement TypeW2', 'technologies.', 'robust data processing, and caching technologies.', 'Disney’s international media and direct-to-consumer businesses globally, including all Disney', 'Disney’s Consumer, Digital, and Ad Sales data in a secure and compliant way.', 'data processing, storage, and APIs, with awesome cutting-edge', 'This candidate will work out a framework for data acquisition, conformance, and validation on the cloud. The data acquisition involves defining and creating schema registry of the source system data using python or Java. Then automate the schema generation process and maintain. The data conformance involves creating a framework that uses stream processing to consume the landed data, apply transformation and sink the data to snowflake. Tools such as Kafka, KafkaStreams, Apache Flink and Java/Scala/Python will be used.', 'and snowflake will be used.', 'Required', '\uf0b7 Experience with real-time and scalable systems are preferred.', 'Direct to Consumer and International (DTCI) is Disney’s global, multiplatform media, product,', '\uf0b7 Data and API ninja –You are also very handy with Streaming technologies such as', 'such as Kafka, KafkaStreams, Apache Flink and Java/Scala/Python will be used.', '\uf0b7 The data acquisition involves defining and creating schema registry of the source system', '\uf0b7 Expert knowledge of data systems and SQL', 'operationalizing clusters in cloud environment.', 'Job Summary:', 'Required Education, Experience/Skills/Training:', '\uf0b7 Ensure performance isn’t our weakness by implementing and refining', '\uf0b7 Have 7+ years of experience developing with a mix of languages (Java, Scala, Python,', 'technologies. We operate in real-time with high-availability.', 'reactive programming and dependency injection such as Spring to develop REST', 'SQL, etc.) and frameworks to implement data acquisition, processing, and serving', 'reliable and then work across our team to put your ideas into action.', 'C2C is allowed for this role role.', 'attributes and/or leadership capabilities)', 'networks outside of the US, the ESPN+, D+ and the Company’s ownership stake in Hulu. The', '\uf0b7 Think of new ways to help make our platform more scalable, resilient and', 'Minimum and Preferred. Inclusive of Licenses/Certs (include functional experience as well as behavioral', 'Apache Kafka, Apache Flink; and framework such as Apache Spark. Understand', 'We are looking to hire Cloud Data Engineer for a long term Opportunity with one of our Direct client.', 'analysts to design systems which can scale elastically in ways which make', '\uf0b7 Experienced with Excel and data manipulation', '\uf0b7 The data validation involves creating a framework that validates the source data with', '\uf0b7 Prior experience building scalable platforms – handling large scale data,', 'acquisition,\xa0conformance, and validation on the cloud.', '-Focus on major areas of work, typically 20% or more of role % of Time', '\uf0b7 The data conformance involves creating a framework that use stream processing to', 'to the data space such as Spark, Kafka, and Snowflake. You’ll have plenty of', 'They must have at least 7 years of experience developing with a mix of languages (Java, Scala, Python, SQL, etc.) and frameworks to implement data acquisition, processing, and serving technologies. Experience with real-time data and scalable systems are preferred.', 'products, and experiences to consumers around the world. The Data Platform group within', 'Basic Responsibilities:', 'Dear Professionals,', 'data using python or Java. Then automate the schema generation process and maintain.', 'consume the landed data, apply transformation and sink the data to snowflake; Tools', 'weeks. It would be awesome if you have a robust portfolio on GitHub and / or open', 'Its Remote Position.', 'DTCI business stakeholders with the means to turn data into actionable insights across', 'DTCI Tech organization focuses on providing ground-breaking innovation and driving the', '\uf0b7 Experience with open source such as Spring, Kafka, Flink, Spark, Snowflake, Cassandra', '\uf0b7 Help us stay ahead of the curve by working closely with data architects,', 'Describe what the person will do in the role - how he/she will impact the organization.']",Mid-Senior level,Full-time,Business Development,Information Technology and Services,2021-03-18 14:34:51
"Software Engineer, Data Engineering - Data Platform",Foursquare,"Los Angeles, CA",1 day ago,Be among the first 25 applicants,"['', 'Write test automation, conduct code reviews, and take end-to-end ownership of deployments to production.', 'Build and run Big Data processing pipelines.', 'Focus on performance, throughput, and latency throughout our architecture.', 'Work with the Data Science team to bring machine learning models into production.', 'BS/BA in a technical field such as Computer Science or equivalent experience.', 'Professional experience in at least one of Python, Java, Scala, or Ruby', 'Responsibilities', 'Write, deploy, and monitor services for data access by systems across our infrastructure.', 'About Team', 'Qualifications', ' Comfort with Unix/Linux and the command line. Experience with CI/CD systems such as Jenkins, Travis, TeamCity, and CircleCI. Experience with containerization and orchestration systems like Docker and Kubernetes. Startup experience or experience at marketing or ad-tech data companies: RTB / real-time bidding. DSP / demand-side platform. ', 'Experience with containerization and orchestration systems like Docker and Kubernetes.', 'Nice to haves', 'Experience with CI/CD systems such as Jenkins, Travis, TeamCity, and CircleCI.', ' BS/BA in a technical field such as Computer Science or equivalent experience. 1-4 years of software development experience Professional experience in at least one of Python, Java, Scala, or Ruby Professional experience with at least one of Hadoop MapReduce and/or Spark data processing pipelines, analytics systems (e.g. OLAP, BI tools), or machine learning technologies. Strong algorithms and data structures knowledge. ', 'Startup experience or experience at marketing or ad-tech data companies: RTB / real-time bidding. DSP / demand-side platform.', '1-4 years of software development experience', 'Comfort with Unix/Linux and the command line.', ' Work with the Data Science team to bring machine learning models into production. Build and run Big Data processing pipelines. Write, deploy, and monitor services for data access by systems across our infrastructure. Focus on performance, throughput, and latency throughout our architecture. Write test automation, conduct code reviews, and take end-to-end ownership of deployments to production. ', 'Strong algorithms and data structures knowledge.', 'Professional experience with at least one of Hadoop MapReduce and/or Spark data processing pipelines, analytics systems (e.g. OLAP, BI tools), or machine learning technologies.']",Entry level,Full-time,Engineering,Marketing and Advertising,2021-03-18 14:34:51
Data Engineer,Sketchy,United States,2 weeks ago,40 applicants,"['', 'Sketchy is a TCG (The Chernin Group) portfolio company (joining other companies such as Headspace, Surfline Food52 and Crunchyroll) and a Reach Capital portfolio company (joining other start-up companies that bring a playfulness to learning.)', 'Requirements:', 'Keep our data separated and secure across national boundaries through multiple data centers', 'Experience building and optimizing ‘big data’ data pipelines, architectures and data sets', 'Create and maintain optimal data pipeline architecture', 'Innovative, high growth and collaborative culture', 'Position overview:', 'What We Offer:', 'Experience with data pipeline and workflow management tools', 'We are looking for a Data Engineer who is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The individual will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross-functional teams. You will play an integral role in helping us become more data-aware as a company and enabling data insights across our teams.', 'Build analytics tools that utilize the data pipeline to provide actionable insights', 'Authorization to work in the U.S. ', 'Create and maintain optimal data pipeline architectureIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery re-designing infrastructure for greater scalability and usabilityBuild analytics tools that utilize the data pipeline to provide actionable insightsBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sourcesKeep our data separated and secure across national boundaries through multiple data centers', 'Sketchy is an online visual learning platform that helps students effortlessly learn and recall information through a blend of art, story, spaced repetition and memory palace techniques. Sketchy was born when four medical students began creating sketched stories to distinguish and memorize similarly named viruses, as they realized that the same learning methodologies can be used across a variety of subjects.', 'Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores', 'Self-starter who is excited to be part of a growing startup company', 'Experience with relational SQL and NoSQL databases', 'Able to get into the weeds and propose and implement solutions without hand holding', 'Strong analytic skills related to working with unstructured datasets', 'Responsibilities:', 'Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery re-designing infrastructure for greater scalability and usability', 'Great Benefits including 99 % Coverage of Medical, dental, vision. Up to 4% match on 401k and more', 'Since its inception in 2013, Sketchy has become the premiere learning destination for Medical School students around the world, currently serving over 30,000 active users (or a third of the total 89,000 medical students in the United States) and an alumni base of 100,000+ students. Sketchy is creating the most engaging and effective educational service for students of higher education everywhere by combining visual storytelling with interactive learning tools that together dramatically enhance recall and knowledge acquisition.', 'Location: This role is open to remote employees in select US states: California, New York, Hawaii, Illinois, Colorado, Massachusetts, Washington and Washington, D.C.\xa0', 'Must have experience in building reports, dashboards, and/or data modeling layers in Looker, Tableau or Mode', 'Competitive compensation plan', 'Fun team events (Monthly and virtual for now)', 'Experience with GCP services', '3+ years experience in Data EngineeringAdvanced working SQL knowledge and experience working with relational databases, query authoring (SQL)Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data storesExperience building and optimizing ‘big data’ data pipelines, architectures and data setsStrong analytic skills related to working with unstructured datasetsMust have experience in building reports, dashboards, and/or data modeling layers in Looker, Tableau or ModeExperience with relational SQL and NoSQL databasesExperience with data pipeline and workflow management toolsExperience with GCP servicesExperience with object-oriented/object function scripting languagesSelf-starter who is excited to be part of a growing startup companyAble to get into the weeds and propose and implement solutions without hand holdingAuthorization to work in the U.S. ', 'Experience with object-oriented/object function scripting languages', 'Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources', 'SketchyGroup LLC is an Equal Opportunity Employer. All applicants will receive consideration without discrimination on the basis of race, religion, color, sex, age, sexual orientation, marital status, national origin, disability or any other basis prohibited by applicable law.', '3+ years experience in Data Engineering', 'Competitive compensation planInnovative, high growth and collaborative cultureGenerous PTO package with floating holidayFun team events (Monthly and virtual for now)Great Benefits including 99 % Coverage of Medical, dental, vision. Up to 4% match on 401k and more', 'Education:', 'Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL)', 'Generous PTO package with floating holiday', ""Bachelor's degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field (Master's degree in a similar field preferred but not required)""]",Mid-Senior level,Full-time,Information Technology,Higher Education,2021-03-18 14:34:51
Data Engineer - Remote - New York,Provision People,"New York, NY",2 weeks ago,Be among the first 25 applicants,"['', 'Construct robust scalable pipelines using Databricks and AWS Lambda, Batch to deploy applications in python / PySpark.', 'Integrate new software tools for data analysis into the existing toolset.', 'Work closely with Data Scientists to build data pipelines to onboard large structured and unstructured data.Construct robust scalable pipelines using Databricks and AWS Lambda, Batch to deploy applications in python / PySpark.Support and maintain existing production processes on Airflow and onboard new processes.Integrate new software tools for data analysis into the existing toolset.Collaborate with the team on quick evaluation of new data sources by assisting with transfer and processing of data.Identify and download public datasets like COVID tracking, temperature, census data to be used by the team for analysis.', 'Must have Data analytics & pipeline experiences with Python, and Databricks/Spark', 'Collaborate with the team on quick evaluation of new data sources by assisting with transfer and processing of data.', 'Required Skills And Experience', 'Experiences with Data Science languages such as Python, R, Scala.', 'Bachelor-degree or higher in Computer Science, Data Science, or Engineering', 'Responsibilities', 'Familiar with Linux administration (bash, network, file systems)', 'Must have AWS Lambda, Batch, Serverless, ECR experiences.', 'Experience in analyzing and crafting efficient algorithms', 'Strongly disciplined approach to software development', 'Summary', 'Identify and download public datasets like COVID tracking, temperature, census data to be used by the team for analysis.', 'Must have experience with Airflow, Git, Docker / Kubernetes or similar.', 'A team-player who is eager to learn with strong analytical and communications skills', 'Nice to have experiences with NoSQL technologies such as MongoDB, and Cassandra.', 'Work closely with Data Scientists to build data pipelines to onboard large structured and unstructured data.', 'Experience with cleaning, aggregating, and pre-processing data from various sources.', 'Experience working within an Agile software development framework', 'Working experiences in Model CI/CD such as AWS CodeDeploy, CodeCommit, SageMaker, or Azure Machine Learning.', 'Bachelor-degree or higher in Computer Science, Data Science, or EngineeringMust have Data analytics & pipeline experiences with Python, and Databricks/SparkMust have AWS Lambda, Batch, Serverless, ECR experiences.Must have experience with Airflow, Git, Docker / Kubernetes or similar.Experiences with Data Science languages such as Python, R, Scala.Working experiences in SQL, or Snowflake (nice to have).Working experiences in Model CI/CD such as AWS CodeDeploy, CodeCommit, SageMaker, or Azure Machine Learning.Deep understanding of algorithms and algorithmic complexityExperience in analyzing and crafting efficient algorithmsNice to have experiences with NoSQL technologies such as MongoDB, and Cassandra.Experience with cleaning, aggregating, and pre-processing data from various sources.Familiar with Linux administration (bash, network, file systems)Experience working within an Agile software development frameworkStrongly disciplined approach to software developmentA team-player who is eager to learn with strong analytical and communications skills', 'Working experiences in SQL, or Snowflake (nice to have).', 'Support and maintain existing production processes on Airflow and onboard new processes.', 'Deep understanding of algorithms and algorithmic complexity']",Entry level,Full-time,Information Technology,Construction,2021-03-18 14:34:51
Data Engineer,NOVEL,"Melbourne, FL",4 weeks ago,Be among the first 25 applicants,"['', 'Knowledge of Delta Lake', 'Novel Engineering offers a competitive employment package. This includes competitive compensation, vacation, dental, vision, employee profit sharing plan, flexible schedules, and a professional yet relaxed culture that gives you the opportunity to work in a team-oriented environment, innovate with co-workers, and thrive as an individual.', 'Position is subject to pre-employment drug and random drug and alcohol testing.', 'Fluent and experienced with Linux OS (and some bash scripting knowledge)', ""Bachelor's in computer science computer engineering or other related fields3+ years’ experience in data engineering and software developmentMust Have: Strong programming skills (python) - solid fundamentals of computer science such as data structures, time/space complexity analysisMust Have: Hands-on experience with Apache Spark (Pyspark)Must Have: Hands-on experience with Apache CassandraMust Have: Hands-on experience with Apache KafkaExperience with file formats such as Parquet and AVROKnowledge of Delta LakeFluent and experienced with Linux OS (and some bash scripting knowledge)Previous experience with AWS tools such as S3, EMR, EC2, DynamoDBGood understanding about CRM data (e.g., Salesforce), the objects, and the relationships among them Position is subject to pre-employment drug and random drug and alcohol testing."", 'Job Description', 'Must Have: Strong programming skills (python) - solid fundamentals of computer science such as data structures, time/space complexity analysis', 'Experience with file formats such as Parquet and AVRO', 'Good understanding about CRM data (e.g., Salesforce), the objects, and the relationships among them ', 'Must Have: Hands-on experience with Apache Cassandra', 'Company Overview', 'Must Have: Hands-on experience with Apache Kafka', 'Must Have: Hands-on experience with Apache Spark (Pyspark)', '3+ years’ experience in data engineering and software development', 'Benefits And Perks', ""Bachelor's in computer science computer engineering or other related fields"", 'Previous experience with AWS tools such as S3, EMR, EC2, DynamoDB']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,The Climate Corporation,"San Francisco, CA",1 week ago,69 applicants,"['', 'Design and develop new systems and tools to enable folks to consume and understand data faster', 'Inspire one another', 'Work closely with other departments to gather new data and leverage existing data to make our products better for us and our users', '2+ years of scripting experience', 'Superb medical, dental, vision, life, disability benefits, and a 401k matching program', 'Develop infrastructure to inform on key metrics, recommend changes and predict future results', 'Rapidly prototype new analytics views and work directly with stakeholders across multiple functions (Science, Marketing, Sales, Risk, Finance, Product)', 'Schema design in data warehouses, working directly with SQL to profile data, generate analytics', 'A stocked kitchen with a large assortment of snacks & drinks to get you through the day', '2+ years of experience with dimensional data modeling & schema design in Data Warehouses.2+ years of scripting experienceExperience with massive scale relational databases (MPP) is a big plus (Vertica / Redshift / Teradata / MemSQL).Experience working in a cloud deployment such as AWS is a plusExcellent communication skills including the ability to identify and communicate data driven insights', 'Encouragement to get out of the office and into the field with agents and farmers to see first-hand how our products are being used', 'Find the possible in the impossible', 'What We Offer', 'Inspire one anotherInnovate in all we doLeave a mark on the worldFind the possible in the impossibleBe direct and transparent', 'Excellent communication skills including the ability to identify and communicate data driven insights', 'Learn More About Our Team And Our Mission', 'Experience with relational databases or NoSql', 'Position Overview', 'Performing shell scripting in a Linux/Unix environment', 'B.S. or B.A. in computer science, math, economics, engineering or other technical field2+ years of SQL and dynamic or static programming languages experience as applied to ETL/ELT tools (Informatics, Kettle, Talend, etc.)Performing shell scripting in a Linux/Unix environmentPerforming dimensional data modelingSchema design in data warehouses, working directly with SQL to profile data, generate analyticsDevelop infrastructure in AWS Cloud environment to inform on key metrics, recommend changesExperience with relational databases or NoSqlExperience in developing models / explores / dashboards in LookerApplicant must be willing to provide 24x7 on call support one week per month', 'Champion data warehousing best practices', 'Develop infrastructure in AWS Cloud environment to inform on key metrics, recommend changes', 'Help design and build a Business Analytics Warehouse. Build and maintain the core data model, ETL / ELT, core data metrics and data quality.Rapidly prototype new analytics views and work directly with stakeholders across multiple functions (Science, Marketing, Sales, Risk, Finance, Product)Champion data warehousing best practicesBuild systems to answer business questions in a timely fashion and expand our product featuresArchitect, build and launch new data models that provide intuitive analytics to business usersDevelop infrastructure to inform on key metrics, recommend changes and predict future resultsWork closely with other departments to gather new data and leverage existing data to make our products better for us and our usersBuild data expertise and own data quality for the pipelines you buildDesign and develop new systems and tools to enable folks to consume and understand data fasterProvide expert advice and education in the usage and interpretation of data systems to the business users', 'B.S. or B.A. in computer science, math, economics, engineering or other technical field', 'Applicant must be willing to provide 24x7 on call support one week per month', 'Build data expertise and own data quality for the pipelines you build', 'Performing dimensional data modeling', 'Provide expert advice and education in the usage and interpretation of data systems to the business users', 'Basic Qualifications', 'Innovate in all we do', 'Leave a mark on the world', '2+ years of SQL and dynamic or static programming languages experience as applied to ETL/ELT tools (Informatics, Kettle, Talend, etc.)', 'Build systems to answer business questions in a timely fashion and expand our product features', 'What You Will Do', 'Superb medical, dental, vision, life, disability benefits, and a 401k matching programA stocked kitchen with a large assortment of snacks & drinks to get you through the dayEncouragement to get out of the office and into the field with agents and farmers to see first-hand how our products are being usedWe take part and offer various workshops, conferences, meet-up groups, tech-talks, and hack-a-thons to encourage participation and growth in both community involvement and career development', 'Architect, build and launch new data models that provide intuitive analytics to business users', '2+ years of experience with dimensional data modeling & schema design in Data Warehouses.', 'We take part and offer various workshops, conferences, meet-up groups, tech-talks, and hack-a-thons to encourage participation and growth in both community involvement and career development', 'Experience in developing models / explores / dashboards in Looker', 'Be direct and transparent', 'Help design and build a Business Analytics Warehouse. Build and maintain the core data model, ETL / ELT, core data metrics and data quality.', 'Preferred Qualifications', 'Experience with massive scale relational databases (MPP) is a big plus (Vertica / Redshift / Teradata / MemSQL).', 'Experience working in a cloud deployment such as AWS is a plus']",Not Applicable,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Engineer (remote),Solugenix,"Irvine, CA",6 days ago,100 applicants,"['', 'Data Engineer - direct hire opportunity within healthcare insurance industryIrvine, CAJob ID 2021-5137', 'Build product-focused datasets and scalable, fault-tolerant pipelines.', 'Develop cloud-based data repositories to store structured and non-structured data (x-rays).', 'Capture clinician’s feedback and store it on a cloud repository.', 'Responsibilities', 'Bachelor’s degree in Computer Science or Engineering; Master’s degree a plus.', 'Experience with SOA applications and cloud-based services.', 'Define and own the data engineering roadmap for our Artificial Intelligence and Data Science initiatives.Partner with the Data Scientist, Web Developer, and Data Warehouse team to render structure and non-structured data through the web for clinicians to evaluate.Capture clinician’s feedback and store it on a cloud repository.Design and manage data pipelines between on-prem and cloud repositories.Advanced working SQL knowledge and experience working with relational databases.Build product-focused datasets and scalable, fault-tolerant pipelines.Able to immerse yourself in all aspects of the product, understand the problems, and tie them back to data engineering solutions.Recommend improvements and modifications to exist data and ETL pipelines.Communicate and influence strategies and processes around data modeling and architecture to multi-functional groups and leadership.Drive internal process improvements and automating manual processes for data quality and SLA management.Strong analytic skills related to working with structured and unstructured datasets.Experience with data pipeline and workflow management tools.Manage cloud data pipelines, ELT/ETL cloud processes for AI/ML, and Power BI reportingExperience with Python a huge plus.Develop cloud-based data repositories to store structured and non-structured data (x-rays).Control cloud environments in accordance with company security guidelines (Ex. HIPAA/HITECH).', 'Design and manage data pipelines between on-prem and cloud repositories.', 'Experience with open-source technology such as Python and R.', 'Qualifications', 'Manage cloud data pipelines, ELT/ETL cloud processes for AI/ML, and Power BI reporting', 'Experience with data pipeline and workflow management tools.', 'Communicate and influence strategies and processes around data modeling and architecture to multi-functional groups and leadership.', 'Partner with the Data Scientist, Web Developer, and Data Warehouse team to render structure and non-structured data through the web for clinicians to evaluate.', 'Recommend improvements and modifications to exist data and ETL pipelines.', 'Advanced working SQL knowledge and experience working with relational databases.', 'Strong analytic skills related to working with structured and unstructured datasets.', 'Experience with Python a huge plus.', 'Define and own the data engineering roadmap for our Artificial Intelligence and Data Science initiatives.', 'Control cloud environments in accordance with company security guidelines (Ex. HIPAA/HITECH).', 'Database experience, including knowledge of SQL and NoSQL, and related data stores such as Postgres.', 'Strong awareness of networking and internet protocols, including TCP/IP, DNS, SMTP, HTTP, and distributed networks.', 'Bachelor’s degree in Computer Science or Engineering; Master’s degree a plus.Certification as an AWS, Azure, or Google Solutions Architect. Cloud Security Administrator a big plus.3-5 years of experience in a Cloud Data Engineer role or related position.Experience with SOA applications and cloud-based services.Experience with open-source technology such as Python and R.Knowledge of web-services, API, REST, and RPC.Database experience, including knowledge of SQL and NoSQL, and related data stores such as Postgres.Strong awareness of networking and internet protocols, including TCP/IP, DNS, SMTP, HTTP, and distributed networks.Solid understanding of data flows through web APIs and web services.Experience with Source Control and DevOps is highly desired.', 'Knowledge of web-services, API, REST, and RPC.', 'Drive internal process improvements and automating manual processes for data quality and SLA management.', 'Certification as an AWS, Azure, or Google Solutions Architect. Cloud Security Administrator a big plus.', '3-5 years of experience in a Cloud Data Engineer role or related position.', 'Solid understanding of data flows through web APIs and web services.', 'Experience with Source Control and DevOps is highly desired.', 'Able to immerse yourself in all aspects of the product, understand the problems, and tie them back to data engineering solutions.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer/ETL Developer ,Total Brain,"San Francisco, CA",3 days ago,Over 200 applicants,"['', 'Coding proficiency in at least one programming language (Python, Ruby, Java, etc)', 'Competitive benefits (options for individual and/or family medical, dental and vision plans, commuter benefits, medical & dependant flexible spending accounts)', 'The below values define how we roll:', 'Amidst the social chaos created by COVID, our company is growing and doubling its headcount in 2021.\xa0', 'Data analysis skills; database query construction, data warehousing, data modeling, and experience in business analysis', 'Our clients are enterprise employers, large consumer groups and mental health/addiction clinics.', '0-2 years of experience in a BI environment with data modeling, data warehousing, and building ETL pipelines\xa0', 'Unlimited Vacation Time', 'Dispassion: we let go of what we don’t control and we make time to create energy for ourselves', 'Enable business users to make data-driven decisions by building upon our data & analytics platform', 'Total Brain is a digital neurotechnology. We leverage digital technology, neuroscience and biometrics to help individuals monitor and support their brain and mental health.\xa0', 'Experience working with AWS big data technologies (EMR, Redshift, S3)', 'Translate business requirements into technical specifications to design, build and deploy various Business Intelligence solutions', 'What You’ll Do:\xa0\xa0', ""Bachelor's or Master's degree in Computer Science, Information Technology, Engineering, or related field\xa0"", 'What you’ll need:\xa0', 'Passion: we are 100% present - 100% of the time', 'Coding proficiency in at least one programming language (Python, Ruby, Java, etc)Experience working with AWS big data technologies (EMR, Redshift, S3)', 'Regular science education sessions, and all hands meetings\xa0', 'Total Brain was founded by Dr Evian Gordon, Md, PhD in 2000. It is headquartered in San Francisco but staff are largely decentralized across the US and Sydney Australia.', ""0-2 years of experience in a BI environment with data modeling, data warehousing, and building ETL pipelines\xa0Bachelor's or Master's degree in Computer Science, Information Technology, Engineering, or related field\xa0Ability to write SQL queriesData analysis skills; database query construction, data warehousing, data modeling, and experience in business analysis"", 'Two Company Wide Offsite Retreats Per Year (once travel restrictions have lifted)', 'Competitive Compensation Package', 'Full access to the Total Brain platform for you and your loved ones', '401(k) Plan', 'Compassion: we deeply understand and help every stakeholderPassion: we are 100% present - 100% of the timeDispassion: we let go of what we don’t control and we make time to create energy for ourselves', 'Analyze and interpret data, identify the gaps, and provide reports, solutions and recommendations', 'Starting with our many source systems (S3,Hubspot,Amplitude,etc.), build data pipelines utilizing Matillion to bring scalable, repeatable data sets into our data warehouse environment (AWS/Redshift/Power BI)', 'Ability to write SQL queries', 'Compassion: we deeply understand and help every stakeholder', 'Employee Stock Options', 'Maternity and Parental paid leave', 'Competitive Compensation PackageEmployee Stock OptionsA flexible, collaborative culture where growth from within is encouragedUnlimited Vacation TimeCompetitive benefits (options for individual and/or family medical, dental and vision plans, commuter benefits, medical & dependant flexible spending accounts)401(k) PlanTwo Company Wide Offsite Retreats Per Year (once travel restrictions have lifted)Regular science education sessions, and all hands meetings\xa0Maternity and Parental paid leaveFull access to the Total Brain platform for you and your loved ones', '\ufeffTotal Brain offers:', 'Bonus Points:', 'Assist other team members in developing queries and visualizations for ad-hoc requests and projects, as well as ongoing BI reporting', 'Starting with our many source systems (S3,Hubspot,Amplitude,etc.), build data pipelines utilizing Matillion to bring scalable, repeatable data sets into our data warehouse environment (AWS/Redshift/Power BI)Translate business requirements into technical specifications to design, build and deploy various Business Intelligence solutionsAssist other team members in developing queries and visualizations for ad-hoc requests and projects, as well as ongoing BI reportingAnalyze and interpret data, identify the gaps, and provide reports, solutions and recommendationsEnable business users to make data-driven decisions by building upon our data & analytics platform', 'A flexible, collaborative culture where growth from within is encouraged']",Mid-Senior level,Full-time,Engineering,"Health, Wellness and Fitness",2021-03-18 14:34:51
Data Engineer L5,Luxoft,United States,2 days ago,Be among the first 25 applicants,"['', ' Work closely with Project Manager/Scrum master to develop and update the task plan for respective work.', 'Languages', ' Develop the task plan for ETL activities', ' Provider Credentialing (Dental & Vision)', ' Assist in the ongoing development of technical best practices for data extraction from source systems/source files, developing transformation routines, data quality checks, data cleansing, and other ETL-‐related activities', ' Lead Unit and Integration Testing activities and assist in User Acceptance Testing as needed', ' Consistently work independently and in a team-oriented, collaborative environment. ', ' Provide technical expertise of Extract/Transform/Load (ETL) best practices and development.', 'Project Skills', 'ETL concepts, preferably Informatica, SSIS, and SSRS', ' Provide accurate time estimates and status reports to project management as required.', ' Perform checkpoints/reviews on presentation layer designs and code being created by self and other ETL developers', 'Skills', ' Work on Informatica, MS SQL Server, T-SQL, SSIS, SSRS tools and environments', ' Articulate and communicate any critical task issues and dependencies on other teams', 'Responsibilities', ' Technical Skills: Assist in the ongoing development of technical best practices for data extraction from source systems/source files, developing transformation routines, data quality checks, data cleansing, and other ETL-‐related activities Provide technical expertise of Extract/Transform/Load (ETL) best practices and development. ETL development and to supervise and guide ETL development activities of other developers Design and implements best practices, tuning, and optimization for batch and intraday data loads Perform checkpoints/reviews on presentation layer designs and code being created by self and other ETL developers Work with database administrators, network administrators, systems analysts, and software engineers to help resolve problems with ETL and data integration service applications. Lead Unit and Integration Testing activities and assist in User Acceptance Testing as needed Work on Informatica, MS SQL Server, T-SQL, SSIS, SSRS tools and environmentsProject Skills Work closely with Business users, Data Architect, Data Analysts, and BI/Reporting Lead to ensure that the end to end designs meet the business and data requirements Work closely with Project Manager/Scrum master to develop and update the task plan for respective work. Articulate and communicate any critical task issues and dependencies on other teams Develop the task plan for ETL activities Assist in the preparation and documentation of software requirements and specifications related to the data warehouse and other data integration services. Provide accurate time estimates and status reports to project management as required. Consistently work independently and in a team-oriented, collaborative environment. SkillsMust have o 10+ Years of IT industry experience, with 5+ years of experience as ETL Architect or data and reporting related projects.Relational database concepts; Strong experience in SQL Server and related technologies.ETL concepts, preferably Informatica, SSIS, and SSRSUnderstanding of data modeling tools -- preferably Erwin.Strong RDBMS knowledge.Significant experience interfacing with both customers and management. Nice to have Health Care Insurance domain knowledge, preferably with an insurance carrier.HIPAA and other regulations that govern the access of data LanguagesEnglish: B2 Upper Intermediate', ' Health Care Insurance domain knowledge, preferably with an insurance carrier.HIPAA and other regulations that govern the access of data ', ' o 10+ Years of IT industry experience, with 5+ years of experience as ETL Architect or data and reporting related projects.Relational database concepts; Strong experience in SQL Server and related technologies.ETL concepts, preferably Informatica, SSIS, and SSRSUnderstanding of data modeling tools -- preferably Erwin.Strong RDBMS knowledge.Significant experience interfacing with both customers and management. Nice to have Health Care Insurance domain knowledge, preferably with an insurance carrier.HIPAA and other regulations that govern the access of data LanguagesEnglish: B2 Upper Intermediate', ' Claims', 'Significant experience interfacing with both customers and management. ', 'Project Description', ' ETL development and to supervise and guide ETL development activities of other developers', ' Work with database administrators, network administrators, systems analysts, and software engineers to help resolve problems with ETL and data integration service applications.', ' Membership/Enrollment', 'Strong RDBMS knowledge.', ' Customer Complaints/Appeals/Grievances', ' Assist in the preparation and documentation of software requirements and specifications related to the data warehouse and other data integration services.', ' Design and implements best practices, tuning, and optimization for batch and intraday data loads', ' Call Center', 'Understanding of data modeling tools -- preferably Erwin.', 'Relational database concepts; Strong experience in SQL Server and related technologies.', ' Work closely with Business users, Data Architect, Data Analysts, and BI/Reporting Lead to ensure that the end to end designs meet the business and data requirements']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Kokua Education,United States,1 week ago,36 applicants,"['', 'This role has the option to be remote.\xa0', 'BS or MS degree in Computer Science or a related technical field', 'Kokua’s mission is to empower students by matching schools with passionate guest educators who maximize substitute teaching time. By the end of high school, teacher absences result in an average of 1+ years of lost learning time, particularly in underserved communities. We solve this problem by placing dedicated substitutes (or Guest Teachers) into classrooms across the United States. We believe that proximity is power, and that one role model can change a student’s life forever. We are a driven team looking to make a lasting impact for future generations. As the world adjusts to new ways of life, inspiring role models are as important as ever, and we’re looking for a new team member to drive this mission forward.', 'EXPERIENCE', 'OUR COMPANY', 'The ideal candidate must be a self-starter willing to proactively diagnose and troubleshoot performance and growth opportunities rather than being asked to take action. You have strong communication skills with a collaborative mindset. You are analytical, highly organized and are an innovative problem solver. You are passionate about serving children, particularly those growing up in marginalized environments. You are a team player, where no task is too small.', 'Expert in Microsoft office', 'Work with vertical leads to develop and maintain datasets.', 'BS or MS degree in Computer Science or a related technical field3-5 years in a data engineer role or similar roleExperience in data infrastructure design and implementation.Experience in managing and communicating data warehouse plans to internal personnel.Experience maintaining data processing systems.Skills in server reporting services (i.e. Tableau, Google drive, etc.), business platforms, (i.e. Salesforce, Greenhouse, Frontline), integration services, or any other data visualization tools.Expert in Microsoft office', 'Work closely with all business verticals to develop strategy for long term data platform architecture.', 'Implement processes and systems to monitor data quality, ensuring data is always accurate and available for all business units and processes that depend on it.', 'Design data integrations for current business platforms (Paylocity, Greenhouse, Salesforce, Frontline).', 'Skills in server reporting services (i.e. Tableau, Google drive, etc.), business platforms, (i.e. Salesforce, Greenhouse, Frontline), integration services, or any other data visualization tools.', 'Kokua began in Chicago in 2011 and has since grown across the country. We w ere recognized by Forbes in 2016 as an up-and-coming organization making an impact in the field of education. We have exciting growth plans ahead, and this is where you come in.', 'YOUR ROLE', 'Identify ways to improve data reliability, efficiency, and quality.', 'Experience in managing and communicating data warehouse plans to internal personnel.', 'Perform a complete assessment of the current data infrastructure and business platforms.Design and implement a new data infrastructure that aligns with business requirements and allows for scalability.Design data integrations for current business platforms (Paylocity, Greenhouse, Salesforce, Frontline).Implement processes and systems to monitor data quality, ensuring data is always accurate and available for all business units and processes that depend on it.Identify ways to improve data reliability, efficiency, and quality.Work closely with all business verticals to develop strategy for long term data platform architecture.Perform data analysis required to troubleshoot data related issues and assist in the resolution of data issues.Work with vertical leads to develop and maintain datasets.', 'RESPONSIBILITIES', 'Design and implement a new data infrastructure that aligns with business requirements and allows for scalability.', 'Attractive base salary and bonus with generous benefits, commensurate or above same-level positions in education. 4 weeks of paid holidays, in addition to 7 PTO days. Position provides autonomy and a unique opportunity to scale an impact-driven organization amongst a close-knit team. Play an integral role on a team that is on a mission to help empower children so they can realize their vast potential.', 'Perform a complete assessment of the current data infrastructure and business platforms.', '3-5 years in a data engineer role or similar role', 'Experience maintaining data processing systems.', 'If you are interested in the role, or have questions about it, please reach out to Mical at mical@kokuaed.com with “Data Engineer” in the subject line.', 'PERKS AND PAY', 'Experience in data infrastructure design and implementation.', 'Kokua is looking for a driven, strategic, and innovative data engineer to design and implement our company’s complete data infrastructure. You will assess our current data environment and work collaboratively with various verticals within the company to identify the ideal data infrastructure that suits our business operations and needs. In addition to design and implementation, you will be asked to manage, maintain, and troubleshoot all items related to our data architecture. You are data-driven, growth-focused, and have experience in consolidating data from multiple business platforms into a centralized data warehouse. You will develop and optimize a data infrastructure that will allow for Kokua to achieve its growth potential and execute its mission of bringing communities together to empower children, particularly those growing up in marginalized communities.', 'Perform data analysis required to troubleshoot data related issues and assist in the resolution of data issues.']",Not Applicable,Full-time,Analyst,Education Management,2021-03-18 14:34:51
Data Engineer,Believe,"Brooklyn, NY",6 days ago,Be among the first 25 applicants,"['', 'Be able to apply their skills in ingenious ways to solve new and existing problems.', 'Be highly proficient in SQL & Python with an ability to work with large and highly complex datasetsHave a thorough understanding of building data pipelines for RDBMS systems and various cloud-based data sources.Be comfortable working in a complex data environment involving in-depth data discovery, exploration and troubleshooting.Be able to apply their skills in ingenious ways to solve new and existing problems.Be open to exploring and learning new technologies on the go.Have the desire to lead and own the data engineering component of our new enterprise cloud-based data architecture.Be comfortable working in a cross-functional responsibility.Ensure adequate and thorough documentation of all existing and new processes.', 'Comprehensive knowledge of data-modeling principles specifically with a focus on data analytics', 'Ensure adequate and thorough documentation of all existing and new processes.', 'In-depth knowledge of RDBMS systems such as MySQL, MariaDB, PostgreSQL as well as cloud-based data warehousing solutions such as RedShift & Snowflake.', 'Strong ETL proficiency using GUI-based tools or code-based patterns.', 'Be comfortable working in a complex data environment involving in-depth data discovery, exploration and troubleshooting.', 'Be open to exploring and learning new technologies on the go.', 'Snowflake', 'Matillion / Fivetran / MDM', 'Experience with software engineering practicesMatillion / Fivetran / MDMAWSSnowflakeTableau', 'Be highly proficient in SQL & Python with an ability to work with large and highly complex datasets', 'Qualifications', 'Have a thorough understanding of building data pipelines for RDBMS systems and various cloud-based data sources.', 'Highly Desired Skills', 'Qualifications: ', 'Must have at least 5 years of demonstrable work experience as a Data Engineer or Data Architect in an enterprise environment.In-depth knowledge of RDBMS systems such as MySQL, MariaDB, PostgreSQL as well as cloud-based data warehousing solutions such as RedShift & Snowflake.Comprehensive knowledge of data-modeling principles specifically with a focus on data analyticsStrong ETL proficiency using GUI-based tools or code-based patterns.Excellent communication and interpersonal skills with the ability to liaise with multiple teams.', 'Be comfortable working in a cross-functional responsibility.', 'TuneCore is seeking a seasoned Data Engineer who thrives in a dynamic and fast-paced environment and is able to work alongside our Software Engineering, Dev-Ops and Data Analytics teams to provide impactful solutions for all our data-driven business needs. ', 'Excellent communication and interpersonal skills with the ability to liaise with multiple teams.', 'AWS', 'Job Description', 'Candidates Should', 'Experience with software engineering practices', 'Tableau', 'Have the desire to lead and own the data engineering component of our new enterprise cloud-based data architecture.', 'Must have at least 5 years of demonstrable work experience as a Data Engineer or Data Architect in an enterprise environment.']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
AWS Cloud Data Engineer,HTC Global Services,"Boston, MA",5 hours ago,Be among the first 25 applicants,"['', 'Requires working knowledge and experience in own job discipline and broadens capabilitiesContinues to build knowledge of the company, processes and customersPerforms a range of assignments related to job disciplineUses prescribed guidelines or policies in analyzing situationsReceives a moderate level of guidance and direction', 'Work collaboratively with Software Engineers to analyze, develop, implement, support, and continuously evolve software-defined infrastructure, application patterns, software, and technology solutions in Cloud environments to govern, monitor, and secure the cloud environments and data for the company. Builds repeatable solutions working within and between engineering and operations teams to identify and implement process improvements.', 'Troubleshoots issues with tools, processes, applications or infrastructure', 'Develops, builds, deploys and maintains cloud platform solutions, primary in the areas of software-defined infrastructureEngages with engineering and operations teams throughout the life cycle to help develop software for reliabilityTroubleshoots issues with tools, processes, applications or infrastructureEvaluate current development and operations procedures, recommend and implement tools and practices to increase efficiency, reliability and repeatabilityConducts operational reviews of cloud architecture with the goal of automating and improving the existing and future environmentsConfigure, deploy, and manage services and applications; troubleshoots systems', 'Performs a range of assignments related to job discipline', 'Primary Accountabilities', 'Level Summary', 'Engages with engineering and operations teams throughout the life cycle to help develop software for reliability', 'Position Summary', 'Senior Cloud (AWS) Data Engineer', 'Continues to build knowledge of the company, processes and customers', 'Evaluate current development and operations procedures, recommend and implement tools and practices to increase efficiency, reliability and repeatability', 'Develops, builds, deploys and maintains cloud platform solutions, primary in the areas of software-defined infrastructure', 'Requires working knowledge and experience in own job discipline and broadens capabilities', 'Conducts operational reviews of cloud architecture with the goal of automating and improving the existing and future environments', 'Configure, deploy, and manage services and applications; troubleshoots systems', 'Receives a moderate level of guidance and direction', 'Uses prescribed guidelines or policies in analyzing situations', 'Full-time hire, on-site role']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,L&T Technology Services Limited,"Ridgefield, NJ",3 weeks ago,Over 200 applicants,"['\xa0Collaborate with data architects, modelers and IT team members on project goals.', '\xa0Build high-performance algorithms, prototypes, predictive models and proof of concepts.', '\xa0Ability to work as part of a team, as well as work independently or with minimal direction.', '\xa0Excellent written, presentation, and verbal communication skills.', 'The main function of the Data Engineer is to develop, evaluate, test and maintain architectures and data solutions within our organization. The typical Data Engineer executes plans, policies, and practices that control, protect, deliver, and enhance the value of the organization s data assets.', '\xa0Design, implement, automate and maintain large scale enterprise data ETL processes.', '\xa0Strong PC skills including knowledge of Microsoft SharePoint', 'Job Responsibilities:', '\xa0Ensure systems meet business requirements and industry practices.', '-> PYTHON, SHELL, SQL', 'Skills:', '\xa0Design, construct, install, test and maintain highly scalable data management systems.']",Mid-Senior level,Full-time,Engineering,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Infosys,"Phoenix, AZ",20 hours ago,Be among the first 25 applicants,"['', 'Country', ' Experience in preparing test scripts and test cases to validate data and maintaining data quality ', ' Experience with Lean / Agile development methodologies ', ' Good expertise in impact analysis due to changes or issues ', 'About Us', ' 1+ year of experience in software development life cycle  1+ years of experience in Project life cycle activities on development and maintenance projects  Good experience in end-to-end implementation of DW BI projects, especially in data warehouse and mart developments  Good understanding of Data integration and Data Quality  Experience to Big data technologies is preferred.  Good expertise in impact analysis due to changes or issues  Experience in preparing test scripts and test cases to validate data and maintaining data quality  Strong understanding and hands-on programming/scripting experience skills – UNIX shell, Perl, and JavaScript  Hands-on development, with a willingness to troubleshoot and solve complex problems  CI / CD exposure  Ability to work in team in diverse/ multiple stakeholder environment  Ability to communicate complex technology solutions to diverse teams namely, technical, business and management teams  Excellent verbal and written communication skills  Experience and desire to work in a Global delivery environment ', ' Candidate must be located within commuting distance of Phoenix, AZ or be willing to relocate to the area. This position may require travel in the US and Canada.  Bachelor’s Degree or foreign equivalent, will consider work experience in lieu of a degree  1+ years of experience with Information Technology  1+ years of experience in Big data and cloud technologies, solution design and documentation  Good experience in end-to-end implementation of DW BI projects, especially in data warehouse and mart developments  Python is a huge plus  Strong knowledge and hands-on experience in SQL, Unix shell scripting  Knowledge and experience with full SDLC lifecycle  Experience with Lean / Agile development methodologies  U.S. Citizenship or Permanent Residency required, we are not able to sponsor at this time ', ' 1+ years of experience in Big data and cloud technologies, solution design and documentation ', ' Experience to Big data technologies is preferred. ', 'This position may require travel in the US and Canada.', ' 1+ years of experience with Information Technology ', ' Experience and desire to work in a Global delivery environment ', 'Work Location', ' Ability to work in team in diverse/ multiple stakeholder environment ', ' EOE/Minority/Female/Veteran/Disabled/Sexual Orientation/Gender Identity/National Origin ', 'Interest Group', 'Phoenix, AZ', ' Knowledge and experience with full SDLC lifecycle ', ' CI / CD exposure ', ' Good experience in end-to-end implementation of DW BI projects, especially in data warehouse and mart developments ', 'Domain', ' Hands-on development, with a willingness to troubleshoot and solve complex problems ', ' Data Engineer ', ' 1+ years of experience in Project life cycle activities on development and maintenance projects ', 'Company', ' Python is a huge plus ', 'Skillset', ' The job may entail extensive travel. The job may also entail sitting as well as working at a computer for extended periods of time. Candidates should be able to effectively communicate by telephone, email, and face to face. ', 'Required Qualifications', ' Ability to communicate complex technology solutions to diverse teams namely, technical, business and management teams ', ' 1+ year of experience in software development life cycle ', ' Excellent verbal and written communication skills ', ' Strong understanding and hands-on programming/scripting experience skills – UNIX shell, Perl, and JavaScript ', 'Job Description', ' Good understanding of Data integration and Data Quality ', ' U.S. Citizenship or Permanent Residency required, we are not able to sponsor at this time ', ' Candidate must be located within commuting distance of Phoenix, AZ or be willing to relocate to the area. This position may require travel in the US and Canada. ', ' Bachelor’s Degree or foreign equivalent, will consider work experience in lieu of a degree ', 'Preferred Qualifications', ' Strong knowledge and hands-on experience in SQL, Unix shell scripting ', 'State / Region / Province']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Sky Solutions,"Philadelphia, PA",4 days ago,Be among the first 25 applicants,"['', 'Job Description']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Compunnel ,"Westbrook, ME",4 days ago,Be among the first 25 applicants,[''],Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer (Remote),ID.me,"McLean, VA",2 weeks ago,55 applicants,"['', 'Collaborate closely with the engineering and devops team to implement DataOps, thus reducing our analytics development cycle', 'Train and educate team members as well as stakeholders about best practices in data engineering and governance', 'Vision', 'Ability to work independently and autonomously, as well as part of a team', 'Demonstrate a passion for serving the needs of internal and external customers by enabling them with self-service reporting tools and analytics capabilities', ' 3+ years of hands-on experience in data engineering for a SaaS company or a mature startup Proven experience working with various tools but more importantly, familiarity with how to best assemble and deploy production ready data stack to any cloud environment BS in a quantitative or scientific field such as computer science, computer engineering or equivalent experience Experience in applying agile software development approach - Git, CI/CD, Jira, etc - to data engineering Familiarity with popular programming languages (such as Ruby, Python, .NET, etc) Exceptional fluency with SQL; you conquered the join venn diagram long ago and have moved on to explaining cost based optimization to your peers on the engineering team Some level of experience working in the cloud - AWS, Azure, or GCP Experience with ingesting, processing, and visualizing data sources of varying types - structured/relational and unstructured Experience in developing, managing, and manipulating large, complex datasets Data-driven, detail-oriented individual with excellent storytelling and problem-solving abilities Ability to work independently and autonomously, as well as part of a team Superb time management, prioritization of tasks and ability to meet deadlines with little supervision Must be located in the continental U.S. ', 'Experience in applying agile software development approach - Git, CI/CD, Jira, etc - to data engineering', 'Develop, prototype, and build frameworks based on open source and commercially available tools', 'Responsibilities', 'Develop technical solutions using proven techniques in data and analytics processes', 'Exceptional fluency with SQL; you conquered the join venn diagram long ago and have moved on to explaining cost based optimization to your peers on the engineering team', ' Develop technical solutions using proven techniques in data and analytics processes Develop, prototype, and build frameworks based on open source and commercially available tools Orchestrate and maintain data pipelines that meet security standards and ensure the integrity and quality of data Demonstrate a passion for serving the needs of internal and external customers by enabling them with self-service reporting tools and analytics capabilities Drive the execution of data initiatives that provide key performance metrics Understand the data related challenges, nuances, and requirements to identify and recommend the optimal technical approach Train and educate team members as well as stakeholders about best practices in data engineering and governance Collaborate closely with the engineering and devops team to implement DataOps, thus reducing our analytics development cycle Research and improve our data platform to ingest, process, transform, and distribute insightful data to our audience ranging from executives, analysts, and engineers to customers, vendors, and partners Evangelize data driven culture by breaking down silos and encouraging data sharing ', 'Familiarity with popular programming languages (such as Ruby, Python, .NET, etc)', 'Research and improve our data platform to ingest, process, transform, and distribute insightful data to our audience ranging from executives, analysts, and engineers to customers, vendors, and partners', 'ID.me Core Values:', 'Experience with ingesting, processing, and visualizing data sources of varying types - structured/relational and unstructured', 'Understand the data related challenges, nuances, and requirements to identify and recommend the optimal technical approach', 'Proven experience working with various tools but more importantly, familiarity with how to best assemble and deploy production ready data stack to any cloud environment', 'Some level of experience working in the cloud - AWS, Azure, or GCP', 'Qualifications', 'BS in a quantitative or scientific field such as computer science, computer engineering or equivalent experience', 'People', '3+ years of hands-on experience in data engineering for a SaaS company or a mature startup', 'Superb time management, prioritization of tasks and ability to meet deadlines with little supervision', 'Evangelize data driven culture by breaking down silos and encouraging data sharing', 'Must be located in the continental U.S.', 'Overview', 'Drive the execution of data initiatives that provide key performance metrics', 'Experience in developing, managing, and manipulating large, complex datasets', 'Mission', 'Orchestrate and maintain data pipelines that meet security standards and ensure the integrity and quality of data', 'Data-driven, detail-oriented individual with excellent storytelling and problem-solving abilities']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Moser Consulting,"Indianapolis, IN",2 weeks ago,47 applicants,"['', 'Certifications are beneficial.', 'Should be proficient in writing advanced / complex SQL statements.', 'Clearly articulate pros and cons of proposed technologies/solutions. ', 'Experience in cloud-based architectures involving Data Lakes, Ingestion Frameworks, Transformation and Data Set generation.', 'Design, Develop, Deploy, and Monitor complex data pipelines which may contain various required transformations of data using various languages and techniques.', 'Benefits', ' Bachelor’s degree from an accredited four-year college or university with major coursework in computer information systems, computer science, data management, information technology, or any other related discipline.  Experience in cloud-based architectures involving Data Lakes, Ingestion Frameworks, Transformation and Data Set generation. Strong understanding of ETL/ELT fundamentals and solutions involving tools such as Azure Data Factory, SSIS, or Informatica. Should be proficient in writing advanced / complex SQL statements. Expertise in performance tuning and optimization of SQL queries required. Previous projects should display technical leadership with an emphasis on data warehouse solutions and/or business intelligence solutions. Proven ability to design and develop tailored data structures.  Develop, implement, and optimize stored procedures and functions using T-SQL. Ability to create functioning ETL prototypes to address quickly changing business needs.  Ability to develop automation solutions with a goal to reduce manual work. Clearly articulate pros and cons of proposed technologies/solutions.  Certifications are beneficial. Preferred Locations: Indianapolis, IN and Baltimore, MD. United States required. ', 'Develop, implement, and optimize stored procedures and functions using T-SQL.', 'Proven ability to design and develop tailored data structures. ', 'Ability to create functioning ETL prototypes to address quickly changing business needs. ', 'Strong understanding of ETL/ELT fundamentals and solutions involving tools such as Azure Data Factory, SSIS, or Informatica.', 'Requirements', 'Identifying, designing and implementing internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes.', 'Description', 'Working with stakeholders on data-related technical issues.', 'Preferred Locations: Indianapolis, IN and Baltimore, MD. United States required.', 'Ability to develop automation solutions with a goal to reduce manual work.', ' Design, Develop, Deploy, and Monitor complex data pipelines which may contain various required transformations of data using various languages and techniques. Identifying, designing and implementing internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes. Working with stakeholders on data-related technical issues. ', 'Expertise in performance tuning and optimization of SQL queries required.', 'Bachelor’s degree from an accredited four-year college or university with major coursework in computer information systems, computer science, data management, information technology, or any other related discipline. ', 'Previous projects should display technical leadership with an emphasis on data warehouse solutions and/or business intelligence solutions.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Alteryx,"Ann Arbor, MI",2 weeks ago,85 applicants,"['', ' 3+ years of data engineering / development / integration experience ', 'Position Details/requirements', ' BA/BS degree in Information Science, Data Analytics, Computer Science, Software Engineering or related technical field  Experience with Alteryx platform  Experience with Snowflake ', ' Experience with Snowflake ', ' Build, test, monitor and maintain a highly scalable data management ecosystem ', ' Strong oral and written communication ', ' Intermediate SQL experience  Strong problem-solving skills and attention to detail  Ability to work with geographically distributed teams  Ability to work with other teams across an organization  Strong oral and written communication ', ' Comfortable with data modeling practices (normalizing, dimensionalizing ) ', ' Experience with Alteryx platform ', ' Deep interest in the data and analytics market with ability to constantly evaluate new ways to enhance the telemetry system and train it to be smarter and more scalable ', ' Ability to work with other teams across an organization ', 'Qualifications', ' 3+ years of data engineering / development / integration experience  Experience with Enterprise Data Warehouse development  Comfortable with data modeling practices (normalizing, dimensionalizing )  Ability to read and create ERDs ', ' Experience supporting or working in enterprise analytics environments ', ' Extreme focus on detail and data quality validation ', ' Experience with automated software testing and deployment ', ' Take business requirements and produce data sets for efficient cross-business analysis ', ' Helpful to Have ', ' Strong problem-solving skills and attention to detail ', 'Compensation', ' BA/BS degree in Information Science, Data Analytics, Computer Science, Software Engineering or related technical field ', ' Knowledge of at least one modern scripting language (Python, R, etc.) ', ' Understanding of cloud infrastructure ', 'Benefits & Perks', ' Knowledge of at least one modern scripting language (Python, R, etc.)  Understanding of cloud infrastructure  Experience with automated software testing and deployment  Experience supporting or working in enterprise analytics environments ', ' Ability to work with geographically distributed teams ', ' Ability to read and create ERDs ', ' Execute architected techniques and solutions for data collection, management and usage ', ' Need to Have ', ' Intermediate SQL experience ', ' Experience with Enterprise Data Warehouse development ', ' Take business requirements and produce data sets for efficient cross-business analysis  Extreme focus on detail and data quality validation  Deep interest in the data and analytics market with ability to constantly evaluate new ways to enhance the telemetry system and train it to be smarter and more scalable ', ' Build, test, monitor and maintain a highly scalable data management ecosystem  Execute architected techniques and solutions for data collection, management and usage ']",Not Applicable,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Data Support Engineer,Homesnap,"Chevy Chase, MD",3 weeks ago,90 applicants,"['', '*This position is based out of our Bethesda, MD office.*   ', ' We welcome all qualified candidates who are currently eligible to work full-time in the United States to apply.  However, please note that CoStar is not able to provide visa sponsorship for this position.', ' ', ""Bachelor's Degree "", '1+ years of experience working as a support representative ', ""Bachelor's Degree 2+ years of experience with Microsoft SQL Server and Transact-SQL 1+ years of experience working as a support representative "", 'Detail-oriented and able to self-manage several concurrent tasks ', 'Detail-oriented and able to self-manage several concurrent tasks Ability to work both independently and collaboratively with team members You love big data Knowledge of .NET/C# and database architecting/design a plus ', 'Build or modify in-house admin tools for recurring issues ', 'Ability to work both independently and collaboratively with team members ', 'Basic Qualifications ', 'Handle all tickets escalated by the Customer Support team Identify and report cosmetic and functional bugs Diagnosis and fix data related issues Build or modify in-house admin tools for recurring issues Write complex SQL queries to generate custom reports ', 'Knowledge of .NET/C# and database architecting/design a plus ', 'Write complex SQL queries to generate custom reports ', ""Homesnap, a member of the CoStar Family, seeks a Data Support Engineer to join the Homesnap development team based in Bethesda, MD. As an integral member of our world-class development team, you'll help us support our customers by resolving issues related to big data, act as a liaison to our support team, and providing other technical support as needed. "", 'Roles and Responsibilities ', 'You love big data ', '2+ years of experience with Microsoft SQL Server and Transact-SQL ', 'Identify and report cosmetic and functional bugs ', 'Preferred Qualifications ', 'Diagnosis and fix data related issues ', 'Handle all tickets escalated by the Customer Support team ']",Associate,Full-time,Engineering,Real Estate,2021-03-18 14:34:51
Staff Data Engineer,Bonobos,"New York, NY",5 days ago,Be among the first 25 applicants,"['', 'Analyzes, designs, codes, tests, configures and modifies software for the functional delivery of data platforms, pipelines and solutions using various programming languages, technologies and development methodologies. ', 'Snowflake Data warehouseKafka based streaming platformDBTCustom pythonic application abstractions to manage PUB/SUB to kafkaCustom customer informatics servicesCustom data registry servicesCommon Components: Snowflake, Flask, AWS: RDS Postgres, Redis Queue, Redis Graph, Redis Key Value Store, AWS: S3, AWS: ECS/ECR, AWS: Lambda / Gateway, and AWS: SagemakerDevOps Tools: Github, Circle CI, Terraform, Ansible, Containers, and ServerlessMethodologies: Agile, Infrastructure as Code, Microservices, and Cleancode', 'Value self-awareness, intellectual honesty, judgment, empathy & positive energy – often over experience', 'Snowflake Data warehouse', 'Are excited to hear from you!', 'Designs, develops, tests, debugs and implements data platforms, pipelines, solutions and/or software tools, and utilities for the purpose of assuring acceptable performance and service levels. ', 'Proficiency Python and other programming languages', 'Custom pythonic application abstractions to manage PUB/SUB to kafka', 'Outlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications. ', 'Owns the automated delivery of data platforms, pipelines and solutions using source control, infrastructure as code, continuous integration throughout the entire delivery model ', 'Analyzes, designs, codes, tests, configures and modifies software for the functional delivery of data platforms, pipelines and solutions using various programming languages, technologies and development methodologies. Designs, develops, tests, debugs and implements data platforms, pipelines, solutions and/or software tools, and utilities for the purpose of assuring acceptable performance and service levels. Owns the automated delivery of data platforms, pipelines and solutions using source control, infrastructure as code, continuous integration throughout the entire delivery model Ensures that implemented data platforms, pipelines and solutions are successfully monitored, with relevant alerts, logging and tracing that guarantee the relevant durability, availability and performance. Ensures the data is modeled correctly for consumption Validates that data platforms, pipelines and solutions follow data governance policies, standards and intent Completes technical documentation that adds value, including but not limited to testing, training governance and software delivery', 'Ensures the data is modeled correctly for consumption ', 'Self-directed with Implementing, designing, guiding and mentoring Deployment & Delivery ', 'Custom customer informatics services', 'Ability to mentor engineers and drive engineering excellence', 'Have 7+ years of experience in a Data Engineering or similar roleProficiency Python and other programming languagesEnjoy working on a Scrum Team in an agile delivery environmentAbility to mentor engineers and drive engineering excellenceSelf-directed with Implementing, designing, guiding and mentoring Deployment & Delivery Self-directed with gathering and assimilating Development RequirementsCan lead and validate engineering design sessionsCan lead and validate data modelingSelf-directed on simple to complex Debugging / Troubleshooting problems Can lead and validate the implementation, design, and mentoring a team through M.A.L.T. engineering practices', 'Custom data registry services', 'Work hard because we love what we’re doing, but also believe in balance (say hello to unlimited vacation days!)Back up our talk with a competitive compensation and benefits package, challenging projects, random acts of team-wide fun and awesome co-workers that feel like familyValue self-awareness, intellectual honesty, judgment, empathy & positive energy – often over experienceAre located in the Flatiron District in the heart of ManhattanHave a passion for delivering a superior experience to our customers, clients, vendors & one anotherAre excited to hear from you!', 'Ensures that implemented data platforms, pipelines and solutions are successfully monitored, with relevant alerts, logging and tracing that guarantee the relevant durability, availability and performance. ', 'Can lead and validate the implementation, design, and mentoring a team through M.A.L.T. engineering practices', 'Methodologies: Agile, Infrastructure as Code, Microservices, and Cleancode', 'Can lead and validate engineering design sessions', 'Validates that data platforms, pipelines and solutions follow data governance policies, standards and intent ', 'Completes technical documentation that adds value, including but not limited to testing, training governance and software delivery', 'Have a passion for delivering a superior experience to our customers, clients, vendors & one another', 'Back up our talk with a competitive compensation and benefits package, challenging projects, random acts of team-wide fun and awesome co-workers that feel like family', 'DevOps Tools: Github, Circle CI, Terraform, Ansible, Containers, and Serverless', 'Self-directed on simple to complex Debugging / Troubleshooting problems ', 'Are located in the Flatiron District in the heart of Manhattan', 'Self-directed with gathering and assimilating Development Requirements', 'DBT', 'Enjoy working on a Scrum Team in an agile delivery environment', 'Common Components: Snowflake, Flask, AWS: RDS Postgres, Redis Queue, Redis Graph, Redis Key Value Store, AWS: S3, AWS: ECS/ECR, AWS: Lambda / Gateway, and AWS: Sagemaker', 'Kafka based streaming platform', 'Sample Tech From Our Stacks', 'Work hard because we love what we’re doing, but also believe in balance (say hello to unlimited vacation days!)', 'Can lead and validate data modeling', 'Have 7+ years of experience in a Data Engineering or similar role', ""Position Summary... What You'll Do..."", 'Outlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications. ']",Associate,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Lead Data Engineer,The Knot Worldwide,"Austin, TX",1 week ago,54 applicants,"['', 'Bring awareness and communicate standards across the data landscape', 'Continuous improvement on current ETL processes to ensure accuracy, timeliness and scalability as data volumes grow.', 'Ability to ramp up in a short time on new technologies', 'Programming experience and a demonstrated interest in data trends, data analysis, quality, movement, measurement, profiling, reporting and analytics', 'You Do the Right Thing. You strengthen your team through respect, fairness, and inclusion.', 'Proven and demonstrated ability to work under pressure', 'Extract, load and transform data from The Knot Worldwide and other sources to our DataWarehouse for the consumption of our end users.', 'Maintaining current ETL processes and resolving daily ETL job failures as they arise', 'WHAT WE DO MATTERS:', 'Conduct research and make recommendations on data requirements, products and data services', 'Experience developing advanced data pipelines and streaming channels', 'Demonstrate self-confidence, energy and enthusiasm', 'Minimum of 7 years experience with SQL and advanced relational, operational, object-oriented and data warehouse development', 'You Love Our Users. You keep our global community at the center of everything you do.', 'Introduce a consistent change management process that aligns with our standards; examples are: new field, object and data creation', 'Present ideas, expectations and information in a concise well-organized way', 'DESIRED SKILLS/EXPERIENCE:', 'Working knowledge of data movement, monitoring and management technologies ', 'Experience with Big Data Solutions such as Snowflake, Redshift, Google BigQuery and or Hadoop', 'Experience with ETL / ELT platforms such as Matillion, Dell Boomi, Luigi, Informatica, Talend, and or SSIS', 'You Hustle Every Day. You favor urgency and own your outcomes. ', 'Architect and develop advanced solutions for integrating between disparate systems which include cloud-based sources, API and AWS Services', ' A Bachelor’s Degree in Computer Science, Engineering, Information Systems, or a related field is preferred. An advanced degree or professional licensing is a plus. Programming experience and a demonstrated interest in data trends, data analysis, quality, movement, measurement, profiling, reporting and analytics Proven and demonstrated ability to work under pressure Working knowledge of data movement, monitoring and management technologies  Minimum of 7 years experience with SQL and advanced relational, operational, object-oriented and data warehouse development Hands-on experience with at least one of the following databases (MYSQL, PostgreSQL, Snowflake, MSSQL, Redshift, MongoDB, Couchbase)  Minimum of 5 years experience actively working with AWS services (S3, Lambda, SNS, EC2, Load Balancers, Routing tables, EMR, VDI / PCI networking) Excellent communication skills, both verbal and written Demonstrate self-confidence, energy and enthusiasm Present ideas, expectations and information in a concise well-organized way Conduct research and make recommendations on data requirements, products and data services Demonstrated ability to act in a lead role Ability to ramp up in a short time on new technologies Ability to troubleshoot complex integration issues Manage time well, whilst correctly prioritizing tasks ', ' Experience with advanced Ralph Kimball dimensional modelling architecture Experience in database OLTP schema design and architecture models Experience developing advanced data pipelines and streaming channels Experience with at least one Business Intelligence tool such as Qlik and or Metabase Experience with Big Data Solutions such as Snowflake, Redshift, Google BigQuery and or Hadoop Experience with ETL / ELT platforms such as Matillion, Dell Boomi, Luigi, Informatica, Talend, and or SSIS Programming experience with Ruby, Python, Scala, Shell Scripting Ability to ramp up quickly and fully exploit platform advanced feature sets ', 'Collaborate with Data Engineering leadership on data standardization and integration best practices', 'Experience with advanced Ralph Kimball dimensional modelling architecture', 'You Dream Big. You iterate and experiment to drive innovation.', 'Programming experience with Ruby, Python, Scala, Shell Scripting', 'ABOUT THE ROLE AND OUR TEAM:', 'Provide guidance to team members with respect to best practices', ' You Dream Big. You iterate and experiment to drive innovation. You Love Our Users. You keep our global community at the center of everything you do. You Do the Right Thing. You strengthen your team through respect, fairness, and inclusion. You Hustle Every Day. You favor urgency and own your outcomes.  You Win Together. People are at the heart of our success and you play as a team. ', 'Demonstrated ability to act in a lead role', 'WHAT WE LOVE ABOUT YOU:', 'Ability to ramp up quickly and fully exploit platform advanced feature sets', 'Experience in database OLTP schema design and architecture models', 'RESPONSIBILITIES: ', 'WHAT YOU LOVE ABOUT US:', 'Manage time well, whilst correctly prioritizing tasks', ' Extract, load and transform data from The Knot Worldwide and other sources to our DataWarehouse for the consumption of our end users. Architect and develop advanced solutions for integrating between disparate systems which include cloud-based sources, API and AWS Services Maintaining current ETL processes and resolving daily ETL job failures as they arise Resolve Ad-hoc data questions from the organization in a timely and accurate manner Continuous improvement on current ETL processes to ensure accuracy, timeliness and scalability as data volumes grow. Identify through documented or current issues, data structural problems that increase inconsistencies and reduce quality and performance within the enterprise set of systems and the related data stores  Bring awareness and communicate standards across the data landscape Introduce a consistent change management process that aligns with our standards; examples are: new field, object and data creation Support project efforts that seek to reduce technical debt, example: (Multiple data stores that are redundant) Analyze, propose and seek to establish data permissions and profile segmentation aligning to standards such as Personas Document built processes and data content and publish needed information to our data dictionary Provide guidance to team members with respect to best practices Participate in maintaining quality and governance standards  Collaborate with Data Engineering leadership on data standardization and integration best practices ', 'Ability to troubleshoot complex integration issues', 'Support project efforts that seek to reduce technical debt, example: (Multiple data stores that are redundant)', 'SUCCESSFUL LEAD DATA ENGINEERS HAVE:', 'Experience with at least one Business Intelligence tool such as Qlik and or Metabase', 'A Bachelor’s Degree in Computer Science, Engineering, Information Systems, or a related field is preferred. An advanced degree or professional licensing is a plus.', 'Identify through documented or current issues, data structural problems that increase inconsistencies and reduce quality and performance within the enterprise set of systems and the related data stores ', 'Document built processes and data content and publish needed information to our data dictionary', 'You Win Together. People are at the heart of our success and you play as a team.', 'Minimum of 5 years experience actively working with AWS services (S3, Lambda, SNS, EC2, Load Balancers, Routing tables, EMR, VDI / PCI networking)', 'Excellent communication skills, both verbal and written', 'Analyze, propose and seek to establish data permissions and profile segmentation aligning to standards such as Personas', 'Participate in maintaining quality and governance standards ', 'Hands-on experience with at least one of the following databases (MYSQL, PostgreSQL, Snowflake, MSSQL, Redshift, MongoDB, Couchbase) ', 'Resolve Ad-hoc data questions from the organization in a timely and accurate manner']",Associate,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Data Engineer – 448835,Revel IT,"Columbus, OH",2 days ago,Be among the first 25 applicants,"['', 'No. 1 supplier with customers: 53%', 'Top 3 supplier with customers: 77%', 'Data Modeling background desired', 'Experience building process that perform data transformations and utilize metadata as needed for task optimization', 'Strong development skills with relational databases and data warehouses (SQL, Stored Procedures, Database tuning, etc)', 'Background with various scripting and coding languages (Python, JavaScript, .NET)', 'Ability to analyze and troubleshoot complex data issues', 'Role Qualifications:', 'Exposure to big data platforms a plus (Hadoop, Spark, Scala, etc)', 'Consultant retention: 94%', '5+ years ETL or related experience', 'In addition to standard health and 401k benefits, we offer referral bonuses and training/continuing education opportunities.', 'Experience working in Agile environment', 'OUR GOAL:', ' Exposure to big data platforms a plus (Hadoop, Spark, Scala, etc) Data Modeling background desired Exposure to API Integration platforms/Microservices a plus (SnapLogic, MuleSoft, Boomi, etc.) ', 'WHY REVEL IT: ', 'OUR MISSION:', ' In addition to standard health and 401k benefits, we offer referral bonuses and training/continuing education opportunities. 5-year client retention: 99% No. 1 supplier with customers: 53% Top 3 supplier with customers: 77% Consultant retention: 94% ', '5-year client retention: 99%', 'Data Engineer ', 'Demonstrable experience with cloud technologies (AWS preferred – S3, Glue, EC2, EMR, DynamoDB, RedShift, Lambda)', 'ABOUT REVEL IT:', 'Exposure to API Integration platforms/Microservices a plus (SnapLogic, MuleSoft, Boomi, etc.)', ' Demonstrable experience with cloud technologies (AWS preferred – S3, Glue, EC2, EMR, DynamoDB, RedShift, Lambda) Strong development skills with relational databases and data warehouses (SQL, Stored Procedures, Database tuning, etc) Working knowledge and proven experience with ETL integration platforms (SSIS, Informatica) Ability to analyze and troubleshoot complex data issues Experience building process that perform data transformations and utilize metadata as needed for task optimization Background with various scripting and coding languages (Python, JavaScript, .NET) 5+ years ETL or related experience Experience working in Agile environment ', 'Desired:', 'Working knowledge and proven experience with ETL integration platforms (SSIS, Informatica)', 'Reference: 448835']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Amitech Solutions,United States,2 weeks ago,146 applicants,"['', '• Knowledge of algorithms and data structures;', 'Preferred Qualifications:', '• Experience with python, Java, R, or Scala.', 'Life insurance', '• Design and maintain ETL workflows;', '• Actively drive skill development of others in the space through proactive coaching, feedback, and training;', '• Work on the development, deployment, and support of systems computing solutions;', '• Implement, configure, and maintain critical third-party solutions related to engineering work, including compute environments, BI platforms, and cloud systems;', 'Location: Remote', ' Amitech is a rapidly growing organization focused on our employees. Our diverse and innovative approach to everything we do means we’re looking for the groundbreakers and the pioneers—people who think differently and create the future.', 'Health, Dental, and Vision insuranceLong and Short-Term Disability401 (k)Life insurancePet insuranceReferral program2018 - 2020 “Top Work Places” Winner', 'o Experience with big data tools (Spark, Kafka, Flink, Hadoop, etc.);', '• Network and Database administration;', '• Experience with tools for authoring workflows and pipelines (Airflow, AWS Step Functions, KubeFlow, etc.);', '• Two plus years of experience with GIS data and relevant tools (ArcGIS Pro, ArcGIS', '• Familiarity with ML workflows including validation and hyper parameter tuning approaches, and ML frameworks such as scikit-learn and tensor flow.', '• Design and maintain data storage systems and access patterns;', '• Proven ability to plan, schedule and deliver quality software DevOps methodology;', '• Work on all aspects of the design, development, validation, scaling and delivery of analytical solutions;', '• Work on the deployment, delivery and expansion of data pipelines;', '• Partner cross-functionally in the development of shared infrastructure where aligned with Breeding business needs;', 'Health, Dental, and Vision insurance', '• Experience with distributed systems;', '• Collaborate with interdisciplinary scientists to gather requirement for data pipelines;', '401 (k)', 'Why Amitech:', '• Collaborate with analytics and discovery teams to design and plan data engineering solutions;', '• Experience in running production cloud systems and diagnosing and fixing problems;', '\ufeffAbout Us', '• Provide consulting and feedback to partner teams on data architecture and strategy;', '• Design, build, and maintain integrated data solutions such as “data lakes” and “data warehouses”;', '• Technical knowledge with at least of seven years of experience in at least four of the following:', '2018 - 2020 “Top Work Places” Winner', 'Online, ArcMap)', 'Required Qualifications:', '• Drive practices which help raise the success of the overall team, such as code reviews, integrated testing, and other practices;', 'Sr. Data Engineer', '• Bachelor’s degree in Computer Science, Electrical Engineering or a closely-related field with at least 6-8 years of industry experience OR Master’s Degree in Computer Science, Electrical Engineering or a closely-related field with at least eight years of industry experience OR Doctorate in Computer Science, Electrical Engineering, or a closely-related field with at least four years of industry experience;', ""• Actively identify new technologies and practice within the domain of engineering and drive review for potential introduction to the team's infrastructure."", 'Description:', 'Pet insurance', '• Experience in Developing and supporting large scale geospatial and Imaging Processing data pipelines', '• Optimize algorithms and data workers to scale horizontally and contribute to the development of new algorithms and capabilities that will enable connected pipeline analytics for all pipelines;', '• Integrate proactive strategies and best practices to ensure security of stored data;', 'o SQL and NoSQL databases (data warehousing, data modeling, etc.);', 'Referral program', ' We believe healthcare should and can be better.\xa0With a single-minded focus on value, we combine people, process, culture and technology to drive real and lasting change. We partner with our clients to deliver data analytics and digital transformation strategies and solutions to make healthcare more proactive, higher quality and less expensive for everyone.', '• Primary focus on developing data pipelines with geospatial and imaging processing data pipelines to enable batch and real-time analytics.', 'Long and Short-Term Disability', '• Proven systems administration and operations experience;', '\xa0', '• Collaborate and influence with cross-functional stakeholders to develop our strategic target state data infrastructure and organization model;', '• Experience with AWS cloud services (EMR, S3, RedShift, EC2, etc.);']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Software Engineer // Data,Brandfolder,"Denver, CO",2 weeks ago,34 applicants,"['', 'The drive to take initiative and own high-impact projects from day one', 'Iconic office location in RiNo, bike and light rail friendly', 'About Brandfolder', 'Generous paid time off', '401k Match (50% up to 6%) to help you save for your future', 'Partner with data scientists and software engineers to develop a best-in-class data architecture, prioritizing scalability, reliability, and functionality.', 'Partner with data scientists and software engineers to develop a best-in-class data architecture, prioritizing scalability, reliability, and functionality.Leverage observability and monitoring tooling such as Rollbar and Datadog for delivering reliable data pipelinesEnsure that all data systems meet the business/company requirements as well as industry practices, and are optimized for cost effectiveness.Develop set processes for data mining, data modeling, and data production.Design, construct, install, test and maintain data management systems, especially the ETL pipeline, currently run through Fivetran', 'Leverage observability and monitoring tooling such as Rollbar and Datadog for delivering reliable data pipelines', 'Legally eligible to work in the U.S. on an ongoing basis', '5+ years of experience in data engineering roles', 'Responsibilities', 'Have a passion for troubleshooting and finding solutions in a production environment', 'Opportunities for professional growth and development', 'Medical and dental insurance, 100% paid by BrandfolderEquity - Restricted Stock Units (RSUs) Equity with all offersLucrative Employee Stock Purchase Program (15% discount)401k Match (50% up to 6%) to help you save for your futureGenerous paid time offParental leave policyOpportunities for professional growth and developmentIconic office location in RiNo, bike and light rail friendly', 'Lucrative Employee Stock Purchase Program (15% discount)', 'Design, construct, install, test and maintain data management systems, especially the ETL pipeline, currently run through Fivetran', 'Develop set processes for data mining, data modeling, and data production.', 'Requirements', '5+ years of experience in data engineering rolesPositive attitude and the ability to thrive in a fast-paced environmentThe drive to take initiative and own high-impact projects from day oneAdvanced skills in Python and SQLExperience with BigQuery, Airflow, Google Cloud Storage, and PostgresExperience developing, maintaining, and monitoring data/ETL pipelinesStrong knack for picking up new tools and technologies, continuously learningHave a passion for troubleshooting and finding solutions in a production environmentLegally eligible to work in the U.S. on an ongoing basis', 'Experience with BigQuery, Airflow, Google Cloud Storage, and Postgres', 'Positive attitude and the ability to thrive in a fast-paced environment', 'Company Description', 'Medical and dental insurance, 100% paid by Brandfolder', 'Experience developing, maintaining, and monitoring data/ETL pipelines', 'Ensure that all data systems meet the business/company requirements as well as industry practices, and are optimized for cost effectiveness.', 'Parental leave policy', 'Job Description', 'Advanced skills in Python and SQL', 'Equity - Restricted Stock Units (RSUs) Equity with all offers', 'Strong knack for picking up new tools and technologies, continuously learning']",Mid-Senior level,Full-time,Engineering,Marketing and Advertising,2021-03-18 14:34:51
Data Engineer,"Agile Resources, Inc.","Atlanta, GA",,N/A,"['', 'Remote:\xa0', 'Experience with ETL orchestration tools to run large scale ETL workflows', 'Role Type:', 'BS degree in Computer Science or related technical field', 'Experience deploying infrastructure as code with tools such as AWS CloudFormation and Terraform', 'Significant experience with large scale data sources, structures and processes', '.', 'Experience working in an Agile development environment will be a plus.', '2-4 years of relevant work experience', 'A deep understanding of general ETL processing and tools as well as data warehousing concepts and workflows', 'BS degree in Computer Science or related technical fieldExperience with Amazon Web Services, particularly Redshift, S3\xa0or comparable services from another cloud providerExperience with ETL orchestration tools to run large scale ETL workflowsExperience deploying infrastructure as code with tools such as AWS CloudFormation and TerraformExperience working with web data sourcesExperience working in an Agile development environment will be a plus.', '2-4 years of relevant work experienceA deep understanding of general ETL processing and tools as well as data warehousing concepts and workflowsStrong knowledge of SQL and PythonSignificant experience with large scale data sources, structures and processes', 'Experience with Amazon Web Services, particularly Redshift, S3\xa0or comparable services from another cloud provider', 'Pay:\xa0', 'Pay:\xa0up to $115k DoE', 'Our Ideal Candidate has the Following:', 'Our client is a software company in South Carolina which builds custom AWS-hosted software solutions which service the real estate industry.', '\xa0', 'Role Type:\xa0Direct Hire / Full-time / Permanent', '\ufeffKeywords: data, python, ETL, SQL, data warehouse, BI', 'Preferred Skills (not required):', 'Experience working with web data sources', 'Strong knowledge of SQL and Python', 'Remote:\xa0100% remote role', '\ufeffKeywords']",Mid-Senior level,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Engineer / DBA,Hays,"Anaheim, CA",2 weeks ago,61 applicants,"['', 'Data Engineer / DBA (Microsoft, Angular) – Perm – Anaheim, CA. - $70,000-$90,00', 'Why Hays?', '• Work with client and management team to prioritize business and reporting/data needs.', '• Good communication skills – both written and oral.', '• Design and develop tables, views, stored procedures, indexes, functions, dictionaries, and complex ad- hoc queries for SQL Server databases.', 'An American Company is seeking a Data Engineer / DBA (Microsoft, Angular) in Anaheim, CA', '• Proficiency in Angular framework', 'Hays is an Equal Opportunity Employer.', '• Design and develop user interfaces using AngularJS best practices. - AngularJS not required at all', 'You will be working with a professional recruiter who has intimate knowledge of the Information Technology industry and market trends . Your Hays recruiter will lead you through a thorough screening process in order to understand your skills, experience, needs, and drivers. You will also get support on resume writing, interview tips, and career planning, so when there’s a position you really want, you’re fully prepared to get it.', '• 2+ years of experience with Microsoft SQL server, including table structuring, creating queries and stored procedures', '• 1+ years of development experience with object-oriented languages', ""• Bachelor's on computer science, mathematics, engineering or related field"", '• Translate client requested features into technical requirements.', '• Communicate/coordinate with team to ensure proper implementation of company standards and procedure', 'Visit the Hays Career Advice section to learn top tips to help you stand out from the crowd when job hunting.', 'Drug testing may be required; please contact a recruiter for more information.', '• Exercise critical thinking to identify opportunities to reduce manual processes through automation.', '• Proficiency in advanced Microsoft Excel and Visual Basic for Applications (VBA) including creating/editing macros.', '• Must be able to work independently, and in team environments.', 'Skills & Requirements', 'Nervous about an upcoming interview? Unsure how to write a new resume?', 'The end client is unable to sponsor or transfer visas for this position; all parties authorized to work in the US without sponsorship are encouraged to apply.', 'Role Description']",Mid-Senior level,Full-time,Information Technology,Staffing and Recruiting,2021-03-18 14:34:51
Data Engineer,Cedar,"New York, NY",6 days ago,33 applicants,"['', ' Experience with AWS data services/tools such as Redshift, S3, Athena, Glue is preferred. Experience with data warehouse or data lake solutions such as Snowflake, BigQuery, Redshift Experience with MapReduce, Hadoop, Spark is preferred ', ' Cedar is committed to a flexible work environment, so this as well as many of our roles are remote friendly.Responsibilities Build and optimize the process of analytics aggregations and experimentation data pipelines Collaborate with data scientists, analysts, product managers and business development teams to understand data needs and transform these into usable data products. Partner with Analytics to systematize and scale high-integrity value-oriented analysis. Partner with other engineering teams to guide refactors of existing data infrastructure to improve data quality and features. Identify and resolve issues that may negatively impact data product timelines or quality. Troubleshoot transformations for data consistency and integrity. Skills And Experience 3+ years experience in designing, building and maintaining data pipelines. Experience working with relational databases, such as PostgreSQL, or MySQL Experience with Python and ORM tools such as SQL Alchemy Solid understanding of data warehouse Demonstrates a passion for breaking down and understanding complex systems and data structures A great teammate with excellent communication and listening skills. Ability to thrive in an entrepreneurial environment, comfortable with ambiguity Excited about Cedar’s mission of improving the healthcare financial experience Encompass Cedar’s core values: mission driven, no mediocrity, use good judgement, positivity Strong computer science fundamentals - a degree in computer science, engineering, or a related field or similar experience Eligible to work in the United States Prefered Skills And Experiences Experience with AWS data services/tools such as Redshift, S3, Athena, Glue is preferred. Experience with data warehouse or data lake solutions such as Snowflake, BigQuery, Redshift Experience with MapReduce, Hadoop, Spark is preferred What do we offer to the ideal candidate? An opportunity to work on a platform that is scaling very rapidly, engaging with millions of patients per year and growing at a rate of 360% YoY as of January 2021 A chance to join a high-growth company at an early stage The ability to impact the growth of our company, we value all comments and suggestions Transparency across teams and interaction with multiple departments Competitive pay, employer-paid healthcare, stock options Applicants must be currently authorized to work in the United States on a full-time basis. Cedar will not hire any applicants for Data Engineer who are present in the United States on an F-1 visa', ' An opportunity to work on a platform that is scaling very rapidly, engaging with millions of patients per year and growing at a rate of 360% YoY as of January 2021 A chance to join a high-growth company at an early stage The ability to impact the growth of our company, we value all comments and suggestions Transparency across teams and interaction with multiple departments Competitive pay, employer-paid healthcare, stock options ', 'Competitive pay, employer-paid healthcare, stock options', 'Experience with AWS data services/tools such as Redshift, S3, Athena, Glue is preferred.', ' Build and optimize the process of analytics aggregations and experimentation data pipelines Collaborate with data scientists, analysts, product managers and business development teams to understand data needs and transform these into usable data products. Partner with Analytics to systematize and scale high-integrity value-oriented analysis. Partner with other engineering teams to guide refactors of existing data infrastructure to improve data quality and features. Identify and resolve issues that may negatively impact data product timelines or quality. Troubleshoot transformations for data consistency and integrity. ', 'Solid understanding of data warehouse', 'A chance to join a high-growth company at an early stage', 'Skills And Experience', 'Experience working with relational databases, such as PostgreSQL, or MySQL', 'Excited about Cedar’s mission of improving the healthcare financial experience', 'Demonstrates a passion for breaking down and understanding complex systems and data structures', 'A great teammate with excellent communication and listening skills.', ' 3+ years experience in designing, building and maintaining data pipelines. Experience working with relational databases, such as PostgreSQL, or MySQL Experience with Python and ORM tools such as SQL Alchemy Solid understanding of data warehouse Demonstrates a passion for breaking down and understanding complex systems and data structures A great teammate with excellent communication and listening skills. Ability to thrive in an entrepreneurial environment, comfortable with ambiguity Excited about Cedar’s mission of improving the healthcare financial experience Encompass Cedar’s core values: mission driven, no mediocrity, use good judgement, positivity Strong computer science fundamentals - a degree in computer science, engineering, or a related field or similar experience Eligible to work in the United States ', 'Responsibilities', 'Transparency across teams and interaction with multiple departments', 'Applicants must be currently authorized to work in the United States on a full-time basis. Cedar will not hire any applicants for Data Engineer who are present in the United States on an F-1 visa', 'The ability to impact the growth of our company, we value all comments and suggestions', 'Strong computer science fundamentals - a degree in computer science, engineering, or a related field or similar experience', 'Collaborate with data scientists, analysts, product managers and business development teams to understand data needs and transform these into usable data products.', 'Encompass Cedar’s core values: mission driven, no mediocrity, use good judgement, positivity', 'Build and optimize the process of analytics aggregations and experimentation data pipelines', '3+ years experience in designing, building and maintaining data pipelines.', 'Eligible to work in the United States', 'Identify and resolve issues that may negatively impact data product timelines or quality. Troubleshoot transformations for data consistency and integrity.', 'What do we offer to the ideal candidate?', 'Experience with Python and ORM tools such as SQL Alchemy', 'Prefered Skills And Experiences', 'Data Engineer', 'Experience with data warehouse or data lake solutions such as Snowflake, BigQuery, Redshift', 'An opportunity to work on a platform that is scaling very rapidly, engaging with millions of patients per year and growing at a rate of 360% YoY as of January 2021', 'Partner with Analytics to systematize and scale high-integrity value-oriented analysis.', 'Ability to thrive in an entrepreneurial environment, comfortable with ambiguity', 'Partner with other engineering teams to guide refactors of existing data infrastructure to improve data quality and features.', 'Experience with MapReduce, Hadoop, Spark is preferred']",Entry level,Full-time,Information Technology,Hospital & Health Care,2021-03-18 14:34:51
Data Engineer,Techgene Solutions,"Irving, TX",3 days ago,68 applicants,"['', 'In addition, there will be work in support of monitoring and analyzing the trends of the dashboards, enabling real-time alarming and coordinating with application teams on logging standardization.', '•\tExperience of Kafka and experience with cross DC replication Experience with Docker and Kubernetes\xa0', 'Must Have Skills:\xa0', '•\tExperience in Data analytics Experience with Big Data technologies Experience in Python and R programming language -\xa0\xa0', '•\tExperience with in-memory caches like Redis / Akka Distributed data Experience with CI/CD process - GIT (Bitbucket), Jenkins, Jira, Confluence Google drive suite services (Google Docs, Google Sheets, etc.)', 'Candidates will be required to develop and design LogStash and other jobs to consolidate application log counters in a standard methodology and output on Elastic Search and Influx DB, building process to migrate history as well as real-time data manipulation and create and organize Grafana dashboards providing business transactional details, operational and server health details, and customer aggregated funnels across channels.', '\xa0', 'Description Candidates will address Identity Access Management / Identity Fraud Gateway dashboarding and alarming used by IT, Operations, Finance and Fraud Prevention organizations built using Splunk, Oracle, Kibana and other data sources consolidating logging detail counters in Elastic and Influx DB and then exposing this data on real-time Grafana dashboards.', '•\tExperience in building dashboards real time and batch Experience with Distributed Data platforms (HDFS, Elasticsearch, Splunk, Casandra) Experience in working with NoSql and in memory databases\xa0']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
"Data Engineer, DevOps",insitro,"South San Francisco, CA",7 hours ago,31 applicants,"['', ' Team lunches (catered daily) ', 'About Insitro', ' Experience architecting reliable infrastructure platforms including monitoring and alerting, load balancing, scalable services, multi-region ', ' Experience with data processing pipelines ', ' 2-3 years of experience with provisioning AWS cloud services (Experience with GCP and Azure is also relevant). ', '  Excellent medical, dental, and vision coverage   Open vacation policy   Team lunches (catered daily)   Commuter benefits   Paid parental leave  ', ' Open vacation policy ', ' Commuter benefits ', ' Experience with cloud configuration and resource management tools such as Terraform ', ' Experience with managing medium-sized data sets (100TB+) in object storage systems like S3 ', ' Experience with batch computing systems such as AWS Batch, SLURM ', '  2-3 years of experience with provisioning AWS cloud services (Experience with GCP and Azure is also relevant).   Experience with cloud configuration and resource management tools such as Terraform   Experience architecting reliable infrastructure platforms including monitoring and alerting, load balancing, scalable services, multi-region   Experience with at least one high-end distributed data processing environment (Hadoop, Spark, etc)   Experience with batch computing systems such as AWS Batch, SLURM   Experience with container build and deployment systems like Docker, Kubernetes, or ECS   Ability to communicate effectively and collaborate with people of diverse backgrounds and job functions   Proficiency in Linux environment (including shell scripting and Python programming), experience with database languages (e.g., SQL, No-SQL) and experience with version control practices and tools (Git, Mercurial, etc.)   Passion for making a difference in the world  ', '  Experience with biological data   Experience with managing medium-sized data sets (100TB+) in object storage systems like S3   Experience with defining infrastructure following compliance (GDPR, HIPAA, etc).   Experience with data processing pipelines   Experience with deploying and monitoring machine learning models in a production environment  ', ' Proficiency in Linux environment (including shell scripting and Python programming), experience with database languages (e.g., SQL, No-SQL) and experience with version control practices and tools (Git, Mercurial, etc.) ', ' Excellent medical, dental, and vision coverage ', ' Ability to communicate effectively and collaborate with people of diverse backgrounds and job functions ', ' Passion for making a difference in the world ', ' Experience with container build and deployment systems like Docker, Kubernetes, or ECS ', ' Experience with biological data ', 'About You', ' Experience with defining infrastructure following compliance (GDPR, HIPAA, etc). ', ' Experience with deploying and monitoring machine learning models in a production environment ', ' Paid parental leave ', ' Experience with at least one high-end distributed data processing environment (Hadoop, Spark, etc) ']",Mid-Senior level,Full-time,Information Technology,Biotechnology,2021-03-18 14:34:51
Data Engineer,Commonwealth Financial Network,"Waltham, MA",2 days ago,Be among the first 25 applicants,"['', ' Ability to write, troubleshoot, and performance-tune stored procedures and ad-hoc scripts ', ' Experience with ETL tools such as SSIS and Azure Data Factory  5–10 years of experience  Experience working with Microsoft Azure Cloud a plus  Financial sector experience a plus ', 'About Commonwealth', ' Solid understanding of relational modeling concepts ', ' 5–10 years of experience ', ' Experience working with Microsoft Azure Cloud a plus ', ' Picture Yourself Here ', 'Additional Skills And Knowledge', ' Fulfilling data requests when the data elements are not yet built in the data warehouse ', ' Creating workflows and transformations with SQL Server Integration Services ', ' Experience with data warehousing ', ' Experience with ETL tools such as SSIS and Azure Data Factory ', ' Ensuring that data is stored efficiently and can be retrieved quickly within our databases and servers that integrate as an ecosystem to serve our clients’ needs ', ' Financial sector experience a plus ', ' Solid understanding of relational modeling concepts  Ability to write, troubleshoot, and performance-tune stored procedures and ad-hoc scripts  Experience with data warehousing ', ' Building ETL processes for data warehouse and data hub to enable business users to self-serve various data analytics  Fulfilling data requests when the data elements are not yet built in the data warehouse  Building the schema and SQL code behind our complex suite of web-based applications  Creating workflows and transformations with SQL Server Integration Services  Ensuring that data is stored efficiently and can be retrieved quickly within our databases and servers that integrate as an ecosystem to serve our clients’ needs  Troubleshooting and tuning complex SQL statements and making recommendations on indexing strategies  Working on a variety of projects and systems, from big to small and complex to simple  Collaborating with a dynamic team of web developers, business analysts, and product owners to build the best applications in the financial industry  Performing code reviews, learning from peers, and sharing your knowledge in a positive, friendly way ', ' Collaborating with a dynamic team of web developers, business analysts, and product owners to build the best applications in the financial industry ', ' Performing code reviews, learning from peers, and sharing your knowledge in a positive, friendly way ', ' Troubleshooting and tuning complex SQL statements and making recommendations on indexing strategies ', 'Key Responsibilities', 'Core Strengths', ' Working on a variety of projects and systems, from big to small and complex to simple ', ' Building the schema and SQL code behind our complex suite of web-based applications ', ' Building ETL processes for data warehouse and data hub to enable business users to self-serve various data analytics ', ' The Fine Print ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Software Engineer - Data Warehouse Engineer,Paychex,"Webster, NY",6 days ago,34 applicants,"['', 'Completes more complex program changes to translate specifications and / or requirements into code.', 'What you can expect to do in this role?', 'Strong SQL and PL/SQL experienceExperience with languages such as Java and PythonExperience with ETL tools such as ODI is a plusUnderstanding of data warehouse conceptsInterest in working with near real-time data transformation', 'Designs, develops and troubleshoots software programs for computer based systems.Performs basic analysis and design for program changes to gain an understanding of application changes.Provides time estimates for development tasks to aid in scheduling.\xa0Monitors progress against task estimates to provide status.Completes more complex program changes to translate specifications and / or requirements into code.Applies application specific technical skills to independently produce deliverables (i.e. specifications, program changes, unit test scripts, documentation, etc.).Acquires and utilizes basic business knowledge to support the applications.Recognizes code inefficiencies and makes suggestions for improvements, if applicable.Provides documentation to support program changes.Plans, documents, and executes unit test plans to ensure all code changes meet requirements / specificationsSupports programming changes during quality assurance, user acceptance testing, and post implementation to ensure integrity of application.Complies with and contributes to standards and procedures to ensure development consistency (e.g. programming standards, change management, version control).What you can expect to do in this role?Work in an agile environment where team collaboration is embraced.Strengthen your technical skills and abilities while applying them to achieve success and influence the product offerings of a nationally recognized Tech company.', 'Supports programming changes during quality assurance, user acceptance testing, and post implementation to ensure integrity of application.', 'Interest in working with near real-time data transformation', 'Strengthen your technical skills and abilities while applying them to achieve success and influence the product offerings of a nationally recognized Tech company.', 'Experience with ETL tools such as ODI is a plus', 'Applies application specific technical skills to independently produce deliverables (i.e. specifications, program changes, unit test scripts, documentation, etc.).', 'Work in an agile environment where team collaboration is embraced.', 'POSITION REQUIREMENTS', 'Understanding of data warehouse concepts', 'Strong SQL and PL/SQL experience', 'Performs basic analysis and design for program changes to gain an understanding of application changes.', 'Complies with and contributes to standards and procedures to ensure development consistency (e.g. programming standards, change management, version control).', 'Provides documentation to support program changes.', 'Acquires and utilizes basic business knowledge to support the applications.', 'Provides time estimates for development tasks to aid in scheduling.\xa0Monitors progress against task estimates to provide status.', 'Plans, documents, and executes unit test plans to ensure all code changes meet requirements / specifications', 'Designs, develops and troubleshoots software programs for computer based systems.', 'The Enterprise Reporting team is responsible for growing and improving our client facing reporting applications within our Flex product suite.\xa0We leverage a large variety of technologies such as GoldenGate, Exadata, Oracle, ODI, OBIEE, Adeptia, SSRS, Azure Data Factory, Azure, Java, Jenkins, and more.\xa0This team also acts as administrators for ODI, OBIEE, and Adeptia.\xa0We will add MicroStrategy to our breadth of tools this year.\xa0We collaborate closely with our product and operations partners to determine how to best design solutions for each business opportunity.\xa0Our agile environment consists of multiple agile teams working in two week sprints.\xa0Software is released on a consistent release schedule (every 2-3 weeks), allowing for frequent content delivery to our customers.', '\xa0', 'Recognizes code inefficiencies and makes suggestions for improvements, if applicable.', 'Experience with languages such as Java and Python']",Associate,Full-time,Engineering,Human Resources,2021-03-18 14:34:51
Data Engineer,Westinghouse Electric Company,"Cranberry, PA",1 week ago,Be among the first 25 applicants,"['', ' Competitive Salary ', ' Speed and Passion to Win ', ' Partner and develop strong relationships with cross-functional teams ', ' Safety and Quality ', ' Build and automate actionable reports ', 'This role is remote eligible ', ' Help build and maintain technical solutions required for optimal ingestion, transformation, and loading of data from a wide variety of data sources and large, complex data sets ', ' Customer Focus and Innovatoin ', ' Collaborate across roles to embrace best practices in reporting and analysis, including data integrity, test design, validation, and documentation ', 'Job Responsibilities', "" Bachelor's or Master's degree in Computer Science or related technical field or equivalent professional experience "", '  Assist with creation of data schemas, stored procedures, data pipelines, and views   Help build and maintain technical solutions required for optimal ingestion, transformation, and loading of data from a wide variety of data sources and large, complex data sets   Collaborate across roles to embrace best practices in reporting and analysis, including data integrity, test design, validation, and documentation   Ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy   Build and automate actionable reports   Collaborate with data analysts, data scientists, and stakeholders during design discussions to uncover more detailed business requirements related to data engineering   Develop strong hypotheses, independently solve problems, and share actionable insights with engineering   Partner and develop strong relationships with cross-functional teams  ', 'Your Day-to-Day', 'The Following Are Representative Of What We Offer', '  Safety and Quality   Integrity and Trust   Customer Focus and Innovatoin   Speed and Passion to Win   Teamwork and Accountability  ', '  Competitive Salary   Comprehensive Health, Wellness and Income Protection Benefits   401(k) Savings Plan with Company Match   Paid Vacations and Holidays   Opportunities for Flexible Work Arrangements   Educational Reimbursement Program   Employee Referral Program  ', ' Develop strong hypotheses, independently solve problems, and share actionable insights with engineering ', ' Collaborate with data analysts, data scientists, and stakeholders during design discussions to uncover more detailed business requirements related to data engineering ', 'Who You Are', ' Employee Referral Program ', ""  Bachelor's or Master's degree in Computer Science or related technical field or equivalent professional experience   Greater than 3 years of experience   Experience with Azure products; Data Factory or Databricks is required  "", ' Experience with Azure products; Data Factory or Databricks is required ', 'Posting Title', 'Data Engineer ', ' Teamwork and Accountability ', ' Comprehensive Health, Wellness and Income Protection Benefits ', ' Opportunities for Flexible Work Arrangements ', ' Greater than 3 years of experience ', ' Educational Reimbursement Program ', 'Why Westinghouse?', ' Integrity and Trust ', 'We Enable Our Delivery Of This Vision By Living Our Value System', 'Get Connected With Westinghouse On Social Media', ' Assist with creation of data schemas, stored procedures, data pipelines, and views ', ' Ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy ', ' Paid Vacations and Holidays ', ' 401(k) Savings Plan with Company Match ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,LRW,"Los Angeles, CA",1 week ago,35 applicants,"['', 'Work with a variety of stakeholders to design, implement, and maintain\xa0data lake and data warehouse\xa0architecture to consolidate data from various APIs, SQL and NoSQL sources and make it\xa0accessible based on different use cases', 'Familiarity deploying solutions through docker or REST APIs', 'The ideal candidate should be curious, self-motivated, responsive, and articulate', 'Is interested in the core business of the company and seeks to identify the business and use implications of various solutions', '\ufeff', 'Experience in R and JavaScript\xa0is a plus', 'Data platforms: PostgreSQL, MongoDB , data in different file formats (Parquet, JSON, CSV, Excel)', 'With advanced knowledge of SQL, Python,\xa0and Bash', 'With deep understanding of data processing concepts and data modeling principles (difference between OLTP, data warehouse, and data lake )', 'Distributed processing: HDFS, Spark, Hive', 'ABOUT YOU', 'Someone who is a tenacious problem-solver who seeks to identify core bottlenecks from both a technological as well as a process oriented stand point.', 'In this role, you will:', 'We are a fast-growing market research firm with an entrepreneurial culture. We’ve spent the past 40 years using analytics and research to help businesses understand their customers, and we work across industries in more than 80 countries with some of the largest brands in the world. We value diverse perspectives and believe that different voices and viewpoints make us stronger. We’re also proud to have a helpful and supportive culture, where we take time to celebrate accomplishments both large and small. And while we’re grounded in our rich history, we never stop searching for new approaches and tools; we were named the #1 Most Innovative Insights Firm in North America by the GRIT Report in 2019.\xa0', 'Execution platform: Apache Airflow', 'ABOUT THE ROLE', 'ABOUT US', 'With offices around the world, our 500+ teammates work across a dozen business units, collaborating with clients in entertainment and media, pharmaceuticals, technology, consumer packaged goods and more. Our experienced leadership team offers stability and structure, while our commitment to innovation fosters groundbreaking initiatives that help us improve our research approaches—like our Pragmatic Brain Science teams, who explore new psychological frameworks to better understand customer motivations.', 'Work with a variety of stakeholders to design, implement, and maintain\xa0data lake and data warehouse\xa0architecture to consolidate data from various APIs, SQL and NoSQL sources and make it\xa0accessible based on different use casesDesign and implement\xa0processes to shape and deliver data in accordance with business needs and various use casesEmploy required languages and\xa0tools to stitch a coherent system oriented toward improving data reliability, efficiency and quality', 'Employ required languages and\xa0tools to stitch a coherent system oriented toward improving data reliability, efficiency and quality', 'cloud technologies: AWS (RDS, EC2, S3, Athena, Lambda, EMR, ECS), GCP (BigQuery, DataProc), Snowflake', 'Design and implement\xa0processes to shape and deliver data in accordance with business needs and various use cases', 'Experience working with', 'LRW is swimming in data, coming from many sources.\xa0The marketing and data science team requires an experienced and all-purpose data engineer to contribute to building\xa0of data processing infrastructure\xa0and ETL pipelines to enable access to the data in efficient ways across multiple platforms.', 'The ideal candidate should be curious, self-motivated, responsive, and articulateIs interested in the core business of the company and seeks to identify the business and use implications of various solutionsSomeone who is a tenacious problem-solver who seeks to identify core bottlenecks from both a technological as well as a process oriented stand point.With deep understanding of data processing concepts and data modeling principles (difference between OLTP, data warehouse, and data lake )With advanced knowledge of SQL, Python,\xa0and BashExperience working withcloud technologies: AWS (RDS, EC2, S3, Athena, Lambda, EMR, ECS), GCP (BigQuery, DataProc), SnowflakeData platforms: PostgreSQL, MongoDB , data in different file formats (Parquet, JSON, CSV, Excel)Execution platform: Apache AirflowDistributed processing: HDFS, Spark, HiveFamiliarity deploying solutions through docker or REST APIsExperience in R and JavaScript\xa0is a plus']",Associate,Full-time,Engineering,Market Research,2021-03-18 14:34:51
Data Engineer,MasterControl,"Salt Lake City, UT",1 week ago,Be among the first 25 applicants,"['', ' Fitness clubs (you get paid to have fun and be active!)', 'Pull data from all customer databases into S3 and put into a queryable format Model the data for optimal performance (ETL, star-schema like, ORC, partitioning, etc ...) Pull data from a real-time streaming architecture (Kafka) and do near-real-time aggregations and projections of the data, storing the results in S3 Help automate the provisioning of AWS Lake Formations, EMR/Spark/S3 and other services in AWS Analyze data to find patterns worthy to expose to the end-user Help tie corporate data to customer data from an OLTP store Help us discover ways to use Big Data technologies in a Machine Learning pipeline (discover, clean, label, train, test) Other assigned duties ', ' Much, much more!', ' 100% medical premium coverage (yes, you read that right!)', ' Competitive compensation ', 'About MasterControl', 'Familiarity with AWS and Cloud Formation or Terraform', 'Ability to operate a computer and work at a desk for extended periods of time.', 'Warehouse Architectures (Star Schema/Snowflake/Vaults/Lakes)', 'Help tie corporate data to customer data from an OLTP store', ' 401(k) plan with company match', 'Ability to communicate effectively in writing, in person, over the telephone and in e-mail.', ' Onsite physician and massage therapist', 'Why Work Here?', 'ETL Best Practices', 'Responsibilities', 'The ability to non-emotionally coach and mentor others.', ' Generous PTO packages that increase with tenure', 'Model the data for optimal performance (ETL, star-schema like, ORC, partitioning, etc ...)', 'Apache Spark with EMR and/or other Big Data tools', 'Analyze data to find patterns worthy to expose to the end-user', 'Other assigned duties', 'Preferred Skills', 'Must be able to work well with people. Ability to operate a computer and work at a desk for extended periods of time. Ability to communicate effectively in writing, in person, over the telephone and in e-mail. ', 'Physical Demands and Working Conditions', 'Here Are Some Of The Benefits MasterControl Employees Enjoy', 'Airflow experience is a plus', ' Company parties and employee recognition programs', 'Summary', 'Help automate the provisioning of AWS Lake Formations, EMR/Spark/S3 and other services in AWS', 'Passionate about creatively solving business problems.', 'Must be able to work well with people.', 'Data Modelling', ' Wellness programs (free Fitbit, gym membership and athletic shoe reimbursements, etc.)', ' Employer paid life insurance policy ', 'Meet multiple, challenging deadlines while communicating expectations clearly.', ' Competitive compensation   100% medical premium coverage (yes, you read that right!)  401(k) plan with company match  Generous PTO packages that increase with tenure  Schedule flexibility  Fitness clubs (you get paid to have fun and be active!)  Company parties and employee recognition programs  Wellness programs (free Fitbit, gym membership and athletic shoe reimbursements, etc.)  Onsite physician and massage therapist  Innovation center and gaming rooms at the office  Dental/vision plans  Employer paid life insurance policy   Much, much more! ', ""The ability to not participate in 'techno-arrogant' conversation."", ' Dental/vision plans', ""Scala/Python/SQL/Java Apache Spark with EMR and/or other Big Data tools Kafka Streams experience is big plus Airflow experience is a plus Big Data Mindset (Spark/Hive/Hadoop/HCatalog/Hudi). Understanding of the Big Data landscape.  Warehouse Architectures (Star Schema/Snowflake/Vaults/Lakes) Familiarity with AWS and Cloud Formation or Terraform Data Modelling ETL Best Practices Passionate about creatively solving business problems. Effectively prioritize and execute tasks in a high-pressure environment. The ability to non-emotionally coach and mentor others. The ability to not participate in 'techno-arrogant' conversation. Meet multiple, challenging deadlines while communicating expectations clearly. "", 'Pull data from a real-time streaming architecture (Kafka) and do near-real-time aggregations and projections of the data, storing the results in S3', 'Help us discover ways to use Big Data technologies in a Machine Learning pipeline (discover, clean, label, train, test)', 'Pull data from all customer databases into S3 and put into a queryable format', ' Schedule flexibility', 'Scala/Python/SQL/Java', 'Effectively prioritize and execute tasks in a high-pressure environment.', 'Kafka Streams experience is big plus', 'Big Data Mindset (Spark/Hive/Hadoop/HCatalog/Hudi). Understanding of the Big Data landscape. ', ' Innovation center and gaming rooms at the office']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer ,ClearBridge Technology Group,"Burlington, MA",6 days ago,102 applicants,"['', 'Required Skills', 'o\xa0\xa0\xa0Experience with data engineering with 1st\xa0party data (Direct to consumer)', 'o\xa0\xa0\xa0Ability to understand the business goals of interpreting first party data for a consumer goods company', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Prior experience working with Tableau', 'o\xa0\xa0\xa0This includes machine learning data engineering', 'Our client in Burlington MA is in need of a team of Data Engineers for a 12 month contract with the strong possibility of going longer and potentially full time to work remotely (then onsite after Covid19 restrictions are lifted). The Data Engineers will work within our customer’s Data Science team which focuses on building first party data pipelines from Azure Cloud. Day to day responsibilities will include building data models and adding more features to the models. The Data Engineers will create data tables and new features to help better describe our client’s customers. They will build pipelines to which they will develop scoring through data science services to enhance our client’s customer’s onsite experience. The data engineers will need to know intuitively the business goals of this group. Develop code (Python) to take data from various data streams, data lakes, and supporting data sources and craft a unified dimensional or star schema data model for ML, analytics, and reporting', 'o\xa0\xa0\xa0Demonstrated experience enhancing data pipelines by developing new data tables and data models', 'o\xa0\xa0\xa0Basic understanding of data science and its importance to a consumer goods company', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience working with Azure Cloud as the primary data source', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa03 or more years of experience designing data systems particularly in Python, as well as their support infrastructure (shell scripts, job schedulers, message queues, etc.).', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Minimum of 3-4 years of data engineering experience']",Mid-Senior level,Contract,Project Management,Information Technology and Services,2021-03-18 14:34:51
Data Engineer III,GHX,"Louisville, CO",3 days ago,Be among the first 25 applicants,"['', 'Required Education, Certifications, And Experience', ' Lead and contribute to backend and ETL development effort of our data platform. Lead and provide hands-on new development as well as enhancement of existing data processes Design, maintain, and tune extraction, transformation, and load (ETL) processes using PL/SQL, SQL, Python, or Spark Provide architectural guidance and development/build standards for the team Promote collaboration through activities including design sessions, design reviews, and pair programming, etcetera ', 'Strong demonstrable SQL and Python skills', ' EC2 S3 Athena Redshift Aurora MySQL RDS Lambda Step Functions Glue ', 'RDS', ' Develop and maintain data engineering solutions for the enterprise data platform Analyze business requirements and work with teammates to formulate supporting design and design documentation Other duties as assigned ', 'Analyze business requirements and work with teammates to formulate supporting design and design documentation', 'Demonstrated organizational, prioritization, and time management skills', 'Glue', 'Ability to think strategically', '5+ years working in an agile development environment', 'Lead and contribute to backend and ETL development effort of our data platform.', 'High-level written and verbal communication skills', 'Step Functions', '5+ years of data engineering experience building business intelligence applications with exceptional SQL, PL/SQL, and/or Python skills', 'GHX expressly prohibits any form of unlawful employee harassment based on race, color, religion, gender, sexual orientation, national origin, age, disability, or veteran status. Improper interference with the ability of GHX’s employees to perform their expected job duties is absolutely not tolerated.', 'Ability to communicate technical concepts and designs to cross-functional and offshore teams who have varying levels of technical experience', 'Promote collaboration through activities including design sessions, design reviews, and pair programming, etcetera', 'Ability and willingness to travel nationally to remote offices and partners approximately 10% of the time', 'Other duties as assigned', 'Technical writing experience in relevant areas, including queries, reports, and presentations', 'Provide architectural guidance and development/build standards for the team', 'Ability to adapt to changing conditions and lead others through change', 'Lambda', 'Experience developing in Snowflake', 'EC2', ' Bachelor’s degree in Computer Science, Mathematics, or Statistics 5+ years of data engineering experience building business intelligence applications with exceptional SQL, PL/SQL, and/or Python skills 5+ years of experience of ETL development in a big data environment 5+ years working in an agile development environment Technical writing experience in relevant areas, including queries, reports, and presentations ', 'GHX provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, gender, sexual orientation, national origin, age, disability, marital status, amnesty, or status as a covered veteran in accordance with applicable federal, state and local laws. GHX complies with applicable state and local laws governing non-discrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment, including, but not limited to, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training.', 'Aurora MySQL', 'Thorough understanding of, and support for, Agile development methodologies', ' Application, system or data architecture experience Machine learning experience ', 'Ability to design, collect, and analyze large datasets', 'Design, maintain, and tune extraction, transformation, and load (ETL) processes using PL/SQL, SQL, Python, or Spark', 'Bachelor’s degree in Computer Science, Mathematics, or Statistics', 'Disclaimer', 'Lead and provide hands-on new development as well as enhancement of existing data processes', 'Athena', '5+ years of experience of ETL development in a big data environment', 'Machine learning experience', 'Develop and maintain data engineering solutions for the enterprise data platform', 'Attention to detail', 'Proven data engineering, problem solving, and analysis skills', ' Thorough understanding of, and support for, Agile development methodologies Ability to design, collect, and analyze large datasets Ability to communicate technical concepts and designs to cross-functional and offshore teams who have varying levels of technical experience Proven data engineering, problem solving, and analysis skills Strong demonstrable SQL and Python skills High-level written and verbal communication skills Ability to think strategically Ability to adapt to changing conditions and lead others through change Analytical and problem-solving ability and orientation Demonstrated organizational, prioritization, and time management skills Attention to detail Ability and willingness to travel nationally to remote offices and partners approximately 10% of the time ', 'Development experience with Python, PySpark, or R', 'Analytical and problem-solving ability and orientation', 'S3', 'GHX: It’s the way you do business in healthcare', 'Experience in a diverse set of Amazon Web Services data services including:', 'Redshift', ' Experience in a diverse set of Amazon Web Services data services including:', 'Preferred Qualifications', 'Application, system or data architecture experience']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Lead Data Engineer,Target,"Minneapolis, MN",3 weeks ago,Be among the first 25 applicants,"['', 'Americans With Disabilities Act (ADA)', 'Description', 'https://corporate.target.com/careers/']",Associate,Full-time,Information Technology,Retail,2021-03-18 14:34:51
Data Engineer,Capital TechSearch,"Bethesda, MD",,N/A,"['', 'Docker', 'Apache Spark', 'High competency with Python', 'Assess, design, and implement data engineering and ETL solutions using Python, Spark, and AWS', 'Experience implementing Machine Learning applications with Python libraries such as scikit-learn', '4+ years of software development experienceHigh competency with PythonSQL-based Data Warehouse services (Amazon Redshift, Snowflake, etc)Apache SparkAgile Scrum', '\ufeffExpertise in the following is a plus:', 'Required Expertise:', '4+ years of software development experience', 'NodeJS', 'Agile Scrum', 'Kubernetes', 'Perform development work training and evaluating Machine Learning models.', 'Contribute your vision to new and existing data products and initiatives.', 'Assess, design, and implement data engineering and ETL solutions using Python, Spark, and AWSContribute your vision to new and existing data products and initiatives.Perform development work training and evaluating Machine Learning models.', 'SQL-based Data Warehouse services (Amazon Redshift, Snowflake, etc)', 'Data Engineer', 'The ideal candidate has real-world experience performing data engineering with commercial data products.', 'AWS', 'In this role you will join a small team of engineers creating and implementing unique data products and applications for the commercial retail apparel industry.', '\xa0Required Expertise:', 'Responsibilities Include:', 'AWSDockerKubernetesNodeJSExperience implementing Machine Learning applications with Python libraries such as scikit-learn']",Mid-Senior level,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Engineer,A Cloud Guru,United States,3 weeks ago,29 applicants,"['', 'Knowledge of software engineering best practices with experience with implementing CI/CD, monitoring & alerting for production systems', ""We’re not a training company that just decided to sell training courses. We grew up out of the cloud ecosystem. We were a bunch of cloud engineers who pulled people together to create a training platform. That’s why we’re genuinely passionate about what we create. And we are known for practicing what we preach. We’ve built a product using cloud-first Serverless Architecture with tools like Lambda, API Gateway, GraphQL and ReactJS. All that aside, we're a friendly, down-to-earth, and collaborative group. There are no high-performing jerks and no heroes. Just great teams."", '4 weeks PTO, plus 10 sick days, and holidays. Because even when your office is your living room, we all need time to unplug.', '\ufeffWhat’s the interview process like at ACG?', 'Coach and mentor other team members', 'Remotely awesome.', 'As a Data Engineer at A Cloud Guru, you will ensure the data platform infrastructure and architecture supports the evolving requirements of the Data Engineering and Data Analytics teams as well as other parts of our business! You will work closely with the Director of Data Engineering to develop a strategy for our long term Data Platform architecture to identify gaps in the data processes and drive improvements while mentoring and coaching other team members. Thanks to your contributions, our data platform will continue to optimize and revolutionize. This role reports to the Director, Data Engineering.', 'Gender-neutral paid parental leave.', 'Be an essential part of designing and building ACG’s new data platform, as we evolve the existing databases into a cutting-edge solution to meet the needs of our 2021 data plans and beyond', '2+ years of Data Engineering, Data Warehousing, or related experience2+ years of development experience with Python or similar scripting language2+ years of SQL experience, including experience with schema design and dimensional data modellingExperience working with AWS services such as DynamoDB, Glue, Lambda, Step Functions, S3, CloudFormation or RedshiftExperience with ETL development, metadata management, and data qualityKnowledge of software engineering best practices with experience with implementing CI/CD, monitoring & alerting for production systemsExperience with complex data structures and No-SQL databasesExperience with open source orchestration platforms (e.g. Airflow)', 'We want the people who care about doing a good job. The ones who have the humility and hunger to learn. - Sam Kroonenburg, Co-Founder and CEO', '2+ years of Data Engineering, Data Warehousing, or related experience', '4 weeks PTO, plus 10 sick days, and holidays. Because even when your office is your living room, we all need time to unplug.Remotely awesome. Get $500 to level up your home office, monthly snack boxes, free Headspace access, weekly lunch funds, and $50 monthly for internet.Human connection. Get to know the Gurus with good times and get-togethers inspired by our values, virtual happy hours, lunchtime trivia, or a socially distanced drive-in movie.Gender-neutral paid parental leave.\xa0Expanding your family? We offer 12 weeks of gender-neutral paid parental leave, and reimburse up to $10,000 for eligible adoption expenses.$1,000 continuing education budget. All Gurus get $250 a quarter to spend on personal development, and 2 hours each week reserved for learning something new.', 'Be an essential part of designing and building ACG’s new data platform, as we evolve the existing databases into a cutting-edge solution to meet the needs of our 2021 data plans and beyondExplore and contribute to discussions around technologies under consideration, such as Snowflake, Kappa/Lambda architecture, Delta Lakes and Data VaultDevelop, test and maintain existing architecture, including databases, data pipelines and large-scale processing systemsCollaborate with the Analytics team on transformation processes to populate data modelsRecommend ways to improve data reliability, efficiency and quality of the data platform and optimize for performance, scalability and costDiscover opportunities for data acquisition and explore new ways of using existing dataIdentify gaps in data processes and drive improvementsCoach and mentor other team members', ""You'll do well at ACG if you're open to learning and trying new things, and you like to be surrounded by other friendly, passionate and driven people.\xa0–Natasja, Makeup Guru (and Software Developer)"", 'What you bring to the table', ""We focus on hiring values-aligned people, because we believe the right person can learn all the things to be successful in their role.\xa0Self-belief plays a big part in what you apply for. We encourage all job applicants to apply even if they are nervous to do so.\xa0Uni degrees aren't required for any roles, and career gaps or switches are totally welcome."", ""There aren't many company cultures like A Cloud Guru's in the world.\xa0This year, we were awarded the #1 Place to work in Austin, as well as Best Company Culture and Best Companies for Diversity."", 'Where you work isn’t just a career decision — it’s a life decision. Everyone has family, friends and interests outside of their careers, so we offer perks and benefits to make work, work better for you.', 'Human connection. Get to know the Gurus with good times and get-togethers inspired by our values, virtual happy hours, lunchtime trivia, or a socially distanced drive-in movie.', '2+ years of development experience with Python or similar scripting language', 'COLLABORATE | OPTIMIZE | EVOLVE', 'A Cloud Guru was built by engineers for everyone, everywhere. Here, you’ll have the freedom to follow your curiosity. We’re not afraid to just try, because when you’re working with cutting edge technologies, experimentation and trying out new ideas have to be encouraged and celebrated.\xa0Our engineers are building the world’s largest (and most awesome) cloud learning platform. Why? Our mission is to teach the world to cloud. Our fun, practical courses have helped over 2 million people learn to cloud, and we’re just getting started.', '$1,000 continuing education budget.', 'Explore and contribute to discussions around technologies under consideration, such as Snowflake, Kappa/Lambda architecture, Delta Lakes and Data Vault', 'Experience with ETL development, metadata management, and data quality', 'Experience working with AWS services such as DynamoDB, Glue, Lambda, Step Functions, S3, CloudFormation or Redshift', 'Recommend ways to improve data reliability, efficiency and quality of the data platform and optimize for performance, scalability and cost', 'The Data Engineer role\xa0', ' Our mission is to teach the world to cloud. ', ""Hello, we're A Cloud Guru"", 'Our friends call us ACG.', 'Human connection.', 'Develop, test and maintain existing architecture, including databases, data pipelines and large-scale processing systems', 'Gender-neutral paid parental leave.\xa0Expanding your family? We offer 12 weeks of gender-neutral paid parental leave, and reimburse up to $10,000 for eligible adoption expenses.', 'As a Data Engineer at ACG, you’ll get to:', '2+ years of SQL experience, including experience with schema design and dimensional data modelling', 'Discover opportunities for data acquisition and explore new ways of using existing data', 'Identify gaps in data processes and drive improvements', 'Collaborate with the Analytics team on transformation processes to populate data models', 'Applying for a job can feel intimidating and like a full-time job of its own. You shouldn’t have to burn through a week of sick time or all your best out-of-office excuses just to put feelers out for a new career opportunity. We want to be as transparent about the process as possible to help ease your mind. It’s our goal to provide you a fair, efficient interviewing experience that respects you and your time — and to do it all with a sidecar of delight.', 'Experience with open source orchestration platforms (e.g. Airflow)', '4 weeks PTO, plus 10 sick days, and holidays.', '$1,000 continuing education budget. All Gurus get $250 a quarter to spend on personal development, and 2 hours each week reserved for learning something new.', 'More than a job', ""Once you submit an application, we’ll review it.\xa0If you’re a good fit, you’ll have an initial chat with a recruiter over the phone. A phone interview with a manager typically follows. Depending on your role, you might then be asked to do a little homework (but nothing too time consuming). Then we’ll schedule a Zoom call to meet other members of the team, answer any questions you have, and give you a feel for what it’s really like to work at ACG. If you're on the fence, just give it a try."", 'What makes the Engineering team awesome...', 'Experience with complex data structures and No-SQL databases', 'Remotely awesome. Get $500 to level up your home office, monthly snack boxes, free Headspace access, weekly lunch funds, and $50 monthly for internet.']",Mid-Senior level,Full-time,Education,E-Learning,2021-03-18 14:34:51
Data Engineer,Guy Carpenter,"New York, NY",10 hours ago,41 applicants,"['', 'Familiarity with modern data productivity frameworks and their alternatives such as Databricks, DataRobot, and Alteryx ', 'Extensive experience integrating data from semi-structured ', '3-5 years of relevant experience as a data engineer or in a similar role ', 'Own the role of data steward for a variety of high value datasets and implement innovative quality assurance practices ', 'Strong understanding of entity resolution, streaming technologies, and ELT/ETL frameworks Ability to articulate the advantages of various cloud and on-premises deployment options Experience with Master Data ManagementExperience with web scraping and crowd sourcing technologies Familiarity with modern data productivity frameworks and their alternatives such as Databricks, DataRobot, and Alteryx Experience with the MS Azure cloud environment, including ARM template deployments Strong knowledge of CI/CD principles and practical experience with a CI/CD technology (Azure Devops, GitLab, Travis, Jenkins) ', 'Strong understanding of entity resolution, streaming technologies, and ELT/ETL frameworks ', 'Strong knowledge of CI/CD principles and practical experience with a CI/CD technology (Azure Devops, GitLab, Travis, Jenkins) ', 'Experience with Master Data Management', 'Good ability to prioritize workload according to volume, urgency, etc. and to deliver on required projects in a timely fashion ', 'We will count on you to:', 'Experience with web scraping and crowd sourcing technologies ', 'Establish and implement metadata management standards and capabilities, including lineage mapping ', 'Experience with the MS Azure cloud environment, including ARM template deployments ', 'What can you expect?', ""3-5 years of relevant experience as a data engineer or in a similar role Bachelor’s or master's degree in data science, computer science or related quantitative field such as applied mathematics, statistics, engineering, or operations research Extensive experience with Spark, Python, and SQL Extensive experience integrating data from semi-structured Experience deploying/maintaining cloud resources (AWS, Azure, or GCP) Knowledge of various industry-leading SQL and NoSQL database systems Experience working in an Agile environment to facilitate the quick and effective fulfillment of group goals Good interpersonal skills for establishing and maintaining good internal relationships, working well as part of a team and for presentations and discussions Strong analytical skills and intellectual curiosity as demonstrated through academic experience or work assignments Good ability to prioritize workload according to volume, urgency, etc. and to deliver on required projects in a timely fashion "", 'Enforce strong development standards across the team through code reviews, unit testing, and monitoring ', 'Experience working in an Agile environment to facilitate the quick and effective fulfillment of group goals ', 'Extensive experience with Spark, Python, and SQL ', 'Strong analytical skills and intellectual curiosity as demonstrated through academic experience or work assignments ', 'What makes you stand out?', 'Innovate new ways to leverage enormous amounts of various datasets to drive revenues via the development of new products with the Data Strategy team, as well as the enhanced delivery of existing products ', ""Bachelor’s or master's degree in data science, computer science or related quantitative field such as applied mathematics, statistics, engineering, or operations research "", ""Keep up-to-date on the latest trends and innovation in data technology and how these trends apply to GC's business and data strategy "", 'Perform basic data analysis within Jupyter Notebooks to validate the fulfillment of requirements for data pipelines ', 'Consume data from a variety of sources (relational DBs, APIs, NetApp and other cloud storage, FTPs) & formats (excel, CSV, XML, parquet, unstructured)) ', 'Ability to articulate the advantages of various cloud and on-premises deployment options ', ""Develop, implement, and deploy custom data pipelines powering machine learning algorithms, insights generation, client benchmarking tools, business intelligence dashboards, reporting and new data products. Innovate new ways to leverage enormous amounts of various datasets to drive revenues via the development of new products with the Data Strategy team, as well as the enhanced delivery of existing products Consume data from a variety of sources (relational DBs, APIs, NetApp and other cloud storage, FTPs) & formats (excel, CSV, XML, parquet, unstructured)) Construct and maintain data pipelines between GC’s databases, and other sources, with the data lake utilizing modern ETL frameworks Own the role of data steward for a variety of high value datasets and implement innovative quality assurance practices Establish and implement metadata management standards and capabilities, including lineage mapping Enforce strong development standards across the team through code reviews, unit testing, and monitoring Perform basic data analysis within Jupyter Notebooks to validate the fulfillment of requirements for data pipelines Evangelize data strategy techniques and best practices throughout global strategic advisory Keep up-to-date on the latest trends and innovation in data technology and how these trends apply to GC's business and data strategy "", 'Develop, implement, and deploy custom data pipelines powering machine learning algorithms, insights generation, client benchmarking tools, business intelligence dashboards, reporting and new data products. ', 'Data Engineer', 'Good interpersonal skills for establishing and maintaining good internal relationships, working well as part of a team and for presentations and discussions ', 'Knowledge of various industry-leading SQL and NoSQL database systems ', 'What You Need to Have:', ' R_107255', 'Evangelize data strategy techniques and best practices throughout global strategic advisory ', 'Experience deploying/maintaining cloud resources (AWS, Azure, or GCP) ', 'Construct and maintain data pipelines between GC’s databases, and other sources, with the data lake utilizing modern ETL frameworks ']",Not Applicable,Full-time,Information Technology,Insurance,2021-03-18 14:34:51
Jr. Data Engineer,Christian Brothers Automotive Corporation,"Houston, TX",4 weeks ago,Over 200 applicants,"['', 'Python programming knowledge preferred, but not required', 'Collaborate with the data engineering team to connect and maintain data sources into our Domo instance.Create, edit and maintain ETL/ELTs to transform input datasets to desired output datasets.Connections often used include, but are not limited to, Quickbooks files, JSON, CSV, MySQL, APIs and much more.Administer data loading tool Domo Workbench which houses 1000+ jobs.Administer user provisioning to Domo instance and OneLogin for the organizationOversee billing and training for users of our Domo instance.Monitor, maintain and periodically troubleshoot data integrity issues that occur on our internal sales reporting web application.Validate and QA data to attest for accuracy.Generate periodic reports or dashboards for executive leadership.Assist on various duties as directed by the management.', 'Create, edit and maintain ETL/ELTs to transform input datasets to desired output datasets.', 'Proficient in Email, Calendar and task tracking software', '1-2 years of experience working with SQL databases', 'Highly organized, efficient and agile in tasks amidst a busy environment', 'BA/BS is preferred, but not required', 'Work Hours', 'Proficient in SQL/MySQL query writing, comfortable creating database schemas and have an understanding of data transformation.Demonstrated ability to learn and adapt to new software and technologyStrong written and verbal communication and technical skillsHighly organized, efficient and agile in tasks amidst a busy environmentResponsible, dependable, quick learnerUpbeat, fast paced, energetic, team-orientedAble to prioritize effectively and act/delegate accordinglyCan stop/start multiple tasks and pick up where left off quicklyThe ability to handle a high volume of work in a fast paced environment', 'Experience in using BI tools: Domo, Tableau, PowerBI, or equivalent', 'Generate periodic reports or dashboards for executive leadership.', 'Able to prioritize effectively and act/delegate accordingly', 'Physical Demands', 'Demonstrated ability to learn and adapt to new software and technology', '1-2 years of experience working with SQL databasesExperience in using BI tools: Domo, Tableau, PowerBI, or equivalentPython programming knowledge preferred, but not requiredProficient in web tools, Microsoft or Google office suites Proficient in Email, Calendar and task tracking softwareBA/BS is preferred, but not required', 'Accountabilities', 'Monitor, maintain and periodically troubleshoot data integrity issues that occur on our internal sales reporting web application.', 'Upbeat, fast paced, energetic, team-oriented', 'Working at a traditional office workstation for 8 hour daysOperating standard office computers and telecommunications equipment', 'Administer data loading tool Domo Workbench which houses 1000+ jobs.', 'Company Description', 'Administer user provisioning to Domo instance and OneLogin for the organization', 'Strong written and verbal communication and technical skills', 'Working beyond “standard” hours as the need arises', 'Connections often used include, but are not limited to, Quickbooks files, JSON, CSV, MySQL, APIs and much more.', 'Can stop/start multiple tasks and pick up where left off quickly', 'Assist on various duties as directed by the management.', 'Workplace Environment', 'The ability to handle a high volume of work in a fast paced environment', 'Operating standard office computers and telecommunications equipment', 'Collaborate with the data engineering team to connect and maintain data sources into our Domo instance.', 'Working at a traditional office workstation for 8 hour days', 'Education & Experience', 'Job Description', 'Responsible, dependable, quick learner', 'Proficient in SQL/MySQL query writing, comfortable creating database schemas and have an understanding of data transformation.', 'Validate and QA data to attest for accuracy.', 'Oversee billing and training for users of our Domo instance.', 'Proficient in web tools, Microsoft or Google office suites ']",Associate,Full-time,Information Technology,Construction,2021-03-18 14:34:51
Data Engineer,"The Mice Groups, Inc.","Scottsdale, AZ",1 week ago,190 applicants,"['', 'Ten or more years of relevant related experience', 'Experience in development / operationalization of Artificial Intelligence / Machine Learning Models / Model development life cycle activities (implementing feature engineering, data pipelines, model operationalization, model monitoring).', 'Working conditions consist of a normal office environment. Work is primarily sedentary and requires extensive use of a computer and involves sitting for periods of approximately four hours. Work may require occasional standing, walking, kneeling and reaching. Must be able to lift 10 pounds occasionally and/or negligible amount of force frequently. Requires visual acuity and dexterity to view, prepare, and manipulate documents and office equipment including personal computers. Requires the ability to communicate with internal and/or external customers.', 'Big Data Engineering (6-9 mo. contract with possibility to hire)', 'Experience designing/developing scalable systems.', 'Participate in the strategic development of methods, techniques, and evaluation criteria for projects and programs.', 'Experience with event-driven architecture and messaging frameworks (Pub/Sub, Kafka, RabbitMQ, etc).', 'Knowledge of Software Development Lifecycle (SDLC) best practices, software development methodologies (Agile, Scrum, LEAN etc) and DevOps practices.', 'Drive all aspects of technical and data architecture, design, prototyping and implementation in support of both product needs as well as overall technology data strategy.', 'The above job description is not intended to be an all-inclusive list of duties and standards of the position.', 'Collaborate and partner with product managers, designers, and other engineering groups to conceptualize and build new features and create product descriptions.', 'Experience with implementing data science solutions using Python, Spark, PySpark, R, Data Robot.', 'MS or PHD', 'Minimum Qualifications', '\ufeff', 'Recent Big Data skills in: Hadoop, Kafka, Spark and/or Scala', 'Monitoring and Alerting systems experience (AppDynamics)', 'Knowledge of ACH/EFT', 'Background and drug screen.', 'Essential Functions', 'Knowledge of mature engineering practices (CI/CD, testing, secure coding, etc).', 'Employee must be able to perform essential functions and physical requirements of position with or without reasonable accommodation.', 'Knowledge of Aerospike, Scality S3, Elastic Search', 'Working experience with cloud infrastructure (Google Cloud Platform, AWS, Azure, etc).', 'Represent engineering in cross-functional team sessions and able to present sound and thoughtful arguments to persuade others. Adapts to the situation and can draw from a range of strategies to influence people in a way that results in agreement or behavior change.', 'Assist Support and Operations teams in identifying and quickly resolving production issues.', 'Database platforms (Oracle, SQL Server)', 'Kubernetes experience', 'Provide leadership and technical expertise in support of building a technical plan and backlog of stories, and then follow through on execution of design and build process through to production delivery.', 'Two or more years of experience in development / operationalization of Artificial Intelligence / Machine Learning Models / Model development life cycle activities (implementing feature engineering, data pipelines, model operationalization, model monitoring).', 'Actively seek out ways to improve engineering and data standards, tooling, and processes.', 'Big Data Platforms (Cloudera, S3)', 'Knowledge of real time payment networks (RTP, FedNow)\xa0', 'Education and/or experience typically obtained through a Bachelor’s degree in computer science or related technical field.', 'Demonstrated experience in delivering business-critical systems to the market.', 'Seven or more years of experience in the development of complex data platform, distributed systems, SaaS, cloud solutions, micro services.', 'Computer language experience (Python, PySpark, and R)', 'Develop and implement tests for ensuring the quality, performance, and scalability of our application.', 'Ability to influence and work in a collaborative team environment.', 'Overall Purpose', 'This position is a key role in the development, test, and deployment of complex solutions.', 'Guide a broad functional area and lead efforts through the functional team members along with the team’s overall planning.', 'Six or more years of experience in the development of Data Warehouse, Big Data – structured & unstructured platforms, real-time & batch processing, data standards.', 'Physical Requirements', 'Supporting the company’s commitment to risk management and protecting the integrity and confidentiality of systems and data.', 'Experience using AI/ML Model Frameworks like Tensorflow, Sage Maker, Scikit, PyCharm', '\xa0', 'Four or more years of experience in development of Business Intelligent Solutions', 'Actively own features or systems and define their long-term health, while also improving the health of surrounding systems.', 'Build data strategy for broad or complex requirements with insightful and forward-looking approaches that go beyond the direct team and solve large open-ended problems.\xa0', 'FinTech experience', 'Preferred Qualifications', '\xa0\xa0\xa0\xa0\xa0\xa0Preferred Qualifications', 'Extensive experience implementing Data Warehouse (Star / Snow flake schemas) using SQL Server or equivalent, Big Data – HDFS, Elastic Search, ETL process development using IBM Infosphere or equivalent, Reusable Frameworks']",Mid-Senior level,Contract,Engineering,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,AirDNA,"Denver, CO",2 weeks ago,45 applicants,"['', 'Build, maintain, and monitor data pipelinesCluster managementHelp design and build out the next generation of our data platformCraft and optimize data pipelines using SparkImplement data and application integrations with key industry partnersWork closely with our Data Science team to develop new products and scale out algorithmsMaintain and migrate legacy data systems', "" Here's What You'll Get To Do "", 'Implement data and application integrations with key industry partners', 'Work closely with our Data Science team to develop new products and scale out algorithms', '3+ years of data engineering experience', '3+ years of data engineering experienceStrong experience with SparkFamiliarity with Scala (or similar JVM language)Experience building composable and stable ETL systemsPassionate about providing well documented and user friendly data sourcesDemonstrated successful experience working in a hands-on, fast-paced, creative, entrepreneurial environmentBS or MS in Computer Science/Engineering preferred', 'Strong experience with Spark', 'Build, maintain, and monitor data pipelines', 'Meaningful equity upside—we are a small team with no outside investors and want every hire to share in our long term financial success', 'Maintain and migrate legacy data systems', 'Experience building composable and stable ETL systems', 'Passionate about providing well documented and user friendly data sources', 'BS or MS in Computer Science/Engineering preferred', 'Familiarity with Scala (or similar JVM language)', 'Cluster management', 'Competitive cash compensation and benefits, the salary range for this position is $110,000 - $140,000Meaningful equity upside—we are a small team with no outside investors and want every hire to share in our long term financial successWe have a great LoDo office space, a short walk from the Union Station light-rail stop; enjoy coffee, tea, beer, snacks, bike parking and great food and drink nearby', 'About AirDNA ', 'Competitive cash compensation and benefits, the salary range for this position is $110,000 - $140,000', 'Help design and build out the next generation of our data platform', 'Craft and optimize data pipelines using Spark', 'Demonstrated successful experience working in a hands-on, fast-paced, creative, entrepreneurial environment', 'We have a great LoDo office space, a short walk from the Union Station light-rail stop; enjoy coffee, tea, beer, snacks, bike parking and great food and drink nearby']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Data Engineer,Accenture Federal Services,"San Antonio, Texas Metropolitan Area",1 week ago,38 applicants,"['', 'Experience working within a hybrid environment.', 'We are:\xa0\xa0', 'Accenture Federal Services is committed to providing veteran employment opportunities to our service men and women.\xa0', 'best on the block', 'Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.\xa0', 'ratings', 'Experience with 2 of 3 - Java, Scala, and Python programming language.\xa0For example, utilizing Java to build data sorting algorithms and machine learning sequences.', 'Experience Leading small teams of engineers, data analysts. Collaborating with data scientists, and data visualizers to bring powerful analytical solutions and insights to our clients', 'You are:', 'Important Information', ""Here's what you need:\xa0"", 'Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.\xa0', 'Must be a U.S. Citizen; no Dual Citizenship', 'Accenture Federal Services is an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.\xa0', ""Professional Cloud Data Engineer certification in AWS, Azure or GCPExperience with 2 of 3 - Java, Scala, and Python programming language.\xa0For example, utilizing Java to build data sorting algorithms and machine learning sequences.Must be conversant with cloud native CLIs for various data engineering related activities such as setting up compute and storage resources, IAM and ETL.Experience working within a hybrid environment.Bachelor's Degree"", 'showstopping\xa0', 'Equal Employment Opportunity:\xa0All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.\xa0', 'An active security clearance or the ability to obtain one may be required for this role.', 'Experience in the full software development lifecycle, utilizing emerging technologies and big data design principles in developing data pipelines, interfaces, and architecture to support big data and analytics initiativesExperience Leading small teams of engineers, data analysts. Collaborating with data scientists, and data visualizers to bring powerful analytical solutions and insights to our clients', 'Minimum of 3 Years’\xa0experience with the following:', 'viewers', 'stay tuned', 'Location: San Antonio, TX area', 'SQL and/or NOSQL databases to organize the collection, processing, and storing of data from different sources.', 'Cloud Technologies (Data Lake, Azure, Google, AWS etc.) or with open-source technologies (Spark, Kafka, Presto, Hive, Cassandra etc.) Will ensure that the data they extract is valid, assisting in documenting any requirements, and resolving ambiguities in the data.', 'Data Engineering or Big Data Technologies, or Data Transformation, and modelingCloud Technologies (Data Lake, Azure, Google, AWS etc.) or with open-source technologies (Spark, Kafka, Presto, Hive, Cassandra etc.) Will ensure that the data they extract is valid, assisting in documenting any requirements, and resolving ambiguities in the data.SQL and/or NOSQL databases to organize the collection, processing, and storing of data from different sources.Experience in architecting and building scalable data platformsMust be a U.S. Citizen; no Dual Citizenship', 'Data Engineering or Big Data Technologies, or Data Transformation, and modeling', 'We are:', 'fixer-upper', 'Organization: Accenture Federal Services', '\xa0\xa0', ""Accenture\xa0Federal Services, helping our federal clients tackle their toughest challenges while unleashing their fullest potential…and then some. What makes our approach so unique? Operating from the nation’s capital, we bring together commercial innovation and leading-edge technologies to deliver an integrated and interactive experience that far exceeds expectations. How? Our passion meets purpose! Through our diverse culture and inclusive thinking, we embrace our employees' ideas taking them from concept to practical solutions. Not to mention, we sleep well at night knowing our work directly impacts and improves the way the world works. We keep our tech smarts sharp by providing abundant training and certification opportunities. Are you ready to learn and grow in a career, while making a difference?"", 'Experience in architecting and building scalable data platforms', ""Bachelor's Degree"", 'Experience in the full software development lifecycle, utilizing emerging technologies and big data design principles in developing data pipelines, interfaces, and architecture to support big data and analytics initiatives', 'Must be conversant with cloud native CLIs for various data engineering related activities such as setting up compute and storage resources, IAM and ETL.', '\xa0', 'Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture Federal Services.\xa0', 'Bonus Points:', 'The work:', 'Professional Cloud Data Engineer certification in AWS, Azure or GCP', 'Federal Data Engineer', 'The Chip and Joanna\xa0Gaines\xa0at developing data pipelines. Like turning a\xa0fixer-upper\xa0house into the\xa0best on the block, you turn complicated datasets into useful,\xa0showstopping\xa0formats. You live in the cloud and open-source world and enjoy working with your team to bring all the pieces of the data story together, keeping\xa0ratings\xa0high and\xa0viewers\xa0wowed. What does a good time look like to you? Hanging out on the reddit big data feeds and chatting about the latest advances in machine learning and AI. And of course, you always\xa0stay tuned\xa0for what is coming next.']",Associate,Full-time,Consulting,Information Technology and Services,2021-03-18 14:34:51
Senior Data Engineer - Austin,Saatva,"Austin, TX",5 days ago,Be among the first 25 applicants,"['', 'Personal Characteristics (Who You Are)', 'Competence with star schema / snowflake schema design', 'Design schemas for efficient storage and query execution', 'A passion for technology and making a real impact', 'Participate in Agile Scrum planning, daily stand-ups, and sprint retrospectives', 'About Us', 'Values creating collaborative relationships across your entire team and organizationAccepts difference of opinions, creativity, and has confidence in your own ideasA passion for technology and making a real impactResults and action oriented, willing to go that extra mile for the best outcome', 'Familiarity with ETL software (i.e. Talend, CloverETL, Pentaho, Informatica or Airflow)', 'Expertise in Python for developing and maintaining data pipeline code', 'Experience with Apache Spark and PySpark library', 'Strong communication skills and ability to work directly with data analytics teams', 'Familiarity with BI software (preferably Tableau or Metabase)', 'Job Requirements (What You Have)', 'Computer Science Degree', 'Enhance data models by developing integrations with business partners', 'Affirmative Action/Diversity Inclusion At Whitestone Home Furnishings, LLC', 'Our company is small yet developing at a rapid pace, unlocking exciting new doors along with personal/professional exponential growth for our team as a whole', 'Build the infrastructure required for the aggregation of data from a variety of sources', 'Values creating collaborative relationships across your entire team and organization', 'Create analytics tools that utilize the data pipeline to provide insights into marketing and', 'Senior Data Engineer', ""Daily Responsibilities (What You'll Do)"", 'Good ideas travel quickly within our small and collective environment; with a great opportunity for impact from just a single individual', 'Design and implement data pipelines utilizing the AWS platform', ""Preferences (Nice To Have's)"", 'Experience with Hadoop ecosystem', 'Proficiency in SQL with multiple dialects incl Postgres, MySQL, Presto, or SQL Server', 'Saatva flourishes off a collaborative environment; we participate in pair programming where we have two developers working together at the same workstation', 'Saatva flourishes off a collaborative environment; we participate in pair programming where we have two developers working together at the same workstationOur company is small yet developing at a rapid pace, unlocking exciting new doors along with personal/professional exponential growth for our team as a wholeGood ideas travel quickly within our small and collective environment; with a great opportunity for impact from just a single individual', 'Accepts difference of opinions, creativity, and has confidence in your own ideas', 'Computer Science DegreeExperience with Apache Spark and PySpark libraryExperience with Hadoop ecosystem', '5+ years of data engineering experience within an AWS environmentProficiency in SQL with multiple dialects incl Postgres, MySQL, Presto, or SQL ServerExpertise in Python for developing and maintaining data pipeline codeFamiliarity with ETL software (i.e. Talend, CloverETL, Pentaho, Informatica or Airflow)Competence with star schema / snowflake schema designStrong communication skills and ability to work directly with data analytics teamsFamiliarity with BI software (preferably Tableau or Metabase)', 'Why Join our Team?', '5+ years of data engineering experience within an AWS environment', 'Results and action oriented, willing to go that extra mile for the best outcome', 'Design and implement data pipelines utilizing the AWS platformDesign schemas for efficient storage and query executionEnhance data models by developing integrations with business partnersBuild the infrastructure required for the aggregation of data from a variety of sourcesCreate analytics tools that utilize the data pipeline to provide insights into marketing and', 'Work with data scientists, analysts, engineers, executives and product managers to']",Associate,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
AWS Data Engineer ,Entech,"Malvern, PA",4 weeks ago,Over 200 applicants,"['', 'A desire to mentor and coach others is highly desirable. You will also help upskill towards adopting cloud technologies and adheres to Shift+Left testing patterns.', 'Performance and stress testing with security, migration, and integration', 'Experience with CI/CD pipeline tools like Bamboo, Bitbucket, Artifactory, Ansible, Nexus, etc.', 'Strong written and oral communication skills.', 'AWS Service and other API related technology - IAM, EC2, Lambda (function-as-a-service)', 'Strong, demonstrated analysis and problem solving skills.', 'Ideal skillset', 'Specializations that’ll make an impact', 'We are looking for a developer, passionate about technology and adopting modern software engineering practices to join us in this very exciting journey to modernize our applications to the cloud. ', 'Entech is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees. Entech does not offer sponsorship for this role at this time.', 'Desired Qualifications:', 'Experience as a data engineer/architect helping to guide and mature AWS data service usage', 'Day to day includes working within a full stack team alongside our Business Partners and Product Owners and an extremely motivated team of developers committed to working in an Agile environment.', 'AWS Service and other API related technology - IAM, EC2, Lambda (function-as-a-service)Experience as a data engineer/architect helping to guide and mature AWS data service usageDynamoDB, RDS, AuroraMulti-region AWS experience (active-active type setup).Some exposure to infrastructure DevOps is also requiredLots of proven experience developing in containersJava/Python background and then transition to AWSAgile, DevOpsStrong experience in serverless architectureSpecializations that’ll make an impactArchitecture design with a wide range of technologies within technical specialty – from state of the art to legacy systemsPerformance and stress testing with security, migration, and integrationAWS certification is preferable in one of the following domains: AWS Developer, ArchitectExperience with CI/CD pipeline tools like Bamboo, Bitbucket, Artifactory, Ansible, Nexus, etc.Analytics technologies such as HIVE, Presto, Hadoop, Tableau, etc. a plusKnowledge of Angular a plus', 'Lots of proven experience developing in containers', 'AWS certification is preferable in one of the following domains: AWS Developer, Architect', 'Java/Python background and then transition to AWS', 'Strong written and oral communication skills.Strong, demonstrated analysis and problem solving skills.Strong planning and organizational skills.', 'Agile, DevOps', 'Analytics technologies such as HIVE, Presto, Hadoop, Tableau, etc. a plus', 'Knowledge of Angular a plus', 'Some exposure to infrastructure DevOps is also required', 'Strong planning and organizational skills.', 'Strong experience in serverless architecture', 'Architecture design with a wide range of technologies within technical specialty – from state of the art to legacy systems', 'Working on Proof of Concepts and validating solutions proposed by our architecture and platform teams and help deploy micro-services to production. Creating and externalizing APIs to deliver data using modern, cloud-based patterns to internal and external consumers.', 'DynamoDB, RDS, Aurora', 'Multi-region AWS experience (active-active type setup).']",Mid-Senior level,Contract,Engineering,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Slickdeals,"Los Angeles, CA",5 days ago,125 applicants,"['', 'Help translate business requirements into specification documents to track and perform analysis of new and existing site features', 'To Be Successful You Will Be', ' Understand data structures and algorithms. Understanding of basic statistics (confidence intervals, statistical significance, etc) Experience in working with large size data sets (Billions of rows/Petabytes of data) Experience in working with various data sources (ODBC, flat files, etc) Experience working with and designing complex data schemas Strong skills in SQL, Java and/or Python Experience with SQL query performance optimization Strong skills Experience with Apache Big Data Frameworks (Hadoop/EMR/Databricks, Spark, Hive) Strong experience with Spark performance optimization and troubleshooting Experience with Kafka and event driven architectures Familiarity with workflow scheduling/orchestration tools (Airflow, Jenkins) Experience with AWS Experience with Tableau and or other Self Service Analytical tools. Implemented Redshift, Snowflake, Azure Data Warehouse, ADLS, S3, Kafka, Presto, EMR, Databricks, or Data Lake Architecture in one or more public clouds in a Production Large Scale environment. ', ' Helping us leverage large-scale data stores and data infrastructure by building out data pipelines, streams, and utilities in Spark and other technologies for feedback to our business systems, partners, or users Developing robust, low latency and fault tolerant pipelines to support business critical systems Aggregating key metrics for business partners to inform key decisions Working with cloud technologies to build and deploy your applications ', 'Experience working with and designing complex data schemas', 'Strong experience with Spark performance optimization and troubleshooting', 'Planning, conducting and directing the analysis of complex business problems and projects', 'Experience with Kafka and event driven architectures', ' Highly motivated with a great attitude and desire to dive into raw data to understand trends in behavior to find insights Excellent at multitasking who can execute multiple requests and reports under tight timelines Inquisitive, self-starter, able to work autonomously Able to work in a fast-paced dynamic startup like environment Detail-oriented tactician who strives for perfection Strong verbal and written communication (and listening) skills Excellent reading comprehension and attention to detail. Strong problem-solving skills Strong documentation skills as you code (Jira, Confluence) ', 'Aggregating key metrics for business partners to inform key decisions', 'Experience with AWS', 'Excellent at multitasking who can execute multiple requests and reports under tight timelines', 'Detail-oriented tactician who strives for perfection', 'Design data pipelines and maintain data pipelines in cloud or on-premise environments', 'Developing robust, low latency and fault tolerant pipelines to support business critical systems', ' Work directly with the business users to understand the reporting needs and lead business users to practical solutions Help translate business requirements into specification documents to track and perform analysis of new and existing site features Understand the necessity of data quality and requirement for confidence of accuracy of any reports Develop/monitor/maintain new reports, dashboards, visualizations, procedures, data structures and databases Design data pipelines and maintain data pipelines in cloud or on-premise environments Design data schema, perform data transformations, enrichments, and manipulations with efficiency and reusability in mind Planning, conducting and directing the analysis of complex business problems and projects ', 'Understand data structures and algorithms. Understanding of basic statistics (confidence intervals, statistical significance, etc)', 'Design data schema, perform data transformations, enrichments, and manipulations with efficiency and reusability in mind', 'Strong documentation skills as you code (Jira, Confluence)', 'As a Data Engineer, Your Day-to-day Tasks Will Include', 'Helping us leverage large-scale data stores and data infrastructure by building out data pipelines, streams, and utilities in Spark and other technologies for feedback to our business systems, partners, or users', 'Education', 'Experience with Tableau and or other Self Service Analytical tools.', 'Strong problem-solving skills', 'Highly motivated with a great attitude and desire to dive into raw data to understand trends in behavior to find insights', 'Implemented Redshift, Snowflake, Azure Data Warehouse, ADLS, S3, Kafka, Presto, EMR, Databricks, or Data Lake Architecture in one or more public clouds in a Production Large Scale environment.', 'Work directly with the business users to understand the reporting needs and lead business users to practical solutions', 'Able to work in a fast-paced dynamic startup like environment', 'Working with cloud technologies to build and deploy your applications', 'The Purpose', ' Understand data structures and algorithms. Understanding of basic statistics (confidence intervals, statistical significance, etc) Experience in working with large size data sets (Billions of rows/Petabytes of data) Experience in working with various data sources (ODBC, flat files, etc) Experience working with and designing complex data schemas Strong skills in SQL, Java and/or Python Experience with SQL query performance optimization Strong skills Experience with Apache Big Data Frameworks (Hadoop/EMR/Databricks, Spark, Hive) Strong experience with Spark performance optimization and troubleshooting Experience with Kafka and event driven architectures Familiarity with workflow scheduling/orchestration tools (Airflow, Jenkins) Experience with AWS Experience with Tableau and or other Self Service Analytical tools. Implemented Redshift, Snowflake, Azure Data Warehouse, ADLS, S3, Kafka, Presto, EMR, Databricks, or Data Lake Architecture in one or more public clouds in a Production Large Scale environment. To Be Successful You Will Be Highly motivated with a great attitude and desire to dive into raw data to understand trends in behavior to find insights Excellent at multitasking who can execute multiple requests and reports under tight timelines Inquisitive, self-starter, able to work autonomously Able to work in a fast-paced dynamic startup like environment Detail-oriented tactician who strives for perfection Strong verbal and written communication (and listening) skills Excellent reading comprehension and attention to detail. Strong problem-solving skills Strong documentation skills as you code (Jira, Confluence) As a Data Engineer, Your Day-to-day Tasks Will Include Helping us leverage large-scale data stores and data infrastructure by building out data pipelines, streams, and utilities in Spark and other technologies for feedback to our business systems, partners, or users Developing robust, low latency and fault tolerant pipelines to support business critical systems Aggregating key metrics for business partners to inform key decisions Working with cloud technologies to build and deploy your applications EnvironmentCan work effectively on a small and nimble team, no trouble context-switchingEducationB.S./M.S. in Computer Science or Computer Engineering or 3+ years of equivalent experience', 'Experience in working with large size data sets (Billions of rows/Petabytes of data)', 'Strong skills in SQL, Java and/or Python', 'Strong skills Experience with Apache Big Data Frameworks (Hadoop/EMR/Databricks, Spark, Hive)', 'Excellent reading comprehension and attention to detail.', 'Experience in working with various data sources (ODBC, flat files, etc)', 'Strong verbal and written communication (and listening) skills', 'Develop/monitor/maintain new reports, dashboards, visualizations, procedures, data structures and databases', 'Understand the necessity of data quality and requirement for confidence of accuracy of any reports', 'The Role', 'THE CANDIDATE:', 'Familiarity with workflow scheduling/orchestration tools (Airflow, Jenkins)', 'Experience with SQL query performance optimization', 'Environment', 'Inquisitive, self-starter, able to work autonomously']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Data Engineer,Point Card,"San Francisco, CA",1 week ago,59 applicants,"['', 'Requirements:', 'Experience working with AWS data products', 'Bonus Points (no pun intended):', 'Experience with 3rd party integration tooling like Segment', ' Design and build scalable and robust data management systems Ensure that all systems meet industry best practices and business requirements Create custom services and tooling when applicable Implement disaster recovery procedures Collaborate with engineers and data scientists on projects ', 'Collaborate with engineers and data scientists on projects', "" Bachelor's degree in Computer Science, Computer Engineering, Applied Mathematics, Physics Statistics or equivalent work experience 5+ years of relevant professional experience Strong algorithmic and data modeling knowledge Experience working with a Hadoop or similar ecosystem Expert in SQL and a scripting language like Python "", 'Experience supporting data analysts', 'Unlimited vacation policy, paid company holidays, and WFH flexible. We shut down the office at the end of the year for a winter holiday.', 'Have built a real-time data pipeline using a streaming technology like Kinesis or Kafka', 'Implement disaster recovery procedures', 'Roles & Responsibilities:', 'Design and build scalable and robust data management systems', 'Experience building a data pipeline from scratch', 'Full health benefits (medical, dental, and vision insurance).', 'Strong algorithmic and data modeling knowledge', 'Create custom services and tooling when applicable', 'Free Point Card membership + 10,000 ($100) monthly points', 'Experience working with a Hadoop or similar ecosystem', 'Opportunity to be part of a brand that is creating a new standard for financial services and enter at the ground floor of a fast growing, mission-driven company.', '5+ years of relevant professional experience', 'Experience maintaining Data Lake/Warehouse', 'Competitive salary, stock options, and 401K.', 'Expert in SQL and a scripting language like Python', 'Data Engineer', 'Ensure that all systems meet industry best practices and business requirements', ' Experience working with AWS data products Experience with 3rd party integration tooling like Segment Experience maintaining Data Lake/Warehouse Experience supporting data analysts Experience building a data pipeline from scratch Have built a real-time data pipeline using a streaming technology like Kinesis or Kafka ', ' Opportunity to be part of a brand that is creating a new standard for financial services and enter at the ground floor of a fast growing, mission-driven company. Competitive salary, stock options, and 401K. Full health benefits (medical, dental, and vision insurance). Unlimited vacation policy, paid company holidays, and WFH flexible. We shut down the office at the end of the year for a winter holiday. Free Point Card membership + 10,000 ($100) monthly points Monthly stipends for continuous learning, health & wellness, and commuting.', ""Bachelor's degree in Computer Science, Computer Engineering, Applied Mathematics, Physics Statistics or equivalent work experience"", 'Point Perks:', 'Monthly stipends for continuous learning, health & wellness, and commuting.']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Data Engineer - Open to most locations,Zapier,"Remote, OR",2 weeks ago,45 applicants,"['', ""Zapier is an equal opportunity employer. We're excited to work with talented and empathetic people no matter their race, color, gender, sexual orientation, religion, national origin, physical or mental disability, or age. Our"", 'Our Commitment to Applicants', 'Zapier is proud to be an equal opportunity workplace dedicated to pursuing and hiring a diverse workforce.', 'You enjoy collaboration and knowledge sharing. ', 'You’re experienced in defining and analysing large, complex datasets', ""Unlimited vacation policy. Plus we require you to take at least 2 weeks off each year. We see most employees take 4-5 weeks off per year. This isn't a vague policy where unlimited vacation means no vacation."", "" Develop ETL and streaming data pipelines. Tools you’ll probably use include AWS Redshift, Looker, Matillion ETL, Airflow, Python, Pandas, SQL, Kafka, Druid. Deploy and maintain machine learning models on Kubernetes Develop CI/CD pipelines for complex long running applications Build internal tooling, applications and libraries As a part of Zapier's all-hands philosophy, help customers via support or research to ensure they have the best experience possible. "", ""Pick your own equipment. We'll set you up with whatever Apple laptop + monitor combo you want plus any software you need."", 'Zapier Guide to Remote Work', '14 weeks paid leave for new parents of biological or adopted children', 'You understand that perfect is the enemy of good. ', 'You care about the details', 'You’re innovative', 'Competitive salary (we pay based on the norms of your country)', 'The Whole Package', 'Tools you’ll probably use include AWS Redshift, Looker, Matillion ETL, Airflow, Python, Pandas, SQL, Kafka, Druid.', 'How To Apply', 'You are a skilled written communicator. ', "" Zapier is a fast-growing and remote-first company, so you'll get experience on many different projects to support our stakeholders. Here are some things you might get to help our teams with: Develop ETL and streaming data pipelines. Tools you’ll probably use include AWS Redshift, Looker, Matillion ETL, Airflow, Python, Pandas, SQL, Kafka, Druid. Deploy and maintain machine learning models on Kubernetes Develop CI/CD pipelines for complex long running applications Build internal tooling, applications and libraries As a part of Zapier's all-hands philosophy, help customers via support or research to ensure they have the best experience possible.   "", 'Hi there!', 'You are a skilled written communicator. Zapier is a 100% remote team and writing is our primary means of communication. You communicate complex technical topics clearly and in an approachable way.You’re experienced in defining and analysing large, complex datasets, finding actionable business solutions to loosely defined problems.You’re innovative, able to ideate and autonomously drive forward improvements to data pieplines that you own, end-to-end.You’re technically competent with Python, including at least one SQL dialect - they’re all about the same; we happen to use Redshift.You have hands-on experience with common machine learning techniques (e.g. predictive modelling, time series modelling, classification and clustering techniques).You care about the details, and understand that without high quality instrumentation, we suffer from Garbage In, Garbage Out.You enjoy collaboration and knowledge sharing. You appreciate our team’s values of eagerness to collaborate with teammates with any level of statistical knowledge, iterating over your deliverables, and being curious.You understand that perfect is the enemy of good. You will default to action by initially shipping solutions that simply work and work simply, while iterating as needed.', ' Our Commitment to Applicants Culture and Values at Zapier Zapier Guide to Remote Work Zapier Code of Conduct Working on Diversity and Inclusivity ', 'Zapier Code of Conduct', ' You are a skilled written communicator. Zapier is a 100% remote team and writing is our primary means of communication. You communicate complex technical topics clearly and in an approachable way.You’re experienced in defining and analysing large, complex datasets, finding actionable business solutions to loosely defined problems.You’re innovative, able to ideate and autonomously drive forward improvements to data pieplines that you own, end-to-end.You’re technically competent with Python, including at least one SQL dialect - they’re all about the same; we happen to use Redshift.You have hands-on experience with common machine learning techniques (e.g. predictive modelling, time series modelling, classification and clustering techniques).You care about the details, and understand that without high quality instrumentation, we suffer from Garbage In, Garbage Out.You enjoy collaboration and knowledge sharing. You appreciate our team’s values of eagerness to collaborate with teammates with any level of statistical knowledge, iterating over your deliverables, and being curious.You understand that perfect is the enemy of good. You will default to action by initially shipping solutions that simply work and work simply, while iterating as needed. ', ""Zapier is a fast-growing and remote-first company, so you'll get experience on many different projects to support our stakeholders. Here are some things you might get to help our teams with: Develop ETL and streaming data pipelines. Tools you’ll probably use include AWS Redshift, Looker, Matillion ETL, Airflow, Python, Pandas, SQL, Kafka, Druid. Deploy and maintain machine learning models on Kubernetes Develop CI/CD pipelines for complex long running applications Build internal tooling, applications and libraries As a part of Zapier's all-hands philosophy, help customers via support or research to ensure they have the best experience possible.  "", ' code of conduct', 'Data Engineers', 'Build internal tooling, applications and libraries', ""Work with awesome companies around the world. We partner with great software companies all over the world and you'll constantly get to interact with people from these great companies"", 'Working on Diversity and Inclusivity', 'Culture and Values at Zapier', ""As a part of Zapier's all-hands philosophy, help customers via support or research to ensure they have the best experience possible."", 'Things You Might Do', 'Location: Planet Earth.', 'Deploy and maintain machine learning models on Kubernetes', 'About Zapier', 'Compensation', 'Develop CI/CD pipelines for complex long running applications', '2 annual company retreats to awesome places', 'You’re technically competent with Python, including at least one SQL dialect ', 'Great healthcare + dental + vision coverage*', 'Develop ETL and streaming data pipelines.', 'About You', 'Retirement plan with 4% company match*', ' provides a beacon for the kind of company we strive to be, and we celebrate our differences because those differences are what allow us to make a product that serves a global user base.', 'Data at Zapier', "" Competitive salary (we pay based on the norms of your country) Great healthcare + dental + vision coverage* Retirement plan with 4% company match* Profit-sharing 2 annual company retreats to awesome places 14 weeks paid leave for new parents of biological or adopted children Pick your own equipment. We'll set you up with whatever Apple laptop + monitor combo you want plus any software you need. Unlimited vacation policy. Plus we require you to take at least 2 weeks off each year. We see most employees take 4-5 weeks off per year. This isn't a vague policy where unlimited vacation means no vacation. Work with awesome companies around the world. We partner with great software companies all over the world and you'll constantly get to interact with people from these great companies "", 'While we take care of our international folks as best we can, currently, healthcare and retirement plans are only available to US-, UK- and CA- based employees.', 'You have hands-on experience with common machine learning techniques', 'Profit-sharing']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Ameex Technologies,"Schaumburg, IL",4 days ago,Be among the first 25 applicants,[''],Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Terray Therapeutics,"Pasadena, CA",4 weeks ago,Be among the first 25 applicants,"['', 'Expert in building and managing scalable relational databases, preferably in the life sciences space', 'Build and manage our databases of billions to trillions of chemical structures, intensities, affinities, and data from other biological assays', 'Expert in engineering big data pipelines using modern technologies and cloud infrastructures', 'Manage and improve our data lake of millions of fluorescence microscopy images', 'Experience with high-end distributed data processing environments (Spark, Hadoop, etc.)', 'Experience with pipeline/workflow managers (Luigi, Airflow, Nextflow, etc.)', 'Highly proficient in Python and the PyData stack (numpy, pandas, scipy, dask, etc.)', 'Design and architect a data warehouse to support downstream analytics', 'Proficiency in Linux environment, experience with database languages (e.g., SQL) and experience with version control practices and tools (Git)', 'Qualifications Include', ' Manage and improve our data lake of millions of fluorescence microscopy images Work with our data scientists to incorporate our image processing workflow into the data pipeline Build and manage our databases of billions to trillions of chemical structures, intensities, affinities, and data from other biological assays Design and architect a data warehouse to support downstream analytics ', 'Experience and Qualifications: ', 'Position Summary:', 'Experience with cloud computing services, preferably AWS (EMR, Redshift)', 'Work with our data scientists to incorporate our image processing workflow into the data pipeline', 'Company Overview:', 'The Core Responsibilities Of This Job Will Be', ' Expert in engineering big data pipelines using modern technologies and cloud infrastructures Expert in building and managing scalable relational databases, preferably in the life sciences space Experience with cloud computing services, preferably AWS (EMR, Redshift) Experience with high-end distributed data processing environments (Spark, Hadoop, etc.) Proficiency in Linux environment, experience with database languages (e.g., SQL) and experience with version control practices and tools (Git) Experience with pipeline/workflow managers (Luigi, Airflow, Nextflow, etc.) Highly proficient in Python and the PyData stack (numpy, pandas, scipy, dask, etc.) ']",Entry level,Full-time,Information Technology,Research,2021-03-18 14:34:51
Data Engineer,strongDM,"Remote, OR",4 weeks ago,Be among the first 25 applicants,"['', 'Build and operate full refresh and incremental pipelines to compute derived models (we’re currently using dbt, python+pandas, and Spark)', ' Customers Love Us Because ', 'Industry-standard base', 'Industry-standard baseMedical, dental, and vision insurance401k, HSA, FSA, short/long-term disability, 3 months parental leave3 weeks PTO + standard holidaysEquity in a fast-growing startupNo travel required', 'Proficiency in SQL and SQL based modelling tools (we’re using dbt)Proficiency in python and pandasverse, and familiarity with Golang, Javascript3+ years of experience in a data-heavy engineering roleExperience working at a tech startup or other high velocity engineering cultureA strong ethos for getting things done (we iterate quickly, and pair program daily)', '401k, HSA, FSA, short/long-term disability, 3 months parental leave', '3 weeks PTO + standard holidays', '3+ years of experience in a data-heavy engineering role', 'Build and operate outbound pipelines to send data in actionable shapes to line-of-business tools', "" What You'll Do "", ' Splunk\'s CISO Joel Fulton says ""strongDM gives you what you can’t get any other way -- the ability to see what happens, replay and analyze incidents.""', ' Chef\'s co-founder Adam Jacob says ""strongDM takes the friction out of getting staff access to the systems they need.""', 'Build and operate batch and streaming pipelines to ingest data into a lakehouse (we’re currently using S3+Glue+Athena+Redshift)Build and operate full refresh and incremental pipelines to compute derived models (we’re currently using dbt, python+pandas, and Spark)Build and operate outbound pipelines to send data in actionable shapes to line-of-business toolsCurate the datasets in the lakehouse to facilitate analysis, reporting, and self-serve querying (we’re currently using Jupyter and Redash)Partner with engineers, data analysts, and business stakeholders on data efforts', 'A strong ethos for getting things done (we iterate quickly, and pair program daily)', 'No travel required', 'Proficiency in SQL and SQL based modelling tools (we’re using dbt)', 'Curate the datasets in the lakehouse to facilitate analysis, reporting, and self-serve querying (we’re currently using Jupyter and Redash)', 'When Was The Last Time You Heard Things Like ', 'we’re a young company that has invested in data efforts early. Our leadership team is data literate and has broad experience building data products and leading analytics functions. And, we have a shared focus on how we expect the data platform to power our business.', 'Build and operate batch and streaming pipelines to ingest data into a lakehouse (we’re currently using S3+Glue+Athena+Redshift)', 'Equity in a fast-growing startup', ' Compensation ', 'Proficiency in python and pandasverse, and familiarity with Golang, Javascript', 'Experience working at a tech startup or other high velocity engineering culture', 'Medical, dental, and vision insurance', ' Requirements ', 'Partner with engineers, data analysts, and business stakeholders on data efforts']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,"Aptima, Inc.","Dayton, OH",1 week ago,Be among the first 25 applicants,"['', 'Working Conditions (ADA accommodations may be sought)', 'Secret Clearance needed', 'Minimum Requirements', 'Excellent project team and customer-focused written and oral communication skills', 'User interface development for Desktop and/or Web applications', 'Coordinate with customers and internal stakeholders regarding requirements for transition and further scientific and technical development of the technology', ':', 'U.S. Citizenship required or the ability to obtain a U.S. Security Clearance', 'Exposure to statistical methods, especially those related to exploratory data analysis', 'Proficiency in C#, JavaScript, Python, Angular, and other programming languages', 'Derive business knowledge from large data repositories', 'What You’ll Do', 'Familiarity with one or more relational (SQL Server) and/or NoSQL (Elasticsearch) data storage technologies', 'Developing innovative dashboard, data analytics, and data telemetry displays', 'Proven technical project leadership at individual and team level', 'Experience working with unstructured data sets comprised of multiple disparate data sources', 'BA, BS, or equivalent in Computer Science or related degree preferred', 'EOE MINORITIES/FEMALES/PROTECTED VETERANS/DISABLED', 'Coordinate software teams on projects leading architecture and design of software systems', 'All applicants selected will be subject to a background investigation and must meet eligibility requirements for access to classified information.', 'U.S. Citizenship required or the ability to obtain a U.S. Security ClearanceSecret Clearance neededBA, BS, or equivalent in Computer Science or related degree preferred8+ years of professional experience in related technical field', 'Aptima, Inc. is an equal opportunity employer dedicated to non-discrimination in employment. We select the most qualified individual for the job based on job-related qualifications regardless of race, color, age, sex, religion, national origin, disability, ancestry, marital status, credit history, sexual orientation, arrest and court record, genetic information, veteran status or any other status protected by federal, state or other applicable laws.', 'Familiarity with one or more data analytics tools (e.g., Python, R, Matlab)', 'Who We Are', ""As a Data Engineer, you will work within Aptima's Performance Assessment Technologies division and have an active role in leading development of tools for assessing and augmenting human performance within a fast-paced, collaborative team. You will work within the entire engineering lifecycle, including software requirements analysis, detailed design, implementation and test of individual software modules along with development and testing of software systems and software applications and will use these tools to derive insights from data.\xa0Other responsibilities include:"", 'Coordinate with customers and internal stakeholders regarding requirements for transition and further scientific and technical development of the technologyMaintain high quality software development leveraging internal and industry standardsCoordinate software teams on projects leading architecture and design of software systemsDerive business knowledge from large data repositoriesContinue to develop and expand software knowledge with acquiring new skill sets and experience through the course of development', ""For 25 years, Aptima's mission has been to improve and optimize performance in mission-critical, technology-intensive settings. We apply deep expertise in how humans think, learn, and perform to today's challenges. Whether for fighter pilots functioning in the cockpit, medical staff in the ICU, or teams collaborating across distributed networks, our solutions help measure, assess, inform, and augment human performance in defense, intel, aviation, law enforcement, and healthcare."", 'Maintain high quality software development leveraging internal and industry standards', 'Experience Needed', 'Continue to develop and expand software knowledge with acquiring new skill sets and experience through the course of development', 'Software development in a team environment (peer reviews, unit testing, configuration management, defect tracking, etc.)Excellent project team and customer-focused written and oral communication skillsProven technical project leadership at individual and team levelProficiency in C#, JavaScript, Python, Angular, and other programming languagesExposure to statistical methods, especially those related to exploratory data analysisUser interface development for Desktop and/or Web applicationsFamiliarity with one or more relational (SQL Server) and/or NoSQL (Elasticsearch) data storage technologiesFamiliarity with one or more data analytics tools (e.g., Python, R, Matlab)Developing innovative dashboard, data analytics, and data telemetry displaysExperience working with unstructured data sets comprised of multiple disparate data sources', '8+ years of professional experience in related technical field', '10% travel possible', '\xa0', 'Software development in a team environment (peer reviews, unit testing, configuration management, defect tracking, etc.)', 'Aptima, Inc. participates in the US Government E-Verify Program. For more information, click on\xa0http://www.uscis.gov/e-verify.\xa0']",Mid-Senior level,Full-time,Business Development,Defense & Space,2021-03-18 14:34:51
Data Engineer,Amply Media,"Kansas City, KS",2 weeks ago,Be among the first 25 applicants,"['', ' Experience using Snowflake, Postgres, or equivalent ', ' Resourceful: Looks for ways to achieve goals with available resources. ', ' Design and implement effective database models to store and retrieve company data ', ' Passion for data management and data technologies ', ' Thank You ', 'The Successful Candidate Will Also Demonstrate The Following Abilities', ' Adaptability: Capable of adjusting to changing priorities in a fast-paced environment. ', 'By submitting your interest in this job, you agree to receive text notifications with additional steps to complete your job application. You will receive up to 6 messages from the number ""63879"". Message & data rates may apply. Please refer to our privacy policy for more information.', ' Support existing reporting infrastructure and troubleshoot issues related to data ', ' Prioritization: Able to arrange projects in order of importance relative to each other. ', ' This is an opportunity to be with an industry leading company that continues to experience tremendous growth ', ' Passion for Technology: Excitement for new technology, bleeding edge applications, and a positive attitude towards solving real world challenges. ', 'Responsibilities', ' Create new reports based on client requirements using metric and dimension based reporting data models. ', ' Experience with visualization and reporting tools, such as Tableau, PowerBI and Looker. ', ' We offer a fun, work hard – play hard culture ', ' Interact face-to-face with business stakeholders, develop cooperative relationships, and acquire domain knowledge of the business. ', ' Experience with developing, executing and maintaining complex SQL queries against multiple databases is required.  Experience using Snowflake, Postgres, or equivalent  Experience with visualization and reporting tools, such as Tableau, PowerBI and Looker.  Desire to work with a team of people solving complex problems and sharing knowledge freely  Passion for data management and data technologies ', 'Apply for this Position', ' Located on the Country Club Plaza- next to world class shopping and restaurants ', 'Qualifications', ' Communication (written and oral): Able to successfully communicate product recommendations based on design knowledge ', ' Support existing reporting infrastructure and troubleshoot issues related to data  Create new reports based on client requirements using metric and dimension based reporting data models.  Proactively improve reporting workflows and automate manual processes throughout the business for higher efficiency, robustness, and speed.  Support the development of new products and services via ingestion, processing, and formatting data for reporting and analytics.  Interact face-to-face with business stakeholders, develop cooperative relationships, and acquire domain knowledge of the business.  Design and implement effective database models to store and retrieve company data ', ' Experience with developing, executing and maintaining complex SQL queries against multiple databases is required. ', ' Complimentary snacks and beverages as well as catered lunches ', ' Fun social events and perks! - check out our Instagram to see more ', ' Team Player: Demonstrates a strong ability to support department staff members and managers; able to establish collaborative relationships with peers. ', ' We offer a fun, work hard – play hard culture  We have shuffleboard, virtual reality, a retro gaming console and a beer fridge  No dress code policy! Wear your flip flops and shorts in the summer  Complimentary snacks and beverages as well as catered lunches  Located on the Country Club Plaza- next to world class shopping and restaurants  Fun social events and perks! - check out our Instagram to see more  This is an opportunity to be with an industry leading company that continues to experience tremendous growth  Great Place to Work Certified and named Ingram’s Best Companies To Work For in Kansas City ', ' Adaptability: Capable of adjusting to changing priorities in a fast-paced environment.  Prioritization: Able to arrange projects in order of importance relative to each other.  Passion for Technology: Excitement for new technology, bleeding edge applications, and a positive attitude towards solving real world challenges.  Creativity: Research the latest trends to design competitive ad units.  Resourceful: Looks for ways to achieve goals with available resources.  Team Player: Demonstrates a strong ability to support department staff members and managers; able to establish collaborative relationships with peers.  Communication (written and oral): Able to successfully communicate product recommendations based on design knowledge ', ' We have shuffleboard, virtual reality, a retro gaming console and a beer fridge ', ' Desire to work with a team of people solving complex problems and sharing knowledge freely ', ' No dress code policy! Wear your flip flops and shorts in the summer ', ' Creativity: Research the latest trends to design competitive ad units. ', 'Company Overview', ' Proactively improve reporting workflows and automate manual processes throughout the business for higher efficiency, robustness, and speed. ', ' Great Place to Work Certified and named Ingram’s Best Companies To Work For in Kansas City ', ' Support the development of new products and services via ingestion, processing, and formatting data for reporting and analytics. ']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Staff Data Engineer,Proven Recruiting,"San Diego, CA",2 weeks ago,45 applicants,"['', ' Data Modeling: Experience choosing from among a variety of data modeling methods -- such as 3rd normal, dimensional, data vault, sub-type/super-type, or wide-table - for a given use case, with an understanding of the associated loading logic.', 'Required Experience/Qualifications', ' Combined hands-on experience with Snowflake, DBT, DAGs, GCP, GitHub, Jira, Confluence and leading data replication tools', ' Experience with test-driven data-pipeline development, CI-CD, statistical process control, source code branching / merging, testing, and pipeline monitoring tools such as Terraform, Ansible, Jenkins, X-Ray, and Great Expectations.', ' Affinity for Agile rigor and a DataOps culture of test-automation, versioning, infrastructure as code, rapid deployments, and continuous monitoring.', ' Bachelors degree in computer science, information technologies, or an equivalent combination of job experience and certifications relevant to the above qualifications. 5 to 10 years of combined work experience in data engineering and data warehousing Strong expertise with ANSI SQL, data profiling, transformations, and end-to-end orchestration of a workflow built with multiple tools, and therein efficiently move terabytes of data Data Modeling: Experience choosing from among a variety of data modeling methods -- such as 3rd normal, dimensional, data vault, sub-type/super-type, or wide-table - for a given use case, with an understanding of the associated loading logic. Combined hands-on experience with Snowflake, DBT, DAGs, GCP, GitHub, Jira, Confluence and leading data replication tools Progressive experience with Python and Kafka Affinity for Agile rigor and a DataOps culture of test-automation, versioning, infrastructure as code, rapid deployments, and continuous monitoring. Comfort with multiple data modeling methods Option: Tableau for data visualization', ' Ingest data using Kafka, Spark, or a leading data replication tool.', ' Design, build, and enhance a set of robust, automated data pipelines, driven by event streams, replication, and/or batches, to create and refresh diverse datasets.', ' Experience working in regulated industry, including FDA and HIPAA', ' Strong expertise with ANSI SQL, data profiling, transformations, and end-to-end orchestration of a workflow built with multiple tools, and therein efficiently move terabytes of data', ' 5 to 10 years of combined work experience in data engineering and data warehousing', ' Progressive experience with Python and Kafka', ' Consulting experience, with an eagerness to exceed expectations on quality and delivery speed by mixing heads-down coding with influencing on DataOps rigor among analysts newer to SDLC.', ' Consulting experience, with an eagerness to exceed expectations on quality and delivery speed by mixing heads-down coding with influencing on DataOps rigor among analysts newer to SDLC. Experience with test-driven data-pipeline development, CI-CD, statistical process control, source code branching / merging, testing, and pipeline monitoring tools such as Terraform, Ansible, Jenkins, X-Ray, and Great Expectations. Comfort with quantitative aspects of data pipeline monitoring Experience working in regulated industry, including FDA and HIPAA Familiarity with Oracle and SQL Server environments', 'Preferred Experience', 'Essential Duties And Responsibilities', ' Work with Manufacturing Operations’ business analysts to ascertain analytics requirements for cloud data repositories and data pipelines in support of business analytics and data science. Design, build, and enhance a set of robust, automated data pipelines, driven by event streams, replication, and/or batches, to create and refresh diverse datasets. Design, instantiate, load, and refresh tables in Snowflake. Ingest data using Kafka, Spark, or a leading data replication tool. Transform data using DBT, stored procedures Write algorithms for data pipeline testing and continuous monitoring Participate in enablement among organizationally distributed business analysts in their delivery of robust, test-driven, self-service data wrangling, visual analytics, and solution sharing.', ' Write algorithms for data pipeline testing and continuous monitoring', ' Bachelors degree in computer science, information technologies, or an equivalent combination of job experience and certifications relevant to the above qualifications.', ' Work with Manufacturing Operations’ business analysts to ascertain analytics requirements for cloud data repositories and data pipelines in support of business analytics and data science.', ' Transform data using DBT, stored procedures', ' Option: Tableau for data visualization', ' Participate in enablement among organizationally distributed business analysts in their delivery of robust, test-driven, self-service data wrangling, visual analytics, and solution sharing.', ' Comfort with multiple data modeling methods', ' Design, instantiate, load, and refresh tables in Snowflake.', ' Comfort with quantitative aspects of data pipeline monitoring', ' Familiarity with Oracle and SQL Server environments']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Progressive Insurance,"Mayfield, OH",2 weeks ago,25 applicants,"['', 'Employee Status', "" Bachelor's Degree or higher in an Information Technology discipline or related field of study and minimum of one year work experience designing, programming, and supporting software programs or applications Instead of a degree, minimum of two years related work described in above bullet "", 'Experience with Linux, Python, Git, Terraform, SQL, building APIs (Flask), data orchestration ', ""Bachelor's Degree or higher in an Information Technology discipline or related field of study and minimum of one year work experience designing, programming, and supporting software programs or applications"", ' Gainshare bonus up to 24% of your eligible earnings; Progressive rewards each of us with an annual bonus based on company performance 401(k) with dollar-for-dollar company match up to 6% Diverse, inclusive and welcoming culture with Employee Resource Groups Career development and tuition assistance Onsite gym and healthcare at large locations Wellness programs to help you maintain a better quality of life Medical, dental and vision, including free preventive care ', 'Must-have Qualifications', 'Diverse, inclusive and welcoming culture with Employee Resource Groups', 'Onsite gym and healthcare at large locations', 'Benefits', 'Schedule', 'Career development and tuition assistance', ' Cloud experience - we use AWS (EC2, S3, Athena, Lambda, Aurora, etc.) Experience with Linux, Python, Git, Terraform, SQL, building APIs (Flask), data orchestration  Ability to develop and support web applications using a popular web framework Experience deploying or working with machine learning models.  Self-learner with ability to scope and recommend new tools (patterns, cloud services, etc.) as required by projects ', 'Primary Location', '401(k) with dollar-for-dollar company match up to 6%', 'Preferred Skills', 'Self-learner with ability to scope and recommend new tools (patterns, cloud services, etc.) as required by projects', 'Gainshare bonus up to 24% of your eligible earnings; Progressive rewards each of us with an annual bonus based on company performance', 'Data Engineer', 'Cloud experience - we use AWS (EC2, S3, Athena, Lambda, Aurora, etc.)', 'Wellness programs to help you maintain a better quality of life', 'Experience deploying or working with machine learning models. ', 'Ability to develop and support web applications using a popular web framework', 'Sponsorship for work authorization for foreign national candidates is not available for this position.', 'Instead of a degree, minimum of two years related work described in above bullet', 'Data Engineer Senior', 'Work From Home', 'Job', 'Medical, dental and vision, including free preventive care']",Not Applicable,Full-time,Information Technology,Insurance,2021-03-18 14:34:51
Data Engineer,Avanade,"Dover, DE",3 weeks ago,Be among the first 25 applicants,"['', ' 1,000 data engineers ', ' Do you enjoy making sure that information is accessible and easy to use? So do we. ', ' Travel as needed to various client locations ', ' 3,500 analytics professionals worldwide ', ' Implement effective metrics and monitoring ', 'About Avanade', ' Experience in preparing data for and building pipelines and architecture ', ' Mapping data and analytics ', ' Knowledge of multiple Azure data applications including Azure Databricks ', ' Transforming business needs into technical solutions ', 'Your Skills And Business Experience Include', ' Use proven methods to solve business problems using Azure Data and Analytics services in combination with building data pipelines, data streams and system integration ', 'Day-to-day, You Will', 'About The Role', ' Build the building blocks for transforming enterprise data solutions ', ' Use your sound eye for business to translate business requirements into technical solutions ', ' Assess client needs to build bespoke data design services ', ' Craft the architectures, data warehouses and databases that support access and Advanced Analytics, and bring them to life through modern visualization tools ', ' Analyze current business practices, processes and procedures to spot future opportunities ', ' 300 cognitive service experts ', ' 17 Gold Competencies ', ' 14-time winner of Microsoft Partner of the Year ', 'How We Support You', ' Give colleagues and clients the tools to find and use data for routine and non-routine analysis ', ' 14-time winner of Microsoft Partner of the Year  24,000+ certifications in Microsoft technology  90+ Microsoft partner awards  17 Gold Competencies  3,500 analytics professionals worldwide  1,000 data engineers  Implemented analytics systems for more than 550 clients  400 AI practitioners  300 cognitive service experts ', ' Design and build modern data pipelines, data streams, and data service Application Programming Interfaces (APIs) ', ' Give colleagues and clients the tools to find and use data for routine and non-routine analysis  Use your sound eye for business to translate business requirements into technical solutions  Analyze current business practices, processes and procedures to spot future opportunities  Assess client needs to build bespoke data design services  Build the building blocks for transforming enterprise data solutions  Design and build modern data pipelines, data streams, and data service Application Programming Interfaces (APIs)  Craft the architectures, data warehouses and databases that support access and Advanced Analytics, and bring them to life through modern visualization tools  Implement effective metrics and monitoring  Be comfortable to make your own decisions and guide your colleagues  Travel as needed to various client locations ', 'Why Avanade', ' Transforming business needs into technical solutions  Mapping data and analytics  Data profiling, cataloguing and mapping to enable the design and build of technical data flows  Use proven methods to solve business problems using Azure Data and Analytics services in combination with building data pipelines, data streams and system integration  Knowledge of multiple Azure data applications including Azure Databricks  Experience in preparing data for and building pipelines and architecture ', ' Implemented analytics systems for more than 550 clients ', ' Data profiling, cataloguing and mapping to enable the design and build of technical data flows ', 'Job Description', 'About You', ' 24,000+ certifications in Microsoft technology ', ' 400 AI practitioners ', ' 90+ Microsoft partner awards ', ' Be comfortable to make your own decisions and guide your colleagues ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Immuta,"Boston, MA",1 week ago,92 applicants,"['', 'Terraform, Kubernetes & AWS Infrastructure', 'Has experience delivering production-ready software or data productsUnderstands the fundamentals of data integration, management, and modellingIs eager to learn new tools and technologiesExcels in a fast-paced and results-driven environment', 'We are humble intellects.', 'The Ideal Candidate', 'Specific Initiatives The Data Engineer Will Lead Include', 'PythonSQL & Database management systems, such as Amazon Redshift and SnowflakeThe Immuta productTerraform, Kubernetes & AWS Infrastructure', 'We are independent achievers.', 'Is eager to learn new tools and technologies', 'Python', 'Deploying new data replication services to increase the variety of analyzable dataImplementing processes to increase data freshness and minimize errors and downtimeDeveloping custom Python packages or implementing open-source projects to improve Immuta’s data governance capabilitiesHelp define the data warehousing strategy and technical direction, advocate for best practices, and investigate new technologies', 'Excels in a fast-paced and results-driven environment', 'The Immuta product', 'Has experience delivering production-ready software or data products', ""TECHNOLOGIES YOU'LL USE"", 'GROWTH ANALYTICS TEAM ', 'Developing custom Python packages or implementing open-source projects to improve Immuta’s data governance capabilities', 'Implementing processes to increase data freshness and minimize errors and downtime', 'Deploying new data replication services to increase the variety of analyzable data', 'YOUR ROLE', 'We are helpful and caring', 'Help define the data warehousing strategy and technical direction, advocate for best practices, and investigate new technologies', 'SQL & Database management systems, such as Amazon Redshift and Snowflake', 'We are mission focused.', 'Understands the fundamentals of data integration, management, and modelling']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,IntraEdge,"Phoenix, AZ",1 week ago,Over 200 applicants,"['Big Data Experience (Hadoop, Hive, Spark)', 'Dimensional Modeling Experience', 'Relational/ER Modeling ExperienceDimensional Modeling ExperienceBig Data Experience (Hadoop, Hive, Spark)Data Governance ExperienceData Quality ExperienceETL Development experienceSQL development experience5+ years of experience the data domainCollaborative and open work ethic', 'Data Quality Experience', '5+ years of experience the data domain', 'Relational/ER Modeling Experience', 'Collaborative and open work ethic', 'SQL development experience', 'Data Governance Experience', 'ETL Development experience']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Lorien,"Seattle, WA",2 weeks ago,190 applicants,"['', ' ', '* Migration of DJS jobs to Datanet', 'under applicable law.\xa0', 'Prior Data Engineering tool experience (such as Datanet, Andes etc)', 'Experienced in at least one massively parallel processing data technology such as AWS Redshift, Teradata, Netezza, Spark or Hadoop based big data solution.', 'Lorien is an Equal Opportunity Employer - All qualified applicants will receive', 'Coding proficiency in at least one modern programming language (Python, Ruby, Java, etc)', 'Required Skills: ', '* Roadmap for Attribution Model', 'We plan to have weekly check-ins for the following projects. Each project will have its own backward looking timelines:', 'Hands on SQL knowledge and experience in relational database concepts.', 'Experience with performance tuning activities at database and ETL level', '* Decouple Associate & Paladin cluster (Phase-II) ($23K in potential annual cost saving for Associate once both phases complete)', '• Large tech company experience (Facebook, Microsoft, Google, IBM, etc.)', 'age, disability, veteran status, or any other factor determined to be unlawful', 'Experience working with AWS big data technologies (EMR, Redshift, S3)', 'Strong SQL and Python Scripting background.', '• Business intelligence experience', 'Knowledge of distributed systems as it pertains to data storage and computing', '9-month contract with likely extensions / possible conversion to FTE ', 'U.S. Citizens, Green Card Holders, and those', '* Publish Andes datasets for cross-platform data sharing', 'consideration without regard to race, color, religion, gender, national origin,', 'Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy.', 'Preferred Skills', 'Disqualifiers / red flags:', 'SERVICES TO BE PERFORMED', 'Lorien', '4-5+ years of experienceHands on SQL knowledge and experience in relational database concepts.Coding proficiency in at least one modern programming language (Python, Ruby, Java, etc)Experienced in at least one massively parallel processing data technology such as AWS Redshift, Teradata, Netezza, Spark or Hadoop based big data solution.Robust experience with data modeling, data warehousing, and building ETL pipelines using big data technologies (Hadoop, Hive, Hbase, Spark, EMR, etc.)Experience with performance tuning activities at database and ETL levelKnowledge of orchestrations and automation tools (Airflow, Oozie, etc.)Knowledge of distributed systems as it pertains to data storage and computingProven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy.', 'Health, Dental, Vision and 401k benefits are available to choose from', '4-5+ years of experience', '* Consolidating Charity tables in Redshift', 'Strong SQL and Python Scripting background.Experience working with AWS big data technologies (EMR, Redshift, S3)Prior Data Engineering tool experience (such as Datanet, Andes etc)Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations.', 'authorized to work in the U.S for any employer will be considered.', '* Accounting jobs migration to Andes tables', '• Someone that will need a lot of guidance / cannot work independently ', 'Robust experience with data modeling, data warehousing, and building ETL pipelines using big data technologies (Hadoop, Hive, Hbase, Spark, EMR, etc.)', '\xa0', '* Smart alarms for detecting Data anomaly', 'Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations.', 'Best vs. Average', '* Individual user access for Redshift', 'Knowledge of orchestrations and automation tools (Airflow, Oozie, etc.)', '* Salesforce: Enable in EU']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Langham Recruitment,"East Kent, CT",5 days ago,Be among the first 25 applicants,"['', 'Skills / Experience']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer - Backend,Roche,"South San Francisco, CA",3 days ago,45 applicants,"['', 'Data Engineer - Backend ', ' Understand and maintain data schemas and architectures ', 'Qualifications, Skills & Knowledge', ' Work on agile teams (Scrum/Kanban) ', ' Synthesize requirements from non-technical stakeholders ', ' Develop and maintain scalable data pipelines ', ' Develop and maintain scalable data pipelines  Develop and maintain RESTful JSON apis (FastAPI)  Consume RESTful apis  Understand and maintain data schemas and architectures  Understand and architect ETL processes  Deploy data science models as microservices  Synthesize requirements from non-technical stakeholders  Work closely with data governance and technology practice areas  Work on agile teams (Scrum/Kanban) ', ' Work closely with data governance and technology practice areas ', ' Docker ', ' AWS experience preferred (lambda/glue/s3) ', ' Develop and maintain RESTful JSON apis (FastAPI) ', ' Consume RESTful apis ', ' RDBMS/NoSQL (MySQL preferred) ', ' Understand and architect ETL processes ', ' Python (FastAPI, Pandas preferred)  Git/Github (branches, code reviews, pull requests)  RDBMS/NoSQL (MySQL preferred)  Docker  Plotly/Dash experience preferred  AWS experience preferred (lambda/glue/s3) ', ' Git/Github (branches, code reviews, pull requests) ', ' Deploy data science models as microservices ', ' Plotly/Dash experience preferred ', ' Python (FastAPI, Pandas preferred) ']",Entry level,Full-time,Information Technology,Hospital & Health Care,2021-03-18 14:34:51
Data Engineer,Suffolk Construction,"Boston, MA",1 week ago,90 applicants,"['', 'Develop and maintain a scalable data pipeline and build out new API integrations to support continuing increases in data volume and complexity.', 'Suffolk is a national enterprise that invests, innovates, and builds. We provide value throughout the entire project lifecycle by leveraging our core construction management services with vertical service lines that include real estate capital investment, design, self-perform construction services, technology start-up investment and innovation research and development. We have $4.5 billion in annual revenue, 2,400 employees, and main offices in Boston (headquarters), New York, Miami, West Palm Beach, Tampa, Estero, Dallas, Los Angeles, San Francisco, and San Diego. We serve clients in every major industry sector, including health care, science and technology, education, gaming, transportation, and aviation, and commercial. Suffolk is privately held and is led by founder, chairman and CEO John Fish. We’re ranked #23 on the Engineering News Record list of “Top 400 Contractors.” And we’re proud to be a certified 2020 “Great Place to Work”.', 'But when we say we build buildings, that’s not the whole truth—it’s our people who build buildings. And it is our mission to empower our people to prove impossible wrong.', 'Create logical and physical data models using best practices to ensure high data quality and reduced redundancy.', 'The Data Engineer will be responsible for creating and optimizing our data models and data pipeline architecture. The ideal candidate is an experienced data pipeline builder, data wrangler, and data modeler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our data analysts and data scientists to ensure optimal delivery of data throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products. The right candidate will be excited by the prospect of designing our company’s data architecture to support our next generation of products and data initiatives.', 'A bachelor’s or master’s degree in computer science, statistics, applied mathematics, data management, information systems, or a related quantitative field [or equivalent work experience] is required.', 'Recommend opportunities for reuse of data models in new environments.', 'Construction experience preferred but not required.', 'Curious and tenacious', 'Strong verbal and written communication skills.', 'The Role:', 'Understand and translate business needs into data models supporting long-term solutions.', 'Design, construct, install, test, and maintain data management systems.', 'Combine raw data from different sources and explore ways to enhance data quality and reliability.', 'Implement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders.', 'Support the data visualization team and data science team with formatted data and provides data model QA/QC.', 'Responsibilities', 'At least 5 years or more work experience in data management disciplines including data integration, data modeling, data management, and data quality.', 'Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources.', 'Proven ability to design, build, and manage data pipelines for data structures encompassing data transformation, data models, schemas, metadata and workload management.', 'Ability to prepare curated datasets for self-service consumption (data democratization) ', 'Work closely with all business units and engineering teams to develop a long-term data platform architecture strategy.', 'Proven ability to perform data cleaning, wrangling, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks.', 'Experienced in working with IT to build a data-driven infrastructure.', 'Prepare data for predictive and prescriptive modeling.', 'Evaluate data models and physical databases for variances and discrepancies.', 'Necessary Attributes', 'Excellent analytical and problem-solving skills', 'Suffolk Construction is seeking an experienced Data Engineer to support Suffolk’s enterprise data management program and systems. The Data Engineer, an emerging role in Suffolk’s data and analytics team, will be pivotal in operationalizing the most-urgent data and analytics initiatives for Suffolk’s digital business initiatives. This role will require both creative and collaborative work with IT and Suffolk’s wider business. It will involve evangelizing effective data management practices and promoting a better understanding of data and analytics.', 'Strong SQL skills with a background for building data models.', 'Demonstrates Suffolk’s Core Values of hard work, professionalism, integrity, passion, and caringExcellent analytical and problem-solving skillsSound business acumenPositive attitude with a strong willingness to learnCurious and tenaciousStrong drive to insight', 'We build buildings. Iconic, complex buildings that are changing our cities and transforming our skylines. We do it by gathering the people, innovations, and partnerships that are redefining what it means to do our jobs. We see every day as an opportunity to challenge the status quo and go beyond what we thought was possible.', 'Qualifications:', 'At least 3 years of experience with cloud data warehouses (i.e., Snowflake, AWS Redshift, Azure Synapse), with an aptitude to learn new tools.', 'Work and collaborate with IT to understand the current data architecture, data pipes, data security, and IT data strategy. ', 'A bachelor’s or master’s degree in computer science, statistics, applied mathematics, data management, information systems, or a related quantitative field [or equivalent work experience] is required.At least 5 years or more work experience in data management disciplines including data integration, data modeling, data management, and data quality.At least 3 years of experience working in a cross-functional team and collaborating with business stakeholders in support of departmental and/or multi-departmental data management and analytics initiatives.At least 3 years of experience with cloud data warehouses (i.e., Snowflake, AWS Redshift, Azure Synapse), with an aptitude to learn new tools.Strong SQL skills with a background for building data models.Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources.Proven ability to perform data cleaning, wrangling, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks.Proven ability to design, build, and manage data pipelines for data structures encompassing data transformation, data models, schemas, metadata and workload management.Strong experience with advanced analytics programming languages such a R, Python, C++, Scala, etc.Well-versed in BI tools to include Microsoft PowerBI, Tableau, etc.Experienced in working with IT to build a data-driven infrastructure.Strong verbal and written communication skills.Construction experience preferred but not required.', 'About our people', 'At least 3 years of experience working in a cross-functional team and collaborating with business stakeholders in support of departmental and/or multi-departmental data management and analytics initiatives.', 'Positive attitude with a strong willingness to learn', 'About Suffolk', 'We want people who are bold. Curious. Innovative. Caring. Looking for the career opportunity of a lifetime. We’ll challenge and inspire you to be your very best. We’ll embrace what makes you unique and lift you up as you take chances. Here, you’ll find a place where you can act with purpose and integrity, bringing intelligence and grit to every aspect of your job.', 'Sound business acumen', 'Strong drive to insight', 'Develop data models that can be used to make predictions and answer questions for the overall business.', 'Demonstrates Suffolk’s Core Values of hard work, professionalism, integrity, passion, and caring', 'Collaborate with the data and analytics team and business teams to improve data models that feed business intelligence tools, increasing data accessibility, and fostering data-driven decision-making across the organization.', 'Research new uses for existing data.', '\xa0', 'Develop best practices for standard naming conventions and coding practices to ensure consistency of data models.', 'Analyze and organize raw data; prepare data for descriptive modeling.Combine raw data from different sources and explore ways to enhance data quality and reliability.Ability to prepare curated datasets for self-service consumption (data democratization) Develop and maintain a scalable data pipeline and build out new API integrations to support continuing increases in data volume and complexity.Understand and translate business needs into data models supporting long-term solutions.Work and collaborate with IT to understand the current data architecture, data pipes, data security, and IT data strategy. Prepare data for predictive and prescriptive modeling.Create logical and physical data models using best practices to ensure high data quality and reduced redundancy.Develop best practices for standard naming conventions and coding practices to ensure consistency of data models.Recommend opportunities for reuse of data models in new environments.Design, construct, install, test, and maintain data management systems.Collaborate with the data and analytics team and business teams to improve data models that feed business intelligence tools, increasing data accessibility, and fostering data-driven decision-making across the organization.Develop data models that can be used to make predictions and answer questions for the overall business.Implement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders.Work closely with all business units and engineering teams to develop a long-term data platform architecture strategy.Research new uses for existing data.Evaluate data models and physical databases for variances and discrepancies.Support the data visualization team and data science team with formatted data and provides data model QA/QC.', 'Strong experience with advanced analytics programming languages such a R, Python, C++, Scala, etc.', 'Analyze and organize raw data; prepare data for descriptive modeling.', 'Well-versed in BI tools to include Microsoft PowerBI, Tableau, etc.']",Mid-Senior level,Full-time,Management,Construction,2021-03-18 14:34:51
Data Engineer,IDEXX,"Westbrook, ME",2 weeks ago,Be among the first 25 applicants,"['', 'What You Can Expect In This Role', 'Bachelor’s degree or equivalent combination of education and experience required.3+ years of professional business experience with data related projects.Experience with databases: Oracle, MySQL, SnowflakeProficiency in Structured Query Language (SQL), Scala, and Python or equivalent programming language.Experience with data integration/ETL tools such as Informatica PowerCenter or Sesame Relational Junction.Familiarity with cloud technology for big data initiatives such as Amazon Web Services (AWS), EMR, Spark, Hive, and Presto.Understanding of data warehousing solutions and relational database theory.Strong communication skills, both verbal and written, including the ability to translate technical subject matter to non-technical audiences (both as a speaker and listener). Demonstrated initiative in resolving problems, balancing conflicting requirements in partnership with others.Strong customer service and business relationship-building skills Planning and organizational skills, with ability to prioritize and be flexible with changing business needs.Initiative and self-motivation to work under minimal supervision with latitude for independent judgment.Ability to work independently and in teams. Ability to problem solve and draw conclusions, in some cases with limited information.Experience with Agile software development methodology.', 'Provide guidance on data design and requirements to other development and business teams.', 'Understanding of data warehousing solutions and relational database theory.', 'Propose or develop semantic layer features for the enterprise model.', 'Initiative and self-motivation to work under minimal supervision with latitude for independent judgment.', 'Implement measures to ensure data accuracy and accessibility.', ""Define, design, and implement data integration, management, storage, consumption, backup and recovery solutions that ensure high performance of the organization's enterprise data."", 'Strong communication skills, both verbal and written, including the ability to translate technical subject matter to non-technical audiences (both as a speaker and listener). ', 'Ability to work independently and in teams. ', 'Strong customer service and business relationship-building skills ', 'Monitor performance and utilization.', 'Experience with databases: Oracle, MySQL, Snowflake', 'Demonstrated initiative in resolving problems, balancing conflicting requirements in partnership with others.', 'Bachelor’s degree or equivalent combination of education and experience required.', 'Experience with Agile software development methodology.', ""Design and implement scalable, reliable distributed data processing frameworks and analytical infrastructure using multiple technologies.Define, design, and implement data integration, management, storage, consumption, backup and recovery solutions that ensure high performance of the organization's enterprise data.Develop Structured Query Language (SQL), Data Definition Language (DDL) and Python, Scala or equivalent programming scripts to support data pipeline development, problem solving, data validation and performance tuning.Adhere and contribute to naming conventions, data governance practices, and thorough testing principles.Document data design tasks or project requirementsImplement measures to ensure data accuracy and accessibility.Identify and resolve data-oriented problems, such as missing, duplicate or incorrect data.Monitor performance and utilization.Provide guidance on data design and requirements to other development and business teams.Propose or develop semantic layer features for the enterprise model.Provide ongoing maintenance and process improvements for data initiatives."", 'Document data design tasks or project requirements', 'Adhere and contribute to naming conventions, data governance practices, and thorough testing principles.', 'Proficiency in Structured Query Language (SQL), Scala, and Python or equivalent programming language.', 'Familiarity with cloud technology for big data initiatives such as Amazon Web Services (AWS), EMR, Spark, Hive, and Presto.', 'What You Will Need To Succeed', '3+ years of professional business experience with data related projects.', ' IDEXX is an equal opportunity employer. Applicants will not be discriminated against because of race, color, creed, sex, sexual orientation, gender identity or expression, age, religion, national origin, citizenship status, disability, ancestry, marital status, veteran status, medical condition, or any protected category prohibited by local, state, or federal laws.', 'Design and implement scalable, reliable distributed data processing frameworks and analytical infrastructure using multiple technologies.', ' IDEXX values a diverse workforce and workplace and strongly encourages women, people of color, LGBTQ+ individuals, people with disabilities, members of ethnic minorities, foreign-born residents, and veterans to apply. ', 'Identify and resolve data-oriented problems, such as missing, duplicate or incorrect data.', 'Develop Structured Query Language (SQL), Data Definition Language (DDL) and Python, Scala or equivalent programming scripts to support data pipeline development, problem solving, data validation and performance tuning.', 'Data Engineer', 'Provide ongoing maintenance and process improvements for data initiatives.', 'Experience with data integration/ETL tools such as Informatica PowerCenter or Sesame Relational Junction.', 'Planning and organizational skills, with ability to prioritize and be flexible with changing business needs.', 'Ability to problem solve and draw conclusions, in some cases with limited information.']",Not Applicable,Full-time,Information Technology,Biotechnology,2021-03-18 14:34:51
Data Engineer,KohliSys,"McLean, VA",3 days ago,Be among the first 25 applicants,"['', '60 Minute Panel Interview', '30 Minutes Screen with Jim Naslund', '323 2-5pm, and 324 1030 and 1130 2nd Round', 'Targeting 329330Freddie Macrsquos InvestmentsCapital Markets Division is currently seeking a Data Engineer to assist in modernizing our Collateral Modeling process. This modernization involves porting SAS code to Python (Pandas and PySpark) code that will run in Amazon Web Services (AWS) Elastic Map Reduce (EMR). This project is in progress with some of the pieces already in production, we are looking to add an additional engineer to accelerate the process. Responsibilities bull Translate existing SAS code into Python code. We are using both Pandas data frames and PySpark data frames so knowledge of both is required. bull Verify that the Python version of the SAS code is equivalent to the SAS version. This involves running both processes, comparing the output, and resolving any differences. bull Leverage PySpark and AWS EMR to parallelize the process and reduce the runtime. bull Optimize the Python code to reduce the runtime. bull Enhance the Python process to be fault-tolerant and contain checkpoints to make rerunning a subset of the process more efficient. bull Write automated tests for Python code. bull Peer review code and automated tests, help team members with design and implementation challenges. Qualifications bull At least 3 years of experience developing production Python code bull A strong understanding of Pandas and PySpark bull A strong understanding of SQL bull Experience with SAS bull Solid understanding of software design principles Preferred Skills bull BS in Computer Science or equivalent experience bull Experience with cloud computing and storage services, particularly AWS EMR bull Experience writing automated unit, integration, regression, performance and acceptance tests bull Strong quantitative skills (statistics, econometrics, linear algebra)', 'Slots TBD', 'Spark and Panda. Hands-on experience working on automated evaluation models - ideally within the mortgage industry.Preference for AWS Elastic Map Reduce. Pyspark is key skill, if candidates does not have it ndash must be willing to learn quickly.Must have SAS Development skills, done a migration from SAS to Python. (Pandas and PySpark)Interview Information 2 Rounds 1st Round', '8 Slots Scheduled']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer - Palo Alto,Prealize Health,"Palo Alto, CA",5 days ago,Be among the first 25 applicants,"['', 'Experience building systems governed under\u202fHIPAA, PHI\u202fand PII compliance\u202frequirements ', 'You Will Love This Job If', 'Work with other teams (data science and clinical) to develop feature sets for their research efforts. ', 'You are a good communicator, able to work with cross-functional groups. ', 'Develop systems to support the ingestion of both streaming and batch data. ', 'Experience building systems governed under\u202fHIPAA, PHI\u202fand PII compliance\u202frequirements Prior experience in the Healthcare Industry and FHIR standards is a plus ', 'Minimum 1 years of AWS cloud platform experience ', 'BS/MS in Engineering, Computer Science, or equivalent. Seasoned engineer built scalable data platform for large volume and complex data structure ', 'Create data assets (data warehouses, data lakes, etc.) based on complex structured/unstructured data sources. ', 'Experience in CI/CD, and modern DevOps practices ', 'Responsibilities', 'Develop and maintain a robust and highly scalable data-pipeline to support our research and operations teams. ', 'Develop framework, processes, and tests to monitor and ensure data quality. Work with other teams (data science and clinical) to develop feature sets for their research efforts. Investigate new technologies to support the continuous improvement of our data-pipeline. Develop automation tools to build more reliable systems and facilitate the ingestion of new data sources. Develop systems to support the ingestion of both streaming and batch data. ', 'Qualifications', 'You are passionate and energetic about helping people and enjoy bringing diverse individuals together. ', 'You are passionate about working with data. You are passionate about making an impact in healthcare and want to work in a fast-paced environment. You are passionate and energetic about helping people and enjoy bringing diverse individuals together. You are a good communicator, able to work with cross-functional groups. ', 'Minimum 4 years of programming experience in Python and SQL Minimum 1 years of Spark data platform experience (PySpark, Databricks, AWS EMR). Minimum 1 years of AWS cloud platform experience Experience with near real-time stream data processing (Spark Streaming). Experience in CI/CD, and modern DevOps practices ', 'BS/MS in Engineering, Computer Science, or equivalent. ', 'You are passionate about working with data. ', 'Develop automation tools to build more reliable systems and facilitate the ingestion of new data sources. ', 'Prior experience in the Healthcare Industry and FHIR standards is a plus ', 'Develop framework, processes, and tests to monitor and ensure data quality. ', 'Minimum 4 years of programming experience in Python and SQL ', 'You are passionate about making an impact in healthcare and want to work in a fast-paced environment. ', 'Experience with near real-time stream data processing (Spark Streaming). ', 'Investigate new technologies to support the continuous improvement of our data-pipeline. ', 'Minimum 1 years of Spark data platform experience (PySpark, Databricks, AWS EMR). ', 'Seasoned engineer built scalable data platform for large volume and complex data structure ', 'You can see the bigger picture while also enjoying getting into the details. ', 'You are always trying to improve systems. ', 'Develop and maintain a robust and highly scalable data-pipeline to support our research and operations teams. Create data assets (data warehouses, data lakes, etc.) based on complex structured/unstructured data sources. ', 'You can see the bigger picture while also enjoying getting into the details. You are always trying to improve systems. ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Vitamin T,United States,2 weeks ago,86 applicants,"['', 'What you will do:', 'Required Skills and Competencies:\xa0', 'Participating in meaningful and thorough code reviews.', '5+ Years of Python Development in Apache SparkYou have a strong passion for new technologies and driving innovation.Delivery of high quality & production ready code is high priority for you.Learning technologies and expanding your skillset is always on your mind.', 'Strong hands on experience in Python and SparkExperience in Databricks, Delta Lake, Parquet, Hive, and Spark SQLProficient in a Linux environmentExperience in AWS: S3 / EMR / Lambda', 'You have a strong passion for new technologies and driving innovation.', 'Create and maintain technical documentation.', 'Experience in Databricks, Delta Lake, Parquet, Hive, and Spark SQL', 'Who you are:', 'Ensure performance, quality and accuracy of our data.', 'Responsibilities', 'Position Description:', 'This role requires relocation to New York or Kansas City * Client will provide relo package', 'Partner with your peers in Product, Design, Scrum Masters and technology to develop our next gen data infrastructure.', 'Hands-on software developer with deep technical expertise in Python and Apache SparkPartner with your peers in Product, Design, Scrum Masters and technology to develop our next gen data infrastructure.Create and maintain technical documentation.Participating in meaningful and thorough code reviews.Ensure performance, quality and accuracy of our data.', 'Qualifications', 'Learning technologies and expanding your skillset is always on your mind.', 'Proficient in a Linux environment', 'Delivery of high quality & production ready code is high priority for you.', 'Strong hands on experience in Python and Spark', '5+ Years of Python Development in Apache Spark', '\xa0', 'The technology team is looking for a Sr. Big Data Engineer who will focus on building out a data lake using AWS Databricks platform. In this role you will work with a team of developers, scrum masters, and business analysts in an Agile environment to make the data on the Big Data Platform accessible for the needs of the organization through Atomic Models and Analytical Data Stores. You will coordinate with a variety of departments to translate high level business requirements into detailed system requirements. You will participate in the entire software development lifecycle by analyzing raw data, writing ETL pipelines in python, and solving data quality issues as they arise during development and after deployment.', 'Hands-on software developer with deep technical expertise in Python and Apache Spark', 'Required Skills and Competencies:', 'Experience in AWS: S3 / EMR / Lambda', 'THE POSITION']",Mid-Senior level,Full-time,Engineering,Staffing and Recruiting,2021-03-18 14:34:51
Data Engineer,Amgen,"Tampa, FL",3 weeks ago,88 applicants,"['', 'Experience on AWS, familiar with EC2, S3, Redshift/Spectrum, Glue, Athena, RDS, Lambda, and API gateway', 'Ability to learn quickly, be organized and detail oriented ', 'Responsibilities', 'Serve as system admin to manage AWS and Databricks platform;', 'Be a key team member assisting in design and development of the data pipeline for Global Data and Analytics teamCollaborate with Data Architects, Business SME’s, and Data Scientists to design and develop end-to-end data pipeline to meet fast paced business need across geographic regionsServe as system admin to manage AWS and Databricks platform;Adhere to best practices for coding, testing and designing reusable code/componentAble to explore new tools, technologies that will help to improve ETL platform performanceParticipate in sprint planning meetings and provide estimations on technical implementation; Collaborate and communicate effectively with the product teams', 'Hands on development experience with Informatica Power Center, MDM, Data Integration Hub', 'Experience with Tableau Dashboard and Tableau Server', 'Experience with software development (Java, Python preferred), end-to-end system design', 'Experience with data modeling for both OLAP and OLTP databases, hands-on experience with SQL, preferred Oracle, PostgreSQL, and Hive SQL', 'Experience with Pharmaceutical industry, commercial operations ', 'Adhere to best practices for coding, testing and designing reusable code/component', 'Collaborate with Data Architects, Business SME’s, and Data Scientists to design and develop end-to-end data pipeline to meet fast paced business need across geographic regions', 'Experience with ETL tool, for example Informatica PowerCenter', 'Experience with software development (Java, Python preferred), end-to-end system designExperience with data modeling for both OLAP and OLTP databases, hands-on experience with SQL, preferred Oracle, PostgreSQL, and Hive SQLExperience with ETL tool, for example Informatica PowerCenterAbility to learn quickly, be organized and detail oriented Hands on development experience with Informatica Power Center, MDM, Data Integration HubExperience with software DevOps CI/CD tools, such Git, JenkinsExperience on AWS, familiar with EC2, S3, Redshift/Spectrum, Glue, Athena, RDS, Lambda, and API gatewayExperience with Apache Airflow and Apache SparkExperience with Tableau Dashboard and Tableau ServerExperience with Pharmaceutical industry, commercial operations ', 'Basic Qualifications', 'Experience with Apache Airflow and Apache Spark', 'Able to explore new tools, technologies that will help to improve ETL platform performance', 'Experience with software DevOps CI/CD tools, such Git, Jenkins', 'Be a key team member assisting in design and development of the data pipeline for Global Data and Analytics team', 'Participate in sprint planning meetings and provide estimations on technical implementation; Collaborate and communicate effectively with the product teams', 'Preferred Qualifications']",Not Applicable,Full-time,Information Technology,Biotechnology,2021-03-18 14:34:51
Lead Data Engineer,Talking Rain Beverage Company®,"Home, KS",22 hours ago,Be among the first 25 applicants,"['', 'Six Weeks of Paid Parental Leave. ', '3+ years of proven experience delivering technical engineering projects, following DevOps and GitFlow methods, ideally operating in an Agile delivery environment. Working knowledge of SQL for the use of data extraction, cleansing, and analysis, as well as demonstrated ability to write code in Python, R, or Scala. Integrity is at the core of who you are. You earn people’s trust with your ability to maintain confidentiality. You drive, not ride. You’re stellar at prioritization, time-management, and troubleshooting. You’re curious, which sometimes means you push back, productively challenging the status quo. You’re approachable, which enables you to build and foster relationships internally and externally with people of all backgrounds and technical proficiency. You’re flexible. You pivot on a dime and embrace the change', 'Integrity', 'Foster a collaborative culture of trust, accountability, and performance, empowering the Rain Makers you support to reach their potential. ', 'Medical/Dental/Vision. ', 'Empower our internal Analytics and Data Science teams by establishing connections with foundational data sources, exploring possibilities, sharing recommendations on architecture, and writing the code. Develop and optimize data pipelines, managing day-to-day maintenance and incremental scaling. Lead the flow of work, prioritizing requests, delegating to balance workload, developing workback schedules, and keeping the teams up the date. Build a strong, collaborative relationship with our supporting vendor, Cloud IQ. Proactively advise our teams in Data Engineering, making recommendations to continuously improve the way we work. Improve our data sets, combining data with aggregations and business logic to make it more accessible to end-users. Foster a collaborative culture of trust, accountability, and performance, empowering the Rain Makers you support to reach their potential. Complete other responsibilities as assigned. ', 'To help ensure a safe, positive work environment, this role may be subject to random drug testing (including alcohol, cannabinoids, cocaine, methamphetamine, opiates, phencyclidine). ', 'drive', 'Develop and optimize data pipelines, managing day-to-day maintenance and incremental scaling. ', 'What We Offer', 'Fitness Reimbursements. ', 'Build a strong, collaborative relationship with our supporting vendor, Cloud IQ. ', 'Integrity is at the core of who you are. You earn people’s trust with your ability to maintain confidentiality. ', 'What You Bring', '401k with Immediate Vesting. ', 'Complete other responsibilities as assigned. ', 'You’re curious, which sometimes means you push back, productively challenging the status quo. ', 'Improve our data sets, combining data with aggregations and business logic to make it more accessible to end-users. ', 'Direct Reports: ', 'Reports to:', 'Lead the flow of work, prioritizing requests, delegating to balance workload, developing workback schedules, and keeping the teams up the date. ', 'Working knowledge of SQL for the use of data extraction, cleansing, and analysis, as well as demonstrated ability to write code in Python, R, or Scala. ', 'Phone Stipend. ', 'You’re flexible. You pivot on a dime and embrace the change', 'Who We Are', 'Three Weeks of Vacation. ', 'You’re approachable, which enables you to build and foster relationships internally and externally with people of all backgrounds and technical proficiency. ', 'Empower our internal Analytics and Data Science teams by establishing connections with foundational data sources, exploring possibilities, sharing recommendations on architecture, and writing the code. ', 'You drive, not ride. You’re stellar at prioritization, time-management, and troubleshooting. ', 'flexible', '3+ years of proven experience delivering technical engineering projects, following DevOps and GitFlow methods, ideally operating in an Agile delivery environment. ', 'curious', ""What You'll Do"", 'approachable', 'Proactively advise our teams in Data Engineering, making recommendations to continuously improve the way we work. ', 'Please note, eligibility for our full benefits package is based on an average of 30+ hours per week in regular (not temporary) positions.']",Associate,Full-time,Information Technology,Food & Beverages,2021-03-18 14:34:51
Consumer Data Engineer,WunderLand Group,"Hayward, CA",7 days ago,64 applicants,"['', 'Skills and Abilities', 'Author data quality requirements and ensure they meet business and technical needs', '5+ years of experience with a third generation programming language such as Python or Java', 'Ensure KCD follows appropriate legal and regulatory guidance provided by Legal and IT (CCPA/GDPR, IT Security)', 'Proactively manage data quality and connections via robust and reliable data validation processes. Ensure data is delivered to where it needs to be, on time, within budget and with minimal errors.', 'Collaborate with agency partners to ensure Clorox’s overall KCD data architecture suits its use casesOn-going management of data architecture as our KCD continues to scale: create structure, schema design, tables, views, stored procedures, and the creation and execution of multiple database queries.Maintain appropriate documentation such as Data dictionaries and Entity Relationship diagrams (ERD).Maintain our growing data model in support of the business', ' 5+ years of ETL & data migration experience 5+ years of API integration experience 5+ years of SQL & data warehouse or database development experience (DDL, DML, Views, Triggers, Functions, Stored Procedures) 5+ years of relational data modeling5+ years of experience with a third generation programming language such as Python or Java 5+ years working in a cross-functional team that must include some Marketing counterpartsSkills and Abilities3+ years of experience working in a cloud environment, Azure or GCP preferred 5+ years of API integration experienceExposure to NoSQL databasesExperience with large volumes of data / data warehousingExperience with database development and administration, SQL server preferred Experience with database / query optimizationExtremely proficient with writing complex queries and best practices', 'Lead acceptance testing around data integrations', 'What’s Required To Apply', ' 5+ years of relational data modeling', '3+ years of experience working in a cloud environment, Azure or GCP preferred', 'Lead the planning of database & data warehouse objects in partnership with our agency partners', 'Understand businesses’ use cases for KCD and their relative priority to help prioritize the engineering queue', 'Identify any needs for additional tools or technology; make recommendations as necessary', ' 5+ years of API integration experience', ' 5+ years of ETL & data migration experience', 'Experience with database development and administration, SQL server preferred', 'Proactively manage data quality and connections via robust and reliable data validation processes. Ensure data is delivered to where it needs to be, on time, within budget and with minimal errors.Author data quality requirements and ensure they meet business and technical needsManage various data quality processes including, but not limited to, de-duplication, identity unification, data completion, etc.Ensure KCD follows appropriate legal and regulatory guidance provided by Legal and IT (CCPA/GDPR, IT Security)', 'Establish and maintain data integrations between Clorox’s datalake and receiving systems', 'Build and manage API integrations', ' 5+ years working in a cross-functional team that must include some Marketing counterparts', 'Extremely proficient with writing complex queries and best practices', ' 5+ years of SQL & data warehouse or database development experience (DDL, DML, Views, Triggers, Functions, Stored Procedures)', ' 491454', 'Maintain appropriate documentation such as Data dictionaries and Entity Relationship diagrams (ERD).', ' Experience with database / query optimization', 'What You’ll Be Doing', 'Build and manage technical integration with Clorox’s privacy software to support compliance procedures for CCPA and GDPR.', 'Experience with large volumes of data / data warehousing', 'Establish and maintain data integrations between Clorox’s datalake and receiving systemsBuild and automate data pipelines between Clorox’s datalake and external systemsLead the planning of database & data warehouse objects in partnership with our agency partnersBuild and manage API integrationsUnderstand businesses’ use cases for KCD and their relative priority to help prioritize the engineering queueLead acceptance testing around data integrationsIdentify any needs for additional tools or technology; make recommendations as necessaryContinuously identify optimizations that improve business resultsBuild and manage technical integration with Clorox’s privacy software to support compliance procedures for CCPA and GDPR.', 'Continuously identify optimizations that improve business results', 'Collaborate with agency partners to ensure Clorox’s overall KCD data architecture suits its use cases', 'Maintain our growing data model in support of the business', 'On-going management of data architecture as our KCD continues to scale: create structure, schema design, tables, views, stored procedures, and the creation and execution of multiple database queries.', 'Manage various data quality processes including, but not limited to, de-duplication, identity unification, data completion, etc.', 'Build and automate data pipelines between Clorox’s datalake and external systems', 'Exposure to NoSQL databases']",Associate,Contract,Production,Marketing and Advertising,2021-03-18 14:34:51
Data Engineer,InCloudCounsel,San Francisco Bay Area,2 weeks ago,30 applicants,"['', 'Requirements:', '3-5 years of experience in data engineering', 'San Francisco', 'In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification form upon hire.', 'Excellent communication and problem solving skills and ability to effectively collaborate with technical and business partners', 'Medical, dental, and vision insurance401K, pre-tax benefitsParental leaveGenerous vacation policyAnnual professional development stipendRemote work flexibility', 'Generous vacation policy', 'Previous experience developing ML pipelines is a plus', 'Develop, implement and maintain data pipelines and data processing code to support product features', 'Fluency in SQL and python and proven ability to ship flexible, maintainable code', 'Benefits', 'Create new data sets and tooling to help ML and analytics teams uncover insights and develop NLP-based solutions to legal tech problems', 'As a Data Engineer, you will implement data pipelines, data infrastructure and python SDKs in support of product initiatives. Collaborating with product and engineering teams, you will translate other teams’ requirements into actionable engineering solutions with a focus on rapid iteration.\xa0', 'Familiarity with data governance frameworks and Agile methodology', 'The role will report directly to and receive mentorship from our Director of Data Infrastructure, and the package includes a competitive base salary, equity, and full benefits. The preferred location for this role is\xa0San Francisco.*', 'Comfort using AWS tooling and experience maintaining modern data infrastructure (warehousing, job scheduling)', 'Champion efforts to enforce data governance, quality and security across the organization', 'About the team:\xa0We’re all about freedom. InCloudCounsel is on a mission to free companies from outdated legal processes and corporate lawyers from outdated work models. We’re a group of former Big Law lawyers, business professionals, and engineers working together to modernize the legal industry. The people behind developing our product, servicing our customers and lawyer partners, and driving our business operations are dynamic individuals who have also achieved the freedom to do what’s important to them - they’re musicians, dancers, photographers, sailors, surfers, world travelers, home flippers, and animal lovers - and our freedom inspires us to free others.', '*Until further notice, all InCloudCounsel employees are working remotely from home.', 'Responsibilities:', 'Annual professional development stipend', 'Support usage and maintenance of our data infrastructure (i.e., AWS stack, Snowflake, Airflow)\xa0', 'Develop, implement and maintain data pipelines and data processing code to support product featuresCreate new data sets and tooling to help ML and analytics teams uncover insights and develop NLP-based solutions to legal tech problemsSupport usage and maintenance of our data infrastructure (i.e., AWS stack, Snowflake, Airflow)\xa0Champion efforts to enforce data governance, quality and security across the organization', ""InCloudCounsel is seeking a\xa0Data Engineer\xa0to join our data team and help us redefine the way contracts are negotiated and tracked, and the way attorneys can work and live. InCloudCounsel modernizes legal processes, globally, for many of the world's leading companies. We offer scalable, end-to-end solutions for processing routine contracts and abstracting complex documents by pairing our worldwide network of experienced lawyers with AI-enabled software. We also offer the first purpose-built software tool designed to help funds manage their obligations."", 'Interest in legal tech is a plus', '401K, pre-tax benefits', 'Data Engineer', 'NOTE to External Agencies: We are not accepting any blind submissions or resumes from recruitment agencies. Any resumes sent to a member of InCloudCounsel will NOT be accepted or considered as a submission.', 'Our data platform is nascent, and you are excited by the opportunity that accompanies this. New tooling around automation and data processing will help our ML and analytics teams build out InCloudCounsel’s legal intelligence services. Great communication -- both written and verbal, tailored to both engineers and business stakeholders -- is key to success in this role.', 'Experience developing and productionizing complex data processing pipelines', 'Medical, dental, and vision insurance', 'Parental leave', 'About the team:', 'Remote work flexibility', '3-5 years of experience in data engineeringFluency in SQL and python and proven ability to ship flexible, maintainable codeExperience developing and productionizing complex data processing pipelinesComfort using AWS tooling and experience maintaining modern data infrastructure (warehousing, job scheduling)Excellent communication and problem solving skills and ability to effectively collaborate with technical and business partnersFamiliarity with data governance frameworks and Agile methodologyPrevious experience developing ML pipelines is a plusInterest in legal tech is a plus']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Airvet,United States,3 weeks ago,90 applicants,"['', 'Mobile: iOS/Swift and Android/Kotlin', 'Infrastructure: Terraform, Kubernetes on AWS/GCP', 'Our Stack', 'Maintain quality, integrity, and consistency of datasets you’ve produced', 'Big plus: AirflowAWS RedshiftQuick learnerGreat sense of humor :-)', 'Great sense of humor :-)', 'Databases: PostgreSQL, Redis', 'Good analytical and problem-solving skills', 'Benefits/Perks', 'Continuous improvement to the data pipeline through new development or optimization', 'Good written and verbal communication skills', 'Completely remote role\xa0', 'Self-driven with the ability to work in a fast-paced environment and dealing with ambiguity', 'Paid maternity/paternity leave', 'AWS Redshift', 'User experience: We need to enhance our UI to provide an unparalleled user experience for our users.', 'Scaling: We are seeing an increase in the number of connections and live streams. Our backend needs to handle the workload of today and the future.', 'Work closely with stakeholders in Engineering, Finance, Marketing, and Product to\xa0', '1 - 3+ years of experience in SQL querying language', 'Thrives to create a company culture in a truly flat organization that values commitment, roles, and hustle over titles and rigid hierarchies', 'Responsibilities', 'Bachelor’s degree in Computer Science, Math, Science or a related field; or equivalent years of experience in a relevant field1 - 3+ years of experience in Python or/and an equivalent language1 - 3+ years of experience in SQL querying language1 - 3+ years of experience in ETLBasic understanding of data architecture and data warehousingBasic knowledge of software system, including Linux and Cloud infrastructure (AWS or Google Cloud)Self-driven with the ability to work in a fast-paced environment and dealing with ambiguityGood analytical and problem-solving skillsGood written and verbal communication skills', 'Healthy work-life balance encouraged!', 'Healthy work-life balance encouraged!Completely remote role\xa0Vision, Dental, and Medical coverageThrives to create a company culture in a truly flat organization that values commitment, roles, and hustle over titles and rigid hierarchiesPTO - 14 days minimum to keep you freshPaid maternity/paternity leave', 'Vision, Dental, and Medical coverage', 'Build tooling and implement systems to overcome limitations of the data consumption portals when appropriateMaintain quality, integrity, and consistency of datasets you’ve producedDefine and enforce best practices and standards for the teamContinuous improvement to the data pipeline through new development or optimization', 'Requirements', 'Enterprise application: We are building an enterprise-class application to help vets and hospitals to run a more productive practice.', '1 - 3+ years of experience in Python or/and an equivalent language', 'Backend: Go (transactions and network), Python (legacy and data pipeline)Databases: PostgreSQL, RedisInfrastructure: Terraform, Kubernetes on AWS/GCPMobile: iOS/Swift and Android/KotlinWeb: React.js, Next.js, JavaScript (legacy), TypeScript', 'Own and develop data pipelines adhering with data governance principlesWork closely with stakeholders in Engineering, Finance, Marketing, and Product to\xa0', 'Extra Credit', 'Build tooling and implement systems to overcome limitations of the data consumption portals when appropriate', 'Analytics: We need to process at scale our sessions to gain insights and alert users about the health of the pets.', 'Since its inception in 2019, Airvet has helped over 100,000 pets live healthier and happier lives and is rapidly growing its network of vets. In July 2020, we raised $14 million in Series A. As the company scales its products and operations to new markets and customers, we are beginning to tackle new challenges that include:', 'Bachelor’s degree in Computer Science, Math, Science or a related field; or equivalent years of experience in a relevant field', '1 - 3+ years of experience in ETL', 'Founded in Los Angeles, Airvet is the #1 rated and reviewed consumer pet telemedicine app that offers face to face consultation and advice from a licensed veterinarian all from the comfort of your mobile phone. Airvet is the preferred platform of Veterinary industry leaders with its cloud-based technology. Airvet offers unique, attentive, and empathetic virtual care and workflow efficiency solutions that allow veterinarians to provide animal lovers the best care they can get. Through Airvet’s mobile app, pets and parents in the US and Canada have access to quality care from the most trusted veterinarians within seconds, anytime and anywhere. Airvet is a $30 flat fee with no time limit and follow-ups included.\xa0', 'Web: React.js, Next.js, JavaScript (legacy), TypeScript', 'PTO - 14 days minimum to keep you fresh', 'Own and develop data pipelines adhering with data governance principles', 'Basic knowledge of software system, including Linux and Cloud infrastructure (AWS or Google Cloud)', 'Data Engineer', 'gather functional and non-functional requirements, and deliver the appropriate data model and high-quality datasets in a timely manner', 'Basic understanding of data architecture and data warehousing', 'User experience: We need to enhance our UI to provide an unparalleled user experience for our users.Scaling: We are seeing an increase in the number of connections and live streams. Our backend needs to handle the workload of today and the future.Enterprise application: We are building an enterprise-class application to help vets and hospitals to run a more productive practice.Analytics: We need to process at scale our sessions to gain insights and alert users about the health of the pets.Optimization: We need to optimize our video transcoding and streaming under high latency network conditions.', 'Optimization: We need to optimize our video transcoding and streaming under high latency network conditions.', 'The company has just started assembling a team of top-notch engineers and product professionals. We are seeking a data engineer to help us build a data pipeline to support business intelligence and later advanced analytics, including ML. It is a greenfield development initiative with a lot of career and leadership opportunities.', 'Quick learner', 'Backend: Go (transactions and network), Python (legacy and data pipeline)', 'Big plus: Airflow', 'Define and enforce best practices and standards for the team']",Entry level,Full-time,Information Technology,Veterinary,2021-03-18 14:34:51
Data Engineer (Infrastructure and ETL),Verana Health,"Knoxville, TN",2 weeks ago,Be among the first 25 applicants,"['', ' A minimum of a BS degree in computer science, software engineering, or related scientific discipline coupled with 5 years of software/data engineering experience.  3+ years of experience as a data engineer using AWS tools.  Extensive Python TDD experience. Demonstrated hands-on experience with Apache Spark (and PySpark), Hadoop, and HDFS. A strong programming background and the ability to apply solid architectural principles and design patterns in data processing implementation using AWS ETL toolchain. Demonstrated expertise in logical and physical data modeling (ER). Demonstrated proficiency in using data modeling tools, data profiling toolchain, and data (de)normalization techniques. ', 'Demonstrated proficiency in using data modeling tools, data profiling toolchain, and data (de)normalization techniques.', 'A strong programming background and the ability to apply solid architectural principles and design patterns in data processing implementation using AWS ETL toolchain.', 'Deep hands-on understanding of data synchronization techniques as well as WORM concepts to develop cloud based microservices architecture that is scalable.', 'Design and create data-centric APIs and services based on AWS stack - Glue, Redshift, Lambda, AWS EMR, Kafka etc.', ' Strong hands-on experience in design & development of cloud services, cloud security in AWS cloud environment. Strong hands-on experience in design & development of large scale distributed applications hosted on cloud that include load balancing using  Deep understanding of Data Quality, Metadata management, semantic datastore and Data Ingestion, Curation and serving. Hands on experience managing containers using Kubernetes environment and strong understanding of load balancers. Design and Development of “stateless” applications on the cloud. Deep hands-on understanding of data synchronization techniques as well as WORM concepts to develop cloud based microservices architecture that is scalable. Familiarity with CI/CD and release concepts for designing distributed & scalable software for the cloud. Design and create data-centric APIs and services based on AWS stack - Glue, Redshift, Lambda, AWS EMR, Kafka etc. Generate software solutions using Apache Spark, Hive, Presto, and other big data frameworks. Analyzing the systems and requirements to provide the best technical solutions w.r.t. flexibility, scalability, and reliability of underlying architecture. Incubate/Design/Create SQL - DDL, DQL, DML, DCL. Characterize database instances by utilizing SQL Server toolchain to identify and understand ER models. Write stored procedures to augment APIs for better performance. Understand the viability and utility of the DevOps style (agile/lean) approaches to software development. Work closely with technology teams to understand processes and policies driving the team goals. Document and improve software testing and release processes across the entire data ingestion team.  ', 'Strong hands-on experience in design & development of large scale distributed applications hosted on cloud that include load balancing using ', 'Benefits', 'Job Duties And Responsibilities', 'Document and improve software testing and release processes across the entire data ingestion team. ', 'Hands on experience managing containers using Kubernetes environment and strong understanding of load balancers.', 'Deep understanding of Data Quality, Metadata management, semantic datastore and Data Ingestion, Curation and serving.', '3+ years of experience as a data engineer using AWS tools. ', 'Demonstrated expertise in logical and physical data modeling (ER).', 'Incubate/Design/Create SQL - DDL, DQL, DML, DCL.', 'Design and Development of “stateless” applications on the cloud.', 'Familiarity with CI/CD and release concepts for designing distributed & scalable software for the cloud.', 'Characterize database instances by utilizing SQL Server toolchain to identify and understand ER models. Write stored procedures to augment APIs for better performance.', 'Demonstrated hands-on experience with Apache Spark (and PySpark), Hadoop, and HDFS.', 'Understand the viability and utility of the DevOps style (agile/lean) approaches to software development.', 'Generate software solutions using Apache Spark, Hive, Presto, and other big data frameworks.', 'Analyzing the systems and requirements to provide the best technical solutions w.r.t. flexibility, scalability, and reliability of underlying architecture.', 'Extensive Python TDD experience.', 'A minimum of a BS degree in computer science, software engineering, or related scientific discipline coupled with 5 years of software/data engineering experience. ', 'Strong hands-on experience in design & development of cloud services, cloud security in AWS cloud environment.', 'Work closely with technology teams to understand processes and policies driving the team goals.', 'Basic Requirements']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Darwill,"Oak Brook, IL",1 week ago,87 applicants,"['Design and implement standard and ad-hoc dashboards and reports using tools such as Tableau and Domo ', 'Recent graduates with related internships are welcome to apply', 'Experience with Business Intelligence and Reporting tools such as Tableau, Domo, etc. ', '\xa0 ', 'Responsibilities', 'Darwill is growing and we are looking to add a Data Engineer to our Data Science Practice. The Data Engineer will work as part of a collaborative team to build and maintain scalable data platforms that support data-informed decision-making across the organization. ', '2+ years of proven experience in the Data Engineering/Business Intelligence field ', 'Architect, develop, and support data pipelines, database designs, and data warehouses ', 'Requirements', 'Thorough experience with RDBMS tools including SSIS and including cloud-based technologies (AWS, etc.) ', 'Excellent written and verbal communication skills, comfort communicating at varying levels of detail across a range of audiences ', 'Develop and maintain build, testing, and documentation standards and ensure standards are set and met across the organization ', 'Architect, develop, and support data pipelines, database designs, and data warehouses Design, build, and launch new data models and efficient data pipelines including ETL processes Design and implement standard and ad-hoc dashboards and reports using tools such as Tableau and Domo Produce written test cases, unit tests, and integration tests and perform necessary application testing to ensure code quality and adherence to project requirements Develop and maintain build, testing, and documentation standards and ensure standards are set and met across the organization Collect, analyze, interpret, and summarize data in preparation for statistical and analytic reports Partner with cross-functional teams of Account Executives, Data Scientist, and Software Engineers to understand data needs and deliver on those needs ', 'Produce written test cases, unit tests, and integration tests and perform necessary application testing to ensure code quality and adherence to project requirements ', 'Design, build, and launch new data models and efficient data pipelines including ETL processes ', 'Strong knowledge of SQL, database performance optimization, and infrastructure components including operating systems, storage, networks, hardware, and virtual machines ', 'Bachelors/Masters degree in Computer Science, Mathematics, or other technical field ', 'Experience with python or R ', 'Bachelors/Masters degree in Computer Science, Mathematics, or other technical field 2+ years of proven experience in the Data Engineering/Business Intelligence field Strong knowledge of SQL, database performance optimization, and infrastructure components including operating systems, storage, networks, hardware, and virtual machines Thorough experience with RDBMS tools including SSIS and including cloud-based technologies (AWS, etc.) Experience with Business Intelligence and Reporting tools such as Tableau, Domo, etc. Experience with python or R Excellent written and verbal communication skills, comfort communicating at varying levels of detail across a range of audiences Experience in Direct Marketing and/or Marketing Analytics a plus Recent graduates with related internships are welcome to apply', 'Responsibilities ', '\xa0', 'Experience in Direct Marketing and/or Marketing Analytics a plus ', 'Requirements ', 'Partner with cross-functional teams of Account Executives, Data Scientist, and Software Engineers to understand data needs and deliver on those needs ', 'Collect, analyze, interpret, and summarize data in preparation for statistical and analytic reports ']",Associate,Full-time,Information Technology,Printing,2021-03-18 14:34:51
Data Platform Engineer,"DynPro, Inc.","Raleigh, NC",1 day ago,Be among the first 25 applicants,"['', 'Build Enterprise data lake/hub(Raw, trusted/Curated, Data Provisioning Layer), Data Warehouse (Subject Areas, Logical and Physical Data marts, and Data Labs) for Reporting and Digital Invocation capabilities.', 'Knowledge of file standards (AVRO, Parquet, ORC, etc.), HIVE Meta stores, SPARK, Scala, Python & other process for persistent Data Frames to access data.', 'Experience with data warehouses, data mart creation, and data mart access control and data provisioning', ' 5+ years Data Modelling experience 10+ years of hands on experience in working with one or more of the following: SQL, Oracle, ETL, and database diagnostic tools; Informatica tools sets (Enterprise Data Catalog, Data Quality, and AXON Data Governance) o  Azure Data Engineer Certification is preferred', 'All other relevant duties as assigned.', ' Knowledge, Skills and Abilities\t Knowledge of EDP Architecture, Reference Data Model (ADRM), Hadoop, HDFS, & Linux directory & file Management concepts & code syntax Good understanding of Azure Data storage options (ADLS, HDInsight, structure & Namespace) Knowledge of file standards (AVRO, Parquet, ORC, etc.), HIVE Meta stores, SPARK, Scala, Python & other process for persistent Data Frames to access data. Knowledge in HIVE, Spark (Scala/Python), Data Lake Infrastructure tools & Techniques Experience with Data Quality tools that can be used to convert data quality business rule logic into HIVE or Spark query language for execution in the Data Lake Experience with ETL/ELT -type data integration tools that can be used to author HIVE QL or Spark SQL code through a visual interface (i.e., low-code / no-code techniques for authoring data transformation pipelines) Experience using Data Ingestion/ ETL / ELT or Change Data Capture (CDC) software in an HDFS environment Experience in using variety of data stores including Azure Data Lakes, SQL Database, Azure Data Warehouse and Azure Synapse Experience with data warehouses, data mart creation, and data mart access control and data provisioning Experience in implementing the Enterprise Data Models, Business Data Models, Logical and Physical Data Marts Models, and Sandboxes Knowledge of modern batch and real-time file transfer protocols, e.g., Kafka, Apache Nifi, Storm, a plus. Knowledge/Experience of SAP Data Ingestion Tools, Modern Data Lake Management, Azure   Experience\t 5+ years Data Modelling experience 10+ years of hands on experience in working with one or more of the following: SQL, Oracle, ETL, and database diagnostic tools; Informatica tools sets (Enterprise Data Catalog, Data Quality, and AXON Data Governance) o  Azure Data Engineer Certification is preferred', 'Experience in implementing the Enterprise Data Models, Business Data Models, Logical and Physical Data Marts Models, and Sandboxes', 'Unit testing, promote tested pipelines to UAT, & PROD environments, Code Productionization and transitioned to Prod Support Analyst teams', 'Implement HIVE / Spark meta-layers and data frames and enable the query access to Data filesWorks in a safe manner collaborating as a team member to achieve all outcomes.', 'Knowledge, Skills and Abilities\t Knowledge of EDP Architecture, Reference Data Model (ADRM), Hadoop, HDFS, & Linux directory & file Management concepts & code syntax Good understanding of Azure Data storage options (ADLS, HDInsight, structure & Namespace) Knowledge of file standards (AVRO, Parquet, ORC, etc.), HIVE Meta stores, SPARK, Scala, Python & other process for persistent Data Frames to access data. Knowledge in HIVE, Spark (Scala/Python), Data Lake Infrastructure tools & Techniques Experience with Data Quality tools that can be used to convert data quality business rule logic into HIVE or Spark query language for execution in the Data Lake Experience with ETL/ELT -type data integration tools that can be used to author HIVE QL or Spark SQL code through a visual interface (i.e., low-code / no-code techniques for authoring data transformation pipelines) Experience using Data Ingestion/ ETL / ELT or Change Data Capture (CDC) software in an HDFS environment Experience in using variety of data stores including Azure Data Lakes, SQL Database, Azure Data Warehouse and Azure Synapse Experience with data warehouses, data mart creation, and data mart access control and data provisioning Experience in implementing the Enterprise Data Models, Business Data Models, Logical and Physical Data Marts Models, and Sandboxes Knowledge of modern batch and real-time file transfer protocols, e.g., Kafka, Apache Nifi, Storm, a plus. Knowledge/Experience of SAP Data Ingestion Tools, Modern Data Lake Management, Azure  ', 'Experience\t 5+ years Data Modelling experience 10+ years of hands on experience in working with one or more of the following: SQL, Oracle, ETL, and database diagnostic tools; Informatica tools sets (Enterprise Data Catalog, Data Quality, and AXON Data Governance) o  Azure Data Engineer Certification is preferred', '10+ years of hands on experience in working with one or more of the following: SQL, Oracle, ETL, and database diagnostic tools;', 'Work with data and analytics experts to strive for greater functionality in our data systems.', 'Document HIVE/ SparkSQL meta-data store standards, & Policies to access the data in Data Lake Assemble large, complex data sets that meet functional / non-functional business requirements.', 'Build data transformation routines to flatten and denormalize data into quarriable data sets and Provide 3rd level Prod Support', 'Works in a safe manner collaborating as a team member to achieve all outcomes.', 'Experience with ETL/ELT -type data integration tools that can be used to author HIVE QL or Spark SQL code through a visual interface (i.e., low-code / no-code techniques for authoring data transformation pipelines)', 'Create data tools for team members to assist them in building and optimizing analytics production.', 'Implement logic and data transformation scripts to convert from raw to semantic / canonical form of data and Implement Data Quality Rules using HIVE/Spark SQL', 'Document Technical Data file standard in Data Lake (AVRO, ORC, Parquet, etc.), HDFS Build the pipeline to push Enterprise + Semantic data to Azure Synapse Analytics', 'Education', ' Knowledge of EDP Architecture, Reference Data Model (ADRM), Hadoop, HDFS, & Linux directory & file Management concepts & code syntax Good understanding of Azure Data storage options (ADLS, HDInsight, structure & Namespace) Knowledge of file standards (AVRO, Parquet, ORC, etc.), HIVE Meta stores, SPARK, Scala, Python & other process for persistent Data Frames to access data. Knowledge in HIVE, Spark (Scala/Python), Data Lake Infrastructure tools & Techniques Experience with Data Quality tools that can be used to convert data quality business rule logic into HIVE or Spark query language for execution in the Data Lake Experience with ETL/ELT -type data integration tools that can be used to author HIVE QL or Spark SQL code through a visual interface (i.e., low-code / no-code techniques for authoring data transformation pipelines) Experience using Data Ingestion/ ETL / ELT or Change Data Capture (CDC) software in an HDFS environment Experience in using variety of data stores including Azure Data Lakes, SQL Database, Azure Data Warehouse and Azure Synapse Experience with data warehouses, data mart creation, and data mart access control and data provisioning Experience in implementing the Enterprise Data Models, Business Data Models, Logical and Physical Data Marts Models, and Sandboxes Knowledge of modern batch and real-time file transfer protocols, e.g., Kafka, Apache Nifi, Storm, a plus. Knowledge/Experience of SAP Data Ingestion Tools, Modern Data Lake Management, Azure ', ' Design, build, Optimize and Maintain data pipeline. Optimize Data Ingestion Infrastructure Validation, understanding of Source Systems, Source and Target Data Model Architecture artifacts Document Technical Data file standard in Data Lake (AVRO, ORC, Parquet, etc.), HDFS Build the pipeline to push Enterprise + Semantic data to Azure Synapse Analytics Build Enterprise data lake/hub(Raw, trusted/Curated, Data Provisioning Layer), Data Warehouse (Subject Areas, Logical and Physical Data marts, and Data Labs) for Reporting and Digital Invocation capabilities. Document HIVE/ SparkSQL meta-data store standards, & Policies to access the data in Data Lake Assemble large, complex data sets that meet functional / non-functional business requirements. Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics. Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Create data tools for team members to assist them in building and optimizing analytics production. Work with data and analytics experts to strive for greater functionality in our data systems. Unit testing, promote tested pipelines to UAT, & PROD environments, Code Productionization and transitioned to Prod Support Analyst teams Build data transformation routines to flatten and denormalize data into quarriable data sets and Provide 3rd level Prod Support Implement the directory structure and namespace for storing transformed + curated data, for both current state plus historical views Implement logic and data transformation scripts to convert from raw to semantic / canonical form of data and Implement Data Quality Rules using HIVE/Spark SQL Implement HIVE / Spark meta-layers and data frames and enable the query access to Data filesWorks in a safe manner collaborating as a team member to achieve all outcomes. Works in a safe manner collaborating as a team member to achieve all outcomes. Demonstrate Behaviours that exhibit our organizational Values: Collaboration, Courage, Perseverance, and Passion. Ensure personal adherence with all compliance programs including the Global Business Ethics and Compliance Program, Global Quality policies and procedures, Safety and Environment policies, and HR policies. All other relevant duties as assigned.', 'Ensure personal adherence with all compliance programs including the Global Business Ethics and Compliance Program, Global Quality policies and procedures, Safety and Environment policies, and HR policies.', 'Knowledge in HIVE, Spark (Scala/Python), Data Lake Infrastructure tools & Techniques', 'Knowledge of modern batch and real-time file transfer protocols, e.g., Kafka, Apache Nifi, Storm, a plus.', 'Knowledge of EDP Architecture, Reference Data Model (ADRM), Hadoop, HDFS, & Linux directory & file Management concepts & code syntax', 'Experience in using variety of data stores including Azure Data Lakes, SQL Database, Azure Data Warehouse and Azure Synapse', 'Implement the directory structure and namespace for storing transformed + curated data, for both current state plus historical views', '5+ years Data Modelling experience', 'Design, build, Optimize and Maintain data pipeline.', 'Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.', ' Education', 'Experience with Data Quality tools that can be used to convert data quality business rule logic into HIVE or Spark query language for execution in the Data Lake', 'Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources', 'Demonstrate Behaviours that exhibit our organizational Values: Collaboration, Courage, Perseverance, and Passion.', 'Optimize Data Ingestion Infrastructure Validation, understanding of Source Systems, Source and Target Data Model Architecture artifacts', 'Good understanding of Azure Data storage options (ADLS, HDInsight, structure & Namespace)', 'Informatica tools sets (Enterprise Data Catalog, Data Quality, and AXON Data Governance) o  Azure Data Engineer Certification is preferred', 'Experience using Data Ingestion/ ETL / ELT or Change Data Capture (CDC) software in an HDFS environment', 'Knowledge/Experience of SAP Data Ingestion Tools, Modern Data Lake Management, Azure', 'Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.', 'Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Synergis IT + Creative,"Atlanta, GA",6 days ago,102 applicants,"['Immediate opening for a Data Engineer with at least 4 years of experience. This is a REMOTE role with a large, highly successful organization with opportunities for career growth! Contract-to-Hire. Must have SQL or Java8+ experience. Python experienced preferred. Please no C2C. Sponsorship not offered. Benefits available via Synergis IT + Creative. Competitive compensation based on experience. The selected Data Engineer job responsibilities will include building out a customer identity platform for marketing purposes and re-platforming a legacy system written in Hadoop and other languages to a Google Cloud Platform. Experience with GCP preferred.', 'Please forward current resumes to mmartin@synergishr.com for immediate consideration.']",Associate,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Engineer,ClientSolv Inc.,"Littleton, CO",1 week ago,Be among the first 25 applicants,"['', 'Create efficiencies and reduce resource allocation for routine tasks and procedures related to partner data management.', 'Experience using Cloud Native tools such as Kubernetes and Docker in private, public, and hybrid clouds.', 'Ability to read, analyze, and interpret common metrics used to measure and monitor operational performance, define problems, collect data, establish facts, draw valid conclusions, and provide clear and concise communication with a wide audience of internal departments.', 'At least two years of experience using one or more of the following (5+ years of experience for Senior): Java, C++, Python, Go, R, or JavaScript in a Unix/Linux environment', 'Garner key insights from data and communicate these findings to key stakeholders to help them make data-driven decisions. Key technologies may include Tableau, Grafana, Kibana, and R.', 'This 6 month contract-to- hire will be located onsite in Littleton, CO. This role will be in the office Monday-Friday during normal business hours (no remote/telecommuting options available- all work is to be performed onsite).', 'Proven ability to implement data-driven solutions in a production environment using tools such as Hadoop, Impala, Hive, NiFi, Athena, Redshift, ElasticSearch, BigTable, or Airflow.', 'Experience applying machine learning and statistical modeling using common data science techniques such as clustering, regression – logistical and linear, confidence intervals, and pattern recognition.', 'Acquire big data input from numerous partners. Key technologies may include Python, Elastic Logstash, and Kafka.Normalize complicated data sources to convert potentially unusable data into a format that can be efficiently used by software and/or employees. Key technologies may include Spark, Lambda, Beam, and Flink.Aggregate data from multiple sources into a single location and format where correlation is possible. Key technologies may include SQL Server, MYSQL, Postgres, Cassandra, Impala, Kudu, and Athena.Validate large amounts of data to ensure data quality in a variety of different ways depending on the data and its consumer. Key technologies may include Python and Excel.Garner key insights from data and communicate these findings to key stakeholders to help them make data-driven decisions. Key technologies may include Tableau, Grafana, Kibana, and R.Provide analyses on data sets to identify trends, issues, and opportunities.Work with partners on efficient data processes while helping them keep their data as clean as possible.Create efficiencies and reduce resource allocation for routine tasks and procedures related to partner data management.Learn industry standards and best practices surrounding data analysis in order to continuously improve our team and systems by implementing them. Create data that is valuable for the organization, either operationally to help drive decisions or financially to gain revenue. Key technologies may include Google Analytics and Qualtrics', 'Bachelor’s degree in Computer Science, Computer Engineering, Applied Math, Statistics, or a related technical degree.At least two years of experience (5+ years of experience for Senior) using ETL (Extract, Transform, and Load) concepts and techniques on messy data sets and large databases.At least two years of experience using one or more of the following (5+ years of experience for Senior): Java, C++, Python, Go, R, or JavaScript in a Unix/Linux environmentAt least two years of experience using Tableau.At least two years of experience with SQL-like query language and table design.Strong communication skills to work with partners internal and external to manage data flow into our infrastructure.Ability to read, analyze, and interpret common metrics used to measure and monitor operational performance, define problems, collect data, establish facts, draw valid conclusions, and provide clear and concise communication with a wide audience of internal departments.Growth mindset: Proven ability to quickly learn new concepts, processes, software, and development ideas.', 'Proven ability to implement data-driven solutions in a production environment using tools such as Hadoop, Impala, Hive, NiFi, Athena, Redshift, ElasticSearch, BigTable, or Airflow.Experience using Cloud Native tools such as Kubernetes and Docker in private, public, and hybrid clouds.Experience applying machine learning and statistical modeling using common data science techniques such as clustering, regression – logistical and linear, confidence intervals, and pattern recognition.', 'Provide analyses on data sets to identify trends, issues, and opportunities.', 'Primary Responsibilities Are As Follows', 'Normalize complicated data sources to convert potentially unusable data into a format that can be efficiently used by software and/or employees. Key technologies may include Spark, Lambda, Beam, and Flink.', 'At least two years of experience (5+ years of experience for Senior) using ETL (Extract, Transform, and Load) concepts and techniques on messy data sets and large databases.', 'At least two years of experience using Tableau.', 'Create data that is valuable for the organization, either operationally to help drive decisions or financially to gain revenue. Key technologies may include Google Analytics and Qualtrics', 'Company Description', 'Aggregate data from multiple sources into a single location and format where correlation is possible. Key technologies may include SQL Server, MYSQL, Postgres, Cassandra, Impala, Kudu, and Athena.', 'Strong communication skills to work with partners internal and external to manage data flow into our infrastructure.', 'Work with partners on efficient data processes while helping them keep their data as clean as possible.', 'Job Description', 'The Following Additional Qualifications Are a Plus', 'Validate large amounts of data to ensure data quality in a variety of different ways depending on the data and its consumer. Key technologies may include Python and Excel.', 'At least two years of experience with SQL-like query language and table design.', 'Growth mindset: Proven ability to quickly learn new concepts, processes, software, and development ideas.', 'Bachelor’s degree in Computer Science, Computer Engineering, Applied Math, Statistics, or a related technical degree.', 'Acquire big data input from numerous partners. Key technologies may include Python, Elastic Logstash, and Kafka.', 'Learn industry standards and best practices surrounding data analysis in order to continuously improve our team and systems by implementing them. ']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Advantis Global,"Austin, TX",4 weeks ago,104 applicants,"['', ' Experience with Docker and Kubernetes preferred Experience with AWS a bonus', 'Working with cloud technologies to deploy your applications', 'Helping us leverage large-scale data stores by building out ETL pipelines and utilities in Spark and Hive ', 'Aggregating key metrics for business partners to inform key decisions', 'Strong documentation and technical writing skills ', 'Developing robust, low latency and fault tolerant pipelines to support business critical systems', 'ETL', 'Opportunity For You', 'Experience with Docker and Kubernetes preferred', 'Experience with Apache Big Data Frameworks (Hadoop, Spark, Hive) ', 'Key Success Factors', 'Strong skills in SQL, Java and/or Python', ' Strong skills in SQL, Java and/or Python Experience with Apache Big Data Frameworks (Hadoop, Spark, Hive)  Familiarity with workflow scheduling/orchestration tools (Oozie, Jenkins) Strong documentation and technical writing skills  Attention to detail and excellent communication skills ETL ', ' Helping us leverage large-scale data stores by building out ETL pipelines and utilities in Spark and Hive  Developing robust, low latency and fault tolerant pipelines to support business critical systems Aggregating key metrics for business partners to inform key decisions Working with cloud technologies to deploy your applications ', 'Experience with AWS a bonus', 'About This Opportunity', 'Familiarity with workflow scheduling/orchestration tools (Oozie, Jenkins)', 'Attention to detail and excellent communication skills']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer - Austin,Kofi Group,"Austin, TX",7 days ago,Be among the first 25 applicants,"['', 'Comfortable with deep learning library (TensorFlow, Pytorch) implementation and integration', 'Comfortable with AWS/Google Cloud', 'Strong knowledge of pipeline', '4 years of experience', 'Apply Now', 'Knowledge of data pipelines, especially Apache Spark/Kafka/Beam', '4 years of experienceStrong knowledge of pipelineKnowledge of data pipelines, especially Apache Spark/Kafka/BeamComfortable with AWS/Google CloudComfortable with deep learning library (TensorFlow, Pytorch) implementation and integration']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Doximity,"San Francisco, CA",1 month ago,Be among the first 25 applicants,"['', 'You know your way around Git and AWS services.', 'You agree that concise and effective written and verbal communication is a must for a successful team.', 'About Us', 'Build, maintain, and scale data pipelines that empower Doximity’s products.', 'Our recruiting process', 'You are user experience and product focused. You build solutions while thinking about the impact it has on our users and enhances the product.', 'Pre-IPO stock incentives', ' Collaborate with product managers, data analysts, and machine learning engineers to develop pipelines and ETL tasks in order to facilitate the extraction of insights. Build, maintain, and scale data pipelines that empower Doximity’s products. Establish data architecture processes and practices that can be scheduled, automated, replicated and serve as standards for other teams to leverage.  Work alongside others in planning and carrying out the implementation of solutions that are focused on enhancing products, leading one or two projects at any given time. ', 'Comprehensive benefits including medical, vision, dental, Life/ADD, 401k, flex spending accounts, commuter benefits, equipment budget, educational resources and conference access', "" Explore our stack We have over 500 private repositories in Github containing our pipelines, our own internal multi-functional tools, and open-source projects We have worked as a distributed team for a long time; we're currently about 65% distributed Find out more information on the Doximity engineering blog Our company core values Our recruiting process Our product development cycle Our on-boarding & mentorship process "", 'Explore our stack', 'You are no stranger to data warehousing and designing data models.', 'Generous time off policy', 'We have over 500 private repositories in Github containing our pipelines, our own internal multi-functional tools, and open-source projects', 'Work alongside others in planning and carrying out the implementation of solutions that are focused on enhancing products, leading one or two projects at any given time.', ""We have worked as a distributed team for a long time; we're currently about 65% distributed"", 'Our product development cycle', 'Our on-boarding & mentorship process', 'Our company core values', '.. and much more! For a full list, see our career page', 'You have the ability to self-manage, prioritize, and deliver functional solutions.', 'Collaborate with product managers, data analysts, and machine learning engineers to develop pipelines and ETL tasks in order to facilitate the extraction of insights.', 'Family support and planning benefits', 'You are foremost an engineer, making you passionate for high code quality, automated testing, design patterns, and other engineering best practices.', 'Benefits & Perks', 'Doximity is proud to be an equal opportunity employer, and committed to providing employment opportunities regardless of race, religious creed, color, national origin, ancestry, physical disability, mental disability, medical condition, genetic information, marital status, sex, gender, gender identity, gender expression, pregnancy, childbirth and breastfeeding, age, sexual orientation, military or veteran status, or any other protected classification. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law.', 'More info on Doximity', ""Here's How You Will Make An Impact"", 'You have professional experience developing data processing, enrichment, transformation, and integration solutions.', 'You are fluent in Python and SQL.', 'You are able to work within an existing data architecture finding gaps in it and enhancing it to ensure solutions are fault tolerant, scalable, and easy to iterate upon.', 'Find out more information on the Doximity engineering blog', 'About You', ' You have professional experience developing data processing, enrichment, transformation, and integration solutions. You are fluent in Python and SQL. You are no stranger to data warehousing and designing data models. You are foremost an engineer, making you passionate for high code quality, automated testing, design patterns, and other engineering best practices. You care deeply about the data being generated. You study the data and extract insights from it before you process it. You are user experience and product focused. You build solutions while thinking about the impact it has on our users and enhances the product. You are able to work within an existing data architecture finding gaps in it and enhancing it to ensure solutions are fault tolerant, scalable, and easy to iterate upon. You have the ability to self-manage, prioritize, and deliver functional solutions. You know your way around Git and AWS services. You agree that concise and effective written and verbal communication is a must for a successful team. ', 'You care deeply about the data being generated. You study the data and extract insights from it before you process it.', 'Establish data architecture processes and practices that can be scheduled, automated, replicated and serve as standards for other teams to leverage. ', ' Generous time off policy Comprehensive benefits including medical, vision, dental, Life/ADD, 401k, flex spending accounts, commuter benefits, equipment budget, educational resources and conference access Family support and planning benefits Pre-IPO stock incentives .. and much more! For a full list, see our career page ']",Entry level,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Engineer,true[X],"Los Angeles, CA",2 weeks ago,57 applicants,"['', 'Strong knowledge of SQL required\xa0', '100% company-paid health, dental, and vision coverage for you and your dependents, as well as life insurance and disability coverage', 'Experience in the advertising industry and with real-time analytics is a plus\xa0', 'Working knowledge of serialization formats and their trade-offs (columnar vs row-based) Experience debugging and optimizing Spark jobs\xa0', ""Changing the established guidelines of an industry, especially one as rooted as digital advertising, isn't an easy or quick effort, but we believe it's the right thing to do and we want to be the ones to do it. We're looking for hungry people who are passionate about disrupting the digital media world, and agree that we can do better for viewers, advertisers and publishers out there.\xa0"", 'Familiarity with database fundamentals, such as ACID, snowflake schema, normalized/denormalized data\xa0', 'About the Job', 'Experience working with HDFS and S3 preferred\xa0', 'Maintain high standards of code quality, and encourage the same by providing constructive code reviews to collaborators', 'A universal communicator\xa0— you are able to explain the most technical data to the least technical people without any confusionA proactive problem solver\xa0— you are constantly looking for ways to work smarter, delivering new efficiencies anywhere an improvement can be madeA builder\xa0— you are passionate about collecting, storing, and analyzing big data', 'What you’ll be doing:', 'Be an active and engaged owner of our data infrastructure', 'Who you are:', 'Familiarity with columnar database, key-value stores, document stores, stream\xa0processing, time series databases, data warehouses, and OLAP\xa0preferred', ""As a Data Engineer, you will be an important part of the team that owns our data infrastructure. This is no small responsibility: As an AdTech company, data is our lifeblood, and there's a lot of it. Someone with experience ingesting, processing, storing, analyzing, and working in a big-data environment will have ample opportunity for success in this role.\xa0\xa0"", 'Must be a strong written and verbal communicator', 'Build and modify Spark jobs (in Scala) to perform various tasks, from reading Kinesis streams using Spark Streaming, to joining and aggregating huge data sets, to integrating with third party data sources', 'Unlimited paid time off - we trust your discretion', 'A proactive problem solver', 'BS in Computer Science or related field required\xa0', 'What you have:', 'We are committed to an inclusive and diverse work environment. true[X] is an equal opportunity employer. We do not discriminate based on race, color, ethnicity, ancestry, national origin, religion, sex, gender, gender identity, gender expression, sexual orientation, age, disability, veteran status, genetic information, marital status or any legally protected status.', 'It’s no secret that we work hard, but we also strive to create an office environment where the lines between work and play are blurred. This means we offer these great perks to help keep our team healthy, productive, and happy.', 'A universal communicator\xa0— you are able to explain the most technical data to the least technical people without any confusion', 'About the Company', 'Benefits & Perks.', 'Be curious and seek to understand all aspects of our business', 'A universal communicator\xa0', 'Build and modify Spark jobs (in Scala) to perform various tasks, from reading Kinesis streams using Spark Streaming, to joining and aggregating huge data sets, to integrating with third party data sourcesDevelop and launch new features to adapt to evolving business needsBe an active and engaged owner of our data infrastructureBe curious and seek to understand all aspects of our businessMaintain high standards of code quality, and encourage the same by providing constructive code reviews to collaboratorsTroubleshoot and resolve issues, problems, and errors encountered across various systemsCollaborate with Data Science, Product, Research, and Engineering teams to iterate on the roadmapGather requirements when underspecified', 'Strong knowledge with Spark (using Scala)\xa0', '100% company-paid health, dental, and vision coverage for you and your dependents, as well as life insurance and disability coverageUnlimited paid time off - we trust your discretionOpportunities for profit sharing, bonuses, and ownership401(k) plan plus company match\xa0Cell phone reimbursement and subsidized gym membershipAnnual professional development stipend\xa0', 'Cell phone reimbursement and subsidized gym membership', 'Gather requirements when underspecified', 'A builder\xa0— you are passionate about collecting, storing, and analyzing big data', 'Annual professional development stipend\xa0', 'A proactive problem solver\xa0— you are constantly looking for ways to work smarter, delivering new efficiencies anywhere an improvement can be made', 'Collaborate with Data Science, Product, Research, and Engineering teams to iterate on the roadmap', 'BS in Computer Science or related field required\xa0Strong knowledge of SQL required\xa0Strong knowledge with Spark (using Scala)\xa0Working knowledge of serialization formats and their trade-offs (columnar vs row-based) Experience debugging and optimizing Spark jobs\xa0Familiarity with database fundamentals, such as ACID, snowflake schema, normalized/denormalized data\xa0Must be a strong written and verbal communicatorFamiliarity with columnar database, key-value stores, document stores, stream\xa0processing, time series databases, data warehouses, and OLAP\xa0preferredExperience working with HDFS and S3 preferred\xa0Familiarity with Data Science tooling in Spark preferredExperience in the advertising industry and with real-time analytics is a plus\xa0', 'A builder', 'Since our founding in 2007, we have been committed to advancing three core principles in our own products and in the broader advertising industry: quality, accountability, and transparency, driven by our core belief that all are necessary for ensuring a better consumer experience. Every decision we make is guided by a deep understanding of human attention.', 'Opportunities for profit sharing, bonuses, and ownership', '401(k) plan plus company match\xa0', 'Familiarity with Data Science tooling in Spark preferred', 'At true[X], acquired by Gimbal in 2020, our mission is to provide the best advertising experience for consumers, the best monetization for premium publishers, and the best return for brand advertisers. Across connected TV, mobile and desktop devices we empower premium publishers to create experiences that allow them to serve the widest possible audience by optimizing consumers’ time and attention, and delivering impactful results for advertisers. For brands and advertisers, true[X] delivers on our true[ATTENTION] guarantee of effective, zero-waste, high-engagement ad experiences that drive measurable brand funnel impact.', 'Troubleshoot and resolve issues, problems, and errors encountered across various systems', 'Develop and launch new features to adapt to evolving business needs']",Mid-Senior level,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Data Engineer - Healthcare Analytics,EXL,New York City Metropolitan Area,3 days ago,39 applicants,"['', 'EXL Analytics offers an exciting, fast paced and innovative environment, which brings together a group of sharp and entrepreneurial professionals who are eager to influence business decisions. From your very first day, you get an opportunity to work closely with highly experienced, world class analytics consultants.', 'We provide guidance/ coaching to every employee through our mentoring program wherein every junior level employee is assigned a senior level professional as advisors.', 'Data engineer is responsible for expanding and optimizing healthcare payer data, data pipeline architecture and data flow to enable analysis across several dimensions.', 'EXL (NASDAQ:EXLS) is a leading operations management and analytics company that helps businesses enhance growth and profitability in the face of relentless competition and continuous disruption. EXL supports companies to improve global operations, enhance data-driven insights, increase customer satisfaction, and manage risk and compliance. Headquartered in New York, New York, EXL has more than 24,000 professionals in locations throughout the United States, Europe, Asia (primarily India and Philippines), Latin America, Australia and South Africa.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Expert level skills in SQL, data integration, data modeling and data architecture.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Working knowledge of Big Data concepts and Hadoop environment.', 'Analytics requires different skill sets at different levels within the organization. At EXL Analytics, we invest heavily in training you in all aspects of analytics as well as in leading analytical tools and techniques.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Build large-scale batch and real-time data pipelines using Snowflake cloud data technologies.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Understanding of enterprise data management concepts (Data Governance, Data Engineering, Data Science, Data Lake, Data Warehouse, Data Sharing, Data Applications)', 'Role and Responsibilities:', '""EOE/Minorities/Females/Vets/Disabilities""', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Strong Database experience in Hadoop/Hive , DBMS , SQL server', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience with programming scripting and data science languages such as Python, UNIX, SQL, Pyspark.', 'What we offer:', 'Sky is the limit for our team members. The unique experiences gathered at EXL Analytics sets the stage for further growth and development in our company and beyond.', 'Please visit www.exlservice.com for more information about EXL Analytics.', 'HL7, FHIR, C-CDA, JSON etc', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience with Snowflake cloud data platform including hands-on experience with Snowflake utilities like SnowSQL , SnowPipe , and experience in administering Snowflake.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Accountable for clinical data review and analysis for complex, global projects collated from various data sources formatted in various industry standards (HL7, FHIR, C-CDA, JSON etc.)', 'Qualifications:', 'EXL Health is seeking a Data Engineer – Healthcare Analytics, this role will initially be remote but once Stay at Home orders are lifted, could relocate to New York.', 'You can expect to learn many aspects of businesses that our clients engage in. You will also learn effective teamwork and time-management skills - key aspects for personal and professional growth', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Understand healthcare data, including clinical data in both proprietary and industry standard formats (FHIR, C-CDA etc.) and develop mappings / transformation solution between various formats.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Some cloud experience as a developer/engineer like Google cloud platform', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience in building data models, including conceptual, logical, and physical for Enterprise Relational, and Dimensional Databases.']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Junior Data Engineer (AWS),Invesco Ltd.,"Atlanta, GA",2 weeks ago,27 applicants,[''],Associate,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer - Operations (contract),Nuna Inc.,"San Francisco, CA",2 weeks ago,25 applicants,"['', ' Work closely with client-facing teams ', ' Use your knowledge of SQL to perform data analysis based on business requirements and data profiling reports. ', ' Experience working with healthcare data and databases. ', 'YOU BRING', 'YOUR IMPACT', ' Collaborate with product managers, data scientists, data analysts and engineers to define user requirements and database design specifications for our clients’ needs. ', ' Ability to use SQL or other query and scripting languages to aggregate, gather and manipulate data. ', ' Ability to construct and debug complex SQL queries. ', ' Perform data quality assessment, measurement, and reporting ', 'YOUR TEAM', ' Analyze data feed requirements received from vendors, translate business requirements into technical design specifications. ', '  Extract, transform and load data from source to target through multiple stages as defined by mapping documents   Perform data quality assessment, measurement, and reporting   Manage, track and communicate client file deliveries   Collaborate with product managers, data scientists, data analysts and engineers to define user requirements and database design specifications for our clients’ needs.   Analyze data feed requirements received from vendors, translate business requirements into technical design specifications.   Use your knowledge of SQL to perform data analysis based on business requirements and data profiling reports.   Maintain and ensure monthly databases are delivered on-time to our customers   Serve as a technical resource in resolving client issues related to database or other data issues   Work closely with client-facing teams  ', 'YOUR OPPORTUNITIES', ' Demonstrated track record working with data warehouse and ETL architectures ', ' Serve as a technical resource in resolving client issues related to database or other data issues ', ' Strong communication and teamwork skills. ', ' Manage, track and communicate client file deliveries ', ' Data Engineering is at the core of Nuna’s promise to deliver exceptional and actionable data insights to our clients. We are responsible for the scalable comprehension, ingestion, cleaning, and deploying of client data on schedule - think of us as the heart muscle that pumps data throughout Nuna. And because quality and consistency are our hallmarks, we’re more than a little obsessed with detail and process. ', ' Experience with data quality processes, data quality checks, validations, data quality metrics definition and measurement. ', ' Ability to operate with cross-functional teams both internal and client facing ', '  Demonstrated track record working with data warehouse and ETL architectures   Experience working with healthcare data and databases.   Ability to use SQL or other query and scripting languages to aggregate, gather and manipulate data.   Experience with data quality processes, data quality checks, validations, data quality metrics definition and measurement.   Ability to construct and debug complex SQL queries.   Ability to operate with cross-functional teams both internal and client facing   Strong communication and teamwork skills.  ', ' Maintain and ensure monthly databases are delivered on-time to our customers ', ' Extract, transform and load data from source to target through multiple stages as defined by mapping documents ']",Associate,Full-time,Information Technology,Hospital & Health Care,2021-03-18 14:34:51
"Lead Data Engineer, Open Source - Data Platform",Visa,"Palo Alto, CA",4 weeks ago,29 applicants,"['', '10 years of work experience with a Bachelor’s Degree or at least 8 years of work experience with an Advanced Degree (e.g. Masters/MBA/JD/MD) or at least 3 years of work experience with a PhD', 'Additional Information', 'Understanding best practices for Big Data (in Hadoop), data warehousing, consumer analytics, knowledge management and key understanding of streaming and other NoSQL databases', 'Payment industry experience is a plus.', 'Relational database and SQL development experience required', 'Provide development lead oversight on Visa’s Hadoop Open Source Platform. Develop and help drive Open Source Hadoop based products and services.', 'Can navigate a Linux terminal with ease', 'In-depth knowledge of the software development life cycle required', 'Quick learner; self-starter, detailed and thorough', 'Experience in using Apache Open source projects, contributed to open source projects is a plus.', 'Experience in logging, metering and alerting open source-based solution is a plus. ', 'Experience in Kubernetes based services development and have strong full stack development background using UI and API frameworks', 'Minimum 5 years of Open Source Hadoop and Big data products experience.', '10 years of work experience with a Bachelor’s Degree or at least 8 years of work experience with an Advanced Degree (e.g. Masters/MBA/JD/MD) or at least 3 years of work experience with a PhDMinimum 5 years of Open Source Hadoop and Big data products experience.', 'Strong skills on mentoring/growing junior people', 'Provide mentorship and help team growth especially on technical side.', 'Strong leadership and team player.', '12 years of work experience with a Bachelor’s Degree or at least 8 years of work experience with an Advanced Degree (e.g. Masters/MBA/JD/MD) or at least 3 years of work experience with a PhDMinimum 5 years of Open Source Hadoop and Big data products experience.Understand all aspects of our distributed systems and learn select set of services in detail. Implement solutions to solve massively distributed technology problems using open source products.Work closely with service engineering, operations, and Hadoop users to engineer applications for Visa Business projects.Active development experience to provide software as service on-prem and in Hybrid-Cloud. Experience in Kubernetes based services development and have strong full stack development background using UI and API frameworksUnderstanding best practices for Big Data (in Hadoop), data warehousing, consumer analytics, knowledge management and key understanding of streaming and other NoSQL databasesHands-on experience in developing and managing development of Data Integration applications for large corporations with experience in both batch and online systems.Experience in using Apache Open source projects, contributed to open source projects is a plus.Experience in logging, metering and alerting open source-based solution is a plus. Can navigate a Linux terminal with easeRelational database and SQL development experience requiredIn-depth knowledge of the software development life cycle requiredOutstanding verbal, written, presentation, facilitation, and interaction skills, including ability to effectively communicate architectural issues and concepts to multiple organization levels and executive management.Quick learner; self-starter, detailed and thoroughStrong customer-centric mindsetStrong leadership and team player.Strong skills on mentoring/growing junior peoplePayment industry experience is a plus.', 'Contribute to the DP strategies and Open Source roadmap development to meet business objectives with existing or emerging technologies. Ability to Understands and Solves Common and Un-common User Problems by Mapping and Onboarding New Use Cases to the Open source Big data services.', ""We're"", 'Work closely with service engineering, operations, and Hadoop users to engineer applications for Visa Business projects.', 'Apply creative thinking/approach to determine technical solutions that further business goals and align with corporate technology strategies, keeping in mind performance, reliability, scalability, usability, security, flexibility, and cost.', 'Strong customer-centric mindset', 'Qualifications', 'Together', 'Must love coding – prepare to spend 80% of the time on hands-on development with development teams', 'Work extensively on providing Open Source Spark as a Service on Kubernetes and Presto to enable users use a data processing service connecting to Hadoop.', 'Company Description', 'Lead internal proof of concept initiatives with Architects and work on multiple enhancement on existing services.', 'Active development experience to provide software as service on-prem and in Hybrid-Cloud. ', 'Primary Responsibilities Will Include', 'Mental/Physical Requirements ', 'Travel Requirements ', 'Hands-on experience in developing and managing development of Data Integration applications for large corporations with experience in both batch and online systems.', ""You're"", 'Basic Qualifications', 'Contribute and holistically offers the Open source Hadoop services that solves current and potential Use Case(s) for Visa Internal Business teams.', 'Outstanding verbal, written, presentation, facilitation, and interaction skills, including ability to effectively communicate architectural issues and concepts to multiple organization levels and executive management.', 'Innovate and Re-pivot Problems with Models and Higher Dimension Concepts to Support Current and Potential Visa Business Use Cases.', 'Understand all aspects of our distributed systems and learn select set of services in detail. Implement solutions to solve massively distributed technology problems using open source products.', 'Job Description', '12 years of work experience with a Bachelor’s Degree or at least 8 years of work experience with an Advanced Degree (e.g. Masters/MBA/JD/MD) or at least 3 years of work experience with a PhD', 'Provide development lead oversight on Visa’s Hadoop Open Source Platform. Develop and help drive Open Source Hadoop based products and services.Contribute and holistically offers the Open source Hadoop services that solves current and potential Use Case(s) for Visa Internal Business teams.Contribute to the DP strategies and Open Source roadmap development to meet business objectives with existing or emerging technologies. Ability to Understands and Solves Common and Un-common User Problems by Mapping and Onboarding New Use Cases to the Open source Big data services.Innovate and Re-pivot Problems with Models and Higher Dimension Concepts to Support Current and Potential Visa Business Use Cases.Work extensively on providing Open Source Spark as a Service on Kubernetes and Presto to enable users use a data processing service connecting to Hadoop.Apply creative thinking/approach to determine technical solutions that further business goals and align with corporate technology strategies, keeping in mind performance, reliability, scalability, usability, security, flexibility, and cost.Lead internal proof of concept initiatives with Architects and work on multiple enhancement on existing services.Must love coding – prepare to spend 80% of the time on hands-on development with development teamsProvide mentorship and help team growth especially on technical side.', 'Preferred Qualifications']",Not Applicable,Full-time,Engineering,Consumer Services,2021-03-18 14:34:51
Data Engineer,Xpanse Inc.,"Seattle, WA",1 month ago,27 applicants,"['', 'Job Overview', 'Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.', ' Experience with big data tools: Hadoop, Spark, Kafka, etc. Experience with relational SQL and NoSQL databases, including Postgres and Cassandra. Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc. Experience with AWS cloud services: EC2, EMR, RDS, Redshift Experience with stream-processing systems: Storm, Spark-Streaming, etc. Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.', 'Qualifications For Data Engineer', 'Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.', 'Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.', 'Assemble large, complex data sets that meet functional / non-functional business requirements.', 'A successful history of manipulating, processing and extracting value from large disconnected datasets.', 'Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.', 'Strong analytic skills related to working with unstructured datasets.', 'Strong project management and organizational skills.', 'Work with data and analytics experts to strive for greater functionality in our data systems.', 'Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.', ' Engineers', 'Build processes supporting data transformation, data structures, metadata, dependency and workload management.', 'Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.', ' Create and maintain optimal data pipeline architecture, Assemble large, complex data sets that meet functional / non-functional business requirements. Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies. Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics. Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Keep our data separated and secure across national boundaries through multiple data centers and AWS regions. Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader. Work with data and analytics experts to strive for greater functionality in our data systems. ', 'Experience with AWS cloud services: EC2, EMR, RDS, Redshift', ' Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases. Experience building and optimizing ‘big data’ data pipelines, architectures and data sets. Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. Strong analytic skills related to working with unstructured datasets. Build processes supporting data transformation, data structures, metadata, dependency and workload management. A successful history of manipulating, processing and extracting value from large disconnected datasets. Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores. Strong project management and organizational skills. Experience supporting and working with cross-functional teams in a dynamic environment. We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools: Experience with big data tools: Hadoop, Spark, Kafka, etc. Experience with relational SQL and NoSQL databases, including Postgres and Cassandra. Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc. Experience with AWS cloud services: EC2, EMR, RDS, Redshift Experience with stream-processing systems: Storm, Spark-Streaming, etc. Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.', 'Experience supporting and working with cross-functional teams in a dynamic environment.', 'Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.', 'Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.', 'Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.', 'Experience with stream-processing systems: Storm, Spark-Streaming, etc.', 'Experience with big data tools: Hadoop, Spark, Kafka, etc.', 'Data', 'Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.', 'Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.', 'Create and maintain optimal data pipeline architecture,', 'We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools: Experience with big data tools: Hadoop, Spark, Kafka, etc. Experience with relational SQL and NoSQL databases, including Postgres and Cassandra. Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc. Experience with AWS cloud services: EC2, EMR, RDS, Redshift Experience with stream-processing systems: Storm, Spark-Streaming, etc. Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.', 'Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.', 'Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Deloitte,"Austin, TX",3 weeks ago,40 applicants,"['', ' Experience in Map Reduce is a plus', ' Leverage automation, cognitive and science-based techniques to manage data, predict scenarios and prescribe actions', 'Work you’ll do', 'Additional Requirements', ' 3+ years of experience in building scalable and high-performance data pipelines using Apache Hadoop, Apache Spark, Pig or Hive', ' Strong data & logical analysis skills', 'Benefits', ' Support the implementation of data integration requirements and develop the pipeline of data from raw to curation layers including the cleansing, transformation, derivation and aggregation of data.', ' Experience with bigdata cross platform compatible file formats like Apache Avro & Apache Parquet', 'Required', "" Bachelor's degree, preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience"", ' Hands on big data/ Hadoop performance tuning and optimization experience', 'Pig ', 'Core JAVA', 'SQL', ' Drive operational efficiency by maintaining their data ecosystems, sourcing analytics expertise and providing As-a-Service offerings for continuous insights and improvements', 'Data Engineer with MapReduce- Project Delivery Specialist', ' Strong SQL knowledge with ability to work with the latest database technologies.', 'Hive', ' Support in the development of technical solutions to business problems', 'Qualifications', 'Recruiter tips', 'Optional / Nice to Have ', ' 5+ years of experience in Core JAVA and SQL', ' Travel up to 25% (While 25% of travel is a requirement of the role, due to COVID-19, non-essential travel has been suspended until further notice.)', 'Corporate citizenship', ' Spark', 'How You’ll Grow', ' 3+ years or experience in Python / Unix Shell Scripting', ' Communicate effectively (written and spoken) and work with the multi-location development teams and self-manage own work', 'The team', ' Limited immigration sponsorship may be available', ' 5+ years of hands-on experience as a Data Engineer or Big Data developer', ' Must be willing to live and work in the Greater Austin, TX area (preferred) or San Jose, California. Relocation assistance provided to qualifying candidates', ' Implement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to generate insights leveraging cloud-based platforms', 'Deloitte’s culture']",Not Applicable,Full-time,Management,Accounting,2021-03-18 14:34:51
Data Engineer (Life sciences),GForce Life Sciences,Greater Boston,1 day ago,64 applicants,"['', ""If you're interested in joining an organization that is working in a truly unique space in a role that is crucial to their continued growth and success, this is a great role for you."", 'Qualifications', 'Advanced degree in bioinformatics, computer science, or related field', 'Experience working in an AWS environment', 'Our client (a cutting-edge biotech) is looking for a Data Engineer to join their growing team.', ""5+ years' relevant industry work experience (software development and/or database engineering)"", ""Advanced degree in bioinformatics, computer science, or related field5+ years' relevant industry work experience (software development and/or database engineering)Experience working in an AWS environment"", ""The hiring manager is looking for someone with both clinical and laboratory experience (this is a MUST) to jump in and design the organization's data warehouse that will be the home for vast amounts of data from a variety of sources. The person in this role will also be building out validation and QC tools that will ensure the quality of the data in the warehouse in addition to working with data visualization teams to make data more available to the team.""]",Mid-Senior level,Full-time,Research,Biotechnology,2021-03-18 14:34:51
Data Analytics Engineer,Lunavi ,United States,5 hours ago,51 applicants,"['', 'Voluntary Long-Term Disability Plan\xa0', 'Work with stakeholders/ end users in the software development lifecycle – PMs, BAs, testing etc. ', 'Degree in information technology, computer science, business, engineering or equivalent work experience ', ' ', 'Training and Development Programs\xa0', 'Reimagine Everything.', '100% Employer-Paid Short-Term Disability Plan', 'Ignite Passion. ', 'Development background using C#, Python, or PowerShell ', 'Knowledge of Microsoft technologies such as Azure Data Lake,Azure Data Factory, Databricks, SQL Data Warehouse (now Synapse Analytics), Azure SQL DB, Azure Data Lake and Power BI ', 'Desire to learn and leverage Microsoft Azure platform data technologies ', 'Be Great.', 'Life Assistance and Wellness Programs', 'Flexible Paid Time Off (YOU take the time you need.)8 Day Holiday PayPaid Volunteer DayEmployer Contributed 3 Tier Medical Plan OptionsEmployer Contributed Dental Plan100% Employer-Paid Vision Plan100% Employer-Paid Short-Term Disability Plan100% Employer-Paid $50,000 Life Insurance Plan including AD&DVoluntary Long-Term Disability Plan\xa0Voluntary Benefits including; Accident, Critical Illness, and Medical Bridge OptionsAdditional Supplemental Life Insurance Plan including Spouse and Children3% Employer Match 401k Retirement PlanLife Assistance and Wellness ProgramsGreen Initiatives\xa0Training and Development Programs\xa0Employee Events\xa0100% Employer Paid Gym MembershipAND MORE...\xa0', 'Are you ready to join us?\xa0', 'Take part in discussions with client leadership explaining architecture options and recommendations Designing and delivering Data Analytics solutions Work with developers to build ""Smart Applications"" that incorporate data analytics to drive decision making within the application\u202f Advise clients on database management, integration and BI tools Work alongside team members in design/development of data analytics implementations Perform architectural assessments of the client\'s Enterprise Data Warehouse (EDW) and data analytics systems Work with the sales and delivery teams to create and deliver client proposals and demonstrations Present to client, industry and internal peer groups Work within agile delivery methodology in a leading role as part of a broader Delivery Team Work with stakeholders/ end users in the software development lifecycle – PMs, BAs, testing etc. ', 'Excellent communication skills, with interests in presenting and/or blogging; ability to engage with senior business leaders ', 'Voluntary Benefits including; Accident, Critical Illness, and Medical Bridge Options', 'Green Initiatives\xa0', 'Additional Supplemental Life Insurance Plan including Spouse and Children', 'Own It. Know that your contributions make a direct impact. Be the difference that leads to more successful experiences and outcomes. ', 'Here are just some of the things that we can offer you:', 'Work within agile delivery methodology in a leading role as part of a broader Delivery Team ', 'Work with the sales and delivery teams to create and deliver client proposals and demonstrations ', 'Work alongside team members in design/development of data analytics implementations ', '8 Day Holiday Pay', 'Employee Events\xa0', '100% Employer Paid Gym Membership', 'Lunavi is looking for a Data Analytics Engineer with a background and experience designing and building business intelligence, modern data warehouse, or modern data platform solution development efforts. This role would also be responsible for working in Agile Delivery teams and working with all cross functional roles to help deliver ""smart applications"" leveraging data and analytics to our customers.', 'AND MORE...\xa0', 'Own It. ', 'Employer Contributed Dental Plan', 'Ignite Passion. Share the very best of who you are in everything you do. Create a positive and uplifting environment that inspires others. ', 'Present to client, industry and internal peer groups ', 'Company Description:', 'Employer Contributed 3 Tier Medical Plan Options', 'Software development background ', ""2+ years full-time professional experience in a project-based role for data platform, data warehouse, or business intelligence solution development Background building and delivering business intelligence, data warehouse, or data analytics solutions Ability to take part in envisioning and discovery efforts, providing detailed estimates of deliverables, timeline, and hours Passion for data and analytics, translating data-driven insights into decisions and actions Self-starter, self-managed, quick learner, problem-solver with a positive, collaborative, and team-based attitude Excellent communication skills, with interests in presenting and/or blogging; ability to engage with senior business leaders Degree in information technology, computer science, business, engineering or equivalent work experience Ability to work remotely, at a home-office, with limited travel to client locations - as travel restrictions ease and it's safe to travel again "", 'Be Great. Aspire to know more, do more, and realize your fullest potential. Keep reaching above and beyond to excel and exceed every expectation. ', ""Ability to work remotely, at a home-office, with limited travel to client locations - as travel restrictions ease and it's safe to travel again "", 'Job Description:    ', '100% Employer-Paid Vision Plan', 'This role includes a mix of project delivery and pre-sales activities including discovery, scoping, and estimating; working with a team of Data Engineers to ensure consistent delivery capability and consulting growth as well as and supporting brand leadership efforts with webinars, blogging, and workshops.', 'Self-starter, self-managed, quick learner, problem-solver with a positive, collaborative, and team-based attitude ', 'Knowledge of Agile/Scrum principles and method', 'Take part in discussions with client leadership explaining architecture options and recommendations ', 'Passion for data and analytics, translating data-driven insights into decisions and actions ', 'Designing and delivering Data Analytics solutions ', 'Flexible Paid Time Off (YOU take the time you need.)', 'At Lunavi, we believe in illuminating the path forward and helping our customers navigate what’s next. We are innovators who are combining the power of human ingenuity and technology to deliver unrivaled customer experience. We’re a trusted partner for companies looking to digitally transform their business, modernize business applications, solve traditional IT challenges, and extract ROI from technology. Our high performing teams, deep expertise, and proven processes help to propel businesses forward. ', 'Preferred ', 'Desire to move into a consulting role ', 'Paid Volunteer Day', 'Lunavi\xa0is an equal opportunity employer and will not discriminate against any individual, employee, or application for employment on the basis of race, color, marital status, religion, age, sex, sexual orientation, national origin, handicap, or any other legally protected status recognized by federal, state or local law.', 'Responsibilities ', 'Knowledge of Microsoft technologies such as Azure Data Lake,Azure Data Factory, Databricks, SQL Data Warehouse (now Synapse Analytics), Azure SQL DB, Azure Data Lake and Power BI Desire to learn and leverage Microsoft Azure platform data technologies Development background using C#, Python, or PowerShell Desire to move into a consulting role Software development background Knowledge of Agile/Scrum principles and method', ""Reimagine Everything. Continuously create value by pursuing what's next, what's possible. Deliver a new level of awesome through relentless curiosity. "", 'Job Description:     ', ""Perform architectural assessments of the client's Enterprise Data Warehouse (EDW) and data analytics systems "", '\xa0', 'Our core values:', '100% Employer-Paid $50,000 Life Insurance Plan including AD&D', 'Requirements ', 'Background building and delivering business intelligence, data warehouse, or data analytics solutions ', '2+ years full-time professional experience in a project-based role for data platform, data warehouse, or business intelligence solution development ', ""Click the Apply button and let's talk.\xa0"", 'Work with developers to build ""Smart Applications"" that incorporate data analytics to drive decision making within the application\u202f ', 'Advise clients on database management, integration and BI tools ', 'Ability to take part in envisioning and discovery efforts, providing detailed estimates of deliverables, timeline, and hours ', '3% Employer Match 401k Retirement Plan']",Mid-Senior level,Full-time,Consulting,Information Technology and Services,2021-03-18 14:34:51
Data Engineer ,Portico Benefit Services,"Minneapolis, MN",3 weeks ago,54 applicants,"['', 'Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.', 'BS/BA in information technology, computer science, or related field.\xa05+ years of experience in a Data Engineer roleHands-on at least 2 of the following: SQL Server, Azure Data Factory, Databricks/Spark, Synapse Analytics (SQL DW), and Data LakeExperience building and optimizing data pipelines, architectures and data sets.A successful history of manipulating, processing and extracting value from large disconnected datasets.Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.Security background with PHI and PII a plusExperience supporting and working in an agile teamProficiency with one or more of the following: C#. .NET Core, Azure Functions, DevOps Pipelines, Microsoft SQL Server, Azure Cloud Infrastructure, Front End JavaScript (Vue.js, Angular, etc.)Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.Strong analytic skills related to working with unstructured datasets.Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management.Familiarity with PowerBI conceptsWorking knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.', 'Familiarity with PowerBI concepts', 'Experience building and optimizing data pipelines, architectures and data sets.', 'A successful history of manipulating, processing and extracting value from large disconnected datasets.', 'Design and implement software solutions using modern, cloud-based architectures', 'Assemble large, complex data sets that meet functional / non-functional business requirements', 'Create and maintain optimal data pipeline architecture using Agile development principlesDesign and implement software solutions using modern, cloud-based architecturesAssemble large, complex data sets that meet functional / non-functional business requirementsIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and ‘big data’ technologies.Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.Work with data and analytics experts to strive for greater functionality in our data systems.Coach and mentor others to build their skills for data/analyticsUnderstand and comply with Portico’s Governance, Risk, and Compliance standards (e.g. internal controls, regulatory compliance, policy compliance).', 'Strong analytic skills related to working with unstructured datasets.', 'Work with data and analytics experts to strive for greater functionality in our data systems.', 'Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.', 'Responsibilities', '5+ years of experience in a Data Engineer role', 'Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.', 'Experience supporting and working in an agile team', 'Ability to build processes supporting data transformation, data structures, metadata, dependency and workload management.', 'Qualifications', 'Proficiency with one or more of the following: C#. .NET Core, Azure Functions, DevOps Pipelines, Microsoft SQL Server, Azure Cloud Infrastructure, Front End JavaScript (Vue.js, Angular, etc.)', 'Create and maintain optimal data pipeline architecture using Agile development principles', 'Hands-on at least 2 of the following: SQL Server, Azure Data Factory, Databricks/Spark, Synapse Analytics (SQL DW), and Data Lake', 'BS/BA in information technology, computer science, or related field.\xa0', 'Coach and mentor others to build their skills for data/analytics', 'Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.', 'Portico is seeking a Data Engineer to be a member of the DevOps team and deliver modern, cloud-based solutions to enable access to data for reporting, business intelligence, and operations.', 'Understand and comply with Portico’s Governance, Risk, and Compliance standards (e.g. internal controls, regulatory compliance, policy compliance).', 'Security background with PHI and PII a plus', 'This position is responsible for creating and optimizing data and data pipeline architecture, as well as optimizing data flow and collection.\xa0 \xa0', '\xa0', 'Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and ‘big data’ technologies.', 'Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.']",Mid-Senior level,Full-time,Information Technology,Financial Services,2021-03-18 14:34:51
Data Engineer,United Talent Agency,"Los Angeles, CA",3 days ago,69 applicants,"['', 'Hands-on experience with the data visualization tools such as Tableau', 'Ability to communicate to non-technical co-workers on very technical topics', 'Documentation first mindset', 'Work out of an amazing office – that happens to house one of the top modern art collections in the world', 'What You’ll Get', '5+ years of relevant experience working in a similar role supporting IT SystemsExperience with multiple types of data stores (NoSQL, SQL, Search Indexes)Understanding of common data issues and how to resolve themAbility to communicate to non-technical co-workers on very technical topicsHands-on experience with the data visualization tools such as TableauUnderstanding of data lifecyclesExperience implementing robust system integrations using industry standard integration techniquesDocumentation first mindsetStrong analytical, leadership, problem solving, organizational, and planning skillsAn agile, automate-first & growth mindsetAbility to mentor and be mentoredFamiliarity with cloud-based solutions and how to leverage them together to create an ideal system', 'Experience with multiple types of data stores (NoSQL, SQL, Search Indexes)', 'An agile, automate-first & growth mindset', 'What You’ll Do', 'Automate and streamline data processes to deliver internal and external data sources to be shared and consumed throughout the company', 'Competitive benefits and programs to support your well-being', 'Familiarity with cloud-based solutions and how to leverage them together to create an ideal system', 'Engage business teams to understand requirements, and deliver robust and scalable solutions that can be leveraged for self-service analytics', 'Access to the tools, leadership and resources you’ll need to create and drive a center of excellence', 'Experience implementing robust system integrations using industry standard integration techniques', 'About UTA', 'Understanding of data lifecycles', '5+ years of relevant experience working in a similar role supporting IT Systems', 'Ability to mentor and be mentored', 'The unique and exciting opportunity to work at one of the leading global entertainment companies in Hollywood.Access to the tools, leadership and resources you’ll need to create and drive a center of excellenceThe opportunity to do the best work of your careerWork out of an amazing office – that happens to house one of the top modern art collections in the worldCompetitive benefits and programs to support your well-being', 'What You Need', 'Engage business teams to understand requirements, and deliver robust and scalable solutions that can be leveraged for self-service analyticsConsult with vendors, contractors, application programming staff, and consultants to ensure adherence to industry best practices, data security, and technical standardsAdoption and support of standard data tools within the Azure data ecosystemAutomate and streamline data processes to deliver internal and external data sources to be shared and consumed throughout the company', 'Adoption and support of standard data tools within the Azure data ecosystem', 'Strong analytical, leadership, problem solving, organizational, and planning skills', 'The unique and exciting opportunity to work at one of the leading global entertainment companies in Hollywood.', 'The opportunity to do the best work of your career', 'Consult with vendors, contractors, application programming staff, and consultants to ensure adherence to industry best practices, data security, and technical standards', 'Understanding of common data issues and how to resolve them']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Data Engineer- 100% Remote,Emvia,US Virgin Islands,1 week ago,Be among the first 25 applicants,"['', 'No Prior Experience In The Energy Industry Required']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Arcadia,"New York, NY",6 days ago,50 applicants,"['', 'Nice-to-haves', 'Eliminating carbon footprints, eliminating carbon copies.', 'Extensive experience in managing data pipelines, schemas, and storage for multiple systems for multiple teams', 'Experience in one or more of the following languages: Python, Java, Ruby, Javascript', 'Join a mashup of energy enthusiasts and creative tech wizards who are taking the fight to climate change. Disrupt and reimagine the energy experience using modern technologies.', 'Manage data pipelines from disparate sources, standardizing and feeding them into our centralized data warehouse', 'Experience with entity resolution at scale', 'A supportive engineering culture that values diversity, empathy, teamwork, trust, and efficiency', 'Benefits', 'Paid Time Off (holidays, vacation, professional development, volunteer, parental leave)', 'Experience in predictive modeling and statistical analysis', 'What You’ll Do', 'Significant experience with and a strong understanding of languages/tools relevant to engineering & data teams’ work', '3+ years combined programming and/or DevOps experience', 'A chance to decarbonize and disrupt the energy sector', 'Undergraduate and/or graduate degree in math, statistics, engineering, computer science, or related technical field', 'Database management experience with PostgreSQL, RDS, or Redshift', 'Work with both the Engineering and Analytics & Data Science teams to optimize data flow and queries for large data sets to improve scalability', ' 3+ years combined programming and/or DevOps experience Significant experience with and a strong understanding of languages/tools relevant to engineering & data teams’ work Experience in one or more of the following languages: Python, Java, Ruby, Javascript Advanced knowledge of algorithms, data structures, and relational algebra Database management experience with PostgreSQL, RDS, or Redshift Data extraction experience with a strong understanding of thread-based and event-based paradigms Extensive experience in managing data pipelines, schemas, and storage for multiple systems for multiple teams Strong communication skills ', 'What will help you succeed:', 'Data extraction experience with a strong understanding of thread-based and event-based paradigms', 'Market-based compensation (salary + equity)', 'Sync data across internal and external systems, such as marketing and sales automation tools, to enable key stakeholders to build best-in-class experiences', ' Manage data pipelines from disparate sources, standardizing and feeding them into our centralized data warehouse Work with both the Engineering and Analytics & Data Science teams to optimize data flow and queries for large data sets to improve scalability Sync data across internal and external systems, such as marketing and sales automation tools, to enable key stakeholders to build best-in-class experiences Support ongoing efforts to establish and enforce best practices on data quality, use, and security across the company ', 'Advanced knowledge of algorithms, data structures, and relational algebra', 'Strong communication skills', 'Free clean energy', 'What We’re Looking For', 'Experience with enterprise database interfaces and messaging APIs', 'Experience with Amazon Web Services (AWS) or other cloud infrastructure platforms', 'Must-haves', 'Healthcare, dental, vision, 401(k) and commuter benefits', ' Undergraduate and/or graduate degree in math, statistics, engineering, computer science, or related technical field Experience in predictive modeling and statistical analysis Experience with enterprise database interfaces and messaging APIs Experience with Amazon Web Services (AWS) or other cloud infrastructure platforms Experience with entity resolution at scale Experience in the energy sector ', 'All-company lunches', 'Support ongoing efforts to establish and enforce best practices on data quality, use, and security across the company', 'Experience in the energy sector', 'a 100% clean energy future.', ' Market-based compensation (salary + equity) Healthcare, dental, vision, 401(k) and commuter benefits Paid Time Off (holidays, vacation, professional development, volunteer, parental leave) A supportive engineering culture that values diversity, empathy, teamwork, trust, and efficiency Professional development opportunities All-company lunches Free clean energy A chance to decarbonize and disrupt the energy sector ', 'Professional development opportunities']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Data Engineer,Arcadia,"New York, NY",6 days ago,50 applicants,[],Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Data Engineer,Quadrant Inc.,"Aberdeen, MD",1 week ago,Be among the first 25 applicants,"['', 'Washington DC (Remote during COVID)', 'Data Engineer', 'Quadrant, Inc. is an equal opportunity and affirmative action employer. Quadrant is committed to administering all employment and personnel actions on the basis of merit and free of discrimination based on race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or status as an individual with a disability. Consistent with this commitment, we are dedicated to the employment and advancement of qualified minorities, women, individuals with disabilities, protected veterans, persons of all ethnic backgrounds and religions according to their abilities.', 'Musts', 'Experience Working With LogStash Is Required.', 'Duties']",Entry level,Contract,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Senior Data Engineer,WHOOP,"Boston, MA",3 days ago,Be among the first 25 applicants,"['', '5+ years relevant experience.Experience with Python and JavaExperience creating data science tools with Spark or AWS Sagemaker or similar technologiesExperience or familiarity with efficient formats like Apache Arrow, Apache Iceberg, Delta Lake or similar technologiesExperience tuning streaming and batch processing applications to increase throughputInterest in communicating experience and expertise to help teammates growExcellent verbal and written communication skillsEnjoy working in a high-growth start up environment Willingness to be both a team player and an owner', 'Advise the Data Science team on feature scope or technical implementations', 'Experience tuning streaming and batch processing applications to increase throughput', 'Advocate for new technological adoptions and or third party software solution', 'Enjoy working in a high-growth start up environment ', 'Willingness to be both a team player and an owner', 'Experience or familiarity with efficient formats like Apache Arrow, Apache Iceberg, Delta Lake or similar technologies', '5+ years relevant experience.', 'WHOOP is an Equal Opportunity Employer and participates in ', 'Responsibilities', 'Qualifications', 'Interest in communicating experience and expertise to help teammates grow', 'Work across teams to inform data model design and storage strategies', 'E-verify ', 'This role is based in the WHOOP office located in Boston, MA. The successful candidate must be prepared to relocate if necessary to work out of the Boston, MA office.', 'Experience with Python and Java', 'Build scalable infrastructure that will enable our data scientists to derive insights and build models efficiently', 'Experience creating data science tools with Spark or AWS Sagemaker or similar technologies', 'Reports to the Data Science Infrastructure technical lead', 'Build scalable infrastructure that will enable our data scientists to derive insights and build models efficientlyWork across teams to inform data model design and storage strategiesAdvocate for new technological adoptions and or third party software solutionAdvise the Data Science team on feature scope or technical implementationsReports to the Data Science Infrastructure technical lead', 'to determine employment eligibility', 'Excellent verbal and written communication skills']",Associate,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
"Data Engineer/Senior Data Engineer, IT Applications",American Airlines,"Fort Worth, TX",2 weeks ago,64 applicants,"['', 'Develop, code, test, and implement data solutions according to business requirements', 'Design job/jobstream flows via scheduling tool, ensuring proper dependencies within enterprise schedules', ""Bachelor's degree in Computer Science, Computer Engineering, Technology, Information Systems (CIS/MIS), Engineering or related technical discipline, or equivalent experience/training3+ years’ of hands on experience in data warehouse development including framework and application solutions leveraging DataStage and Teradata3+ years’ experience with UNIX Shell Scripting and SQLExperience delivering data solutions for, or within, an analytic or business intelligence environmentSource code management in Git or Subversion"", 'Additional Benefits: Other great benefits include our Employee Assistance Program, pet insurance and discounts on hotels, cars, cruises and more', 'Source code management in Git or Subversion', 'Minimum Qualifications- Education & Prior Job Experience', '401(k) Program: Available upon hire and, depending on the workgroup, employer contributions to your 401(k) program are available after one year.', 'Travel Perks: Ready to explore the world? You, your family and your friends can reach 365 destinations on more than 6,800 daily flights across our global network.', '3+ years’ experience with UNIX Shell Scripting and SQL', 'Interpret business data and data access requirements', 'Provide appropriate estimates on development tasks and capacity requirements', 'Demonstrated achievement in developing analytical data layers/applications with large data volume', 'Strong problem-solving ability with a positive ""can-do"" attitude', '5 years of hands on experience in data warehouse development including framework and application solutions leveraging DataStage and Teradata', '5 years of hands on experience in data warehouse development including framework and application solutions leveraging DataStage and TeradataExperience in Big Data development including Python, Spark, Scala, ParquetExperience in Cloud; IBM or Azure', 'Travel Perks: Ready to explore the world? You, your family and your friends can reach 365 destinations on more than 6,800 daily flights across our global network.Health Benefits: On day one, you’ll have access to your health, dental, prescription and vision benefits to help you stay well. And that’s just the start, we also offer virtual doctor visits, flexible spending accounts and more. Wellness Programs: We want you to be the best version of yourself - that’s why our wellness programs provide you with all the right tools, resources and support you need.401(k) Program: Available upon hire and, depending on the workgroup, employer contributions to your 401(k) program are available after one year.Additional Benefits: Other great benefits include our Employee Assistance Program, pet insurance and discounts on hotels, cars, cruises and more', 'Health Benefits: On day one, you’ll have access to your health, dental, prescription and vision benefits to help you stay well. And that’s just the start, we also offer virtual doctor visits, flexible spending accounts and more. ', 'Works in conjunction with Product Owner and Agile teamInteract with business and technologies peersInterpret business data and data access requirementsProvide appropriate estimates on development tasks and capacity requirementsComplete source to target mappingsDevelop, code, test, and implement data solutions according to business requirementsDesign job/jobstream flows via scheduling tool, ensuring proper dependencies within enterprise schedulesProvide assistance and resolution for any production related issuesBe accountable for application performance monitoring and tuningContribute to continuous improvement of On Premise Data warehouse and BigData applications and the Cloud applicationsDevelop POC’s when necessary', 'Feel Free to be yourself at American', ""What You'll Get"", ""Bachelor's degree in Computer Science, Computer Engineering, Technology, Information Systems (CIS/MIS), Engineering or related technical discipline, or equivalent experience/training"", '3+ years’ of hands on experience in data warehouse development including framework and application solutions leveraging DataStage and Teradata', ""All you'll need for success"", 'Intro', 'A passion for technology, continuous improvement, quality and helping others grow', ""Why you'll love this job"", 'Demonstrated achievement in developing analytical data layers/applications with large data volumeStrong problem-solving ability with a positive ""can-do"" attitudeDevOps CI/CD using Jenkins or other competing tools in the marketA passion for technology, continuous improvement, quality and helping others grow', 'Experience in Big Data development including Python, Spark, Scala, Parquet', 'Preferred Qualifications- Education & Prior Job Experience', 'Be accountable for application performance monitoring and tuning', 'Interact with business and technologies peers', 'Skills, Licenses & Certifications', 'Works in conjunction with Product Owner and Agile team', ""What You'll Do"", 'DevOps CI/CD using Jenkins or other competing tools in the market', 'This position is a member of the Information Technology Team, within the RPT Commercial Data Engineering & Business Analytics group supporting Revenue Management Product.The role of the Date Engineer, IT Applications will be to translate business requirements into solutions enabling business value in areas which may include analytics, data pipelines, and complex batch processing of airline commercial data.Success in this role is defined by the ability to leverage strong data application skills to open new capabilities being defined by our business community. You will collaborate with our business partners, fellow developers, and platform architects to achieve these', 'Experience in Cloud; IBM or Azure', 'Wellness Programs: We want you to be the best version of yourself - that’s why our wellness programs provide you with all the right tools, resources and support you need.', 'Contribute to continuous improvement of On Premise Data warehouse and BigData applications and the Cloud applications', 'Experience delivering data solutions for, or within, an analytic or business intelligence environment', 'Develop POC’s when necessary', 'Complete source to target mappings', 'Provide assistance and resolution for any production related issues']",Not Applicable,Full-time,Information Technology,Airlines/Aviation,2021-03-18 14:34:51
Lead Data Engineer,goHUNT,"Las Vegas, NV",3 days ago,Be among the first 25 applicants,"['', 'Job Overview', 'Experience building and optimizing data pipelines, architectures and data sets.', 'Experience with AWS cloud services: EC2, EMR, RDS,', 'Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS Data technologies.', 'Assemble large, complex data sets that meet functional / non-functional business requirements.', 'Experience with relational SQL and NoSQL databases, including MySQL Postgres and Dynamodb.', 'A successful history of manipulating, processing and extracting value from large disconnected datasets.', 'Strong project management and organizational skills.', 'Experience with object-oriented/object function scripting languages: Python and Java.', 'We support conservation through hunting.', 'Work with data and analytics experts to strive for greater functionality in our data systems.', 'Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.', 'Build processes supporting data transformation, data structures, metadata, dependency and workload management.', 'Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.', 'Experience with AWS data pipeline and workflow management tools.', 'Hunters secure more tags and have more success because we put the best technology and gear into their hands. Our tech cuts through hunting’s complexities to uncover opportunities that most folks don’t even know exist, and we provide the expertise and gear needed to get it done in the field. That means more money for conservation and more support for wildlife management, preserving public lands for generations to come.', 'Vision & Mission', 'We are looking for a savvy Lead Data Engineer to join our growing team of experts. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will work closely with our software developers, GIS architects and data analysts on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives. The position is full-time and is based in our Las Vegas office reporting directly to the Chief Technology Officer.\xa0', ""goHUNT is a rapidly growing company that puts the best technology and gear into the hands of hunters and anyone who loves the outdoors. We've taken over the Western big game hunting industry by giving ethical, fair-chase hunters better tools to pursue their passion. We take pride in staying mission-focused while still enjoying ourselves and offering a world-class customer experience. If you’re someone with grit and integrity, we’d love to have you on our team."", 'Experience supporting and working with cross-functional teams in a dynamic environment.', 'Responsibilities for Data Engineer', 'Create and maintain optimal data pipeline architecture.', 'About goHUNT', 'Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.', 'Create data tools for analytics and data science projects for building and optimizing our product into an innovative industry leader.', 'Create and maintain optimal data pipeline architecture.Assemble large, complex data sets that meet functional / non-functional business requirements.Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS Data technologies.Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.Work with stakeholders including the Executive, Product, GIS and Design teams to assist with data-related technical issues and support their data infrastructure needs.Create data tools for analytics and data science projects for building and optimizing our product into an innovative industry leader.Work with data and analytics experts to strive for greater functionality in our data systems.', 'Qualifications\xa0', 'Work with stakeholders including the Executive, Product, GIS and Design teams to assist with data-related technical issues and support their data infrastructure needs.', 'Working knowledge of message queuing, stream processing, and highly scalable structured/relational data stores.', 'Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.Experience building and optimizing data pipelines, architectures and data sets.Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.Build processes supporting data transformation, data structures, metadata, dependency and workload management.A successful history of manipulating, processing and extracting value from large disconnected datasets.Working knowledge of message queuing, stream processing, and highly scalable structured/relational data stores.Strong project management and organizational skills.Experience supporting and working with cross-functional teams in a dynamic environment.We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools:Experience with relational SQL and NoSQL databases, including MySQL Postgres and Dynamodb.Experience with AWS data pipeline and workflow management tools.Experience with AWS cloud services: EC2, EMR, RDS,Experience with stream-processing systems: Storm, Spark-Streaming, etc.Experience with object-oriented/object function scripting languages: Python and Java.Experience with big data tools: Hadoop/Spark, Kafka, etc.', 'Experience with stream-processing systems: Storm, Spark-Streaming, etc.', '\xa0', 'We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools:', 'Experience with big data tools: Hadoop/Spark, Kafka, etc.', 'Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.']",Associate,Full-time,Information Technology,Internet,2021-03-18 14:34:51
"Data Engineer- Buffalo, NY",IBM,"Buffalo, NY",1 week ago,Be among the first 25 applicants,"['', 'Knowledge of ERWin, MDM, and/or ETL ', 'Knowledge and experience with SQL', 'At Our Core, We Are Committed To Believing And Investing In Our Workforce Through', 'http://www.ibm.com/ibm/responsibility/initiatives.html', 'http://www.ibm.com/ibm/responsibility/initiatives.htmlhttp://www.ibm.com/ibm/responsibility/corporateservicecorps', 'CORPORATE CITIZENSHIP', 'Ability to thrive in an ever changing, technology based consulting environmentAbility to translate business solutions into technical requirementsExperience with database management and database structures needed. Strong Excel skills requiredDemonstrated leadership experience and ability to adapt, with willingness to readily take ownership of tasks and problems, which often extend beyond initial scope of responsibilityThorough and analytical, with capability to apply logic to solve problemsAbility to handle multiple tasks concurrently and meet deadlines, while maintaining focus in an environment with conflicting demandsDrive to overcome the most challenging or difficult obstacles and look for ways to improve resultsInitiative to actively seek new knowledge and improve skillsStrong interpersonal skills with ability to collaborate and work effectively with individuals, strengthening relationships to achieve win-win solutionsAbility to communicate complex situations clearly and simply by listening actively and conveying difficult messages in a positive mannerA passion for innovative ideas, coupled with the ability to understand and assimilate different points of viewDemonstrated ability to evaluate clients’ needs and develop solutions to address those needs', 'Demonstrated leadership experience and ability to adapt, with willingness to readily take ownership of tasks and problems, which often extend beyond initial scope of responsibility', 'Thorough and analytical, with capability to apply logic to solve problems', 'About IBM', 'Initiative to actively seek new knowledge and improve skills', 'Skill development:', 'Benefits', ""Creating ERD's"", ' or be willing to relocate to, Buffalo, NY.', 'Demonstrated ability to evaluate clients’ needs and develop solutions to address those needs', 'Qualities The Data Modeler Will Possess', 'CAREER GROWTH ', 'Experience in database and database structures', 'Ability to translate business solutions into technical requirements', 'Experience with database management and database structures needed. Strong Excel skills required', 'Ability to communicate complex situations clearly and simply by listening actively and conveying difficult messages in a positive manner', 'http://www.ibm.com/ibm/responsibility/corporateservicecorps', 'Finding the dream job at IBM:', 'Logical and physical data modeling', 'Drive to overcome the most challenging or difficult obstacles and look for ways to improve results', ""Logical and physical data modelingCreating ERD'sKnowledge and experience with SQLKnowledge of ERWin, MDM, and/or ETL Defining and analyzing data requirements to support the business processesExperience in database and database structures"", 'Diversity of people:', ""Master's Degree"", 'Ability to thrive in an ever changing, technology based consulting environment', 'Ability to handle multiple tasks concurrently and meet deadlines, while maintaining focus in an environment with conflicting demands', 'A passion for innovative ideas, coupled with the ability to understand and assimilate different points of view', 'Strong interpersonal skills with ability to collaborate and work effectively with individuals, strengthening relationships to achieve win-win solutions', 'About Business Unit', 'Preferred Technical And Professional Expertise', 'Defining and analyzing data requirements to support the business processes']",Not Applicable,Full-time,Other,Computer Hardware,2021-03-18 14:34:51
Data Engineer,Airspace,"Carlsbad, CA",2 weeks ago,57 applicants,"['', ' Provide technical guidance for design and implementation of data storage and governance systems', ' Knowledge and use of a source control system, such as Git', ' Experience developing ETL applications that move data to and from various platforms including REST APIs, SQL/Cloud DBs, and Cloud File Storage (such as S3)', ' Knowledge of data warehousing best practices and data quality management', ' Work as part of the data engineering team to define and develop data ingestion, validation, transformation and loading code Maintain and improve existing data pipelines Collaborate with leadership and project leads to ensure the data, reporting, analytics, and automation needs of the business are met Provide technical guidance for design and implementation of data storage and governance systems Work closely with Analytics and Data Science teams to design informative user metrics and models', ' Experience with data pipeline and workflow management tools (Airflow, Luigi) a huge plus', ' Work as part of the data engineering team to define and develop data ingestion, validation, transformation and loading code', ' Advanced knowledge of SQL (CTEs, window functions, querying semi-structured data) and relational databases (Postgres, Snowflake, Redshift, BigQuery)', ' Collaborate with leadership and project leads to ensure the data, reporting, analytics, and automation needs of the business are met', 'Requirements', ' Familiarity with AWS/GCP cloud computing tools', ' Proficient with Python', 'Essential Roles And Responsibilities', ' 2+ years experience building and optimizing data pipelines, warehouses and data sets', ' Maintain and improve existing data pipelines', 'Job Description', ' Work closely with Analytics and Data Science teams to design informative user metrics and models', 'What We Do', ' 2+ years experience building and optimizing data pipelines, warehouses and data sets Proficient with Python Advanced knowledge of SQL (CTEs, window functions, querying semi-structured data) and relational databases (Postgres, Snowflake, Redshift, BigQuery) Experience developing ETL applications that move data to and from various platforms including REST APIs, SQL/Cloud DBs, and Cloud File Storage (such as S3) Familiarity with AWS/GCP cloud computing tools Knowledge and use of a source control system, such as Git Knowledge of data warehousing best practices and data quality management Experience with data pipeline and workflow management tools (Airflow, Luigi) a huge plus']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer - Remote,Ferguson Enterprises,"Newport News, VA",1 month ago,58 applicants,"['Proven experience building and validating data modelsExperience with Python scriptingSelf-motivated, team player, take initiative, quick learner, systematic, adaptable, etc.Strong SQL skills required for building tables, views, and implementing complex logicAbility to learn and use database software (SQL)Outstanding organizational skillsSolid problem solving, numerical fluency, and analytical skillsBA or BS degree preferred', 'Proven experience building and validating data models', ' What do we bring to the ', 'Manage the BI development lifecycle from development to production', ' What are you going to  do? ', ' What are you going to bring to the ', ' Amazing work environment that makes you want to be at work everyday ', 'Perform extensive data validation', 'Maintain and build upon the semantic layer and measure/attribute definitions', ' do? ', ' The Company is an equal opportunity employer as well as a government contractor that shall abide by the requirements of 41 CFR 60-300.5(a), which prohibits discrimination against qualified protected Veterans and the requirements of 41 CFR 60-741.5(A), which prohibits discrimination against qualified individuals on the basis of disability.', 'Design and build data models using SQLMaintain, modify, and build upon the current model within the platformManage data refresh processes and uptimeManage the BI development lifecycle from development to productionTranslate business requirements into dimensional modelsMaintain and build upon the semantic layer and measure/attribute definitionsPerform extensive data validationCommunicate effectively with analytics colleagues and business stakeholders', 'Self-motivated, team player, take initiative, quick learner, systematic, adaptable, etc.', 'Manage data refresh processes and uptime', 'The Data Engineer will have end-to-end responsibility for maintaining our data flows into the data warehouse. This includes ETL, designing & building data models, managing the business intelligence development cycle, data validation, and managing uptime. They will work closely alongside our business analysts to ensure alignment among both the technical and business aspects of business intelligence development.', ' A wide variety of local discounts plus all of Ferguson Inc. employee perks ', 'Outstanding organizational skills', ' Competitive Compensation Packages ', 'Communicate effectively with analytics colleagues and business stakeholders', 'Maintain, modify, and build upon the current model within the platform', 'Strong SQL skills required for building tables, views, and implementing complex logic', ' What are you going to bring to the  table? ', ' We realize that our greatest assets are our best-in-class associates, which is why we’re dedicated to offering limitless opportunities for growth and advancement. We want to help you build a long-lasting career with Ferguson, we can continue to lead the industry and help build our nation’s infrastructure from the ground up. Join our team today. ', ' Team building and company-wide events throughout the year (sometimes too many to count) ', 'Ability to learn and use database software (SQL)', 'Solid problem solving, numerical fluency, and analytical skills', ' Internal Training programs suited to what you are wanting to develop in ', ' Ferguson is currently seeking the right individual to fill an immediate need for a Data Engineer. ', ' What do we bring to the  table? ', ' Great 401(k), PTO and benefits package which includes Medical, Dental, Life and Vision ', 'Design and build data models using SQL', 'Experience with Python scripting', ' table? ', 'Job Description', ' Competitive Compensation Packages  Great 401(k), PTO and benefits package which includes Medical, Dental, Life and Vision  Amazing work environment that makes you want to be at work everyday  Team building and company-wide events throughout the year (sometimes too many to count)  Internal Training programs suited to what you are wanting to develop in  A wide variety of local discounts plus all of Ferguson Inc. employee perks ', 'Translate business requirements into dimensional models', ' What are you going to ', ' What is the opportunity for you? ', 'BA or BS degree preferred']",Entry level,Full-time,Information Technology,Wholesale,2021-03-18 14:34:51
Principal Data Engineer ,Dau International,"Austin, Texas Metropolitan Area",,N/A,"['', 'Optimize data modelling, mining, and production processes ', 'Performance, Optimization and Scalability mindset', 'Your Profile:', 'You are a Senior Engineer who is passionate about data engineering and intrigued by Network Security and Network Threat Detection Software. \xa0', 'Artificial Intelligence/Network (Cloud) Threat Detection Software Product Company processing and managing exponentially increasing volumes of user Data for customer security. \xa0\xa0', 'What our client can offer you:\xa0', '\xa0Some of the Responsibilities you will take on:\xa0', 'Git, Jenkins, JIRA', 'Solid understanding of algorithms, data structures, and parallel computing', 'You hold permanent residency status or US Citizenship', 'Enhance and Improve Extraction, transforming and loading large data sets ', 'Python ', 'Long Term Career Opportunities for technical and managerial career tracks', 'Austin based engineering team with work from home flexibility\xa0', 'Parquet, Orc, Avro, Arrow', 'Responsibilities you will take on:\xa0', 'Design & build a data pipeline infrastructure for real-time processing of Streaming Data at     Massive scale \xa0Enhance and Improve Extraction, transforming and loading large data sets Optimize data modelling, mining, and production processes Facilitate Cloud Migration of existing data applications and large-scale data sets ', 'Who You Are:', 'Facilitate Cloud Migration of existing data applications and large-scale data sets ', 'Understanding of Multithreaded, multi-process, distributed computing concepts', 'PCAP analysis, Zeek/Bro format', '10+ years’ of industry-related experience ', 'Experience with AWS or similar \xa0', 'College Degree in Computer Science, Engineering, Electrical Engineering or related field, ', 'If you are ambitious and driven and want to take your career to the next level, please contact me to discuss this opportunity along with alternative modern Senior Software Engineering roles within the Austin Computer Software Technology market.', 'Equity Participation via Pre-IPO Stock grants', 'Family-Friendly work-life balanced company culture ', 'You seek to evaluate and identify the correct approach or technology to better solve problems. You interact well with product management and seek to help define requirements for new high-value customer features. You collaborate well with Data Science and Security Teams to further simplify cybersecurity.', 'All conversations are confidential, and we will only submit your information after attaining your consent.', 'You are experienced and enjoy running Spark at scale as part of an enterprise product offering, ideally within AWS.', 'You are in Austin, Texas or already planning to relocate here\xa0', 'What our client can offer you:\xa0\xa0', 'Austin based engineering team with work from home flexibility\xa0Equity Participation via Pre-IPO Stock grantsFamily-Friendly work-life balanced company culture Highly Competitive Compensation + Benefits Long Term Career Opportunities for technical and managerial career tracks', 'Spark, Presto, Impala, MapReduce', 'Proficiency with Python or related language ', 'Some of the tools you will be working with and able to influence:', 'You are comfortable and enjoy writing code in Python. \xa0', 'Principal Data Engineer: Apache Spark AWS\xa0', 'Python Spark, Presto, Impala, MapReduceParquet, Orc, Avro, ArrowPCAP analysis, Zeek/Bro formatGit, Jenkins, JIRA', 'College Degree in Computer Science, Engineering, Electrical Engineering or related field, 10+ years’ of industry-related experience Proficiency with Python or related language Experience with AWS or similar \xa0Understanding of Multithreaded, multi-process, distributed computing conceptsSolid understanding of algorithms, data structures, and parallel computingPerformance, Optimization and Scalability mindsetYou are in Austin, Texas or already planning to relocate here\xa0You hold permanent residency status or US Citizenship', '\xa0', 'Highly Competitive Compensation + Benefits ', 'Design & build a data pipeline infrastructure for real-time processing of Streaming Data at     Massive scale \xa0']",Mid-Senior level,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Engineer,Wimmer Solutions,"Sunnyvale, CA",1 week ago,117 applicants,"['', 'Experienced in data mining and visualization of large data sets', 'More About Wimmer Solutions', 'Design and build updates to our data solutions supporting robotics and machine learning development cycle', ' Design and build updates to our data solutions supporting robotics and machine learning development cycle Develop and architect enhanced systems to enable rapid retrieval of imagery and time series data Promote standard methodologies in data modeling, storage, and processing Assess, benchmark and select new technologies to be added to the digital product portfolio. Help enable our users to find their data! Develop best practices for data access and queries. Work with product, ML scientists, roboticists, and software engineers to build a data platform that supports development of Intelligent Machines ', 'Excellent communicator', 'Bachelor’s Degree in Computer Science or related technical subject area', ' Experience working with high dimensional data: images, videos, point clouds, etc. Experience crafting data systems to support machine learning and robotics applications Experienced in data mining and visualization of large data sets Experience developing on Kubernetes based systems ', 'Preferred Skills & Experience', 'Work with product, ML scientists, roboticists, and software engineers to build a data platform that supports development of Intelligent Machines', 'Experience with a diversity of datastores such as Redshift, Dynamo, Athena, Mongo, Postgres', ' Expertise in data modeling for time series, spatial, and image data for analytic and operational use cases Strong Python programmer Experience with a diversity of datastores such as Redshift, Dynamo, Athena, Mongo, Postgres Experience developing ETL in a microservice architecture Data lifecycle management experience Experience with infrastructure as code, such as Terraform Self-motivated, ability to work both independently and in team environments Excellent communicator Bachelor’s Degree in Computer Science or related technical subject area ', 'Assess, benchmark and select new technologies to be added to the digital product portfolio.', 'Develop and architect enhanced systems to enable rapid retrieval of imagery and time series data', 'Self-motivated, ability to work both independently and in team environments', 'Promote standard methodologies in data modeling, storage, and processing', 'Strong Python programmer', 'Data EngineerJob ID: 18920Position Description', 'Experience developing on Kubernetes based systems', 'Role Responsibilities', 'Required Professional Skills & Experience', 'Help enable our users to find their data! Develop best practices for data access and queries.', 'Experience working with high dimensional data: images, videos, point clouds, etc.', 'Experience with infrastructure as code, such as Terraform', 'Experience crafting data systems to support machine learning and robotics applications', 'Expertise in data modeling for time series, spatial, and image data for analytic and operational use cases', 'Data lifecycle management experience', 'Experience developing ETL in a microservice architecture']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,RightStone,"Wausau, WI",,N/A,"['', 'Bachelors or Master’s degree in a technical or business discipline, or equivalent experience.3 to 5+ years of data engineering and software engineering experienceProficiency in software engineering languages and tools, including Python, Java and RESTful services, spanning horizontal and vertical packagesExperience working in an agile environment utilizing Scrum, Kanban or XPExperience building non-cloud native vendor products in AWS, Azure or GCPDemonstrated success in building infrastructure in the public cloud and deploying through CI/CD pipelines', 'Expert knowledge of predictive toolset; reflects as expert resource for tool development', 'Demonstrated success in building infrastructure in the public cloud and deploying through CI/CD pipelines', 'Position: Data Engineer', 'Can sit remotely on EST (Open for US Citizen and Green Card holder candidates on W2)', ' Data Engineer', 'We are looking for a Data Engineer. This Data Engineer is to be in Wausau, WI for a 6-month contract opportunity. If you have 3 to 5+ years of data and software engineering experience, proficient with Python, Java and RESTful services, experience building non-cloud native vendor products in AWS, Azure or GCP, and working in an Agile environment deploying through CI/CD pipelines, APPLY NOW!', 'Experience working in an agile environment utilizing Scrum, Kanban or XP', 'Employment: 6 Month Contract (Possibility to Extend)', 'Forward thinker: Simply fixing the problem isn’t enough; using your proactive mindset and initiative, you’ll continually look for ways to improve performance, quality and efficiency', 'Problem solver: Complex problems often require innovative, creative approaches-and you’ll work to come up with outside-the-box solutions to solve them', 'Qualifications/Skill Requirements', '3 to 5+ years of data engineering and software engineering experience', 'Experience building non-cloud native vendor products in AWS, Azure or GCP', 'This person will be a member of full stack engineer team. The product that is being created, is a technology health portal. This eventual product will be collecting data from databases and then using this data to tell a story. Eventually this will get to the point where it can be used for predictive for health monitoring.Collaborative partner: Working side-by-side with business colleagues and interacting with customers, you’ll address their technical challenges and ensure quality through collaborative design and developmentProblem solver: Complex problems often require innovative, creative approaches-and you’ll work to come up with outside-the-box solutions to solve themCustomer-centric engineer: You understand who were here to serve and the products you engineer will keep the end-user front and centerForward thinker: Simply fixing the problem isn’t enough; using your proactive mindset and initiative, you’ll continually look for ways to improve performance, quality and efficiencyBroad knowledge of predictive analytic techniques and statistical diagnostics of modelsExpert knowledge of predictive toolset; reflects as expert resource for tool developmentHas a business value-driven perspective with regards to understanding of work context and impact - interacts with customers to gather and define requirementsWorks within the team on iterative development that delivers a high-quality productIdentifies and recommends appropriate continuous improvement opportunities', 'Customer-centric engineer: You understand who were here to serve and the products you engineer will keep the end-user front and center', ' Data Engineer.', 'This person will be a member of full stack engineer team. The product that is being created, is a technology health portal. This eventual product will be collecting data from databases and then using this data to tell a story. Eventually this will get to the point where it can be used for predictive for health monitoring.', 'Identifies and recommends appropriate continuous improvement opportunities', ', APPLY NOW!', 'Pay Rate: $55-$60/hr. W2', 'Proficiency in software engineering languages and tools, including Python, Java and RESTful services, spanning horizontal and vertical packages', '\xa0', 'Job Description', 'Has a business value-driven perspective with regards to understanding of work context and impact - interacts with customers to gather and define requirements', 'Works within the team on iterative development that delivers a high-quality product', 'Collaborative partner: Working side-by-side with business colleagues and interacting with customers, you’ll address their technical challenges and ensure quality through collaborative design and development', 'Location: Boston, MA', 'Bachelors or Master’s degree in a technical or business discipline, or equivalent experience.', 'Broad knowledge of predictive analytic techniques and statistical diagnostics of models']",Mid-Senior level,Contract,Information Technology,Insurance,2021-03-18 14:34:51
Data Engineer,Plein Air Agency,"Dallas, TX",3 weeks ago,Over 200 applicants,"['', 'Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.', 'A successful history of manipulating, processing, and extracting value from large disconnected datasets.', 'The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.', 'Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.', 'Build processes supporting data transformation, data structures, metadata, dependency, and workload management.', 'Assemble large, complex data sets that meet functional / non-functional business requirements.', 'Create data tools for analytics team members that assist them in building and optimizing our products', 'We are a distributed company, so you will have lots of video meetings, but you can work from pretty much anywhere that has good internet. This job could be full-team or contract depending on the requirements of the candidate.', 'Experience building and optimizing ‘big data’ data pipelines, architectures, and data sets.', 'Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.', 'Strong analytic skills related to working with unstructured datasets.', 'Create and maintain optimal data pipeline architecture,Assemble large, complex data sets that meet functional / non-functional business requirements.Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.Create data tools for analytics team members that assist them in building and optimizing our products', 'Strong project management and organizational skills.', 'Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.', 'Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.', 'Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.Experience building and optimizing ‘big data’ data pipelines, architectures, and data sets.Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.Strong analytic skills related to working with unstructured datasets.Build processes supporting data transformation, data structures, metadata, dependency, and workload management.A successful history of manipulating, processing, and extracting value from large disconnected datasets.Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.Strong project management and organizational skills.We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools:Experience with big data tools: Hadoop, Spark, Kafka, etc.Experience with CDP selection and managementExperience with relational SQL and NoSQL databases, including Postgres and Cassandra.Experience with AWS cloud services: EC2, EMR, RDS, RedshiftExperience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.', 'Experience with AWS cloud services: EC2, EMR, RDS, Redshift', 'Qualifications for Data Engineer', 'Experience with CDP selection and management', 'We are looking for a Data Engineer to join our team of analytics experts. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross-functional teams. We work primarily in the restaurant industry, so experience in that space is a plus.', 'Responsibilities for Data Engineer', 'Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.', 'Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.', 'Experience with big data tools: Hadoop, Spark, Kafka, etc.', 'We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools:', 'Create and maintain optimal data pipeline architecture,', 'Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.', 'Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Data Engineer,"InvestEdge, Inc.",United States,6 days ago,Be among the first 25 applicants,"['', 'Debug and troubleshoot logical issues in database code.', 'Location: Remote/Work from Home', 'T-SQL (DDL, Stored Process, Views, CTEs, etc.VCS (Git, SVN, etc.)', 'Work with the senior data architect to implement data routines in a financial services environment.', 'Work with the senior data architect to implement data routines in a financial services environment.Create new queries, views, and stored procedures for a large existing relational data set.Debug and troubleshoot logical issues in database code.Debug and troubleshoot performance issues in database code.Serve as a subject matter expert on a large in-house enterprise database model.Understand the business domain of the application.Work in an Agile environment on a cross-functional team.', 'VS Code, SSMS, and other IDEs.', 'Position Summary', '\xa0\xa0\xa0Additional Skills that are highly favorable for this position ', 'Work in an Agile environment on a cross-functional team.', 'Understand the business domain of the application.', 'Responsibilities:', 'Serve as a subject matter expert on a large in-house enterprise database model.', 'Company Profile:', 'Qualifications:', 'T-SQL (DDL, Stored Process, Views, CTEs, etc.', 'Location:', 'Debug and troubleshoot performance issues in database code.', 'Qualifications: ', 'VCS (Git, SVN, etc.)', 'InvestEdge is seeking a database specialist with expert knowledge in relational data modeling, querying, and data analysis. The candidate should have expert knowledge of working with MSSQL and Postgres databases, and can write complex queries, stored procedures, and views. This person has likely worked in a senior data developer or data architect role, and also understands database administration concepts such as indexing strategies, backup and fault-tolerance strategies, and how to organize and secure data at rest. An ideal candidate would also have a background in financial services and understand how financial markets work. A CPA/CFA with the ability to write advanced SQL should be a shoe-in.', 'Additional Skills that are highly favorable for this position ', 'CI/CD experience with integrating database changes into deployment models.', 'Overview', 'Our integrated solutions provide a breadth of tools that simplify complicated wealth management processes and reduce overall operational risk. Using the integrated solutions automates key front office functions like easy-to-use portfolio management, trading/rebalancing, performance measurement, reporting, compliance/fiduciary monitoring, and data aggregation tools.', ""\xa0The candidate will work with InvestEdge's senior data architect to implement new solutions as well as improve existing ones. They should be comfortable working with large data sets and large database footprints and should understand concepts such as performance tuning, SQL Injection, and data security best practices."", 'Candidates with a CPA/CFA or other financial services background are considered the ideal candidate.\xa0Candidates who understand web technologies and can program in other languages in addition to SQL will be preferred. JavaScript/ES6/NodeJS preferred.VS Code, SSMS, and other IDEs.CI/CD experience with integrating database changes into deployment models.', '\xa0', 'Candidates who understand web technologies and can program in other languages in addition to SQL will be preferred. JavaScript/ES6/NodeJS preferred.', 'InvestEdge is a leading provider of innovative advisor solutions to financial institutions.', 'Create new queries, views, and stored procedures for a large existing relational data set.', 'Candidates with a CPA/CFA or other financial services background are considered the ideal candidate.\xa0', 'To apply send resume and cover letter to careers@investedge.com with the job title in the subject line.\xa0']",Entry level,Full-time,Information Technology,Financial Services,2021-03-18 14:34:51
Data Engineer,Alterra Mountain Company,"Denver, CO",3 weeks ago,28 applicants,"['', '1+ years hands-on experience with Azure ', '3+ years developing in a formal SDLC environment, using GitHub, Jira, Confluence or similarly tools ', 'SDLC concepts including application lifecycle management, release management, and optionally continuous delivery. ', 'Data cataloging and metadata management. ', '3+ years working with SQL in a data warehouse environment ', 'Collaborative and mentoring work style. ', 'This job description is not an express or implied contract, guarantee, promise, or covenant of employment for any set term or duration, or for termination only for cause. ', 'To perform this job successfully, an individual must be able to perform each essential duty satisfactorily.  Reasonable accommodations may be made to enable individuals with disabilities to perform the essential duties.', 'Commercial ETL tools such as SSIS, Talend, Informatica and/or the use of 3GL languages such as Python to implement ETL services. ', 'Experience with metadata applications and solutions ', 'Alterra Mtn Co Shared Services Inc. and its affiliates are equal opportunity employers and maintain drug-free workplaces. All employees and candidates are reminded that Alterra Mtn Co Shared Services Inc. and its affiliates adhere to all applicable labor and employment laws, and State, County, and City-specific labor and employment regulations, where applicable.', 'Indoor/Outdoor: While performing the duties of this job, the employee may be exposed to harsh and varying outside weather conditions. ', 'Enterprise level data modeling at the logical and physical levels for 3NF, star schema, slowly changing data. ', ' ESSENTIAL DUTIES', 'Various data connectivity techniques including FTP and APIs, ', 'SQL databases, DDL, DML, DQL, DCL, and TCL, triggers, stored procedures/functions, indexes, sequences, table schema types, transactions, and replication. ', 'The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job.  Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. ', 'This position is located in Colorado, and the work is primarily in Denver, CO and, as such, employment in this position is subject to the labor and employment laws of the state of Colorado. ', 'Ensure data pipelines provide appropriate access security, encryption (at rest and in motion) and data masking/stripping based on content and corporate security and compliance guidelines. ', 'Provide input to and execute development of logical and physical enterprise data models; enterprise master data and reference data models; metadata models; and data catalogs. Support the governance and stewarding of master and reference data. ', 'To perform this job successfully, an individual must be able to perform each essential duty satisfactorily.  Reasonable accommodations may be made to enable individuals with disabilities to perform the essential duties. ', 'Provide input to and execute development of design patterns and best practices for data integration and data analysis across the enterprise. ', 'Master data management using mapping tables and/or commercial MDM tools. ', 'General Responsibilities', 'Data Architecture, Data Management Services, Data Governance, Data Quality processes and Data Lifecycle. ', 'Organizing data at scale including data lakes, data marts, and data warehouses. ', ' ', 'Passion for statistics & analytics, process engineering, and information management. ', 'Manage the packaging of code assets, models, configurations, schemas and migration instructions to support updates to development, test and production environments. ', 'Database schema migration techniques. ', 'Additional experiences in the following areas are beneficial: ', 'Employment with Alterra Mtn Co Shared Services Inc. or any of its affiliates is “at will” meaning either party may terminate the employment relationship at any time with or without cause and with or without notice.', 'Serverless and microservice architectures and techniques. ', 'Technical development, testing and deployment of T-SQL ETL pipelines and target data warehouse schemas for Alterra’s Unified Data Platform. ', 'This job description is not an express or implied contract, guarantee, promise, or covenant of employment for any set term or duration, or for termination only for cause.', 'Cloud-based data platforms such as Azure and AWS; cloud security (accounts, users, groups, roles); logging and monitoring; lambdas and functions; data versioning and life cycle management; infrastructure-as-code. ', 'KNOWLEDGE, SKILL AND ABILITY REQUIREMENTS', 'Experience with Informatica & SSIS is a plus ', '3+ years building information architectures ', 'EDUCATION & EXPERIENCE REQUIREMENTS', 'General Responsibilities ', 'Ensure that ETL jobs are scheduled, monitored and generate detailed logs to support ongoing diagnostics, exception processing, and audit trails for compliance. ', 'POSITION SUMMARY ', 'Code and infrastructure-as-code testing techniques including unit, integration, system, performance/stress, and acceptance tests. ', 'Undergraduate Major preference: Business, IS/IT, Computer Science, related field and/or experience Commercial ETL tools such as SSIS, Talend, Informatica and/or the use of 3GL languages such as Python to implement ETL services. Design and construction of information architectures that enable well-integrated transactional, collaborative and analytical systems. Schema-on-read query paradigm, columnar file formats such as Parquet/AVRO, compression and partitioning techniques. SDLC concepts including application lifecycle management, release management, and optionally continuous delivery. Data Architecture, Data Management Services, Data Governance, Data Quality processes and Data Lifecycle. SQL databases, DDL, DML, DQL, DCL, and TCL, triggers, stored procedures/functions, indexes, sequences, table schema types, transactions, and replication. Enterprise level data modeling at the logical and physical levels for 3NF, star schema, slowly changing data. Version management via a version control system/repository. Data cataloging and metadata management. Master data management using mapping tables and/or commercial MDM tools. ETL job scheduling tools/techniques, job control, exception handling, logging, and monitoring. ETL workflow design including change data capture, transformations, mapping, and data quality screens. Various data connectivity techniques including FTP and APIs, Cloud-based data platforms such as Azure and AWS; cloud security (accounts, users, groups, roles); logging and monitoring; lambdas and functions; data versioning and life cycle management; infrastructure-as-code. ', '3+ years building data platforms (architecture, storage, management, monitoring) ', 'Monitor and provide input into the enterprise IT architecture including cloud infrastructure and connectivity; database architecture and connectivity; external data connectivity (S/FTP, APIs); security, backup and DR as these subject areas relate to the enterprise information architecture. ', 'Contribute to the technical standards and data dictionaries ', '3+ years data profiling & data mining ', 'Conceptual understanding of advanced analytic and machine learning data processing requirements. ', 'Employment with Alterra Mtn Co Shared Services Inc. or any of its affiliates is “at will” meaning either party may terminate the employment relationship at any time with or without cause and with or without notice. ', 'Contribute to the design, implementation, maintenance, enhancement, monitoring and governance of enterprise data repositories ', 'PowerBI, MSFT SSRS and other reporting & visualization tools Code and infrastructure-as-code testing techniques including unit, integration, system, performance/stress, and acceptance tests. Database schema migration techniques. Conceptual understanding of advanced analytic and machine learning data processing requirements. Serverless and microservice architectures and techniques. Organizing data at scale including data lakes, data marts, and data warehouses. ', 'Ensure proper governance of enterprise data assets including data access at the subject and row level, enforcement of data privacy (PII), protection of financial data (PCI), and country specific treatment and regional storage of data. ', 'Support data integration and business intelligence teams. ', 'PHYSICAL REQUIREMENTS ', 'Loves being hands-on in a fast-paced, entrepreneurial environment. ', 'Schema-on-read query paradigm, columnar file formats such as Parquet/AVRO, compression and partitioning techniques. ', 'Technical development, testing and deployment of T-SQL ETL pipelines and target data warehouse schemas for Alterra’s Unified Data Platform. Contribute to the design, implementation, maintenance, enhancement, monitoring and governance of enterprise data repositories Develop data processing code with a focus on consistency, reliability, and accuracy Profile inbound data and work with subject matter experts to ensure data is conformed to enterprise standards Contribute to the technical standards and data dictionaries Ensure data ingestion processes catalog and tag arriving data and provide data life cycle and version management across landing, near-term archive, long term cold storage, and data destruction events based on corporate security, compliance, and data retention policies. Ensure data pipelines provide appropriate access security, encryption (at rest and in motion) and data masking/stripping based on content and corporate security and compliance guidelines. Ensure proper governance of enterprise data assets including data access at the subject and row level, enforcement of data privacy (PII), protection of financial data (PCI), and country specific treatment and regional storage of data. Collaborate with teams that manage operational data masters and execute the design and development of data mastering processes. Provide input to and execute development of logical and physical enterprise data models; enterprise master data and reference data models; metadata models; and data catalogs. Support the governance and stewarding of master and reference data. Support ongoing development and code reviews of data acquisition, data movement, data cleansing, data transformation, data mapping, data quality screens, ETL jobs and schedules, and other ETL and data integration activities. Ensure that ETL jobs are scheduled, monitored and generate detailed logs to support ongoing diagnostics, exception processing, and audit trails for compliance. Manage the packaging of code assets, models, configurations, schemas and migration instructions to support updates to development, test and production environments. Support data integration and business intelligence teams. Provide input to and execute development and enforcement of naming conventions for enterprise data assets including data models; database, schema, table, view, index, trigger, stored proc/function names; object level storage container names and paths; file names; and ETL scripts. Monitor and provide input into the enterprise IT architecture including cloud infrastructure and connectivity; database architecture and connectivity; external data connectivity (S/FTP, APIs); security, backup and DR as these subject areas relate to the enterprise information architecture. Provide input to and execute development of design patterns and best practices for data integration and data analysis across the enterprise. ', 'Education & Experience ', 'ETL job scheduling tools/techniques, job control, exception handling, logging, and monitoring. ', 'This job description is not an exhaustive list of all functions and responsibilities that an employee may be required to perform in this position. Alterra Mtn Co Shared Services Inc. and its affiliates reserve the right to modify, increase, decrease, suspend, and or eliminate any of the essential duties and/or the position in its entirety. ', 'PHYSICAL REQUIREMENTS', 'Design and construction of information architectures that enable well-integrated transactional, collaborative and analytical systems. ', 'POSITION SUMMARY', 'This job description is not an exhaustive list of all functions and responsibilities that an employee may be required to perform in this position. Alterra Mtn Co Shared Services Inc. and its affiliates reserve the right to modify, increase, decrease, suspend, and or eliminate any of the essential duties and/or the position in its entirety.', 'Undergraduate Major preference: Business, IS/IT, Computer Science, related field and/or experience ', 'Develop data processing code with a focus on consistency, reliability, and accuracy ', 'Provide input to and execute development and enforcement of naming conventions for enterprise data assets including data models; database, schema, table, view, index, trigger, stored proc/function names; object level storage container names and paths; file names; and ETL scripts. ', 'WORKING CONDITIONS', 'Support ongoing development and code reviews of data acquisition, data movement, data cleansing, data transformation, data mapping, data quality screens, ETL jobs and schedules, and other ETL and data integration activities. ', 'Mechanical tendencies and a curiosity to know how things work and how to make them better. ', 'Version management via a version control system/repository. ', 'PowerBI, MSFT SSRS and other reporting & visualization tools ', 'WORKING CONDITIONS ', '5+ years working with SQL, preferably MSSQL ', 'Collaborate with teams that manage operational data masters and execute the design and development of data mastering processes. ', 'This position is located in Colorado, and the work is primarily in Denver, CO and, as such, employment in this position is subject to the labor and employment laws of the state of Colorado.', 'Strong project management and organizational skills. ', 'Profile inbound data and work with subject matter experts to ensure data is conformed to enterprise standards ', 'While performing the duties of this job, the employee is regularly required to walk, talk, see, hear, and operate a computer and other office productivity machinery. ', 'Business Process Management process and application experience ', 'Strong analytic skills and hands-on attention to detail. ', 'Hazardous Materials/Noise: The noise level in the work place is usually moderate. ', 'ETL workflow design including change data capture, transformations, mapping, and data quality screens. ', 'Alterra is looking for a Strong SQL Server database developer with advanced query and stored procedure experience for ETL/ELT use cases to join our Unified Data Platform team. Bonus for experience with ETL tools and recent version of SQL Server 2017 or higher or Azure SQL Database.  You will be primarily responsible for profiling source data, building new T-SQL transformations, defining and building target data schemas in support of strategic data projects. ', '5+ years working with SQL, preferably MSSQL 3+ years working with SQL in a data warehouse environment 3+ years building data platforms (architecture, storage, management, monitoring) 3+ years developing in a formal SDLC environment, using GitHub, Jira, Confluence or similarly tools 3+ years building information architectures 3+ years data profiling & data mining 1+ years hands-on experience with Azure Business Process Management process and application experience Experience with Informatica & SSIS is a plus Experience with metadata applications and solutions Experience with master data management Strong project management and organizational skills. Collaborative and mentoring work style. Strong analytic skills and hands-on attention to detail. Mechanical tendencies and a curiosity to know how things work and how to make them better. Passion for statistics & analytics, process engineering, and information management. Loves being hands-on in a fast-paced, entrepreneurial environment. ', 'Education & Experience', 'Experience with master data management ', 'Ensure data ingestion processes catalog and tag arriving data and provide data life cycle and version management across landing, near-term archive, long term cold storage, and data destruction events based on corporate security, compliance, and data retention policies. ']",Mid-Senior level,Full-time,Information Technology,Hospitality,2021-03-18 14:34:51
"Data Engineer, CPG Analytics",Square,"Chicago, IL",3 days ago,134 applicants,"['', '5+ years of data engineer experience in a successful data engineering or business intelligence team with expert knowledge of data warehouse architecture and experience with data modeling design, ETL pipeline implementation and building reports with Looker or similar BI visualization tools.', 'Company Description', '5+ years of data engineer experience in a successful data engineering or business intelligence team with expert knowledge of data warehouse architecture and experience with data modeling design, ETL pipeline implementation and building reports with Looker or similar BI visualization tools.Hands-on experience in cloud-based computing services and data warehouses like Snowflake, Redshift, Azure, or similar technologies.Strong communication, business intuition, and empathy partnered with the ability to understand complex business systemsOpenness to learning new languages', 'Develop the CPG engagement data warehouse and related data pipelines to empower data access and self-service', 'Job Description', 'You Have', 'Partner with business stakeholders and downstream data consumers to understand data and translate business requirements into scalable data pipelines.', 'Hands-on experience in cloud-based computing services and data warehouses like Snowflake, Redshift, Azure, or similar technologies.', 'You Will', 'Strong communication, business intuition, and empathy partnered with the ability to understand complex business systems', 'Develop the CPG engagement data warehouse and related data pipelines to empower data access and self-servicePartner with business stakeholders and downstream data consumers to understand data and translate business requirements into scalable data pipelines.Perform analysis, insight requests, and data extractions to resolve business issues', 'Perform analysis, insight requests, and data extractions to resolve business issues', 'Openness to learning new languages']",Associate,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Engineer,"ITCO Solutions, Inc.","Carlsbad, CA",3 weeks ago,54 applicants,"['', 'Required Skills', 'Computer science degree is a major plus', ' Jasmine, Mocha, Chai, ', 'Location: ', 'Reduce complexity', '2+ years of professional development experience and at least one year of creating object-oriented JavaScript from scratch', 'Only react experience from code camps', 'Other Testing Tools', '3+ years of overall experience', 'Infrastructure, Virtualization, Cloud, and Managed services', 'Consulting Services ', 'Focus on React, and Redux', 'Jasmine, Mocha, Chai,', 'No testing experience', 'Simplify the management and maintenance of software purchases', '4 Year Computer Science Degree', 'Not Ideal', 'Hard-dollar costs savings by as much as 30% for enterprise software licensing', ' Hard-dollar costs savings by as much as 30% for enterprise software licensing Simplify the management and maintenance of software purchases Provide an assessment of your existing licensing', 'Ideal Candidate', 'SmartSoft ', 'Want work horses on the team', 'ITCO Overview', 'Currently working on a mob X but anyone can pick it up', 'A thorough understanding of full stack web development using a modern JavaScript framework, APIs, and microservices (backend experience preferred, Ruby on Rails a big plus).', 'Provide an assessment of your existing licensing', 'Workforce Solutions ', 'Developers that test their own code', '2+ years of professional React experience building state driven applications (Redux or MobX preferred)', 'Individuals who are young and hungry to learn, take direction and contribute', 'Professional experience working for one person', 'Turn-Key project delivery', 'Love candidates who look at languages as tools, and development as the end goal', '2 years of professional experience in a team environment (no 1-2 person teams)', 'Job Description', 'A development style that puts a high value on testing, ideally using React Testing Library and/or Cypress', 'At least one year of professional experience with react and read', 'Professional experience working in a team environment', ' 2+ years of professional React experience building state driven applications (Redux or MobX preferred) A development style that puts a high value on testing, ideally using React Testing Library and/or Cypress A thorough understanding of full stack web development using a modern JavaScript framework, APIs, and microservices (backend experience preferred, Ruby on Rails a big plus). 4 Year Computer Science Degree 2 years of professional experience in a team environment (no 1-2 person teams) Focus on React, and Redux Developers that test their own code ', ' Professional experience working for one person Only react experience from code camps No testing experience ', ' At least one year of professional experience with react and read 3+ years of overall experience Currently working on a mob X but anyone can pick it up Want work horses on the team Individuals who are young and hungry to learn, take direction and contribute 2+ years of professional development experience and at least one year of creating object-oriented JavaScript from scratch Professional experience working in a team environment Love candidates who look at languages as tools, and development as the end goal Computer science degree is a major plus ', ' Turn-Key project delivery Reduce complexity Infrastructure, Virtualization, Cloud, and Managed services ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Calibrate,"New York, NY",3 weeks ago,56 applicants,"['', 'As a member of our fast growing product and engineering team, you have the opportunity to work on a wide range of products. ', ' A passionate, people-minded and mission-driven group of product, technical and medical professionals from companies like One Medical, Oscar Health, Capsule and more. Solution-oriented and motivated by complex challenges that have a deep impact. ', 'Senior Data Engineers ', '5+ years of Data engineering experience shipping production level code. Should be able to hit the ground running and be comfortable deploying a change in the first week. ', 'Experience with data transformation system, like dbt', 'Robust internal framework that connects to external healthcare and non-healthcare data sources.', 'Our teams are organized based on the Spotify squad model. ', 'Calibrate ', 'recruits, employs, compensates, and promotes regardless of race, religion, color, national origin, gender identity, disability, age, veteran status, and other protected status as required by applicable law and as a matter of our company values.', 'AIl telehealth communication platform (chat, video, phone) between patients and providers.', 'Consumer facing application (mobile and web) to engage and change behaviors to improve weight health.', 'Experience with relational databases, like PostgreSQL', 'About The Role', 'Products you will build and work on:', 'Responsibilities:', 'You will be working with our member facing, coaches and operation, medical, growth team ', ""We're in this together"", ' As a member of our fast growing product and engineering team, you have the opportunity to work on a wide range of products.  Our teams are organized based on the Spotify squad model.  You will be working with our member facing, coaches and operation, medical, growth team  ', 'Qualifications:', 'Real results matter', 'Calibrate is committed to bringing together people from different backgrounds and perspectives to deliver real results for our members. ', 'About Our Team', 'Who We Are', 'Doctor and coaching internal platform that helps our team efficiently deliver the right care to the right members at the right time.', 'A passionate, people-minded and mission-driven group of product, technical and medical professionals from companies like One Medical, Oscar Health, Capsule and more.', 'Solution-oriented and motivated by complex challenges that have a deep impact.', 'OUR MISSION', 'Growth product and infrastructure to drive sales from initial ad campaigns through conversion.', ' 5+ years of Data engineering experience shipping production level code. Should be able to hit the ground running and be comfortable deploying a change in the first week.  Experience with relational databases, like PostgreSQL Experience with data warehouses, like Big Query, Redshift Experience with data transformation system, like dbt ', 'Experience with data warehouses, like Big Query, Redshift', ""You're in control"", 'OUR VALUES', ' Consumer facing application (mobile and web) to engage and change behaviors to improve weight health. AIl telehealth communication platform (chat, video, phone) between patients and providers. Doctor and coaching internal platform that helps our team efficiently deliver the right care to the right members at the right time. Growth product and infrastructure to drive sales from initial ad campaigns through conversion. Robust internal framework that connects to external healthcare and non-healthcare data sources. ', 'Small wins create big wins']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Data Engineer,Adstra,"Princeton, NJ",2 weeks ago,45 applicants,"['', 'Strong communication, analytical and collaborative problem-solving skills.', 'Adstra', 'EOE/M/F/D/V', 'Implement custom processes for data cleansing and transformation.', 'Equal Employment Opportunity', 'Experience with Big Data technologies such as Spark and HadoopExperience with Cloud Platforms such as AWS or Google CloudPrevious AdTech or MarTech industry experience', 'Bachelor’s in CS, Engineering, Mathematics, Economics, Statistics, or related field.', 'In Addition, Candidates Having Below Experience Is a Plus', 'We want to be your Employer of Choice! ', 'You are a self-starter, quick learner, problem-solver and someone who relishes challenges.', 'Responsibilities', 'You are an analytical, result driven individual with high attention to detail.', '!', 'You are an analytical, result driven individual with high attention to detail.You are a self-starter, quick learner, problem-solver and someone who relishes challenges.Accomplish targeted performance objectives with a positive and can-do attitude.3+ years using SQL or other query and scripting languages to aggregate, gather, and manipulate data.2+ years using scripting languages such as Python or Bash.Deep understanding of relational databases, ETL tools, data conversion and data cleansing methodologiesExperience with (or at least exposure to) cloud-based databases like Redshift, Snowflake, or BigQuery.Your verbal and written communication skills are excellent.You have a love for all things data and data-driven solutions.Pride in your work, and high standards for yourself and the company you work for.Bachelor’s in CS, Engineering, Mathematics, Economics, Statistics, or related field.Strong communication, analytical and collaborative problem-solving skills.Proven ability to learn quickly, work independently, and adapt to change in a fast-paced environment.', 'Research new techniques and best practices within the industry.', 'Interpret data, analyze results using statistical techniques and provide ongoing reports.', 'Experience with (or at least exposure to) cloud-based databases like Redshift, Snowflake, or BigQuery.', 'Your verbal and written communication skills are excellent.', 'Qualifications', 'Pride in your work, and high standards for yourself and the company you work for.', 'is the industry’s first Data Bureau', '3+ years using SQL or other query and scripting languages to aggregate, gather, and manipulate data.', 'Previous AdTech or MarTech industry experience', 'Our “ideal candidate”', 'Accomplish targeted performance objectives with a positive and can-do attitude.', 'Adstra Careers', 'Proven ability to learn quickly, work independently, and adapt to change in a fast-paced environment.', 'Data Engineer ', 'Perform data quality control reviews and data assessments to identify trends and anomalies.', 'Interpret data, analyze results using statistical techniques and provide ongoing reports.Identify, analyze, and interpret trends or patterns in complex data sets.Implement custom processes for data cleansing and transformation.Utilize multiple tools and programs to analyze and convert source data to a standardized format.Build and maintain documentation for data conversion processes while seeking out process improvements.Perform data quality control reviews and data assessments to identify trends and anomalies.Research new techniques and best practices within the industry.Work with product and engineering to define objectives and develop analytic tools and products.', 'Experience with Cloud Platforms such as AWS or Google Cloud', 'At Adstra we know great things happen when people come together with one shared goal. The future of data is happening at Adstra. Come join our growing team.', 'You have a love for all things data and data-driven solutions.', 'Deep understanding of relational databases, ETL tools, data conversion and data cleansing methodologies', 'About Adstra', 'Overview', 'Utilize multiple tools and programs to analyze and convert source data to a standardized format.', '2+ years using scripting languages such as Python or Bash.', 'You will be responsible for ', 'Identify, analyze, and interpret trends or patterns in complex data sets.', 'Build and maintain documentation for data conversion processes while seeking out process improvements.', 'Work with product and engineering to define objectives and develop analytic tools and products.', 'Experience with Big Data technologies such as Spark and Hadoop']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Data Engineer - El Segundo,MotorTrend Group,"El Segundo, CA",2 days ago,Be among the first 25 applicants,"['', ' Build and maintain dimensional data warehouses in support of business intelligence tools', 'Knowledge, Skills, & Abilities', ' Tableau or any other data visualization tool', ' Collaborate with product teams and data analysts to design and build data-forward solutions', ' Strong background in scripting language using Python, Bash, Perl, PHP or any other language to solving data problems', ' Proficiency with the AWS cloud services : EC2, EMR, RDS, S3, Redshift (spectrum)', ' Experience with Stream Processing systems: Storm ,Spark-Streaming etc', ' Data integration tools', ' Develop load and transformation processes in support of the requirements, validate that they meet business and technical specifications, manage ongoing maintenance of the system and data, and make recommendations for process improvements to optimize data movement from source to target', 'Responsibilities', ' Investigate and understand different data sources and ability to connect to a wide variety of 3rd party APIs', ' Design, enhance and implement ETL/data ingestion platform on the cloud', ' Excellent problem solving skills', ' Provide production and operational support to existing ETL jobs. Monitor and manage production ETL jobs to verify execution and measure performance to assure ongoing data quality and optimization of the system to manage scalability and performance and identify improvement opportunities for key ETL processes.', ' Experience with Big Data tools; Hadoop, Spark, Kafka, Hive etc', ' Experience with BI tools like Tableau or any other open source BI tools etc.', ' Knowledge of the Python data ecosystem using pandas and numpy  Data integration tools  Proficiency in SQL, data modeling, and data warehousing  Excellent problem solving skills  Exposure to cloud platforms (preferably AWS)', ' Build and deploy streaming and batch data pipelines capable of processing and storing petabytes of data quickly and reliably', ' Strong troubleshooting and problem-solving skills in large data environment', ' Exposure to cloud platforms (preferably AWS)', ' Capable of investigating, familiarizing and mastering new data sets quickly', ' Integrate with a variety of data metric providers ranging from advertising, web analytics, and consumer devices', ' Proficiency with data exchange types and protocols (json, xml, soap, rest)', 'Work Environment', ' SQL, MySQL or other relational databases', ' SiteCatalyst (Omniture)/Google Analytics or any other web analytics tools experience (Nice to have)', ' Drive and maintain a culture of quality, innovation and experimentation', ' Develop data catalogs and data validations to ensure clarity and correctness of key business metrics', ' Knowledge of the Python data ecosystem using pandas and numpy', ' Microsoft Office Suite (Outlook, Word, Excel, PowerPoint)  SQL, MySQL or other relational databases  Linux, Python, AWS Stack (EC2,EMR S3, Redshift)  Tableau or any other data visualization tool  SiteCatalyst (Omniture)/Google Analytics or any other web analytics tools experience (Nice to have)', ' Microsoft Office Suite (Outlook, Word, Excel, PowerPoint)', ' Bachelor’s degree – Computer Science or equivalent', 'Supervisory Responsibility', ' Work is performed in an office environment that is well lit and ventilated.', ' Linux, Python, AWS Stack (EC2,EMR S3, Redshift)', 'Job Summary & Responsibilities', ' Bachelor’s degree – Computer Science or equivalent  Strong background in scripting language using Python, Bash, Perl, PHP or any other language to solving data problems  Experience with relational SQL and NoSQL databases, including Postgres, ,Neo4j and MongoDb  Experience with Big Data tools; Hadoop, Spark, Kafka, Hive etc  Proficiency with the AWS cloud services : EC2, EMR, RDS, S3, Redshift (spectrum)  Proficiency with data exchange types and protocols (json, xml, soap, rest)  Experience with Stream Processing systems: Storm ,Spark-Streaming etc  Experience with BI tools like Tableau or any other open source BI tools etc.', ' Deliver strong Python and SQL development and maintenance techniques surrounding data movement to include technologies', ' This position will not include supervising one or more employees where applicable.', ' Experience with relational SQL and NoSQL databases, including Postgres, ,Neo4j and MongoDb', ' Collaborate with product teams and data analysts to design and build data-forward solutions  Build and deploy streaming and batch data pipelines capable of processing and storing petabytes of data quickly and reliably  Integrate with a variety of data metric providers ranging from advertising, web analytics, and consumer devices  Build and maintain dimensional data warehouses in support of business intelligence tools  Develop data catalogs and data validations to ensure clarity and correctness of key business metrics  Drive and maintain a culture of quality, innovation and experimentation  Deliver strong Python and SQL development and maintenance techniques surrounding data movement to include technologies  Investigate and understand different data sources and ability to connect to a wide variety of 3rd party APIs  Design, enhance and implement ETL/data ingestion platform on the cloud  Development of ETL source and target mapping design/specifications based on the business requirements. Create ETLs/ELTs to take data from various operational systems and create a unified/enterprise data model for analytics and reporting  Develop load and transformation processes in support of the requirements, validate that they meet business and technical specifications, manage ongoing maintenance of the system and data, and make recommendations for process improvements to optimize data movement from source to target  Provide production and operational support to existing ETL jobs. Monitor and manage production ETL jobs to verify execution and measure performance to assure ongoing data quality and optimization of the system to manage scalability and performance and identify improvement opportunities for key ETL processes.  Strong troubleshooting and problem-solving skills in large data environment  Capable of investigating, familiarizing and mastering new data sets quickly', 'Education/Experience', ' Proficiency in SQL, data modeling, and data warehousing', ' Data Engineer', ' Development of ETL source and target mapping design/specifications based on the business requirements. Create ETLs/ELTs to take data from various operational systems and create a unified/enterprise data model for analytics and reporting', 'Equipment/Software Used']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
 Data Engineer,Dunhill Professional Search & Government Solutions,"San Antonio, TX",3 weeks ago,76 applicants,"['', 'Excellent attention to detail and analytical skills.\xa0', '1-2 years hands-on experience with SQL relational databases: PostgreSQL, Oracle, MySQL etc...\xa0', 'Review, design, and develop data quality management processes and automated procedures intended to produce high levels of data integrity;\xa0', 'Ability to\xa0be a strong\xa0team\xa0player.\xa0', 'Develop or provide input to engineering artifacts associated with the data repositories\xa0', '5-10 years of related work experience.\xa0', 'Experience with DBA productivity and performance tools.\xa0', '1-2 years hands-on experience in ETL design, and development in large scale data environment\xa0', 'Support design and development of data access APIs that allow enterprise access to data;\xa0', 'Preferred Skills & Qualifications:\xa0', 'US Citizenship required, dual citizens not eligible\xa0', 'Advanced written and verbal communication skills.\xa0', 'Data Engineer\xa0', '2-3 years of related experience and master’s degree in computer science, information engineering, information systems, or other related discipline.\xa0', 'Perform\xa0database management functions across one or more environments, including designing, implementing and maintaining databases, backup/recovery and configuration management.\xa0\xa0', 'Architect, design, and implement updates and enhancements to data repositories and indices to support data ingest, enrichment, analysis, visualization and dissemination;\xa0', 'In depth knowledge of troubleshooting skills and out of the box thinking to overcome data obstacles.\xa0', '1-2 years hands-on experience with python as a scripting language\xa0', 'Design and implement storage and indexing strategies to provide efficient storage and retrieval to support visualizations such as graph-based and geospatial;\xa0', '1-2 years hands-on experience with AWS or Azure\xa0', 'Minimum Qualifications\xa0', '1-2 years hands-on experience designing and implementing data integration, Big Data, and/or business intelligence solutions\xa0', 'Deploy data analytics applications and COTS products for\xa0client environments;\xa0', 'Experience with DoD and Federal Government is strongly desired.\xa0', '2-3 years hands-on experience designing and implementing data integration, Big Data, and/or business intelligence solutions\xa0', 'Ability to multitask and work well under pressure.\xa0', 'Perform database engineering and management activities associated with designing, maintaining, and enhancing data analytics\xa0systems/applications\xa0using an Agile DevSecOps model;\xa0', 'Advanced knowledge of security network and infrastructure tools, including access control and/or encryption.\xa0', 'Advanced knowledge of database backup and recovery strategies.\xa0', 'Job Responsibilities\xa0', 'Perform research and development and provide technical support to identify and integrate new and emerging technologies into the data environment;\xa0', 'Support development and operations of AIP (a\xa0data analytics and AI/ML platform as a service) focusing on\xa0data analytics applications,\xa0engineering of data pipelines and database administration.\xa0', 'Bachelor’s Degree in a related field of study.\xa0', 'Other Job Specific Skills\xa0']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Lead Data Engineer,Sohum Inc,San Francisco Bay Area,1 day ago,Be among the first 25 applicants,"['', 'Sohum Inc.', 'Please send us your resume to :', 'Expert in containerization, including Docker and Kubernetes', 'Full time position in Bay Area, CA with excellent benefits and work life balance.', 'Architect and implement best practices of big data tools such as Hadoop, Spark, Airflow, Kafka, Presto and Time-series Databases', 'Architect and implement Machine Learning Pipelines for Data Science team', 'Experience with Columnar database such as Redshift, Vertica or Parquet', 'Expert in tools such as Spark, Airflow, Presto', 'Understand and influence logging to support our Data flow.', 'vpatil@sohum.biz', 'Expert in design and implement reliable, scalable, and performant distributed systems and data pipelines', 'Responsibilities:', 'We are actively looking for a Lead Data engineer for our client in Bay Area, who would not only build data pipelines to efficiently and reliably move data across systems, but also to build the next generation of data tools to enable them to take full advantage of this data.\xa0', 'Must love coding – prepare to spend 80% of the time on hands-on development with development teams', '4+ years of software development experience in Scala or Java.', 'Qualifications:', '10+ years of experience in design and development of large-scale data platforms', 'Worked on AWS services -\xa0Lambda, S3, Redshift, SNS (or Apache Kafka), SQS, Redis, Data-Lakes, and AWS cloud deployment models', '10+ years of experience in design and development of large-scale data platforms4+ years of software development experience in Scala or Java.Expert in tools such as Spark, Airflow, PrestoExpert in design and implement reliable, scalable, and performant distributed systems and data pipelinesExpert in containerization, including Docker and KubernetesExperience with running large-scale distributed computing infrastructure such as load balancing, Zookeeper, Micro service architectureExperience with Columnar database such as Redshift, Vertica or ParquetHand-on experience in optimizing existing Big Data applications written in Spark, Hive. OptimizingGood understanding of building utilization frameworks on Hadoop, AWS, Azure or GCP (Restful services, Self-service BI and Analytics).Experience in one or more analytical modeling tools/workbench (Jupyter or Zeppelin)Worked on AWS services -\xa0Lambda, S3, Redshift, SNS (or Apache Kafka), SQS, Redis, Data-Lakes, and AWS cloud deployment models', 'Experience in one or more analytical modeling tools/workbench (Jupyter or Zeppelin)', 'Build and expand Hadoop Open source Big data Clusters within an agile collaboration environment', 'Experience with running large-scale distributed computing infrastructure such as load balancing, Zookeeper, Micro service architecture', 'Vandana Patil', 'Experience with streaming data would be a huge plus.', 'Hand-on experience in optimizing existing Big Data applications written in Spark, Hive. Optimizing', 'Architect and implement best practices of big data tools such as Hadoop, Spark, Airflow, Kafka, Presto and Time-series DatabasesArchitect and implement Machine Learning Pipelines for Data Science teamBuild and expand Hadoop Open source Big data Clusters within an agile collaboration environmentMust love coding – prepare to spend 80% of the time on hands-on development with development teamsUnderstand and influence logging to support our Data flow.', 'Good understanding of building utilization frameworks on Hadoop, AWS, Azure or GCP (Restful services, Self-service BI and Analytics).', ""In this role, your work will broadly influence the company's products, data consumers and data scientists.""]",Director,Full-time,Engineering,Animation,2021-03-18 14:34:51
Data Engineer / Analytics Engineer,Upfront Healthcare Services,"Chicago, IL",5 days ago,59 applicants,"['', 'Job Responsibilities', 'Conducting discovery and writing product requirements for new data products', 'Building upon our core infrastructure to co-locate and access all of our data from one place', 'Integrating datasets (e.g. census data, patient response data) to continue to drive personalization into our platform', 'What’s Up?', 'Enhancing the Upfront Data Lake and expanding our Data Governance to ensure data remains organized and easily accessible', 'Who are You?', 'Why Now?', 'Creating data & ML pipelines for to scale our products and operations', 'Creating transparency into our product release process', 'Constructing reporting dashboards in Tableau for end users to consume', 'Building upon our core infrastructure to co-locate and access all of our data from one placeEnhancing the Upfront Data Lake and expanding our Data Governance to ensure data remains organized and easily accessibleConstructing reporting dashboards in Tableau for end users to consumeCreating data & ML pipelines for to scale our products and operationsIntegrating datasets (e.g. census data, patient response data) to continue to drive personalization into our platformConducting discovery and writing product requirements for new data productsCreating transparency into our product release process']",Entry level,Full-time,Information Technology,Hospital & Health Care,2021-03-18 14:34:51
Data Engineer,Nielsen,"Columbia, MD",3 weeks ago,54 applicants,"['', 'Expertise with Tableau, or other data visualization software and techniques', 'Maintain and continuously improve the data infrastructure and analysis framework for the Audio Data Science team', 'Implement prevention and detection controls to ensure data integrity, as well as detect and address quality escapes', 'Work closely with internal customers and IT personnel to improve current processes and engineer new methods, frameworks and data pipelines', 'Must be proficient with Python (and Spark/Scala) to develop sharable software with the appropriate technical documentation', 'Experience in containerization such as Docker and/or Kubernetes', 'Experience utilizing Apache Spark, Databricks & Airflow', 'Secondary Locations: ', ' Maintain and continuously improve the data infrastructure and analysis framework for the Audio Data Science team Research and develop new technology solutions to meet the needs of upcoming big data projects Transition the data science tech infrastructure away from legacy systems Work with cross-functional teams to implement and validate enhanced audience measurement methodologies Build and refine data queries from large relational databases/data warehouses/data lakes for various analyses and/or requests Utilize tools such as Python, Tableau, AWS, Databricks etc. to independently develop, test and implement high quality custom, modular code to perform complex data analysis, visualizations, and answer client queries Maintain and update comprehensive documentation on departmental procedures, checklists and metrics Implement prevention and detection controls to ensure data integrity, as well as detect and address quality escapes Work closely with internal customers and IT personnel to improve current processes and engineer new methods, frameworks and data pipelines Work as an integral member of the Audio Data Science team in a time-critical production environment Key tasks include – but are not limited to – data integration, data harmonization, automation, examining large volumes of data, identifying & implementing methodological, process & technology improvements Develop and maintain the underlying infrastructure to support forecasting & statistical models, machine learning solutions, big data pipelines (from internal and external sources) used in a production environment ', 'Knowledge of statistical tests and procedures such as ANOVA, Chi-squared, Correlation, Regression, etc', 'Transition the data science tech infrastructure away from legacy systems', 'Utilize tools such as Python, Tableau, AWS, Databricks etc. to independently develop, test and implement high quality custom, modular code to perform complex data analysis, visualizations, and answer client queries', ' Undergraduate or graduate degree in mathematics, statistics, engineering, computer science, economics, business or fields that employ rigorous data analysis. Must be proficient with Python (and Spark/Scala) to develop sharable software with the appropriate technical documentation Experience utilizing Gitlab, Git or similar to manage code development Experience utilizing Apache Spark, Databricks & Airflow Expertise with Tableau, or other data visualization software and techniques Experience in containerization such as Docker and/or Kubernetes Expertise in querying large datasets with SQL and of working with Oracle, Netezza, Data Warehouse and Data Lake data structures Experience in leveraging CI/CD pipelines Experience utilizing cloud computing platforms such as AWS, Azure, etc Strong ability to proactively gather information, work independently as well as within an multi disciplinary team ', 'Maintain and update comprehensive documentation on departmental procedures, checklists and metrics', 'Work with cross-functional teams to implement and validate enhanced audience measurement methodologies', 'Primary Location', 'Knowledge of machine learning and data modeling techniques such as Time Series, Decision Trees, Random Forests, SVM, Neural Networks, Incremental Response Modeling, and Credit Scoring', 'Undergraduate or graduate degree in mathematics, statistics, engineering, computer science, economics, business or fields that employ rigorous data analysis.', 'What will I do?', 'Expertise in querying large datasets with SQL and of working with Oracle, Netezza, Data Warehouse and Data Lake data structures', 'Key tasks include – but are not limited to – data integration, data harmonization, automation, examining large volumes of data, identifying & implementing methodological, process & technology improvements', 'Research and develop new technology solutions to meet the needs of upcoming big data projects', 'Strong ability to proactively gather information, work independently as well as within an multi disciplinary team', 'Develop and maintain the underlying infrastructure to support forecasting & statistical models, machine learning solutions, big data pipelines (from internal and external sources) used in a production environment', 'Experience utilizing Gitlab, Git or similar to manage code development', 'Preferred Experience', 'Experience in leveraging CI/CD pipelines', 'Build and refine data queries from large relational databases/data warehouses/data lakes for various analyses and/or requests', 'Work as an integral member of the Audio Data Science team in a time-critical production environment', 'Data Engineer - 80661', ' Is this for me? ', 'Experience utilizing cloud computing platforms such as AWS, Azure, etc', 'Travel:', ' Knowledge of machine learning and data modeling techniques such as Time Series, Decision Trees, Random Forests, SVM, Neural Networks, Incremental Response Modeling, and Credit Scoring Knowledge of survey sampling methodologies Knowledge of statistical tests and procedures such as ANOVA, Chi-squared, Correlation, Regression, etc ', 'Knowledge of survey sampling methodologies', 'About Nielsen', 'Job Type:']",Not Applicable,Full-time,Information Technology,Market Research,2021-03-18 14:34:51
Data Engineer (Python/PySpark/SAS),Technology Ventures,"McLean, VA",6 days ago,69 applicants,"['', 'Experience with cloud computing and storage services, particularly AWS EMR', 'Experience writing automated unit, integration, regression, performance and acceptance tests', 'BS in Computer Science or equivalent experienceExperience with cloud computing and storage services, particularly AWS EMRExperience writing automated unit, integration, regression, performance and acceptance testsStrong quantitative skills (statistics, econometrics, linear algebra)', 'At least 3 years of experience developing production Python code', 'Write automated tests for Python code.', '1) Please describe your experience converting SAS code to Python (Pandas / PySpark)? What were the major challenges?', 'Leverage PySpark and AWS EMR to parallelize the process and reduce the runtime.', 'Peer review code and automated tests, help team members with design and implementation challenges.', 'A strong understanding of SQL', 'Optimize the Python code to reduce the runtime.', 'Verify that the Python version of the SAS code is equivalent to the SAS version. This involves running both processes, comparing the output, and resolving any differences.', '2) Please describe a project where you wrote production Pandas or PySpark code. How did you test it?', 'Qualifications:', 'Solid understanding of software design principles', 'Strong quantitative skills (statistics, econometrics, linear algebra)', 'BS in Computer Science or equivalent experience', 'A strong understanding of Pandas and PySpark', 'At least 3 years of experience developing production Python codeA strong understanding of Pandas and PySparkA strong understanding of SQLExperience with SASSolid understanding of software design principles', 'Preferred Skills:', 'Enhance the Python process to be fault-tolerant and contain checkpoints to make rerunning a subset of the process more efficient.', 'Translate existing SAS code into Python code. We are using both Pandas data frames and PySpark data frames so knowledge of both is required.Verify that the Python version of the SAS code is equivalent to the SAS version. This involves running both processes, comparing the output, and resolving any differences.Leverage PySpark and AWS EMR to parallelize the process and reduce the runtime.Optimize the Python code to reduce the runtime.Enhance the Python process to be fault-tolerant and contain checkpoints to make rerunning a subset of the process more efficient.Write automated tests for Python code.Peer review code and automated tests, help team members with design and implementation challenges.', 'Experience with SAS', 'Translate existing SAS code into Python code. We are using both Pandas data frames and PySpark data frames so knowledge of both is required.']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer-Remote,"Career Developers, Inc.","New York, NY",2 days ago,Be among the first 25 applicants,"['', 'Participate in internal and client meetings to provide data strategy recommendations, knowledge and expertise.', 'Assist in the creation and documentation of database processes and business rules.', 'Expierence with ESP such as Salesforce Marketing Cloud, Adobe Campaign, Eloqua, Marketo, etc.', 'YOUR RESPONSIBILITIES:', 'Able to write complex SQL statements and map relational database structure.', 'Experience R, Python or other analytical platforms.', 'Must work CST Hours', 'Prior experience with an advertising or consulting agency, a plus.', 'Establish quality control measures to ensure accuracy of data and reports.', 'Audience selection', 'Any data-related certifications, such as SQL, SAS, etc., are an advantage.', 'Provide recommendations for continuous improvement in data accessibility and reporting as driven by the business needs.', 'REQUIRED SKILLS:', 'Work to automate data processes and reports.', ""Bachelor's degree in a quantitative or technology field."", 'Strong experience in MS Excel, Access and PowerPoint.', '2-5 \xa0years of experience in an analytics or technology role.', 'Experience with BI tools such as Tableau, Domo, QlikView, GoodData, Looker, etc., are an advantage.', 'Strong drive to improve and learn new skills.', 'Ability to deliver work within tight timelines, to budget and with high quality.', 'Excellent communication skills (written and verbal) and first-rate interpersonal skills at all levels.', 'Write SQL queries and design data flows to facilitate campaign development and deployment.', 'Assist with mentoring and training junior staff.', 'Experience managing large data extraction and manipulation.', 'Experience with audience management tools such as Adobe Campaign, Unica, Redpoint, etc', ""Audience selectionEstablish quality control measures to ensure accuracy of data and reports.Assist in the creation and documentation of database processes and business rules.Provide recommendations for continuous improvement in data accessibility and reporting as driven by the business needs.Participate in internal and client meetings to provide data strategy recommendations, knowledge and expertise.Work to automate data processes and reports.Write SQL queries and design data flows to facilitate campaign development and deployment.Assist with mentoring and training junior staff.REQUIRED SKILLS:Bachelor's degree in a quantitative or technology field.2-5 \xa0years of experience in an analytics or technology role.Prior experience with an advertising or consulting agency, a plus.Strong drive to improve and learn new skills.Able to write complex SQL statements and map relational database structure.Any data-related certifications, such as SQL, SAS, etc., are an advantage.Strong experience in MS Excel, Access and PowerPoint.Ability to deliver work within tight timelines, to budget and with high quality.Experience managing large data extraction and manipulation.Excellent communication skills (written and verbal) and first-rate interpersonal skills at all levels.Experience with audience management tools such as Adobe Campaign, Unica, Redpoint, etcExpierence with ESP such as Salesforce Marketing Cloud, Adobe Campaign, Eloqua, Marketo, etc.Experience with BI tools such as Tableau, Domo, QlikView, GoodData, Looker, etc., are an advantage.Experience R, Python or other analytical platforms.""]",Entry level,Contract,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Analytics Engineer,Varian Medical Systems,"Atlanta, GA",3 days ago,44 applicants,"['', 'Support DB change requests and manage associated impact', 'Ability to problem solve and willingness to learn new skills and technologies', 'Technologies Required', 'Computer skills in Word, Excel, Power Point, email, etc.', 'Incorporate structured programming techniques for scalable, maintainable code', 'Degree or demonstrated related experience with software engineering in the high technology Service/Engineering industry with experience in the medical capital equipment industry preferred', 'Flexible style (“do whatever it takes” approach)', 'Personality based on strong ethical standard, values, and good judgement', 'Design unit tests and data validation plans and conduct appropriate testing', 'Manage cloud-based data warehouses, data lakes and data marts', 'Programming Skills: Python or similar programming language with willingness to learn Python', 'Job Requirements', 'Fighting cancer calls for big ideas.', 'Perform other project activities as required for team / organizational support', 'Programming Skills: Python or similar programming language with willingness to learn PythonDatabase Management: SQLData Science Toolkits / Libraries: Pandas, NumPy or other data mining solutionsExperience using the following Azure® Services is a plus: Data Factory, Data Lake, Databricks, AzureSQL, Azure Cosmos DBExperience using Splunk® for queries, reports, dashboards, and alerts is a plus', 'Work as part of a cross functional team to define and implement solutions from proof of concept to productionDevelop algorithms to organize unstructured data into useful data setsIncorporate structured programming techniques for scalable, maintainable codeEngage with key stakeholders to develop & deploy the infrastructure necessary to enable the continued development of our analytics capability by leveraging data from remote service for predictive maintenanceDesign & implement database models for SQL Server, including DB schemas and optimizationManage cloud-based data warehouses, data lakes and data martsDocument design specifications and detailed design of algorithms and DB architecturesEngage key stakeholders to define and implement a process that allows our proactive maintenance capability to respond to emergent issues in the field.Design unit tests and data validation plans and conduct appropriate testingSupport DB change requests and manage associated impactDetect, report, investigate, analyze and fix defects with implemented solutionsPerform other project activities as required for team / organizational support', 'Experience using the following Azure® Services is a plus: Data Factory, Data Lake, Databricks, AzureSQL, Azure Cosmos DB', 'Availability for global travel as required (20% estimated)', 'Detect, report, investigate, analyze and fix defects with implemented solutions', 'Design & implement database models for SQL Server, including DB schemas and optimization', 'Data Science Toolkits / Libraries: Pandas, NumPy or other data mining solutions', 'Experience using Splunk® for queries, reports, dashboards, and alerts is a plus', 'Work as part of a cross functional team to define and implement solutions from proof of concept to production', 'Must be fluent in both written and spoken English', 'Develop algorithms to organize unstructured data into useful data sets', 'Role Responsibilities', 'Together, we can beat cancer.', 'Engage key stakeholders to define and implement a process that allows our proactive maintenance capability to respond to emergent issues in the field.', 'Excellent communication skills (verbal and written), ability to speak and present publicly', 'Degree or demonstrated related experience with software engineering in the high technology Service/Engineering industry with experience in the medical capital equipment industry preferredCan demonstrate excellent results in a technical positionAbility to problem solve and willingness to learn new skills and technologiesExcellent communication skills (verbal and written), ability to speak and present publiclyFlexible style (“do whatever it takes” approach)Strong in building and maintaining successful and effective working relationshipsPersonality based on strong ethical standard, values, and good judgementAvailability for global travel as required (20% estimated)Computer skills in Word, Excel, Power Point, email, etc.Experience in using enterprise software applications such as SAP and SalesforceMust be fluent in both written and spoken English', 'Experience in using enterprise software applications such as SAP and Salesforce', 'Engage with key stakeholders to develop & deploy the infrastructure necessary to enable the continued development of our analytics capability by leveraging data from remote service for predictive maintenance', 'Can demonstrate excellent results in a technical position', '#TogetherWeFight', 'Strong in building and maintaining successful and effective working relationships', 'Document design specifications and detailed design of algorithms and DB architectures', 'Database Management: SQL']",Entry level,Full-time,Information Technology,Electrical/Electronic Manufacturing,2021-03-18 14:34:51
Data Engineer,"Stanley Black & Decker, Inc.","New Britain, CT",2 weeks ago,Be among the first 25 applicants,"['', ' Strong Experience with data, wrangling and cleansing, analysis ', ' Purpose-Driven Company ', ' 2+ years relevant experience or equivalent preferred. ', ' Diverse & Inclusive Culture ', 'What You’ll Also Get', ' Experience with exploratory data analysis, data gathering, data mining and data analysis techniques. ', ' Experience in web scraping and consuming REST based API (with JSON payload) highly preferred. ', ' Experience with Cloud based HaaS/PaaS solutions such as AWS EMR, MS Azure preferred. ', ' Support and provide expertise in building and operationalizing performant data pipelines in a cloud-based data lake environment.  Perform data wrangling, cleansing, transformation, analysis and big data technologies is required.  Work with relational and unstructured data formats to create analytics-ready datasets for user friendly analytic solutions that identifies and aggregates data elements into decision models and other analytical support tools. ', ' MS degree in Computer Science, Engineering, Information Systems Management, or Data Science preferred  2+ years relevant experience or equivalent preferred.  Strong Experience with data, wrangling and cleansing, analysis  Advanced SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.  Strong knowledge of Python data analysis libraries including Pandas required  Fluency in big data platforms including Hadoop, MapReduce, Hive, SQL DBs, Spark  Experience with exploratory data analysis, data gathering, data mining and data analysis techniques.  Familiarity with Machine Learning concepts required.  Experience building and optimizing ‘big data’ data pipelines, in spark or hive highly preferred.  Experience in web scraping and consuming REST based API (with JSON payload) highly preferred.  Experience with data modeling, data architecture design from complex data sources.  Hands on experience with Machine Learning in python and/or spark highly preferred  Experience in optimizing big data pipelines in hive or Spark or traditional database workloads preferred.  Experience in Shell scripting, R and Snowflake a plus.  Experience with Cloud based HaaS/PaaS solutions such as AWS EMR, MS Azure preferred.  Familiar with software development and agile concepts.  Fluency in various Operating Systems fundamentals (Unix etc.) ', 'What You’ll Do', ' Hands on experience with Machine Learning in python and/or spark highly preferred ', ' Experience in Shell scripting, R and Snowflake a plus. ', ' Work with relational and unstructured data formats to create analytics-ready datasets for user friendly analytic solutions that identifies and aggregates data elements into decision models and other analytical support tools. ', ' Data Engineer - Data & Analytics - Remote ', ' Strong knowledge of Python data analysis libraries including Pandas required ', '  Diverse & Inclusive Culture  : We pride ourselves on being an awesome place to work. We respect and embrace differences because that’s how the best work gets done. You’ll find we like to have fun here, too. ', ' Perform data wrangling, cleansing, transformation, analysis and big data technologies is required. ', 'EEO Statement', 'Eeo', 'Featured Category on SBD Careers', 'No. of Positions', 'Who You Are', ' Advanced SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases. ', '  Career Opportunity  : Career paths aren’t linear here. Being part of our global company with 60+ brands gives you the chance to grow and develop your skills along multiple career paths. ', ' Fluency in various Operating Systems fundamentals (Unix etc.) ', ' Experience in optimizing big data pipelines in hive or Spark or traditional database workloads preferred. ', '  Purpose-Driven Company  : You’ll help us continue to make positive changes in the local communities where we work and live as well as in the broader world through volunteerism, giving back and sustainable business practices. ', '  Career Opportunity  : Career paths aren’t linear here. Being part of our global company with 60+ brands gives you the chance to grow and develop your skills along multiple career paths.   Learning & Development  : Our lifelong learning philosophy means you’ll have access to a wealth of state-of-the-art learning resources, including our Lean Academy and online university (where you can get certificates and specializations from renowned colleges and universities).   Diverse & Inclusive Culture  : We pride ourselves on being an awesome place to work. We respect and embrace differences because that’s how the best work gets done. You’ll find we like to have fun here, too.   Purpose-Driven Company  : You’ll help us continue to make positive changes in the local communities where we work and live as well as in the broader world through volunteerism, giving back and sustainable business practices. ', ' MS degree in Computer Science, Engineering, Information Systems Management, or Data Science preferred ', 'All qualified applicants to Stanley Black & Decker are considered for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran’s status or any other protected characteristic.', '  Learning & Development  : Our lifelong learning philosophy means you’ll have access to a wealth of state-of-the-art learning resources, including our Lean Academy and online university (where you can get certificates and specializations from renowned colleges and universities). ', 'Who We Are', 'Benefits & Perks', ' Familiar with software development and agile concepts. ', ' Experience with data modeling, data architecture design from complex data sources. ', ' Support and provide expertise in building and operationalizing performant data pipelines in a cloud-based data lake environment. ', ' Career Opportunity ', ' Fluency in big data platforms including Hadoop, MapReduce, Hive, SQL DBs, Spark ', ' Familiarity with Machine Learning concepts required. ', 'Requisition Number', 'Business', 'Job Description', ' Experience building and optimizing ‘big data’ data pipelines, in spark or hive highly preferred. ', ' Learning & Development ', 'Function']",Associate,Full-time,Information Technology,Construction,2021-03-18 14:34:51
Data Engineer,Numerator,"Chicago, IL",1 week ago,51 applicants,"['', 'Regular hackathons to build your own projects and Engineering Lunch and Learns.', 'We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law.', 'Experience with a data pipeline scheduling framework (Airflow)', 'Knowledge of and experience implementing data security and governance best practices', 'Volunteer time off and charitable donation matching.', 'Market competitive total compensation package.', 'An inclusive and collaborative company culture - we work in an open environment while working together to get things done, and adapt to the changing needs as they come.', ' Develop expertise in the different upstream data stores and systems across Numerator Design, develop and maintain data integration pipelines for Numerators growing data sets and product offerings Collaborate with product and engineering teams to take requirements from prototype to production Build data validation testing frameworks to ensure high data quality and integrity Write and maintain documentation on data pipelines and schemas ', 'An opportunity to have an impact in a technologically data driven company.', ' Amazon Web Services (EC2, DMS, RDS) experience Terraform and/or ansible (or similar) for infrastructure deployment Airflow -- Experience building and monitoring DAGs, developing custom operators and using script templating solutions Experience supporting production systems and developing on-call/incident management playbooks Ability to work with team members located in multiple geographies and time zones. Curious and interested in learning about the latest in data warehouse technology Interest and willingness to mentor junior team members ', 'Bachelors degree in Computer Science or related field of study required; Masters degree preferred', '5 + years of experience in the data warehouse space', 'Amazon Web Services (EC2, DMS, RDS) experience', 'Curious and interested in learning about the latest in data warehouse technology Interest and willingness to mentor junior team members', 'What We Offer You', 'Experience with schema design and dimensional data modeling', 'Design, develop and maintain data integration pipelines for Numerators growing data sets and product offerings', ' An inclusive and collaborative company culture - we work in an open environment while working together to get things done, and adapt to the changing needs as they come. An opportunity to have an impact in a technologically data driven company. Ownership over platforms and environments of an industry leading product. Market competitive total compensation package. Volunteer time off and charitable donation matching. Strong support for career growth, including mentorship programs, leadership training, access to conferences and employee resource groups. Regular hackathons to build your own projects and Engineering Lunch and Learns. Great benefits package including health/vision/dental, unlimited PTO, 401k matching, travel reimbursement and more. ', 'Exceptional candidates will have', 'Write and maintain documentation on data pipelines and schemas', 'Collaborate with product and engineering teams to take requirements from prototype to production', 'Terraform and/or ansible (or similar) for infrastructure deployment', 'Build data validation testing frameworks to ensure high data quality and integrity', 'Great benefits package including health/vision/dental, unlimited PTO, 401k matching, travel reimbursement and more.', 'Experience supporting production systems and developing on-call/incident management playbooks', ' Bachelors degree in Computer Science or related field of study required; Masters degree preferred 5 + years of experience in the data warehouse space Knowledge of software engineering best practices across the development lifecycle, coding standards, code reviews, source management, build processes, testing, and operations Knowledge of and experience implementing data security and governance best practices Expert in SQL, including advanced analytical queries, window functions, CTEs and query optimization Advanced proficiency in Python (data structures, algorithms, object oriented programming, using APIs)Experience administering a cloud data warehouse (Redshift, Snowflake, Vertica) Experience with a data pipeline scheduling framework (Airflow) Experience with schema design and dimensional data modeling ', 'Duties/Responsibilities Include', 'Skills & Requirements', 'Develop expertise in the different upstream data stores and systems across Numerator', 'Advanced proficiency in Python (data structures, algorithms, object oriented programming, using APIs)Experience administering a cloud data warehouse (Redshift, Snowflake, Vertica)', 'Job Description', 'Knowledge of software engineering best practices across the development lifecycle, coding standards, code reviews, source management, build processes, testing, and operations', 'Airflow -- Experience building and monitoring DAGs, developing custom operators and using script templating solutions', 'Ability to work with team members located in multiple geographies and time zones.', 'Strong support for career growth, including mentorship programs, leadership training, access to conferences and employee resource groups.', 'Expert in SQL, including advanced analytical queries, window functions, CTEs and query optimization', 'Ownership over platforms and environments of an industry leading product.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer / Data Integration,ArborMetrix,"Ann Arbor, MI",3 weeks ago,42 applicants,"['', 'Experience in using data integration / ETL tools to construct production-quality, real-time interfaces', 'Knowledge of web technologies such as RESTful APIs', 'Focus and initiative in learning and enhancing existing products and future enhancements', 'Monitoring operational ETL processes, triaging and resolving issues as they arise to meet SLAs', 'Maintaining data standards (including adherence to HIPAA), documenting work, and championing product and process improvements', 'Ability to manage multiple deliverables with competing deadlines in a fast-paced environment', 'Experience in using data integration / ETL tools to construct production-quality, real-time interfacesStrong understanding of relational databases, data modeling / architecture, and SQL query design / optimizationFamiliarity with a variety of structured and unstructured file formatsKnowledge of web technologies such as RESTful APIsExperience using SQL to manage data as well as proficiency in at least one scripting language (e.g. JavaScript, Python, Linux shell script)Focus and initiative in learning and enhancing existing products and future enhancementsAbility to manage multiple deliverables with competing deadlines in a fast-paced environmentStrong communication and problem-solving skillsBachelor’s degree in a quantitative field such as Computer Science or Health Informatics', 'Master’s degree in a quantitative field such as Computer Science or Health InformaticsFamiliarity with healthcare data integration technologies (e.g. HL7 2.x, C-CDA, and FHIR)Project experience involving direct integration with EMR systemsExperience with or knowledge of Amazon Web Services', 'Implementing robust ETL solutions that integrate heterogeneous healthcare data feeds using tools such as Apache NiFi, Mirth (JavaScript), CloverDX, PostgreSQL, Python, and Linux shell scripting', 'Collaborating with an innovative team in an Agile/Scrum environment', 'Familiarity with a variety of structured and unstructured file formats', 'Strong communication and problem-solving skills', 'Experience with or knowledge of Amazon Web Services', 'Minimum Qualifications', 'Utilizing APIs and web services to permit bi-directional flows of data to and from our partner organizations', 'Project experience involving direct integration with EMR systems', 'Participating in the design, implementation, and validation of a robust, scalable, next-generation Data Integration Engine', 'Participating in the design, implementation, and validation of a robust, scalable, next-generation Data Integration EngineImplementing robust ETL solutions that integrate heterogeneous healthcare data feeds using tools such as Apache NiFi, Mirth (JavaScript), CloverDX, PostgreSQL, Python, and Linux shell scriptingBuilding direct interfaces to hospitals and EMR vendors utilizing industry standards such as HL7 2.x, C-CDA, and FHIR as well as implementing bespoke interfaces that consume formats such as XML, JSON, and delimited filesUtilizing APIs and web services to permit bi-directional flows of data to and from our partner organizationsMonitoring operational ETL processes, triaging and resolving issues as they arise to meet SLAsWriting PL/SQL functions and packages to automate workflows in a Postgres environment linked with web technologiesApplying industry best practices of software design, implementation, testing, and deployment, with a particular emphasis on quality assurance, fault tolerance, and component reuseMaintaining data standards (including adherence to HIPAA), documenting work, and championing product and process improvementsCollaborating with an innovative team in an Agile/Scrum environment', 'Building direct interfaces to hospitals and EMR vendors utilizing industry standards such as HL7 2.x, C-CDA, and FHIR as well as implementing bespoke interfaces that consume formats such as XML, JSON, and delimited files', 'Strong understanding of relational databases, data modeling / architecture, and SQL query design / optimization', 'Bachelor’s degree in a quantitative field such as Computer Science or Health Informatics', 'Company Description', 'Experience using SQL to manage data as well as proficiency in at least one scripting language (e.g. JavaScript, Python, Linux shell script)', 'Master’s degree in a quantitative field such as Computer Science or Health Informatics', 'Writing PL/SQL functions and packages to automate workflows in a Postgres environment linked with web technologies', 'Job Description', 'Familiarity with healthcare data integration technologies (e.g. HL7 2.x, C-CDA, and FHIR)', 'Preferred Qualifications', 'Applying industry best practices of software design, implementation, testing, and deployment, with a particular emphasis on quality assurance, fault tolerance, and component reuse']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,"Marketo, an Adobe company","Lehi, UT",2 weeks ago,141 applicants,"['', 'Document models, relationships, transformations, and definitions', 'Experience with Linux, crontab, and bash', 'Exceptional: We’re committed to creating exceptional experiences that delight our employees and customers.', 'Architect solid data models which will provide actionable insights', 'Experience building sales funnel and marketing attribution data models', 'Job Summary', 'Improve overall robustness and efficiency of existing data marts', 'Experience developing data models for use in business intelligence tools', 'Job Responsibilities', 'Exposure to Tableau, Power BI, Clik, Quicksight, or other BI tools', 'Involved: We’re inclusive, open, and actively engaged with our customers, partners, employees, and the communities we serve.', 'Innovative: We’re highly creative and always striving to connect new ideas with business realities.', 'Work with business stakeholders to understand analytics needsArchitect solid data models which will provide actionable insightsDemonstrate the functionality and power of data models to the businessImprove overall robustness and efficiency of existing data martsGain a deep knowledge of data sources, definitions, and business metricsBuild data models that are easy to use and understand by business stakeholdersDesign and build data models which combine multiple data sourcesExpress business definitions through SQL transformations and calculationsDevelop SQL code optimized for performanceDocument models, relationships, transformations, and definitions', 'Bonus Skills', 'Genuine: We are sincere, trustworthy, and reliable.', 'Experience with scripting languages such as Python', 'Our Company', 'Expert SQL programmer within a data warehousing environment', 'About Workfront', 'Express business definitions through SQL transformations and calculations', 'Develop SQL code optimized for performance', 'Experience building slowly changing dimensions', 'Advanced SQL functions including windowing', 'Experience with data lake or MPP database technologies', 'Values Fit', 'Experience working with multiple business groups to meet their analytics needsStrong understanding of data marts, data modeling, and related methodologiesExperience working with star schemasExpert SQL programmer within a data warehousing environmentAdvanced SQL functions including windowingExperience with data lake or MPP database technologiesExperience developing data models for use in business intelligence toolsExperience building slowly changing dimensionsExperience with data validationExposure to Tableau, Power BI, Clik, Quicksight, or other BI tools', 'Demonstrate the functionality and power of data models to the business', 'Working knowledge of code repositories (preferably Git)', 'Understanding of Salesforce, Marketo, Netsuite, and other enterprise systems', 'Experience with Presto, Redshift, Snowflake, Dremio, Vertica, Greenplum, or NetezzaExperience with scripting languages such as PythonExperience with Linux, crontab, and bashUnderstanding of file formats like yaml, json, and xmlWorking knowledge of code repositories (preferably Git)Experience building sales funnel and marketing attribution data modelsUnderstanding of Salesforce, Marketo, Netsuite, and other enterprise systems', 'Design and build data models which combine multiple data sources', 'Work with business stakeholders to understand analytics needs', 'Build data models that are easy to use and understand by business stakeholders', 'Experience', 'Experience working with star schemas', 'Experience with data validation', 'Understanding of file formats like yaml, json, and xml', 'Experience working with multiple business groups to meet their analytics needs', 'Strong understanding of data marts, data modeling, and related methodologies', 'Genuine: We are sincere, trustworthy, and reliable.Exceptional: We’re committed to creating exceptional experiences that delight our employees and customers.Innovative: We’re highly creative and always striving to connect new ideas with business realities.Involved: We’re inclusive, open, and actively engaged with our customers, partners, employees, and the communities we serve.', 'Experience with Presto, Redshift, Snowflake, Dremio, Vertica, Greenplum, or Netezza', 'Gain a deep knowledge of data sources, definitions, and business metrics']",Mid-Senior level,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Data Engineer,Presidio,"Charlotte, NC",2 weeks ago,46 applicants,"['', ' for assistance.', ' ', 'About Presidio', 'Collaborate as assist with solution design', 'Experience with GCP or Azure cloud solution', 'Presidio is a VEVRAA Federal Contractor requesting priority referrals of protected veterans for its openings. State Employment Services, please provide priority referrals to ', 'Deep understanding of Batch and streaming data pipelines', 'Presidio is committed to Diversity, Equity, and Inclusion at the highest levels and has strengthened its drive to build and drive systemic DE&I change process across all levels of the organization.', 'THE ROLE: Data Engineer', 'Recruitment Agencies Please Note', ' Experience with GCP or Azure cloud solution Experience with Qubole Data Platform ', 'Cultivating a culture of inclusion where the expression of all our differences are valued, celebrated, and add to our collective achievements.', 'Assist project team with oversight of junior team members: task delegation; technical assistance; oversight and QA', 'Deep understanding of AWS Data & Analytics services (Kinesis, S3, EMR, Athena, Redshift, etc.)', 'Be able to build data pipelines and platforms using proven development tools and languages such as Eclipse, IntelliJ, Python, Scala, Spark, PySpark, AWS Glue', 'Ability to articulate a problem and find solutions in a timely manner', 'Experience implementing Machine Learning solutions and services on AWS or GCP', 'Identify, build, and implement optimal Data Platform', ' 2+ years of engineering experience Bachelor’s degree in computer science, mathematics, statistics or a similar quantitative field Core understanding of Big Data principles and architectural patterns Deep understanding of Batch and streaming data pipelines Have proven experience in building out Data Lakes using AWS services and other CSPs Deep understanding of AWS Data & Analytics services (Kinesis, S3, EMR, Athena, Redshift, etc.) Proven experience building or administering BI tools such as Tableau, Domo, Lookr, etc. Be able to build data pipelines and platforms using proven development tools and languages such as Eclipse, IntelliJ, Python, Scala, Spark, PySpark, AWS Glue Experience implementing Machine Learning solutions and services on AWS or GCP ', '.', '(Get Acrobat Reader)', 'To read more about discrimination protections under Federal Law, please visit: ', 'https://www1.eeoc.gov/employers/upload/eeoc_self_print_poster.pdf', 'Build out ETL jobs per architectural guidelines which integrates within the data pipeline', 'recruitment@presidio.com', 'Presidio is an Equal Opportunity / Affirmative Action Employer / VEVRAA Federal Contractor. All qualified candidates will receive consideration for this position regardless of race, color, creed, religion, national origin, age, sex, citizenship, ethnicity, veteran status, marital status, disability, sexual orientation, gender identification or any other characteristic protected by applicable federal, state and local statutes, regulations and ordinances.To read more about discrimination protections under Federal Law, please visit: https://www1.eeoc.gov/employers/upload/eeoc_self_print_poster.pdf (Get Acrobat Reader)If you have any difficulty using our online system and need an accommodation in the job application process due to a disability, please send an email to recruitment@presidio.com for assistance.Presidio is a VEVRAA Federal Contractor requesting priority referrals of protected veterans for its openings. State Employment Services, please provide priority referrals to recruitment@presidio.com.Recruitment Agencies Please NoteAgencies/3rd Parties may not solicit to any employee of Presidio. Any candidate information received from any Agency/3rd Party will be considered a gift and property of Presidio, unless the Agency/3rd Party is an Authorized Vendor of Presidio with an up-to-date Presidio Contract in hand signed by Presidio Talent Acquisition. No payment will be made to any Agency/3rd Party who is not an Authorized Vendor, nor has specific approval in writing from Presidio Talent Acquisition to engage in recruitment efforts for Presidio.', 'COME BUILD YOUR FUTURE WITH PRESIDIO!', 'Core understanding of Big Data principles and architectural patterns', 'For more information visit: ', 'WHY YOU SHOULD JOIN US?', 'Desired Qualifications', 'Bachelor’s degree in computer science, mathematics, statistics or a similar quantitative field', 'Have proven experience in building out Data Lakes using AWS services and other CSPs', 'Presidio is an Equal Opportunity / Affirmative Action Employer / VEVRAA Federal Contractor. All qualified candidates will receive consideration for this position regardless of race, color, creed, religion, national origin, age, sex, citizenship, ethnicity, veteran status, marital status, disability, sexual orientation, gender identification or any other characteristic protected by applicable federal, state and local statutes, regulations and ordinances.', 'About PresidioPresidio is committed to Diversity, Equity, and Inclusion at the highest levels and has strengthened its drive to build and drive systemic DE&I change process across all levels of the organization. Cultivating a culture of inclusion where the expression of all our differences are valued, celebrated, and add to our collective achievements.Presidio is a leading global IT solutions provider assisting clients in harnessing technology innovation and simplifying IT complexity to digitally transform their businesses and drive return on IT investment. Our Digital Infrastructure, Business Analytics, Cloud, Security & Emerging solutions enable our almost 7,000 middle market, enterprise, and government clients to take advantage of new digital revenue streams, omnichannel customer experience models, and the rich data insights generated by those interactions.We serve as an extension of our clients’ IT teams, providing deep expertise and letting them focus on their core business. Within Presidio’s 40+ US offices and offices in Ireland, London, Singapore, and India, we support 2,800+ professionals, including 1,600 technical engineers. Presidio is a trusted advisor to our clients on a national level while also bringing our global scale and expertise to bear.For more information visit: www.presidio.com Presidio is an Equal Opportunity / Affirmative Action Employer / VEVRAA Federal Contractor. All qualified candidates will receive consideration for this position regardless of race, color, creed, religion, national origin, age, sex, citizenship, ethnicity, veteran status, marital status, disability, sexual orientation, gender identification or any other characteristic protected by applicable federal, state and local statutes, regulations and ordinances.To read more about discrimination protections under Federal Law, please visit: https://www1.eeoc.gov/employers/upload/eeoc_self_print_poster.pdf (Get Acrobat Reader)If you have any difficulty using our online system and need an accommodation in the job application process due to a disability, please send an email to recruitment@presidio.com for assistance.Presidio is a VEVRAA Federal Contractor requesting priority referrals of protected veterans for its openings. State Employment Services, please provide priority referrals to recruitment@presidio.com.Recruitment Agencies Please NoteAgencies/3rd Parties may not solicit to any employee of Presidio. Any candidate information received from any Agency/3rd Party will be considered a gift and property of Presidio, unless the Agency/3rd Party is an Authorized Vendor of Presidio with an up-to-date Presidio Contract in hand signed by Presidio Talent Acquisition. No payment will be made to any Agency/3rd Party who is not an Authorized Vendor, nor has specific approval in writing from Presidio Talent Acquisition to engage in recruitment efforts for Presidio.', 'If you have any difficulty using our online system and need an accommodation in the job application process due to a disability, please send an email to ', 'Ability to proactively identify performance gaps in a solution and provide guidance for improvements', 'Ability to work independently with minimal supervision', 'Experience with Qubole Data Platform', 'Proven experience building or administering BI tools such as Tableau, Domo, Lookr, etc.', 'Required Qualifications', ' Participate as lead engineer on projects and assist in communication/collaboration sessions with clients Assist project team with oversight of junior team members: task delegation; technical assistance; oversight and QA Collaborate as assist with solution design Identify, build, and implement optimal Data Platform Identify, build, and implement highly scalable Data pipeline with automation Build out ETL jobs per architectural guidelines which integrates within the data pipeline Work with stakeholders including executive and data teams to troubleshoot and identify issues within the Data Platform Suggest and implement optimization patterns for an existing or new Data Platform Identify and implement BI tools per architectural guidelines Ability to proactively identify performance gaps in a solution and provide guidance for improvements Ability to work independently with minimal supervision Ability to articulate a problem and find solutions in a timely manner Ability to travel up to 30% to customer sites ', '2+ years of engineering experience', 'Ability to travel up to 30% to customer sites', 'Identify and implement BI tools per architectural guidelines', 'www.presidio.com', 'About PresidioPresidio is committed to Diversity, Equity, and Inclusion at the highest levels and has strengthened its drive to build and drive systemic DE&I change process across all levels of the organization. Cultivating a culture of inclusion where the expression of all our differences are valued, celebrated, and add to our collective achievements.Presidio is a leading global IT solutions provider assisting clients in harnessing technology innovation and simplifying IT complexity to digitally transform their businesses and drive return on IT investment. Our Digital Infrastructure, Business Analytics, Cloud, Security & Emerging solutions enable our almost 7,000 middle market, enterprise, and government clients to take advantage of new digital revenue streams, omnichannel customer experience models, and the rich data insights generated by those interactions.We serve as an extension of our clients’ IT teams, providing deep expertise and letting them focus on their core business. Within Presidio’s 40+ US offices and offices in Ireland, London, Singapore, and India, we support 2,800+ professionals, including 1,600 technical engineers. Presidio is a trusted advisor to our clients on a national level while also bringing our global scale and expertise to bear.For more information visit: www.presidio.com ', 'Participate as lead engineer on projects and assist in communication/collaboration sessions with clients', 'Work with stakeholders including executive and data teams to troubleshoot and identify issues within the Data Platform', 'Suggest and implement optimization patterns for an existing or new Data Platform', 'Identify, build, and implement highly scalable Data pipeline with automation']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,TriCom Technical Services,"Kansas City, MO",3 days ago,31 applicants,"['Create strategic, innovative software solutions to create customer value from data.', 'Data Engineer', 'Support existing infrastructure and troubleshoot issues related to data.', 'Responsibilities', 'Design and implement effective database models to store and retrieve company data.', 'Analyze internal and external data sources for opportunities to drive optimization and improvement of products and services.', ' Create strategic, innovative software solutions to create customer value from data. Design and implement effective database models to store and retrieve company data. Implement and extend streaming data, E (L) TL, data cataloging, and reporting frameworks. Provide documentation to support implementation deigns. Support existing infrastructure and troubleshoot issues related to data. Research and create PoCs for new data related technologies. Analyze internal and external data sources for opportunities to drive optimization and improvement of products and services. ', 'Experience with advance database systems crossing relational and non-relational including MS SQL, Hive, MySQL, MongoDB, and Elasticsearch.', 'Provide documentation to support implementation deigns.', ' 3 years of experience with C#, Java, or Scala. Experience with advance database systems crossing relational and non-relational including MS SQL, Hive, MySQL, MongoDB, and Elasticsearch. Experience with Cloudera/Hortonworks Hadoop distribution. ', '3 years of experience with C#, Java, or Scala.', 'Implement and extend streaming data, E (L) TL, data cataloging, and reporting frameworks.', 'Experience with Cloudera/Hortonworks Hadoop distribution.', 'Requirements', 'Research and create PoCs for new data related technologies.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,"Stellar Consulting Solutions, LLC","Marietta, GA",3 days ago,Be among the first 25 applicants,[''],Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Karat,Greater Seattle Area,2 weeks ago,73 applicants,"['', 'Level up the skills of everyone around you through direct mentorship, brown bags, clean code, and setting a strong example.\xa0', 'The ideal candidate for this role is comfortable designing and building software for real customers, manipulating data and wielding statistics, providing technical and product recommendations, working cross-functionally to support ideas from inception to launch, and capable of presenting and being a thought partner across the company.\xa0', 'Experience with data visualization and presentation, familiar with data analysis and BI tools', 'Our Data Engineers work directly with analysts, scientists, and stakeholder teams to build the infrastructure, reporting, and product features that power insights at Karat. Data Engineers at Karat design and build the data infrastructure needed to make interviews more predictive. They also engineer the systems and processes needed to derive meaning from this data. Working on the Data and Delivery engineering teams you will shape the future of how our company ingests, analyzes, and talks about data.', 'About You:', 'Design, build and deploy efficient and reliable cloud-based data pipelines to move data across a number of platforms and disparate data sources', 'Medical / dental / vision insurance\xa0', 'Strong data analysis and scientific thinking skills', 'Core Responsibilities:', 'In keeping with our beliefs and goals, no employee or applicant will face discrimination or harassment based on: race, color, ancestry, national origin, religion, age, gender, marital/domestic partner status, sexual orientation, gender identity or expression, disability status, or veteran status. Above and beyond discrimination and harassment based on “protected categories,” we also strive to prevent other subtler forms of inappropriate behavior (i.e., stereotyping) from ever gaining a foothold in our office. Whether blatant or hidden, barriers to success have no place at Karat.', 'Collaborate with Software Engineers, Data Scientists, and Data Analysts to design, implement and scale our data processing and analysis platformsDesign, build and deploy efficient and reliable cloud-based data pipelines to move data across a number of platforms and disparate data sourcesIdentify data needs for business and product teams, understand their metrics and analysis requirements and build solutions to enable data-driven decisionsProtect our data sources against quality issues. Design and develop data quality monitoring.As we collect richer data about user interactions during interviews, you will drive the collection and refinement of our data to increase the signal captured in every minute of our interviews and maximize the value provided to our customers.Support our engineering teams by reviewing code and designsNurture the data conversations and culture at Karat by educating technical and non-technical teammates about data literacy and usage.Level up the skills of everyone around you through direct mentorship, brown bags, clean code, and setting a strong example.\xa0', 'Benefits of joining Karat:', 'As we collect richer data about user interactions during interviews, you will drive the collection and refinement of our data to increase the signal captured in every minute of our interviews and maximize the value provided to our customers.', 'Experience with and a knack for communicating technical work to stakeholders', 'Experience with development best practices, including version control, code reviews, and documentation', 'While doing meaningful work is rewarding in itself, we also offer the following programs and benefits for all our full-time employees:', 'Significant experience with relational databases with a strong knowledge of SQL and data modeling best principles', 'Experience with Amazon Web Services', 'Flexible vacation and paid company holidays', 'Paid parental leave\xa0', '2+ years of experience in data engineering, software engineering, or other related roles, with a focus on building data pipelines and conducting data analysis', 'Collaborate with Software Engineers, Data Scientists, and Data Analysts to design, implement and scale our data processing and analysis platforms', 'Bachelor’s degree in Computer Science or a related field (or equivalent experience)', 'Protect our data sources against quality issues. Design and develop data quality monitoring.', 'Experience with Python, Docker and job orchestration frameworks such as Apache Airflow', '2+ years of experience in data engineering, software engineering, or other related roles, with a focus on building data pipelines and conducting data analysisExperience with development best practices, including version control, code reviews, and documentationBachelor’s degree in Computer Science or a related field (or equivalent experience)Experience with Amazon Web ServicesExperience with and a knack for communicating technical work to stakeholdersExperience with data visualization and presentation, familiar with data analysis and BI toolsStrong data analysis and scientific thinking skillsExperience with Python, Docker and job orchestration frameworks such as Apache AirflowSignificant experience with relational databases with a strong knowledge of SQL and data modeling best principles', 'Statement of Non-Discrimination:', 'State-of-the-art equipment for your work station', 'Identify data needs for business and product teams, understand their metrics and analysis requirements and build solutions to enable data-driven decisions', 'Competitive salary and benefitsMedical / dental / vision insurance\xa0Flexible vacation and paid company holidaysPaid parental leave\xa0State-of-the-art equipment for your work station', 'Support our engineering teams by reviewing code and designs', 'We value a diverse workforce: people of color, women, and LGBTQIA+ individuals are strongly encouraged to apply.', 'Competitive salary and benefits', 'Nurture the data conversations and culture at Karat by educating technical and non-technical teammates about data literacy and usage.', '\xa0']",Mid-Senior level,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Quantitative Data Engineer,Schonfeld,"New York, NY",1 day ago,28 applicants,"['', 'Highly skilled and analytical problem solver with logical thought processes and quantitative aptitude', 'Experience within a quantitative trading firm/hedge fund', 'Experience with real-time market data (Thomson Reuters, Bloomberg)', 'We’d Love If You Had', 'Excellent communication skills, both written and verbal', 'Expertise in handling and working with equity data', 'What You’ll Do', 'Experience with data storage and processing - Kafka, Spark, Hadoop', ' Experience within a quantitative trading firm/hedge fund Experience with real-time market data (Thomson Reuters, Bloomberg) ', 'Our culture', 'Knowledge of NLP and ML', 'Experience with high volume, high availability distributed systems', 'Ability to work under pressure handling Level 3 support issues', '2-4 years of experience engineering Big Data projects', 'What you’ll bring', 'Who We Are', 'Experience implementing and deploying on the cloud – AWS, Docker, Kubernetes', 'The Role', 'What You Need', ' 2-4 years of experience engineering Big Data projects Expertise with OO programming - C++, Python highly preferred Expertise in handling and working with equity data Experience with high volume, high availability distributed systems Knowledge of NLP and ML Experience implementing and deploying on the cloud – AWS, Docker, Kubernetes Experience with data storage and processing - Kafka, Spark, Hadoop Excellent communication skills, both written and verbal Ability to work under pressure handling Level 3 support issues Highly skilled and analytical problem solver with logical thought processes and quantitative aptitude Strong ownership experience and a track record of delivering results ', 'Strong ownership experience and a track record of delivering results', 'Expertise with OO programming - C++, Python highly preferred']",Mid-Senior level,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Engineer,Arcadia,United States,3 weeks ago,99 applicants,"['', 'Developing internal and external professional communication skills including presentation of issues using appropriate industry vocabulary', 'Delivery', 'Opportunity to be a part of a mission driven organization focused on helping provider organizations change the way they provide care to their patients', 'Design and documentation of connectors / ingestion pipelines', 'SQL: 2-4 year (Preferred)', 'Understanding of shared value contracts that our customers are in and how data is impacted by them', 'TECH', 'The expectations of the day to day of an engineer is as follows:', 'Amazing benefits including unlimited FTO', 'Properly contribute to scrum ceremonies and ceremonies within the dev cycles while successfully updating status and progress in Jira\xa0', 'Set your own personal vision of development and career aspirations and set a working path forward with leadership to work on how we can help you attain those goals\xa0\xa0', 'Work with Product, Engineering or Implementation to build out tools for better data integration', 'Chance to be surrounded by a team of extremely talented and dedicated individuals driven to succeed', 'Working and growing knowledge of new tech stack with less focus on finding efficiency in the technology and greater focus on understanding use of it.', 'Developing ability to understand technical issues and communicate potential solutions to team members or engineering team', 'You will be expected to contribute to multiple implementations simultaneously, which will include both new customer setup as well as support and enhancements for existing customers.', 'Able to identify risk to project success and communicate to leadership', 'Business Domain Knowledge:', 'Developing a range of data pipelines with varying complexity', 'Opportunity to be a part of a mission driven organization focused on helping provider organizations change the way they provide care to their patientsChance to be surrounded by a team of extremely talented and dedicated individuals driven to succeedCompetitive compensationAmazing benefits including unlimited FTO', 'This position is part of the Arcadia Data Engineering team, we are responsible for the onboarding, enhancement, and support of data feed integrations between Client Claim and Clinical data mgmt. platforms and our Healthcare Solution Platform. Our customers are top Healthcare providers and payers, and we help them integrate their internal systems with our analytic platform. The Data Engineering team is responsible for the data architecture that drives the partnership with customers and other internal organizations to drive success through adoption of cutting edge analytic solutions that leverage new age technologies and best practices. Our Data Engineers require both SQL Database knowledge and design , along with multiple programing languages .', 'What Success Looks Like:', 'Able to apply critical thinking and problem solving skills to propose solutions for complex problems within day to day work', 'Pick an SME (Subject Matter Expert) path for what excites you the most', 'Responsible for contributing to the advancement of team processes and internal', 'Work within Data Engineering Scrum team', 'Team Projects:', ""What You'll Bring"", 'Healthcare Analytics: 1-3 years (Preferred)', 'Learn the different areas of the data connector life cycle, while having a working knowledge of the technical stacks , storage platforms , data models , and Dev. CycleWork within Data Engineering Scrum teamSet to work on new ingestion pipelines with full bandwidth available (as formal training will end)', 'Spark: 1-2 years (Preferred)', ""What You'll Be Doing"", 'Working and growing knowledge of new tech stack with less focus on finding efficiency in the technology and greater focus on understanding use of it.Developing ability to understand technical issues and communicate potential solutions to team members or engineering team', 'DATA', ""What You'll Get"", 'Working on standardized data connector development', 'Healthcare Data: 2-4 years (Preferred)', 'NoSQL Databases: 1-2 years (Preferred)', 'Developing a range of data pipelines with varying complexityWork with Product, Engineering or Implementation to build out tools for better data integrationPick an SME (Subject Matter Expert) path for what excites you the mostWorking on standardized data connector development', 'Learn the different areas of the data connector life cycle, while having a working knowledge of the technical stacks , storage platforms , data models , and Dev. Cycle', 'Developing working knowledge of the business of healthcare data and how it interacts within the Arcadia productsUnderstanding of shared value contracts that our customers are in and how data is impacted by themDeveloping knowledge of industry data expected values such as PMPM by LOBs, MM trends, etc.', 'Database Architecture: 2-3 years (Preferred)', 'Developing working knowledge of the business of healthcare data and how it interacts within the Arcadia products', 'Experience Level 2-5 years post-grad with relevant industry experience or graduate level Degree.', '\xa0Technical Domain Knowledge:', 'Responsible for delivery of work on expected timelines.', 'Design and documentation of connectors / ingestion pipelinesBuild and Unit testing of delivery connectors / ingestion pipelinesSupport of our processes in partaking in peer code reviews , sprint planning , product grooming , maintaining Jira tasks and peer test reviewsYou will be expected to contribute to multiple implementations simultaneously, which will include both new customer setup as well as support and enhancements for existing customers.The expectations of the day to day of an engineer is as follows:', 'Developing knowledge of industry data expected values such as PMPM by LOBs, MM trends, etc.', 'Support of our processes in partaking in peer code reviews , sprint planning , product grooming , maintaining Jira tasks and peer test reviews', 'As a Data Engineer, you will drive the successful development of solution architecture and the completion of data pipeline connectors that automate the flow of data between client Claim and Clinical data platforms and our analytic health solution platform. Your efforts will be critical to driving the long-term partnership between Arcadia and our customers.', 'Work on higher level enhancement requests and ingestion pipelines', 'Communication Skills:', 'Ability to Deliver Data related Reviews to clients and other departments regarding code quality and test cases.', 'Responsible for delivery of work on expected timelines.Able to identify risk to project success and communicate to leadershipWorks mostly independently on delivery w/decreasing involvement from engineering and more senior team membersConsistently deliver increasing connectors of increasing quality with ""lessons learned"" incorporated into next projectAble to apply critical thinking and problem solving skills to propose solutions for complex problems within day to day work', 'Build and Unit testing of delivery connectors / ingestion pipelines', 'Cloud Architecture: 1-2 years (Preferred)', 'As a data engineer you will be expected to problem solve some basic coding issues and enhancements with frameworks that are built in Spark Scala, while also leveraging technical skills to partake in idea sessions on process improvement and POC design of how to carry out a solution.\xa0', 'In 6 months', 'SQL: 2-4 year (Preferred)Spark: 1-2 years (Preferred)NoSQL Databases: 1-2 years (Preferred)Database Architecture: 2-3 years (Preferred)Cloud Architecture: 1-2 years (Preferred)', 'Works mostly independently on delivery w/decreasing involvement from engineering and more senior team members', 'Competitive compensation', '\xa0', 'Set to work on new ingestion pipelines with full bandwidth available (as formal training will end)', 'As a data engineer you will be expected to problem solve some basic data analysis issues and work the data to create analytic enhancements.', 'Properly contribute to scrum ceremonies and ceremonies within the dev cycles while successfully updating status and progress in Jira\xa0Work on higher level enhancement requests and ingestion pipelinesAbility to Deliver Data related Reviews to clients and other departments regarding code quality and test cases.Set your own personal vision of development and career aspirations and set a working path forward with leadership to work on how we can help you attain those goals\xa0\xa0', 'Consistently deliver increasing connectors of increasing quality with ""lessons learned"" incorporated into next project', 'In 12 months\xa0', 'As a data engineer you will be expected to problem solve some basic data analysis issues and work the data to create analytic enhancements.Healthcare Data: 2-4 years (Preferred)Healthcare Analytics: 1-3 years (Preferred)', 'In 3 months']",Mid-Senior level,Full-time,Engineering,Information Technology and Services,2021-03-18 14:34:51
Data/Software Engineer,Known Medicine,"Salt Lake City, UT",1 week ago,136 applicants,"['', 'We’re a seed-stage startup backed by Y Combinator, Khosla Ventures, and other phenomenal VCs and Angel investors. You’ll be our first engineering hire, and we’re looking to build out more of our team in the next several months.\xa0', 'Proficiency using AWS', 'Who You’ll Join', '3+ years experience in software engineering, data engineering, or similarBachelor’s degree in CS, Math, or similarProficiency using AWSPreferred: some data science experience', 'Work with our CTO to monitor and troubleshoot analysis of images', 'Your Background', 'Basic python programming of robotics systems', 'Help build out and productionize ML models', 'Our founding team consists of a biomedical engineer and an AI for drug discovery data scientist. We’re building out a fully automated pipeline in our Salt Lake City, UT lab and have research collaborations with several top cancer centers to receive and process patient samples. We offer health insurance, snacks and drinks in the office, lunch provided twice weekly, and flexible vacation/sick leave.\xa0', 'Your Role', 'Preferred: some data science experience', 'You will be the first engineer. Your responsibilities include:', 'Preferred:', 'As a company, Known Medicine is helping oncologists pick the best treatment for cancer patients, faster. Using a patient’s own tumor sample, we use image analysis and Machine Learning to give oncologists insight into how their cells will respond to different treatments.', 'Bachelor’s degree in CS, Math, or similar', '3+ years experience in software engineering, data engineering, or similar', 'Design, implement, and automate image storage and retrieval', 'Design, implement, and automate image storage and retrievalWork with our CTO to monitor and troubleshoot analysis of imagesHelp build out and productionize ML modelsBasic python programming of robotics systems', 'Known Medicine']",Not Applicable,Full-time,Biotechnology,N/A,2021-03-18 14:34:51
AWS Data Engineer,Rivian,"Irvine, CA",2 weeks ago,Be among the first 25 applicants,"['', 'Strong understanding of all Big Data and data warehousing services offered by AWS.', 'Design, implement and support an analytical data infrastructure providing access to large datasets and computing power.', 'Strong hands-on experience in one or more programming languages like Java, Python, Scala', ' Design, implement and support an analytical data infrastructure providing access to large datasets and computing power. Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using API, SQLS, Change Data Capture Tools and AWS big data technologies. Continuous research of the latest big data and visualization technologies to provide new capabilities and increase efficiency Creation and support of real-time data pipelines built on AWS technologies including EMR, Glue, Kinesis, Redshift/Spectrum and Athena Help continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for business. Collaborate with other tech teams to implement advanced analytics algorithms that exploit our rich datasets for statistical analysis, prediction, clustering and machine learning ', 'Degree/Diploma in computer science, engineering, mathematics, or a related technical discipline preferred', 'Creation and support of real-time data pipelines built on AWS technologies including EMR, Glue, Kinesis, Redshift/Spectrum and Athena', 'Hands-on experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets using AWS S3, Glue, Kinesis, Kafka, SQS, Change data capture tools, Spark', 'Self-starter with excellent communication skills', 'Responsibilities', '7-10 years of industry experience in software development, data architecture, data engineering, business intelligence, data science with a track record of manipulating, processing, and extracting value from large datasets', 'Collaborate with other tech teams to implement advanced analytics algorithms that exploit our rich datasets for statistical analysis, prediction, clustering and machine learning', 'Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using API, SQLS, Change Data Capture Tools and AWS big data technologies.', 'Experience in a query framework for business users and data scientists using Athena, APIs and spinning data science clusters.', ' Self-starter with excellent communication skills 7-10 years of industry experience in software development, data architecture, data engineering, business intelligence, data science with a track record of manipulating, processing, and extracting value from large datasets Strong understanding of all Big Data and data warehousing services offered by AWS. Hands-on experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets using AWS S3, Glue, Kinesis, Kafka, SQS, Change data capture tools, Spark Experience in a query framework for business users and data scientists using Athena, APIs and spinning data science clusters. Strong data base experience in both Relational, Columnar, NOSQL & Timeseries database like Redshift, DynamoDB, MongoDB, SQL Server, Druid etc. Strong hands-on experience in one or more programming languages like Java, Python, Scala Demonstrated strength in data modeling, ETL development, and data warehousing Good knowledge of statistical models and data mining algorithms Experience using analytics & reporting tools like Tableau, Power BI, Qlikview etc. Understanding of business domains like Finance, Supply Chain, Manufacturing is a plus Degree/Diploma in computer science, engineering, mathematics, or a related technical discipline preferred ', 'Understanding of business domains like Finance, Supply Chain, Manufacturing is a plus', 'Experience using analytics & reporting tools like Tableau, Power BI, Qlikview etc.', 'Qualifications', 'Strong data base experience in both Relational, Columnar, NOSQL & Timeseries database like Redshift, DynamoDB, MongoDB, SQL Server, Druid etc.', 'Good knowledge of statistical models and data mining algorithms', 'Demonstrated strength in data modeling, ETL development, and data warehousing', 'Help continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for business.', 'Continuous research of the latest big data and visualization technologies to provide new capabilities and increase efficiency']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,CBTS,United States,4 weeks ago,200 applicants,"['', 'Education and Certifications', 'Fully Remote', 'No Supervisory Responsibility', '3 to 5 years of work experience3+ plus years in corporate Data Engineering/Development', 'JOB PURPOSE:', '2-4 years working knowledge of Python, C#, Node.js, Javascript', '3 to 5 years of work experience', 'Education and Certifications:', 'Collaborate with Analytics & Insights team members on agile approaches to achieving project goals - 15%', 'Supervisory Responsibility:', 'Build and maintain data pipelines & ETL for data modeling, mining and analytical processes - 15%', 'Familiarity with scientific & statistical methodologies -- preferred', 'Work Environment:', 'Microsoft SQL Server 2012, 2016, 2019', 'ESSENTIAL FUNCTIONS', 'Familiarity with Agile methodologies', ""Employ a variety of programming languages and tools (i.e. Python, C++, IDE's, etc.) to integrate systems/data packages and infrastructure - 20%Employ data extraction and modeling methodologies to generate consumable information for reporting. - 20%Build and maintain data pipelines & ETL for data modeling, mining and analytical processes - 15%Develop and maintain front end user interfaces for data input/output, optimized for user experience - 15%Performance tuning and documentation of new data assets - 15%Collaborate with Analytics & Insights team members on agile approaches to achieving project goals - 15%"", 'Experience with ETL Tools required (e.g. Talend, Pentaho, SSIS) Required', 'JOB PURPOSE', ""Employ a variety of programming languages and tools (i.e. Python, C++, IDE's, etc.) to integrate systems/data packages and infrastructure - 20%"", 'AWS Data Analytics or Azure Data Engineer -- Preferred', 'PowerBI & Visual Studio Experience Required', ""Four years of College resulting in a Bachelor's Degree or equivalent"", 'Demonstrated proficiency working with APIs, SDKs and Databases', 'Special Knowledge, Skills and Abilities:', 'Demonstrated proficiency working with APIs, SDKs and DatabasesMicrosoft SQL Server 2012, 2016, 2019PowerBI & Visual Studio Experience RequiredMySQL working knowledge2-4 years working knowledge of Python, C#, Node.js, JavascriptExperience with ETL Tools required (e.g. Talend, Pentaho, SSIS) Required2-4 years working knowledge Strong understanding of Relational Data StructuresExperience with non-relational data structures PreferredFamiliarity with Agile methodologiesStrong understanding of APIs and programmatic data extractionFamiliarity with scientific & statistical methodologies -- preferred', 'Work Environment', 'The primary role of the Data Engineer II is to support the extraction & transformation of data objects between systems across the enterprise. The role will focus on creating, delivering and maintaining ETL & Data packages across MS systems that support needs for the entire organization.', 'Special Knowledge, Skills and Abilities', 'MySQL working knowledge', '3+ plus years in corporate Data Engineering/Development', 'Performance tuning and documentation of new data assets - 15%', 'ESSENTIAL FUNCTIONS:', 'Experience', 'Supervisory Responsibility', ""Four years of College resulting in a Bachelor's Degree or equivalentAWS Data Analytics or Azure Data Engineer -- Preferred"", 'Develop and maintain front end user interfaces for data input/output, optimized for user experience - 15%', 'Strong understanding of APIs and programmatic data extraction', 'Employ data extraction and modeling methodologies to generate consumable information for reporting. - 20%', 'Experience:', '2-4 years working knowledge Strong understanding of Relational Data Structures', 'Experience with non-relational data structures Preferred']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
ASSOCIATE DATA SCIENTIST ENGINEER,Gap Inc.,"San Francisco, CA",3 weeks ago,Over 200 applicants,"['', 'Maintain and support deployed solutions and data products.', 'Strong understanding of relational databases and SQL.', ""WHAT YOU'LL DO"", 'See more\xa0of the benefits we offer.', 'BA/BS in a technical or engineering field (Master’s preferred).1-3 years of experience in a data engineering or full-stack data scientist role.Strong understanding of relational databases and SQL.Solid programming foundations and proficiency with data related languages such as Python/Spark/R.Excellent communication skills. Ability to effectively communicate with both technical and non-technical audiences.', 'Navigate various data sources and efficiently locate data in a complex data ecosystem.', 'Gap Inc. is an equal-opportunity employer and is committed to providing a workplace free from harassment and discrimination. We are committed to recruiting, hiring, training and promoting qualified people of all backgrounds, and make all employment decisions without regard to any protected status. We have received numerous awards for our long-held commitment to equality and will continue to foster a diverse and inclusive environment of belonging. This year, we’ve been named as one of the\xa0Best Places to Work by the Humans Rights Campaign\xa0for the fourteenth consecutive year and have been included in the\xa02019 Bloomberg Gender-Equality Index\xa0for the second year in a row.', 'Our brands bridge the gaps we see in the world.\u202fOld Navy democratizes style to ensure everyone\u202fhas access to quality fashion at every price point. Athleta unleashes the potential of every woman,\u202fregardless of body size, age or ethnicity. Banana\u202fRepublic believes in sustainable luxury for all. And Gap\u202finspires the world to bring individuality to modern, responsibly made\u202fessentials.\xa0', 'Develop, deploy, and support analytic data products, such as data marts, ETL’s (extract/transform/load), functions (in Python/SQL/Spark/R), and visualizations.', 'BA/BS in a technical or engineering field (Master’s preferred).', '\ufeff*For eligible employees', 'Work closely with our data scientists to ensure production models are built using a scalable back-end.', 'Partner with internal customers to understand business needs and build strong relationships with key stakeholders.', 'WHO YOU ARE', 'Extensive 401(k) plan with company matching for contributions up to four percent of an employee’s base pay.*', 'ABOUT THE ROLE', 'Employees can take up to five “on the clock” hours each month to volunteer at a charity of their choice.*', 'Merchandise discount for our brands: 50% off regular-priced merchandise at Gap, Banana Republic and Old Navy, 30% off at Outlet and 25% off at Athleta for all employees.One of the most competitive Paid Time Off plans in the industry.*Employees can take up to five “on the clock” hours each month to volunteer at a charity of their choice.*Extensive 401(k) plan with company matching for contributions up to four percent of an employee’s base pay.*Employee stock purchase plan.*Medical, dental, vision and life insurance.*See more\xa0of the benefits we offer.', 'One of the most competitive Paid Time Off plans in the industry.*', 'This simple idea—that we all deserve to belong,\u202fand on our own terms—is core to who we are as a\u202fcompany and how we make decisions.\u202fOur team\xa0is made up of thousands of people across the globe who take risks, think big, and do good for our customers, communities, and the planet.\u202fReady to learn fast, create with audacity\u202fand lead boldly? Join our team.', 'Merchandise discount for our brands: 50% off regular-priced merchandise at Gap, Banana Republic and Old Navy, 30% off at Outlet and 25% off at Athleta for all employees.', 'Medical, dental, vision and life insurance.*', '\xa0', 'Partner with internal customers to understand business needs and build strong relationships with key stakeholders.Develop, deploy, and support analytic data products, such as data marts, ETL’s (extract/transform/load), functions (in Python/SQL/Spark/R), and visualizations.Navigate various data sources and efficiently locate data in a complex data ecosystem.Work closely with our data scientists to ensure production models are built using a scalable back-end.Maintain and support deployed solutions and data products.', 'This position will be part of the Data Science Engineering team at Gap Inc, whose primary goals include building data products and infrastructure that support analytics and data-science at scale. With business users all across the company, the team works cross-functionally to ensure reports, analytics, and models are supported by a stable, efficient, and accurate back-end.', 'BENEFITS AT GAP INC.', 'ABOUT GAP INC.', 'Employee stock purchase plan.*', 'Excellent communication skills. Ability to effectively communicate with both technical and non-technical audiences.', '1-3 years of experience in a data engineering or full-stack data scientist role.', 'Solid programming foundations and proficiency with data related languages such as Python/Spark/R.']",Associate,Full-time,Engineering,Retail,2021-03-18 14:34:51
Data Engineer - Somerville,GNS Healthcare,"Somerville, MA",6 days ago,Be among the first 25 applicants,"['', 'Background in statistics, biostatistics, public health, research design, health economics, or other related quantitative healthcare field. Experience with backend web (API) development, Kubernetes, Docker, and Tableau Experience developing data framesExperience with data processing frameworks such as Apache Spark, Beam, Dataflow, Crunch, Scalding, Storm, Hive and BigQuery Experience extracting data from REST APIs and parallel processing large datasets', 'An expert in cloud data warehousing tools (e.g. Snowflake, Amazon RedShift, BigQuery, Microsoft SQL Server, Oracle, PostgreSQL, or equivalent) and ELT tools (e.g. Stitch, Fivetran, DBT, Glue). You thrive on building modern, cloud-native data pipelines and operations.', '3-5 years of data engineering experience with a thorough understanding of data lake architectures.An expert in cloud data warehousing tools (e.g. Snowflake, Amazon RedShift, BigQuery, Microsoft SQL Server, Oracle, PostgreSQL, or equivalent) and ELT tools (e.g. Stitch, Fivetran, DBT, Glue). You thrive on building modern, cloud-native data pipelines and operations.Fluent in SQL scripting Experience working in healthcare, life sciences, and/or with diverse healthcare data sets (e.g. medical, pharmacy claims, and lab results) Experience with industry standard measures and code sets (e.g. ICD-10 codes, ICD-9 codes, HCPCS codes, CPT codes, HEDIS metrics, ETGs, HCCs, DRGs, etc.) and publicly available sources (e.g. HCUP) You thrive on mapping and designing ingestion and transformation of data from multiple sources, creating a cohesive data asset and Common Data Model (CDM)Experience using a scripting language (Python, R, Java, Scala, C++, C# and Bash/PowerShell) to automate processes and support proprietary software. Machine learning, R, and python skills highly preferred. Experience using Git/Bitbucket and working on shared code repositoriesExperience using Tableau or other in-app data visualization platforms ', 'Equal Employment Opportunity', 'Company Culture', 'Provides complete documentation and communication of all processes, methods, and results. ', 'Experience using Tableau or other in-app data visualization platforms ', 'Assists team members with the design and development of RWD analyses and predictive models. ', 'Responsibilities', 'Background in statistics, biostatistics, public health, research design, health economics, or other related quantitative healthcare field. ', 'Nice To Have Skills', '3-5 years of data engineering experience with a thorough understanding of data lake architectures.', 'Experience with data processing frameworks such as Apache Spark, Beam, Dataflow, Crunch, Scalding, Storm, Hive and BigQuery ', 'Qualifications', 'You thrive on mapping and designing ingestion and transformation of data from multiple sources, creating a cohesive data asset and Common Data Model (CDM)', 'Experience using a scripting language (Python, R, Java, Scala, C++, C# and Bash/PowerShell) to automate processes and support proprietary software. Machine learning, R, and python skills highly preferred. ', 'Experience working in healthcare, life sciences, and/or with diverse healthcare data sets (e.g. medical, pharmacy claims, and lab results) ', 'Experience developing data frames', 'Assists team members with the design and development of RWD analyses and predictive models. Designs and builds consistent, reproducible, and testable ETL pipelines to ingest, normalize, and store data from large healthcare datasets, from clinical trials, to claims, and EHRs.Supports projects including specific epidemiology, health outcomes and other observational studies to better understand disease natural history, prevalence, comorbidities, treatment patterns, and health and safety outcomes in ‘real world’ patient populations. Functions as a healthcare data subject matter expert to support the design, development, testing, implementation, and support of clinical information and intelligence solutions. Provides complete documentation and communication of all processes, methods, and results. Supports production solutions, the ongoing updates, and maintenance of our reference data sources. ', 'Experience extracting data from REST APIs and parallel processing large datasets', 'Functions as a healthcare data subject matter expert to support the design, development, testing, implementation, and support of clinical information and intelligence solutions. ', 'Experience using Git/Bitbucket and working on shared code repositories', 'Supports production solutions, the ongoing updates, and maintenance of our reference data sources. ', 'Experience with industry standard measures and code sets (e.g. ICD-10 codes, ICD-9 codes, HCPCS codes, CPT codes, HEDIS metrics, ETGs, HCCs, DRGs, etc.) and publicly available sources (e.g. HCUP) ', 'Supports projects including specific epidemiology, health outcomes and other observational studies to better understand disease natural history, prevalence, comorbidities, treatment patterns, and health and safety outcomes in ‘real world’ patient populations. ', 'in silico ', 'Designs and builds consistent, reproducible, and testable ETL pipelines to ingest, normalize, and store data from large healthcare datasets, from clinical trials, to claims, and EHRs.', 'Company Overview', 'Fluent in SQL scripting ', 'Experience with backend web (API) development, Kubernetes, Docker, and Tableau ']",Entry level,Full-time,Information Technology,Research,2021-03-18 14:34:51
Data Engineer (Remote),Jun Group,"New York, NY",2 weeks ago,39 applicants,"['', 'Maintain high code quality through code reviews and automated tests', 'You are comfortable writing SQL and manipulating large structured or unstructured datasets for analysis', 'Annual personal development budget to attend a conference of your choice', 'Experiment with new tech to find the right tool for the job', 'Familiarity with AWS or Google Cloud big data products', 'Use Kanban to manage multiple releases per week', 'Collaborate with our engineering team to improve our existing machine learning models and tooling', ' Contribute to exciting greenfield projects Own all things data - including our ETL processes, reporting APIs, and internal dashboards Collaborate with our engineering team to improve our existing machine learning models and tooling Experiment with new tech to find the right tool for the job Use Kanban to manage multiple releases per week Maintain high code quality through code reviews and automated tests ', 'What We Offer', 'Macbook Pros and any other equipment you need to work effectively from home', "" Experience working with streaming and batch data processing tools like Apache Beam, Spark, Flink, etc. You've built and maintained an ETL pipeline using a data warehouse like BigQuery or Redshift You are comfortable writing SQL and manipulating large structured or unstructured datasets for analysis Practical knowledge of how to build efficient end-to-end ML workflows Familiarity with AWS or Google Cloud big data products "", ""You've built and maintained an ETL pipeline using a data warehouse like BigQuery or Redshift"", 'You enjoy a fun, creative, and engaging working atmosphere free of brilliant jerks', 'You want to be part of a small team inside a large company with massive opportunity for growth', 'Own all things data - including our ETL processes, reporting APIs, and internal dashboards', 'Jun Group will only consider candidates for this position who are currently legally authorized to work in the United States.', 'Experience working with streaming and batch data processing tools like Apache Beam, Spark, Flink, etc.', 'A highly competitive compensation package', '401k with company match', 'Qualifications', 'Who You Are', 'You eagerly dig into complex engineering problems', 'Contribute to exciting greenfield projects', 'Designated time to work on company-related projects you feel strongly about', ' A highly competitive compensation package 401k with company match Paid vacation, work from home, and sick days Annual personal development budget to attend a conference of your choice Designated time to work on company-related projects you feel strongly about Macbook Pros and any other equipment you need to work effectively from home Monthly company events  ', 'Practical knowledge of how to build efficient end-to-end ML workflows', 'You enjoy collaboration with other teams including product, biz dev, and our in-house QA team', ""What You'll Do"", ' You enjoy a fun, creative, and engaging working atmosphere free of brilliant jerks You want to be part of a small team inside a large company with massive opportunity for growth You enjoy collaboration with other teams including product, biz dev, and our in-house QA team You eagerly dig into complex engineering problems ', 'Monthly company events ', 'Paid vacation, work from home, and sick days']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Data Engineer,Expedite Technology Solutions LLC,"Tampa, FL",3 days ago,Be among the first 25 applicants,[''],Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Hagerty,"Traverse City, MI",1 week ago,Be among the first 25 applicants,"['', 'This Might Describe You:', 'Functional knowledge of relational databases and query authoring (SQL)', 'To apply for this position please visit our Career site at careers.hagerty.com', 'Strong problem-solving abilities and attention to detail', 'Regular recognition, feedback, and open communication across all levels', 'Team building, bonding, mentorship and support to grow confidence, trust, and friendships', 'What You’ll Do:', 'Ability to authentically and effectively communicate (written and verbally) with various stakeholders', 'Associates degree, preferably in a technical/analytical field, or relevant workAdditional 3+ years working in another role within an IT delivery team, such as a developer, business systems analyst, data analyst, quality assurance analyst, ETL developer or DBAStrong problem-solving abilities and attention to detailAbility to authentically and effectively communicate (written and verbally) with various stakeholdersAbility to create technical artifacts and documentation to support development and maintenance of data productsExperience in successful delivery of data products as productionizable software solutionsExperience or willingness to learn open source data processing technologies such as Kafka, Hadoop, Hive, Presto, Spark, GraphXExperience ensuring rigorous code development, testing, automation, and other engineering best practices.Experience in imperative (e.g., Airflow) or declarative (e.g., Informatica/Talend/Pentaho) ETL design, implementation, and maintenance.Experience cataloging and processing non-relational data.Experience or willingness to learn one or multiple of the following languages Python, Scala, or SQLFunctional knowledge of relational databases and query authoring (SQL)Experience or willingness to learn productionizing data science models in frameworks such as numpy, ML Spark, pandas, scikit-learn, tensorflow, MOA, mlpack, etc.Preferred experience in machine learning techniques such as feature engineering, features selection, supervised and unsupervised algorithms, clustering, graph analytics, and time series analysis, K-means clustering, Gaussian distribution, decision tree, etc.', 'Implement data engineering best practices', 'Additional 3+ years working in another role within an IT delivery team, such as a developer, business systems analyst, data analyst, quality assurance analyst, ETL developer or DBA', 'Experience or willingness to learn productionizing data science models in frameworks such as numpy, ML Spark, pandas, scikit-learn, tensorflow, MOA, mlpack, etc.', ' Data Engineer', 'A culture of company-wide collaboration and shared success', ""Hagerty, an automotive lifestyle company and the world's pre-eminent membership, insurance and media organization for enthusiast vehicle owners, has an opportunity for a Data Engineer to work on our data science team. In this role you will be building and maintaining our data pipeline and scalable analytics platform. You will also be partnering with other technical and business stakeholders to develop and productionize data science models."", 'Ability to create technical artifacts and documentation to support development and maintenance of data products', 'At Hagerty, we’re focused on building a world-class company and culture, and that starts with the people we hire. We take pride in being an equal opportunity, inclusive employer', 'Experience ensuring rigorous code development, testing, automation, and other engineering best practices.', 'A flexible culture that understands the importance of quality of work over quantity', 'Partner with Data Scientists to design, code, train, test, deploy and iterate machine learning algorithms and systems at scale.', 'Experience in successful delivery of data products as productionizable software solutions', 'Experience in imperative (e.g., Airflow) or declarative (e.g., Informatica/Talend/Pentaho) ETL design, implementation, and maintenance.', 'Company supported and employee-driven resource groups that promote diversity, career development and empowerment', 'An opportunity to work with a diverse, global community of 1000+ Hagerty team members across multiple countries, united by our values of open, direct, and kind', 'Create and manage AWS resources using infrastructure-as-code bestpractices, specifically in terraform.', 'Partner with internal and external stakeholders to collect requirements and recommend best practice solutions.', 'Develop solutions to catalog and manage metadata to support data governance and data democratization.', 'This position can be based remotely, or located in our Traverse City, Ann Arbor, Michigan or Dublin, OH offices.', 'Competitive compensationInclusive benefits package allowing enrollment of dependents and partnersA flexible culture that understands the importance of quality of work over quantityAn opportunity to work with a diverse, global community of 1000+ Hagerty team members across multiple countries, united by our values of open, direct, and kindA culture of company-wide collaboration and shared successCompany supported and employee-driven resource groups that promote diversity, career development and empowermentCorporate social responsibility initiatives with global reachRegular recognition, feedback, and open communication across all levelsTeam building, bonding, mentorship and support to grow confidence, trust, and friendshipsAt Hagerty, we’re focused on building a world-class company and culture, and that starts with the people we hire. We take pride in being an equal opportunity, inclusive employer', 'Preferred experience in machine learning techniques such as feature engineering, features selection, supervised and unsupervised algorithms, clustering, graph analytics, and time series analysis, K-means clustering, Gaussian distribution, decision tree, etc.', 'Develop and implement data pipeline orchestration utilities using Apache', 'Support AWS platform DevOps best practices throughout all data engineering', 'Experience or willingness to learn one or multiple of the following languages Python, Scala, or SQL', 'Experience or willingness to learn open source data processing technologies such as Kafka, Hadoop, Hive, Presto, Spark, GraphX', 'Implement data engineering best practicesDevelop and implement robust and scalable data integration (ETL) pipelines using Python, SQL, Spark, and other AWS/Salesforce cloud solutions.Develop and implement data pipeline orchestration utilities using ApacheSupport AWS platform DevOps best practices throughout all data engineeringCreate and manage AWS resources using infrastructure-as-code bestpractices, specifically in terraform.Partner with internal and external stakeholders to collect requirements and recommend best practice solutions.Develop solutions to catalog and manage metadata to support data governance and data democratization.Develop and implement automated test cases and data reconciliation to validate ETL processes and data quality & integrity.Partner with Data Scientists to design, code, train, test, deploy and iterate machine learning algorithms and systems at scale.', 'We Offer:', 'Develop and implement robust and scalable data integration (ETL) pipelines using Python, SQL, Spark, and other AWS/Salesforce cloud solutions.', 'Competitive compensation', 'Experience cataloging and processing non-relational data.', 'Corporate social responsibility initiatives with global reach', 'Inclusive benefits package allowing enrollment of dependents and partners', 'Associates degree, preferably in a technical/analytical field, or relevant work', 'Develop and implement automated test cases and data reconciliation to validate ETL processes and data quality & integrity.']",Mid-Senior level,Full-time,Engineering,Automotive,2021-03-18 14:34:51
SQL Data Engineer,Piper Companies,"Horsham, PA",1 week ago,Be among the first 25 applicants,"['', ' Responsible for moving SSIS packages for SQL server 2016 to 2019 migration.', ' Experience with Boomi would be highly desirable.', ' Experience working with Redshift and/or Snowflake preferred.', ' Experience in a financial services industry preferred.', ' Ability to work both independently as well as a team.', 'Qualifications For The SQL Data Engineer Include', ' Create reconciliation processes to ensure data is loaded into the ODS completely and accurately.', ' Deep knowledge of database structures, normalization, de-normalization and entity relationships.', ' 5+ years’ experience performing data engineering in a ODS environment.', 'Compensation And Benefits', ' Work closely with the data modelers, data analysts, and QA team to ensure that the data is properly loaded into the ODS', ' Write complex SQL queries to move data from various source systems (flat files, SQL Server databases, xls files, JSON files, etc.) into the ODS.', ' Create error handling and logging processes in the ODS', 'Security Clearance', ' Horsham, PA ', ' Experience with SQL Server upgrade would be very helpful.', 'Responsibilities For The SQL Data Engineer Include', 'SQL Data Engineer ', 'Remote during Covid', ' Strong data manipulation and data analysis skills.', ' Implement performance and tuning activities in the ODS', 'Comprehensive benefit package; Medical, Dental, Vision, 401k, PTO', 'Job Category', ' Experience working in an Agile environment.', 'Comprehensive benefit package; Medical, Dental, Vision, 401k, PTORemote during Covid', ' 5+ years’ experience in creating and modifying SSIS packages.', ' Deep knowledge of ODS concepts, data loading issues and best practices.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer (DE - Client Data Engineering) - Data Design & Cura,Goldman Sachs,"Dallas, TX",2 days ago,25 applicants,"['', 'How You Will Fulfill Your Potential', ' Deploy modern data management tools to curate our most important data sets, models and processes, while identifying areas for process automation and further efficiencies', 'Skills And Experience We Are Looking For', ' Working knowledge of more than one programming language (Python, Java, C++, C#, etc.)', ' General knowledge of business processes, data flows and the quantitative models that generate or consume data', ' Financial Services industry experience Experience with the Hadoop eco-system (HDFS, Spark)', ' Experience with the Hadoop eco-system (HDFS, Spark)', ' Engage with data consumers and producers in order to design appropriate models to suit all needs', ' RESPONSIBILITIES AND QUALIFICATIONS ', ' 5+ years of relevant work experience in a team-focused environment', ' Strong work ethic, a sense of ownership and urgency', ' 5+ years of relevant work experience in a team-focused environment A Bachelor’s degree (Masters preferred) in a computational field (Computer Science, Applied Mathematics, Engineering, or in a related quantitative discipline) Working knowledge of more than one programming language (Python, Java, C++, C#, etc.) Extensive knowledge and proven experience applying domain driven design to build complex business applications Deep understanding of multidimensionality of data, data curation and data quality, such as traceability, security, performance latency and correctness across supply and demand processes In-depth knowledge of relational and columnar SQL databases, including database design General knowledge of business processes, data flows and the quantitative models that generate or consume data Excellent communications skills and the ability to work with subject matter expert to extract critical business concepts Independent thinker, willing to engage, challenge or learn Ability to stay commercially focused and to always push for quantifiable commercial impact Strong work ethic, a sense of ownership and urgency Strong analytical and problem solving skills Ability to collaborate effectively across global teams and communicate complex ideas in a simple manner', ' Independent thinker, willing to engage, challenge or learn', ' Deploy modern data management tools to curate our most important data sets, models and processes, while identifying areas for process automation and further efficiencies Evaluate, select and acquire new internal & external data sets that contribute to business decision making Engineer streaming data processing pipelines Drive adoption of Cloud technology for data processing and warehousing Engage with data consumers and producers in order to design appropriate models to suit all needs', ' In-depth knowledge of relational and columnar SQL databases, including database design', ' Extensive knowledge and proven experience applying domain driven design to build complex business applications', ' Deep understanding of multidimensionality of data, data curation and data quality, such as traceability, security, performance latency and correctness across supply and demand processes', 'About Goldman Sachs', ' Ability to collaborate effectively across global teams and communicate complex ideas in a simple manner', ' Ability to stay commercially focused and to always push for quantifiable commercial impact', ' Drive adoption of Cloud technology for data processing and warehousing', ' Excellent communications skills and the ability to work with subject matter expert to extract critical business concepts', ' Evaluate, select and acquire new internal & external data sets that contribute to business decision making', ' Strong analytical and problem solving skills', ' Engineer streaming data processing pipelines', ' Financial Services industry experience', 'Preferred Qualifications', ' A Bachelor’s degree (Masters preferred) in a computational field (Computer Science, Applied Mathematics, Engineering, or in a related quantitative discipline)']",Executive,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer Lead,New Resources Consulting,Greater Madison Area,1 week ago,Be among the first 25 applicants,"['', 'Preferred: Bachelors or better in Computer Science or related field.', 'Behaviors', 'Demonstrated Technology Leadership', 'Preferred', 'Minimum 8-12 years working in an IT department.', 'Team Player:\xa0Works well as a member of a group', 'Blend of business (functional), technical (IT), and project management/agile skills.', 'Required', 'Education', 'Strong Analytical skills with the ability to identify and solve business problems.', 'Leader:\xa0Inspires teammates to follow them', 'Mastery of T-SQL and ELT/ETL methodologies with familiarity of SnowFlake & Talend.', 'Ability to work collaboratively within a team environment.', 'Excellent verbal, written, and interpersonal communication skills.', 'Team Player:\xa0Works well as a member of a groupLeader:\xa0Inspires teammates to follow them', 'Experience', 'This position will lead technical personnel, interact directly with business leadership, and lead project initiatives, as such, strong leadership, interpersonal, and communication skills are required.\xa0', 'Required: Bachelors or better in Information Technology or related field.', ""The Data Engineer Lead – performs all the activities involved in managing, creating, and enhancing our next generation data and analytics platform capabilities.\xa0They work under the direction of an IT Manager to help ensure the data related applications work together to support the business process requirements.\xa0They will review, analyze, and modify systems including configuration, testing, debugging and installing to support an organization's data platform systems.\xa0A strong understanding of cloud architecture, ETL/ELT, data management, and data modeling must be possessed with demonstrated experience partnering with business leaders to deliver enterprise data solutions.\xa0"", 'Ability to work collaboratively within a team environment.Demonstrated Technology LeadershipMastery of T-SQL and ELT/ETL methodologies with familiarity of SnowFlake & Talend.Excellent verbal, written, and interpersonal communication skills.Strong Analytical skills with the ability to identify and solve business problems.Blend of business (functional), technical (IT), and project management/agile skills.Minimum 8-12 years working in an IT department.']",Mid-Senior level,Full-time,Information Technology,Consumer Goods,2021-03-18 14:34:51
Data Engineer,RevolutionParts,"Phoenix, AZ",2 weeks ago,51 applicants,"['', 'Fluency with a scripting language - we use Python and PHP heavily ', '3+ years experience as a data engineerAbility to own data problems and help to shape the solution for business challengesAbility to work with others and know when to support and when to pushGood communication and collaboration skills; comfortable discussing projects with anyone from end users up to the executive company leadershipFluency with a scripting language - we use Python and PHP heavily Ability to write and optimize complex SQL statementsFamiliarity with ETL pipeline tools such as Airflow or AWS GlueFamiliarity with data visualization and reporting tools, especially TableauExperience working in a cloud-based software development environment, preferably with AWSFamiliarity with no-SQL databases such as ElasticSearch, DynamoDB or MongoDB', 'Support technical and business stakeholders by providing key reports and supporting the BI team to become fully self-service', 'Understand our data sources, ETL logic, and data schemas and help craft tools for managing the full data lifecycle', 'Familiarity with data visualization and reporting tools, especially Tableau', 'Support and optimize existing ETL pipelines', 'Understand our data sources, ETL logic, and data schemas and help craft tools for managing the full data lifecyclePlay a key role in building the next generation of our data ingestion pipeline and data warehouseRun ad hoc analysis of our data to answer questions and help prototype solutionsSupport and optimize existing ETL pipelinesSupport technical and business stakeholders by providing key reports and supporting the BI team to become fully self-serviceOwn problems through to completion both individually and as part of a data teamSupport product engineering teams by performing query analysis and optimization', 'Ability to work with others and know when to support and when to push', 'Ability to own data problems and help to shape the solution for business challenges', 'Good communication and collaboration skills; comfortable discussing projects with anyone from end users up to the executive company leadership', 'Familiarity with no-SQL databases such as ElasticSearch, DynamoDB or MongoDB', 'Familiarity with ETL pipeline tools such as Airflow or AWS Glue', 'Own problems through to completion both individually and as part of a data teamSupport product engineering teams by performing query analysis and optimization', 'Play a key role in building the next generation of our data ingestion pipeline and data warehouse', '3+ years experience as a data engineer', 'Run ad hoc analysis of our data to answer questions and help prototype solutions', 'Experience working in a cloud-based software development environment, preferably with AWS', 'Ability to write and optimize complex SQL statements', ' Responsibilities ']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Data Ops Engineer,"Centauri Health Solutions, Inc",United States,6 days ago,Be among the first 25 applicants,"['', 'Build a collaborative and supportive working environment', 'Centauri Health Solutions is a healthcare technology and services company powered by analytics. Our tailored solutions enable health plans and hospitals to manage variable revenue, through a custom-built workflow platform, which seamlessly integrates cross-functional service and support. We offer Risk Adjustment, a variety of Eligibility and Enrollment Services to meet the needs of specialized populations, Out-of-State Billing and Quality program efforts.', 'Assist with coding of medical records as needed', 'Monitor daily coding quality specialist productivity and accuracy', 'Strong written and verbal communication skills', 'Ability to work independently in a remote environment', ' ', 'Assist Coding Services Supervisor with billing and invoice approvals and reconciliations for assigned coders where applicable', 'The Team Leader, Coding Quality remotely supervises a team of high-performing Coding Quality Specialists and assists the Coding Quality Manager with daily activities to support Coding Services Operations. The Team Leader, Coding Quality is accountable for monitoring and maintaining daily schedules, trackers, and staff productivity, and is charged with building strong working relationships across the team. Additionally, The Team Leader, Coding Quality collaborates in creating training materials and documentation for Centauri’s software modules and assists in coding of medical records as needed.', '1+ year of experience in a similar leadership role', 'Role Responsibilities:', 'High School Diploma', 'Ensure safety and confidentiality of data and systems by adhering to the organizations information security policies', 'Role Overview:', '1+ year of experience in a similar leadership role3+ years recent coding experience with at least 1 of those years in Risk Adjustment coding\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Current core coding credential through AHIMA or AAPC (RHIT, CCS, CCS-P, CPC, CIC, etc.) or AAPC CRC (Certified Risk Adjustment Coder) coding certification\xa0\xa0Strong Project Management competencies and experienceRisk Adjustment experienceStrong organizational skillsTechnical savvy with high level of competence in Microsoft products along with intermediate Excel skillsStrong written and verbal communication skillsAbility to work independently in a remote environmentHigh School DiplomaCompletion of an accredited medical coding program is highly desired, but not required', 'Strong organizational skills', 'Data Ops Engineer', 'Handle other related duties as required or assigned', 'Current core coding credential through AHIMA or AAPC (RHIT, CCS, CCS-P, CPC, CIC, etc.) or AAPC CRC (Certified Risk Adjustment Coder) coding certification\xa0\xa0', 'Supervise a team of up to 12 remote coding quality specialists for assigned coding projects and campaignsBuild a collaborative and supportive working environmentMonitor and maintain assigned coding quality specialists’ daily trackers and weekly schedules including PTO approvals and payroll related activitiesMonitor daily coding quality specialist productivity and accuracyAssist Coding Services Supervisor with billing and invoice approvals and reconciliations for assigned coders where applicableAssist Coding Quality Manager in completing and providing weekly coder report cardsAssist in providing feedback and education to assigned coders based upon quality reviewAssist with coding of medical records as neededHandle other related duties as required or assignedUnderstand and agree to role-specific information security access and responsibilitiesEnsure safety and confidentiality of data and systems by adhering to the organizations information security policiesRead, understand and agree to security policies and complete all annual security and compliance training', '\xa0\xa0\xa0', 'About Centauri Health Solutions:', 'Centauri Health Solutions is an equal opportunity employer.', 'Supervise a team of up to 12 remote coding quality specialists for assigned coding projects and campaigns', 'Read, understand and agree to security policies and complete all annual security and compliance training', 'Risk Adjustment experience', 'Completion of an accredited medical coding program is highly desired, but not required', 'Technical savvy with high level of competence in Microsoft products along with intermediate Excel skills', 'Assist Coding Quality Manager in completing and providing weekly coder report cards', 'Monitor and maintain assigned coding quality specialists’ daily trackers and weekly schedules including PTO approvals and payroll related activities', '\xa0', 'Understand and agree to role-specific information security access and responsibilities', 'We believe strongly in providing employees a rewarding work environment in which to grow, excel and achieve personal as well as professional goals. We offer our employees competitive compensation and a comprehensive benefits package that includes generous paid time off, a matching 401(k) program, tuition reimbursement, annual salary reviews, a comprehensive health plan, the opportunity to participate in volunteer activities on company time, and development opportunities.', 'Strong Project Management competencies and experience', 'Role Requirements: ', '3+ years recent coding experience with at least 1 of those years in Risk Adjustment coding\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0', 'Assist in providing feedback and education to assigned coders based upon quality review']",Mid-Senior level,Full-time,Information Technology,Hospital & Health Care,2021-03-18 14:34:51
Data Engineer ,Civitas Learning,"Austin, TX",3 weeks ago,185 applicants,"['', '\ufeffWhat you are looking for in your career:', 'Write SQL queries to aggregate and transform data into our canonical data model', 'Amazon Web ServicesHigher Education Student Information Systems or Learning Management Systems (Ellucian Banner, PeopleSoft, Blackboard, Canvas, etc.)JIRA or other ticketing systemsPythonWhile we are an open source shop, some of our customers use Microsoft solutions: SSIS, SSRS, Azure etc...', 'For more information, visit:', 'What you will do as a Data Engineer:', 'Skills using SQL to aggregate and transform data. Any flavor but Postgres / Redshift preferred.\xa0', 'www.civitaslearning.com. Interested in learning more about our Culture & Mission? Check out \u200b#civmission\u200b on \u200bLinkedIn\u200b. Civitas offers a comprehensive benefits package including medical, dental, vision, disability insurance, onsite paid parking, 401-K Program and a flexible paid time off policy. Civitas Learning is an equal opportunity employer, and we know diversity makes us stronger. We do not discriminate on the basis of race, religion, national origin, gender, sexual orientation, age, marital status, veteran status, or disability.', 'Python', 'Experience withmany SQL techniques including all types of joins, aggregate and window functions, CTE etc.', 'While we are an open source shop, some of our customers use Microsoft solutions: SSIS, SSRS, Azure etc...', 'Comfortable on the command line and write or understand basic scripts', '2-4 years of experience - Data Engineer', 'What we are looking for in a Data Engineer:', 'About Civitas Learning:', 'Train and operationalize predictive models', 'Continuously improve our data platform mappings', 'At Civitas Learning, the Data Engineer acts as the technical lead on projects to integrate customers with our platform and applications. In this role, you will develop SQL logic using our internally developed Python tool to map customer data into our platform. You’ll work closely with customer-facing teams, such as the Project Management or Training & Enablement teams, to guide users through the integration process. You will also collaborate closely with product and engineering teams. In short, you will work side by side with a diverse range of people dedicated to our customers.', '4-6 years of experience - Sr. Data Engineer', 'The chance to use your skills to do something that truly matters.\xa0', 'Amazon Web Services', '6+ years of experience - Principal Data Engineer', 'Guide customers through platform requirements and code validation exercises', '2-4 years of experience - Data Engineer4-6 years of experience - Sr. Data Engineer6+ years of experience - Principal Data EngineerSkills using SQL to aggregate and transform data. Any flavor but Postgres / Redshift preferred.\xa0Experience withmany SQL techniques including all types of joins, aggregate and window functions, CTE etc.Comfortable on the command line and write or understand basic scriptsExperience using Git and working with code collaborativelyEffective communicator with different levels of technical audiencesWork, life, educational, or other experience with diverse groups of people', 'An office full of talented, mission-driven people who are committed to inclusion and strengthened by diversityThe chance to use your skills to do something that truly matters.\xa0', 'Experience using Git and working with code collaboratively', 'Higher Education Student Information Systems or Learning Management Systems (Ellucian Banner, PeopleSoft, Blackboard, Canvas, etc.)', 'Data Engineer', 'Civitas Learning is helping colleges and universities all over the world to make the most of their data by building custom data pipelines, patented data science, and SaaS applications to improve student outcomes. If you love working in fast-paced environments that require learning, critical thinking, and strong communication skills, then we’d love to talk.', 'Work, life, educational, or other experience with diverse groups of people', 'Civitas Learning helps colleges and universities harness the power of their student data to improve student success outcomes. We embed actionable intelligence in workflow tools so higher education can focus their student success strategies, deliver proactive care, inspire holistic advising, and quickly measure what’s working for whom. With our platform, software and services, our customers empower leaders, advisors, faculty, & students to measurably improve enrollment, persistence, and graduation outcomes. Today, we work with 375 colleges and universities, serving nearly 8 million students. Together with our growing community of customers, we are making the most of the world’s learning data to help graduate a million more students per year by 2025.', '\xa0', 'JIRA or other ticketing systems', 'An office full of talented, mission-driven people who are committed to inclusion and strengthened by diversity', 'Write SQL queries to aggregate and transform data into our canonical data modelGuide customers through platform requirements and code validation exercisesTrain and operationalize predictive modelsContinuously improve our data platform mappings', 'Expertise in some of the following is valued but not required:', 'Effective communicator with different levels of technical audiences']",Mid-Senior level,Full-time,Engineering,Computer Software,2021-03-18 14:34:51
Data Engineer - NYC / DC,Beyond Identity,"New York, NY",1 month ago,Be among the first 25 applicants,"['', 'Lead data architecture design and implementation', 'Lead data architecture design and implementationBuild and deliver high quality data architecture and pipelines to support business analysts, data scientists, and customer reporting needs.Interface with other technology teams to extract, transform, and load data from a wide variety of data sources.Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers.', 'Demonstrate the ability to optimize processes', 'Advanced working SQL knowledge and experience working with relational databases', 'Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers.', 'Interface with other technology teams to extract, transform, and load data from a wide variety of data sources.', 'Knowledge of data integrity and relational rules', '5+ years of experience as a Data Engineer or in a similar roleAdvanced working SQL knowledge and experience working with relational databasesBuild processes supporting data transformation, data structures, metadata, dependency, and workload managementExperience with Spark or another distributed cluster-computing frameworkExperience with Kafka and AWSShow proficiency understanding complex ETL processesDemonstrate the ability to optimize processesKnowledge of data integrity and relational rules', 'Job Description', 'Show proficiency understanding complex ETL processes', '5+ years of experience as a Data Engineer or in a similar role', 'Experience with Spark or another distributed cluster-computing framework', 'Experience with Kafka and AWS', 'Build processes supporting data transformation, data structures, metadata, dependency, and workload management', 'Build and deliver high quality data architecture and pipelines to support business analysts, data scientists, and customer reporting needs.', 'Required Qualifications']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Associate Data Engineer,State Auto Insurance,"Columbus, OH",1 week ago,Be among the first 25 applicants,"['', ' Manage the source code in GitHub ', ' Expertise in using IDEs and Tools like Eclipse, GitHub, Jenkins, Maven and IntelliJ ', ' Proficient in executing Hive queries using Hive cli, Web GUI Hue and Impala to read, write and query the data ', ""We're committed to bringing passion and customer focus to the business."", ' Develop scheduling and monitoring Oozie workflows for parallel execution of jobs ', ' Strong hands-on experience in Spark, Scala, R, Python, and/or Java. ', ' Expertise in various scripting languages like Linux/Unix shell scripts and Python ', ' Worker Sub-Type ', ""It's fun to work in a company where people truly BELIEVE in what they're doing!"", ' Bachelor’s Degree/Master degree in Computer Science, Computer Engineering, Programming, Management Information Systems, or related field. Insurance industry experience is a plus.  Minimum of 2 years of prior Data engineer experience.  Strong hands-on experience in Spark, Scala, R, Python, and/or Java.  Programming experience with the Hadoop ecosystem of applications and functional understanding of distributed data processing systems architecture (Data Lake / Big Data /Hadoop/ Spark / HIVE, etc).  Amazon Big Data ecosystem (EMR, Kinesis, Aurora) experience is a plus. ', ' Harmonize, transform, and move data from a raw format to consumable and curated views ', ' Minimum of 2 years of prior Data engineer experience. ', ' Expert in Spark SQL and Spark DataFrames using Scala for Distributed Data Processing ', ' Experience in working with cloud environment AWS EMR, EC2, S3 and Athena and GCP BigQuery ', ' Programming experience with the Hadoop ecosystem of applications and functional understanding of distributed data processing systems architecture (Data Lake / Big Data /Hadoop/ Spark / HIVE, etc). ', 'Worker Sub-Type', ' Diverse experience in working with variety of Database like SQL Server, MySql, IBM DB2 etc… ', ' Model, design, develop, code, test, debug, document and deploy application to production through standard processes ', ' Bachelor’s Degree/Master degree in Computer Science, Computer Engineering, Programming, Management Information Systems, or related field. Insurance industry experience is a plus. ', ' Develop DataFrame and RDD (Resilient Distributed Datasets) to achieve unified transformations on the data load ', ' Build distributed, scalable, and reliable data pipelines that ingest and process data at scale and in real-time ', 'Full Time', 'Communication And Collaboration Skills', ' Experience in working on Apache Hadoop ecosystem components like Map-Reduce, Hive, SQOOP, Spark, and Oozie with AWS EC2 cloud computing ', ' Optimize the Spark application to improve performance and reduced time on the Hadoop cluster ', 'Written', ' Track and delivery requirements in Jira ', ' Create metrics and apply business logic using Spark, Scala, R, Python, and/or Java ', ' Amazon Big Data ecosystem (EMR, Kinesis, Aurora) experience is a plus. ', 'Problem Solving:', 'Summary & Key Responsibilities', 'Oral:', 'Full Time / Part Time', 'Key Responsibilities', ' Apply all Phases of Software Development Life Cycle (Analysis, Design, Development, Testing and Maintenance) using Waterfall and Agile methodologies ', ' / Part Time', ' Transfer data from different platform’s into AWS platform ', ""If you like wild growth and working with happy, enthusiastic over-achievers, you'll enjoy your career with us!"", ' Apply strong Data Governance principles, standards, and frameworks to promote data consistency and quality while effectively managing and protecting the integrity of corporate data ', ' Apply all Phases of Software Development Life Cycle (Analysis, Design, Development, Testing and Maintenance) using Waterfall and Agile methodologies  Experience in working on Apache Hadoop ecosystem components like Map-Reduce, Hive, SQOOP, Spark, and Oozie with AWS EC2 cloud computing  Expert in Spark SQL and Spark DataFrames using Scala for Distributed Data Processing  Develop DataFrame and RDD (Resilient Distributed Datasets) to achieve unified transformations on the data load  Expertise in various scripting languages like Linux/Unix shell scripts and Python  Develop scheduling and monitoring Oozie workflows for parallel execution of jobs  Experience in working with cloud environment AWS EMR, EC2, S3 and Athena and GCP BigQuery  Transfer data from different platform’s into AWS platform  Diverse experience in working with variety of Database like SQL Server, MySql, IBM DB2 etc…  Manage the source code in GitHub  Track and delivery requirements in Jira  Expertise in using IDEs and Tools like Eclipse, GitHub, Jenkins, Maven and IntelliJ  Optimize the Spark application to improve performance and reduced time on the Hadoop cluster  Proficient in executing Hive queries using Hive cli, Web GUI Hue and Impala to read, write and query the data  Build distributed, scalable, and reliable data pipelines that ingest and process data at scale and in real-time  Create metrics and apply business logic using Spark, Scala, R, Python, and/or Java  Model, design, develop, code, test, debug, document and deploy application to production through standard processes  Harmonize, transform, and move data from a raw format to consumable and curated views  Apply strong Data Governance principles, standards, and frameworks to promote data consistency and quality while effectively managing and protecting the integrity of corporate data ', 'Minimum Experience/Education']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Senior Data Engineer,"Mercury Systems, Inc.","Seattle, WA",1 day ago,Be among the first 25 applicants,"['', 'Must Have', 'Experience in handling semi-structured data (JSON, XML) using the VARIANT attribute in Snowflake Experience in in Re-clustering of the data in Snowflake with good understanding on how Micro-Partition works inside Snowflake is a plus', 'Experience working directly with business users and solving business questions with data', 'Data migration', 'At least 10+ years of hands-on experience in Development/Data Integration/Data Modeling/Data migration', 'Sound experience in Python scripting to read & load data from', 'data migration', 'Matillion', 'Develop and own ETL Data Pipeline and Data Model solutions for integrating new data sets into theSnowflake Enterprise Warehouse from different sources.', 'Experience in Netsuite/Zuora is a huge plus', 'AWS - S3, Glacier, EC2, Lambda, SQS, Redshift', 'In-depth understanding of database management systems and advantages & disadvantages of different types of systems (OLAP, OLTP, etc.).', 'Develop complex SQL queries, scripts, user defined functions, views, and triggers for business logic implementation inSnowflake Enterprise Data Warehouse', 'Nice to Have', 'Python', 'Very good understanding of the Snowflake architecture', 'Strong experience with Data warehousing methodologies and modelling techniques', 'Snowflake Warehouse ', 'Oracle Financial', 'Data Integration', 'Perform data mapping data between source systems and DW', 'Strong experience with ELT concepts & tools such as Talend, Matillion', ' AWS - S3, Glacier, EC2, Lambda, SQS, Redshift Azure - Blob Storage, Cool Blob Storage, Virtual Machine, Functions, SQL Datawarehouse Own, monitor, and improve automated solutions to ensure quality and performance SLAs are met Experience working directly with business users and solving business questions with data In-depth understanding of database management systems and advantages & disadvantages of different types of systems (OLAP, OLTP, etc.). Strong experience with Data warehousing methodologies and modelling techniques Sound experience in working with Massively Parallel Processing (MPP) Analytical Datastores such as Netezza, Teradata Very good understanding of the Snowflake architecture Experience in handling semi-structured data (JSON, XML) using the VARIANT attribute in Snowflake Experience in in Re-clustering of the data in Snowflake with good understanding on how Micro-Partition works inside Snowflake is a plus Experience in working with Batch and Stream data ', 'Snowflake Enterprise Data Warehouse', 'Good experience in data migration from various systems to Snowflake warehouse', 'Sound experience in working with Massively Parallel Processing (MPP) Analytical Datastores such as Netezza, Teradata', 'Create scripts for data warehouse testing ranging from unit to integration testing', ' Prior experience in Oracle Financial data is a plus Experience in Netsuite/Zuora is a huge plus Advanced data visualization skills using any technology is a plus Experience in Hadoop, Hive, HBASE, Spark is a plus', ' At least 10+ years of hands-on experience in Development/Data Integration/Data Modeling/Data migration Experience in design or development of enterprise data solutions, applications, and integrations in Snowflake Warehouse environment Design and develop both physical and logical data models along with define metadata standards for the DW Develop complex SQL queries, scripts, user defined functions, views, and triggers for business logic implementation inSnowflake Enterprise Data Warehouse Develop and own ETL Data Pipeline and Data Model solutions for integrating new data sets into theSnowflake Enterprise Warehouse from different sources. Perform data mapping data between source systems and DW Develop & execute ETL procedures/pipelines as well monitor and tune data loads and queries Prepare technical documentation including metadata and diagrams of entity relationships, process flows and others Create scripts for data warehouse testing ranging from unit to integration testing Good experience in data migration from various systems to Snowflake warehouse Strong experience with ELT concepts & tools such as Talend, Matillion Sound experience in Python scripting to read & load data from ', 'Experience in design or development of enterprise data solutions, applications, and integrations in Snowflake Warehouse environment', 'Experience in Hadoop, Hive, HBASE, Spark is a plus', 'Own, monitor, and improve automated solutions to ensure quality and performance SLAs are met', 'Azure', 'Azure - Blob Storage, Cool Blob Storage, Virtual Machine, Functions, SQL Datawarehouse', 'Primary Skill : AWS, Snowflake', 'Experience in working with Batch and Stream data', 'Netsuite/Zuora', 'Develop & execute ETL procedures/pipelines as well monitor and tune data loads and queries', 'Design and develop both physical and logical data models along with define metadata standards for the DW', 'Prepare technical documentation including metadata and diagrams of entity relationships, process flows and others', 'AWS - S3', 'Snowflake Enterprise Warehouse ', 'Advanced data visualization skills using any technology is a plus', 'Prior experience in Oracle Financial data is a plus']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Senior Data Engineer,Remind,"Las Vegas, NM",3 weeks ago,Be among the first 25 applicants,"['', 'About This Role', ' You have built scalable, performant, highly available services and understand the value of a good SLA. ', 'By submitting your interest in this job, you agree to receive text notifications with additional steps to complete your job application. You will receive up to 6 messages from the number ""63879"". Message & data rates may apply. Please refer to our privacy policy for more information.', ' Define and own organization-level data architecture for a trusted, governed, dimensionally-modeled repository of data that enables the Remind team to quickly and reliably answer their questions. ', ' Competitive salary and equity ', ' Open vacation policy ', ' Partner closely with the Head of Data and the Head of Technology to define the vision and roadmap for data architecture at Remind. ', ' Delight data consumers (internal and external) by ensuring they have the data they need to inform decisions, where and when they need it. ', ' You are fast and rigorous analytically, with a degree in engineering, math, statistics, analytics, or relevant alternative education (adult learning, on the job experience etc). ', ' 100% health coverage for you and your dependents ', ' Help democratize data at Remind: create data exploration processes and promote adoption of data sources across the company. ', ' You are proficient in Python and familiar with at least one other programming language. ', ' Competitive salary and equity  401K  100% health coverage for you and your dependents  Open vacation policy  Paid parental leave ', ' You are an expert in building complex, multistage data-pipelines with heterogeneous data sources using Spark and Redshiftwith demonstrated expertise in optimizing performance. ', ' You have expert level SQL skills, but more importantly you know how to design performant and understandable datasets for the whole Remind team. ', 'Compensation', ' Protect data sources against data quality issues: work with data producers to ensure data passes acceptance tests; design, develop and maintain data quality monitoring and assurance framework; and continuously improve the processes for developing new ones, raising the level of quality expected from our work. ', ' You are a curious and communicative personyou seek to understand before building and therefore more often build the right thing first. ', ' Delight data consumers (internal and external) by ensuring they have the data they need to inform decisions, where and when they need it.  Define and own organization-level data architecture for a trusted, governed, dimensionally-modeled repository of data that enables the Remind team to quickly and reliably answer their questions.  Partner closely with the Head of Data and the Head of Technology to define the vision and roadmap for data architecture at Remind.  Protect data sources against data quality issues: work with data producers to ensure data passes acceptance tests; design, develop and maintain data quality monitoring and assurance framework; and continuously improve the processes for developing new ones, raising the level of quality expected from our work.  Help democratize data at Remind: create data exploration processes and promote adoption of data sources across the company. ', ""What You'll Do"", 'About The Company', ' You are an expert in building complex, multistage data-pipelines with heterogeneous data sources using Spark and Redshiftwith demonstrated expertise in optimizing performance.  You have built scalable, performant, highly available services and understand the value of a good SLA.  You are proficient in Python and familiar with at least one other programming language.  You have expert level SQL skills, but more importantly you know how to design performant and understandable datasets for the whole Remind team.  You are a curious and communicative personyou seek to understand before building and therefore more often build the right thing first. ', 'About You', ' 401K ', ' Paid parental leave ']",Associate,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Engineer,Cricut,"San Francisco, CA",4 weeks ago,112 applicants,"['', 'CS or CE degree or commensurate experience required', 'Hands-on expertise in one or more Amazon Web Services (AWS) technologies, such as Kinesis, S3, Redshift, Athena, Lambda', 'Optimize data flow and data collection for cross functional teams', 'Hands-on expertise in one or more Amazon Web Services (AWS) technologies, such as Kinesis, S3, Redshift, Athena, LambdaExperience in Kanban methodologiesExperience in Test Driven Development and CI/CD.Demonstrated ability to develop and support large-sized international-scale software systemsExperience in expanding and optimizing data pipeline architectureOptimize data flow and data collection for cross functional teams', 'Experience in Kanban methodologies', 'Strong understanding of scalability, performance, and reliability', 'Experience in expanding and optimizing data pipeline architecture', 'ADA: If you require reasonable accommodation during the application or selection process please do not hesitate to reach out to Cricut HR or your assigned recruiter.', 'CS or CE degree or commensurate experience requiredMS SQL Server, MySQL (Aurora), MSSQL, REST API, etc.Strong understanding of scalability, performance, and reliabilityExperience with OOP frameworks, languages, design patterns, concepts and data sources such as C#/.NET, Java, Python, Kafka, SparkAbility to work on multiple areas including data pipeline ETL, data modeling & design, writing complex SQL queries, etc.', 'Equal Opportunity: Cricut is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees and members. Applicants will be considered based on their qualifications and without regards to age, race, ethnicity, gender identity or expression, national origin, religion, physical or mental disability, protected veteran states, sex (including pregnancy), sexual orientation or any other protected characteristic protected by applicable laws, regulations or ordinances. ADA: If you require reasonable accommodation during the application or selection process please do not hesitate to reach out to Cricut HR or your assigned recruiter.', 'Experience in Test Driven Development and CI/CD.', 'Qualifications', 'Preferred Skills', 'MS SQL Server, MySQL (Aurora), MSSQL, REST API, etc.', 'Ability to work on multiple areas including data pipeline ETL, data modeling & design, writing complex SQL queries, etc.', 'Experience with OOP frameworks, languages, design patterns, concepts and data sources such as C#/.NET, Java, Python, Kafka, Spark', 'Equal Opportunity: Cricut is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees and members. Applicants will be considered based on their qualifications and without regards to age, race, ethnicity, gender identity or expression, national origin, religion, physical or mental disability, protected veteran states, sex (including pregnancy), sexual orientation or any other protected characteristic protected by applicable laws, regulations or ordinances. ', 'Job Description', 'Demonstrated ability to develop and support large-sized international-scale software systems']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
"Data Engineer, Business Intelligence",Winnebago Industries,"Eden Prairie, MN",7 days ago,Be among the first 25 applicants,"['', 'Preferred Qualifications:', 'Bachelor’s degree in Business, Information Technology or related field', 'Identify and escalate issues and risks early to minimize impact to business', 'Critical Competencies:', 'Translate business objectives into requirements to facilitate information solutions', 'An understanding of Big Data platforms', 'Be part of a team that treats each other with the respect, trust and humility needed to create extraordinary outdoor experiences for our customers.', 'Key Areas of Responsibility:', '6+ years of IT experience', 'As our Data Engineer you will support our business partners in sales, operations, marketing, finance, and others by using a variety of internal and external information sources and solve business needs. You will need to bring the right blend of analytical expertise, SQL skills, Microsoft BI experience, and communication acumen.', 'Strong analytical skills focusing on market and operational data analyticsStrong results orientation.\xa0Ability to lead and motivate a team to deliver to expectations\xa0Ability to communicate and interact with business partners and collaboratively define solutions to address key opportunitiesAbility to take initiative and work independently with minimal supervision\xa0Demonstrate the ability to serve as a strong team member with solid communication skills and the ability to handle multiple priorities in a fast-paced, growth environment\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0', 'Ability to communicate and interact with business partners and collaboratively define solutions to address key opportunities', 'Experience with Machine Learning', 'Ensure solutions are architected to support proper data capture and processing', '\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0', 'Bachelor’s degree in Business, Information Technology or related field6+ years of IT experience2+ years experience with Dimensional Modeling & Cloud DatabasesExperience with Visualization tools (Power BI, SSRS, R, Python, other analytics solutions)', 'Data Integration experience including moving data efficiently from On-Premise to the Cloud, leveraging tools like Informatica Cloud Services, SSIS, or Azure Data Factory', 'Experience with Visualization tools (Power BI, SSRS, R, Python, other analytics solutions)', 'We’re looking for people who are as passionate about their work as they are about the outdoors. We seek fresh minds and skilled hands that can bring new perspectives, ways of working and technology to deliver exceptional experiences. We do this best when we allow ourselves the time to enjoy the environment — so we seek to keep a healthy balance between work and play.', 'Winnebago is looking for individuals to join our Business Intelligence group. We help uncover tomorrow’s opportunities and create actionable analytics, comprehensive views of the data, and thoughtful visualizations to enable business partners take action.', 'Required Education and Experience:', 'At Winnebago Industries we help our customers explore the outdoor lifestyle, enabling extraordinary mobile experiences as they travel, live, work and play.', 'Draw conclusions and effectively communicate findings with both technical and non-technical team members', 'Strong results orientation.\xa0Ability to lead and motivate a team to deliver to expectations\xa0', 'Demonstrate the ability to serve as a strong team member with solid communication skills and the ability to handle multiple priorities in a fast-paced, growth environment\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0', 'Develop BI Solutions in the Microsoft stack\xa0', 'Proactively identify opportunities to improve current digital solutions or processes', 'Strong analytical skills focusing on market and operational data analytics', 'Contribute to creation of processes that improve data quality', '2+ years experience with Dimensional Modeling & Cloud Databases', 'Experience with Streaming Analytics – Azure Stream Analytics', 'Demonstrate experience and leadership in the areas of advanced data techniques, including data modeling, data access, data integration, data visualization, big data solutions, data discovery, statistical methods, and database design', '\xa0', 'Work with business stakeholders and IT partners to define projects for delivering BI capabilities.\xa0Provide technical and execution leadership to design solutions and successfully deliver projects within time, cost, and scope expectations.\xa0', 'Strongly preferred - an understanding of IoT platforms and the ability to show the value of the data', 'Work with business partners to define ways to leverage data to develop platforms and solutions to drive business growth', 'Develop BI Solutions in the Microsoft stack\xa0Work with business stakeholders and IT partners to define projects for delivering BI capabilities.\xa0Provide technical and execution leadership to design solutions and successfully deliver projects within time, cost, and scope expectations.\xa0Demonstrate experience and leadership in the areas of advanced data techniques, including data modeling, data access, data integration, data visualization, big data solutions, data discovery, statistical methods, and database designWork with business partners to define ways to leverage data to develop platforms and solutions to drive business growthEnsure solutions are architected to support proper data capture and processingTranslate business objectives into requirements to facilitate information solutionsResponsible for introducing and following best practicesIdentify and escalate issues and risks early to minimize impact to businessProactively identify opportunities to improve current digital solutions or processesContribute to creation of processes that improve data qualityDraw conclusions and effectively communicate findings with both technical and non-technical team members', 'Strongly preferred - an understanding of IoT platforms and the ability to show the value of the dataAn understanding of Big Data platformsData Integration experience including moving data efficiently from On-Premise to the Cloud, leveraging tools like Informatica Cloud Services, SSIS, or Azure Data FactoryExperience with Streaming Analytics – Azure Stream AnalyticsExperience with Machine Learning', 'Ability to take initiative and work independently with minimal supervision\xa0', 'Responsible for introducing and following best practices']",Associate,Full-time,Information Technology,"Leisure, Travel & Tourism",2021-03-18 14:34:51
Data Engineer,Maritz,"Frederick, MD",3 weeks ago,Be among the first 25 applicants,"['', 'Qualifications', 'Maritz will only employ applicants who have authorization to work permanently in the U.S. This is not a position for which sponsorship will be provided. Those who need sponsorship for work authorization now or in the future are not eligible for hire. No calls or agencies please.', 'Maritz is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to sex, race, color, religion, national origin, age, marital status, political affiliation, sexual orientation, gender identity, genetic information, disability or protected veteran status. We are committed to providing a workplace free of any discrimination or harassment. If you have a disability and are having difficulty accessing or using this website to apply for a position, you can request help by calling 1-636-827-1650 or by sending an email to idisability.administrator@maritz.com .', ' DISCLAIMER: ', 'Primary Responsibilities', 'EXCITED TO GROW YOUR CAREER? WE’RE GLAD YOU’RE HERE! ', 'C# experience preferred. ', '3-5 years’ experience working with Microsoft SQL Server or equivalent.', 'Bachelor’s degree in Computer Science, equivalent field of study or equivalent job experience.3-5 years’ experience working with Microsoft SQL Server or equivalent.C# experience preferred. AWS Experience preferred.Experience with Microsoft SQL server or equivalent in development and production environment. Experience with creating database object and performing data analysis.Experience implementing 3rd party tools for monitoring, comparing, or auditing.Performing tuning experience preferred.', 'AWS Experience preferred.', 'Performing tuning experience preferred.', 'Bachelor’s degree in Computer Science, equivalent field of study or equivalent job experience.', 'Experience implementing 3rd party tools for monitoring, comparing, or auditing.', 'Experience with Microsoft SQL server or equivalent in development and production environment. Experience with creating database object and performing data analysis.', 'EXCITED TO GROW YOUR CAREER? WE’RE GLAD YOU’RE HERE!  ']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Data Engineer,Amount,"Chicago, IL",4 weeks ago,90 applicants,"['', 'Develop features to support dynamic ETL, automated data quality validation, and data delivery', 'Participate in design and architecture planning for our infrastructure and code', 'Gain experience in Big Data technologies such as Apache Spark in a Cloud-first environment ', 'AWS experience preferred', 'You have 1+ years of experience processing data with Pandas, Spark, Hive, or Hadoop', 'Automate operational data tasks', "" What You'll Do "", 'Perform periodic on-call duties', 'You have a bachelors in Computer Science or a related degree or equivalent experience', 'Build data ingestion pipelines for various data sources including Postgres, SQLServer, and REST APIs', ' Our Values ', 'You have experience performance tuning data ETL', 'You have a bachelors in Computer Science or a related degree or equivalent experienceYou have 1+ years of development experience in Python, Java, or ScalaYou have 1+ years of experience processing data with Pandas, Spark, Hive, or HadoopYou have experience performance tuning data ETLYou have experience with technologies like Airflow, Luigi, Nifi, AWSAWS experience preferredData modeling experience preferred', 'You have experience with technologies like Airflow, Luigi, Nifi, AWS', ' Who You Are ', 'Gain experience in Big Data technologies such as Apache Spark in a Cloud-first environment Build data ingestion pipelines for various data sources including Postgres, SQLServer, and REST APIsParticipate in design and architecture planning for our infrastructure and codeDevelop features to support dynamic ETL, automated data quality validation, and data deliveryAutomate operational data tasksPerform periodic on-call duties', ' Benefits And Perks ', 'You have 1+ years of development experience in Python, Java, or Scala', 'Data modeling experience preferred']",Entry level,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Engineer,HelloFresh,"Boulder, CO",2 weeks ago,76 applicants,"['', 'Come see what’s cookin’ at HelloFresh!', '$0 monthly premium and other flexible health plans effective first day of employment', 'Able to develop an in-depth understanding of HelloFresh’s core product and architecture, and act as an ambassador for state of the art software solutions and industry best practices', 'Company sponsored outings & Employee Resource Groups', 'Proficient in Python (with knowledge of OOP) and SQL (DDL, DML, CTEs, query optimization, ...). ', 'The ability to design, implement and deliver maintainable and high-quality code using best practices (e.g. git/github, secrets, configurations, yaml/json)', 'Design and deploy cloud-based Data infrastructure (AWS, Databricks)', 'Competitive Salary & 401k company match that vests immediately upon participation', 'Past experience working with Apache Spark required', 'You’ll get …', 'Data Transformations: implement the logic of the data pipeline (aggregations, projections, selections, etc …)', '75% discount on your subscription to HelloFresh (as well as other product initiatives)', 'You are ...', ' Competitive Salary & 401k company match that vests immediately upon participation Generous parental leave of 16 weeks & PTO policy $0 monthly premium and other flexible health plans effective first day of employment 75% discount on your subscription to HelloFresh (as well as other product initiatives) Snacks, cold brew on tap & monthly catered lunches Company sponsored outings & Employee Resource Groups Collaborative, dynamic work environment within a fast-paced, mission-driven company ', 'BSc in a STEM discipline', ' BSc in a STEM discipline 2+ years’ data engineering experience is required Proficient in Python (with knowledge of OOP) and SQL (DDL, DML, CTEs, query optimization, ...).  Past experience working with Apache Spark required Experience with end-to-end testing and general DevOps practices for data pipelines The ability to design, implement and deliver maintainable and high-quality code using best practices (e.g. git/github, secrets, configurations, yaml/json) Knowledge of data structures (DataFrames, RDDs, Dataclasses) and data formats (CSV, JSON, Parquet, Avro, ORC) Experience with software design patterns, and building highly scalable solutions preferred Experience with job orchestration tools like Airflow, Luigi or similar preferred ', 'Experience with software design patterns, and building highly scalable solutions preferred', 'Generous parental leave of 16 weeks & PTO policy', 'Knowledge of data structures (DataFrames, RDDs, Dataclasses) and data formats (CSV, JSON, Parquet, Avro, ORC)', 'You will do ...', 'Data cleaning/enrichment: keeping data clean and consistent with production systems (e.g. bug fixes, backfills …)', 'Collaborative, dynamic work environment within a fast-paced, mission-driven company', 'Experience with end-to-end testing and general DevOps practices for data pipelines', 'Implement ETLs monitoring automation', '2+ years’ data engineering experience is required', 'Job Description', 'Experience with job orchestration tools like Airflow, Luigi or similar preferred', "" Design and deploy cloud-based Data infrastructure (AWS, Databricks) Implement ETLs monitoring automation Help design, update and extend HelloFresh's data model (create new schemas, fact tables, mat views, joins, etc.) Data cleaning/enrichment: keeping data clean and consistent with production systems (e.g. bug fixes, backfills …) Design and implement end-to-end data products and marketing automation flows: from data ingestions for data science modeling to creation of automated pipelines to external software (Salesforce, etc.) Data Transformations: implement the logic of the data pipeline (aggregations, projections, selections, etc …) "", 'Design and implement end-to-end data products and marketing automation flows: from data ingestions for data science modeling to creation of automated pipelines to external software (Salesforce, etc.)', 'Snacks, cold brew on tap & monthly catered lunches', 'At a minimum, you have ...', 'An active, solution-oriented member of autonomous, cross-functional agile teams collaborating with Product Owners, Data Scientists, and Business Intelligence teams', ""Help design, update and extend HelloFresh's data model (create new schemas, fact tables, mat views, joins, etc.)"", 'Salary: $95,000-$105,000', ' An active, solution-oriented member of autonomous, cross-functional agile teams collaborating with Product Owners, Data Scientists, and Business Intelligence teams Able to develop an in-depth understanding of HelloFresh’s core product and architecture, and act as an ambassador for state of the art software solutions and industry best practices ']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Data Engineer,COMPASS Pathways plc,"New York, United States",2 weeks ago,42 applicants,"['', 'Note that we are not able to sponsor employment visas at this time, and therefore can only accept applications from people who have employment rights in the US.', 'COMPASS Pathways is a mental health care company dedicated to accelerating patient access to evidence-based innovation in mental health (https://compasspathways.com). Our focus is on improving the lives of those who are suffering with mental health challenges and who are not helped by current treatments. We are pioneering the development of a new model of psilocybin therapy, in which our proprietary formulation of synthetic psilocybin, COMP360, is administered in conjunction with psychological support.', 'US applicants', 'How to apply', 'Job overview', 'Experience working with distributed data processing frameworks (Spark, Flink), message queues (Kafka, Kinesis) and real-time stream processing in general', 'Reports to', 'Develop and maintain sophisticated data tools and platforms that will help democratizing data in the company', '\ufeff', 'Ensure Data Science team is not blocked by absence of high-quality data', 'Experience and understanding of modern development practices, CI/CD pipelines, infrastructure-as-code frameworks, secret management', 'If interested, please send your CV and statement of interest to hiring@compasspathways.com', 'Background in healthtech is a strong plus', 'The Data Engineer will be responsible for developing and maintaining COMPASS’ data infrastructure.', 'Ensure the privacy, security and regulatory compliance of our datasets', 'Deep understanding of various transactional (ideally Postgres) and analytical databases, understanding of their strengths and weaknesses ', 'hiring@compasspathways.com', 'Strong knowledge and passion for SQL. Experience debugging and optimising queries', 'Experience working in AWS environment, being open to DevOps-related tasks', 'Maintain and own the centralized Data Warehouse and Data LakeEnsure the privacy, security and regulatory compliance of our datasetsEnsure availability, timeliness and correctness of our datasets (ETL, etc)Enforce best practices of modern data warehousing, security and data engineeringEnsure Data Science team is not blocked by absence of high-quality dataDevelop and maintain sophisticated data tools and platforms that will help democratizing data in the company', 'Profile', 'Expert knowledge of Python and best practices of the ecosystem. Additional languages (Java, Go, JavaScript/Typescript) are a strong plusStrong knowledge and passion for SQL. Experience debugging and optimising queriesDeep understanding of various transactional (ideally Postgres) and analytical databases, understanding of their strengths and weaknesses Experience working with distributed data processing frameworks (Spark, Flink), message queues (Kafka, Kinesis) and real-time stream processing in generalExperience working in AWS environment, being open to DevOps-related tasksExperience and understanding of modern development practices, CI/CD pipelines, infrastructure-as-code frameworks, secret managementPassion for front-end and data visualization is a plusBackground in healthtech is a strong plus', 'Roles and responsibilities', 'Reports to: Data Engineer will report to Head of Data Engineering', 'Equal opportunities ', 'Passion for front-end and data visualization is a plus', 'Expert knowledge of Python and best practices of the ecosystem. Additional languages (Java, Go, JavaScript/Typescript) are a strong plus', 'Ensure availability, timeliness and correctness of our datasets (ETL, etc)', 'Enforce best practices of modern data warehousing, security and data engineering', 'COMPASS Pathways is proud to be an equal opportunity employer. All employment decisions are based on business needs, job requirements, and individual qualifications, without regard to race, religion, color, national origin, sex (including pregnancy, childbirth, and related medical conditions), ethnicity, age, disability, sexual orientation, gender identity, gender expression, military service, genetic information, familial or marital status, or any other status, category, or characteristic protected by applicable law.', 'Maintain and own the centralized Data Warehouse and Data Lake']",Entry level,Full-time,Information Technology,Mental Health Care,2021-03-18 14:34:51
Data Engineer - Cloud,Rock Central,"Detroit, MI",5 days ago,Be among the first 25 applicants,"['', 'Create standards and best practices for database design and development', 'Ensure quality data creation, presentation and flow across systems', 'Job Summary', 'Knowledge of VBScript', 'Proficiency in the Microsoft Office suite', 'Minimum Qualifications', 'Responsibilities', 'Knowledge of PowerShell', 'Lead code and design reviews', 'Write complex stored procedures and enhance database applications', 'Meet with team leaders to ensure data collection satisfaction and presentation', 'Understand large data models quickly', 'Proficiency in the Microsoft Office suiteKnowledge of SQL Server integrationKnowledge of PowerShellKnowledge of VBScriptKnowledge of .Net', 'Knowledge of .Net', 'Act as a mentor for other team members', 'Who We Are', 'Disclaimer', 'Oversee physical database design, monitor database performance and implement performance tuning', 'Be available to be on call', '2 years of experience in SQL database applicationsBachelor’s degree in computer science or equivalent work experience', 'Oversee physical database design, monitor database performance and implement performance tuningLead code and design reviewsCreate standards and best practices for database design and developmentEnsure quality data creation, presentation and flow across systemsAct as a mentor for other team membersWrite complex stored procedures and enhance database applicationsMeet with team leaders to ensure data collection satisfaction and presentationUnderstand large data models quicklyBe available to be on call', '2 years of experience in SQL database applications', 'Knowledge of SQL Server integration', 'Bachelor’s degree in computer science or equivalent work experience', 'Preferred Qualifications']",Entry level,Full-time,Other,Information Technology and Services,2021-03-18 14:34:51
Data Engineer Intern,Label Insight,"Chicago, IL",3 days ago,Be among the first 25 applicants,"['', 'We Share a Common Purpose', 'Who You Are', 'About The Engineering Team', 'The Perks Of Joining Us This Summer', ""How You'll Contribute"", 'At The End Of This Internship You Will Have', 'We Live Our Values', 'At Label Insight we embrace diversity and are committed to building a team that represents a variety of backgrounds and qualifications. We’re proud to be an Equal Opportunity Employer']",Internship,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Engineer - R&D / Homebase,Opendoor,San Francisco Bay Area,7 days ago,Be among the first 25 applicants,"['', ' Generous parental leave', 'Opendoor values Openness', 'More About Us', 'About Opendoor', ' Flexible vacation policy', ' Your choice of coverage for medical, dental, and vision (optional for dependents)', 'Please note that these benefits and perks are available only to Full Time team members and do not apply to contract roles.', ' Your choice of coverage for medical, dental, and vision (optional for dependents) Flexible vacation policy Commuter benefit Generous parental leave Paid time off to volunteer', ' Paid time off to volunteer', 'We Offer The Following Benefits And Perks', ' Commuter benefit']",Mid-Senior level,Full-time,Engineering,Real Estate,2021-03-18 14:34:51
Data Engineer Manager,Dascena,"California, United States",2 weeks ago,Be among the first 25 applicants,"['', 'Bachelor or graduate degree in computer science, software engineering, or related field', 'Recruit and mentor data engineers and interns ', 'Lead a team of data engineers and software engineers to design, develop, and maintain our large-scale clinical data platform and data tools', 'Remote work', 'To build a diverse workforce, Dascena encourages applications from individuals with disabilities, minorities, veterans, and women. Dascena is an equal opportunity employer. All qualified applicants will receive consideration for employment without regards to race, color, religion, sex, sexual orientation, gender identity, marital status, national origin, disability, veteran status, or any other basis protected by applicable federal, state, or local law.', 'To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. Reasonable Accommodations may be made to enable qualified individuals with disabilities to perform the essential functions.To build a diverse workforce, Dascena encourages applications from individuals with disabilities, minorities, veterans, and women. Dascena is an equal opportunity employer. All qualified applicants will receive consideration for employment without regards to race, color, religion, sex, sexual orientation, gender identity, marital status, national origin, disability, veteran status, or any other basis protected by applicable federal, state, or local law.', 'Expert knowledge in Python ecosystem and related data frameworks and libraries', 'Experience with security standards and compliance', 'Responsibilities', 'Health benefits', 'Other Information:', 'Learn new techniques in the worlds of machine learning and artificial intelligence and put them into practice to develop novel approaches in the domain of healthcare', 'Extensive experience in managing data warehouse systems and large datasets in cloud environments', 'Qualifications', 'Competitive compensationHealth benefitsFlexible hours and PTORemote work', 'Flexible hours and PTO', 'Bachelor or graduate degree in computer science, software engineering, or related fieldExtensive experience in managing data warehouse systems and large datasets in cloud environmentsStrong understanding of NoSQL and SQL databasesStrong understanding of distributed computing and scalability Expert knowledge in Python ecosystem and related data frameworks and librariesExperience with security standards and complianceExcellent communication and writing skillsAbility to operate in a fast-moving environment supporting a rapidly growing business', 'Excellent communication and writing skills', 'Establish processes to help the team operate efficiently', 'Ability to operate in a fast-moving environment supporting a rapidly growing business', 'Benefits & Perks', 'Strong understanding of distributed computing and scalability ', 'Lead a team of data engineers and software engineers to design, develop, and maintain our large-scale clinical data platform and data toolsDesign and implement flexible workflows that support the ingestion, transformation, and storage of complex clinical dataEstablish processes to help the team operate efficientlyCollaborate with the data science team to deliver data products that support the data science research and developmentCollaborate with the engineering team and the business team to drive cross-product data projects Learn new techniques in the worlds of machine learning and artificial intelligence and put them into practice to develop novel approaches in the domain of healthcareRecruit and mentor data engineers and interns ', 'Design and implement flexible workflows that support the ingestion, transformation, and storage of complex clinical data', 'Collaborate with the engineering team and the business team to drive cross-product data projects ', 'Competitive compensation', 'To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. Reasonable Accommodations may be made to enable qualified individuals with disabilities to perform the essential functions.', 'Collaborate with the data science team to deliver data products that support the data science research and development', 'Strong understanding of NoSQL and SQL databases', 'Data Engineer Manager']",Mid-Senior level,Full-time,Engineering,Medical Devices,2021-03-18 14:34:51
Data Engineer (Mobile and Social Games) - Orlando or NY,The Topps Company,"New York, NY",2 days ago,Be among the first 25 applicants,"['', ' Cataloging and collecting data from sources including Postgres DBs, Kinesis Firehoses, Internal and External APIs, etc. ', ' Execute design, implementation and maintenance of ETL ', ' Looker or other similar visualization tools ', ' Be a resource and advocate for data within the division through empowering others to use the dataprovided  Building, maintaining and optimizing data pipelines written in Python and Scala using Spark  Working with engineering, product, and strategy groups to create and gather data requirements  Integrating Data Warehouse with BI tools such as Looker  Execute design, implementation and maintenance of ETL  Adopt best practices in reporting and analysis including data integrity, testing, maintainability, validation and documentation  Cataloging and collecting data from sources including Postgres DBs, Kinesis Firehoses, Internal and External APIs, etc.  Working in an agile DataOps environment ', 'Responsibilities', ' Integrating Data Warehouse with BI tools such as Looker ', ' Good communication skills ', ' Working with engineering, product, and strategy groups to create and gather data requirements ', ' 5-10 years of experience applying quantitative methods to understand and answer questions ', ' Passion for learning and exploring ', ' AWS ', ' Experience working with multiple business groups to meet their analytics needs ', ' SQL proficiency and the ability to extract data from large data sources with differing structures ', 'Requirements', ' Working in an agile DataOps environment ', ' Experience with modern work tracking tools ', ' Strong proficiency with a statistical analysis tool such as Python or Scala ', ' Analytical/mathematical mindset ', ' Passion for playing and thinking about games is a plus ', ' High interest in Big Data processing ', ' BS/BA in Computer Science, Statistics, Mathematics or equivalent experience ', 'Preferred Technologies', ' Experience building clear, user-friendly data visualizations, tables, and dashboards ', ' Interest in building machine learning systems ', 'Data Engineer', ' Adopt best practices in reporting and analysis including data integrity, testing, maintainability, validation and documentation ', ' Qualifications ', 'Overview', ' Experience with Test Driven Development is a plus ', ' BS/BA in Computer Science, Statistics, Mathematics or equivalent experience  5-10 years of experience applying quantitative methods to understand and answer questions  Experience building clear, user-friendly data visualizations, tables, and dashboards  Strong proficiency with a statistical analysis tool such as Python or Scala  SQL proficiency and the ability to extract data from large data sources with differing structures  Familiarity with version control and reproducible research best practices  Experience with modern work tracking tools  Experience working with multiple business groups to meet their analytics needs  Passion for playing and thinking about games is a plus  High interest in Big Data processing  Experience with Test Driven Development is a plus  Interest in building machine learning systems  Analytical/mathematical mindset  Good communication skills  Passion for learning and exploring ', ' Familiarity with version control and reproducible research best practices ', ' Spark/Databricks ', ' Building, maintaining and optimizing data pipelines written in Python and Scala using Spark ', ' AWS  Spark/Databricks  Looker or other similar visualization tools ', ' Be a resource and advocate for data within the division through empowering others to use the dataprovided ', ' Responsibilities ']",Entry level,Full-time,Information Technology,Sports,2021-03-18 14:34:51
Data Engineer,Talution Group,"Chicago, IL",1 week ago,64 applicants,"['', 'Talution Group is hiring a Data Engineer to place on contract with our client in Chicago. This is currently a remote position, but our consultant will need to be on-site in the future.', 'Job Description']",Entry level,Contract,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,First Republic Bank,"San Francisco, CA",3 weeks ago,Be among the first 25 applicants,"['', ' Responsible for driving and managing data source integration between various vendor systems.', ' Bachelors or Master degree in information technology, computer science or data science Strong skills in python and knowledge of various frameworks like pandas, pyspark. Experience in building cloud native data lakes, pipelines and stream processing Experience with cloud services preferably AWS and Snowflake. Background in data science, analytics, or data mining. Familiar with Data Virtualization concepts ideally with Denodo/Composite experience. Familiar with Data Virtualization concepts ideally with Denodo/Composite experience. Experience in DevSecOps and automation using CICD tools and process Proven history of learning and implementing new technology in fast moving environment. Hands-On experience with data pipeline design and development. Experience with both SQL and NoSQL as well as their relevant data modeling patterns Demonstrated experience working in large-scale data environments which included real-time and batch processing requirements. Familiar with Data Virtualization concepts ideally with Denodo/Composite experience.', ' Perform detailed analysis to troubleshoot and resolve identified issues and maintain data integrity', ' Design, develop and maintain various data model for regulatory and corporate domain Develop Pipelines, ensuring the best practice are implemented for data governance, data quality, data lineage and data cleansing. Apply data science skills to model data for quality verification Perform detailed analysis to troubleshoot and resolve identified issues and maintain data integrity Responsible for driving and managing data source integration between various vendor systems.', ""What You'll Do As a Data Engineer"", ' Experience in building cloud native data lakes, pipelines and stream processing', ' Proven history of learning and implementing new technology in fast moving environment.', ' Strong skills in python and knowledge of various frameworks like pandas, pyspark.', ' Background in data science, analytics, or data mining.', ' Must be able to communicate effectively via telephone and in person.', 'Responsibilities', 'Job Demands', ' Must be able to review and analyze data reports and manuals; must be computer proficient.', ' Apply data science skills to model data for quality verification', ' Experience with cloud services preferably AWS and Snowflake.', ' Must be able to review and analyze data reports and manuals; must be computer proficient. Must be able to communicate effectively via telephone and in person.', 'Qualifications', ' Bachelors or Master degree in information technology, computer science or data science', 'Description', ' Hands-On experience with data pipeline design and development.', ' Demonstrated experience working in large-scale data environments which included real-time and batch processing requirements.', ' Design, develop and maintain various data model for regulatory and corporate domain', ' Experience in DevSecOps and automation using CICD tools and process', ' Familiar with Data Virtualization concepts ideally with Denodo/Composite experience.', ' Develop Pipelines, ensuring the best practice are implemented for data governance, data quality, data lineage and data cleansing.', ' Experience with both SQL and NoSQL as well as their relevant data modeling patterns']",Not Applicable,Full-time,Information Technology,Banking,2021-03-18 14:34:51
Data Engineer,Meredith Corporation,"New York, NY",3 days ago,69 applicants,"['', 'Experience with NoSQL databases (Key Value, Document-Oriented, etc).', 'Strong experience in Testing: mocking and stubbing. ', 'Training models and tuning their hyper-parameters', 'Data Quality & Processing', 'Identifying differences in data distribution that could affect performance when deploying the model in the real world', 'Analyzing the errors of the model and designing strategies to overcome them', 'Proficiency with a deep learning framework such as TensorFlow, Keras, Amazon, Google, MonkeyLearn, etc.', 'Exploring and visualizing data to gain an understanding of it', '100%', 'Managing available resources such as hardware, data, and personnel so that deadlines are met', 'Verifying data quality, and/or ensuring it via data cleaningSupervising the data acquisition process if more data is neededFinding/filtering available datasets online that could be used for training', 'Strong experience using Git.', 'Specific Knowledge, Skills And Abilities', 'Defining validation strategies', 'Constantly seeking ways to improve the data, algorithms, and processes. ', 'Defining validation strategiesDefining the pre-processing or feature engineering to be done on a given datasetDefining data augmentation pipelinesTraining models and tuning their hyper-parametersAnalyzing the errors of the model and designing strategies to overcome themDeploying models to production', 'Proficiency with a deep learning framework such as TensorFlow, Keras, Amazon, Google, MonkeyLearn, etc.Proficiency with Python and libraries for machine learning such as scikit-learn and pandasExpertise in visualizing and manipulating big datasets (up to billions of records)Proficiency with OpenCVFamiliarity with LinuxAbility to select and configure cloud resources to run an ML model with the required latencyAbility to set up Sisense / Periscope visualizations based on the data sets', 'Deploying models to production', 'Education', 'Developing models that help to achieve the objectives', 'Expertise in visualizing and manipulating big datasets (up to billions of records)', 'II. Essential Job Functions', 'Proficiency with OpenCV', 'Data & ML Algorithm Exploration & Performance', 'Finding/filtering available datasets online that could be used for training', 'Excellent problem solving and communication.', 'Familiarity with Linux', 'I. Job Summary ', '25%', 'Ability to select and configure cloud resources to run an ML model with the required latency', 'Experience', 'Ability to set up Sisense / Periscope visualizations based on the data sets', 'Strong experience in Algorithms, Object Oriented Programming and Design Patterns.', 'Supervising the data acquisition process if more data is needed', 'Strong experience with data modeling and SQL (PostgreSQL 10.5+, MySQL 5.2+).', 'Excellent problem solving and communication.Strong experience in Algorithms, Object Oriented Programming and Design Patterns.Strong experience in Testing: mocking and stubbing. Strong experience using Git.Strong experience with data modeling and SQL (PostgreSQL 10.5+, MySQL 5.2+).Experience with NoSQL databases (Key Value, Document-Oriented, etc).Comfortable working in a fast paced and agile environment. Constantly seeking ways to improve the data, algorithms, and processes. ', 'III. Minimum Qualifications and Job Requirements ', 'Analyzing the ML algorithms that could be used to solve a given problem and ranking them by success probability', 'Proficiency with Python and libraries for machine learning such as scikit-learn and pandas', 'Comfortable working in a fast paced and agile environment. ', 'Data Validation, Training, Deployment', 'Understanding business objectivesDeveloping models that help to achieve the objectivesDeveloping metrics to track their progressManaging available resources such as hardware, data, and personnel so that deadlines are met', 'Understanding business objectives', 'Job Description', 'Analyzing the ML algorithms that could be used to solve a given problem and ranking them by success probabilityExploring and visualizing data to gain an understanding of itIdentifying differences in data distribution that could affect performance when deploying the model in the real world', 'Developing metrics to track their progress', 'Converting business objectives to measurable models', 'Verifying data quality, and/or ensuring it via data cleaning', 'Defining data augmentation pipelines', 'Major goals and objectives.', 'Job Title', 'All must be met to be considered.', 'Defining the pre-processing or feature engineering to be done on a given dataset']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Data Platform Engineer,THE TIFIN GROUP,"Boulder, CO",3 weeks ago,35 applicants,"['', 'OUR VALUES:', 'BENEFITS:', 'Assemble, splice, and merge large, complex data sets that meet functional / non-functional business requirements', 'Expertise in various big data technologies both open source and cloud native, AWS preferred (Kafka/Kinesis, Presto/Athena, Spark/EMR, Airflow, Hive)', 'Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies', 'A top performer with a proactive approach who has a “doer” & problem-solver mentality\xa0', 'Create a shared understanding. We communicate with radical candor, precision and compassion to create a shared understanding. We challenge, but once a decision is made, commit fully. We listen attentively, speak candidly.', 'Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader', 'Create a shared understanding. We communicate with radical candor, precision and compassion to create a shared understanding. We challenge, but once a decision is made, commit fully. We listen attentively, speak candidly.Teamwork for Teamwin. We believe in win together, learn together. We fly in formation. We cover each other’s backs. We inspire each other with our energy and attitude.\xa0Make magic for our users. We center around the voice of the customer. With deep empathy for our clients, we create technology that transforms investor experiences.\xa0Grow at the edge. We are driven by personal growth. We get out of our comfort one and keep egos aside to find our genius zones. We strive to be the best we can possibly be. No excuses.\xa0Disrupt with creative solutions. We believe that disruptive innovation begins with curiosity and creativity. We challenge the status quo and problem solve to find new answers.\xa0', 'An exceptional team player with strong communication skills', 'Create a shared understanding.', 'Make magic for our users. We center around the voice of the customer. With deep empathy for our clients, we create technology that transforms investor experiences.\xa0', ""WHAT YOU'LL BE DOING:"", 'Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs', 'Strong coding skills in Python', 'Teamwork for Teamwin. We believe in win together, learn together. We fly in formation. We cover each other’s backs. We inspire each other with our energy and attitude.\xa0', 'Disrupt with creative solutions', 'Disrupt with creative solutions. We believe that disruptive innovation begins with curiosity and creativity. We challenge the status quo and problem solve to find new answers.\xa0', 'Degree in Computer Science, related field or equivalent experience', 'WHO YOU ARE:', 'Track record of choosing the right transit, storage, and analytical technology to simplify and optimize user experience', 'Highly flexible, good tolerance for ambiguity, and able to quickly adapt to changing priorities', 'The TIFIN Group, which starts, invests and operates first-in-category fintech companies in investment management, investment advisory, and personal finance. The TIFIN Group companies are shaping the future of investment experience. Our fintech solutions are powered by science, world-class technology, investment intelligence and algorithmic expertise.', 'Design data platforms and tools that abstract implementation details for developers, analysts, and data scientists, enabling data transit and storage “as a service”', 'This is a greenfield project so you must be a motivated and tenacious self-starter who is comfortable interpreting technology requirements, based on business requirements, and implementing them with maximum impact to the users. You must be results- and execution-oriented. You will be expected to develop technology in a way that minimizes technology debt while maximizing the customer experience. This is a rare opportunity to join an early-stage startup founded under a broader fintech platform, called The TIFIN Group, at a post-product, pre-revenue stage.', 'Competitive base salary, health benefits, unlimited PTO and performance-linked variable compensation.', 'WHO WE ARE:', '5+ years of increasing responsibility in technical data roles\xa0', 'Teamwork for Teamwin', 'Degree in Computer Science, related field or equivalent experience5+ years of increasing responsibility in technical data roles\xa0Real-world experience developing highly scalable solutions using micro-service architecture designed to democratize data to everyone in the organizationStrong coding skills in PythonExpertise in various big data technologies both open source and cloud native, AWS preferred (Kafka/Kinesis, Presto/Athena, Spark/EMR, Airflow, Hive)Ability to work comprehensively with a variety of databases (Postgres, SQL, MongoDB, etc.)Expertise in data model design with sensitivity to usage patterns and goals – schema, scalability, immutability, idempotency, etc.Track record of choosing the right transit, storage, and analytical technology to simplify and optimize user experienceAbility to thrive in a highly demanding, entrepreneurial, and fast-paced environment\xa0Highly flexible, good tolerance for ambiguity, and able to quickly adapt to changing prioritiesA top performer with a proactive approach who has a “doer” & problem-solver mentality\xa0An exceptional team player with strong communication skillsA local presence in Denver or Boulder is preferred', 'Expertise in data model design with sensitivity to usage patterns and goals – schema, scalability, immutability, idempotency, etc.', 'Our companies have experienced promising growth in 2020 and as we enter into the next stage of expansion, we are looking to leverage the combined data from all of our platforms in order to build a more complete picture of what investors want. We are looking for an exceptional and ambitious Data Platform Engineer to join this new world-class data team. The ideal candidate is excited to build and optimize a distributed data platform to be used for machine learning, and is an experienced data wrangler who enjoys optimizing data systems and building them from the ground up.\xa0', 'A local presence in Denver or Boulder is preferred', 'Ability to work comprehensively with a variety of databases (Postgres, SQL, MongoDB, etc.)', 'Make magic for our users', 'Assemble, splice, and merge large, complex data sets that meet functional / non-functional business requirementsBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologiesDesign data platforms and tools that abstract implementation details for developers, analysts, and data scientists, enabling data transit and storage “as a service”Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leaderWork with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needsKeep our data separated and secure across national boundaries through multiple data centers and AWS regions', 'Real-world experience developing highly scalable solutions using micro-service architecture designed to democratize data to everyone in the organization', 'Ability to thrive in a highly demanding, entrepreneurial, and fast-paced environment\xa0', 'THE ROLE:', 'Grow at the edge. We are driven by personal growth. We get out of our comfort one and keep egos aside to find our genius zones. We strive to be the best we can possibly be. No excuses.\xa0', 'Keep our data separated and secure across national boundaries through multiple data centers and AWS regions', 'Grow at the edge', 'Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.', 'The TIFIN Group is proud to be an equal opportunity workplace and values the multitude of talents and perspectives that a diverse workforce brings. All qualified applicants will receive consideration for employment without regard to race, national origin, religion, age, color, sex, sexual orientation, gender identity, disability, or protected veteran status.']",Mid-Senior level,Full-time,Engineering,Venture Capital & Private Equity,2021-03-18 14:34:51
Google Data Engineer,The Judge Group,"Minneapolis, MN",3 days ago,Be among the first 25 applicants,"['', 'Description', 'Technical/Functional Skills:', 'This job and many more are available through The Judge Group. Find us on the web at www.judge.com', 'Location: ', 'Required', 'Contact:', 'Requirements']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Foundation for California Community Colleges,"Sacramento, CA",4 weeks ago,65 applicants,"['', '— Manage, refine, and enhance AWS cloud services infrastructure which include the following: EC2, VPC, RDS, CloudWatch, CloudFormation, CloudTrail, Transfer for sFTP, S3, Lambda, Secrets Manager, and Route 53.', 'For immediate consideration, please submit a letter of interest and resume saved as Microsoft Word (.doc/.docx) or Adobe Acrobat PDF (.pdf) documents to jobs@foundationccc.org.', 'Data Engineer, Data Infrastructure and Analytics California College Guidance Initiative ', '● Pending removal of COVID restrictions, overnight travel (up to 5%) by land and air.', 'Remote/Virtual within California — Full Time', '— Advance projects without detailed supervision, balancing multiple responsibilities, and providing colleagues with actionable proposals for advancing collective efforts.', '— Excellent verbal communication skills; ability to communicate with various levels of professionals.', 'Equal Employment Opportunity', '— Advanced proficiency with ETL tools like Matillion, Talend, etc.', 'Equal Employment Opportunity:', 'What Intangibles Do You Need? ', '— Strong decision-making skills and collaborative spirit with the ability to take abstract brainstorming and generate concrete proposals for action.', '— Expert SQL knowledge (SQL Server, PostgreSQL, MySQL, etc.) and understanding of relational databases, query authoring and optimization, as well as working familiarity with a variety of databases.', '— Maintain and update documentation of data architecture from both the macro view (i.e. architectural diagrams) and the micro view (i.e. script level tasks in Airflow).', '— Thrive in a fast-paced environment with changing priorities and deadlines.', 'CCGI is housed at the Foundation for California Community Colleges but is an autonomous initiative with its own mission, goals, and leadership team.', '● Ability to speak on the telephone for a total of up to 3 hours per day.', '● Ability to perform repetitive movements, such as typing, filing, and the use of commonly used office machines and supplies.', '— Advanced proficiency with big data tools: AWS, Snowflake, Hadoop, Spark, Kafka, etc.', '— A successful history of manipulating, processing and extracting value from large structured and unstructured datasets.', 'Additional requirements are following:', '— Lead ETL/ELT processes which include the development, refinement, and implementation of data loading and processing tools, including Snowflake, Airflow, Python, SQL, and shell scripts.', '— Continuously improve data pipelines and architecture by staying updated on industry trends and best practices.', 'The California College Guidance Initiative is looking for a data engineering expert to join the Data Infrastructure and Analytics Team. As part of the Data Infrastructure and Analytic Team, you will be a key strategic leader in the expansion, refinement, and continued development of CCGI’s infrastructure utilizing Amazon Web Services (AWS), as well as continuing to build out our data analytics and engineering tools.', ""This role will be instrumental in helping us scale our work statewide and open the door for expanded use of California's official college and career planning platform for K-12th grade students, parents, and educators, CaliforniaColleges.edu."", 'Application Instructions:', '● Ability to lift and move a minimum of 20 pounds.', 'What Technical Skills Do You Need? ', '— Strong organizational, project, and time management skills.', 'More About the Data Infrastructure and Analytics Team', '— Lead meetings, research processes, collect data, analyze information and collaborate with the key stakeholders of your projects.', '— Develop and maintain expert knowledge of CaliforniaColleges.edu', 'Unfortunately we cannot consider C2C or provide visa sponsorship', 'Please include in the subject line: “AWS Certified Solutions Architect-Data Engineer, California College Guidance Initiative.” The application process will be open until the position is filled.', 'There are no direct supervisory responsibilities for this position, but you must be able to proactively and successfully partner and collaborate with other subject matter experts on a project management basis. Lastly, you must be comfortable leading and project managing work with many unknowns and ambiguous solutions, as well as carry a deep knowledge and/or curiosity about the needs and behaviors of students, educators, and parents, and a passion for educational equity.', '● Ability to operate personal vehicle for Foundation business and possess current valid California driver’s license and insurance.', '— Advanced proficiency with scripting languages like Python, R, etc.', '— Advanced proficiency with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.', 'Working Conditions, Travel, and Physical Requirements: ', '— Build processes supporting data transformation, data structures, metadata, dependency, and workload management.', '● Well-lighted, heated, and air-conditioned indoor office setting with adequate ventilation.', ""As the official nonprofit auxiliary to the Chancellor's Office, we aim to ensure our team reflects the diversity of the California Community Colleges and the 2.1 million students, campuses, and communities it serves. Individuals are hired for their deep understanding of each population’s unique needs and will join a collaborative environment where each team member plays an important role in helping Californians across all communities improve their social and economic mobility and build a better future for themselves and their families."", 'The Foundation for California Community Colleges is committed to providing an environment of mutual respect where equal employment opportunities (EEO) are available to all employees and applicants without regard to race, color, ancestry, national origin, genetic characteristics, sex, gender identity, gender expression, sexual orientation, marital/parental status, political affiliation, religion, age, disability, pregnancy, childbirth, breastfeeding or veteran status. In addition to federal law requirements, The Foundation for California Community Colleges complies with applicable state and local laws governing non-discrimination in employment.', '— Advanced proficiency developing cloud infrastructure in AWS or currently has or is in the process of obtaining AWS Certification as a Solutions Architect', '— Juggle multiple projects of various scopes with ease and grace.', 'We are all located in various parts of California. We rarely meet in person. Instead, we make use of tools, such as ZOOM, Slack, and Salesforce, to communicate and document our work.', '● Ability to sit for up to 3 hours without breaks at meetings; Ability to walk and stand for up to 4 hours without breaks at program site visits and meetings.', '● This is a remote position that requires a quiet home office space.', '— Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, designing infrastructure for greater scalability and automation.', 'This role reports to the Director of Data Infrastructure and Analytics. More About CCGIThe California College Guidance Initiative (CCGI) works to ensure that all 6th-12th grade students in California have access to a systematic baseline of guidance and support as they plan, prepare, and pay for postsecondary education and training. This baseline is provided by CaliforniaColleges.edu. CCGI partners with K-12 school districts to support students, counselors, and parents with the systematic use of CaliforniaColleges.edu, including transcript-informed tools. CaliforniaColleges.edu also houses, audits, and transmits student data to help ensure more accurate and efficient decisions regarding admissions, financial aid, and course placement.', 'The ideal candidate for this team has extensive experience building out complex data architectures, including the testing, maintenance, and refinement of all data pipelines. They also have experience successfully scaling up an organization’s data intake to terabytes and petabytes of data, as well as optimizing data delivery and automating manual processes. This person is comfortable working with structured and unstructured data and can troubleshoot data loading and processing tools via SQL, Python, shell scripting, AWS, etc., and take on a leadership role in special projects when needed. Additionally, this person had extensive experience developing robust data documentation and data governance protocols.', 'What Will You Be Doing?', '— Meticulous attention to detail.', 'CCGI is a positive, diverse, and supportive culture. At our core, we prioritize the needs of students above all else. Everyone at CCGI works remotely.', 'Remote employees must reside within California', 'This is a full-time position, 40 hours per week, with additional hours as needed to address the needs of the organization. Employees are required to have the ability to work remotely successfully and at a computer workstation for periods up to 4 hours at a time and for up to 8 hours per day for up to five consecutive days. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions/physical requirements of the job.', '— Advanced proficiency building and optimizing ‘big data’ data pipelines, architectures, and data sets.', ""The team oversees all of CCGI’s internal cloud-based data analytics and engineering infrastructure and technical strategy. This includes data analytics, data warehousing, and data engineering projects, ensuring alignment with the organization's mission and goals. This is a new role for the team. We are keen for you to bring your experience where you can thrive in an environment that allows for knowledge sharing and creates opportunities for you to take initiative to learn even more.""]",Associate,Full-time,Education,Higher Education,2021-03-18 14:34:51
Data Engineer,Gentis Solutions,Cincinnati Metropolitan Area,5 days ago,Be among the first 25 applicants,"['', 'Required Skills and Experience:', 'Drive digital innovation by leveraging innovative new technologies and approaches to renovate, extend, and, transform the existing core technology base and IT estate', 'Gentis Solutions is seeking a Data Engineer for a remote role in Cincinnati, OH.\xa0', 'Proven ability to think and contribute at the strategic level', 'Lead the analysis of the technology environment to detect critical deficiencies and recommend solutions for improvement', 'Present opportunities with cost/benefit analysis to leadership to shape sound architectural decisions', '5+ years of a proven track record of designing and delivering large scale, high-quality systems', '2+ years of experience with Spark/PySpark and Python', '\ufeff', 'Experience building solutions using elastic architectures (preferably Microsoft Azure and Google Cloud Platform)', 'Demonstrated capability to build, mentor and maintain an inclusive team-based environment', 'Oversee the implementation of enterprise standards across domains: operations, infrastructure, data, applications, development, security & risk, and business continuance', 'Draft and review architectural diagrams, interface specifications, class structures and, other design documents', '2+ years of experience with IaC (infrastructure as code) using Terraform or Azure Resource Manager', 'Must be able to perform the essential job functions of this position with or without reasonable accommodation', 'Implement high-level migration plans to address the gaps between the current and future state, typically in sync with the budgeting or other capital planning processes', 'Demonstrated Skills and Experience:', 'Strong knowledge of industry trends and industry competition', 'Provide technical leadership to ensure clarity between ongoing projects and the strategic objectives of the company', 'Mentor team members in software development principles, patterns, processes and, practices', ""\ufeffBachelor's Degree in Computer Science or in a STEM major"", '5+ years of successful and applicable experience taking a lead role in building complex software and automation systems that has been successfully delivered to customers', ""\ufeffBachelor's Degree in Computer Science or in a STEM major5+ years of successful and applicable hands-on experience in software or infrastructure development domain and principles, including design patterns and code structure5+ years of a proven track record of designing and delivering large scale, high-quality systems5+ years of successful and applicable experience taking a lead role in building complex software and automation systems that has been successfully delivered to customersStrong knowledge of industry trends and industry competitionProven ability to think and contribute at the strategic levelDemonstrated written, oral and, presentation/public speaking communication skillsUnderstanding of network and security architectureDeep knowledge in a minimum of two of the following technical disciplines: infrastructure and network design, application development, application programming interfaces (APIs), cloud, middleware, servers and storage, database management, and operations"", 'Promote the capture and reuse of intellectual capital, including code objects and components', 'Demonstrated written, oral and, presentation/public speaking communication skills', 'Position Duties:', 'Develop a high-quality codebase, lead design discussions, execute development against design, and navigate complex codebase', '2+ years of experience in Azure ETL (Data Factory, Data Lake, Databricks, Snypase, Analytics, CosmosDB, etc.)', 'Experience with the implementation of advanced security, authentication, and SSO for cloud and on prem.', 'Understanding of network and security architecture', '2+ years of experience in Azure ETL (Data Factory, Data Lake, Databricks, Snypase, Analytics, CosmosDB, etc.)2+ years of experience with Spark/PySpark and Python2+ years of experience with IaC (infrastructure as code) using Terraform or Azure Resource ManagerExperience with the implementation of advanced security, authentication, and SSO for cloud and on prem.Experience building solutions using elastic architectures (preferably Microsoft Azure and Google Cloud Platform)Demonstrated capability to build, mentor and maintain an inclusive team-based environment', '5+ years of successful and applicable hands-on experience in software or infrastructure development domain and principles, including design patterns and code structure', 'Deep knowledge in a minimum of two of the following technical disciplines: infrastructure and network design, application development, application programming interfaces (APIs), cloud, middleware, servers and storage, database management, and operations', 'Lead the development implementation of technology strategy within a defined set of teams. Executes against defined technology roadmap to support project portfolio and business strategy', 'Develop a high-quality codebase, lead design discussions, execute development against design, and navigate complex codebaseLead the development implementation of technology strategy within a defined set of teams. Executes against defined technology roadmap to support project portfolio and business strategyOversee the implementation of enterprise standards across domains: operations, infrastructure, data, applications, development, security & risk, and business continuanceProvide technical leadership to ensure clarity between ongoing projects and the strategic objectives of the companyDrive digital innovation by leveraging innovative new technologies and approaches to renovate, extend, and, transform the existing core technology base and IT estateImplement high-level migration plans to address the gaps between the current and future state, typically in sync with the budgeting or other capital planning processesPresent opportunities with cost/benefit analysis to leadership to shape sound architectural decisionsLead the analysis of the technology environment to detect critical deficiencies and recommend solutions for improvementMentor team members in software development principles, patterns, processes and, practicesPromote the capture and reuse of intellectual capital, including code objects and componentsDraft and review architectural diagrams, interface specifications, class structures and, other design documentsMust be able to perform the essential job functions of this position with or without reasonable accommodation']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Daugherty Business Solutions,Greater St. Louis,,N/A,"['', 'Daugherty Business Solutions is an inclusive Equal Employment Opportunity employer that considers applicants without regard to gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law.', 'Interest working with AWS technologies such as Redshift, RDS, S3, EMR, ADP, Hive, Kinesis, SNS/SQS and QuickSight.Interest in Python, R, sh/bash and JVM-based languages including Scala and Java.Interest in Hadoop family languages including Pig and Hive.Familiarity with high performance data libraries including Spark, NumPy and TensorFlow.Proven ability to pick up new languages and technologies quickly.Intermediate level of SQL programming and query performance tuning techniques for data integration and consumption using design for optimum performance against large data assets within an OLTP, OLAP and MPP architecture.Knowledge of cloud and distributed systems principles, including load balancing, networks, scaling, and in-memory versus disk.Experience building data pipelines to connect analytics stacks, client data visualization tools and external data sources.Exposure to stream-processing and messaging, such as Storm, Spark-Streaming, Kafka and MQ.Understanding of DevOps and CI/CD toolset, such as Jenkins, GitLab CI, Buildbot, Drone and Bamboo.Some experience with programming Languages, such as Scala, Java, R and Python.', 'Some experience with programming Languages, such as Scala, Java, R and Python.', 'Recommend ways to improve data reliability, efficiency and quality.', 'Interest in Hadoop family languages including Pig and Hive.', 'Exposure to stream-processing and messaging, such as Storm, Spark-Streaming, Kafka and MQ.', 'Contribute to the creation and maintenance of optimal data pipeline architectures.', 'If you require accommodations or assistance to complete the online application process, please inform any recruiter you are working with (or send an email to careers@daugherty.com) and identify the type of accommodation or assistance you are requesting. Do not include any medical or health information in this email. The recruiting team will respond to your email promptly.', 'Revenue sharing and a 401(k) retirement savings plan.', 'Employ a variety of languages and tools to marry systems together.', 'Maintain and manage Hadoop clusters in development and production environments.', 'Due to COVID-19, most of our employees are working remotely. We’ve implemented a virtual hiring process and continue to interview candidates by phone or video and are onboarding new hires remotely. We value the safety of each member of our community because we know we’re all in this together.', 'Life, disability and long-term care insurance.', 'We offer members of Team Daugherty:', 'Interested? Apply today to take your career to the next level!', 'Interest working with AWS technologies such as Redshift, RDS, S3, EMR, ADP, Hive, Kinesis, SNS/SQS and QuickSight.', 'Collaborate and work closely with team to build data platforms.', 'Understanding of DevOps and CI/CD toolset, such as Jenkins, GitLab CI, Buildbot, Drone and Bamboo.', 'When you are a Daugherty employee, your job doesn’t end when a contract is up. You stay on as an indispensable member of the team with career growth opportunities tailored to your interests and talents. We want you to be eager to take on a new challenge. We are always 100% honest about what to expect, because we don’t want Daugherty to be just another job; we want Daugherty to be your dream job.\xa0', 'Interest in Python, R, sh/bash and JVM-based languages including Scala and Java.', 'Little to no travel.', 'Implement & automate high-performance algorithms, prototypes and predictive models.', 'Excellent health, dental and vision insurance.Revenue sharing and a 401(k) retirement savings plan.Life, disability and long-term care insurance.Little to no travel.Robust career development and training.', 'As a Data Engineer you will have the opportunity to:', 'Robust career development and training.', 'Knowledge of cloud and distributed systems principles, including load balancing, networks, scaling, and in-memory versus disk.', 'Proven ability to pick up new languages and technologies quickly.', 'We are looking for someone with:', 'Assemble large, complex data sets that meet functional/non-functional business requirements.', 'Familiarity with high performance data libraries including Spark, NumPy and TensorFlow.', '\xa0', 'Intermediate level of SQL programming and query performance tuning techniques for data integration and consumption using design for optimum performance against large data assets within an OLTP, OLAP and MPP architecture.', 'Create custom software components (e.g. specialized UDFs) and analytics applications.', 'Excellent health, dental and vision insurance.', 'Team Daugherty is hiring a Data Engineer to join us in St. Louis. The ideal candidate for this position is a problem solver with the ability to utilize insights, creativity and perspective to drive business success for our clients.', 'Experience building data pipelines to connect analytics stacks, client data visualization tools and external data sources.', 'Work with team members and functional leads to understand existing data requirements and validation rules to support moving existing data warehouse workloads into a distributed data platform.', 'Contribute to the creation and maintenance of optimal data pipeline architectures.Collaborate and work closely with team to build data platforms.Maintain and manage Hadoop clusters in development and production environments.Assemble large, complex data sets that meet functional/non-functional business requirements.Work with team members and functional leads to understand existing data requirements and validation rules to support moving existing data warehouse workloads into a distributed data platform.Create custom software components (e.g. specialized UDFs) and analytics applications.Employ a variety of languages and tools to marry systems together.Recommend ways to improve data reliability, efficiency and quality.Implement & automate high-performance algorithms, prototypes and predictive models.']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
"Data Engineer, Data Science & Analytics",Earnin,"Palo Alto, CA",1 month ago,Be among the first 25 applicants,"['', 'Build robust and reliable data products that enable automated reporting, experimentation, A/B testing, anomaly detection, and root cause analysis.', 'Experience with data modeling in Redshift is a big plus.', 'Experience with workflow orchestration tools like Airflow.', 'Curiosity and a drive to learn. Willingness to be assertive and drive solutions independently. ', 'Substantial experience working at a startup. ', 'Work with data scientists and analysts to productionalize model deployments and pipelines.', 'Hands-on experience working with a varied set of data storage technologies (e.g. Mysql, Postgres, DynamoDB, S3, etc.). You know where and when to use each. ', 'Experience building, deploying, maintaining, and tuning Spark-based applications.', 'Substantial experience with testing, data validation, and data quality assurance.', ""What We're Looking For"", 'Work cross functionally with other teams (data science, design, product, marketing, and analytics) in high visibility roles.', '4+ years of experience working with analytical data systems and building production applications. ', 'Experience with BI tools like Looker, Tableau, and Periscope.', 'We are a collaborative team and genuinely enjoy working with each other. ', 'Design and build massively scalable, production-grade data services and pipelines that power machine learning model development and actionable analytics.', 'We’re building a product that inspires fairness across the financial world.', 'Experience deploying microservices and jobs on Kubernetes infrastructure is a big plus.', 'Experience building and deploying machine learning models is a big plus.', 'Experience with physical data modeling on cloud storage: file formats, compression, partitioning strategies, etc. ', 'What Sets Us Apart', 'Experience working with alerting and monitoring tools like DataDog and PagerDuty.', ' High impact roles at a relatively small company that’s aggressively growing our user base.  We are a collaborative team and genuinely enjoy working with each other.  We believe in empowering our people to be successful. We’re building a product that inspires fairness across the financial world. ', 'About The Team', 'About Earnin', ' Translate complex, open-ended problems into elegant design and build high quality, maintainable data products and tools. Design and build massively scalable, production-grade data services and pipelines that power machine learning model development and actionable analytics. Build robust and reliable data products that enable automated reporting, experimentation, A/B testing, anomaly detection, and root cause analysis. Work with data scientists and analysts to productionalize model deployments and pipelines. Champion data quality and governance throughout Earnin by maintaining and extending a clean data ecosystem for the company.  Actively engage and drive design reviews and code reviews.  Work cross functionally with other teams (data science, design, product, marketing, and analytics) in high visibility roles. Communicate the tradeoffs of technical decisions to multiple stakeholders, including non-technical audiences. ', 'High impact roles at a relatively small company that’s aggressively growing our user base. ', ' BS or MS degree in Computer Science, Engineering, or a related technical field. Excellent written and verbal communication skills, including the ability to identify and communicate data driven insight.  Curiosity and a drive to learn. Willingness to be assertive and drive solutions independently.  4+ years of development experience in a fast-paced environment. 4+ years of experience working with analytical data systems and building production applications.  Advanced knowledge of analytical SQL and data modeling.  Strong Python programming skills. Experience building, deploying, maintaining, and tuning Spark-based applications. Taking pride in your code quality and helping others elevate their own code quality. Substantial experience developing production ETL processes.  Substantial experience with testing, data validation, and data quality assurance. Hands-on experience working with a varied set of data storage technologies (e.g. Mysql, Postgres, DynamoDB, S3, etc.). You know where and when to use each.  Experience with physical data modeling on cloud storage: file formats, compression, partitioning strategies, etc.  Experience with cloud data platforms like Snowflake or Databricks and/or cloud data warehouses like Redshift.  Experience with BI tools like Looker, Tableau, and Periscope. ', 'Taking pride in your code quality and helping others elevate their own code quality.', 'Experience using query engines like Athena, Presto, and Impala is a big plus.', 'Advanced knowledge of analytical SQL and data modeling. ', 'Excellent written and verbal communication skills, including the ability to identify and communicate data driven insight. ', 'Experience working with streaming infrastructure like AWS Kinesis and/or Kafka is a big plus.', 'Experience with cloud data platforms like Snowflake or Databricks and/or cloud data warehouses like Redshift. ', 'We believe in empowering our people to be successful.', 'BS or MS degree in Computer Science, Engineering, or a related technical field.', ' Experience building and deploying machine learning models is a big plus. Experience working with streaming infrastructure like AWS Kinesis and/or Kafka is a big plus. Experience with data modeling in Redshift is a big plus. Experience using query engines like Athena, Presto, and Impala is a big plus. Experience using Terraform is a big plus.  Experience deploying microservices and jobs on Kubernetes infrastructure is a big plus. Experience working with alerting and monitoring tools like DataDog and PagerDuty. Substantial experience working at a startup.  Experience building and deploying AWS Lambda applications. Experience with workflow orchestration tools like Airflow. ', 'Communicate the tradeoffs of technical decisions to multiple stakeholders, including non-technical audiences.', ""What You'll Do"", 'Translate complex, open-ended problems into elegant design and build high quality, maintainable data products and tools.', 'Actively engage and drive design reviews and code reviews. ', 'Champion data quality and governance throughout Earnin by maintaining and extending a clean data ecosystem for the company. ', '4+ years of development experience in a fast-paced environment.', 'Substantial experience developing production ETL processes. ', 'Strong Python programming skills.', 'Nice To Haves', 'Experience building and deploying AWS Lambda applications.', 'Experience using Terraform is a big plus. ']",Entry level,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Engineer,Aptonet Inc,"St Louis, MO",4 weeks ago,51 applicants,"['', 'Required Skills', 'Prior experience with designing JSON and/or AVRO data schemas is required. We are a cloud first organization and will require general fluency in writing efficient SQL queries, basic understanding of SQL performance tuning, and experience with translating relational data models into JSON/AVRO schema objects.\xa0', 'Experience with at least one cloud technology like AWS, Azure, Google Cloud(AWS preferred)\xa0', 'Our ideal candidate takes pride in software craftsmanship, is curious, proactive, ego-less and collaborative. He/she must be able to think through hard problems and work with an autonomous team to make them reality.\xa0', 'Experience working with JSON/AVRO schema\xa0', 'Strong Java experience is a big plus', 'Nice To Have', 'Experience in using oAuth\xa0', '\u200bWe\xa0expect developers to challenge the status quo and bring new ideas. You must be proficient in modern API development practices and common deployment patterns using mostly open source toolsets and cloud technologies such as AWS, Jenkins, Docker, CloudFoundry/Fargate, Scala Akka, Play.\xa0', 'Position: Scala/Kafka Engineer', 'Experience with CI/CD pipeline and related tools like Jenkins, Docker\xa0', 'You will be working with several teams to source data and business rules and developing RESTful APIs hosted in AWS native services and CloudFoundry/Fargate to deliver refined data sets that standardize identifiers, codes, business logic and data schemas across multiple lines of business.\xa0', ""Biotech Company is the leading supplier of cutting-edge biotechnology and innovative agricultural products that enhance the livelihood of a global farming community, ensure resilient food supplies for a growing population, and reduce the environmental impact of agriculture. We are looking for a Senior Technical Data Engineer/Application Developer to join our Event360 team to help build global business focused APIs that incorporate data sourced from RDBMS, streaming, API, and 3rd party supplied files.\xa0You will be working with several teams to source data and business rules and developing RESTful APIs hosted in AWS native services and CloudFoundry/Fargate to deliver refined data sets that standardize identifiers, codes, business logic and data schemas across multiple lines of business.\xa0You will also be responsible for maintaining existing applications, enhancing our infrastructure and solving the challenging problems that come across.\xa0The candidate should demonstrate passion and ownership by embracing all aspects of software development: analysis, development, testing and deployment.\xa0Our ideal candidate takes pride in software craftsmanship, is curious, proactive, ego-less and collaborative. He/she must be able to think through hard problems and work with an autonomous team to make them reality.\xa0\u200bWe\xa0expect developers to challenge the status quo and bring new ideas. You must be proficient in modern API development practices and common deployment patterns using mostly open source toolsets and cloud technologies such as AWS, Jenkins, Docker, CloudFoundry/Fargate, Scala Akka, Play.\xa0Prior experience with designing JSON and/or AVRO data schemas is required. We are a cloud first organization and will require general fluency in writing efficient SQL queries, basic understanding of SQL performance tuning, and experience with translating relational data models into JSON/AVRO schema objects.\xa0Requires a Bachelor's or Master's degree in Computer Science or related field and 5-10 years professional experience in related areas."", 'Strong Java experience is a big plusExperience with Kafka/Kafka Connect is a huge plus\xa0Experience in using oAuth\xa0Experience in writing test cases or TDD\xa0Setting technical direction, patterns, and coding standards\xa0\u200bGood understanding of logging and alerting; tools like Splunk, Datadog Updating\xa0', 'The candidate should demonstrate passion and ownership by embracing all aspects of software development: analysis, development, testing and deployment.\xa0', '\u200bExperience in working with Elasticsearch or similar document store\xa0', 'Setting technical direction, patterns, and coding standards\xa0', 'Strong experience in Scala\xa0Building Microservices/REST APIs using Scala libraries/frameworks like Akka Http/Play\xa0Experience working with JSON/AVRO schema\xa0Strong experience in building Kafka Producer and Consumer applications\xa0Experience with one of RDBMS systems\xa0Experience with CI/CD pipeline and related tools like Jenkins, Docker\xa0Experience with at least one cloud technology like AWS, Azure, Google Cloud(AWS preferred)\xa0\u200bExperience in working with Elasticsearch or similar document store\xa0', 'Duration: Long Term W2 contract', 'Experience with Kafka/Kafka Connect is a huge plus\xa0', '\u200bGood understanding of logging and alerting; tools like Splunk, Datadog Updating\xa0', 'Strong experience in building Kafka Producer and Consumer applications\xa0', 'Experience in writing test cases or TDD\xa0', 'Biotech Company is the leading supplier of cutting-edge biotechnology and innovative agricultural products that enhance the livelihood of a global farming community, ensure resilient food supplies for a growing population, and reduce the environmental impact of agriculture. We are looking for a Senior Technical Data Engineer/Application Developer to join our Event360 team to help build global business focused APIs that incorporate data sourced from RDBMS, streaming, API, and 3rd party supplied files.\xa0', ""Requires a Bachelor's or Master's degree in Computer Science or related field and 5-10 years professional experience in related areas."", 'Building Microservices/REST APIs using Scala libraries/frameworks like Akka Http/Play\xa0', 'Strong experience in Scala\xa0', 'Job Description', '\xa0', 'Location: Must be local to Creve Coeur OR willing to relocate after the pandemic is over', 'Experience with one of RDBMS systems\xa0', 'You will also be responsible for maintaining existing applications, enhancing our infrastructure and solving the challenging problems that come across.\xa0']",Mid-Senior level,Full-time,Engineering,Pharmaceuticals,2021-03-18 14:34:51
Senior Data Engineer,"Pyramid Consulting, Inc","McLean, VA",6 days ago,33 applicants,"['', '• Leverage PySpark and AWS EMR to parallelize the process and reduce the runtime.', '• Enhance the Python process to be fault-tolerant and contain checkpoints to make rerunning a subset of the process more efficient.', '• Experience writing automated unit, integration, regression, performance and acceptance tests', '• Experience with SAS', '• Write automated tests for Python code.', '• Strong quantitative skills (statistics, econometrics, linear algebra)', 'We are looking Senior Data Engineer Someone good with SAS code into Python code. We are using both Pandas data frames and PySpark data frames so knowledge of both is required. Must have exp…with Banking/Finance/ mortgage industry.', '• Optimize the Python code to reduce the runtime.', '• A strong understanding of SQL', '• Experience with cloud computing and storage services, particularly AWS EMR', '• Translate existing SAS code into Python code. We are using both Pandas data frames and PySpark data frames so knowledge of both is required.', 'Responsibilities', '• Solid understanding of software design principles', '• Verify that the Python version of the SAS code is equivalent to the SAS version. This involves running both processes, comparing the output, and resolving any differences.', 'Qualifications', '• A strong understanding of Pandas and PySpark', 'Preferred Skills', '• BS in Computer Science or equivalent experience', '• At least 3 years of experience developing production Python code', '• Peer review code and automated tests, help team members with design and implementation challenges.']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-03-18 14:34:51
AWS Data Engineer,DTI (Diversified Technology Inc.),"Baltimore, MD",,N/A,"['', 'You will be responsible for the technical delivery of the project on AWS using tools like Amazon EMR, Amazon Redshift, Glue, Apache Spark, Informatica BDM.', 'Design, build, and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties- Spark, EMR, RedShift, Informatica BDM, and Glue', 'Skill', 'Collaborate with Data Architects and Solution Architects to understand data requirements and design scalable and highly performant data and analytical solutions', 'Communicate all technical decisions, issues, and risks to IT leadership and manage technical dependencies within or external to the program', 'AWS Analytics Services', 'Are you an AWS Data Engineer looking for your next long-term project? If so we would like to speak to you! DTI has an immediate need for an AWS Data Engineer for a 1year contract.', 'Informatica BDM', 'AWS Data Engineer', 'Spark', 'Expectations', 'Design, build, and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties- Spark, EMR, RedShift, Informatica BDM, and GlueAnalyze, re-engineer, and re-platform on-premise data warehouses to data platforms on AWS cloud using AWS or 3rd party servicesCollaborate with Data Architects and Solution Architects to understand data requirements and design scalable and highly performant data and analytical solutionsCommunicate all technical decisions, issues, and risks to IT leadership and manage technical dependencies within or external to the program', 'Remote opportunity', 'Consultant must work on our W2', 'Analyze, re-engineer, and re-platform on-premise data warehouses to data platforms on AWS cloud using AWS or 3rd party services']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Team Cymru,"Lake Mary, FL",5 days ago,Be among the first 25 applicants,"['', 'Virtual', 'Working knowledge of networking protocols', 'Experience with one or alternative database technologies like: ElasticSearch, Apache Cassandra, Mongo DB, Spark', 'Additional Desired Skills/Abilities', 'Contributes to efforts in maintaining and improving product quality', 'Creates quality product and support documentation', 'Job Summary', ' Prolonged periods of sitting at a desk and working on a computer. Must be able to travel up to 5% of the time. ', "" Familiarity with design patterns and industry best practices Experience with one or alternative database technologies like: ElasticSearch, Apache Cassandra, Mongo DB, Spark Experience with Cloud technologies like: AWS, Google Cloud, Azure Ability to effectively create and utilize REST APIs Proactively creates automated analytics solutions to push team's capabilities and increased situational awareness Knowledge of MVC frameworks Ability to execute complex queries and design relational databases in PostgreSQL using referential integrity, views, stored procedures and proper indices Ability to create visualizations from resultant analytic results Experience creating and distributing Jupyter Notebooks for repeatable data analysis "", "" Proficiency in designing and developing innovative data analytics software and methods. Contributes across whole project lifecycle, utilizing peers for guidance where necessary. Operate independently and seeks assistance or guidance when required. Ability to recognize trends and patterns in the data that can be exploited into a repeatable analysis process Performs triage of product support requests, problem determination and assists with escalation when appropriate Demonstrates a complete understanding of a core-product or service offering's features, construction and operating characteristics Incorporates effective test procedures, logging and monitoring in software with minimal oversight Participates in regular review of individual output to ensure it conforms to department and company standards Contributes to efforts in maintaining and improving product quality identification and submission of product improvement when appropriate Creates quality product and support documentation Identifies risks to projects, communicates and formulates mitigation plans Actively contributes to cross-functional team efforts Conducts self-assessments by comparing required skills with existing knowledge to develop, present and execute plans for improvement Consistently delivers to deadlines at the required quality standards "", 'High school diploma or equivalent required', 'Proficient in the use of databases: query and data definition', 'Typically has two to four years combined industry / education experience', 'Able to break-down complex requirements into workflows and identify key performance indicators.', 'Embodies and demonstrates maturity, professionalism, and ethics', 'Articulate in oral and written communication', 'Solid oral and written communications skills', ' High school diploma or equivalent required Typically has two to four years combined industry / education experience Some specialized training or education beyond high school is preferred ', 'None.', 'Proficient in the use of industry standard tooling (i.e. the Atlassian Stack, etc.)', 'Performs triage of product support requests, problem determination and assists with escalation when appropriate', 'Knowledge of MVC frameworks', 'Ability to create visualizations from resultant analytic results', 'Ability to recognize trends and patterns in the data that can be exploited into a repeatable analysis process', 'Ability to effectively create and utilize REST APIs', 'Ability to execute complex queries and design relational databases in PostgreSQL using referential integrity, views, stored procedures and proper indices', 'Competent with Linux', 'Working-level knowledge of algorithms', 'Some specialized training or education beyond high school is preferred', 'identification and submission of product improvement when appropriate', 'Education And Experience', 'Location:', 'Proficiency in one or more core languages: Golang, Python, SQL, Bash, Perl', 'Participates in regular review of individual output to ensure it conforms to department and company standards', 'Incorporates effective test procedures, logging and monitoring in software with minimal oversight', 'Familiarity with design patterns and industry best practices', 'Experience with Cloud technologies like: AWS, Google Cloud, Azure', ' None. ', 'Prolonged periods of sitting at a desk and working on a computer.', 'Supervisory Responsibilities', 'Must be able to travel up to 5% of the time.', 'Demonstrates sound coding techniques', 'Contributes across whole project lifecycle, utilizing peers for guidance where necessary.', ' Proficient in the use of industry standard tooling (i.e. the Atlassian Stack, etc.) ', 'Required Skills/Abilities', ' Embodies and demonstrates maturity, professionalism, and ethics Articulate in oral and written communication Working-level knowledge of algorithms Demonstrates sound coding techniques Able to break-down complex requirements into workflows and identify key performance indicators. Proficient in the use of databases: query and data definition Proficiency in one or more core languages: Golang, Python, SQL, Bash, Perl ', 'Data Engineer', 'Operate independently and seeks assistance or guidance when required.', 'Physical Requirements', 'Consistently adheres to commitments with respect to delivery and timeframe', ""Proactively creates automated analytics solutions to push team's capabilities and increased situational awareness"", ' Competent with Linux Solid oral and written communications skills Consistently adheres to commitments with respect to delivery and timeframe Working knowledge of networking protocols ', 'Experience creating and distributing Jupyter Notebooks for repeatable data analysis', 'Actively contributes to cross-functional team efforts', 'Identifies risks to projects, communicates and formulates mitigation plans', 'Proficiency in designing and developing innovative data analytics software and methods.', 'Conducts self-assessments by comparing required skills with existing knowledge to develop, present and execute plans for improvement', 'Duties/Responsibilities', ""Demonstrates a complete understanding of a core-product or service offering's features, construction and operating characteristics"", 'Consistently delivers to deadlines at the required quality standards']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,JupiterOne,Salt Lake City Metropolitan Area,3 weeks ago,Be among the first 25 applicants,"['', 'Experience with big data tools such as Hadoop, Athena, Apache Hive, and Apache Spark', 'Apache Spark', 'Able to guide other engineers on data engineering best practices', 'Docker', 'Serverless (AWS Lambda and AWS Fargate)', 'Redis', ' Key Qualifications ', 'Able to make practical decisions regarding data storage and retrieval', 'Able to work independentlyExperience working with graph databasesExperience working with micro-servicesExperience with DevOps automation and Infrastructure as Code with tools like Terrafom or CloudFormationWilling to explore new approaches to solving problems and challenging the status quoEager to improve processes via automation', 'Experience working with micro-services', 'Terraform', 'Willing to explore new approaches to solving problems and challenging the status quo', 'Python', 'Able to write production code that is reliable and easy to support', 'Experience working with graph databases', 'AWS (Athena, Neptune, Lambda, API Gateway, DynamoDB, Kinesis, SQS, S3, Comprehend, etc.)', 'Elasticsearch', 'Apache Hadoop', 'Able to help architect a data ingestion and query pipeline that enables efficient and cost efficient data science use cases', 'JVM languages (Java, Scala, etc.)', 'GraphQL', 'SQL', 'JVM languages (Java, Scala, etc.)PythonSQLNode.jsTypeScriptApache SparkApache HiveApache HadoopServerless (AWS Lambda and AWS Fargate)DockerAWS (Athena, Neptune, Lambda, API Gateway, DynamoDB, Kinesis, SQS, S3, Comprehend, etc.)GraphQLElasticsearchRedisTerraform', '3+ years of experience in a Data Engineering roleExperience with big data tools such as Hadoop, Athena, Apache Hive, and Apache SparkAble to write production code that is reliable and easy to supportAble to guide other engineers on data engineering best practicesUnderstands the importance of maintaining customer data privacy and security complianceAble to make practical decisions regarding data storage and retrievalAble to help architect a data ingestion and query pipeline that enables efficient and cost efficient data science use cases', 'Experience with DevOps automation and Infrastructure as Code with tools like Terrafom or CloudFormation', '3+ years of experience in a Data Engineering role', 'Able to work independently', 'Node.js', 'Eager to improve processes via automation', 'TypeScript', 'Apache Hive', 'Understands the importance of maintaining customer data privacy and security compliance']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer (m/w/d) in Flensburg // remote,MAC IT-Solutions GmbH,"Flensburg, MN",21 hours ago,Be among the first 25 applicants,"['', ' Du verfügst über ein abgeschlossenes Studium (Informatik,', ' Azure Synapse Analytics', ' FlensburgÜber unsDie MAC IT-Solutions GmbH ist ein Spezialdienstleister für den Versand- undOnlinehandel. Das besondere Augenmerk richtet sich auf die Entwicklung undImplementierung von ERP Software für den unter Multichannelzusammengefassten klassischen Versandhandel, den Online- sowie denstationären Handel. Die MAC versteht sich als kompetenter Lösungspartnerder Anforderungen ihrer Kunden und ist langjähriger zertifizierterMicrosoft Gold Certified Partner und IBM Advanced Business Partner. Die MACist europaweit tätig. Über 100 Versandhandelsunternehmen arbeiten in 13europäischen Ländern mit den Softwarelösungen von MAC. Darunter sindzahlreiche namhafte Versender sowie einige Unternehmen der Top 100 desDeutschen Versandhandels.Die Standorte der MAC befinden sich in Flensburg, Kiel und Hamburg. DieGründung erfolgte im Jahre 2002 durch den heutigen Geschäftsführer DirkWieland. Derzeit beträgt die Mitarbeiterzahl über 120 Personen mitwachsender Tendenz.Wir suchen dich!Du hast ein Talent für die Konzeption von effizienten DataWarehouse-Architekturen? Es macht Dir Spaß, Datenbanken zu modellieren, zuskalieren und so den Datenfluss sicherzustellen? Du hast eine Leidenschaftfür gut strukturierte Systementwicklung? Dann bist Du bei uns genaurichtig. Wir bieten Dir ein spannendes Arbeits- und Projektumfeld imBereich Data Engineering in unserem Team in Flensburg.Unsere Teams arbeiten remote. Wenn es für eine Phase der Zusammenarbeitsinnvoll ist *, kommen die Teammitglieder in den Räumen der MAC zusammen.Du solltest die Möglichkeit haben in unsere Büros in Flensburg zu kommen.(* und Corona es zulässt)Deine Aufgaben', ' Idealerweise hast du bereits Erfahrungen im Microsoft Azure-Umfeld', ' Du hast praktische Erfahrungen im Umgang mit SQL (Structured Query', ' Du bist teamfähig, engagiert und scheust dich nicht, Verantwortung zu', ' SSIS / SSAS', ' Visual Studio', ' Du überzeugst durch Deine analytische und konzeptionelle', ' Azure Data Factory', ' Azure DevOps', ' Du begleitest die Umsetzung einer Data Warehouse-Architektur von der', ' Power BI', ' Du entwirfst und optimierst Datenarchitekturen inkl. der', ' Azure Data Lake Storage Gen 2', ' Du bringst Erfahrungen in den Bereichen Data Warehouse und', ' Azure SQL Databases', 'Wir freuen uns über Zertifizierungen für den Azure Data Analyst']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,CareJourney,"Arlington, VA",2 weeks ago,41 applicants,"['', 'Preferred', ' 3 to 6 years of experience working in at least two different positions in Data Engineering or related roles. A degree in Computer Science, Data Engineering, or equivalent. Experience with ETL/ELT, data analytics, and building data pipelines. 3+ years experience with SQL, Snowflake a plus. 2+ years experience with Python. Hands on experience with Spark, AWS Glue a plus.', ' Experience with ETL/ELT, data analytics, and building data pipelines.', ' A degree in Computer Science, Data Engineering, or equivalent.', ' Support and develop data flows by developing processes that verify, standardize, and scale data input, transformation and storage. Build and maintain CareJourney’s data pipeline orchestration system. Ensure the reliability and performance of computational and orchestration code. Research and implement cutting edge solutions to solve challenges related to ETL/ELT, data processing, and analytics.', ' Experience with data pipeline orchestration tools, Apache Airflow.', ' Research and implement cutting edge solutions to solve challenges related to ETL/ELT, data processing, and analytics.', 'Requirements', ' Experience working in cloud environments, preferably AWS.', ' Hands on experience with Spark, AWS Glue a plus.', ' Experience with data pipeline orchestration tools, Apache Airflow. Experience working in an Agile environment. Experience working in cloud environments, preferably AWS. Experience with microservices and serverless technologies. Experience with CI/CD systems and environments.', ' Support and develop data flows by developing processes that verify, standardize, and scale data input, transformation and storage.', 'Key Responsibilities', ' Ensure the reliability and performance of computational and orchestration code.', ' 2+ years experience with Python.', ' 3 to 6 years of experience working in at least two different positions in Data Engineering or related roles.', ' 3+ years experience with SQL, Snowflake a plus.', ' Experience working in an Agile environment.', ' Experience with CI/CD systems and environments.', ' Experience with microservices and serverless technologies.', ' Build and maintain CareJourney’s data pipeline orchestration system.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Palnar,"Virginia, United States",7 days ago,Be among the first 25 applicants,"['', 'Ubair Anwaar,Cell:e:', 'Cell:', 'They Should Also Have Experience Using The Following Software/tools', 'Job Description', 'Ubair Anwaar,', 'e:']",Entry level,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Developer/Engineer,M&T Bank,"Buffalo, NY",1 month ago,Be among the first 25 applicants,"['About M&T', ' Technology Stack ', ' Primary Responsibilities: ', 'Primary Responsibilities:', 'Combine data visualizations, pipelines and repositories into services that deliver value to customers. ', ' Python ', ' Elastic stack  Python  MS SQL  Tableau Desktop/Server ', 'Data Developer/Engineer', 'The Technology Infrastructure Engineering and Operations Team engineers and supports all the foundational technology used across M&T Bank. We support two Data Centers, 15,000 Distributed platforms, 24 Mainframe Regions, a secure and reliable Network connecting all the technology across the bank footprint and process approximately 635,000 automated jobs per month. In addition to that foundational technology level, we own the operational support processes, monitoring and command center that provide a unified, well-managed technology response when issues arise. We deliver innovative, and compelling technology solutions, in partnership with our line of business CIOs, to enhance the internal customer experience that enables our business partners to deliver.The Capacity Management and Analytics team assists application, network and operations team in making critical decisions though data engineering and analytics.The Data Developer/Engineer is a senior position in the Capacity Management and Analytics team, leading initiatives, mentoring other team members and driving innovation. Technology Stack  Elastic stack  Python  MS SQL  Tableau Desktop/Server  Primary Responsibilities: Combine data visualizations, pipelines and repositories into services that deliver value to customers. Engage with customers regularly to understand needs and incorporate feedback into services. Support service delivery platforms, ensuring SLAs are met. As part of continuous learning the Data Developer/Engineer will drive opportunities to automate, improve or expand services. Education and Experience Required: Combined minimum of 6 years’ higher education and/or work experience in systems design, management and/or architectureStrong understanding of the system development and infrastructure lifecycle and architecture, vendor best practices, IT Service Management and systems designAnalyzing and interpreting dataset in multiple formats (CSV, JSON, XML)Experience with structured and unstructured data repositoriesLeveraging Tableau or other Business Intelligence tool to visualize data for customersUnix administrationEducation And Experience PreferredBachelor’s Degree in Computer Science or Computer Engineering, and a minimum of 8 years’ professional experience in a technical engineering position involving infrastructure design technologies, data management and interchange, system design and/or development for complex applicationsAbility to translate complex business/functional requirements into structured high quality implementations using any variety of industry standard approachesDemonstrated advanced technical and analytical, troubleshooting and problem solving skillsAbility to work both independently and collaboratively with others in team environmentExcellent written and verbal communication skillsAbility to complete complex tasks with minimal supervisionExperience with Agile MethodologyTrack record of building and deploying solutionsDeveloping and promoting code using Gitlab or other code management platformMicrosoft SQL administration At M&T Tech  , we’re a team of makers, doers, and builders, working to create the most advanced technology solutions in banking. We’re not your stereotypical suit and tie bankers: we’re an innovative team of leading tech experts, pushing boundaries, and taking risks. We’re building an agile team of the most skilled and creative workers to solve complex problems, architect solutions, write high-performance software, and chart our new path, all to make the lives of our customers, and the communities that we serve, better. Join us and be part of something new as we build tomorrow’s bank, today.About M&TM&T Bank is a Top 20 US bank holding company and one of the best performing and financial stable regional banks in the country, we offer our technology employees a wide range of performance-based career development opportunities. We have a strong commitment to our customers and the communities we serve, and we continue to grow with a focus on the future. So, when looking to advance your career, look to M&T. Grow with us.#Unix, #Python, #Tableau, #BI, #DatasetsLocationBuffalo, New York, United States of America', 'Support service delivery platforms, ensuring SLAs are met. ', 'The Technology Infrastructure Engineering and Operations Team', 'Location', 'Combine data visualizations, pipelines and repositories into services that deliver value to customers. Engage with customers regularly to understand needs and incorporate feedback into services. Support service delivery platforms, ensuring SLAs are met. As part of continuous learning the Data Developer/Engineer will drive opportunities to automate, improve or expand services.', 'Engage with customers regularly to understand needs and incorporate feedback into services. ', ' At M&T Tech ', ' Education and Experience Required: ', ' Required:', 'Overview', 'As part of continuous learning the Data Developer/Engineer will drive opportunities to automate, improve or expand services.', 'Education and ', ' Tableau Desktop/Server ', 'Education And Experience Preferred', ' MS SQL ', ' Elastic stack ']",Not Applicable,Full-time,Information Technology,Banking,2021-03-18 14:34:51
Data Engineer,"Frontdoor, Inc.","Denver, CO",3 weeks ago,31 applicants,"['', 'Desired Skills', ' Enforce production standards and governance best practices in the management of enterprise-level data, metrics, and reports.Qualifications', 'Build and support complex ETL infrastructure to deliver clean and reliable data to the organization.', ' Build and maintain scalable data pipelines for both batch and stream processing in a cloud-computing environment.', ' Interact face-to-face with business stakeholders, develop cooperative relationships, and acquire domain knowledge of the business.', ' Optimize database architecture by trading off storage and computation to achieve low cost and high performance.', ' Demonstrable proficiency in Python development and advanced SQL querying.', ' Build and maintain scalable data pipelines for both batch and stream processing in a cloud-computing environment. Apply dimensional modeling to design tables and views that map business processes into an enterprise data model. Optimize database architecture by trading off storage and computation to achieve low cost and high performance.Build and support complex ETL infrastructure to deliver clean and reliable data to the organization.Support the development of new products and services via ingestion, processing, and formatting data for reporting and analytics. Interact face-to-face with business stakeholders, develop cooperative relationships, and acquire domain knowledge of the business.Proactively automate manual processes throughout the business for higher efficiency, robustness, and speed. Enforce production standards and governance best practices in the management of enterprise-level data, metrics, and reports.QualificationsBachelor’s in Computer Science, Engineering, Data Science, or related field (Masters or PhD preferred)Excellent communication and inter-personal skills. Versatile and quick learner with ability to pick up any new skills necessary to get the job done.Demonstrated strength in data modeling, ETL development, data warehousing, data pipeline and data lake creation. Extensive experience with cloud infrastructure and tools for AWS and GCP. Demonstrable proficiency in Python development and advanced SQL querying. Strong grasp of business elements and ability to convert requirements into database models and full-data pipeline systems.Experience with visualization and reporting tools, such as Looker and Tableau.3-4 years of experience in data engineering or similar work.Desired Skills Snowflake experience (highly desirable). Full stack experience, including microservice and web app development. Knowledge of big data platforms, such as Hadoop and Spark.Frontdoor is a company that’s obsessed with taking the hassle out of owning a home. With services powered by people and enabled by technology, it is the parent company of four home service plan brands: American Home Shield, HSA, Landmark and OneGuard, as well as AHS Proconnect , an on-demand membership service for home repairs and maintenance, and Streem, a technology company that enables businesses to serve customers through an enhanced augmented reality, computer vision and machine learning platform. Frontdoor serves more than two million customers across the U.S. through a network of more than 16,000 pre-qualified contractor firms that employ over 45,000 technicians. The company’s customizable home service plans help customers protect and maintain their homes from costly and unexpected breakdowns of essential home systems and appliances. With nearly 50 years of experience, the company responds to over four million service requests annually (or one request every eight seconds).For more details, visit frontdoorhome.com.Job Category: EngineeringID: R0015252', '3-4 years of experience in data engineering or similar work.Desired Skills', 'Demonstrated strength in data modeling, ETL development, data warehousing, data pipeline and data lake creation.', 'Responsibilities', ' Apply dimensional modeling to design tables and views that map business processes into an enterprise data model.', 'Support the development of new products and services via ingestion, processing, and formatting data for reporting and analytics.', 'Excellent communication and inter-personal skills.', 'Proactively automate manual processes throughout the business for higher efficiency, robustness, and speed.', 'Qualifications', ' Versatile and quick learner with ability to pick up any new skills necessary to get the job done.', ' Knowledge of big data platforms, such as Hadoop and Spark.Frontdoor is a company that’s obsessed with taking the hassle out of owning a home. With services powered by people and enabled by technology, it is the parent company of four home service plan brands: American Home Shield, HSA, Landmark and OneGuard, as well as AHS Proconnect , an on-demand membership service for home repairs and maintenance, and Streem, a technology company that enables businesses to serve customers through an enhanced augmented reality, computer vision and machine learning platform. Frontdoor serves more than two million customers across the U.S. through a network of more than 16,000 pre-qualified contractor firms that employ over 45,000 technicians. The company’s customizable home service plans help customers protect and maintain their homes from costly and unexpected breakdowns of essential home systems and appliances. With nearly 50 years of experience, the company responds to over four million service requests annually (or one request every eight seconds).For more details, visit frontdoorhome.com.Job Category: EngineeringID: R0015252', ' Full stack experience, including microservice and web app development.', ' Strong grasp of business elements and ability to convert requirements into database models and full-data pipeline systems.', ' Snowflake experience (highly desirable).', 'Experience with visualization and reporting tools, such as Looker and Tableau.', ' Extensive experience with cloud infrastructure and tools for AWS and GCP.', 'Frontdoor is looking for a very strong data engineer who will bring a mindset of automation and innovation to the table. In addition to sharp technical skills, this person must be a strong communicator and collaborator who can partner up with business stakeholders to understand their needs and solve their data problems. As a data engineer at Frontdoor, you will be working in a fast-paced environment and using cutting-edge cloud technologies to develop a scalable data platform that will support years of company growth.', 'Bachelor’s in Computer Science, Engineering, Data Science, or related field (Masters or PhD preferred)']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer - Azure,OmniData,"Portland, OR",4 days ago,Be among the first 25 applicants,"['Times New Roman>, serif"">Azure Data Lake', 'Times New Roman>, serif"">Must be decisive and show ability to work with clients and assist their business and technical decision-making.', 'Times New Roman>, serif"">Execution of team-leading Solution Architect and Data Architect vision for client success, with Azure based data warehouse and business intelligence tools.', 'Times New Roman>, serif"">Willingness to travel (post-COVID) and work with a consultant\'s diligence.', 'Times New Roman>, serif"">Responsibilities and Duties', 'Times New Roman>, serif"">Microsoft SQL base Data SkillsTimes New Roman>, serif"">SSISTimes New Roman>, serif"">SQL ServerTimes New Roman>, serif"">SQL Server Analysis Services (Tabular & Multi-Dimensional)', 'Times New Roman>, serif"">SQL Server', 'Times New Roman>, serif"">2+ years of experience in Analytics and Data Warehousing, with Azure knowledge a plus.  Times New Roman>, serif"">Data Warehouse Automation experience ', 'Times New Roman>, serif"">You must be humble, hungry and a fast learner.  OmniData and our clients place a high value on inventiveness.', 'Times New Roman>, serif"">Azure Analysis Services', 'Times New Roman>, serif"">Job Summary', 'Times New Roman>, serif"">Power BI, DAX', 'Times New Roman>, serif"">SSIS', 'Times New Roman>, serif"">Benefits and Perks', 'Times New Roman>, serif"">About OmniData', 'Times New Roman>, serif"">OmniData Is An Equal Opportunity Employer And All Qualified Applicants Will Receive Consideration For Employment Without Regard To Race, Color, Religion, Sex, National Origin, Disability Status, Protected Veteran Status, Or Any Other Characteristic Protected By Law.', 'Times New Roman>, serif"">Great communication skills tying technologies and architectures to business results will advance your position.', 'Times New Roman>, serif"">Requirements Analysis and Project Delivery methodology', 'Times New Roman>, serif"">Salary and benefits commensurate with experience.  Times New Roman>, serif"">High growth potential for those with an entrepreneurial spirit.', 'Times New Roman>, serif"">Work independently toward client success, at the same time knowing your own limitations and when to call on your OmniData team for help.', 'Times New Roman>, serif"">OmniData is small and growing.  You will be exposed to senior leaders and you will be given the opportunity to learn and grow quickly in your career.', 'Times New Roman>, serif"">OmniData is offering you the opportunity to work with the entire lifecycle of large Data Projects, focused on next generation data warehousing, with surface points to Analytics, Machine Learning and AI. We offer a collaborative work culture, that enables you to produce client results with a safety net from your team.  You will get to work closely with very experienced consultants who will be able to provide mentorship and career guidance.  At the same time, you will be rewarded for learning fast and executing within our teams to provide solutions for OmniData clients.', 'Times New Roman>, serif"">What does our recruitment process look like?', 'Times New Roman>, serif"">You need to be a Microsoft Certified Azure Data Engineer or demonstrate experience in the same skill set.  You need to be proficient in Power BI.  You need to have solid experience working with data and analytics, a strong technical aptitude, and be a quick learner.  In return, we offer an exciting position at a young startup experiencing rapid growth, deep mentorship and the opportunity to be part of creating a consulting firm that makes a difference for our clients every day we are with them.  ', 'Times New Roman>, serif"">Azure based Data Services Times New Roman>, serif"">Azure Data FactoryTimes New Roman>, serif"">Azure Data LakeTimes New Roman>, serif"">Azure Synapse AnalyticsTimes New Roman>, serif"">Azure Analysis Services', 'Times New Roman>, serif"">Analytical approach to problem-solving; ability to use technology to solve business problemsTimes New Roman>, serif"">Azure based Data Services Times New Roman>, serif"">Azure Data FactoryTimes New Roman>, serif"">Azure Data LakeTimes New Roman>, serif"">Azure Synapse AnalyticsTimes New Roman>, serif"">Azure Analysis ServicesTimes New Roman>, serif"">Microsoft SQL base Data SkillsTimes New Roman>, serif"">SSISTimes New Roman>, serif"">SQL ServerTimes New Roman>, serif"">SQL Server Analysis Services (Tabular & Multi-Dimensional)Times New Roman>, serif"">Power BI, DAXTimes New Roman>, serif"">Requirements Analysis and Project Delivery methodologyTimes New Roman>, serif"">You must be humble, hungry and a fast learner.  OmniData and our clients place a high value on inventiveness.Times New Roman>, serif"">Great communication skills tying technologies and architectures to business results will advance your position.Times New Roman>, serif"">Experience and a proven track-record with designing and building solutions in Azure is required.Times New Roman>, serif"">Must be decisive and show ability to work with clients and assist their business and technical decision-making.Times New Roman>, serif"">Willingness to travel (post-COVID) and work with a consultant\'s diligence.Times New Roman>, serif"">OmniData is small and growing.  You will be exposed to senior leaders and you will be given the opportunity to learn and grow quickly in your career.', 'Times New Roman>, serif"">Contribute collaboratively to team meetings using your experience base to further the cause of innovating for OmniData clients.', 'Times New Roman>, serif"">2+ years of experience in Analytics and Data Warehousing, with Azure knowledge a plus.  ', 'Times New Roman>, serif"">Analytical approach to problem-solving; ability to use technology to solve business problems', 'Times New Roman>, serif"">Data Warehouse Automation experience ', 'Times New Roman>, serif"">SSISTimes New Roman>, serif"">SQL ServerTimes New Roman>, serif"">SQL Server Analysis Services (Tabular & Multi-Dimensional)', 'Times New Roman>, serif"">Azure Data FactoryTimes New Roman>, serif"">Azure Data LakeTimes New Roman>, serif"">Azure Synapse AnalyticsTimes New Roman>, serif"">Azure Analysis Services', 'Times New Roman>, serif"">About You', 'Times New Roman>, serif"">Qualifications and Skills', 'Times New Roman>, serif"">Salary and benefits commensurate with experience.  ', 'Times New Roman>, serif"">You will work on various Big Data, Data Warehouse Automation, and Data Analytics projects for our world class clients.  In addressing complex client needs, you will be integrated into appropriately sized and skilled teams. You\'ll be asked to analyze requirements, develop data and analytical solutions and execute as part of the project team, all while working with the latest tools, such as Azure Synapse Analytics and related Microsoft technologies. ', 'Times New Roman>, serif"">SQL Server Analysis Services (Tabular & Multi-Dimensional)', 'Times New Roman>, serif"">Execution of team-leading Solution Architect and Data Architect vision for client success, with Azure based data warehouse and business intelligence tools.Times New Roman>, serif"">Contribute collaboratively to team meetings using your experience base to further the cause of innovating for OmniData clients.Times New Roman>, serif"">Instill confidence in the client, your team, and your team membersTimes New Roman>, serif"">Work independently toward client success, at the same time knowing your own limitations and when to call on your OmniData team for help.', 'Times New Roman>, serif"">Our process is highly personalized. Some candidates complete their process in one week, while others can take several weeks or even months. Deciding to take a new job is a big decision, so regardless of how long or short the process may be for you, the most important thing is that you find your dream job.', 'Times New Roman>, serif"">Azure Synapse Analytics', 'Times New Roman>, serif"">High growth potential for those with an entrepreneurial spirit.', 'Times New Roman>, serif"">Instill confidence in the client, your team, and your team members', 'Times New Roman>, serif"">Experience and a proven track-record with designing and building solutions in Azure is required.', 'Times New Roman>, serif"">OmniData is a US-based Data and Analytics focused consulting firm leveraging the Microsoft technology stack to help organizations build their Modern Data Estates, designed to serve their digital innovation needs for many years to come.  To do this, we apply deep experience in Solution Architecture, Data, Analytics, and technology to simplify the complex.   ', 'Times New Roman>, serif"">Azure Data Factory']",Entry level,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Engineer,Omnicom Media Group,United States,3 weeks ago,Over 200 applicants,"['', 'Required Skills', 'Extended time off around the holiday season. Our office is closed between Xmas and New Year to encourage our hardworking employees time to rest, recharge and celebrate the season with family and friends.', '4+ years of development experience on web applications using Python, Ruby, Java, or C#', 'Additional Skills', 'Perks of working at Annalect', 'Generous vacation policy. Paid time off (PTO) includes vacation days, personal days, and a Summer Friday program.', '8+ years of SQL experience.', 'Significant experience with Python, C++, or other popular language', 'Experience with big data and/or infrastructure. Bonus for having experience in setting up Petabytes of data so they can be easily accessed. Understanding of data organization, ie partitioning, clustering, file sizes, file formats. Data cataloging with Hive/Hive metastore or Glue or something similar.', 'Proven ability to independently execute projects from concept to implementation to launch and to maintain a live product', 'Passion for Technology: Excitement for new technology, bleeding edge applications, and a positive attitude towards solving real world challenges', 'Write at-scale ETL processes in Python, Spark, and other technologies', 'Culture. We have an incredibly fun, collaborative and friendly environment, and often host social and learning activities such as game night, speaker series, and so much more! Generous vacation policy. Paid time off (PTO) includes vacation days, personal days, and a Summer Friday program.Extended time off around the holiday season. Our office is closed between Xmas and New Year to encourage our hardworking employees time to rest, recharge and celebrate the season with family and friends.As part of Omnicom, we have the backing and resources of a global billion-dollar company, but also have the flexibility and pace of a “startup” - we move fast, break things, and innovate.', 'BS, MS or PhD in Computer Science, Engineering, or equivalent real-world experience Significant experience with Python, C++, or other popular languageExperience with big data and/or infrastructure. Bonus for having experience in setting up Petabytes of data so they can be easily accessed. Understanding of data organization, ie partitioning, clustering, file sizes, file formats. Data cataloging with Hive/Hive metastore or Glue or something similar.Experience working with classical relational databases. You will be writing/maintaining aggregates and optimizing queries for quick reads from a Django web app.Experience with Hadoop, Hive, Spark, or other data processing tools (Lots of time will be spent building and optimizing transformations)Experience building scalable data pipelines (Airflow experience a plus)Significant experience working with AWS and/or GCPProven ability to independently execute projects from concept to implementation to launch and to maintain a live productInterest or experience in ML technologies (TensorFlow, PyTorch or SageMaker, BigQueryML)', 'Curiosity in learning the business requirements that are driving the engineering requirements', 'Position Overview', 'Experience working with classical relational databases. You will be writing/maintaining aggregates and optimizing queries for quick reads from a Django web app.', 'Interest in new technologies and eager to bring those technologies and out of the box ideas to the team', 'Designing, building, testing and deploying scalable, reusable and maintainable applications that handle large amounts of dataWrite at-scale ETL processes in Python, Spark, and other technologiesPerform code reviews and provide leadership and guidance to junior developersAbility to learn and teach new technologies', 'Ability to learn and teach new technologies', 'Significant experience working with AWS and/or GCP', 'As part of Omnicom, we have the backing and resources of a global billion-dollar company, but also have the flexibility and pace of a “startup” - we move fast, break things, and innovate.', 'Perform code reviews and provide leadership and guidance to junior developers', 'This position is with Annalect, the Data, Technology, and Analytics division of Omnicom Media Group.', 'Experience with Hadoop, Hive, Spark, or other data processing tools (Lots of time will be spent building and optimizing transformations)', 'Culture. We have an incredibly fun, collaborative and friendly environment, and often host social and learning activities such as game night, speaker series, and so much more! ', 'Key Responsibilities', 'Annalect, a division of Omnicom Media Group, reaffirms its commitment to the policy of Equal Employment Opportunity and to carrying out this policy at all of its offices. It shall be the policy of Omnicom Media Group to (1) recruit, select, hire, train, promote, pay, discipline and terminate employees in all job classifications without regard to age, race, color, creed, national origin, citizenship status, alienage, religion, sex, sexual orientation, marital status, veteran status, disability or any other basis upon which discrimination against or harassment of employees or applicants for employment is prohibited under any applicable federal, state or local equal opportunity employment laws and (2) ensure that all personnel actions are administered without discrimination in violation of applicable law.', 'BS, MS or PhD in Computer Science, Engineering, or equivalent real-world experience ', 'We are an equal opportunity employer', ""Annalect is currently seeking a Data Engineer to join our technology team. In this role, you will build Annalect products which sit atop our Big Data infrastructure and utilize our componentized design system. We're looking for people who have a shared passion for technology, design & development, data, and fusing these disciplines together to build cool things. In this role, you will work on one or more software and data products in the Annalect Engineering Team. You will participate in technical architecture, design and development of software products as well as research and evaluation of new technical solutions."", 'Designing, building, testing and deploying scalable, reusable and maintainable applications that handle large amounts of data', 'Intellectual curiosity and drive; self-starters will thrive in this position', 'Experience building scalable data pipelines (Airflow experience a plus)', 'Curiosity in learning the business requirements that are driving the engineering requirementsInterest in new technologies and eager to bring those technologies and out of the box ideas to the team4+ years of development experience on web applications using Python, Ruby, Java, or C#8+ years of SQL experience.Intellectual curiosity and drive; self-starters will thrive in this positionPassion for Technology: Excitement for new technology, bleeding edge applications, and a positive attitude towards solving real world challenges', 'Interest or experience in ML technologies (TensorFlow, PyTorch or SageMaker, BigQueryML)']",Mid-Senior level,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Data Engineer,Transcat,"Rochester, NY",2 weeks ago,Be among the first 25 applicants,"['', 'Participates in the design, build and management of data ETL (Extract / Transform / Load) workflowsLead efforts to design, develop and implement databases enhancement to improve efficiency and streamline the use for analytics, business analysis and data governance.Develop a mastery of the corporate data structure to be able to perform ETL operations from data tables existing in the company.Test, design, and implement process data automation techniques to support efficiencies.Clearly communicate complex findings to colleagues influencing outcomes and communicating the value add of the data engineering work being completed.Foster relationships with colleagues to identify and explore new sources of data.Other job duties as assigned.', 'Proficient in SQL and experience with AWS data tools', 'Participates in the design, build and management of data ETL (Extract / Transform / Load) workflows', 'Practical experience consuming web services', 'Knowledge of how data entities and elements should be structured to assure accuracy, performance, understanding, operational, analytical, reporting, and data science efficiencies.', 'Bachelor’s degree required in Computer Science or another field.Minimum 3-4 years of experience in similar role ', 'Bachelor’s degree required in Computer Science or another field.', 'Work collaboratively with internal partners/departments.', 'Test, design, and implement process data automation techniques to support efficiencies.', 'Foster relationships with colleagues to identify and explore new sources of data.', 'Clearly communicate complex findings to colleagues influencing outcomes and communicating the value add of the data engineering work being completed.', 'Applied knowledge of data modeling principles (e.g., dimensional modeling and star schemas)', 'Must be a self-directed professional with the ability to work effectively in a fast-paced demanding environment, handle multi-tasks, problem solve and effectively follow-through.', 'Develop a mastery of the corporate data structure to be able to perform ETL operations from data tables existing in the company.', 'Minimum 3-4 years of experience in similar role ', 'Education And/or Experience', 'Lead efforts to design, develop and implement databases enhancement to improve efficiency and streamline the use for analytics, business analysis and data governance.', 'Applied knowledge of cloud computing.', 'Proficient in SQL and experience with AWS data toolsKnowledge of how data entities and elements should be structured to assure accuracy, performance, understanding, operational, analytical, reporting, and data science efficiencies.Applied knowledge of cloud computing.Applied knowledge of data modeling principles (e.g., dimensional modeling and star schemas)Practical experience consuming web servicesMust be a self-directed professional with the ability to work effectively in a fast-paced demanding environment, handle multi-tasks, problem solve and effectively follow-through.Work collaboratively with internal partners/departments.', 'Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status.', 'Other job duties as assigned.']",Entry level,Full-time,Information Technology,Electrical/Electronic Manufacturing,2021-03-18 14:34:51
Data Engineer,SpartanNash,"Byron Center, MI",2 weeks ago,Be among the first 25 applicants,"['', ""Bachelor's Degree (Required) Computer Science Information Technology, Business Administration or related field or equivalent combination of education and/or experienceFive (5) years of Information Technology experience in database modeling/relational database administration. Experience with and knowledge of creating Data Warehouse dimensional models, data modeling tools, principles and practices and related configuration management concepts.Programming PL/SQL and ETL development experience is necessary, Microsoft Azure (preferred)Experience creating logical and physical data models to optimize database design – experience in Erwin a plus.Experience profiling and retrieving data from business applications using a wide variety of database platforms.Experience with modern ETL/ELT tools (Talend/Matillion/Datastage) preferably in a cloud environment.Experience with version control systems (Git/SCCS)Ability to identify, troubleshoot, and resolve data integrity and performance issuesBackground supporting retail or wholesale distribution would be advantageous"", 'Establish and maintain common data definitions (i.e., naming guidelines, standard abbreviations, naming conventions) for database users to eliminate data redundancy and improve data integrity. Document data definitions and coordinate data models, dictionaries, and other database documentation across multiple applications.', 'Programming PL/SQL and ETL development experience is necessary, Microsoft Azure (preferred)', 'Experience with version control systems (Git/SCCS)', 'Experience profiling and retrieving data from business applications using a wide variety of database platforms.', 'Background supporting retail or wholesale distribution would be advantageous', 'Participate in the development of project scope and milestones to meet assigned project requirements. Partner cross-functionally to meet deadlines and audit databases/reports, and work with management on critical issues.', 'Gather and document business requirements and convert requirements into testable designs; and work with the business and application teams to refine design requirements on assigned projects.', 'Create and maintain various database versions to model data to optimize database design and share relational data across various applications. Document data model standards and database diagrams. Work with cross-functional departments to coordinate the database changes in a timely and accurate manner.', 'Create and maintain various database versions to model data to optimize database design and share relational data across various applications. Document data model standards and database diagrams. Work with cross-functional departments to coordinate the database changes in a timely and accurate manner.Utilize modeling tools to build and analyze databases and to develop logical and physical models to include relationships, constraints, attributes, and other modeling information to understand impact for proposed systems, and setup approved systems. Identify, troubleshoot and resolve data integrity and performance issues.Design data model structures that enhance the application performance through the development of aggregation and summarization structures. Document model structures (i.e., keys, indexes, constraints) and partner with cross-functional IT (i.e., Data Warehouse/Database Administration) for the process of data normalization and physical data base design.Gather and document business requirements and convert requirements into testable designs; and work with the business and application teams to refine design requirements on assigned projects.Maintain data reliability and integrity to eliminate data redundancy and to streamline the reporting platform. Maintain current knowledge of organization data uses to recommend improvements and maintain data integrity.Establish and maintain common data definitions (i.e., naming guidelines, standard abbreviations, naming conventions) for database users to eliminate data redundancy and improve data integrity. Document data definitions and coordinate data models, dictionaries, and other database documentation across multiple applications.Provide technical guidance to the applications development team regarding database design to reduce risk of design issues. Interface with appropriate IT sub-departments and/or cross functional business areas to address concerns and resolve issues in a timely manner.Participate in the development of project scope and milestones to meet assigned project requirements. Partner cross-functionally to meet deadlines and audit databases/reports, and work with management on critical issues.', 'Here’s What You’ll Need', 'Experience with modern ETL/ELT tools (Talend/Matillion/Datastage) preferably in a cloud environment.', 'Location:', 'Utilize modeling tools to build and analyze databases and to develop logical and physical models to include relationships, constraints, attributes, and other modeling information to understand impact for proposed systems, and setup approved systems. Identify, troubleshoot and resolve data integrity and performance issues.', 'Experience creating logical and physical data models to optimize database design – experience in Erwin a plus.', 'Provide technical guidance to the applications development team regarding database design to reduce risk of design issues. Interface with appropriate IT sub-departments and/or cross functional business areas to address concerns and resolve issues in a timely manner.', 'Five (5) years of Information Technology experience in database modeling/relational database administration. Experience with and knowledge of creating Data Warehouse dimensional models, data modeling tools, principles and practices and related configuration management concepts.', 'Job Description', 'Design data model structures that enhance the application performance through the development of aggregation and summarization structures. Document model structures (i.e., keys, indexes, constraints) and partner with cross-functional IT (i.e., Data Warehouse/Database Administration) for the process of data normalization and physical data base design.', ""Bachelor's Degree (Required) Computer Science Information Technology, Business Administration or related field or equivalent combination of education and/or experience"", 'Ability to identify, troubleshoot, and resolve data integrity and performance issues', 'Maintain data reliability and integrity to eliminate data redundancy and to streamline the reporting platform. Maintain current knowledge of organization data uses to recommend improvements and maintain data integrity.']",Entry level,Full-time,Information Technology,Food & Beverages,2021-03-18 14:34:51
Junior Data Engineer - Shanghai,Dashmote,"China, ME",1 week ago,Be among the first 25 applicants,"['', 'Entrepreneurial minded, hands-on and willing to work in a fast-paced environment', "" Bachelor's Degree or above in Computer Science, Engineering or related subjects. Proficiency with Python Experienced with SQL and NoSQL technologies Experienced with building web spiders with scrapy/puppeteer, Familiarity with commonly used anti-crawling methods Solid understanding of version control tools (e.g. git). Proficient in English Entrepreneurial minded, hands-on and willing to work in a fast-paced environment Fast learner and open minded to new technologies and methodologies Team oriented "", 'If this sounds like a match, we would love to hear from you!', 'Solid understanding of version control tools (e.g. git).', 'Job Requirements', 'Team oriented', 'Process unstructured data into a form suitable for analysis.', 'Develop set processes for data mining.', 'Great office location right in the center of Shanghai', 'Familiarity with commonly used anti-crawling methods', 'Monthly team events and weekly Friday company catch-ups and drinks', 'Proficiency with Python', 'Collaborate closely with teammates to ensure consistency and maximize use of data', ""Bachelor's Degree or above in Computer Science, Engineering or related subjects."", 'Gather and process raw data at scale (including writing scripts, web scraping, calling APIs, writing SQL queries, etc.).', 'Experienced with SQL and NoSQL technologies', 'Fast learner and open minded to new technologies and methodologies', 'Monitoring performance and advising any necessary infrastructure changes.', ' Gather and process raw data at scale (including writing scripts, web scraping, calling APIs, writing SQL queries, etc.). Process unstructured data into a form suitable for analysis. Monitoring performance and advising any necessary infrastructure changes. Testing and validation in order to support the accuracy of data transformations and data verification used in machine learning models Develop set processes for data mining. Collaborate closely with teammates to ensure consistency and maximize use of data ', 'Growing company full of opportunities & awarded by Google, McKinsey and Rocket Internet for best B2B startup in Europe', 'Testing and validation in order to support the accuracy of data transformations and data verification used in machine learning models', 'About us', 'What’s In It For You', ' Great office location right in the center of Shanghai Working within an international team of over 65 people that truly values your contribution Growing company full of opportunities & awarded by Google, McKinsey and Rocket Internet for best B2B startup in Europe An awesome culture of responsibility and the freedom to turn your ambition into reality - regardless of your role and level Exciting work atmosphere with no shortage of snacks, drinks, birthday treats, and social events Monthly team events and weekly Friday company catch-ups and drinks If this sounds like a match, we would love to hear from you! Salary will depend on the candidate’s level of tech skills and experience.', 'Exciting work atmosphere with no shortage of snacks, drinks, birthday treats, and social events', 'Role Description', 'Proficient in English', 'Salary will depend on the candidate’s level of tech skills and experience.', 'Job Description', 'Experienced with building web spiders with scrapy/puppeteer,', 'Working within an international team of over 65 people that truly values your contribution', 'An awesome culture of responsibility and the freedom to turn your ambition into reality - regardless of your role and level']",Associate,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
"Data Engineer III (Remote, US)",Bombora,United States,1 week ago,Be among the first 25 applicants,"['', 'Creativity, pragmatism, curiosity, and a good sense of humor', '\xa0\xa0On Demand Learning (Udemy)', 'Create and refine bounded (batch) and unbounded (streaming) ETL and ML data pipelines that comprise our production systems.', 'Working knowledge of: Algorithms/Data Structures/Design patterns, Functional Programming, Databases (relational and non-relational), Data Processing at scale, and Build and Release Toolchains\xa0', '\xa0\xa0Health / Dental / Vision', '\xa0\xa0Education / Tuition Assistance', 'Advance development and integration of our major analytics and ML codebases using modern and rigorous software engineering principles.', 'Mentor and advance the development of your colleagues', 'You Will….\xa0\xa0', 'Employ test-driven development, performance benchmarking, rapid release schedule, and continuous integration.', 'Create and refine bounded (batch) and unbounded (streaming) ETL and ML data pipelines that comprise our production systems.Advance development and integration of our major analytics and ML codebases using modern and rigorous software engineering principles.Develop applications, libraries and workflows with Python, Java, Apache Spark, Apache Beam, and Apache Airflow.Design and implement systems that run at scale on Google’s Dataproc, Dataflow, Kubernetes, Pub/Sub, and BigQuery platforms; and implement algorithms and machine learning operations, at-scale, using SciPy, PySpark, Spark Streaming, and MLBase libraries.Employ test-driven development, performance benchmarking, rapid release schedule, and continuous integration.Mentor and advance the development of your colleaguesParticipate in daily stand-ups, story planning, reviews, retrospectives, and the occasional outings to nearby local cuisine and / or culture.\xa0', 'Develop applications, libraries and workflows with Python, Java, Apache Spark, Apache Beam, and Apache Airflow.', 'Design and implement systems that run at scale on Google’s Dataproc, Dataflow, Kubernetes, Pub/Sub, and BigQuery platforms; and implement algorithms and machine learning operations, at-scale, using SciPy, PySpark, Spark Streaming, and MLBase libraries.', 'An understanding of how to have great impact with TDD and agile methodologies in software', '5+ years of real-world development experience and 2+ years of experience with cloud and/or big-data platform (GCP experience preferred).Language Fluency in Java / Python\xa0The ability to leverage data to understand systems.An understanding of how to have great impact with TDD and agile methodologies in softwareCreativity, pragmatism, curiosity, and a good sense of humorWorking knowledge of: Algorithms/Data Structures/Design patterns, Functional Programming, Databases (relational and non-relational), Data Processing at scale, and Build and Release Toolchains\xa0', 'As a Data Engineer, you will join our Data Engineering team, working alongside our data scientists and machine learning engineers to support Bombora R&D’s mission to design, develop and maintain our world class B2B DaaS products, leveraging machine intelligence and web-content consumption data at-scale.', '5+ years of real-world development experience and 2+ years of experience with cloud and/or big-data platform (GCP experience preferred).', '\xa0\xa0Team Lunches / Outings /Events (Yes! We found a way to do virtually!)', 'Bombora provides a global B2B intent platform powered by the world’s largest publisher data co-op. Our data allows sales teams to base their actions on the knowledge of which companies are in-market for their products and empowers marketing teams to practice #SustainableMarketing. We process billions of content interactions daily to detect intent signals from companies around the world.', 'At Bombora, we embrace diversity because it breeds innovation. Bombora is an equal opportunity employer and participates in E-Verify. Employment offers are contingent upon completion of successful background checks.', 'The ability to leverage data to understand systems.', 'Perks and Benefits', 'You Have…', 'Participate in daily stand-ups, story planning, reviews, retrospectives, and the occasional outings to nearby local cuisine and / or culture.\xa0', '\xa0\xa0401K / Match', '\xa0\xa0Competitive Salary\xa0\xa0Health / Dental / Vision\xa0\xa0Flexible Spending Account\xa0\xa0Commuter Benefits\xa0\xa0Unlimited Vacation / Paid Holidays\xa0\xa0Education / Tuition Assistance\xa0\xa0401K / Match\xa0\xa0Generous Parental Leave\xa0\xa0On Demand Learning (Udemy)\xa0\xa0Team Lunches / Outings /Events (Yes! We found a way to do virtually!)\xa0\xa0Offices (for when you want one)\xa0', '\xa0\xa0Unlimited Vacation / Paid Holidays', '\xa0#BI-Remote', 'Bombora is growing and we need you to help us succeed!', '\xa0\xa0Commuter Benefits', '\xa0\xa0Competitive Salary', '\xa0', 'Language Fluency in Java / Python\xa0', '\xa0\xa0Generous Parental Leave', '\xa0\xa0Offices (for when you want one)\xa0', '\xa0\xa0Flexible Spending Account']",Mid-Senior level,Full-time,Information Technology,Internet,2021-03-18 14:34:51
Data Engineer - Princeton,Ergo,"Princeton, NJ",1 week ago,29 applicants,"['', ' Strong oral and written communication and interpersonal skills', 'Qualifications / Job Requirements', ' Drive data architecture design decisions considering future growth', ' Bachelor’s degree (MS preferred) in Computer Science, Statistics, Math or equivalent combination of education and experience', ' Execute on projects to provide relevant datasets in support of business initiatives  Independently perform the extraction, cleaning, transformation and loading of various data file formats, databases and cloud/web sources  Work with various complex databases containing data from multiple sources, at various levels of granularity to deliver high-quality data  Build data pipelines for the automation of data processes  Drive data architecture design decisions considering future growth  Develop expert knowledge of data and analytics infrastructure within Munich Re', ' Experience in insurance industry (preferred experience working with insurance auto/mobility data)', ' Execute on projects to provide relevant datasets in support of business initiatives', ' Develop expert knowledge of data and analytics infrastructure within Munich Re', ' Demonstrated ability to experiment with and learn new technologies', ' Independently perform the extraction, cleaning, transformation and loading of various data file formats, databases and cloud/web sources', ' Drive and dedication, as well as creativity and hands-on attitude', ' Build data pipelines for the automation of data processes', ' 1-3 years of experience with statistical modeling, data modeling, data intake, and data-curation procedures', ' Excellent analytical, problem solving and organizational skills.', ' Experience with data visualization tools such as Power BI', ' Experience in data pipeline development using Databricks and Azure Data Factory', ' Bachelor’s degree (MS preferred) in Computer Science, Statistics, Math or equivalent combination of education and experience  1-3 years of experience with statistical modeling, data modeling, data intake, and data-curation procedures  Experience in insurance industry (preferred experience working with insurance auto/mobility data)  Expert SQL skills including experience with distributed and spatial queries  1+ years of hands-on R or Python experience is highly preferred  Experience in data pipeline development using Databricks and Azure Data Factory  Experience with data visualization tools such as Power BI  Drive and dedication, as well as creativity and hands-on attitude  Curiosity in searching for new solutions outside of traditional approaches  Demonstrated ability to experiment with and learn new technologies  Strong oral and written communication and interpersonal skills  Excellent analytical, problem solving and organizational skills.', 'Key Responsibilities Of This Position Include', ' Curiosity in searching for new solutions outside of traditional approaches', ' 1+ years of hands-on R or Python experience is highly preferred', ' Work with various complex databases containing data from multiple sources, at various levels of granularity to deliver high-quality data', 'Job Description', ' Expert SQL skills including experience with distributed and spatial queries']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Sr. Data Engineer,"Windows Management Experts, Inc. (WME)","Philadelphia, PA",3 days ago,Be among the first 25 applicants,[''],Associate,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Advanced Tech Placement,"Atlanta, GA",,N/A,"['', 'Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and ‘big data’ technologies.\xa0', 'Responsibilities for Data Engineer:', 'Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\xa0', 'Experience supporting and working with cross-functional teams in a dynamic environment.\xa0', 'Work with stakeholders including the Executive, Product,\xa0Development,\xa0Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.\xa0', 'Experience with data tools:\xa0Spark/Scala.\xa0', 'We are looking for savvy Data Engineers to join our growing team of experts. This role is responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data engineer and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. The ideal candidate must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives. Strong candidates will thrive in the collaborative environment while also being strong individual contributors with a proactive mindset.', 'Assemble large, complex data sets that meet functional/non-functional business requirements.\xa0', 'Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.\xa0', 'Strong analytic skills related to working with unstructured datasets.\xa0', 'Advanced working SQL knowledge and experience working with relational\xa0and non-relational\xa0databases, query authoring (SQL) as well as working familiarity with a variety of databases.\xa0', 'Experience with relational SQL and NoSQL databases, including\xa0Snowflake\xa0and\xa0MariaDB AX.\xa0', 'Keep\xa0data separate and secure across boundaries through\xa0different\xa0regions.\xa0', '\ufeffQualifications:', 'Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.\xa0', 'Create and maintain optimal data pipeline architecture.\xa0', 'Advanced working SQL knowledge and experience working with relational\xa0and non-relational\xa0databases, query authoring (SQL) as well as working familiarity with a variety of databases.\xa0Experience building and optimizing data pipelines, architectures and data sets.\xa0Strong analytic skills related to working with unstructured datasets.\xa0Build processes supporting data transformation, data structures, metadata, dependency and workload management.\xa0A successful history of manipulating, processing and extracting value from large disconnected datasets.\xa0Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.\xa0Working knowledge of message queuing, stream processing, and highly scalable data stores.\xa0Strong\xa0communication\xa0project management and organizational skills.\xa0Experience supporting and working with cross-functional teams in a dynamic environment.\xa0We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has\xa0obtained a\xa0Bachelor’s\xa0degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.\xa0They\xa0should also have experience using the following software/tools:\xa0Experience with data tools:\xa0Spark/Scala.\xa0Experience with relational SQL and NoSQL databases, including\xa0Snowflake\xa0and\xa0MariaDB AX.\xa0Experience with AWS cloud services: EC2, EMR, RDS\xa0and\xa0Redshift\xa0Experience with stream-processing systems: Storm, Spark-Streaming, etc.\xa0Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.\xa0', 'Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.\xa0', 'Create and maintain optimal data pipeline architecture.\xa0Assemble large, complex data sets that meet functional/non-functional business requirements.\xa0Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\xa0Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and ‘big data’ technologies.\xa0Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.\xa0Work with stakeholders including the Executive, Product,\xa0Development,\xa0Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.\xa0Keep\xa0data separate and secure across boundaries through\xa0different\xa0regions.\xa0Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.\xa0Work with data and analytics experts to strive for greater functionality in our data systems.\xa0', 'Working knowledge of message queuing, stream processing, and highly scalable data stores.\xa0', 'We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has\xa0obtained a\xa0Bachelor’s\xa0degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.\xa0They\xa0should also have experience using the following software/tools:\xa0', 'Work with data and analytics experts to strive for greater functionality in our data systems.\xa0', 'Experience with AWS cloud services: EC2, EMR, RDS\xa0and\xa0Redshift\xa0', 'Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.\xa0', 'Strong\xa0communication\xa0project management and organizational skills.\xa0', 'Description:', 'Experience building and optimizing data pipelines, architectures and data sets.\xa0', 'A successful history of manipulating, processing and extracting value from large disconnected datasets.\xa0', 'Build processes supporting data transformation, data structures, metadata, dependency and workload management.\xa0', 'Experience with stream-processing systems: Storm, Spark-Streaming, etc.\xa0']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Cloud Data Engineer,HUNTER Technical - htrjobs.com,"Arlington, VA",7 days ago,93 applicants,"['', 'Use standard version control tools to check in and perform code reviews with Data Integration SME', 'Perform technical analysis on existing code in Unix/Datastage for migration to AWS', 'Prepare unit test cases, test data and perform unit testing for the jobs developed', ' Design and develop moderate to complex data pipelines for reusability using AWS Glue Integrate AWS Glue jobs with AWS data lake technologies including S3, Step Functions, Lambda and Kinesis Integrate AWS Glue jobs with supporting AWS services including SQS, SMS, DynamoDB Setup repeatable patterns for migration of data to AWS Data Lake from on-premises, third parties, and other cloud providers Work with AWS tools like Athena, QuickSight, and Sagemaker to provide platforms for Cyber Fraud and Analytics team to perform necessary analytics and machine learning Perform technical analysis on existing code in Unix/Datastage for migration to AWS Prepare detailed design, architecture, mapping sheet and other ETL documents Prepare and validate end to end business logic and mappings. Prepare unit test cases, test data and perform unit testing for the jobs developed Use standard version control tools to check in and perform code reviews with Data Integration SME Use standard CI/CD pipelines to roll and deploy the code.', 'Work with AWS tools like Athena, QuickSight, and Sagemaker to provide platforms for Cyber Fraud and Analytics team to perform necessary analytics and machine learning', 'Design and develop moderate to complex data pipelines for reusability using AWS Glue', 'Use standard CI/CD pipelines to roll and deploy the code.', 'Prepare and validate end to end business logic and mappings.', 'Integrate AWS Glue jobs with supporting AWS services including SQS, SMS, DynamoDB', 'Prepare detailed design, architecture, mapping sheet and other ETL documents', 'Setup repeatable patterns for migration of data to AWS Data Lake from on-premises, third parties, and other cloud providers', 'Requirements', 'Integrate AWS Glue jobs with AWS data lake technologies including S3, Step Functions, Lambda and Kinesis']",Entry level,Contract,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer - Digital Transformation - Remote,NI (National Instruments),United States,1 week ago,66 applicants,"['', 'Demonstrated success and experience using high-level programming languages', 'Own the ingestion and pipeline process at key customer engagements', 'Research new ETL technologies, tools and methodologies that may improve the overall solution operation and performance', 'Experience working closely with customers to understand their data and data source. Experience collecting data from multiple systems using ETL tool', 'Work with customers’ business and operational users and IT infrastructure teams to ensure adherence to customer guidelines', 'Familiar with industry available tools that enable accelerated ETL building and deployment', 'USA - #LIRemote', 'Knowledgeable with the data sources of various electronics test technologies: In-Circuit testing, Optical and X-Ray inspection, Boundary-Scan JTAG, Board functional tests with deep knowledge in software development tools and processes', 'Assist with dashboard development and occasionally provide training to technical staff', 'Understanding of computer network protocols and topographies, i.e. VLANs, switches, firewalls, factory private networks', 'Minimum BS degree in a STEM (Science, Technology, Engineering or Math) discipline5-7+ years of experience in development of test data acquisition software solutions for industrial applications such as systems in design, NPI and volume manufacturingEffective in building and\xa0influencing\u200bExperience working with\xa0various geographies and\xa0functions\u200bWork effectively in an\xa0ambiguous environment\u200bServices oriented attitude and associated communication skillsAnalytical mindset and\xa0effective\xa0problem-solving\xa0skills and self-starterExperience working closely with customers to understand their data and data source. Experience collecting data from multiple systems using ETL toolExperience working with different data formats such as XML, CSV, DB dumps, etc.Deep understanding of development languages such as Python, C#, Java, LabVIEW, SQL, and software development tools: Agile dev, Git, perforce, TFSExperience with Electronic systems for Consumer electronics, Mobile devices, Computer systems, Servers, Automotive electronics, etc.Knowledgeable with the data sources of various electronics test technologies: In-Circuit testing, Optical and X-Ray inspection, Boundary-Scan JTAG, Board functional tests with deep knowledge in software development tools and processesWilling and able to travel up to 25% of the timeUS Citizen or Permanent Resident (Green Card)', 'MS in Computer Science or Electrical EngineeringUnderstanding of the NI Tools suite such as LabVIEW, TestStand, Diadem, SystemLinkUnderstanding of computer network protocols and topographies, i.e. VLANs, switches, firewalls, factory private networksStrong understanding of electrical engineering, electronics & computer architecturesExperience working within a manufacturing or engineering environment building data pipelinesExperience working directly with customers in support of sales activitiesDemonstrated success and experience using high-level programming languagesIndustry/application experience in Aerospace and Defense or Automotive', 'Understand customers’ functional processes, data flow and infrastructure to efficiently and effectively, extract, transform, and load customer data from multiple sources. Sources can be from individual test equipment to Enterprise MES and parametric database systemsFamiliar with industry available tools that enable accelerated ETL building and deploymentDrive the definition, development and implementation of ETL solutionProvide customized solutions for ad-hoc customer requestsRespond to and resolve emergent customer data transfer problems; create automated tools to prevent problems recurrenceResearch new ETL technologies, tools and methodologies that may improve the overall solution operation and performanceWork closely with the Solution Architecture team to identify, understand, and prioritize data parsing and customer requirementsWork with customers’ business and operational users and IT infrastructure teams to ensure adherence to customer guidelinesAssist with dashboard development and occasionally provide training to technical staff', 'Experience working with different data formats such as XML, CSV, DB dumps, etc.', 'Willing and able to travel up to 25% of the time', 'Minimum BS degree in a STEM (Science, Technology, Engineering or Math) discipline', 'Provide customized solutions for ad-hoc customer requests', 'Industry/application experience in Aerospace and Defense or Automotive', 'Respond to and resolve emergent customer data transfer problems; create automated tools to prevent problems recurrence', 'Preferred Location', 'Qualifications and Skills', 'Develop a continuous improvement strategy for Data Engineering at NI', 'Deep understanding of development languages such as Python, C#, Java, LabVIEW, SQL, and software development tools: Agile dev, Git, perforce, TFS', 'Deliver clean, useable data sets to tools and Data Scientists', 'Experience working within a manufacturing or engineering environment building data pipelines', 'Experience working directly with customers in support of sales activities', '5-7+ years of experience in development of test data acquisition software solutions for industrial applications such as systems in design, NPI and volume manufacturing', 'Strong understanding of electrical engineering, electronics & computer architectures', 'The Data Engineer within Methodology Consulting services (MCS) is responsible for the data identification, extract and loading in support of the Digital Transformation team. A Data Engineer transforms data into a useful format for analysis. The ideal candidate is adept in many development languages and tools, supporting data ingestion in an engineering and manufacturing domain. You will map, model, clean and prepare data for loading or transition and ultimately for analysis. At NI this will typically involve python, Java, databases, LabVIEW and exposure to Diadem and SystemLink. This individual is always willing to be adaptable and ready to look at the next tool set to enable efficient process of data. You will be influential in defining the process of Data Engineering at NI.', 'Key Performance Objectives', 'Basic Qualifications', 'Core Responsibilities', 'Analytical mindset and\xa0effective\xa0problem-solving\xa0skills and self-starter', 'Experience with Electronic systems for Consumer electronics, Mobile devices, Computer systems, Servers, Automotive electronics, etc.', 'MS in Computer Science or Electrical Engineering', 'Work closely with the Solution Architecture team to identify, understand, and prioritize data parsing and customer requirements', 'US Citizen or Permanent Resident (Green Card)', 'Drive the definition, development and implementation of ETL solution', 'Experience working with\xa0various geographies and\xa0functions\u200b', 'Effective in building and\xa0influencing\u200b', 'Drive the data ingestion process', 'Drive the data ingestion processOwn the ingestion and pipeline process at key customer engagementsDeliver clean, useable data sets to tools and Data ScientistsDevelop a continuous improvement strategy for Data Engineering at NI', 'Work effectively in an\xa0ambiguous environment\u200b', 'Understand customers’ functional processes, data flow and infrastructure to efficiently and effectively, extract, transform, and load customer data from multiple sources. Sources can be from individual test equipment to Enterprise MES and parametric database systems', 'Preferred Qualifications', 'Understanding of the NI Tools suite such as LabVIEW, TestStand, Diadem, SystemLink', 'Services oriented attitude and associated communication skills']",Mid-Senior level,Full-time,Quality Assurance,Electrical/Electronic Manufacturing,2021-03-18 14:34:51
Data Engineer,Cresset,"Reston, VA",1 week ago,Be among the first 25 applicants,"['', ' Assist in developing a data reporting and analytics framework, including industry best practices ', ' Strong analytical experience and understanding technology processes of information and data ', ' Thank You ', ' Bachelor’s degree required in computer science, engineering and/or similar field  3+ years of experience in performing data analysis on cross functional business processes involving multiple IT systems  Experience building API integrations, including learning vendor API documentation, and staying current with updates to the tech stack, requirements, and advancements  Experience with Sisense preferred  Ability to code in SQL and Python  Structured & unstructured data expertise  Strong analytical experience and understanding technology processes of information and data  Excellent communication skills and ability to present data analytics to senior leadership  Strong teamwork and interpersonal skills to collaborate with people across different functions  Self-motivated and detail oriented  Knowledge of RIA industry preferred ', ' Collaborate with business partners to prioritize requests/needs and provide a holistic view of the analysis ', ' Experience building API integrations, including learning vendor API documentation, and staying current with updates to the tech stack, requirements, and advancements ', 'By submitting your interest in this job, you agree to receive text notifications with additional steps to complete your job application. You will receive up to 6 messages from the number ""63879"". Message & data rates may apply. Please refer to our privacy policy for more information.', ' Present data in a clear and concise manner allowing the internal partners to quickly understand the results and make data driven decisions ', ' Experience with Sisense preferred ', ' Identify data relationships such as trends, patterns, and correlations in order to answer business questions as well as provide actionable recommendations ', 'Apply for this Position', ' Improve the performance of queries and analysis through automation ', 'Qualifications', ' 3+ years of experience in performing data analysis on cross functional business processes involving multiple IT systems ', ' Acquire and integrate data from multiple sources/systems for analysis ', ' Perform advanced data analytics (e.g., data mining, statistical analysis, predictive analytics) ', ' Structured & unstructured data expertise ', ' Self-motivated and detail oriented ', ' Bachelor’s degree required in computer science, engineering and/or similar field ', ' Assist in developing a data reporting and analytics framework, including industry best practices  Improve the performance of queries and analysis through automation  Collaborate with business partners to prioritize requests/needs and provide a holistic view of the analysis  Acquire and integrate data from multiple sources/systems for analysis  Identify data relationships such as trends, patterns, and correlations in order to answer business questions as well as provide actionable recommendations  Present data in a clear and concise manner allowing the internal partners to quickly understand the results and make data driven decisions  Perform advanced data analytics (e.g., data mining, statistical analysis, predictive analytics) ', 'Job Description', ' Ability to code in SQL and Python ', ' Excellent communication skills and ability to present data analytics to senior leadership ', ' Strong teamwork and interpersonal skills to collaborate with people across different functions ', 'About Cresset', ' Knowledge of RIA industry preferred ']",Entry level,Full-time,Information Technology,Financial Services,2021-03-18 14:34:51
"Data Engineer, eCommerce",Anheuser-Busch InBev,"New York, NY",,N/A,"['', 'BEES, a part of the AB InBev family, is a digital organization within ABI building a platform to improve the ways retailers run their businesses and interact with the world’s largest brewer & other suppliers. We provide transactional and educational resources to SMB retailers across the world to help reduce the overhead of their day-to-day operations and make their businesses more profitable. Today more than 1 million SMB retailers across 18 countries use the BEES platform to source manage their business.', 'About the Team', 'Experience with Data Warehouses such as Google Big Query, Snowflake, or Redshift\xa0', 'The Data Engineer will create, oversee and maintain commercial and external data pipelines working closely with our commercial, product and technology leads to standardize our commercial and external data. This person should have experience building data integrations to sales and ERP systems such as SAP, maintaining centralized data warehouses and data dictionaries for multiple commercial stakeholders.\xa0', 'Experience working in a full Data Engineering team: QA, Data Engineers, Data Analysts, DBAs, etc.\xa0', 'About the Job', 'AB InBev is the leading global brewer and one of the world’s top 5 consumer product companies. With over 500 beer brands we’re number one or two in many of the world’s top beer markets: North America, Latin America, Europe, Asia, and Africa.\xa0', 'Have strong communication skills: able to\u202fclearly understand business objectives\u202fand translate them to an end to end data strategy and execution\xa0', 'Understanding of CI/CD and DevOps best practices\xa0', 'Experience building cloud-based solutions (AWS, Azure, GCP)\xa0', 'Take ownership of understanding the business and operational problems at hand and how to best solve them through a data-driven approach\xa0', 'Functional programming experience\xa0', 'We’re rethinking the way AB InBev does business with its retail customers and creating digital experiences to serve them. You will be joining a new digital organization within AB InBev consisting of digital strategy,\u202fproduct, design, analytics, operations and engineering. This organization is responsible for building the products and platforms that transform our traditional sales operations across the world.\xa0', 'Be a key leader in our global organization, working fluidly across ABI’s global and local teams and functions (sales, finance, marketing, product, IT, etc.) to make data-driven initiatives successful in an efficient way\xa0', 'Prioritize and find the most efficient path towards solving complex, ambiguous business problems with data, keeping a mindset of simplicity, robustness and speed above all\xa0', 'Familiarity with ML development lifecycle and related tools/libraries\xa0', 'Expertise in Python, C++, and/or a JVM language\xa0', 'Have experience operating in a production level data warehouse and/or data lake\xa0', 'Experience managing the full end-to-end pipeline for data warehousing and data pipelines of commercial data for multiple analytics and BI stakeholders\xa0', 'BA/BS degree (Computer Science, Software/Computer Engineering, Information Systems, Statistics, or similar technical field)\xa0Working knowledge of “Big Data” technologies such as MapReduce and/or Spark\xa0Experience building cloud-based solutions (AWS, Azure, GCP)\xa0Understanding of CI/CD and DevOps best practices\xa0Familiarity with Data Governance and related concepts, e.g., lineage, quality, integrity, security\xa0Familiarity with containerization technologies, e.g., Docker, Kubernetes\xa0Experience with workflow orchestration tools, e.g., Airflow, Luigi\xa0Functional programming experience\xa0Familiarity with ML development lifecycle and related tools/libraries\xa0Experience building streaming data pipelines using Kafka, Kinesis, Spark, and/or Flink\xa0API design expertise', 'About the Team\xa0', 'Create and maintain optimal data pipeline architecture, – own all data sources (transactional, internal and external data\u202fwithin our e-commerce organization) and ensure their accuracy to maintain commercial functions in our digital platforms\xa0Take ownership of understanding the business and operational problems at hand and how to best solve them through a data-driven approach\xa0Execute, and maintain the strategy for our data warehousing and pipelines to cater to specific business objectives, from implementation to maintenance\xa0Be a key leader in our global organization, working fluidly across ABI’s global and local teams and functions (sales, finance, marketing, product, IT, etc.) to make data-driven initiatives successful in an efficient way\xa0Prioritize and find the most efficient path towards solving complex, ambiguous business problems with data, keeping a mindset of simplicity, robustness and speed above all\xa0', 'Desired Qualifications\xa0', 'Familiarity with containerization technologies, e.g., Docker, Kubernetes\xa0', 'PowerBI\xa0', 'Working knowledge of “Big Data” technologies such as MapReduce and/or Spark\xa0', 'New Relic', 'Azure (Active Directory, Data Factory, Data Share, DevOps, Event Hub, Key Vault, Storage accounts/Blob containers/ADLS Gen2)\xa0Snowflake\xa0Databricks\xa0Spark/PySpark\xa0ReTool\xa0PowerBI\xa0New Relic', 'Have experience\u202fworking with\u202fboth\u202fhighly technical\u202fand non-technical profiles on a day-to-day basis\xa0', 'Experience with workflow orchestration tools, e.g., Airflow, Luigi\xa0', 'Our team is in search of a Data Engineer to be a leader in our Global Revenue Management & Commercial organization and help build, and maintain our data engineering infrastructure geared towards solving business problems and provide commercial value for ABI. This position reports directly to the Global Director of eCommerce Data Engineering.', 'BA/BS degree (Computer Science, Software/Computer Engineering, Information Systems, Statistics, or similar technical field)\xa0', 'About You\xa0', 'Desired Qualifications', 'About AB InBev\xa0', 'Spark/PySpark\xa0', 'ReTool\xa0', 'Azure (Active Directory, Data Factory, Data Share, DevOps, Event Hub, Key Vault, Storage accounts/Blob containers/ADLS Gen2)\xa0', 'Our Technology', 'Experience building streaming data pipelines using Kafka, Kinesis, Spark, and/or Flink\xa0', 'You Will', 'Have strong communication skills: able to\u202fclearly understand business objectives\u202fand translate them to an end to end data strategy and execution\xa0Have experience\u202fworking in a central analytics or data strategy role within a large enterprise\u202fenvironment\xa0Have experience operating in a production level data warehouse and/or data lake\xa0Live and breathe data – providing the best, most correct data is your obsession\xa0Have experience\u202fworking with\u202fboth\u202fhighly technical\u202fand non-technical profiles on a day-to-day basis\xa0', 'Create and maintain optimal data pipeline architecture, – own all data sources (transactional, internal and external data\u202fwithin our e-commerce organization) and ensure their accuracy to maintain commercial functions in our digital platforms\xa0', 'Familiarity with Data Governance and related concepts, e.g., lineage, quality, integrity, security\xa0', 'Live and breathe data – providing the best, most correct data is your obsession\xa0', 'API design expertise', 'About the Job\xa0', 'Experience working for an international organization with teams distributed across geographies/timezones\xa0', 'You Will\xa0', 'Required Qualifications', 'Our Technology\xa0', 'Experience structuring data from unstructured data sources and maintaining SLAs for delivery\xa0', 'About BEES ', 'Execute, and maintain the strategy for our data warehousing and pipelines to cater to specific business objectives, from implementation to maintenance\xa0', 'About AB InBev', 'Main stakeholders will include sales functions (commercial reporting, promo analytics, and sales algorithms) and customer data product leadership.\xa0', 'Expertise in Python, C++, and/or a JVM language\xa0Familiarity with SQL\xa0Experience with Data Warehouses such as Google Big Query, Snowflake, or Redshift\xa0Experience managing the full end-to-end pipeline for data warehousing and data pipelines of commercial data for multiple analytics and BI stakeholders\xa0Experience structuring data from unstructured data sources and maintaining SLAs for delivery\xa0Experience working in a full Data Engineering team: QA, Data Engineers, Data Analysts, DBAs, etc.\xa0Experience working for an international organization with teams distributed across geographies/timezones\xa0', 'Snowflake\xa0', 'About You', 'Have experience\u202fworking in a central analytics or data strategy role within a large enterprise\u202fenvironment\xa0', 'Familiarity with SQL\xa0', 'Required Qualifications\xa0', 'Databricks\xa0']",Mid-Senior level,Full-time,Analyst,Consumer Goods,2021-03-18 14:34:51
Data Engineer - BS/MS,Procter & Gamble,"Boston, MA",1 week ago,67 applicants,"['', ' A BS or MS in Computer Science, Computer or Electrical Engineering, but we also consider other, similar engineering degree ', 'The Ideal Candidate', ' History of working independently and effectively multi-tasking ', ' Evaluate tools and develop pipelines to capture, integrate and clean data to support edge analytics solutions ', ' Strong data wrangling skills ', ' Strong interpersonal communication and collaboration skills  History of working independently and effectively multi-tasking  Familiarity with machine learning workflows (desirable)  Have experience with sensors and IoT cloud architecture (desirable)  Familiarity with RESTful Application Programming Interface (API), containers and microservices  Familiarity with data privacy and data governance  Experience with NoSQL databases ', ' A BS or MS in Computer Science, Computer or Electrical Engineering, but we also consider other, similar engineering degree  Overall G.P.A. of 3.0 or above on a 4.0 scale ', ' Overall G.P.A. of 3.0 or above on a 4.0 scale ', ' Deliver optimal data solution architectures, automation and technology choices starting from experimentation through proof of concept and often through delivery ', 'We Are Also Looking For Someone Who Has', ' Familiarity with data privacy and data governance ', ' Have experience with sensors and IoT cloud architecture (desirable) ', 'Qualifications', 'Description', ' Experience with NoSQL databases ', ' Strong interpersonal communication and collaboration skills ', 'In this role you will:', ' Strong problem-solving skills paired with extensive experience programming (Python, Java, C++, etc...) ', ' Familiarity with machine learning workflows (desirable) ', ' Strong problem-solving skills paired with extensive experience programming (Python, Java, C++, etc...)  Strong data wrangling skills  Hands on experience with relational databases and the use of SQL to extract and manipulate data  Experience with cloud services (AWS, Azure or GCP) ', ' Develop and maintain scalable data pipelines that will ingest, transform, and distribute numerous data streams and batches in support of key R&D initiatives  Support and collaborate with Data Scientists developing advanced machine learning and statistical models  Evaluate tools and develop pipelines to capture, integrate and clean data to support edge analytics solutions  Deliver optimal data solution architectures, automation and technology choices starting from experimentation through proof of concept and often through delivery ', ' Familiarity with RESTful Application Programming Interface (API), containers and microservices ', ' Develop and maintain scalable data pipelines that will ingest, transform, and distribute numerous data streams and batches in support of key R&D initiatives ', ' Experience with cloud services (AWS, Azure or GCP) ', ' Hands on experience with relational databases and the use of SQL to extract and manipulate data ', ' Support and collaborate with Data Scientists developing advanced machine learning and statistical models ', ' If you want to join us, you will need: ']",Not Applicable,Full-time,Research,Consumer Goods,2021-03-18 14:34:51
Data Engineer,Saggezza,"Chicago, IL",1 week ago,128 applicants,"['', 'Experience with analytic modeling in a scripting language (Python, R, etc.).An understanding of machine learning techniques and algorithms, such as k-NN, Naive Bayes, SVM, Decision Forests, etc.Experience with common data science toolkits, such as R, Weka, NumPy, MatLab, etc (Depending on specific project requirements).Cloud certification, or any certification related to database or BI Tools.', 'Consulting Magazine - Fastest Growing Firms 2019', 'What We’d Love To See', 'An understanding of machine learning techniques and algorithms, such as k-NN, Naive Bayes, SVM, Decision Forests, etc.', 'Proficiency in modern data tools and cloud technologies would be advantageous, including but not limited to Spark, Hadoop, Tableau, AWS, Azure, Kafka, GCP & IBM Cloud.', 'Analytical mindset and business acumen. Self-motivated, individual contributor.', '5+ years of practical hands-on experience working within data modeling, data extraction, data manipulation, and data warehousing concepts.', 'Cleaning and preparing data for analysis and processing', 'At Saggezza, we are fortunate to have a strong mentorship program that provides every one of our employees the ability to thrive professionally and personally.', 'Examining and reporting results to stakeholders in leadership, technology, marketing, sales, and product teams.', ""A Bachelor's in Computer Science, Information Technology, Mathematics, Engineering or an equivalent field."", '3+ years of working with data modeling and entity-relationship diagrams', 'Why Join Our Team?', 'Entrepreneurial spirit:', 'Expert level experience writing complex SQL queries, including but not limited to stored procedures, functions, views, and triggers.', 'Working across multiple clients and industries to add value and strategic insights within data & analytics.Cleaning and preparing data for analysis and processingProviding proficiency in analyzing data and formulating insights/conclusions. Building, developing and maintaining reporting systems that support key business decisions. Helping clients reach solutions by utilizing data management & operations, data quality & governance, cloud transformation, self-service analytics & visualization, and data intelligence. Working hands-on with SQL/SQL server & Python to deliver analytics to clients. Examining and reporting results to stakeholders in leadership, technology, marketing, sales, and product teams.', 'Knowledge of indexes and how they can be used to enhance query performance.', 'Experience with common data science toolkits, such as R, Weka, NumPy, MatLab, etc (Depending on specific project requirements).', 'Entrepreneurial spirit: We seek individuals who enjoy contributing to the growth of an organization and who show commitment to the success of their team.Problem-solving skills: Individuals at our company have well-honed analytical skills coupled with business acumen to structure problems, deliver solutions, and communicate insights.Drive: Our team sets ambitious goals and seeks energetic professionals, enjoy a fast pace environment, and thrive in taking on responsibility.', 'Building, developing and maintaining reporting systems that support key business decisions. ', 'Helping clients reach solutions by utilizing data management & operations, data quality & governance, cloud transformation, self-service analytics & visualization, and data intelligence. ', 'What You’ll Definitely Need', 'Drive: ', 'Built-In Top Places to Work in Chicago 2020', 'Problem-solving skills: ', 'Our nurturing and supportive environment fosters collaboration across the entire organization.', 'Working hands-on with SQL/SQL server & Python to deliver analytics to clients. ', 'Saggezza is an Equal Employment Opportunity Employer:', '2020 Inc. 5000 List - Honored as one of the fastest-growing private companies in America ', 'We are only as good as our people. Saggezza, Italian for wisdom, is rooted from the perspective that knowledge is power. We create thought-leaders who are constantly exposed and trained in different technologies in the ever-evolving world of software development.', 'Data Engineer', 'Entrepreneurial spirit: We seek individuals who enjoy contributing to the growth of an organization and who show commitment to the success of their team.', 'Great communication and data-oriented personality with strong problem-solving skills.', 'Drive: Our team sets ambitious goals and seeks energetic professionals, enjoy a fast pace environment, and thrive in taking on responsibility.', 'Experience with analytic modeling in a scripting language (Python, R, etc.).', 'We are not hierarchical but operate as a flat surface where every opinion matters, ideas are cultivated and innovation is encouraged.', 'Cloud certification, or any certification related to database or BI Tools.', 'Diverse culture, experiences, and skills.', 'Best and Brightest Companies in the Nation 2019 and 2020, Best and Brightest Companies in Milwaukee 2020 and Best and Brightest Companies in Chicago 2020', ""A Bachelor's in Computer Science, Information Technology, Mathematics, Engineering or an equivalent field.5+ years of practical hands-on experience working within data modeling, data extraction, data manipulation, and data warehousing concepts.Minimum of 5+ years of practical hands-on experience working with SQL/SQL Server and Python for analytics and data science purposes as well as working in a large data warehousing environment.3+ years of working with data modeling and entity-relationship diagramsExpert level experience writing complex SQL queries, including but not limited to stored procedures, functions, views, and triggers.Knowledge of indexes and how they can be used to enhance query performance.Proficiency in modern data tools and cloud technologies would be advantageous, including but not limited to Spark, Hadoop, Tableau, AWS, Azure, Kafka, GCP & IBM Cloud.Analytical mindset and business acumen. Self-motivated, individual contributor.Great communication and data-oriented personality with strong problem-solving skills."", 'Providing proficiency in analyzing data and formulating insights/conclusions. ', 'We welcome innovators with entrepreneurial spirits to grow with our team. ', 'Problem-solving skills: Individuals at our company have well-honed analytical skills coupled with business acumen to structure problems, deliver solutions, and communicate insights.', 'Diverse culture, experiences, and skills.Our nurturing and supportive environment fosters collaboration across the entire organization.We are not hierarchical but operate as a flat surface where every opinion matters, ideas are cultivated and innovation is encouraged.At Saggezza, we are fortunate to have a strong mentorship program that provides every one of our employees the ability to thrive professionally and personally.We are only as good as our people. Saggezza, Italian for wisdom, is rooted from the perspective that knowledge is power. We create thought-leaders who are constantly exposed and trained in different technologies in the ever-evolving world of software development.We welcome innovators with entrepreneurial spirits to grow with our team. Consulting Magazine - Fastest Growing Firms 2019Built-In Top Places to Work in Chicago 2020Best and Brightest Companies in the Nation 2019 and 2020, Best and Brightest Companies in Milwaukee 2020 and Best and Brightest Companies in Chicago 20202020 Inc. 5000 List - Honored as one of the fastest-growing private companies in America ', 'Working across multiple clients and industries to add value and strategic insights within data & analytics.', 'Minimum of 5+ years of practical hands-on experience working with SQL/SQL Server and Python for analytics and data science purposes as well as working in a large data warehousing environment.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer III,Premera Blue Cross,"Mountlake Terrace, WA",3 days ago,Be among the first 25 applicants,"['', 'Life and disability insurance', 'Ability to use Kimball methodology for dimensional data modeling, 3rd Normal form DW', 'Translate needs into requirements and specifications, while maintaining contact with the customers throughout project completion', 'Troubleshoot issues independently and work collaboratively to share lessons learned and provide mentorship to more junior Data Engineers', 'Generous Paid Time Off to reenergize', 'Consult with the business to create understanding of the needs, pace, and direction for our business partners', 'Free parking', 'Bachelor’s degree in computer science, computer engineering, or similar area or 4 years applicable experience', 'Collaborate with data scientists and other analysts to understand and solve business problems', 'Bachelor’s degree in computer science, computer engineering, or similar area or 4 years applicable experience5 years’ experience in data integration, design, and managementKnowledge of software development lifecycle, relational database theory, and skills to utilize one or more programming languagesFamiliarity with healthcare specific regulatory requirements for data managementExperience providing data integration services within healthcare organizationsKnowledge of Tableau, SAS, R, and other analytic toolsAdvanced data processing programming skills across SQL-based and Hadoop-based technologiesAbility to communicate information and ideas verbally and in writing so others will understandKnowledge of Agile and Scrum project methodologies utilizing TFS, Jira/ConfluenceAbility to use Extract Transform Load (ETL) tools (SSIS, Data Stage, Cask)Ability to use Kimball methodology for dimensional data modeling, 3rd Normal form DW', 'What we offer', 'Knowledge of software development lifecycle, relational database theory, and skills to utilize one or more programming languages', 'Solve business and data science problems using data centric programming and scripting skills to create data models and pipelinesConsult with the business to create understanding of the needs, pace, and direction for our business partnersTranslate needs into requirements and specifications, while maintaining contact with the customers throughout project completionTroubleshoot issues independently and work collaboratively to share lessons learned and provide mentorship to more junior Data EngineersCollaborate with data scientists and other analysts to understand and solve business problemsProgram and manage APIs for data exchangeLead and conduct unit and system testing to ensure design is still relevant and implementation is producing a useful, maintainable, reliable product', 'Advanced data processing programming skills across SQL-based and Hadoop-based technologies', 'Program and manage APIs for data exchange', 'Medical, vision and dental coverageLife and disability insuranceRetirement programs (401K employer match and pension plan)Wellness incentives, onsite services, a discount program and moreTuition assistance for undergraduate and graduate degreesGenerous Paid Time Off to reenergizeFree parking', 'Wellness incentives, onsite services, a discount program and more', 'Familiarity with healthcare specific regulatory requirements for data management', 'Tuition assistance for undergraduate and graduate degrees', 'Join Our Team: Do Meaningful Work and Improve People’s Lives ', 'Data Engineer III', 'Solve business and data science problems using data centric programming and scripting skills to create data models and pipelines', 'Ability to communicate information and ideas verbally and in writing so others will understand', 'Retirement programs (401K employer match and pension plan)', 'Lead and conduct unit and system testing to ensure design is still relevant and implementation is producing a useful, maintainable, reliable product', '5 years’ experience in data integration, design, and management', 'What You Will Bring', 'Ability to use Extract Transform Load (ETL) tools (SSIS, Data Stage, Cask)', 'Experience providing data integration services within healthcare organizations', 'What You Will Do', 'Knowledge of Agile and Scrum project methodologies utilizing TFS, Jira/Confluence', 'Knowledge of Tableau, SAS, R, and other analytic tools', 'Equal employment opportunity/affirmative action:', 'Medical, vision and dental coverage']",Not Applicable,Full-time,Information Technology,Insurance,2021-03-18 14:34:51
"Data Reporting Engineer, Data Products",Flatiron Health,"New York, NY",2 weeks ago,47 applicants,"['', 'We Offer', 'Flatiron University training curriculum which includes presentation skills, meeting mastery, coding languages and more', 'Professional development benefit for attending conferences, industry events and external courses', 'You have experience with T-SQL and building reports in SQL Server Reporting Services', 'Work with platform software engineers to improve the data infrastructure that powers clinical analytics ', ' You have healthcare industry knowledge/context (especially oncology-specific knowledge) You have experience with T-SQL and building reports in SQL Server Reporting Services You have experience visualizing data in Looker dashboards You have experience working with Visual Studio, C#, .Net, python, or spark ', 'You have experience visualizing data in Looker dashboards', 'Collaborate with customers and synthesize feedback to design and prototype new reporting products', 'Work/life autonomy via flexible work hours and flexible paid time off', 'What You’ll Do', 'You are organized with strong prioritization and communication skills', 'Build and maintain data pipelines that power parts of our analytics product', 'Maintain cross-functional relationships with customer-facing teams and continually enhance team efficiency', 'Hackathons for all employees (not just our engineers!)', 'Write and edit custom SQL queries to generate customer-facing dashboards and reports for community oncology clinics designed to meet their clinical and operational needs', ' Flatiron University training curriculum which includes presentation skills, meeting mastery, coding languages and more Career coaching opportunities Hackathons for all employees (not just our engineers!) Professional development benefit for attending conferences, industry events and external courses Work/life autonomy via flexible work hours and flexible paid time off Employee Resource Groups (ERGs) that encourage our employees to share their unique experiences and perspectives Generous parental leave (16 weeks for either parent) Back-up child care Flatiron-sponsored fitness classes ', 'Analyze reporting requests to develop solutions addressing common needs across oncology practices', 'Who You Are', 'Why You Should Join Our Team', 'You have experience working with Visual Studio, C#, .Net, python, or spark', 'Back-up child care', 'Extra Credit', 'You have experience with:', 'You have healthcare industry knowledge/context (especially oncology-specific knowledge)', 'Generous parental leave (16 weeks for either parent)', 'You love working with engineering teams to develop analytics content that complements new application features and workflows', ' Write and edit custom SQL queries to generate customer-facing dashboards and reports for community oncology clinics designed to meet their clinical and operational needs Analyze reporting requests to develop solutions addressing common needs across oncology practices Build and maintain data pipelines that power parts of our analytics product Collaborate with customers and synthesize feedback to design and prototype new reporting products Work with platform software engineers to improve the data infrastructure that powers clinical analytics  Maintain cross-functional relationships with customer-facing teams and continually enhance team efficiency ', ' Database performance and interpretation of query execution plans Database scripting and Extract, Transform and Load processes ', 'Flatiron-sponsored fitness classes', 'Database scripting and Extract, Transform and Load processes', 'Employee Resource Groups (ERGs) that encourage our employees to share their unique experiences and perspectives', ' You have experience with:', 'Database performance and interpretation of query execution plans', 'Career coaching opportunities', 'You thrive in a cross-functional environment ']",Not Applicable,Full-time,Research,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,"Karsun Solutions, LLC","Herndon, VA",1 week ago,58 applicants,"['', 'Desired Skills:', '1+ year of experience in data profiling and quality management', 'Experience of working in AWS environments and any AWS certifications is advantageousExperience with big data tools such as EMR/Spark, Databricks/PySpark is an advantageExperience working with database versioning tools such as Flyway is an advantageExperience in Jenkins scripting is an advantage', 'Karsun Solutions pays 100% of employee-only Medical, Dental, Life Insurance, Short Term Disability, and Long-term disability.', 'We offer Vision coverage, 401(k) with immediate vesting, and a competitive PTO policy.', 'The Data Engineer is responsible for developing and supporting the creation of dashboards with efficient queries and adhering to design and performance specifications from various designated data sources. The engineer works with the product teams in order to understand, analyze, document, and efficiently implement reusable components such as reports, filters, context-based navigation, and visualizations utilizing data APIs, relational sources, various file formats, and streaming data.\xa0They are expected to work in close collaboration with data suppliers, customer and data consumer teams for effective and efficient delivery to schedules and requirements.', 'Experience of working in AWS environments and any AWS certifications is advantageous', ""Bachelor's degree in Computer Science or related discipline"", '1+ year experience working with CI/CD tools including Git for etl and scripts repository', 'Experience with big data tools such as EMR/Spark, Databricks/PySpark is an advantage', '5+ years of hands-on experience with supporting and enabling complex dashboards and SQL and NoSQL backends3+ year of work experience on AWS based datasources such as RDS, Athena, Redhsift1+ year of experience using complex data sets and formats such as JSON, AVRO, parquet1+ year of experience in data profiling and quality management1+ year experience working with CI/CD tools including Git for etl and scripts repository', 'The Data Engineer brings passion about applying data visualization & analytics to problems, strong knowledge on modern best practices for data visualization application platforms that support multiple languages such as Python, R, and SQL.\xa0The Data Engineer drives towards a DataOps approach utilizing automation and orchestration for version control, deployments, and code management.\xa0They bring a good understanding of AWS as a platform and the motivation to learn proactively to enhance skills needed to support tasks at hand.', 'The candidate must have a successful track record in handling complex datasets with minimal supervision.\xa0The candidate will be expected to support a variety of structured and semi-structured data in streaming and batch frameworks. Troubleshoot, monitor and coordinate defect resolution related to all dashboard components including access and performance; Responsible for the creation and support of all data visualizations, queries, components and modules within the current scope of the system.', '5+ years of hands-on experience with supporting and enabling complex dashboards and SQL and NoSQL backends', '1+ year of experience using complex data sets and formats such as JSON, AVRO, parquet', 'Understand requirements and specifications such as visualization types, pallettes, and other standards including navigation', 'Responsibilities:', 'The successful candidate will be able to rapidly support & develop visualizations, dashboards, and reusable components using Athena, S3, App UI and Web service backends ', 'Utilize identified python and AWS tools and services such as TabPy and Python libraries and utilities as needed', 'Experience in Jenkins scripting is an advantage', 'Experience working with database versioning tools such as Flyway is an advantage', 'Benefits:', '\xa0', 'The successful candidate will be able to rapidly support & develop visualizations, dashboards, and reusable components using Athena, S3, App UI and Web service backends Understand requirements and specifications such as visualization types, pallettes, and other standards including navigationUtilize identified python and AWS tools and services such as TabPy and Python libraries and utilities as neededCreate and execute medium to complex SQL scripts to validate and test data prior to and during the lifecycle of reports and dataset development', '3+ year of work experience on AWS based datasources such as RDS, Athena, Redhsift', 'Education:', 'Create and execute medium to complex SQL scripts to validate and test data prior to and during the lifecycle of reports and dataset development', 'Required Skills:']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Junior Data Engineer,TalTeam,"New York, NY",3 weeks ago,56 applicants,"['Attention to detail demonstrated in work and communication', 'Requirements:', ' Perform routine functions in support of the Tech Operations data team. Run standardized queries, extracts and parses files, loads data into platforms, and ensures the highest levels of quality. Under the direction of the Tech Operations data team and following strict InfoSec procedures Run standardized queries to extract data from data lakes and other tools. Parses files per platform ingestion limitations. Loads files into platforms (e.g. SFMC, Braze) via UI or other methods. Completes quality assurance and resolves any discrepancies. Document completion and audit trail in Jira. ', 'Experience with Jira preferred', 'Run standardized queries to extract data from data lakes and other tools.', 'Ability to manage multiple tasks simultaneously', 'Experience with email/push/inapp platforms (Salesforce, Braze) preferred', 'Talteam Inc. is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or protected veteran status and will not be discriminated against on the basis of disability.**', 'Jr. Data EngineerNew York, NYResponsibilities Perform routine functions in support of the Tech Operations data team. Run standardized queries, extracts and parses files, loads data into platforms, and ensures the highest levels of quality. Under the direction of the Tech Operations data team and following strict InfoSec procedures Run standardized queries to extract data from data lakes and other tools. Parses files per platform ingestion limitations. Loads files into platforms (e.g. SFMC, Braze) via UI or other methods. Completes quality assurance and resolves any discrepancies. Document completion and audit trail in Jira.  Requirements: Minimum of 2+ years scripting languages and scheduling jobs (SQL, Python) Experience with data platforms Snowflake/Databricks preferred. Experience with email/push/inapp platforms (Salesforce, Braze) preferred Experience with quality assurance and Infosec procedures. Experience with Agile software development methodology (SCRUM, Kanban). Experience with Jira preferred Flexible and willing to assist other team members Able to work in a fast-paced environment with quick turnaround Strong problem solving skills Attention to detail demonstrated in work and communication Ability to manage multiple tasks simultaneously Excellent interpersonal skills Talteam Inc. is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or protected veteran status and will not be discriminated against on the basis of disability.**', 'Strong problem solving skills', 'Responsibilities', 'Completes quality assurance and resolves any discrepancies.', 'Run standardized queries, extracts and parses files, loads data into platforms, and ensures the highest levels of quality.', 'Flexible and willing to assist other team members', 'Jr. Data Engineer', 'Parses files per platform ingestion limitations.', 'Under the direction of the Tech Operations data team and following strict InfoSec procedures', 'Experience with data platforms Snowflake/Databricks preferred.', 'Excellent interpersonal skills', 'Loads files into platforms (e.g. SFMC, Braze) via UI or other methods.', 'Experience with Agile software development methodology (SCRUM, Kanban).', 'Perform routine functions in support of the Tech Operations data team.', 'Able to work in a fast-paced environment with quick turnaround', 'Experience with quality assurance and Infosec procedures.', 'Document completion and audit trail in Jira.', 'Minimum of 2+ years scripting languages and scheduling jobs (SQL, Python)', ' Minimum of 2+ years scripting languages and scheduling jobs (SQL, Python) Experience with data platforms Snowflake/Databricks preferred. Experience with email/push/inapp platforms (Salesforce, Braze) preferred Experience with quality assurance and Infosec procedures. Experience with Agile software development methodology (SCRUM, Kanban). Experience with Jira preferred Flexible and willing to assist other team members Able to work in a fast-paced environment with quick turnaround Strong problem solving skills Attention to detail demonstrated in work and communication Ability to manage multiple tasks simultaneously Excellent interpersonal skills ']",Associate,Contract,Information Technology,Information Technology and Services,2021-03-18 14:34:51
100% REMOTE Data Engineer,CyberCoders,"San Diego, CA",1 day ago,Be among the first 25 applicants,"['', ' Elastic Search, MySQL, MongoDB, Solar', ' Translate source system data into reporting structures based on business metrics definitions', ' Design, test, implement and improve next generation data engine platforms Collaborate with stakeholders to define and document data requirements for services Continue to enhance data systems to efficiently intake, transform, and store massive data sets Design data models that support product and business needs Translate source system data into reporting structures based on business metrics definitions Build datasets for use in internal and external APIs and data products Implement best practices and testing to validate data quality, availability, and reliability', ' SQL and NoSQL databases REST/SOAP APIs and services', 'Your Right to Work', ""At Least 5 Years' Experience In Data Engineering Including"", 'Email Your Resume In Word To', ' Build datasets for use in internal and external APIs and data products', 'CyberCoders, Inc is proud to be an Equal Opportunity Employer', 'Please do NOT change the email subject line in any way. You must keep the JobID: linkedin : PT3-1620622 -- in the email subject line for your application to be considered.***', ' Design, test, implement and improve next generation data engine platforms', ' PHP/Python', 'THIS POSITION IS 100% REMOTE--', ' Collaborate with stakeholders to define and document data requirements for services', ' Elastic Search, MySQL, MongoDB, Solar AWS/GCP/Azure PHP/Python Salesforce or Mercato', ' AWS/GCP/Azure', ' Design data models that support product and business needs', ' REST/SOAP APIs and services', ' Salesforce or Mercato', ' Continue to enhance data systems to efficiently intake, transform, and store massive data sets', ' Implement best practices and testing to validate data quality, availability, and reliability', ' SQL and NoSQL databases']",Mid-Senior level,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Data Platform Engineer,Xsense.ai,"Mountain View, CA",6 days ago,Be among the first 25 applicants,"['', 'What do we provide:', 'Work closely with cross-functional teams to ship data products on time with quality.', 'Strong software design and coding skills using Python/Java/Scala.', '·\xa0Competitive salary', 'Strong knowledge and hands on working experience of Big Data ecosystem components, such as Spark, Hadoop, Kafka, Flink, etc.', 'Design and implement the state of the art data and machine learning pipelines for Autonomous Driving solutions.', 'MS in Computer Science or equivalent (in lieu of degree, relevant work experience). 3+ years of working experience in data platform or infrastructure.', 'We provide autonomous self-driving features for the Xiaopeng electric vehicles in China market. We are looking for people passionate about self-driving vehicles to join in and help us build top class autonomous self-driving vehicles.', 'Design and implement the state of the art data and machine learning pipelines for Autonomous Driving solutions.Work closely with cross-functional teams to ship data products on time with quality.Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Participate in planning, design and code reviews.', '·\xa0A fun, supportive and engaging environment', '·\xa0Opportunities to pursue and work on cutting edge technologies', 'Experience working with Docker and kubernetes.\xa0\xa0', 'We are an Equal Opportunity Employer. It is our policy to provide equal employment opportunities to all qualified persons without regard to race, age, color, sex, sexual orientation, religion, national origin, disability, veteran status or marital status or any other prescribed category set forth in federal or state regulations.', 'Passionate about self-driving technology; excellent communication skills and strong teamwork spirit.', 'Job Responsibilities:', 'Participate in planning, design and code reviews.', 'Basic Qualifications', 'MS in Computer Science or equivalent (in lieu of degree, relevant work experience). 3+ years of working experience in data platform or infrastructure.Strong software design and coding skills using Python/Java/Scala.Strong knowledge and hands on working experience of Big Data ecosystem components, such as Spark, Hadoop, Kafka, Flink, etc.Working knowledge of relational and NoSQL database systems.Passionate about self-driving technology; excellent communication skills and strong teamwork spirit.', '·\xa0Snacks, lunches and fun activities', '\xa0', 'Hands-on experience of working in cloud environments such as AWS, Azure etc.Experience working with Docker and kubernetes.\xa0\xa0', 'Working knowledge of relational and NoSQL database systems.', 'Hands-on experience of working in cloud environments such as AWS, Azure etc.', 'Preferred Qualifications', 'Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer [1917],Accelere,"New York, United States",1 week ago,43 applicants,"['', 'When you submit your resume, please include the hourly contract rate you want to consider for this 100% remote role.\xa0Please also mention how long you have worked remotely!', 'Adheres to and advocates for software engineering best practices (e.g. technical design, technical design review, unit testing, monitoring & alerting, checking in code, code review, documentation),', 'Architects, designs, implements and maintains reliable and scalable data infrastructure.', 'Responsible for service reliability and following site-reliability engineering best practices: on-call rotations for services they maintain, responsible for defining and maintaining SLAs.', 'Mentors others.', 'Development experience in one or more object-oriented programming languages (e.g. Python, Go, Java, C++)', 'Responsibilities', ""Completed Bachelor's degree in Computer Science5+ years of recent professional data engineering experienceDeep and hands-on experience designing, planning, productionizing, maintaining, and documenting reliable and scalable data infrastructure and data products in complex environments.Development experience in one or more object-oriented programming languages (e.g. Python, Go, Java, C++)Advanced\xa0SQL\xa0knowledgeExperience designing and implementing large-scale distributed systemsDeep knowledge and hands-on experience in technologies across all data lifecycle stagesStrong stakeholder management and ability to lead large organizations through the influenceContinuous learning and improvement mindsetNo prior experience in the energy industry required"", 'Part of a cross-disciplinary team, working closely with other data engineers, software engineers, data scientists, data managers, and business partners.', 'Responsible for deploying secure and well-tested software that meets privacy and compliance requirements; develops, maintains, and improves CI/CD pipeline,', 'Writes, deploys, and maintains software to build, integrate, manage, maintain, and quality assure data at the company', 'Advanced\xa0SQL\xa0knowledge', 'Deep and hands-on experience designing, planning, productionizing, maintaining, and documenting reliable and scalable data infrastructure and data products in complex environments.', 'Qualifications', 'Strong stakeholder management and ability to lead large organizations through the influence', 'Continuous learning and improvement mindset', 'Deep knowledge and hands-on experience in technologies across all data lifecycle stages', ""Completed Bachelor's degree in Computer Science"", 'Part of a cross-disciplinary team, working closely with other data engineers, software engineers, data scientists, data managers, and business partners.Architects, designs, implements and maintains reliable and scalable data infrastructure.Writes, deploys, and maintains software to build, integrate, manage, maintain, and quality assure data at the companyAdheres to and advocates for software engineering best practices (e.g. technical design, technical design review, unit testing, monitoring & alerting, checking in code, code review, documentation),Responsible for deploying secure and well-tested software that meets privacy and compliance requirements; develops, maintains, and improves CI/CD pipeline,Responsible for service reliability and following site-reliability engineering best practices: on-call rotations for services they maintain, responsible for defining and maintaining SLAs.Design, build, deploy and maintain infrastructure as code. Containerizes server deployments.Actively contributes to improving developer velocity.Mentors others.', 'Design, build, deploy and maintain infrastructure as code. Containerizes server deployments.', 'Actively contributes to improving developer velocity.', 'If you are a staffing firm, you need to submit candidates with an ""all-inclusive"" hourly rate.', '5+ years of recent professional data engineering experience', 'No prior experience in the energy industry required', 'Experience designing and implementing large-scale distributed systems']",Mid-Senior level,Contract,Engineering,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,ADT,"Boca Raton, FL",1 week ago,29 applicants,"['', 'Ability to develop working tables and data marts that can be referenced and leveraged across multiple users ', 'Analyze ADT customer operational, demographic, 3rd party, and/or big data ', 'Coachable, strong desire to learn, and enthusiasm for creating customers for life', 'Experience using ML tools such as H2O.ai, Alteryx, DataRobot, or equivalent tools preferred', ' Experience with data extraction, cleansing and normalization of data inconsistencies Analyze ADT customer operational, demographic, 3rd party, and/or big data  Utilize correlations, clustering, profiling and other statistical analyses to identify customer insights and drivers affecting retention, engagement & satisfaction Own end-to-end predictive modeling processes including ETL, model creation, calibration, cross-validation, and maximizing model accuracy and performance Develop, maintain, and continuously improve propensity models under the mentorship of senior team members  Have the capability to utilize advanced analytical tools, including statistical modeling and having the ability to interpret and translate the findings into business strategy and results  Develop recommendations including Operational changes based on customer insights Be capable of working closely with the business to define and build implementation strategies to drive business results and measure value creation Develop presentations to illustrate data insights, present key findings and drive key decision-making Provide innovative ideas on how to streamline and optimize new and existing processes and results ', 'Position Responsibilities', '5+ year’s directly applicable experience', 'Own end-to-end predictive modeling processes including ETL, model creation, calibration, cross-validation, and maximizing model accuracy and performance', 'Experience with Tableau, OBIEE, or equivalent tools', 'Outstanding analytical skills; comfortable working with, interpreting and presenting data', 'Have the capability to utilize advanced analytical tools, including statistical modeling and having the ability to interpret and translate the findings into business strategy and results ', 'Expertise and working proficiency with Python and/or R programming languages', 'Be capable of working closely with the business to define and build implementation strategies to drive business results and measure value creation', 'Education', 'Experience with data extraction, cleansing and normalization of data inconsistencies', ' ADT LLC is an Equal Employment Opportunity (EEO) employer. We are committed to having a diverse and inclusive workforce and do our best to foster a culture and environment where every employee feels valued. Our goal is to serve our customers and help save lives. We can achieve this goal when we have the best talent working in an environment where employees feel included and recognized. Visit us online at jobs.adt.com to learn more.', 'Efficiency and comfort with querying and manipulating large data sets', 'P Osition Summary', 'Develop, maintain, and continuously improve propensity models under the mentorship of senior team members ', 'Passion for delivering the ideal customer experience', ' Expertise and proven proficiency with SQL and/or Oracle query languages Expertise and working proficiency with Python and/or R programming languages Experience with Tableau, OBIEE, or equivalent tools Experience using ML tools such as H2O.ai, Alteryx, DataRobot, or equivalent tools preferred Efficiency and comfort with querying and manipulating large data sets Ability to develop working tables and data marts that can be referenced and leveraged across multiple users  Experience working within a big data environment preferred  Outstanding analytical skills; comfortable working with, interpreting and presenting data Solid sense of business acumen alongside strong verbal and written communication skills Passion for delivering the ideal customer experience Coachable, strong desire to learn, and enthusiasm for creating customers for life ', 'Provide innovative ideas on how to streamline and optimize new and existing processes and results', 'Experience', 'Expertise and proven proficiency with SQL and/or Oracle query languages', 'Utilize correlations, clustering, profiling and other statistical analyses to identify customer insights and drivers affecting retention, engagement & satisfaction', 'Solid sense of business acumen alongside strong verbal and written communication skills', 'Skills And Capabilities', '4-year college degree in either analytics, mathematics, operations or computer science, statistics, or similar highly quantitative degree required with a graduate degree preferred', 'Develop presentations to illustrate data insights, present key findings and drive key decision-making', ' 4-year college degree in either analytics, mathematics, operations or computer science, statistics, or similar highly quantitative degree required with a graduate degree preferred ', ' 5+ year’s directly applicable experience ', 'Company Overview', 'Develop recommendations including Operational changes based on customer insights', 'Experience working within a big data environment preferred ']",Entry level,Full-time,Information Technology,Construction,2021-03-18 14:34:51
Data Engineer,ADM,"Erlanger, KY",3 weeks ago,Be among the first 25 applicants,"['', 'Essential Job Functions', 'Data Engineer- Erlanger, KY ', ' Document solutions through high/low level design documentation ', ' Strong collaboration skills working with design and development teams ', ' Knowledge in programming scripting languages like Java, Scala and Python ', 'About ADM', ' Work simultaneously on multiple projects without sacrificing delivery  Work with data warehouse and data integration teams to ensure successful delivery of enterprise data warehouse solutions  Have natural curiosity and real passion for data investigation, talent for strategic leadership, interest in digital product development, and experience in people management  Lead development of strategy in an environment rich in complex biological, environmental, operational, global economic, and business data  Establish and manage collaborations engaging business units to develop novel data analytic approaches and coordinated decision science solutions  Self-starter who is organized , communicative, quick learner, and team-oriented  Prepare and present ideas and recommendations to colleagues and management  Document solutions through high/low level design documentation  Learn new groundbreaking data engineering and analytic tools as needed ', ' Motivated, demonstrates initiative and leadership ', ' 1-3 years of work experience or equivalent academic background in data processing using Python ', 'Additional Responsibilities Of This Role Will Include The Following', ' Strong written communication skills including functional design documentation ', ' 4-year Bachelor’s degree or equivalent in IT, Computer Science, science, engineering, statistics, programming or mathematical field ', 'EEO', ' Mapping data ecosystems and creating data model diagrams ', ' Required education: ', ' Fluency in SQL ', ' Experience in delivering solutions using iterative development methodologies like Agile, KanBan, DevOps, etc. ', 'Position Summary', ' Working knowledge of BI architecture, data warehousing concepts, and data integration standard methodologies ', ' 5+ years of hands-on experience crafting and implementing data and analytics solutions  1-3 years of work experience or equivalent academic background in data processing using Python  Fluency in SQL  Experience delivering Cloud based Data Solutions in Azure  Experience working with Big Data technologies (Spark, Kafka, DataBricks, Hive, or equivalent)  Knowledge in programming scripting languages like Java, Scala and Python  Experience in delivering solutions using iterative development methodologies like Agile, KanBan, DevOps, etc.  Working knowledge of BI architecture, data warehousing concepts, and data integration standard methodologies  Experience with metadata management discipline and practices  High accountability with a demonstrated ability to deliver  Strong written communication skills including functional design documentation  Strong collaboration skills working with design and development teams  Motivated, demonstrates initiative and leadership  Able to critically think and be solution-driven with strong interpersonal skills and able to work easily with team members  Great organizational, time management and problem-solving skills  Possesses a professional attitude ', ' Great organizational, time management and problem-solving skills ', ' 5+ years of hands-on experience crafting and implementing data and analytics solutions ', ' Analyzing data and developing insights (e.g., via data visualization tools like Excel/Power BI/Tableau ', ' Understanding of software development principles such as project architecture, version control (Git), test-driven development, etc. ', ' Experience delivering Cloud based Data Solutions in Azure ', ' Work simultaneously on multiple projects without sacrificing delivery ', ' This is an exempt level position. ', ' Creating custom queries, scripts, and job runs for ad hoc data processing and/or data investigation in Python ', ' Learn new groundbreaking data engineering and analytic tools as needed ', ' Experience with metadata management discipline and practices ', ' Establishing and maintaining information security standards ', ' Monitor and troubleshoot data issues in solution pipelines ', ' High accountability with a demonstrated ability to deliver ', ' Prepare and present ideas and recommendations to colleagues and management ', ' Extending traditional ETL solutions (Informatica, Alteryx, SQL, etc.) ', ' Able to critically think and be solution-driven with strong interpersonal skills and able to work easily with team members ', ' Possesses a professional attitude ', ' Automating data flows with resilient, production-grade code ', ' Have natural curiosity and real passion for data investigation, talent for strategic leadership, interest in digital product development, and experience in people management ', ' Job Requirements: ', ' Developing prototypes and proof of concepts visualizations/dashboards for the selected solutions ', ' Work with data warehouse and data integration teams to ensure successful delivery of enterprise data warehouse solutions ', ' Creating custom queries, scripts, and job runs for ad hoc data processing and/or data investigation in Python  Understanding and navigating a wide array of source data systems (enterprise data warehouses, relational databases, IT systems, in house and COTS applications, documents, APIs, unstructured data, big data, NoSQL databases, etc.)  Understanding of software development principles such as project architecture, version control (Git), test-driven development, etc.  Automating data flows with resilient, production-grade code  Monitor and troubleshoot data issues in solution pipelines  Developing prototypes and proof of concepts visualizations/dashboards for the selected solutions  Establishing and maintaining information security standards  Mapping data ecosystems and creating data model diagrams  Extending traditional ETL solutions (Informatica, Alteryx, SQL, etc.)  Analyzing data and developing insights (e.g., via data visualization tools like Excel/Power BI/Tableau ', ' Self-starter who is organized , communicative, quick learner, and team-oriented ', ' Lead development of strategy in an environment rich in complex biological, environmental, operational, global economic, and business data ', ' Understanding and navigating a wide array of source data systems (enterprise data warehouses, relational databases, IT systems, in house and COTS applications, documents, APIs, unstructured data, big data, NoSQL databases, etc.) ', ' Experience working with Big Data technologies (Spark, Kafka, DataBricks, Hive, or equivalent) ', ' Establish and manage collaborations engaging business units to develop novel data analytic approaches and coordinated decision science solutions ']",Not Applicable,Full-time,Information Technology,Food Production,2021-03-18 14:34:51
Data Engineer,the agency worX,"New York, NY",1 week ago,Be among the first 25 applicants,"['', 'Requirements:', 'Daily Tasks / Average Day:', 'Run standardized queries to extract data from data lakes and other tools.', 'Ability to manage multiple tasks simultaneously', 'Experience with email/push/inapp platforms (Salesforce, Braze) preferred', 'Run standardized queries to extract data from data lakes and other tools.Parses files per platform ingestion limitations.Loads files into platforms (e.g. SFMC, Braze) via UI or other methods.Completes quality assurance and resolves any discrepancies.Document completion and audit trail in Jira.', 'Contract: 9-18 months (W2 ONLY)', 'Data Engineer - TOP MASS MEDIA CLIENT - New York - REMOTE - Contract', 'Strong problem solving skills', 'Completes quality assurance and resolves any discrepancies.', 'Minimum of 2+ years scripting languages and scheduling jobs (SQL, python)Experience with data platforms Snowflake/Databricks preferred.Experience with email/push/inapp platforms (Salesforce, Braze) preferredExperience with quality assurance and Infosec procedures.Experience with Agile software development methodology (SCRUM, Kanban).Experience with Jira preferred.', 'Parses files per platform ingestion limitations.', 'Experience with Jira preferred.', 'Experience with data platforms Snowflake/Databricks preferred.', '\ufeff Must Have', 'Able to work in a fast paced environment with quick turnaroundStrong problem solving skillsAttention to detailAbility to manage multiple tasks simultaneouslyExcellent interpersonal skills.', 'Loads files into platforms (e.g. SFMC, Braze) via UI or other methods.', 'Experience with Agile software development methodology (SCRUM, Kanban).', 'Attention to detail', 'Location: Remote (East Coast Hours)', 'Experience with quality assurance and Infosec procedures.', 'Job Description', 'Document completion and audit trail in Jira.', 'Excellent interpersonal skills.', 'Able to work in a fast paced environment with quick turnaround', 'Minimum of 2+ years scripting languages and scheduling jobs (SQL, python)']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Analyst/Engineer,The Laughton Team,"Peoria, AZ",2 days ago,Be among the first 25 applicants,"['', 'Snowflake, Teradata or alike', 'Ability to identify, analyze and solve complex problems', 'Provides support for deployed data applications and analytical models; Identifies data problems and guides issue resolutions', ' $70,000 - $80,000 ', 'Big Data stacks/ecosystem including Kafka, Spark, Python, NoSQL', 'Ability to work within a team environment as well as independently', '2+ years Data analysis, business analysis, data application programming, ETL development, next generation databases, or related', 'We invite you to join our mission', 'Experience in data science and mining', 'What We Offer', 'Sponsorship is not available for this position', 'Health Insurance: We offer a variety of comprehensive medical, dental, and vision plans with low out-of-pocket expenses', 'Provides data and technical consulting during data application design; Provide technical consulting on data composition and data engineering', 'Major Responsibilities', 'Assists with the development of models, analytic processes, and reports; Provides guidance on the development of data consumption processes', 'API Nation, Integromat, Zapier', 'Understanding of relational databases, data integration tools:', 'Experience leading a high performing work team', 'Paid time off including holidays and sick leave', 'Career Advancement: Training, coaching and development, as well as growth within the organization', 'Experience with writing complex SQL queries', 'AWS, Snowflake', 'Experience in agile process and technology', ' Experience with writing complex SQL queries Experience with Cloud-based technologies:', ' Snowflake, Teradata or alike Ab Initio or Informatica or Datastage ', '$70,000 - $80,000', 'In lieu of education, 4+ years of experience in Data analysis, business analysis, data application programming, ETL development, next generation databases, or related', ' Preferred Bachelor’s degree in Information Technology. 2+ years Data analysis, business analysis, data application programming, ETL development, next generation databases, or related In lieu of education, 4+ years of experience in Data analysis, business analysis, data application programming, ETL development, next generation databases, or related ', 'Preferred Bachelor’s degree in Information Technology.', 'Strong team player who is able to work across multiple functions and disciplines', 'Excellent attention to detail', ' AWS, Snowflake API Nation, Integromat, Zapier Big Data stacks/ecosystem including Kafka, Spark, Python, NoSQL ', 'Expertise in enabling business intelligence solutions through data integration; including roles that span the complete BI lifecycle, from strategy to ETL to report implementation', ' Competitive compensation package Health Insurance: We offer a variety of comprehensive medical, dental, and vision plans with low out-of-pocket expenses Paid time off including holidays and sick leave Career Advancement: Training, coaching and development, as well as growth within the organization ', 'Compensation', ' Partner with business and technology teams to develop data applications. Identify business data ingestion and processing frameworks. Coordinates and obtains data application requirements from the business. Translates business requirements to development teams. Assist with development and testing processes. Coordinate data application and model deployments and validations. Assists with the development of models, analytic processes, and reports; Provides guidance on the development of data consumption processes Ensures data governance policies are followed by implementing and validating data lineage, quality checks, classification, etc. Provides support for deployed data applications and analytical models; Identifies data problems and guides issue resolutions Provides data and technical consulting during data application design; Provide technical consulting on data composition and data engineering Demonstrated experience with integration technologies and how to leverage them into data mapping between systems Experience with transforming business requirements into technical specification for data designs and solutions Experience in agile process and technology Strong oral and written communication skills; Demonstrated ability to clearly articulate information or solution Experience leading a high performing work team Good time management skills (i.e. works efficiently) Excellent attention to detail Ability to work within a team environment as well as independently Ability to identify, analyze and solve complex problems Strong team player who is able to work across multiple functions and disciplines Expertise with the design and development of ETL data solutions Understanding of relational databases, data integration tools:', 'Basic Qualifications', 'Ensures data governance policies are followed by implementing and validating data lineage, quality checks, classification, etc.', 'Qualifications You’ll Need', 'Experience with transforming business requirements into technical specification for data designs and solutions', 'Demonstrated experience with integration technologies and how to leverage them into data mapping between systems', 'Experience with Cloud-based technologies:', 'Competitive compensation package', 'Good time management skills (i.e. works efficiently)', 'Strong oral and written communication skills; Demonstrated ability to clearly articulate information or solution', 'Get To Know Us', 'Expertise with the design and development of ETL data solutions', 'Partner with business and technology teams to develop data applications. Identify business data ingestion and processing frameworks. Coordinates and obtains data application requirements from the business. Translates business requirements to development teams. Assist with development and testing processes. Coordinate data application and model deployments and validations.', 'Ab Initio or Informatica or Datastage', 'Work Authorization/security Clearance Requirements', 'Preferred Qualifications', ' Sponsorship is not available for this position']",Entry level,Full-time,Information Technology,Real Estate,2021-03-18 14:34:51
Data Engineer - Data Management & Governance,Cityblock Health,"New York, NY",1 week ago,Be among the first 25 applicants,"['', 'About Us:', 'Requirements for the Role:', 'You have 3+ years of data modeling and SQL experience.', 'A short cover letter, please!', ' You have a passion for doing mission-oriented work. You are an excellent communicator to a diverse set of audiences including stakeholders, product managers, peers, and direct reports. You are a self-motivated learner. You have experience and enjoy building and improving cloud-native data pipelines and warehouses. You have 3+ years of data modeling and SQL experience. You have 3+ years of experience building data pipelines. ', 'Previous exposure to clinical operations and/or working with physicians', 'Aim for Understanding', 'You have experience and enjoy building and improving cloud-native data pipelines and warehouses.', 'You are an excellent communicator to a diverse set of audiences including stakeholders, product managers, peers, and direct reports.', 'About the Role:', ' Experience with ELT and modeling of health data a plus Experience with the cloud native data technologies Experience with the Google Cloud Platform Previous exposure to clinical operations and/or working with physicians ', 'Triage issues effectively and use that information to help guide the work of the team to make our data products more reliable and maintainable.', 'We do not accept unsolicited resumes from outside recruiters/placement agencies. Cityblock will not pay fees associated with resumes presented through unsolicited means.', 'Our Values:', ' Aim for Understanding Be All In Bring Your Whole Self Lean Into Discomfort Put Members First ', 'Build and maintain strong relationships with your collaborators, including fellow Engineering teams, the Product team, Data Analytics, Data Science, and Actuary', 'everyone ', 'Create an engaging work environment for the Data Management and Governance Engineering team by focusing on the most impactful work, mentoring others, modeling transparency, and clearly communicating.', ""Define, maintain, and promote standards for the team's infrastructure, code, and development processes."", 'Experience with the cloud native data technologies', 'You are a self-motivated learner.', 'Experience with the Google Cloud Platform', 'Embrace JIRA and other team collaboration tools to effectively identify, track, and communicate priorities both for day-to-day and long-term planning.', 'You have 3+ years of experience building data pipelines.', ""What We'd Like From You:"", ' A resume and/or LinkedIn profile  A short cover letter, please! ', 'Be All In', 'Lean Into Discomfort', 'Bring Your Whole Self', 'How We Define Success:', 'You have a passion for doing mission-oriented work.', 'Put Members First', 'Collaborate with the DMG Product Manager and technical leadership to support and co-own a strong technical vision, strategy, and roadmap for impactful health data warehousing and resulting data products.', 'A resume and/or LinkedIn profile ', "" Collaborate with the DMG Product Manager and technical leadership to support and co-own a strong technical vision, strategy, and roadmap for impactful health data warehousing and resulting data products. Build and maintain strong relationships with your collaborators, including fellow Engineering teams, the Product team, Data Analytics, Data Science, and Actuary Create an engaging work environment for the Data Management and Governance Engineering team by focusing on the most impactful work, mentoring others, modeling transparency, and clearly communicating. Embrace JIRA and other team collaboration tools to effectively identify, track, and communicate priorities both for day-to-day and long-term planning. Rapidly deliver value to delight our stakeholders, care teams, and ultimately our members. Triage issues effectively and use that information to help guide the work of the team to make our data products more reliable and maintainable. Define, maintain, and promote standards for the team's infrastructure, code, and development processes. "", 'Rapidly deliver value to delight our stakeholders, care teams, and ultimately our members.', 'Experience with ELT and modeling of health data a plus', 'Nice to Have, But Not Required:']",Entry level,Full-time,Information Technology,Nonprofit Organization Management,2021-03-18 14:34:51
Data Engineer,Brady Corporation,Greater Milwaukee,1 week ago,Be among the first 25 applicants,"['', 'Required Knowledge, Skills & Abilities', 'Provide support in fixing database and data quality issues via reverse engineering.', 'Advanced experience leveraging various strategies for ingesting, modelling, processing, and persisting data as well as excellent analytical and problem-solving skills.', 'Bachelor’s degree in CIS, MIS, IT, or related Database Theory degree; or equivalent experience4+ years of experience in the development and maintenance of complex data systemsWorking knowledge of emerging database technologies such as Elastic Search, Big Query, Mongo or equivalent.Excellent database tuning techniques and practices, including partitioning, performance tuning, and working with application development teams for enhanced performanceAt least 1 year of Spark experience.Advanced experience leveraging various strategies for ingesting, modelling, processing, and persisting data as well as excellent analytical and problem-solving skills.Troubleshoot live site issues, engage appropriate parties, and drive through to resolution', 'Master’s degree, MBA or Ph.D. degree is a plus.', '\ufeff', 'Adhere to timelines and excel in a fast-paced, high-energy environment.', 'At least 1 year of Spark experience.', 'Monitor data warehouse eco-system and identify opportunities to make enhancements.', 'Position Reports To:\xa0', 'Desired Knowledge, Skills & Abilities', 'Cloud Experience preferred (e.g. AWS, Azure, Google Cloud Platform, etc.). Demonstrated Azure experience with Data Factory, Analysis Services, SQL Managed Instance, Synapse, and DevOps.', 'Excellent database tuning techniques and practices, including partitioning, performance tuning, and working with application development teams for enhanced performance', 'Drive data best practices and contribute to the development of overall data strategy and roadmap.', 'Bachelor’s degree in CIS, MIS, IT, or related Database Theory degree; or equivalent experience', 'Working knowledge of emerging database technologies such as Elastic Search, Big Query, Mongo or equivalent.', 'Participate in brainstorming sessions and contribute ideas to our technology, algorithms, and products.', '4+ years of experience in the development and maintenance of complex data systems', 'Lead design sessions to develop ETL logic that meets business and data scientists’ requirements.', 'Position Summary:\xa0', 'Provide mentoring on database systems to other development and business teams.', 'Position Summary:\xa0Brady IDS BI is interested in hiring a data engineer to work closely with business leaders, the data scientist team, and corporate IT to create data pipelines to support data-driven business decision-making processes.', 'Troubleshoot live site issues, engage appropriate parties, and drive through to resolution', 'Essential Duties and Responsibilities:', ""Create and build robust data structures to support end user's analysis and decision-making across multiple business verticals. This includes both end-to-end architecting and business solutions."", ""Lead design sessions to develop ETL logic that meets business and data scientists’ requirements.Create and build robust data structures to support end user's analysis and decision-making across multiple business verticals. This includes both end-to-end architecting and business solutions.Provide mentoring on database systems to other development and business teams.Monitor data warehouse eco-system and identify opportunities to make enhancements.Provide support in fixing database and data quality issues via reverse engineering.Contribute to and care about data quality efforts, ensuring data accuracy and transparency for our stakeholders.Participate in brainstorming sessions and contribute ideas to our technology, algorithms, and products.Adhere to timelines and excel in a fast-paced, high-energy environment.Drive data best practices and contribute to the development of overall data strategy and roadmap."", 'Master’s degree, MBA or Ph.D. degree is a plus.Cloud Experience preferred (e.g. AWS, Azure, Google Cloud Platform, etc.). Demonstrated Azure experience with Data Factory, Analysis Services, SQL Managed Instance, Synapse, and DevOps.', 'The successful candidate will have strong verbal and written communication skills and effectively communicate with the client and internal team. A strong understanding of databases, SQL, cloud technologies, and modern data integration and orchestration tools.', 'Contribute to and care about data quality efforts, ensuring data accuracy and transparency for our stakeholders.', 'Position Reports To:\xa0\xa0Director of Business Intelligence']",Mid-Senior level,Full-time,Analyst,Printing,2021-03-18 14:34:51
Data Engineer II,NOV,"Conroe, TX",2 weeks ago,Be among the first 25 applicants,[''],Entry level,Full-time,Information Technology,Banking,2021-03-18 14:34:51
Data Engineer,"Adroit Resources, Inc.","San Jose, CA",2 weeks ago,128 applicants,"['', 'Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.', 'Optimize data delivery, automate manual processes, re-design infrastructure for improved scalability and identify, design and implement data ingestion, data delivery process improvements.Collaborate with analytics, data scientists and product teams to assist in building and optimizing innovative products and services to marketDevelop analytics tools that utilize data pipeline to provide actionable insights into customer acquisition, operational efficiency and key business performance metrics.Build big data pipelines, data sets and conduct performance tuningWorking knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.Experience supporting and working with cross-functional teams in a dynamic environmentKeep data separated and secure across architectural boundaries through multiple data centres and AWS regions.', 'Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.', 'Collaborate with analytics, data scientists and product teams to assist in building and optimizing innovative products and services to market', 'What You Know', 'Keep data separated and secure across architectural boundaries through multiple data centres and AWS regions.', 'Skills', 'Experience supporting and working with cross-functional teams in a dynamic environment', 'Experience with relational SQL and NoSQL databases such as Cassandra.', '\ufeffHadoop, Spark, Kafka, SQL, NoSQL, Cassandra/MongoDB, AWS/Azure/GCP, Python/Java/C++/Scala', 'Optimize data delivery, automate manual processes, re-design infrastructure for improved scalability and identify, design and implement data ingestion, data delivery process improvements.', '8+ years of experience in a Data Engineer roleGraduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.Experience with big data tools: Hadoop, Spark, Kafka, etc.Experience with relational SQL and NoSQL databases such as Cassandra.Experience with AWS cloud services: EC2, EMR, AthenaExperience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.', 'Develop analytics tools that utilize data pipeline to provide actionable insights into customer acquisition, operational efficiency and key business performance metrics.', 'Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.', 'Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.', ""What You'll Do"", 'Experience with big data tools: Hadoop, Spark, Kafka, etc.', 'Build big data pipelines, data sets and conduct performance tuning', '8+ years of experience in a Data Engineer role', 'Experience with AWS cloud services: EC2, EMR, Athena']",Mid-Senior level,Contract,Information Technology,Staffing and Recruiting,2021-03-18 14:34:51
Data Engineer - Digital health,OpenClinica,"Waltham, MA",3 days ago,58 applicants,"['', 'Flexible Spending Account', '401k plan with generous matching', 'Required skills and experience. You might be a good candidate for this job if you have:', 'Excellent written and verbal communication skills, including the ability to explain ideas to both technical and nontechnical people.', 'BS and or Masters in Computer Science or equivalent experience.', 'Health and dental, and life insurance', 'Free coffee to keep you peppy', 'Short and long term disability', '5+ years of experience building and maintaining data products in a stringent security and quality assurance environment.', 'Competitive base + bonus401k plan with generous matchingFlexible Spending AccountHealth and dental, and life insuranceShort and long term disabilityFlexible paid time offA professional development budget just for you!Free parking to keep you saneFree gym to keep you healthyFree coffee to keep you peppyA fun culture and happy people!', 'A professional development budget just for you!', '5+ years of professional experience (not internship) building and maintaining solutions for business intelligence, ETL, and data integration.', '5+ years of professional experience (not internship) building and maintaining solutions for business intelligence, ETL, and data integration.5+ years of experience building and maintaining data products in a stringent security and quality assurance environment.5+ years of experience working with and testing complex relational databases, SQL (PostgreSQL), and Python.Excellent written and verbal communication skills, including the ability to explain ideas to both technical and nontechnical people.Familiarity with agile, git, BDD (Cucumber / Gherkin), DevOps, and related processes/technologies.Strong desire to learn, push the envelope, and share knowledge with others.Excellent written and verbal communication skills, including the ability to explain ideas to both technical and nontechnical people.Prior experience working with healthcare or life science software, and/or contributing to open source software projects a plus.BS and or Masters in Computer Science or equivalent experience.', '5+ years of experience working with and testing complex relational databases, SQL (PostgreSQL), and Python.', 'Competitive base + bonus', 'Free gym to keep you healthy', 'A fun culture and happy people!', 'Familiarity with agile, git, BDD (Cucumber / Gherkin), DevOps, and related processes/technologies.', 'Job Location:', 'Strong desire to learn, push the envelope, and share knowledge with others.', 'Prior experience working with healthcare or life science software, and/or contributing to open source software projects a plus.', 'Flexible paid time off', 'Free parking to keep you sane', 'Compensation & Perks']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer - Remote,"Yoh, A Day & Zimmermann Company","New York, NY",,N/A,"['', 'Experience utilizing tools like: Snowflake, Apache Airflow, Apache Spark, Looker, Appsflyer (Data Locker)', 'Company Data Knowledge Center:\xa0Become a go-to expert around internal data and train people on how to interpret the documentation and use Looker explores.', 'For data reporting, dashboards and visualization, we are ideally seeking experience in Appsflyer, Tableau or PowerBI. ', 'Responsibilities', 'Data Flows:\xa0Ensure that data flows accurately from all sources to Snowflake and meets established SLAs. Transforming raw internal data into custom attributes for CRM systems.System Administration:\xa0Monitor the performance of Snowflake and introduce optimizations. Manage regular Airflow deployments and updates.Project Management:\xa0Gather business requirements and create technical implementations and end-user documentation.Reporting Automation:\xa0Create Looker Looks and Dashboards for internal and external audiences.Company Data Knowledge Center:\xa0Become a go-to expert around internal data and train people on how to interpret the documentation and use Looker explores.Data Modeling:\xa0Active participation in data modeling and data structure design with the Product and Engineering teams.', 'We are seeking a Data Analytics Engineer to join our client’s team. This is a Full-Time Remote opportunity. You will be responsible for gathering requirements to create technical solutions. The ideal candidate will have deep experience in Data Analysis, Data Engineering, Data Modeling, Data Transformation, Data Ingestion, Data Reporting and Visualization using tools like Snowflake, Apache Spark, PySpark, and SQL. This role will also require coding experience in Python. ', 'Project Management:\xa0Gather business requirements and create technical implementations and end-user documentation.', 'Qualifications', 'Data Modeling:\xa0Active participation in data modeling and data structure design with the Product and Engineering teams.', 'Please Note: This is not a C2C opportunity. We need Full-Time W2 applicants.', 'Bachelor’s Degree in Computer Science, Information Technology & Management or related fields required. 4-6 + years of Data Analysis and Data Engineering. Experience utilizing tools like: Snowflake, Apache Airflow, Apache Spark, Looker, Appsflyer (Data Locker)Strong SQL skills are a core technical requirement.Strong Python coding skills. .Prior experience with Spark required.', '4-6 + years of Data Analysis and Data Engineering. ', 'Bachelor’s Degree in Computer Science, Information Technology & Management or related fields required. ', 'Strong Python coding skills. .', 'Reporting Automation:\xa0Create Looker Looks and Dashboards for internal and external audiences.', 'Strong SQL skills are a core technical requirement.', 'Prior experience with Spark required.', '\xa0', 'Data Flows:\xa0Ensure that data flows accurately from all sources to Snowflake and meets established SLAs. Transforming raw internal data into custom attributes for CRM systems.', 'System Administration:\xa0Monitor the performance of Snowflake and introduce optimizations. Manage regular Airflow deployments and updates.']",Mid-Senior level,Full-time,Information Technology,Insurance,2021-03-18 14:34:51
Big Data Engineer,CareerAddict,"Herndon, VA",2 days ago,Be among the first 25 applicants,"['', 'Participate in all team ceremonies including planning, grooming, product demonstration and team retrospectives', 'Expert proficiency in Object Oriented Design (OOD) and analysis.', ' Support the team in the writing of deployment scripts and place strong emphasis in automated deployment, infrastructure automation solutions, and continuous delivery process. Work with product owners and other development team members to determine new features and user stories needed in large/complex development projects Create or Update documentation in support of development efforts. Documents may include detailed specifications, implementation guides, architecture diagrams or design documents. Participate in code reviews with peers and managers to ensure that each increment adheres to original vision as described in the user story and all standard resource libraries and architecture patterns as appropriate. Respond to trouble/support calls for applications in production in order to make quick repair to keep application in production. Serve as a technical lead for an Agile team and actively participate in all Agile ceremonies. Participate in all team ceremonies including planning, grooming, product demonstration and team retrospectives Mentor or provide technical guidance to less experienced staff; may use high end development tools to assist or facilitate development process. Leverage tool stack to build, inspect, deploy, test and promote new or updated features. May serve as technical lead, architect, project lead or principle developer in course of large or complex project. Expert proficiency in unit testing as well as coding in 1-2 languages (eg Java, etc). Expert proficiency in Object Oriented Design (OOD) and analysis. Expert proficiency in application of analysis/design engineering functions. Expert proficiency in application of non-functional software qualities such as resiliency, maintainability, etc. Expert proficiency in advanced behavior-driven testing techniques. Provide expertise for teams in all matters related to deployment, building and release process. ', ' 8-10 years of related experience; Highly experienced with Agile practices/methodologies (eg Scrum, TDD, BDD, etc). Highly experienced in the use continuous integration tools (eg Jenkins, Hudson, etc) and infrastructure automation (VM Ware, Puppet, Chef, Vagrant, Docker, etc). Develops and maintains scalable data pipelines and builds out new API integrations to support continuing increases in data volume and complexity. Strong analytic skills related to working with unstructured datasets A successful history of manipulating, processing and extracting value from large disconnected datasets Build the infrastructure required to process data from a variety of data sources using SQL. Create data tools for analytics and data scientists to optimize data Experience working with either a Map Reduce or an MPP system on any size/scale Experience with big data tools: Hadoop, Spark, Kafka, etc. 5+ years of Experience with object-oriented/object function Scripting languages: Python, Java, C++, Scala, etc. ', 'Responsibilities', 'Highly experienced in the use continuous integration tools (eg Jenkins, Hudson, etc) and infrastructure automation (VM Ware, Puppet, Chef, Vagrant, Docker, etc).', 'Respond to trouble/support calls for applications in production in order to make quick repair to keep application in production.', 'Expert proficiency in advanced behavior-driven testing techniques.', 'Experience working with either a Map Reduce or an MPP system on any size/scale', 'Strong analytic skills related to working with unstructured datasets', 'Develops and maintains scalable data pipelines and builds out new API integrations to support continuing increases in data volume and complexity.', 'Qualifications', 'Preferred Skills', ' AWS cloud services: EC2, EMR, RDS, Redshift SQL experience and No-SQL experience is a plus', 'Serve as a technical lead for an Agile team and actively participate in all Agile ceremonies.', 'Provide expertise for teams in all matters related to deployment, building and release process.', 'AWS cloud services: EC2, EMR, RDS, Redshift', 'Expert proficiency in application of analysis/design engineering functions.', 'Support the team in the writing of deployment scripts and place strong emphasis in automated deployment, infrastructure automation solutions, and continuous delivery process.', 'May serve as technical lead, architect, project lead or principle developer in course of large or complex project.', 'A successful history of manipulating, processing and extracting value from large disconnected datasets', 'Expert proficiency in application of non-functional software qualities such as resiliency, maintainability, etc.', 'Expert proficiency in unit testing as well as coding in 1-2 languages (eg Java, etc).', 'Leverage tool stack to build, inspect, deploy, test and promote new or updated features.', 'Create or Update documentation in support of development efforts. Documents may include detailed specifications, implementation guides, architecture diagrams or design documents.', 'Participate in code reviews with peers and managers to ensure that each increment adheres to original vision as described in the user story and all standard resource libraries and architecture patterns as appropriate.', 'Build the infrastructure required to process data from a variety of data sources using SQL.', 'SQL experience and No-SQL experience is a plus', 'Mentor or provide technical guidance to less experienced staff; may use high end development tools to assist or facilitate development process.', 'Experience with big data tools: Hadoop, Spark, Kafka, etc.', 'Work with product owners and other development team members to determine new features and user stories needed in large/complex development projects', 'Create data tools for analytics and data scientists to optimize data', '5+ years of Experience with object-oriented/object function Scripting languages: Python, Java, C++, Scala, etc.', '8-10 years of related experience; Highly experienced with Agile practices/methodologies (eg Scrum, TDD, BDD, etc).']",Entry level,Full-time,Engineering,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Holiday Inn Club Vacations,"Orlando, FL",6 days ago,25 applicants,"['', 'At Holiday Inn Club Vacations, we believe in strengthening families. And we look for people who exhibit the courage, caring, and creativity to help us become the most loved brand in family travel. We’re committed to growing our people, memberships, resorts, and guest love. That’s why we need individuals who are passionate in life and bring those qualities to work every day. Do you instill confidence, trust, and respect in those around you? Do you encourage success and build relationships? If so, we’re looking for you.', 'This position will be responsible for architecting the overall structure of data and data-related resources as an integral part of the Insights, Analytics, and Data team, as well as any other duties as required to successfully implement the data strategy. The Data Engineer role will act as a technical subject matter expert in support of several business units within our enterprise. It entails close collaboration with the Senior Analysts, and Management of the internal IAD department, as well as frequent interdepartmental collaborations. Works independently and must be extremely analytical and precise. Daily activities include, but not limited to, creating ETL processes, data modeling within SQL and Power BI, creating data dictionary definitions, complying with data governance practices and standards.', 'Intermediate to advanced level of MS Office Suite', '2 plus years of experience with data modeling and visualization (Power BI preferred)', 'POSITION DESCRIPTION:', 'Ability to adapt in fast-paced work environment', 'QUALIFICATIONS:', 'Special projects and ad-hoc reporting as required. (i.e. Assisting in data migration processes, query optimization, assist IT job troubleshooting).', 'Craft, develop and manipulate MS SQL databases, tables, queries, and stored procedures.', '3 plus years of experience with SQL Server (Tables, Views, Stored Procedures, Functions)', 'Microsoft Certifications a plus', 'Experience with Power Query (M) and DAX', 'ESSENTIAL DUTIES AND TASKS:', 'Identify and define common data requirements for use in models and metadata.', 'Advanced level of SQL proficiency preferred', 'Bachelor’s Degree in Computer Science, Engineering, or related business area preferred3 plus years of experience with SQL Server (Tables, Views, Stored Procedures, Functions)2 plus years of experience with ETL Processes (SSIS)2 plus years of experience with data modeling and visualization (Power BI preferred)Microsoft Certifications a plusTimeshare/Hospitality industry experience a plusWritten and verbal communication and organization skills are criticalBasic understanding of data warehousing design/conceptsIntermediate to advanced level of MS Office SuiteAdvanced level of SQL proficiency preferredExperience with Power Query (M) and DAXStrong attention to detailAbility to adapt in fast-paced work environmentAbility to prioritize work to meet deadlines', 'Create Power BI Reporting Solutions – Modeling, Visualization, and Documentation.Craft, develop and manipulate MS SQL databases, tables, queries, and stored procedures.ETL Processes – mapping, coding, testing.Identify and define common data requirements for use in models and metadata.Special projects and ad-hoc reporting as required. (i.e. Assisting in data migration processes, query optimization, assist IT job troubleshooting).', 'Strong attention to detail', 'Timeshare/Hospitality industry experience a plus', 'Create Power BI Reporting Solutions – Modeling, Visualization, and Documentation.', 'Bachelor’s Degree in Computer Science, Engineering, or related business area preferred', '2 plus years of experience with ETL Processes (SSIS)', 'Basic understanding of data warehousing design/concepts', 'Ability to prioritize work to meet deadlines', 'ETL Processes – mapping, coding, testing.', 'Written and verbal communication and organization skills are critical']",Associate,Full-time,Analyst,Hospitality,2021-03-18 14:34:51
Data Engineer (BigQuery/Tableau),"Cook Systems, Inc.","Memphis, TN",1 day ago,52 applicants,"['', 'Ability to translate business ask to technical requirements.', 'This is a technical role and requires the candidate to have expertise in relational databases like SQL Server and Tableau Visualization with at least 4 plus years of experience.', 'e/o/e', 'Responsible for designing, developing and implementing data models with Statistical and Machine learning expertise. Responsible for Data Mining and Analysis from company databases to drive optimization and improvement of Client and Media Dashboards.', 'Should have experience in Data Mapping and Data analytics along with ability to recommend solutions to optimize the SQL queries for visualizations as well as organizing and storing, the large scale data.', 'In this role, candidate is expected to provide technical expertise and support in the design, development, and implementation of Audience centric Dashboards.', 'Experience in Digital technology, Google Big query and AWS is a plus.', '\xa0', 'COOK SYSTEMS INTERNATIONAL has an IMMEDIATE need for a Data Engineer with Google BigQuery and Tableau', ""Requires Bachelor's degree in Computer Information Systems, or other related field and 6 year experience in an interactive marketing role with emphasis in analytics; or a Master's degree, preferably in Analytics, and 4 years' experience in an interactive marketing role with emphasis in analytics."", 'To be successful in this role candidate must be able to work effectively in a fluid, fast-paced environment while maintaining good communication with management and team members and be able to mentor entry level analysts. Ability to develop experimental and analytic plans for data modeling processes, use of strong baselines, ability to accurately determine cause and effect relations. Understanding of relevant statistical measures such as confidence intervals, significance of error measurements, development and evaluation data sets, etc. Track record of diving into data to Client hidden patterns and of conducting error/deviation analysis. Design and develop data visualizations, complex reports and dashboards based on business requirements using complex SQL/Tableau. Must have Business Intelligence experience using data warehouse tools such as Business Objects preferred. Extensive experience solving analytical problems using quantitative approaches, operations research and optimization algorithms. Comfort manipulating and analyzing complex, high-volume, high dimensionality data from varying sources.\xa0', 'Required:']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-03-18 14:34:51
"Data Engineer, Operations Decision Science",Delta Air Lines,"Atlanta, GA",6 days ago,37 applicants,"['', 'Design, implement data engineering solutions (e.g. locate and extract data from a variety of sources for use in reporting, analysis, and statistical modeling to drive continuous improvement)', 'Define and execute the data engineering roadmap (e.g. establish an operational data lake and a real-time reporting environment for the operations)', 'Proficiency in SQL (CTE, window functions, temporal data)', 'Embraces diverse people, thinking and styles', 'Working knowledge of statistical/machine learning tools (e.g. SAS,R, TensorFlow) preferred', 'Must be able to interact and collaborate at all levels within Operations Analysis & Performance, OCC, cross-divisional working groups, and outside entities', 'Working knowledge of ""Big Data"" solutions such as Hadoop, NoSQL, MapReduce, etc preferred', ""Bachelor's degree in Data Science, Statistics, Mathematics, Operations Research, Computer Science, or equivalent combination of education and experience (Master's degree preferred)"", 'Mentor junior members in technical proficiency and business acumen', ""Provide technical leadership to the Delta's business units"", 'Primary Functions', 'Strong project management, organizational, and prioritizations skills', ""Bachelor's degree in Data Science, Statistics, Mathematics, Operations Research, Computer Science, or equivalent combination of education and experience (Master's degree preferred)Embraces diverse people, thinking and stylesConsistently makes safety and security, of self and others, the priorityProficiency in SQL (CTE, window functions, temporal data)Proficiency in Python"", 'Consistently makes safety and security, of self and others, the priority', 'Working knowledge of statistical/machine learning tools (e.g. SAS,R, TensorFlow) preferredWorking knowledge of ""Big Data"" solutions such as Hadoop, NoSQL, MapReduce, etc preferredWorking knowledge of API Consumption preferredStrong written, oral communication, and interpersonal skillsStrong project management, organizational, and prioritizations skillsMust be able to interact and collaborate at all levels within Operations Analysis & Performance, OCC, cross-divisional working groups, and outside entitiesMust be performing satisfactorily in present position', 'Must be performing satisfactorily in present position', 'Interface with business unit leaders to develop and maintain internal customer relationships', 'Lead complex process improvement and project management engagements for both individual business units and cross-divisional initiatives.', 'Proficiency in Python', ""Design, implement data engineering solutions (e.g. locate and extract data from a variety of sources for use in reporting, analysis, and statistical modeling to drive continuous improvement)Define and execute the data engineering roadmap (e.g. establish an operational data lake and a real-time reporting environment for the operations)Partner with Information Technology to optimize and enhance the database environment for optimal efficiency and best practicesProvide technical leadership to the Delta's business unitsLeverage emerging technologies and identify efficient and meaningful ways to disseminate data and analysis in order to satisfy the business’ needsMentor junior members in technical proficiency and business acumenLead complex process improvement and project management engagements for both individual business units and cross-divisional initiatives.Interface with business unit leaders to develop and maintain internal customer relationshipsPractices safety-conscious environment resulting in employee safety and well-being"", 'Working knowledge of API Consumption preferred', 'Leverage emerging technologies and identify efficient and meaningful ways to disseminate data and analysis in order to satisfy the business’ needs', 'Strong written, oral communication, and interpersonal skills', 'Practices safety-conscious environment resulting in employee safety and well-being', 'Partner with Information Technology to optimize and enhance the database environment for optimal efficiency and best practices']",Entry level,Full-time,Information Technology,Airlines/Aviation,2021-03-18 14:34:51
Data Engineer - PySpark - AWS,Fannie Mae,"Herndon, VA",4 weeks ago,39 applicants,"['', 'Serve as a technical lead for an Agile team and actively participate in all Agile ceremonies. Participate in all team ceremonies including planning, grooming, product demonstration and team retrospectives.', 'Set up and configure a continuous integration environment.', 'Apache Spark experience', 'Experience with application integrations such as RESTful Web Services, and File/Data transfers, etc.', 'Apache Spark experienceETL experience Pyspark, AWS GlueHands-on experience in implementation and development using cloud technologies (AWS)Knowledge of UNIX (Linux) environment, scripting (bash, shell) and AutosysExperience with application integrations such as RESTful Web Services, and File/Data transfers, etc.Experience with CI/CD with knowledge of Git Hub, Maven and JenkinsExperience with Agile Development Methodology', 'Bachelor’s Degree or Equivalent Experience (required)', 'Hands-on experience in implementation and development using cloud technologies (AWS)', 'Bachelor’s Degree or Equivalent Experience (required)4-6 years of related experience2+ years of development experience using Pyspark', 'Work with product owners and other development team members to determine new features and user stories needed in new/revised applications or large/complex development projects.', 'Responsibilities', 'Respond to trouble/support calls for applications in production in order to make quick repair to keep application in production.', 'Experience with Agile Development Methodology', 'Knowledge of UNIX (Linux) environment, scripting (bash, shell) and Autosys', 'Qualifications', 'Leverage Fannie Mae DevOps tool stack to build, inspect, deploy, test and promote new or updated features.', 'Experience with CI/CD with knowledge of Git Hub, Maven and Jenkins', 'Company Description', 'Advanced proficiency in unit testing as well as coding in 1-2 languages (e.g. Java, etc).', '2+ years of development experience using Pyspark', '4-6 years of related experience', 'THE IMPACT YOU WILL MAKE', 'Work with product owners and other development team members to determine new features and user stories needed in new/revised applications or large/complex development projects.Create or Update documentation in support of development efforts. Documents may include detailed specifications, implementation guides, architecture diagrams or design documents.Participate in code reviews with peers and managers to ensure that each increment adheres to original vision as described in the user story and all standard resource libraries and architecture patterns as appropriate.Respond to trouble/support calls for applications in production in order to make quick repair to keep application in production.Serve as a technical lead for an Agile team and actively participate in all Agile ceremonies. Participate in all team ceremonies including planning, grooming, product demonstration and team retrospectives.Mentor less experienced technical staff; may use high end development tools to assist or facilitate development process.Leverage Fannie Mae DevOps tool stack to build, inspect, deploy, test and promote new or updated features.Set up and configure a continuous integration environment.Advanced proficiency in unit testing as well as coding in 1-2 languages (e.g. Java, etc).Advanced proficiency in Object Oriented Design (OOD) and analysis. Advanced proficiency in application of analysis/design engineering functions. Advanced proficiency in application of non-functional software qualities such as resiliency, maintainability, etc. Advanced proficiency in advanced behavior-driven testing techniques.', 'Basic Qualifications', 'Create or Update documentation in support of development efforts. Documents may include detailed specifications, implementation guides, architecture diagrams or design documents.', 'Participate in code reviews with peers and managers to ensure that each increment adheres to original vision as described in the user story and all standard resource libraries and architecture patterns as appropriate.', 'ETL experience Pyspark, AWS Glue', 'Job Description', 'THE EXPERIENCE YOU BRING TO THE TEAM', 'Mentor less experienced technical staff; may use high end development tools to assist or facilitate development process.', 'Preferred Qualifications', 'Advanced proficiency in Object Oriented Design (OOD) and analysis. Advanced proficiency in application of analysis/design engineering functions. Advanced proficiency in application of non-functional software qualities such as resiliency, maintainability, etc. Advanced proficiency in advanced behavior-driven testing techniques.']",Entry level,Full-time,Information Technology,Financial Services,2021-03-18 14:34:51
Data Engineer,FLYR Labs (We're Hiring),"San Francisco, CA",1 week ago,114 applicants,"['', 'Complete complex customer ingests and data warehousing projects.', 'Our hyper-accurate contextual forecasts enable the most effective scheduling, marketing, and leadership decisions while directly managing the pricing for billions of dollars worth of product and revenue.', 'Develop and implement best practices and standardized, repeatable processes for implementing enterprise-grade data transformation/ingest pipelines, with an eye toward scalability.\xa0', '• Flexible working arrangements — full-time remote eligible from anywhere in US', '• Comprehensive healthcare plans (Choice of PPO & HMO available)', 'Engage with customers from data discovery to ETL development, to data QA/QC metrics determination and delivery SLAs.\xa0', ""• Follow-up Call - A more in-depth call focusing more on the specifics of candidates' finance backgrounds."", 'We are on a path to become the single largest provider of commercial intelligence and automation across the travel and transportation industry.', '• Herman Miller chairs and Autonomous SmartDesks', 'At FLYR, we are proud to be an equal opportunity workplace and embrace a commitment to have our teams better reflect the world around us as we scale the business.', ""Collaborate closely with FLYR's Poland-based data platform team to define tool requirements to support onboarding and ingest of new data sources."", 'Our Commitment to Fairness', 'Experience working with large-scale, complicated datasets.', 'Quantitative work/education background (computer science or equivalent).Ability to ship production-quality Python code.Hands-on experience with cloud computing services, Airflow, and BigQuery.Previous experience building and operating data transformation pipelines.Experience working with large-scale, complicated datasets.Experience with data warehouses, ETL automation, BI visualization tools, and cloud-based data management tools.Advanced SQL. You know your way around analytical and aggregate functions, complex joins, window functions, and are confident in wrangling all types of data in SQL.Can identify impediments to customer onboarding and propose improvements to processes. Can work with Product, Data Science, and Data Platform teams to define product and platform capabilities to improve customer onboarding.Ability to clearly communicate status, blockers, risks, and dependencies to FLYR and customer stakeholders.', 'Our Vision', 'What can you bring on this trip?', '• Follow-up Call - A hiring manager at FLYR will have a follow-up, in-depth conversation with candidates.', 'Advanced SQL. You know your way around analytical and aggregate functions, complex joins, window functions, and are confident in wrangling all types of data in SQL.', 'Starting with airlines, we provide the Cirrus Revenue Operating System™ that reshapes how travel and transportations businesses plan their commercial operation. We displace legacy data, forecasting, pricing, and reporting solutions with a single enterprise SaaS platform that leverages the latest advancements in deep learning, cloud computing, and user experience.', 'Defining platform data validation test suites.\xa0', 'About FLYR’s product', '• Onsite/Virtual Onsite - A series of meetings with the hiring team/team members to finalize our evaluation.', 'Ability to clearly communicate status, blockers, risks, and dependencies to FLYR and customer stakeholders.', 'At Our SFHQ', 'Impact of COVID-19', 'Hands-on experience with cloud computing services, Airflow, and BigQuery.', '• Technical Assignment/Deep Dive - A brief technical assignment touching the necessary skills listed above provided on a take-home basis to be utilized during the onsite.', '• Introductory Call - A FLYR crew member will reach out for an initial call to learn more about each other.', '• Bright, modern office in excellent SoMa location', '• Dog-friendly Candidate Selection Process', 'What will your destination look like?', 'Experience with data warehouses, ETL automation, BI visualization tools, and cloud-based data management tools.', 'Quantitative work/education background (computer science or equivalent).', 'Previous experience building and operating data transformation pipelines.', '• Equity in Series B startup with high growth potential', ""Founder Alex Mans started FLYR in 2013, initially with the intent to remove inefficiencies in airline pricing for travelers. Since then, we've shifted focus to address the forecasting and pricing problems at the core of major airlines, and we have built a team with a wide range of experiences in enterprise technology, aviation, and entrepreneurship. We are a dynamic and fast-growing travel tech start-up headquartered in the SoMa neighborhood in San Francisco with a European development hub in Krakow, Poland."", '• High career growth potential', '• Generous PTO policy and paid holidays', 'Cirrus™, our Revenue Operating System, is deployed by major commercial airlines both internationally and domestically. Cirrus leverages an extensive data- and deep learning infrastructure that was built from the ground up to solve airlines’ most complex commercial challenges.', ""While the effects of COVID-19 on commercial aviation (our primary client base) have been widely publicized, demand from airlines around the globe has exceeded all expectations. On the back of incredible outperformance compared to legacy incumbents in our space, we are on track for an exceptionally strong 2021 and beyond. While the road to travel recovery isn’t easy, you won't find any other industry as dedicated to continuous improvement and reinvention."", 'Learn our customers’ business needs and apply business acumen to ensure the success of highly technical projects.\xa0', 'Provide mentorship and training to newer crew members.', 'Ability to ship production-quality Python code.', '\xa0', '• 401K with company match', '• Initial Screening - Active candidates are screened against the criteria listed above.', 'Benefits ', 'Our Selection Process:', ""Complete complex customer ingests and data warehousing projects.Develop and implement best practices and standardized, repeatable processes for implementing enterprise-grade data transformation/ingest pipelines, with an eye toward scalability.\xa0Engage with customers from data discovery to ETL development, to data QA/QC metrics determination and delivery SLAs.\xa0Develop and implement best practices and standardized, repeatable processes for implementing enterprise-grade data transformation/ingest pipelines, with an eye toward scalability.\xa0Learn our customers’ business needs and apply business acumen to ensure the success of highly technical projects.\xa0Collaborate closely with FLYR's Poland-based data platform team to define tool requirements to support onboarding and ingest of new data sources.Defining platform data validation test suites.\xa0Provide mentorship and training to newer crew members."", 'Can identify impediments to customer onboarding and propose improvements to processes. Can work with Product, Data Science, and Data Platform teams to define product and platform capabilities to improve customer onboarding.']",Mid-Senior level,Full-time,Information Technology,Internet,2021-03-18 14:34:51
Data Engineer,InfoVision Inc.,"Plano, TX",2 weeks ago,82 applicants,"['Preferred Qualifications:', ""Bachelor's DegreeAt least 3 years of experience with AWSAt least 3 years of experience with Python programmingAt least 3 years’ experience with SQLAt least 2 years’ experience with Spark"", 'At least 3 years’ experience with SQL', 'At least 2 years of experience with Linux, server automation and scripting.', ""Bachelor's Degree"", 'At least 3 years of experience with AWS', '\xa0', 'At least 3 years of experience with Python programmingAt least 2 years of experience in technology delivery in a Cloud Engineering environment.At least 2 years of experience with Linux, server automation and scripting.At least 1 year of experience working with Agile Development Practices.AWS Certified', 'AWS Certified', 'Basic Qualifications:', 'At least 2 years of experience in technology delivery in a Cloud Engineering environment.', 'At least 3 years of experience with Python programming', 'At least 1 year of experience working with Agile Development Practices.', 'At least 2 years’ experience with Spark']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Ocher Technology Group,"Atlanta, GA",3 days ago,Be among the first 25 applicants,[''],Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Horizontal Talent,"Schaumburg, IL",2 weeks ago,82 applicants,"['', 'Utilize Enterprise Technology assets, when applicable.', 'Generally work is self-directed and not prescribed.', 'Criteria to meet-', ' A bachelor’s degree in Computer Sciences or related field, or equivalent work experience; master’s degree preferred 8+ years of experience in software development 4+ years of experience leading small/medium teams Skilled in Java, Micro services, Spring Boot, SQL, Hibernate Good understanding and ability to debug JavaScript, React.js, HTML, CSS Able to contribute towards test automation and DevOps pipelines Strong interpersonal skills, coupled with equally strong Team Building and Communication Sense of urgency in daily work ethic Strong leadership, organizational and time management skills Experience in Pega (nice to have) Criteria to meet- Experience - 6+ years in web application development. Must have worked on building green field applications. Technologies Proficient in - Java (High), OpenShift/Kubernetes (Medium), SQL (High), Spring Boot / Spring Batch (High), Pega (Low/Medium) ', 'Works with less structured, more complex issues.', 'Technologies Proficient in - Java (High), OpenShift/Kubernetes (Medium), SQL (High), Spring Boot / Spring Batch (High), Pega (Low/Medium)', '4+ years of experience leading small/medium teams', '1 Product Owner; 1 Scrum Master; 5 Developers.', 'You will be responsible to Adopt Enterprise Architecture guidelines Utilize Enterprise Technology assets, when applicable. Work with Product owner to develop stories and user flows. Set up CI/CD Pipelines and maintain. Generally work is self-directed and not prescribed. Works with less structured, more complex issues. Serves as a resource to others.  ', ' 1 Product Owner; 1 Scrum Master; 5 Developers. ', 'b. Video vs. phone? Video  ', 'Strong interpersonal skills, coupled with equally strong Team Building and Communication', 'Set up CI/CD Pipelines and maintain.', 'Video', 'Work with Product owner to develop stories and user flows.', 'Strong leadership, organizational and time management skills', ' Adopt Enterprise Architecture guidelines Utilize Enterprise Technology assets, when applicable. Work with Product owner to develop stories and user flows. Set up CI/CD Pipelines and maintain. Generally work is self-directed and not prescribed. Works with less structured, more complex issues. Serves as a resource to others. ', 'Serves as a resource to others.', 'Experience in Pega (nice to have)', 'Good understanding and ability to debug JavaScript, React.js, HTML, CSS', 'Description', '2 rounds', ' Technical ', 'c. How technical will the interviews be? Technical  ', 'Able to contribute towards test automation and DevOps pipelines', ' a. How many rounds? 2 rounds   b. Video vs. phone? Video   c. How technical will the interviews be? Technical   ', 'In the role of Lead Software Engineer, Come and join our teams in building our Clinical Products and other applications supporting. ', 'You will work independently and in a team as needed, in developing new features to build new products and enhance our applications as needed. ', 'A bachelor’s degree in Computer Sciences or related field, or equivalent work experience; master’s degree preferred', 'Sense of urgency in daily work ethic', 'a. How many rounds? 2 rounds  ', 'Adopt Enterprise Architecture guidelines', ' Video ', 'Skills/attributes', 'Our platforms are primarily web applications, built on Micro services Architecture (Commonly used stacks - Azure Functions, Spring Boot, Spring Batch, Java, Oracle, OpenShift, React) and other Enterprise Vendor Technologies (Pega).', 'Interview Process', ' 2 rounds ', '8+ years of experience in software development', 'In this role, you will train yourself in coding on the Pega platform and other new technologies, as needed, utilizing Enterprise training assets', 'Skilled in Java, Micro services, Spring Boot, SQL, Hibernate', 'Team And Team Size', ' In the role of Lead Software Engineer, Come and join our teams in building our Clinical Products and other applications supporting.  You will work independently and in a team as needed, in developing new features to build new products and enhance our applications as needed.  Our platforms are primarily web applications, built on Micro services Architecture (Commonly used stacks - Azure Functions, Spring Boot, Spring Batch, Java, Oracle, OpenShift, React) and other Enterprise Vendor Technologies (Pega). You will be responsible to Adopt Enterprise Architecture guidelines Utilize Enterprise Technology assets, when applicable. Work with Product owner to develop stories and user flows. Set up CI/CD Pipelines and maintain. Generally work is self-directed and not prescribed. Works with less structured, more complex issues. Serves as a resource to others.   In this role, you will train yourself in coding on the Pega platform and other new technologies, as needed, utilizing Enterprise training assets ', 'Experience - 6+ years in web application development. Must have worked on building green field applications.', 'Technical']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Sr Data Engineer ( Power BI & ALTRYX ) (100% remote opportunity),"ChaTeck,Inc","Seattle, WA",2 days ago,Be among the first 25 applicants,"['', 'Preferred Background']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Vivid Resourcing Ltd,"Austin, Texas Metropolitan Area",3 weeks ago,120 applicants,"['', '**Unfortunately this position can not sponsor at this time. If you are authorized to work in the US you are encouraged to apply (US citizen or GC holder)**\xa0', 'Fully paid medical benefits', 'Send me your resume at carlos.rueda@vividresourcing.com', 'Fulltime Direct Hire\xa0', '2-5 years in Data Engineering or Analytics3+ years in SQL3+ years in Python, Scala, or Spark.Experienced in AWS but okay with GCP or AzureBonus if you have experience with Snowflake or KafkaExperience building ETL pipelines', 'Remote flexibility', 'Experience building ETL pipelines', 'Unlimited PTO', 'With a growing team, you will make a big impact on the direction they are looking to move forward and ultimately have the opportunity to grow with the company!', '3+ years in Python, Scala, or Spark.', '2-5 years in Data Engineering or Analytics', '3+ years in SQL', 'Location: Austin, TX (Remote)', 'What they offer:', 'Data Engineer', 'Competitive salary + bonus structureRemote flexibilityFully paid medical benefitsUnlimited PTOAmazing office culture; hosted lunches, happy hours, and special events!', 'Experienced in AWS but okay with GCP or Azure', 'Please reach out if you are interested.', 'Competitive salary + bonus structure', 'Bonus if you have experience with Snowflake or Kafka', 'Currently recruiting for an excellent Data Engineer in the Austin area for an exciting tech company! They are looking to grow their Data platform team extensively this year and are looking to bring on two Data Engineers.', 'Skills:', 'Amazing office culture; hosted lunches, happy hours, and special events!']",Mid-Senior level,Full-time,Information Technology,Staffing and Recruiting,2021-03-18 14:34:51
Data Engineer,"U.S. Xpress, Inc.","Chattanooga, TN",2 weeks ago,45 applicants,"['', 'Relentlessly Delivering Big Ideas.', 'Unlimited Vacation', 'What You’ll Do', 'Experience with ETL tools (such as Informatica)', 'Develop robust, scalable solutions for collecting & analyzing large data sets. Must be proficient in creating & maintaining data pipelines.Proficiency in developing packages, stored procedures, functions, triggers, and complex SQL statements. Experience with ETL tools (such as Informatica)Design logical data models and their physical schema design. Some experience programming in Java & Python. Big data knowledge is a plusExperience with Git and/or bitbucket is a plus. Ability to work with multiple data sources and types (structured/semi-structured/unstructured)Cloud experience (AWS/Azure/Google)', 'Bachelors or Master’s Degree in Computer Science, Information Systems, or a related field.', 'Some experience programming in Java & Python. Big data knowledge is a plus', 'POWER OF U.S.', 'Partner with Data Owners to design solutions that align to data governance and data management principles best practices.', 'Experience with Git and/or bitbucket is a plus. ', 'Ability to work with multiple data sources and types (structured/semi-structured/unstructured)', 'High level expertise with SQL. Graph database experience is highly desired.Partner with Data Owners to design solutions that align to data governance and data management principles best practices.Data Visualization with PowerBI / Tableau / QlikView or equivalent is a plusAbility to work in a fast-paced, agile and dynamic environment with both virtual and face-to-face interactions.Strong collaborative mindset, good judgment with great interpersonal skills required to help solve complex business problems.', 'Data Visualization with PowerBI / Tableau / QlikView or equivalent is a plus', 'Work Environment / Physical Requirements – Normal Office Settings.', 'Design logical data models and their physical schema design. ', 'Ability to work in a fast-paced, agile and dynamic environment with both virtual and face-to-face interactions.', 'High level expertise with SQL. Graph database experience is highly desired.', 'Proficiency in developing packages, stored procedures, functions, triggers, and complex SQL statements. ', 'Who We Are', 'What We’re Looking For', 'Experience', ' Medical, Dental and Vision Unlimited Vacation Paid Parental Leave Tuition Reimbursement On-site work out facility ', 'Cloud experience (AWS/Azure/Google)', 'Tuition Reimbursement', 'Medical, Dental and Vision', 'Data Engineer', 'Paid Parental Leave', 'Strong collaborative mindset, good judgment with great interpersonal skills required to help solve complex business problems.', 'On-site work out facility', 'Develop robust, scalable solutions for collecting & analyzing large data sets. Must be proficient in creating & maintaining data pipelines.', 'Why U.S. Xpress?']",Entry level,Full-time,Information Technology,Construction,2021-03-18 14:34:51
Data Engineer - Santa Clara,Decision Minds,"Santa Clara, CA",19 hours ago,Be among the first 25 applicants,"[' Engineer who can work with business and understand business processes Build Batch ETL with Spark Programming skills in python (Preferred), Scala/ Kafka in GCP Environment. Strong in Spark (Scala or pyspark) Basic SQL skills to build views/Tables in Big query. Knowledge on airflow to schedule airflow is good to have.""', ' Basic SQL skills to build views/Tables in Big query.', ' Engineer who can work with business and understand business processes', ' Strong in Spark (Scala or pyspark)', 'Job Description', ' Build Batch ETL with Spark Programming skills in python (Preferred), Scala/ Kafka in GCP Environment.', ' Knowledge on airflow to schedule airflow is good to have.""', ' 8+ years of experience Engineer who can work with business and understand business processes Build Batch ETL with Spark Programming skills in python (Preferred), Scala/ Kafka in GCP Environment. Strong in Spark (Scala or pyspark) Basic SQL skills to build views/Tables in Big query. Knowledge on airflow to schedule airflow is good to have.""']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Makena Partners,"Seattle, WA",1 week ago,Be among the first 25 applicants,"['', 'A day in the life', 'Use machine learning to implement near real time anomaly detection against data as it flow through our platform.', 'You are proficient in one of: Python, Apache Spark, and SQL.', 'Strong written and oral communication skills.', 'Bonus experience with: Java, Machine Learning, Anomaly Detection, AWS: Lambda, S3, SQS, SNS, DynamoDB.', 'Work with a small agile team to design and develop ETLT, Big Data, and machine learning implementations.Develop tools that allow teams to configure high performance, highly scalable ETLT pipelines using Apache Spark, Python, and AWS EMR.Use machine learning to implement near real time anomaly detection against data as it flow through our platform.Develop Java and Node based micro-services and lambdas as part of the Accolade Cloud Platform.Contribute to engineering best practices and help shape the future of our big data and platform technologies.Support internal customers of our ETLT and customer configuration tooling including researching and validating detected anomalies, failed pipelines, and monitor alarms.Work with our internal users and product team to gather requirements and feedback.', 'Minimum of 2 years of experience designing and building Big Data and ETLT products using cloud native solutions.You are proficient in one of: Python, Apache Spark, and SQL.Some production experience with cloud native Big Data or ETL.Experience with AWS and AWS EMR is a plus.Bonus experience with: Java, Machine Learning, Anomaly Detection, AWS: Lambda, S3, SQS, SNS, DynamoDB.Desire and willingness to work in an Agile, collaborative, innovative, flexible, and team-oriented environment.Strong written and oral communication skills.', 'Contribute to engineering best practices and help shape the future of our big data and platform technologies.', 'Some production experience with cloud native Big Data or ETL.Experience with AWS and AWS EMR is a plus.', 'Experience with AWS and AWS EMR is a plus.', 'Work with a small agile team to design and develop ETLT, Big Data, and machine learning implementations.', 'Develop tools that allow teams to configure high performance, highly scalable ETLT pipelines using Apache Spark, Python, and AWS EMR.', 'Minimum of 2 years of experience designing and building Big Data and ETLT products using cloud native solutions.You are proficient in one of: Python, Apache Spark, and SQL.', 'Role Overview', 'Desire and willingness to work in an Agile, collaborative, innovative, flexible, and team-oriented environment.', 'What We Are Looking For', 'Support internal customers of our ETLT and customer configuration tooling including researching and validating detected anomalies, failed pipelines, and monitor alarms.', 'Develop Java and Node based micro-services and lambdas as part of the Accolade Cloud Platform.', 'Work with our internal users and product team to gather requirements and feedback.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer - Pittsburgh,Idelic,"Pittsburgh, PA",6 days ago,Be among the first 25 applicants,"['', 'Comfortable with Linux, BASH, git', 'About Idelic', 'Analytical and Problem Solving Skills', 'Familiarity with training machine learning models', 'Assist with devops and automation prior to full production releases', 'A Dynamic and Supportive Environment ', 'Experience with Python, SQL', 'Experience deploying machine learning models in productionFamiliarity with training machine learning modelsExperience with containerization and KubernetesExperience with TensorFlow Serving', '3+ years of experience building backend architecture for distributed systemsBachelor’s or equivalent degree in computer science, or a related fieldExperience with Python, SQLComfortable with Linux, BASH, gitExperience working within AWS and AWS technologies (e.g. EC2, RDS, VPC, etc.)Analytical and Problem Solving SkillsProven ability to work in a collaborative and fast-paced environment', 'What You’ll Do', '401(k) with Company Matching Funds', 'Experience with containerization and Kubernetes', 'Design and implement microservices to run machine learning models', 'Experience deploying machine learning models in production', 'What Will Set You Apart', 'Be Part of a Small Team (to Start)— Which Translates To You Having A Big Personal Impact', 'Working Conditions', 'Help implement the next version of a distributed task queue for data processing', '3+ years of experience building backend architecture for distributed systems', 'Support data access layers and data ingestion for PostgreSQL databases', 'About Our Team', 'What You’ll Need To Succeed In The Role', 'Regular Company Outings and Events', 'Medical, Dental, Vision and Life Insurance', 'Competitive Compensation Package Including OptionsMedical, Dental, Vision and Life Insurance401(k) with Company Matching FundsRegular Company Outings and EventsKickstarter Company Breakfast every Monday / Company Lunch Every FridayA Dynamic and Supportive Environment Professional Development OpportunitiesBe Part of a Small Team (to Start)— Which Translates To You Having A Big Personal Impact', 'Kickstarter Company Breakfast every Monday / Company Lunch Every Friday', 'Work on any task and help solve problems when needed — be humble and scrappy!', 'Competitive Compensation Package Including Options', 'Professional Development Opportunities', 'Experience with TensorFlow Serving', 'Overview Of The Role', 'Proven ability to work in a collaborative and fast-paced environment', 'Experience working within AWS and AWS technologies (e.g. EC2, RDS, VPC, etc.)', 'Bachelor’s or equivalent degree in computer science, or a related field', 'Design and implement microservices to run machine learning modelsHelp implement the next version of a distributed task queue for data processingSupport data access layers and data ingestion for PostgreSQL databasesAssist with devops and automation prior to full production releases']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Cloud Data Engineer,Indigo Slate,"Renton, WA",4 weeks ago,66 applicants,"['', ""What's in it for you?"", 'Maintain code and scripting version control repositories and related systems supporting initiatives around “infrastructure as code”. \uf0b7', '\uf0b7 Proactively support product health by building solutions that are automated, scalable, and sustainable – be relentlessly focused on minimizing defects and technical debt \uf0b7', ""Bachelor's degree in Computer Science, Informatics, Business Administration or related field requiredMaster’s Degree in Computer Science, Business Administration or related field preferred"", 'Work with Systems Engineers, Developers, Architects, and Cloud Engineers to design and implement automated solution with CI/CD pipelines for infrastructure and software build, deployment, and configuration. \uf0b7', 'Build and document automated data pipelines from a wide range of data sources with an emphasis on automation and scale \uf0b7', 'Work with Systems Engineers, Developers, Architects, and Cloud Engineers to design and implement automated solution with CI/CD pipelines for infrastructure and software build, deployment, and configuration. \uf0b7Possess strong RESTful API design, including API versioning, backwards compatibility, and authentication, with a preference towards proficiencies in AWS services. \uf0b7Maintain code and scripting version control repositories and related systems supporting initiatives around “infrastructure as code”. \uf0b7Build and document automated data pipelines from a wide range of data sources with an emphasis on automation and scale \uf0b7Reduce complexity wherever possible – automated the handling of it everywhere else. \uf0b7Ensure security is integrated into all architecture solutions to meet compliance standards.\uf0b7 Contribute to overall architecture, framework, and design patterns to store and process high data volumes \uf0b7Develop solutions to measure, improve, and monitor data quality based on business requirements \uf0b7Design and implement reporting and analytics features in collaboration with product owners, reporting analysts / data analysts, and business partners within an Agile / Scrum methodology\uf0b7 Proactively support product health by building solutions that are automated, scalable, and sustainable – be relentlessly focused on minimizing defects and technical debt \uf0b7Provide post-implementation production support for data pipelines', '3+ years of experience in a data engineering role \uf0b7', 'Master’s Degree in Computer Science, Business Administration or related field preferred', ""Bachelor's degree in Computer Science, Informatics, Business Administration or related field required"", '\uf0b7 Experience with participating in time sensitive projects in a highly collaborative, multi-disciplinary team environment', '\uf0b7 Basic proficiency with a dialect of ANSI SQL and Python', '\uf0b7 Contribute to overall architecture, framework, and design patterns to store and process high data volumes \uf0b7', 'Qualifications', 'Develop solutions to measure, improve, and monitor data quality based on business requirements \uf0b7', 'Job Functions / Responsibilities', 'Design and implement reporting and analytics features in collaboration with product owners, reporting analysts / data analysts, and business partners within an Agile / Scrum methodology', 'Masters’ degree in Computer Science preferred \uf0b7', 'Possess strong RESTful API design, including API versioning, backwards compatibility, and authentication, with a preference towards proficiencies in AWS services. \uf0b7', ""Bachelors' degree in Computer Science, Informatics, or a related field required \uf0b7Masters’ degree in Computer Science preferred \uf0b73+ years of experience in a data engineering role \uf0b72+ years of experience with AWS and related services (e.g., EC2, S3, SNS, Lambda, IAM, Snowflake) \uf0b7Hands-on experience with ETL tools and techniques (Desirable)\uf0b7 Basic proficiency with a dialect of ANSI SQL and Python\uf0b7 Knowledge of and experience with RDBMS platforms, such as MS SQL Server, Oracle, DB2, MySQL, Postgres, SAP HANA, and Teradata\uf0b7 Experience with participating in time sensitive projects in a highly collaborative, multi-disciplinary team environment"", '\uf0b7 Knowledge of and experience with RDBMS platforms, such as MS SQL Server, Oracle, DB2, MySQL, Postgres, SAP HANA, and Teradata', 'Reduce complexity wherever possible – automated the handling of it everywhere else. \uf0b7', ""Bachelors' degree in Computer Science, Informatics, or a related field required \uf0b7"", '2+ years of experience with AWS and related services (e.g., EC2, S3, SNS, Lambda, IAM, Snowflake) \uf0b7', 'Ensure security is integrated into all architecture solutions to meet compliance standards.', 'Hands-on experience with ETL tools and techniques (Desirable)', 'Provide post-implementation production support for data pipelines']",Mid-Senior level,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Data Engineer II,Availity,"Jacksonville, FL",2 days ago,Be among the first 25 applicants,"['', ' http://www.dhs.gov/e-verify ', 'Must have experience with Linux', 'Design, build, quality, maintenance, and production support of data processing pipelines and systems (Real-time and batch)', ' Availity is an equal opportunity employer and makes decisions in employment matters without regard to race, religious creed, color, age, sex, sexual orientation, gender identity, gender expression, genetic information, national origin, religion, marital status, medical condition, disability, military service, pregnancy, childbirth and related medical conditions, or any other classification protected by federal, state, and local laws and ordinances. ', ' NOTICE: Federal law requires all employers to verify the identity and employment eligibility of all persons hired to work in the United States. When required by state law or federal regulation, Availity uses I-9, Employment Eligibility Verification in conjunction with E-Verify to determine employment eligibility. Learn more about E-Verify at ', 'Design, build, quality, maintenance, and production support of data processing pipelines and systems (Real-time and batch)Design, build, quality, maintenance, and production support of RESTful APIsResponsible for ensuring data processing pipelines and systems are: secure, reliable, fault-tolerant, scalable, accurate and efficientPerform data analysis and provide extracts on large and complex data setsData WranglingDeliver automated functional tests on data processing pipelines and systemsFull participation in the assigned Agile team (e.g., standups, planning, peer reviews, etc.)Provide on-call support (24x7); may be rotationalCollaborate within and outside assigned Agile teamContinuously seek opportunities to improve skillsetsProactively identify and communicate roadblocks. Evaluate and suggest alternativesSupport multiple projects and accommodate frequent interruptions and changing priorities', 'Bachelors degree in Information Systems or Computer science (e.g. specialization: Machine learning/Artificial Intelligence /Visualization, databases, and Big Data)2+ years of work experience in a Data Engineer, ETL Developer or similar role', 'Perform data analysis and provide extracts on large and complex data sets', 'The above cited duties and responsibilities describe the general nature and level of work performed by people assigned to the job. They are not intended to be an exhaustive list of all the duties and responsibilities that an incumbent may be expected or asked to perform.', 'Collaborate within and outside assigned Agile team', 'Must have experience with at least one compiled language (Java)', 'Other (List)', 'Deliver automated functional tests on data processing pipelines and systems', ' Availity is a drug-free workplace. Candidates are required to pass a drug test before beginning employment. ', 'Familiarity with common data science toolkits, such as R and Python', 'Provide on-call support (24x7); may be rotational', 'Skills And Knowledge', 'Support multiple projects and accommodate frequent interruptions and changing priorities', 'Experience working with Agile and Lean Practices', 'Excellent communication skills', 'Must have experience in a big data environment (such as: Hadoop, Spark, Impala, Solr, HBase, Kudu)', '2+ years of work experience in a Data Engineer, ETL Developer or similar role', 'Physical Requirements: (X = Required for job)', 'Bachelors degree in Information Systems or Computer science (e.g. specialization: Machine learning/Artificial Intelligence /Visualization, databases, and Big Data)', 'Must have experience designing, building and operating in-production big data/stream processing and/or enterprise data warehouse', 'Customer Scope (additional Specifications For Job Posting)', 'Data Wrangling', 'Education And Experience', 'Must have experience with large scale, concurrent applications', 'Hazards: (X = Required for job)', ' Click the links below to view Federal Employment Notices. ', 'Strong problem-solving capabilities and exhibits strong Computer Science Fundamentals', 'Continuously seek opportunities to improve skillsets', 'Key Responsibilities', ' . ', 'Must have experience building backend RESTful web services', 'Must have experience in a big data environment (such as: Hadoop, Spark, Impala, Solr, HBase, Kudu)Must have experience designing, building and operating in-production big data/stream processing and/or enterprise data warehouseMust have experience with large scale, concurrent applicationsMust have experience with at least one compiled language (Java)Must have experience building backend RESTful web servicesMust have experience with SQL and relational database systems (e.g., Oracle, SQL Server)Must have experience with LinuxFamiliarity with DevOps and Microservices architecture concepts and toolsFamiliarity with common data science toolkits, such as R and PythonStrong problem-solving capabilities and exhibits strong Computer Science FundamentalsExperience working with Agile and Lean PracticesExcellent communication skills', 'Familiarity with DevOps and Microservices architecture concepts and tools', 'Design, build, quality, maintenance, and production support of RESTful APIs', 'Must have experience with SQL and relational database systems (e.g., Oracle, SQL Server)', 'Full participation in the assigned Agile team (e.g., standups, planning, peer reviews, etc.)', 'Responsible for ensuring data processing pipelines and systems are: secure, reliable, fault-tolerant, scalable, accurate and efficient', 'Proactively identify and communicate roadblocks. Evaluate and suggest alternatives']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
REMOTE - Data Engineer - SQL + Snowflake + Informatica,KORE1,United States,3 days ago,93 applicants,"['', 'This is a full-time direct hire role that will be 100% remote (even after covid). Client is looking for a mid-level resource with 2-8 years of experience. This client will NOT sponsor or transfer work visas.', '·\xa0\xa0\xa0\xa0\xa0\xa03+ years - Experience using Power BI, Tableau, or similar data visualization tool', 'Motivated individual with strong analytic, problem solving, and troubleshooting skills', '·\xa0\xa0\xa0\xa0\xa0\xa0Expert SQL Fluency (Well versed in CTEs and window functions)', '·\xa0\xa0\xa0\xa0\xa0\xa03+ years - Experience with ETL/ELT Tools (ex. API, Informatica)', '·\xa0\xa0\xa0\xa0\xa0\xa0Demonstrated ability in data modeling, ETL/ELT, data pipelines, EDW', '·\xa0\xa0\xa0\xa0\xa0\xa03+ years - Experience with MS SQL Server and Snowflake', '·\xa0\xa0\xa0\xa0\xa0\xa0Experienced building data warehouse infrastructure and BI tables']",Associate,Full-time,Engineering,Staffing and Recruiting,2021-03-18 14:34:51
"Senior Data Engineer, Data Platform Engineering",Glassdoor,"Atlanta, GA",6 days ago,32 applicants,"['Build, automate, evangelize core data services (not limited to): real-time data ingestions, data access APIs, data discoverability, data quality automation etc. Empower stakeholders with self-service software to interact with our data assets. Design and develop tools and frameworks that makes our big data ecosystemmore effective. Identify inefficiencies in existing Apache Airflow workflows and introduce improvements. Measure, improve efficiency and resource utilization across resources used to deliver big data solutions. Create solutions on AWS using services such as Kinesis, Lambda, API Gateway, etc. Document services, share your knowledge freely and proactively with others. Participate in rotational on-call support. Provide ongoing maintenance and enhancements to existing systems. Learn our business domain and technology infrastructure quickly. Actively participate in open source endeavours. ', 'Provide ongoing maintenance and enhancements to existing systems. ', 'Identify inefficiencies in existing Apache Airflow workflows and introduce improvements. ', 'Ability to work full stack while sticking to rigorous software development disciplines and standards ', 'Participate in rotational on-call support. ', 'Ability to program in multiple programming/scripting languages: Python, Java, JS, Scala, etc ', '2+ years of experience using Python as a programming language. ', 'Background in Scrum/Agile development methodologies. ', 'Capable of delivering on multiple competing priorities with little supervision. ', 'Ability to work full stack while sticking to rigorous software development disciplines and standards Experience building customer facing products or data products. Familiarity with AWS or GCS technologies. Ability to program in multiple programming/scripting languages: Python, Java, JS, Scala, etc Be passionate about or have contributed to open sourced engineering projects in the past.', '\xa0 ', 'Experience with scripting languages: Perl, Shell, etc. ', 'Nice To Have', 'Excellent verbal and written communication skills. ', 'Please note this role is open to remote hiring. Our main office locations are in San Francisco, CA, Chicago, IL and Uniontown, OH. ', 'Exposure to test driven development and automated testing frameworks. ', ""Bachelor's Degree in computer science or equivalent experience. "", 'Experience building customer facing products or data products. ', 'Nice To Have ', 'Responsibilities', '2-3 years of hands-on experience in modeling and designing schema for data lakes or for RDBMS platforms. ', 'Measure, improve efficiency and resource utilization across resources used to deliver big data solutions. ', 'Create solutions on AWS using services such as Kinesis, Lambda, API Gateway, etc. ', 'Learn our business domain and technology infrastructure quickly. ', 'Document services, share your knowledge freely and proactively with others. ', 'We embrace a wide variety of technologies and work very closely with data scientists and business stakeholders to deliver end to end solutions. Although this is an individual contributor role, we are a tightly knit team who widely collaborate with one another. If you are interested in a fast paced environment, the latest technologies, and keen on driving productivity improvements, come join us! ', 'Actively participate in open source endeavours. ', 'Be passionate about or have contributed to open sourced engineering projects in the past.', 'Build, automate, evangelize core data services (not limited to): real-time data ingestions, data access APIs, data discoverability, data quality automation etc. ', '2+ years of hands-on experience developing a distributed data processing platform with Hadoop, Hive, Spark, Airflow, Kafka, etc. ', ""5+ years of hands-on experience with developing data lake platforms and data products. 2+ years of hands-on experience developing a distributed data processing platform with Hadoop, Hive, Spark, Airflow, Kafka, etc. 2+ years experience building solutions that deliver data near-real time 2-3 years of hands-on experience in modeling and designing schema for data lakes or for RDBMS platforms. 2+ years of experience using Python as a programming language. Experience with scripting languages: Perl, Shell, etc. Practice working with, processing, and managing large data sets (multi TB/PB scale). Exposure to test driven development and automated testing frameworks. Background in Scrum/Agile development methodologies. Capable of delivering on multiple competing priorities with little supervision. Excellent verbal and written communication skills. Bachelor's Degree in computer science or equivalent experience. "", '2+ years experience building solutions that deliver data near-real time ', '5+ years of hands-on experience with developing data lake platforms and data products. ', 'Responsibilities ', 'Our mission is to help people everywhere find a job and company they love. We’re transforming an entire industry through the power of transparency. As the worldwide leader in employer branding and insights, our vision is for a world where transparency holds companies accountable to strive to become better employers. ', 'Key Qualifications ', 'Practice working with, processing, and managing large data sets (multi TB/PB scale). ', 'Familiarity with AWS or GCS technologies. ', 'Empower stakeholders with self-service software to interact with our data assets. ', 'Key Qualifications', 'We are looking for a talented Senior Engineer to join our growing Data Engineering team. The ideal candidate has significant experience in building scalable data platforms that enable self service data products for data engineering, analytics and data science. This position is within the Data Platform Engineering team which is responsible for delivering solutions to other engineering functions as well as business users. You must have strong software engineering fundamentals, have hands-on expertise working with data at scale and the proven ability to fashion robust, scalable solutions. You should have a passion for continuous quality improvement. ', 'Design and develop tools and frameworks that makes our big data ecosystemmore effective. ']",Mid-Senior level,Full-time,Consulting,Internet,2021-03-18 14:34:51
Data Engineer,Finecast,"New York, NY",2 weeks ago,Be among the first 25 applicants,"['', 'Deep knowledge and understanding of PaaS and IaaS features in Cloud.', 'Design solutions for both small data (Cloud SQL) and Big Data (BigQuery, BigTable) business cases', 'Plan and implement integrations with large data sets and existing software solutions', 'Adequate knowledge on Google Anthos.', 'Responsible for design, development, implementation, operation improvement and debug cloud environments in GCP and Cloud Management Platform', 'Hands on experience in scripting languages like Perl, Shell etc.', 'Experience in programming languages like Java & Python, R, and SQL. ', 'GroupM and all its affiliates embrace and celebrate diversity, inclusivity, and equal opportunity.\u202fWe are committed to building a team that represents a variety of backgrounds, perspectives, and skills. We are a worldwide media agency network that represents global clients. The more inclusive we are, the more great work we can create together.', '4+ Proven experience using GCP services in architecting PaaS solutions.', 'Understand and evaluate existing business processes, workflows, and supporting systems to inform how to implement requirements', 'Oversee, or consult on, technology implementation and modification activities (for example, projects), particularly for new or shared infrastructure solutions.', 'Your Impact', 'Experience in application migration to GCP cloud', 'When applicable, apply AI/ML tools to handle complex problems', 'Technical Skills:', 'Hands on Experience on Cloud Deployment Manager and Cloud Build services in GCP.', 'At least 4+ years of experience hands on experience in cloud native architecture design, implementation of distributed, fault tolerant enterprise applications for Cloud.Experience in application migration to GCP cloud4+ Proven experience using GCP services in architecting PaaS solutions.Should be a certified GCP Professional Cloud Architect.Technical Skills:Deep understanding of Cloud Native and fundamentals of GCP.Deep knowledge and understanding of PaaS and IaaS features in Cloud.Hands on experience in GCP services i.e. GCE, GKE, GAE, GCS, Cloud SQL, VPC, Resource Manager, Stack Driver, Cloud CDN, Cloud IAM, and Cloud PUB/SUB.Adequate knowledge on Google Anthos.Hands on Experience on Cloud Deployment Manager and Cloud Build services in GCP.Good exposure to Big Table, Big Query, Data Proc/Dataflow/ML API’s etc.Experience automation and provisioning of cloud environments using API’s, CLI and scripts.Experience in deploy, manage, and scale applications.Experience in programming languages like Java & Python, R, and SQL. Hands on experience in scripting languages like Perl, Shell etc.', 'Understand and evaluate existing business processes, workflows, and supporting systems to inform how to implement requirementsDesign solutions for both small data (Cloud SQL) and Big Data (BigQuery, BigTable) business casesWhen applicable, apply AI/ML tools to handle complex problemsPlan and implement integrations with large data sets and existing software solutionsResponsible for design, development, implementation, operation improvement and debug cloud environments in GCP and Cloud Management PlatformLead the analysis of the current GCP practices and recommend solutions for improvement.Oversee, or consult on, technology implementation and modification activities (for example, projects), particularly for new or shared infrastructure solutions.Define high-level migration plans to address the gaps between the current and future state, typically in sync with the IT budgeting or other capital planning processes.Understand technology trends and the practical application of existing, new, and emerging technologies to enable new and evolving business and operating models.Document technology architecture design and analysis work', 'Experience automation and provisioning of cloud environments using API’s, CLI and scripts.', 'Define high-level migration plans to address the gaps between the current and future state, typically in sync with the IT budgeting or other capital planning processes.', 'Your Qualifications', 'Understand technology trends and the practical application of existing, new, and emerging technologies to enable new and evolving business and operating models.', 'Document technology architecture design and analysis work', 'The Role', 'Deep understanding of Cloud Native and fundamentals of GCP.', 'Good exposure to Big Table, Big Query, Data Proc/Dataflow/ML API’s etc.', 'About Groupm', 'Experience in deploy, manage, and scale applications.', 'Hands on experience in GCP services i.e. GCE, GKE, GAE, GCS, Cloud SQL, VPC, Resource Manager, Stack Driver, Cloud CDN, Cloud IAM, and Cloud PUB/SUB.', 'Should be a certified GCP Professional Cloud Architect.', 'Lead the analysis of the current GCP practices and recommend solutions for improvement.', 'At least 4+ years of experience hands on experience in cloud native architecture design, implementation of distributed, fault tolerant enterprise applications for Cloud.']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Data Engineer,Team Rubicon,"Los Angeles, CA",6 days ago,Be among the first 25 applicants,"['', ' Everyone Has A Role Know It ', ' Adults Only ', ' Generous vacation and sick time ', 'Job Type', ' Holiday parties', ' Twitter: @TeamRubicon', ' GSD  : We are entrepreneurial, resourceful and determined no matter how chaotic the situation. ', ' Mission First,  Greyshirt  Always:  Anyone joining TR must understand that our mission to provide disaster response comes first ', 'T ', ' Your Mothers A Donor  : Every leader must be committed to fiduciary responsibility, transparency and financial stewardship', ' Generous holiday schedule (including a paid week off between winter holidays) ', ' Greyshirt ', ' Everyone Has A Role Know It  : Ability to successfully navigate a fast-paced, high-growth environment and solve problems in the face of ambiguity ', 'By submitting your interest in this job, you agree to receive text notifications with additional steps to complete your job application. You will receive up to 6 messages from the number ""63879"". Message & data rates may apply. Please refer to our privacy policy for more information.', ' Annual 5 day National Leadership Conference ', ' I mplement and manage analytics tools that utilize the data pipeline to provide actionable insights ', ' Always: ', ' Mission First,  Greyshirt  Always:  Anyone joining TR must understand that our mission to provide disaster response comes first  Step into the Arena  : TR needs leaders who arent afraid to dare to be great  Everyone Has A Role Know It  : Ability to successfully navigate a fast-paced, high-growth environment and solve problems in the face of ambiguity  GSD  : We are entrepreneurial, resourceful and determined no matter how chaotic the situation.  Change Your Socks  : We take care of ourselves and each other so we are best equipped to serve those in greatest need  Adults Only  : Every team member is an adult until proven otherwise  Your Mothers A Donor  : Every leader must be committed to fiduciary responsibility, transparency and financial stewardship', ' Professional development, leadership development and events/conferences ', ' GSD ', ' 100% company-paid health benefits for employees and their dependents ', ' Strong background working with Relational and Non-Relational Databases ', ' eam Rubicon (TR ', ' Demonstrated ability working with business users to define data and analytics requirements, analyze and profile data and define a solution', ' Professional development, leadership development and events/conferences  100% company-paid health benefits for employees and their dependents  Matching 401k contributions up to 4%  Annual 5 day National Leadership Conference  Paid time off to volunteer with the non-profit of your choice  Generous vacation and sick time  Generous holiday schedule (including a paid week off between winter holidays)  Holiday parties', ' Website: TeamRubiconUSA.org ', 'Learn More About Team Rubicon', ' W ork with stakeholders across the - Executive, Data, Tech and Ops teams, assisting with data-related projects, and supporting technical issues and data infrastructure needs, develop data tools, as needed, to support and enhance the use of data across the organization', ' Step into the Arena ', ' Full-time, salary, exempt', ' Hands-on experience designing and implementing analytical data environments, data modeling and ETL / ELT workflows, semantic layer architecture and BI schema ', ' Mission First, ', ' D ata architecture and data modeling for key subject areas and business domains to generate curated business-ready data for reporting and analytics  Data integration (ETL / ELT) design and implementation  B uild the infrastructure required for optimal extraction, transformation and loading of data from a wide variety of data sources using SQL and AWS big data technologies  I mplement and manage analytics tools that utilize the data pipeline to provide actionable insights  U se data to create an engaging, informative, compelling story, and provide analytic insights that inform and influence TRs strategic direction and key performance metrics  I dentify, design, and implement internal process improvements around automation, optimizing data delivery, and re-designing infrastructure for greater scalability  W ork with stakeholders across the - Executive, Data, Tech and Ops teams, assisting with data-related projects, and supporting technical issues and data infrastructure needs, develop data tools, as needed, to support and enhance the use of data across the organization', ' Bachelors degree in Computer Science, Statistics, Informatics, Information - Systems or any other relevant field ', ' 2 - 3 years of experience in a Data Engineer o r Full-Stack Developer roles with proficiency in SQL, Python and other ETL languages ', ' 2 - 3 years of experience in a Data Engineer o r Full-Stack Developer roles with proficiency in SQL, Python and other ETL languages  Bachelors degree in Computer Science, Statistics, Informatics, Information - Systems or any other relevant field  Hands-on experience designing and implementing analytical data environments, data modeling and ETL / ELT workflows, semantic layer architecture and BI schema  Strong background working with Relational and Non-Relational Databases  Solid understanding of data modeling and data warehousing techniques  Demonstrated ability working with business users to define data and analytics requirements, analyze and profile data and define a solution', ' Website: TeamRubiconUSA.org  Facebook: Facebook.com/TeamRubicon  Twitter: @TeamRubicon', ' B uild the infrastructure required for optimal extraction, transformation and loading of data from a wide variety of data sources using SQL and AWS big data technologies ', ' Change Your Socks ', ' Your Mothers A Donor ', 'Perks Of The Team', 'Experience And Background', ' Change Your Socks  : We take care of ourselves and each other so we are best equipped to serve those in greatest need ', 'Duties', ' Step into the Arena  : TR needs leaders who arent afraid to dare to be great ', ' Paid time off to volunteer with the non-profit of your choice ', ' U se data to create an engaging, informative, compelling story, and provide analytic insights that inform and influence TRs strategic direction and key performance metrics ', ' Facebook: Facebook.com/TeamRubicon ', ' D ata architecture and data modeling for key subject areas and business domains to generate curated business-ready data for reporting and analytics ', ' ) ', ' Matching 401k contributions up to 4% ', ' Solid understanding of data modeling and data warehousing techniques ', ' Data integration (ETL / ELT) design and implementation ', ' Adults Only  : Every team member is an adult until proven otherwise ', ' I dentify, design, and implement internal process improvements around automation, optimizing data delivery, and re-designing infrastructure for greater scalability ', 'Cultural Principles']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Systems Data Engineer,CarolinaEast Health System,"New Bern, NC",2 weeks ago,Be among the first 25 applicants,"['', 'About CarolinaEast Medical Center', 'Job Summary:', 'Minimum Requirements:']",Entry level,Full-time,Information Technology,"Health, Wellness and Fitness",2021-03-18 14:34:51
Senior Data Engineer,INSIGHT2PROFIT,"Beachwood, OH",4 days ago,Be among the first 25 applicants,"['', 'Modeling extracts for Continuous Improvement Models', 'Results-driven and thrives in a fast-paced environment.', 'Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.', 'Experience building and optimizing ‘big data’ data pipelines, architectures and data sets', 'Automate the full process to ensure it is repeatable, contains validation and proper notification/monitoring. ', 'A successful history of manipulating, processing and extracting value from large, disconnected datasets.', 'Preferred', 'Cloud hosted client facing dashboards.', 'Excellent verbal and written communication skills with the ability to simplify technical concepts and present to senior-level executives.', 'Build processes supporting data transformation, data structures, metadata, dependency, and workload management.', ' Experience with and understanding of agile methodologies such as Scrum and/or Kanban. Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores. Experience with cloud-based technologies and working in a cloud environment. Experience with Microsoft Azure is preferred. Experience building and optimizing ‘big data’ data pipelines, architectures and data sets Experience with big data tools such as Hadoop, Spark, Kafka, etc. Experience with data pipeline and workflow management tools such as Azure Data Factory and Azure Databricks. Experience with Azure messaging services such as Event Grid, Event Hub, and Service Bus. Familiarity with NoSQL (document and key-value) database technologies. Familiarity with scripting languages such as R, Python, JavaScript, Scala, etc. Understanding of JSON format and experience working with JSON documents.', 'Assemble large, complex data sets that meet functional / non-functional business requirements.', 'Experience with big data tools such as Hadoop, Spark, Kafka, etc.', 'By submitting your interest in this job, you agree to receive text notifications with additional steps to complete your job application. You will receive up to 6 messages from the number ""63879"". Message & data rates may apply. Please refer to our privacy policy for more information.', 'Contribute to and maintain data pipeline architecture.', 'Design and automate data pipelines into: Cloud hosted client facing dashboards. Modeling extracts for Continuous Improvement Models Services and Custom tools for online models and client facing interfaces. ', 'Services and Custom tools for online models and client facing interfaces.', 'Design a data mart solution that will meet the data needs for discovery and implementation, while planning for longer term supporting needs of measurement and continuous improvements.', 'Support ongoing client data requests and additional automations.', 'Experience with cloud-based technologies and working in a cloud environment. Experience with Microsoft Azure is preferred.', 'Ability to impact company culture and meaningfully shape the vision of our company.', 'Responsibilities', 'Ability to communicate effectively with both technical and non-technical audiences.', 'Make the data available to the delivery team including Data Science Group and Profit Realization team', 'Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.', 'Building expertise in consuming 3rd party data for client specific analysis.', 'Understanding of JSON format and experience working with JSON documents.', 'Required', ' Bachelor’s degree in Computer Science or related field, or equivalent experience working as a Database developer, ETL Developer, BI Developer or similar role. Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) 3-5 year of SQLServer T-SQL experience Experience with SSIS or other ETL Tools  Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. Build processes supporting data transformation, data structures, metadata, dependency, and workload management. A successful history of manipulating, processing and extracting value from large, disconnected datasets. Ability to communicate effectively with both technical and non-technical audiences. Ability to work autonomously as well as part of a team. Exceptional work ethic, judgment, business acumen, attention to detail, analytical skills, and problem-solving skills with the ability to make informed decisions. Excellent verbal and written communication skills with the ability to simplify technical concepts and present to senior-level executives. Results-driven and thrives in a fast-paced environment. Ability to impact company culture and meaningfully shape the vision of our company. Ability to train and delegate to summer analysts and junior team members.', 'Assist in implementation of internal process improvements to make solutions more scalable and reliable.', 'Experience with and understanding of agile methodologies such as Scrum and/or Kanban.', ' INSIGHT2PROFIT ', 'As we invest in our data ingress process and technology: Contribute to and maintain data pipeline architecture. Assemble large, complex data sets that meet functional / non-functional business requirements. Assist in implementation of internal process improvements to make solutions more scalable and reliable. Building expertise in consuming 3rd party data for client specific analysis. ', 'Qualifications', 'Load, analyze, and profile data received to understand what needs to be addressed via transformations and what needs additional involvement from the client.', 'Familiarity with NoSQL (document and key-value) database technologies.', 'Bachelor’s degree in Computer Science or related field, or equivalent experience working as a Database developer, ETL Developer, BI Developer or similar role.', ' Lead client data calls to communicate project data requirements. Load, analyze, and profile data received to understand what needs to be addressed via transformations and what needs additional involvement from the client. Design a data mart solution that will meet the data needs for discovery and implementation, while planning for longer term supporting needs of measurement and continuous improvements. Load the data into our proprietary Measurement tool (ProfitBuilder) Make the data available to the delivery team including Data Science Group and Profit Realization team Design inbound data merges for data refreshes Automate the full process to ensure it is repeatable, contains validation and proper notification/monitoring.  Design and automate data pipelines into: Cloud hosted client facing dashboards. Modeling extracts for Continuous Improvement Models Services and Custom tools for online models and client facing interfaces.  Support ongoing client data requests and additional automations. Troubleshoot production issues that maybe related to unexpected client data, bugs, or technical issues. ', 'Troubleshoot production issues that maybe related to unexpected client data, bugs, or technical issues.', ' Contribute to and maintain data pipeline architecture. Assemble large, complex data sets that meet functional / non-functional business requirements. Assist in implementation of internal process improvements to make solutions more scalable and reliable. Building expertise in consuming 3rd party data for client specific analysis. ', 'Experience with data pipeline and workflow management tools such as Azure Data Factory and Azure Databricks.', 'Experience with Azure messaging services such as Event Grid, Event Hub, and Service Bus.', 'Ability to train and delegate to summer analysts and junior team members.', 'Experience with SSIS or other ETL Tools ', ' Cloud hosted client facing dashboards. Modeling extracts for Continuous Improvement Models Services and Custom tools for online models and client facing interfaces. ', 'Design inbound data merges for data refreshes', 'Load the data into our proprietary Measurement tool (ProfitBuilder)', 'Overview', '3-5 year of SQLServer T-SQL experience', 'Ability to work autonomously as well as part of a team.', ' Participate in all data related aspects of the consulting engagement life cycle, including discovery, implementation, measurement, and continuous improvements. Lead client data calls to communicate project data requirements. Load, analyze, and profile data received to understand what needs to be addressed via transformations and what needs additional involvement from the client. Design a data mart solution that will meet the data needs for discovery and implementation, while planning for longer term supporting needs of measurement and continuous improvements. Load the data into our proprietary Measurement tool (ProfitBuilder) Make the data available to the delivery team including Data Science Group and Profit Realization team Design inbound data merges for data refreshes Automate the full process to ensure it is repeatable, contains validation and proper notification/monitoring.  Design and automate data pipelines into: Cloud hosted client facing dashboards. Modeling extracts for Continuous Improvement Models Services and Custom tools for online models and client facing interfaces.  Support ongoing client data requests and additional automations. Troubleshoot production issues that maybe related to unexpected client data, bugs, or technical issues.  As we invest in our data ingress process and technology: Contribute to and maintain data pipeline architecture. Assemble large, complex data sets that meet functional / non-functional business requirements. Assist in implementation of internal process improvements to make solutions more scalable and reliable. Building expertise in consuming 3rd party data for client specific analysis. ', 'Exceptional work ethic, judgment, business acumen, attention to detail, analytical skills, and problem-solving skills with the ability to make informed decisions.', 'Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL)', 'Participate in all data related aspects of the consulting engagement life cycle, including discovery, implementation, measurement, and continuous improvements. Lead client data calls to communicate project data requirements. Load, analyze, and profile data received to understand what needs to be addressed via transformations and what needs additional involvement from the client. Design a data mart solution that will meet the data needs for discovery and implementation, while planning for longer term supporting needs of measurement and continuous improvements. Load the data into our proprietary Measurement tool (ProfitBuilder) Make the data available to the delivery team including Data Science Group and Profit Realization team Design inbound data merges for data refreshes Automate the full process to ensure it is repeatable, contains validation and proper notification/monitoring.  Design and automate data pipelines into: Cloud hosted client facing dashboards. Modeling extracts for Continuous Improvement Models Services and Custom tools for online models and client facing interfaces.  Support ongoing client data requests and additional automations. Troubleshoot production issues that maybe related to unexpected client data, bugs, or technical issues. ', 'Lead client data calls to communicate project data requirements.', 'Familiarity with scripting languages such as R, Python, JavaScript, Scala, etc.']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,SonderMind,"Denver, CO",4 weeks ago,60 applicants,"['', 'A generous PTO policy and a company mandate of at MINIMUM three-weeks of PTO a year; flexible work-from-home policy', 'Experience with fivetran ', 'Exceptional SQL skills.', ' This is a full-time, exempt position  Starting salary range for this role is $72,000. Actual compensation and title is commensurate with experience and skills. This role will be eligible to participate in the following:', ' Healthcare experience Previous Startup Experience Experience with fivetran  ', 'Experience managing the entire data pipeline from source to end user', 'in addition', 'Growth mindset', 'This is a full-time, exempt position ', 'Other Details', 'Nice to Have', 'Competitive market salary, up-to 4% salary company match on 401K, professional development and advancement opportunities as we rapidly scale, and so much more.', 'Exceptional Excel and data analysis skills, including a strong grasp of statistics. Experience in a business intelligence tool (Looker, Tableau, etc.)', 'variable performance-based bonus incentives', 'Experience using command line and git', 'A commitment to reward our team members for the long-term success and growth of SonderMind with stock option grants and bonus potential', 'As our Ideal Candidate', ' full employee benefits package employee stock option purchase plan variable performance-based bonus incentives ', 'Starting salary range for this role is $72,000. Actual compensation and title is commensurate with experience and skills.', 'full employee benefits package', 'Previous Startup Experience', 'A great downtown location and stocked with unlimited drinks and snacks (NOTE: company is fully remote during COVID pandemic)', ""Employer-paid disability & AD&D to cover life's unexpected - not only that, we cover the difference in salary for up to Eight weeks of short-term disability leave"", 'Eight weeks of paid parental leave (if the parent also qualifies for STD, this benefit is in addition)', '3+ years experience as a data analyst', "" Therapy coverage benefits to enable our employees to get the care they need Employer-paid disability & AD&D to cover life's unexpected - not only that, we cover the difference in salary for up to Eight weeks of short-term disability leave Eight weeks of paid parental leave (if the parent also qualifies for STD, this benefit is in addition) A generous PTO policy and a company mandate of at MINIMUM three-weeks of PTO a year; flexible work-from-home policy A great downtown location and stocked with unlimited drinks and snacks (NOTE: company is fully remote during COVID pandemic) A commitment to reward our team members for the long-term success and growth of SonderMind with stock option grants and bonus potential Competitive market salary, up-to 4% salary company match on 401K, professional development and advancement opportunities as we rapidly scale, and so much more."", 'Ability to communicate clearly and concisely about complex technical projects', 'employee stock option purchase plan', 'This role will be eligible to participate in the following:', '1+ years of experience working with dbt', 'Our employees may be exposed to sensitive personal information throughout their regular duties. For this reason, we maintain exceptionally high expectations of ethical conduct and require all incoming employees to pass a background check.', 'Our Employee Benefits Philosophy', 'Healthcare experience', 'Commitment to an Inclusive Workplace', 'Gotta Have', ' 3+ years experience as a data analyst Ability to communicate clearly and concisely about complex technical projects 1+ years of experience working with dbt Experience using command line and git Experience managing the entire data pipeline from source to end user Growth mindset Exceptional Excel and data analysis skills, including a strong grasp of statistics. Experience in a business intelligence tool (Looker, Tableau, etc.) Exceptional SQL skills. Comfortable with ambiguity ', 'Therapy coverage benefits to enable our employees to get the care they need', 'Comfortable with ambiguity']",Entry level,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Engineer,Underdog.io,New York City Metropolitan Area,1 week ago,26 applicants,"['', '---', 'To get started, click through to the link and drop your email.', 'Building an inclusive and diverse workplace is one of Underdog.io’s core values. We warmly welcome people of all backgrounds, experiences, and perspectives.', ""We're looking for a data engineer to join a company in the Underdog.io network."", ""If accepted, you'll hear directly from founders, hiring managers, and other key decision makers starting the following Monday. Our platform will hide your profile from your current employer. "", ""We accept companies that offer competitive salaries, benefits, and perks. They're working on interesting technical challenges and must be respectful of your time to stay active."", ""To apply to the network, we'll ask you to fill out a 60-second web form. It's absolutely free."", 'The Underdog.io network is a curated group of some of the fastest growing companies in New York and San Francisco. We actively turn away more than 50% of companies that attempt to join.', 'Our companies look for data engineers proficient in Python, Java, or C++. The ideal candidate is passionate about building clean pipelines and maintaining data products relied on by many.']",Mid-Senior level,Full-time,Engineering,Internet,2021-03-18 14:34:51
"Jr. Data Engineer (Cloud, Python, Datawarehouse)","Precise Software Solutions, Inc.","Rockville, MD",3 weeks ago,60 applicants,"['', 'Experience in developing Python programsExperience with AWS cloud service environment', 'Strong analytical and problem-solving skills', 'Awards and Recognition', 'Precise Software Solutions, Inc', 'Junior Cloud Data engineer', 'Experience in analyzing and processing structured or unstructured data sets.', 'Annual Charity Donation Match ', 'Our Equal Employment Opportunity Policy', 'Forward thinking on new technologies and patterns', 'About Us', 'Currently Remote, this position will be located in Rockville, MD once the office reopens', 'Health Benefits (Medical, Dental and Vision) ', 'Paid Time Off', 'Understand logical design of Databases, Data Warehouses and Multidimensional Databases including data flows, conception model and logical models', 'Experience with AWS cloud service environment', 'Data Analysis with a degree of creativity and lateral thinking is expected', 'Health Benefits (Medical, Dental and Vision) Flexible Spending Accounts (FSA) & Health Savings Account (HSA) Retirement Plan Paid Time OffParental Leave Life Insurance Training and Development Two Innovation Days Employee Referral Program Annual Charity Donation Match Awards and RecognitionStanding Desks ', 'Two Innovation Days ', 'Bachelors or MS in related field ', 'Experience on premise to cloud data migration in enterprise systems', 'Employee Referral Program ', 'Strong understanding of data governance', 'Participate into other cross-function design, development activities.', 'Parental Leave ', 'Job Description:', 'Experience with enterprise BI tools such as QuickSight, Tableau is preferred.', 'Attention to detail, creative problem-solving abilities, and coaching and influencing skills are a must', 'Communicate and accurately document all architectural decisions, plans, goals, and functional requirements', 'Training and Development ', 'BENEFITS AND PERKS: ', 'Maintain metadata with data definitions, relationships, sources, and lineage', 'Ensures the realization of the solution architecture, making sure it is operable & cost-effective', 'Implement application migration from on-premises to Cloud in support of the Data Warehouse objectives for FDA CFSAN program', 'Implement application migration from on-premises to Cloud in support of the Data Warehouse objectives for FDA CFSAN programParticipate into other cross-function design, development activities.Ensure consistency, quality, and accuracy of the data mapped from source to the target data warehouse environmentsWork closely with the PM, Architect, customers, and end users to translate business requirements into technical designs to map and load data from source systems to the data warehouseCommunicate and accurately document all architectural decisions, plans, goals, and functional requirementsMaintain metadata with data definitions, relationships, sources, and lineageWorks with business users to translate business requirements into technical designs to map and load data from source systems to the data warehouseDesigns solutions that leverage enterprise tools and have reusable components and servicesEnsures data designs meet business requirementsDevelops strategies for data acquisitions, archive recovery, and implementation of databaseEnsures the realization of the solution architecture, making sure it is operable & cost-effectiveEnsures technical requirements are delivered with quality and completeness through peer reviews', 'Ensure consistency, quality, and accuracy of the data mapped from source to the target data warehouse environments', 'Work closely with the PM, Architect, customers, and end users to translate business requirements into technical designs to map and load data from source systems to the data warehouse', 'Develops strategies for data acquisitions, archive recovery, and implementation of database', 'Experience in maintaining SQL scripts and complex queries for analysis and extraction.', 'Retirement Plan ', 'Designs solutions that leverage enterprise tools and have reusable components and services', 'Work Location:', 'Experience in design and implementation of stored procedures, views and other application database code objects to aid complex mappings.', 'Flexible Spending Accounts (FSA) & Health Savings Account (HSA) ', 'Life Insurance ', 'Job Duties: ', 'Standing Desks ', 'Participated in providing the project estimates for development team efforts', 'Essential Skills: ', 'Preferred Skills:', 'Understand of Infrastructure as a server platforms', 'Data Security knowledge', 'Ensures data designs meet business requirements', 'Education: ', 'Public Trust background check', 'Experience in developing Python programsExperience with AWS cloud service environmentExperience in design and implementation of stored procedures, views and other application database code objects to aid complex mappings.Experience in maintaining SQL scripts and complex queries for analysis and extraction.Understand of Infrastructure as a server platformsUnderstand logical design of Databases, Data Warehouses and Multidimensional Databases including data flows, conception model and logical modelsExperience on premise to cloud data migration in enterprise systemsExperience in analyzing and processing structured or unstructured data sets.Experience with enterprise BI tools such as QuickSight, Tableau is preferred.Strong analytical and problem-solving skillsStrong understanding of data governanceData Security knowledgeData Analysis with a degree of creativity and lateral thinking is expectedAttention to detail, creative problem-solving abilities, and coaching and influencing skills are a mustAbility to assesses risks, anticipates bottlenecks, provide escalation management, and collaborates to drive toward mutually beneficial outcomes', 'Ability to assesses risks, anticipates bottlenecks, provide escalation management, and collaborates to drive toward mutually beneficial outcomes', 'AWS certification', 'Works with business users to translate business requirements into technical designs to map and load data from source systems to the data warehouse', 'Ensures technical requirements are delivered with quality and completeness through peer reviews', 'Forward thinking on new technologies and patternsParticipated in providing the project estimates for development team effortsAWS certification']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Software Engineer-Data,CaaStle,"Delhi, OH",1 day ago,40 applicants,"['', 'Good communication skills.', 'Must Have', 'Experience: 1-3 years of hands-on coding experience in data driven applications and machine learning algorithms.', 'Advocate for good, clean, well documented and performing code; follow standards and best practices.', 'Good understanding of Java/J2EE, Python, MySql, HDFS environments (Hive, Impala, Spark, Hbase etc), Messaging queues(Kafka, RabbitMq) and ElasticSearch.', ' Statistics and Machine Learning Algorithms knowledge. ', 'Contribute to continuous system/infrastructure improvements.', ' Education: B. Tech. in Computer Science from tier - 1 engineering colleges in India Must Have Strong grasp of algorithms, design patterns, memory management, and multithreaded programming. Good understanding of Java/J2EE, Python, MySql, HDFS environments (Hive, Impala, Spark, Hbase etc), Messaging queues(Kafka, RabbitMq) and ElasticSearch. A willingness to dive deep, experiment rapidly and get things done. Good communication skills.   Good To Have Statistics and Machine Learning Algorithms knowledge.   ', 'Must Have Strong grasp of algorithms, design patterns, memory management, and multithreaded programming. Good understanding of Java/J2EE, Python, MySql, HDFS environments (Hive, Impala, Spark, Hbase etc), Messaging queues(Kafka, RabbitMq) and ElasticSearch. A willingness to dive deep, experiment rapidly and get things done. Good communication skills.  ', 'Statistics and Machine Learning Algorithms knowledge.', 'Education: B. Tech. in Computer Science from tier - 1 engineering colleges in India', 'About CaaStle', 'What You’ll Do', 'About The Role', 'Experience: ', ' Strong grasp of algorithms, design patterns, memory management, and multithreaded programming. Good understanding of Java/J2EE, Python, MySql, HDFS environments (Hive, Impala, Spark, Hbase etc), Messaging queues(Kafka, RabbitMq) and ElasticSearch. A willingness to dive deep, experiment rapidly and get things done. Good communication skills. ', 'We’d Love For You To Have', ' Do the Right Thing by People  Connect the Dots Differently  Make Hard Choices for Long Term Growth  Winning is a Shared Experience  ', 'A willingness to dive deep, experiment rapidly and get things done.', 'Good To Have', 'Strong grasp of algorithms, design patterns, memory management, and multithreaded programming.', ' Experience: 1-3 years of hands-on coding experience in data driven applications and machine learning algorithms. ', 'Do the Right Thing by People ', 'Designing, development and maintenance of highly scalable and performant applications.', 'Software Engineer', ' Designing, development and maintenance of highly scalable and performant applications. Collaborate with Product Management and Data Science teams to get maximum value out of existing data. Contribute to continuous system/infrastructure improvements. Advocate for good, clean, well documented and performing code; follow standards and best practices. Troubleshooting and helping out the team in case of any issues in the applications. ', 'Connect the Dots Differently ', 'Winning is a Shared Experience ', 'Collaborate with Product Management and Data Science teams to get maximum value out of existing data.', 'Education: ', 'Make Hard Choices for Long Term Growth ', 'Troubleshooting and helping out the team in case of any issues in the applications.', 'CaaStle Is Looking For Talented People Who Are Excited To Work In a Dynamic, Fast Growing Environment And Who Believe In Our Values', 'Good To Have Statistics and Machine Learning Algorithms knowledge.  ']",Entry level,Full-time,Engineering,Marketing and Advertising,2021-03-18 14:34:51
Data Engineer,Grabango,"Berkeley, CA",3 weeks ago,47 applicants,"['', 'Integrity', 'Excellent SQL and Python skills', ' 4-7 years of experience as a data engineer Excellent SQL and Python skills Experience with both SQL and NoSQL databases, such as Cassandra and MongoDB Experience with data pipelining tools such as dbt, airflow, luigi or Dagster Comfort working within systems running Kubernetes, Docker, Linux, Git Strong communication skills to collaborate with cross-functional stakeholders You can rapidly become familiar with and apply new tools and technologies ', 'Inclusion: ', 'You can rapidly become familiar with and apply new tools and technologies', ' BS in Computer Science, Engineering, or related field ', 'Grabango Values:', 'Comfort working within systems running Kubernetes, Docker, Linux, Git', 'Simplify', ""Don't Live with Broken"", 'Bold Innovation', '4-7 years of experience as a data engineer', 'Grabango is proud to be an equal opportunity employer and is committed to developing a workplace where diversity and inclusion are an essential part of who we are. We strive to hire and support a workforce as diverse as our shopper base, so we can develop products and services that best suit our customers. We do not make employment decisions based on race, color, religion, ethnic or national origin, nationality, sex, gender, gender-identity, sexual orientation, disability, age, military or veteran status and we comply with all local, state and federal employment laws. ', 'BS in Computer Science, Engineering, or related field', 'Customer Focus', 'Implement best practices to ensure data quality and integrity in our pipelines', 'People Matter', 'Build out our data warehouse and own the data flow between production databases and our data warehouse', 'Overview:', ' Create and maintain ETL pipelines that can robustly ingest, transform, and store both external customer data and internally generated data Build out our data warehouse and own the data flow between production databases and our data warehouse Collaborate with computer vision and machine learning teams to create data pipelines for iteration of cutting edge computer vision models Implement best practices to ensure data quality and integrity in our pipelines ', 'Strong communication skills to collaborate with cross-functional stakeholders', 'Experience with both SQL and NoSQL databases, such as Cassandra and MongoDB', 'Create and maintain ETL pipelines that can robustly ingest, transform, and store both external customer data and internally generated data', ""Grabango participates in the E-Verify Program, an internet-based system operated by the Department of Homeland Security and the Social Security Administration. It allows employers to confirm an individual's employment eligibility to work in the United States."", 'Experience with data pipelining tools such as dbt, airflow, luigi or Dagster', 'Collaborate with computer vision and machine learning teams to create data pipelines for iteration of cutting edge computer vision models']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Senior Data Engineer - Work From Home,PLUM Commercial Real Estate Lending,"Austin, TX",2 days ago,Be among the first 25 applicants,"['', ' Job Summary ', ' Experience in working with graph database such as TigerGraph or Neo4j', ' Autonomy, flexibility and a flat corporate structure.', ' High proficiency in Java', ' Participate heavily in architecture, design and implementation of machine learning pipelines', ' Development of data-driven products.', ' Conscientious and well organized', ' Experience with finance technology/analytics and commercial real estate', ' Hands-on Industry experience with machine learning frameworks like SageMaker, TensorFlow', ' Experience with data visualization tools like Tableau', ' Bachelor’s degree in Mathematics, Statistics, Engineering, Computer Science or related discipline 2+ years of production-level machine learning model deployment Experience in working with big data technologies Spark, MapReduce, NoSQL databases Experience with AWS infrastructure such as EC2, S3 and Glue Hands-on Industry experience with machine learning frameworks like SageMaker, TensorFlow High proficiency in Java Knowledge of both neural networks and traditional ML techniques Experience with data visualization tools like Tableau An entrepreneurial spirit as well as passion for solving difficult challenges through innovation and creativity, with a strong focus on results Conscientious and well organized Eager to produce results and drive forward progress while managing deadlines', ' Eager to produce results and drive forward progress while managing deadlines', ' Experience in working with graph database such as TigerGraph or Neo4j Have worked in a fast paced startup environment Experience with finance technology/analytics and commercial real estate', ' Benefits ', ' Develop, refine and scale data management and analytics procedures, systems, workflows, best practices and other issues.', ' Have worked in a fast paced startup environment', ' Generous health, dental and vision coverage for employees and family members, along with commuter pre-tax program.', ' Chance for your direct input to be realized and put into action.', ' Collaborate with our engineers to produce excellent products for Plum clients as well as streamline internal processes.', ' Early equity in a startup that is revolutionizing commercial real estate lending.', ' Knowledge of both neural networks and traditional ML techniques', ' An entrepreneurial spirit as well as passion for solving difficult challenges through innovation and creativity, with a strong focus on results', ' About Plum ', 'Must be authorized to work in the US. Plum does not sponsor employee visas.', ' Freedom to stretch the boundaries of your past work experience, learn skills outside of your immediate job description and grow your career.', ' Unlimited vacation policy.', ' Experience in working with big data technologies Spark, MapReduce, NoSQL databases', ' Opportunity to make a meaningful impact on the disruption of an industry and to shape the building of a company and culture.', ' 2+ years of production-level machine learning model deployment', ' Experience with AWS infrastructure such as EC2, S3 and Glue', ' Bachelor’s degree in Mathematics, Statistics, Engineering, Computer Science or related discipline', ' Generous health, dental and vision coverage for employees and family members, along with commuter pre-tax program. Unlimited vacation policy. Opportunity to make a meaningful impact on the disruption of an industry and to shape the building of a company and culture. Chance for your direct input to be realized and put into action. Freedom to stretch the boundaries of your past work experience, learn skills outside of your immediate job description and grow your career. Autonomy, flexibility and a flat corporate structure.', ' Responsibilities ', ' Required Qualifications ', ' Preferred Qualifications ', ' Participate heavily in architecture, design and implementation of machine learning pipelines Develop, refine and scale data management and analytics procedures, systems, workflows, best practices and other issues. Development of data-driven products. Visualize and communicate data clearly for use both internally and externally. Collaborate with our engineers to produce excellent products for Plum clients as well as streamline internal processes.', ' Visualize and communicate data clearly for use both internally and externally.']",Associate,Full-time,Information Technology,Banking,2021-03-18 14:34:51
Data Engineer - Irving,Inform Diagnostics,"Irving, TX",4 days ago,Be among the first 25 applicants,"['', ' Experience performing root cause analysis on internal and external data and processes, to answer specific business questions and identify opportunities for improvement.', ' Design, development and maintenance of Inform Diagnostics  Azure Data Lake  (ADL) and Enterprise  Data Warehouse (EDW).  Data warehouse design, testing and support; including data design, database architecture, metadata and repository creation along with security and privacy management.  Create and maintain optimal data pipeline architecture using  Microsoft SQL Server,  SQL Data Warehouse , and Azure Data Lake/Blob Storage.  Develop ETLand support ETL (Extract, Load Transform) and ETL (Extract, Transform, Load) processes using SQL, SSIS,  Azure Data Factory with consideration to fault-tolerance, error logging, auditing and data quality.  Build/automate reports/dashboards (in Power BI and Microsoft Analysis Services) to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.  Implement data cleanup procedures, transformations, scripts, stored procedures, and execution of test plans for landing data successfully into the appropriate destinations.  Create tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.  Work with tools in the Microsoft Stack; Azure Data Factory, Azure Data Lake, Azure SQL Databases, Azure Data Warehouse, Azure Analysis Services and Power BI .', ' Build/automate reports/dashboards (in Power BI and Microsoft Analysis Services) to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.', ' Power BI or Reporting Services (SSRS).', ' Ability to work under deadlines in a fast-paced environment while prioritizing competing demands.', ' Design, development and maintenance of Inform Diagnostics  Azure Data Lake  (ADL) and Enterprise  Data Warehouse (EDW).', ' Strong Data Modeling skills to include data quality, source systems analysis, business rules validation, source to target, mapping design, performance tuning and high-volume data loads.', ' Create and maintain optimal data pipeline architecture using  Microsoft SQL Server,  SQL Data Warehouse , and Azure Data Lake/Blob Storage.', ' Work with tools in the Microsoft Stack; Azure Data Factory, Azure Data Lake, Azure SQL Databases, Azure Data Warehouse, Azure Analysis Services and Power BI .', ' Teamwork – Commitment to the successful achievement of team and organizational goals through a desire to participate with and help other members of the team.', ' Customer Service Focus – Demonstrate a focus on listening to and understanding client/customer needs and then delighting the client/customer by exceeding service and quality expectations.', ' Ability to quickly learn and work with new tools in the Microsoft Stack; Azure Data Factory, Azure Data Lake, Azure SQL Databases, Azure Data Warehouse, Azure Analysis Services and Power BI.', 'Responsibilities', ' Experience working with data analysts, data scientists, and other data consumers to optimize reports by defining business requirements and populating the data warehouse table structures.', ' Bachelor’s degree preferred.', ' Data warehouse design, testing and support; including data design, database architecture, metadata and repository creation along with security and privacy management.', ' Ability to be a quick problem solver.', ' Data Warehouse', ' Experience working in an agile team environment with the understanding of scrum and agile development methodologies.', ' 5+ years of progressive experience in relevant Business Data & Insights Administration & Support positionssoftware application and data warehouse. Healthcare industry experience preferred.  Bachelor’s degree preferred.  Advanced working knowledge of SSIS, T-SQL, SQL Server, and SQL Data Warehouse., ELT (Extract, Load, Transform), ETL (Extract, Transfer, Load) and Microsoft Applications.  SSAS Tabular/Multidimensional (including DAX).  Power BI or Reporting Services (SSRS).  Experience building and optimizing big data pipelines, architectures and data sets.  Excellent communication skills.  Ability to quickly learn and work with new tools in the Microsoft Stack; Azure Data Factory, Azure Data Lake, Azure SQL Databases, Azure Data Warehouse, Azure Analysis Services and Power BI.  Experience working with data analysts, data scientists, and other data consumers to optimize reports by defining business requirements and populating the data warehouse table structures.  Experience working in an agile team environment with the understanding of scrum and agile development methodologies.  Must be an experienced thought leader and expert in Business Insights, Warehouses, Data Enrichment, and Analytics.  Understanding of scrum and agile development methodologies  Ability to be a quick problem solver.  Outstanding client management skills and the ability to work with customers to execute on an implementation plan.  Excellent communication on both technical and non-technical contexts.  Ability to work under deadlines in a fast-paced environment while prioritizing competing demands.  Experience performing root cause analysis on internal and external data and processes, to answer specific business questions and identify opportunities for improvement.  Strong Data Modeling skills to include data quality, source systems analysis, business rules validation, source to target, mapping design, performance tuning and high-volume data loads.  Drive for Results (Service, Quality, and Continuous Improvement) – Ensure procedures and processes are in place that lead to delivery of quality results and continually reassess their effectiveness to achieve continuous improvement.  Communication – Excellent verbal and written communication skills. Willingness to share and receive information and ideas from all levels of the organization in order to achieve the desired results.  Teamwork – Commitment to the successful achievement of team and organizational goals through a desire to participate with and help other members of the team.  Customer Service Focus – Demonstrate a focus on listening to and understanding client/customer needs and then delighting the client/customer by exceeding service and quality expectations.', ' Excellent communication on both technical and non-technical contexts.', ' Understanding of scrum and agile development methodologies', 'Requirements', ' SSAS Tabular/Multidimensional (including DAX).', 'Description', ' Azure Data Factory', ' Power BI', ' Develop ETLand support ETL (Extract, Load Transform) and ETL (Extract, Transform, Load) processes using SQL, SSIS,  Azure Data Factory with consideration to fault-tolerance, error logging, auditing and data quality.', ' Communication – Excellent verbal and written communication skills. Willingness to share and receive information and ideas from all levels of the organization in order to achieve the desired results.', ' Experience building and optimizing big data pipelines, architectures and data sets.', ' Create tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.', ' Advanced working knowledge of SSIS, T-SQL, SQL Server, and SQL Data Warehouse., ELT (Extract, Load, Transform), ETL (Extract, Transfer, Load) and Microsoft Applications.', ' SQL Data Warehouse', ' Drive for Results (Service, Quality, and Continuous Improvement) – Ensure procedures and processes are in place that lead to delivery of quality results and continually reassess their effectiveness to achieve continuous improvement.', ' Azure Data Lake ', ' Outstanding client management skills and the ability to work with customers to execute on an implementation plan.', ' Implement data cleanup procedures, transformations, scripts, stored procedures, and execution of test plans for landing data successfully into the appropriate destinations.', ' Must be an experienced thought leader and expert in Business Insights, Warehouses, Data Enrichment, and Analytics.', ' 5+ years of progressive experience in relevant Business Data & Insights Administration & Support positionssoftware application and data warehouse. Healthcare industry experience preferred.', ' Microsoft SQL Server,', ' Data Engineer', ' Excellent communication skills.']",Entry level,Full-time,Information Technology,Medical Devices,2021-03-18 14:34:51
Data Engineer,Fortive,"Pittsburgh, PA",1 week ago,Be among the first 25 applicants,"['', 'Partner with data scientists to support modeling and reporting needs and to build and maintain internal data processing and visualization ', 'Design and implement real-time ETL and batch processing of data to support modeling and reporting needs', 'Proficiency with multi cloud environment such as AWS and Azure', 'Empathy & a Teacher’s Mindset: The ability to teach and mentor effectively, successfully facilitate teams and different personalities in training sessions, and to care deeply about his/her colleague’s growth, understanding, and experience.', 'Entrepreneurial Attitude: A proactive outlook that provides the chutzpah needed to overcome barriers, creatively problem solve, and challenge conventional thinking.', 'These Are The Traits We Value', 'Responsibilities', ' Design and implement real-time ETL and batch processing of data to support modeling and reporting needs Connect data to users in multi/mixed cloud and on-prem environments Define and share best practices and design for the management of data with The Fort and across Fortive OpCos Partner with data scientists to support modeling and reporting needs and to build and maintain internal data processing and visualization  Translate requests into replicable analytic reports using varying applications Create tools to serve data such as APIs and packages as needed ', 'Hands-on experience in Snowflake', 'Positive Outlook: A desire to look past the challenged in search of the opportunity.', 'Proficiency with Python, databases and SQL expertise', 'Must have worked with AWS compute infrastructure such as Lambda, ECS, EC2, Docker, Cloudformation, Step function, Glue, SageMaker', 'Connect data to users in multi/mixed cloud and on-prem environments', 'Some experience with MLOps either in AWS or Azure', 'Excellent problem solving, critical thinking, and communication skills', 'Create tools to serve data such as APIs and packages as needed', 'Snowflake experience is a plus', ""Bachelor's degree in Computer Science or related technical field or equivalent practical experience "", 'Bias for Action: A need for speed; demonstrated ability to make decisions quickly and to act upon them.', ' Ability to Deliver Results: A track record of setting vision and strategy, organize and lead a team, and implement to meet and exceed expectations. Entrepreneurial Attitude: A proactive outlook that provides the chutzpah needed to overcome barriers, creatively problem solve, and challenge conventional thinking. Comfort with Ambiguity: A willingness and aptitude for spending time in and thriving with deep uncertainty and environments where there is no clear “right answer”. Passion for Innovation: A demonstrated interest and desire to participate in innovation through personal study, on-the-job initiative, or other endeavors. Positive Outlook: A desire to look past the challenged in search of the opportunity. Empathy & a Teacher’s Mindset: The ability to teach and mentor effectively, successfully facilitate teams and different personalities in training sessions, and to care deeply about his/her colleague’s growth, understanding, and experience. Bias for Action: A need for speed; demonstrated ability to make decisions quickly and to act upon them. Alongside a team of entrepreneurial, high-performing, curious people, you’ll deliver breakthrough solutions to drive sustainable growth for Fortive. As a Lead Data Scientist, you’ll drive execution and play a vital role in building a culture of innovation at Fortive. You will build, manage, and inspire alongside a multi-functional team, that solves for our toughest growth challenges. ', 'Knowledge of machine learning and data science processes', 'Passion for Innovation: A demonstrated interest and desire to participate in innovation through personal study, on-the-job initiative, or other endeavors.', 'Experience supporting data science and analytical efforts is preferred', ' MS or PhD in Computer Science or other technical field Knowledge of machine learning and data science processes Experience supporting data science and analytical efforts is preferred Hands-on experience in Snowflake ', 'Define and share best practices and design for the management of data with The Fort and across Fortive OpCos', ' FOR000662', 'Ability to Deliver Results: A track record of setting vision and strategy, organize and lead a team, and implement to meet and exceed expectations.', 'Comfort with Ambiguity: A willingness and aptitude for spending time in and thriving with deep uncertainty and environments where there is no clear “right answer”.', 'Required Qualifications', 'Translate requests into replicable analytic reports using varying applications', 'Experience with distributed version control system (e.g. git)', 'MS or PhD in Computer Science or other technical field', "" Bachelor's degree in Computer Science or related technical field or equivalent practical experience  Good unordaining of Data Warehouse and Business Intelligence environment Proficiency with Python, databases and SQL expertise Proficiency with multi cloud environment such as AWS and Azure Must have worked with AWS compute infrastructure such as Lambda, ECS, EC2, Docker, Cloudformation, Step function, Glue, SageMaker Some experience with Azure environment such as Synapse, Azure SQL DB, Azure ML Pipeline, Azure Data Factory Snowflake experience is a plus Some experience with MLOps either in AWS or Azure Excellent problem solving, critical thinking, and communication skills Experience with distributed version control system (e.g. git) "", 'Some experience with Azure environment such as Synapse, Azure SQL DB, Azure ML Pipeline, Azure Data Factory', 'Preferred Qualifications', 'Good unordaining of Data Warehouse and Business Intelligence environment', 'Alongside a team of entrepreneurial, high-performing, curious people, you’ll deliver breakthrough solutions to drive sustainable growth for Fortive. As a Lead Data Scientist, you’ll drive execution and play a vital role in building a culture of innovation at Fortive. You will build, manage, and inspire alongside a multi-functional team, that solves for our toughest growth challenges.']",Not Applicable,Full-time,Information Technology,Medical Devices,2021-03-18 14:34:51
Data Engineer,Conduent,"Morrisville, NC",2 weeks ago,118 applicants,"['', 'Job Overview', '5+ years of related experience is required.', 'We are looking for a Data Engineer who will be part of our Analytics Practice and will be expected to actively work in a multi-disciplinary fast paced environment. This role requires a broad range of skills and the ability to step into different roles depending on the size and scope of the project; its primary responsibility is the development and mainteneance of data pipelines including acquisition, transformation, loading and processing of data.\xa0', 'Performs data management tasks, such as conduct data profiling, assess data quality, and write SQL queries to extract and integrate data', 'Title: Data Engineer', 'Experience connecting to varied data sources', 'Demonstrates high standards of professional behavior in dealings with clients, colleagues and staff.', 'Prior experience with application delivery using an Onshore/Offshore model', 'Additional', 'Location: Morrisville, North Carolina / REMOTE option available', 'Advising on new technology trends and possible adoption to maintain competitive advantage', 'Experience in deployment and maintenance of ETL Jobs.', 'Additional Requirements', '\ufeff', 'Learn and develop new ETL techniques as required to keep up with the contemporary technologies.', 'A BS or Masters degree in Computer Science or related technical discipline is required', 'Experience Needed:', 'Collaborate in design reviews and code reviews to ensure standards are met. Recommend new standards for visualizations.', 'Time management and multitasking skills to effectively meet deadlines under time-to-market pressure', 'Experience with gathering end user requirements and writing technical documentation', 'Experience in cloud-based ETL development processes.', 'Responsibilities:', 'Requirements', 'Support presentations to Customers and Partners', 'Requires some travel (on average 10%-20%)', 'Provide support to new or existing applications while recommending best practices and leading projects to implement new functionality.', 'Design, develop and maintain ETL processes using tools and scripting. Troubleshoot and debug ETL processes. Performance tuning and opitimization of the ETL processes.', 'Worked in big data environments, cloud data stores, different RDBMS and OLAP solutions.', 'Demonstrated ability to have successfully completed multiple, complex technical projectsPrior experience with application delivery using an Onshore/Offshore modelExperience with business processes across multiple Master data domains in a services based companyDemonstrates a rational and organized approach to the tasks undertaken and an awareness of the need to achieve quality.Demonstrates high standards of professional behavior in dealings with clients, colleagues and staff.Is able to make sound and far reaching decisions alone on major issues and to take full responsibility for them on a technical basis.Strong written communication skills. Is effective and persuasive in both written and oral communication.Experience with gathering end user requirements and writing technical documentationTime management and multitasking skills to effectively meet deadlines under time-to-market pressureRequires some travel (on average 10%-20%)', 'Is able to make sound and far reaching decisions alone on major issues and to take full responsibility for them on a technical basis.', 'Reviews the solution requirements and architecture to ensure selection of appropriate technology, efficient use of resources and integration of multiple systems and technology.', 'Strong written communication skills. Is effective and persuasive in both written and oral communication.', 'Has strong technical background and remains evergreen with technology and industry developments.', ' Data Engineer ', 'Engineer and maintain a modern Cloud data pipeline to collect, organize, and process data from disparate sources.Performs data management tasks, such as conduct data profiling, assess data quality, and write SQL queries to extract and integrate dataDevelop / maintain efficient data collection systems and sound strategies for getting quality data from different sourcesConsume and analyze data from the data pool to support inference, prediction and recommendation of actionable insights to support business growth.Design, develop and maintain ETL processes using tools and scripting. Troubleshoot and debug ETL processes. Performance tuning and opitimization of the ETL processes.Provide support to new or existing applications while recommending best practices and leading projects to implement new functionality.Collaborate in design reviews and code reviews to ensure standards are met. Recommend new standards for visualizations.Learn and develop new ETL techniques as required to keep up with the contemporary technologies.Reviews the solution requirements and architecture to ensure selection of appropriate technology, efficient use of resources and integration of multiple systems and technology.Support presentations to Customers and PartnersAdvising on new technology trends and possible adoption to maintain competitive advantage', 'Develop / maintain efficient data collection systems and sound strategies for getting quality data from different sources', '5+ years of related experience is required.A BS or Masters degree in Computer Science or related technical discipline is requiredETL experience with data integration to support data marts, extracts and reportingExperience connecting to varied data sourcesExcellent SQL coding experience with performance optimization for data queries.Understands different data models like normalized, de-normalied, stars, and snowflake models. Worked with transactional, temporal, time series, and structured and unstructured data.Worked in big data environments, cloud data stores, different RDBMS and OLAP solutions.Experience in cloud-based ETL development processes.Experience in deployment and maintenance of ETL Jobs.Is familiar with the principles and practices involved in development and maintenance of software solutions and architectures and in service delivery.Has strong technical background and remains evergreen with technology and industry developments.', 'Engineer and maintain a modern Cloud data pipeline to collect, organize, and process data from disparate sources.', 'Excellent SQL coding experience with performance optimization for data queries.', 'Is familiar with the principles and practices involved in development and maintenance of software solutions and architectures and in service delivery.', '\xa0\xa0', 'Consume and analyze data from the data pool to support inference, prediction and recommendation of actionable insights to support business growth.', 'ETL experience with data integration to support data marts, extracts and reporting', 'Understands different data models like normalized, de-normalied, stars, and snowflake models. Worked with transactional, temporal, time series, and structured and unstructured data.', '\xa0', 'Demonstrates a rational and organized approach to the tasks undertaken and an awareness of the need to achieve quality.', 'Demonstrated ability to have successfully completed multiple, complex technical projects', 'Experience with business processes across multiple Master data domains in a services based company']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,ASM Research,"San Antonio, TX",5 days ago,Be among the first 25 applicants,"['', 'Minimum Qualifications', 'Preferred Skills & Qualifications', 'Other Job Specific Skills']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer - ETL,Sayari Labs,United States,3 weeks ago,101 applicants,"['', 'Limitless growth and learning opportunities\xa0A collaborative and positive culture - your team will be as smart and driven as youA strong commitment to diversity, equity & inclusion\xa0Outstanding competitive compensation & comprehensive benefits package, including full healthcare coverage plans, commuter benefits, 401K matching, generous vacation, and a variety of other benefits.\xa0', 'Resume & any salary requirement', 'What We Offer:', 'Sayari maintains a graph of global corporate information containing hundreds of millions of entities and relationships. The structures and techniques that we use to store the information in this graph are critical to allowing our clients to benefit from this unique dataset. The application that serves this graph to our clients is powered by several databases fed by ETL pipelines that run in Apache Spark.', 'Experience with, or interest in, graph databases', 'Strong process-oriented self-starter, with impeccable organizational skills', 'To apply, please email the documents listed below to HR@sayarianalytics.com\xa0', 'Limitless growth and learning opportunities\xa0', 'Sayari is looking for a mid-level to Senior Data Engineer to join our Infrastructure team located in Washington, DC. The Infrastructure team is an integral part of our Engineering division and works closely with our Software Engineering & Data Science teams, as well as other key stakeholders across the business.', 'About Sayari Labs:', 'Experience using Apache Spark', 'Experience working on a cloud platform like GCP, AWS, or Azure', 'We at Sayari define our culture by a dedication to our mission of using open data to prevent illicit commercial and financial activity, a passion for finding novel approaches to complex problems, and an understanding that diverse perspectives create optimal outcomes. We embrace cross-team collaboration, open-source work, encourage training and learning opportunities, and reward initiative and innovation. If you like working with supportive, high-performing, and curious teams, Sayari is the place for you.\xa0', 'Solid experience working with multiple databases, for example, Cassandra, Neo4J, or Elasticsearch', 'Position Description:', 'A collaborative and positive culture - your team will be as smart and driven as you', 'Strong process-oriented self-starter, with impeccable organizational skillsExperienced in supporting and working with cross-functional teams in a dynamic environmentInterested in learning from and mentoring team members\xa0Passionate about open source development and innovative technology', '2+ years of experience designing and maintaining ETL pipelines', 'Sayari is an equal opportunity employer and strongly encourages diverse candidates to apply. We believe diversity and inclusion mean our team members should reflect the diversity of the United States. No employee or applicant will face discrimination or harassment based on race, color, ethnicity, religion, age, gender, gender identity or expression, sexual orientation, disability status, veteran status, genetics, or political affiliation. We strongly encourage applicants of all backgrounds to apply.', 'What You Will Need:', 'Who You Are:', 'Sayari is a venture-backed and founder-led global corporate data provider and commercial intelligence platform, serving financial institutions, legal and advisory service providers, multinationals, journalists, and governments. Thousands of analysts and investigators in over 30 countries rely on our products to safely conduct cross-border trade, research front-page news stories, confidently enter new markets, and prevent financial crimes such as corruption and money laundering.', 'A strong commitment to diversity, equity & inclusion\xa0', 'What You Will Do:\xa0', 'Resume & any salary requirementOptional: Brief note to highlight relevant experience or skills.\xa0\xa0Optional: Share links to any public repos of your previous work.\xa0', 'Passionate about open source development and innovative technology', 'Optional: Share links to any public repos of your previous work.\xa0', ""As a member of Sayari's engineering team, you will work to maintain existing ETL pipelines and add additional pipelines to implement new features. These pipelines terminate in several different databases including Apache Cassandra, Elasticsearch, Postgres, and Tigergraph (a cutting edge in-memory graph database). In addition to developing pipelines, you'll contribute to the design of the database schemas to ensure that data can be efficiently retrieved by the application.\xa0"", 'Experience with data orchestration frameworks like Apache Airflow', 'What We Would Like:', 'Experience with Docker and Kubernetes', 'Experienced in supporting and working with cross-functional teams in a dynamic environment', 'Strong experience with any two of Python, Java, Scala', 'Experience with, or interest in, graph databasesExperience with Docker and Kubernetes', 'We are continually looking to expand our tech stack with new and innovative technologies.\xa0 Have an idea for something cutting edge?\xa0 Come talk to us about it.', 'Experience working collaboratively with git', 'What We Offer:\xa0', 'Optional: Brief note to highlight relevant experience or skills.\xa0\xa0', 'Strong experience with any two of Python, Java, Scala2+ years of experience designing and maintaining ETL pipelinesExperience using Apache SparkExperience with data orchestration frameworks like Apache AirflowSolid experience working with multiple databases, for example, Cassandra, Neo4J, or ElasticsearchExperience working on a cloud platform like GCP, AWS, or AzureExperience working collaboratively with git', 'Outstanding competitive compensation & comprehensive benefits package, including full healthcare coverage plans, commuter benefits, 401K matching, generous vacation, and a variety of other benefits.\xa0', 'How to Apply:\xa0', 'Interested in learning from and mentoring team members\xa0']",Associate,Full-time,Engineering,Information Services,2021-03-18 14:34:51
Data Engineer,Finxact,"Jacksonville, FL",7 days ago,Be among the first 25 applicants,"['', 'Ensures the database design and operations support the availability, scalability, and recoverability needs of the business ', ',', 'BS degree in Information Systems, Engineering, or equivalent ', 'Provide support for development and testing including test data setup, automated testing scenarios, and DEV/UAT environment data refreshes', 'Ability to manage diverse activities simultaneously, delivering on commitments and operating with speed, accuracy and strong judgment', 'Experience with agile software development and SRE principles a plus', 'Research and evaluate emerging database technologies and trends to enhance the use of data and databases within the organization ', 'Expert in query optimization, indexing knowledge, modeling basics, materialized views and partitioning', 'Expert with PL/PgSQL, triggers, and stored procedure development and optimization in PostgreSQL', 'Provide leadership for all policies and procedures that govern data operations, security, management, and usage', 'Experience with cloud-based RDBMS systems a plus ', '7+ years experience with relational database administration', 'Proficiency with operating PostgreSQL in a Linux environment', 'Responsible for designing, implementing, and maintaining critical database systems (PostgreSQL) which support a growing 24x7 SaaS platformEnsures the database design and operations support the availability, scalability, and recoverability needs of the business Performs tuning and capacity management to ensure the database meets the performance needs of the businessGuides and provides direction to developers and engineers on database development, design principles, query optimization, and index management Provide support for development and testing including test data setup, automated testing scenarios, and DEV/UAT environment data refreshesResearch and evaluate emerging database technologies and trends to enhance the use of data and databases within the organization Provide leadership for all policies and procedures that govern data operations, security, management, and usageProvides on-call coverage for production support and management', 'Performs tuning and capacity management to ensure the database meets the performance needs of the business', '7+ years experience with relational database administration3+ years experience administering highly-available PostgreSQL databasesExpert in RDBMS principles, database design and normalizationExpert in PostgreSQL database architecture, concepts, features, and high-availability technologiesExpert in query optimization, indexing knowledge, modeling basics, materialized views and partitioningExpert with PL/PgSQL, triggers, and stored procedure development and optimization in PostgreSQLProficiency with operating PostgreSQL in a Linux environmentExperience with cloud-based RDBMS systems a plus Experience with agile software development and SRE principles a plusDisplays an entrepreneurial instinct for identifying value and pragmatic solutionsAbility to manage diverse activities simultaneously, delivering on commitments and operating with speed, accuracy and strong judgmentAbility to operate independently as well as influence across the technology organizationStrong project management skills and detail orientedBS degree in Information Systems, Engineering, or equivalent ', 'Displays an entrepreneurial instinct for identifying value and pragmatic solutions', ""\u202aWhat You'll Do"", 'Guides and provides direction to developers and engineers on database development, design principles, query optimization, and index management ', 'Expert in RDBMS principles, database design and normalization', 'Responsible for designing, implementing, and maintaining critical database systems (PostgreSQL) which support a growing 24x7 SaaS platform', 'Ability to operate independently as well as influence across the technology organization', 'Data Engineer', 'Finxact', '3+ years experience administering highly-available PostgreSQL databases', 'Expert in PostgreSQL database architecture, concepts, features, and high-availability technologies', ""\u202aWhat You'll Bring To Finxact"", 'Provides on-call coverage for production support and management', 'Strong project management skills and detail oriented']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer - Data Platform,Rover.com,"Seattle, WA",3 weeks ago,53 applicants,"['', ""Monitoring, maintaining and improving Rover's data pipelines and its associated infrastructure. "", 'Due to COVID-19, Rover Employees are not required to be in office until July 2021 at the earliest. All new hires will be expected to work from Seattle/ Spokane/ Barcelona once Rover Employees return to office.', 'Competitive compensation401k MatchStock optionsFlexible PTOCompetitive benefits package, including medical, dental, and vision insuranceCommuter benefitsBring your dog to work (and unlimited puppy time)Pet benefits, including $1000 toward adopting your first dog or catStocked fridges, coffee, soda, and lots of treats (for humans and dogs) and free catered lunches semi-monthly (currently on hold due to Covid19)Regular team activities, including happy hours, snow tubing, game nights, and more (currently performed virtually due to Covid19)Due to COVID-19, Rover Employees are not required to be in office until July 2021 at the earliest. All new hires will be expected to work from Seattle/ Spokane/ Barcelona once Rover Employees return to office.', 'Solid understanding of relational databases and schema design. ', 'Pet benefits, including $1000 toward adopting your first dog or cat', 'Passionate about automated testing and delivering good quality code.', 'Regular team activities, including happy hours, snow tubing, game nights, and more (currently performed virtually due to Covid19)', 'At least 3 years of relevant industry experience.', 'Familiarity with large-scale distributed real-time tools such as Kafka and Spark. ', 'Designing, developing, testing, and automating high-performance batch and stream data processing systems. ', 'Plus: Experience working with AWS, Docker, Luigi, Jupyter, Kubernetes, Redshift, RDS, Terraform. ', '401k Match', 'Building self-serve tools that make it easy to get key business insights.', 'Flexible PTO', ' Data Engineering At Rover ', 'Who We Are ', 'Supporting our partner teams by solving complex problems in development and production in a timely manner. ', ""Designing, developing, testing, and automating high-performance batch and stream data processing systems. Monitoring, maintaining and improving Rover's data pipelines and its associated infrastructure. Supporting our partner teams by solving complex problems in development and production in a timely manner. Building self-serve tools that make it easy to get key business insights."", 'Commuter benefits', 'Stocked fridges, coffee, soda, and lots of treats (for humans and dogs) and free catered lunches semi-monthly (currently on hold due to Covid19)', 'Bring your dog to work (and unlimited puppy time)', 'Stock options', 'Experience designing and deploying systems with effective monitoring and logging practices. ', 'Competitive compensation', ' Your Qualifications ', ' Your Responsibilities ', ' Benefits Of Working For Rover ', 'At least 3 years of relevant industry experience.Strong Python skills in both data and software engineering contexts.Experience designing and deploying systems with effective monitoring and logging practices. Solid understanding of relational databases and schema design. Passionate about automated testing and delivering good quality code.Familiarity with large-scale distributed real-time tools such as Kafka and Spark. Plus: Experience working with AWS, Docker, Luigi, Jupyter, Kubernetes, Redshift, RDS, Terraform. ', 'Competitive benefits package, including medical, dental, and vision insurance', 'Strong Python skills in both data and software engineering contexts.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Windstream,"Greenville, SC",6 days ago,Be among the first 25 applicants,"['', ' Database Administration  Strong understanding of event handling Exposure to scheduling tools like Airflow or Clover  ', 'HIRING NOW', 'Desired Skills', 'Ability to build processes that support data transformation, workload management, data structures, dependency and metadata. ', 'Minimum Requirements', 'Self-starter, relentlessly curious, resourceful, collaborative, and inventive. ', 'Highly organized and meticulous. ', 'Experience working with large, disparate data sets ', 'Drive to succeed and improve personally, and in ability to add value to the role, team, and company. ', ' Positivity, and the desire to solve problems in elegant and creative ways.  ', 'Job Responsibilities', ' Promotion of code through weekly code reviews into the Test and Production environments.  Manage projects through to completion.  Manage work through Agile tools/methodology, collaborative repositories, issue tracking platforms, and wikis.  ', 'Scale and optimize performance via schema design, query tuning, and index creation. ', 'Promotion of code through weekly code reviews into the Test and Production environments. ', 'Positivity, and the desire to solve problems in elegant and creative ways. ', 'Familiarity with various functional areas pertaining to telecommunications networks eg circuit design, traffic engineering, network system design ', ' Strong SQL experience, DML/DDL (PL/SQL, T-SQL, etc).  Integration of multiple data sources and databases into one system.  ', 'Exposure to Business Intelligence (BI) reporting tools like Tableau', ' Knowledge of data warehouse concepts (star schema, fact and dimension tables)  Experience working with large, disparate data sets  Excellent analytic skills associated with working on unstructured datasets  Exposure to code versioning tools, such as Git  ', 'Promotion of code through weekly code reviews into the Test and Production environments.', 'Exposure to code versioning tools, such as Git ', 'Primary Location', 'Exposure to Atlassian data stack: JIRA, Confluence, Fisheye ', 'EEO Statement', ' Ability to build processes that support data transformation, workload management, data structures, dependency and metadata.  Drive to succeed and improve personally, and in ability to add value to the role, team, and company.  Self-starter, relentlessly curious, resourceful, collaborative, and inventive.  Good team player and communicator.  Highly organized and meticulous.  ', 'Troubleshoot production issues, identifying root cause and implementing sound technical resolutions in a timely manner. ', 'Database Administration ', 'Windstream is considered an essential business and we are ', 'Management of database environment working in concert with IT resources.', 'Data Engineer-Network Business Intelligence Job Description', 'Strong SQL experience, DML/DDL (PL/SQL, T-SQL, etc). ', 'Monitoring daily job scheduling activities. ', 'Other Locations', 'Strong understanding of event handling', 'Excellent analytic skills associated with working on unstructured datasets ', 'Manage projects through to completion. ', 'Essential Skills', 'Work Locations', 'Development of Extract-Transform-Load (ETL) logic to meet functional business requirements primarily using Oracle packages and scheduling software to support business intelligence needs using large, disparate data sets. ', ' Exposure to Atlassian data stack: JIRA, Confluence, Fisheye  Exposure to Python  Exposure to Business Intelligence (BI) reporting tools like Tableau Familiarity with various functional areas pertaining to telecommunications networks eg circuit design, traffic engineering, network system design  Agile Planning understanding  ', 'Exposure to scheduling tools like Airflow or Clover ', '. As our company responds to COVID-19, the safety and wellbeing of our employees, customers, partners and communities is our top priority!', 'Good team player and communicator. ', 'Manage work through Agile tools/methodology, collaborative repositories, issue tracking platforms, and wikis. ', 'Exposure to Python ', ' Development of Extract-Transform-Load (ETL) logic to meet functional business requirements primarily using Oracle packages and scheduling software to support business intelligence needs using large, disparate data sets.  Promotion of code through weekly code reviews into the Test and Production environments. Management of database environment working in concert with IT resources. Monitoring daily job scheduling activities.  Troubleshoot production issues, identifying root cause and implementing sound technical resolutions in a timely manner.  Scale and optimize performance via schema design, query tuning, and index creation.  Identify and document data sources, criteria, and data mapping.  ', 'Integration of multiple data sources and databases into one system. ', 'Job Category', 'Identify and document data sources, criteria, and data mapping. ', 'Agile Planning understanding ', 'Knowledge of data warehouse concepts (star schema, fact and dimension tables) ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,FutureSoftIT,"Farmington Hills, MI",3 days ago,33 applicants,"['', 'Read files with S3 buckets using python to load into the database', 'SQS', 'SQL experience - create SQL queries from scratch/modify ', 'Assemble large, complex data sets that meet functional / non-functional business requirements.', 'You will be bringing new data into their data warehouse (Snowflake), and help integrate the data into other systems (i.e. Salesforce Marketing Cloud, Adobe Analytics, etc). You will also utilize real time data and analytics (i.e. e-mail campaigns that take consumers to the website).', 'Get data from the systems, read the source files from source systems, do ETL of data, write ETL logic, and load into Data-warehouse.', 'S3 buckets', 'Responsibilities', 'Create and maintain optimal data pipeline', 'Keys', ' S3 buckets SQS Lambda ', 'Relational database experience', 'AWS cloud services: EC2, S3, SQS, Lambda', 'Lambda', 'Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases', 'Location', 'AWS and associated technologies such as: S3 buckets SQS Lambda  ', 'Help build out data lake', ' You will be bringing new data into their data warehouse (Snowflake), and help integrate the data into other systems (i.e. Salesforce Marketing Cloud, Adobe Analytics, etc). You will also utilize real time data and analytics (i.e. e-mail campaigns that take consumers to the website). Get data from the systems, read the source files from source systems, do ETL of data, write ETL logic, and load into Data-warehouse. Read files with S3 buckets using python to load into the database Create and maintain optimal data pipeline Help build out data lake Assemble large, complex data sets that meet functional / non-functional business requirements. Interact with the business to understand the data requirements, and what data they need, and also work with the IT teams to gather additional data. Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases AWS cloud services: EC2, S3, SQS, Lambda', 'Role', 'Interact with the business to understand the data requirements, and what data they need, and also work with the IT teams to gather additional data.', 'AWS', '5-7 years of experience in Data Engineering', ' 5-7 years of experience in Data Engineering Python Coding (MUST HAVE) AWS and associated technologies such as: S3 buckets SQS Lambda   SQL experience - create SQL queries from scratch/modify  Relational database experience ', 'Python Coding (MUST HAVE)', 'SQL experience - ']",Entry level,Contract,Information Technology,Electrical/Electronic Manufacturing,2021-03-18 14:34:51
Data Engineer - Los Angeles,Gimbal ,"Los Angeles, CA",2 weeks ago,51 applicants,"['', ' Familiarity with Data Science tooling in Spark preferred', ' Experience working with HDFS and S3 preferred ', ' Be an active and engaged owner of our data infrastructure', '  A universal communicator  — you are able to explain the most technical data to the least technical people without any confusion   A proactive problem solver — you are constantly looking for ways to work smarter, delivering new efficiencies anywhere an improvement can be made   A builder — you are passionate about collecting, storing, and analyzing big data', ' Strong knowledge with Spark (using Scala) ', ' BS in Computer Science or related field required ', ' Troubleshoot and resolve issues, problems, and errors encountered across various systems', ' Be curious and seek to understand all aspects of our business', ' A builder', ' Gather requirements when underspecified', ' Familiarity with columnar database, key-value stores, document stores, stream processing, time series databases, data warehouses, and OLAP preferred', 'About The Job', ' Cell phone reimbursement and subsidized gym membership', ' A universal communicator ', 'What You Have', ' 401(k) plan plus company match ', ' Build and modify Spark jobs (in Scala) to perform various tasks, from reading Kinesis streams using Spark Streaming, to joining and aggregating huge data sets, to integrating with third party data sources  Develop and launch new features to adapt to evolving business needs  Be an active and engaged owner of our data infrastructure  Be curious and seek to understand all aspects of our business  Maintain high standards of code quality, and encourage the same by providing constructive code reviews to collaborators  Troubleshoot and resolve issues, problems, and errors encountered across various systems  Collaborate with Data Science, Product, Research, and Engineering teams to iterate on the roadmap  Gather requirements when underspecified', ' Maintain high standards of code quality, and encourage the same by providing constructive code reviews to collaborators', 'Who You Are', ' Opportunities for profit sharing, bonuses, and ownership', ' Experience in the advertising industry and with real-time analytics is a plus ', ' Must be a strong written and verbal communicator ', ' 100% company-paid health, dental, and vision coverage for you and your dependents, as well as life insurance and disability coverage', 'What You’ll Be Doing', 'Benefits & Perks.', ' 100% company-paid health, dental, and vision coverage for you and your dependents, as well as life insurance and disability coverage  Unlimited paid time off - we trust your discretion  Opportunities for profit sharing, bonuses, and ownership  401(k) plan plus company match   Cell phone reimbursement and subsidized gym membership  Annual professional development stipend ', ' Annual professional development stipend ', ' Develop and launch new features to adapt to evolving business needs', ' Working knowledge of serialization formats and their trade-offs (columnar vs row-based) Experience debugging and optimizing Spark jobs ', ' A proactive problem solver', ' Build and modify Spark jobs (in Scala) to perform various tasks, from reading Kinesis streams using Spark Streaming, to joining and aggregating huge data sets, to integrating with third party data sources', '  A universal communicator  — you are able to explain the most technical data to the least technical people without any confusion', 'About The Company', '  A proactive problem solver — you are constantly looking for ways to work smarter, delivering new efficiencies anywhere an improvement can be made', ' Unlimited paid time off - we trust your discretion', ' Collaborate with Data Science, Product, Research, and Engineering teams to iterate on the roadmap', '  A builder — you are passionate about collecting, storing, and analyzing big data', ' Strong knowledge of SQL required ', ' Familiarity with database fundamentals, such as ACID, snowflake schema, normalized/denormalized data ', ' BS in Computer Science or related field required   Strong knowledge of SQL required   Strong knowledge with Spark (using Scala)   Working knowledge of serialization formats and their trade-offs (columnar vs row-based) Experience debugging and optimizing Spark jobs   Familiarity with database fundamentals, such as ACID, snowflake schema, normalized/denormalized data   Must be a strong written and verbal communicator   Familiarity with columnar database, key-value stores, document stores, stream processing, time series databases, data warehouses, and OLAP preferred  Experience working with HDFS and S3 preferred   Familiarity with Data Science tooling in Spark preferred  Experience in the advertising industry and with real-time analytics is a plus ']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Data Engineer,Zuora,"Redwood City, CA",4 weeks ago,Be among the first 25 applicants,"['', 'Designing, implementing and operating robust API services', 'data pipeline orchestration (we currently use Airflow)', 'Working cross-functionally to support an effective corporate data governance program', ""As An Engineer On The Zuora Corporate Data Team, You'll Have The Opportunity To Design And Build Robust End-to-end Solutions To Support Data-driven Decision Making Across The Entire Company, Including"", 'Specific Desired Experience And Knowledge Includes', 'Pipelines and tools for delivering a reliable flow of metrics from all our products to help understand customer usage', 'A centralized data warehouse that assembles key data and insights about customers, users and products into one central location', 'interactive data exploration and visualization', 'Orchestration for extracting and transforming data from all our business applications and systems', ' python for data integration and application development complex SQL for data transformation, validation, and analytics data pipeline orchestration (we currently use Airflow) provisioning and monitoring public cloud infrastructure (AWS) interactive data exploration and visualization', 'Writing design and implementation documentation', 'Integrations with BI tools and other applications to provide business and technical users with secure, flexible trustworthy access to the data they need, through interfaces that meet their specific needs', ' Working directly with end users in a variety of different roles to understand business requirements and identify appropriate technical solutions Working cross-functionally to support an effective corporate data governance program Writing design and implementation documentation Running a transparent, predictable software delivery lifecycle, including planning, testing and release management Operating high-availability, mission-critical services ', 'Skills', 'Provisioning and maintaining complete architectures on public cloud infrastructure (AWS)', 'python for data integration and application development', 'Operating high-availability, mission-critical services', 'Building and monitoring data pipelines using open source tools and frameworks', 'Running a transparent, predictable software delivery lifecycle, including planning, testing and release management', 'Working directly with end users in a variety of different roles to understand business requirements and identify appropriate technical solutions', 'provisioning and monitoring public cloud infrastructure (AWS)', 'Data modeling and solving complex analytic transformations using SQL', ' Pipelines and tools for delivering a reliable flow of metrics from all our products to help understand customer usage Orchestration for extracting and transforming data from all our business applications and systems A centralized data warehouse that assembles key data and insights about customers, users and products into one central location Integrations with BI tools and other applications to provide business and technical users with secure, flexible trustworthy access to the data they need, through interfaces that meet their specific needs ', 'complex SQL for data transformation, validation, and analytics', 'Working with a variety of data stores and technologies', ' Building and monitoring data pipelines using open source tools and frameworks Designing, implementing and operating robust API services Provisioning and maintaining complete architectures on public cloud infrastructure (AWS) Working with a variety of data stores and technologies Data modeling and solving complex analytic transformations using SQL ']",Entry level,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Engineer,Experis,"Los Angeles, CA",6 days ago,Be among the first 25 applicants,"['', 'Must have expert level knowledge creating complex SQL Scripts ', '7+ years of business/data analysis skills and 5+ years of using tools such as SQL and Excel to gather and analysis data', 'Requirements:', ' ', 'Must be analytical minded and have sold analysis experience reviewing large datasets ', ""We are actively seeking a Data Enginer for our well-known client in the entertainment / gaming industry. Here's what we are looking for!"", ""Master's degree in Analytics, Computer Science, or Data Science; Mathematics or in a related field and 3-5 years of industry experience in data analysis using SQL"", ""7+ years of business/data analysis skills and 5+ years of using tools such as SQL and Excel to gather and analysis dataMust have expert level knowledge creating complex SQL Scripts Must be analytical minded and have sold analysis experience reviewing large datasets Master's degree in Analytics, Computer Science, or Data Science; Mathematics or in a related field and 3-5 years of industry experience in data analysis using SQL"", 'Desired Skills and Experience']",Mid-Senior level,Contract,Information Technology,Computer Games,2021-03-18 14:34:51
Data Engineer,Flock Safety,"Atlanta, GA",1 week ago,26 applicants,"['', 'Monitoring (e.g. Grafana, Prometheus)', ' Experience with ', 'Data Storage (e.g. RDBMS, RedShift, Druid, Elastic, HDFS, Hudi, InfluxDB, Cassandra, Neo4j)', 'Task Schedulers (e.g. Airflow, Prefect, Luigi)', ""Some challenges you'll tackle"", 'Eliminate Crime. Build Community. ', 'Machine Learning Quality Monitoring', 'Why join the Flock? ', '.', 'Stream Processing (e.g. Kinesis, Kafka, Storm, Flink, Spark)', ' Data Storage (e.g. RDBMS, RedShift, Druid, Elastic, HDFS, Hudi, InfluxDB, Cassandra, Neo4j) Stream Processing (e.g. Kinesis, Kafka, Storm, Flink, Spark) Task Schedulers (e.g. Airflow, Prefect, Luigi) Monitoring (e.g. Grafana, Prometheus) ', 'Creating pipelines and data stores for specific use cases', 'Connecting and aggregating various data sources into data warehouses', 'About The Opportunity', 'Effectively communicate, at the level of your audience, and seek to understand and be understood', 'Experience with ', ' Connecting and aggregating various data sources into data warehouses Creating pipelines and data stores for specific use cases', 'Machine Learning Active Learning', 'Experience is Data Engineering AWS services', 'Evidence Search and Advanced Search', 'Able to take on complex problems, learn quickly, iterate, and persist towards a good solution', ' Evidence Search and Advanced Search Machine Learning Active Learning Machine Learning Quality Monitoring Aggregating Company Operational Metrics ', 'About You', 'Basic git knowledge', 'Aggregating Company Operational Metrics']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,BuildZoom,"Scottsdale, AZ",1 week ago,Be among the first 25 applicants,"['', 'Collaboration. Our team is stronger than the sum of its parts.', '3+ years of hands-on software engineering experience in a professional team-based environment.', 'Broad interests to influence data and business strategy.', 'Quantitative. Our business decisions are made with data.', 'Drive. A passionate desire to improve the construction and remodeling industry.', 'Self Improvement. A dedication to personal growth, achievement, and self-actualization.', 'Scrappy and Tenacious. Win in the face of seemingly insurmountable obstacles.', 'Must have professional experience: Python, SQL, Data Modeling', 'This position can be located in Phoenix, AZ or San Francisco, CA. ', 'Nice to have experience: Go, Airflow, Elasticsearch, Redis, AWS Lambda, AWS Aurora, Snowflake', 'Exceptional written/verbal communication and emotional intelligence.3+ years of hands-on software engineering experience in a professional team-based environment.Broad interests to influence data and business strategy.Must have professional experience: Python, SQL, Data ModelingNice to have experience: Go, Airflow, Elasticsearch, Redis, AWS Lambda, AWS Aurora, Snowflake', 'Exceptional written/verbal communication and emotional intelligence.', 'By submitting your interest in this job, you agree to receive text notifications with additional steps to complete your job application. You will receive up to 6 messages from the number ""63879"". Message & data rates may apply. Please refer to our privacy policy for more information.', 'Drive. A passionate desire to improve the construction and remodeling industry.Self Improvement. A dedication to personal growth, achievement, and self-actualization.Collaboration. Our team is stronger than the sum of its parts.Quantitative. Our business decisions are made with data.Scrappy and Tenacious. Win in the face of seemingly insurmountable obstacles.', 'Requirements', 'Values']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,ATN International,"Castle Rock, CO",3 days ago,Be among the first 25 applicants,"['', 'JOB DUTIES', 'QUALIFICATIONS:', 'Equal Opportunity Employer/Veterans/Disabled', 'PI131932722', '(include but are not limited to):', 'PREFERRED QUALIFICATIONS:']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,G2 Solutions ,Raleigh-Durham-Chapel Hill Area,,N/A,"['', '• Operate with minimal supervision and once given general assignments, prioritizes and executes tasks. ', '• Ability to communicate complex technical information in common language to foster teaching and analytics guidance to internal customers. • Advanced experience in analytics, data cleaning, and predictive modeling. ', '• Ability to write advanced SQL queries. ', '• Bachelor’s degree in statistics, applied mathematics, or related discipline', '• Detail-oriented and ability to work collaboratively', '• Research new statistical and mathematical techniques that are suitable and helpful for solving business related problems ', '• Develop predictive systems and algorithms for identifying trends and driving business solutions. ', '• Develop new analysis to improve metrics around in stocks, turns, and trans cost. ', 'G2 Solutions is searching for a Data Scientist for a direct hire position located in Raleigh, NC. In this role you will explore large data sets to build model ready data products and leverage analytics tools like SAS to build machine learning models for predictive strategies. You must be very familiar with modeling tools such as Python, R, & SAS with the ability to write advanced SQL queries. We are looking for detail-oriented individuals who are self-starting and have the ability to work collaboratively with a team. Please review the requirements and responsibilities below and apply today to speak with one of our experienced recruiters. ', 'Qualifications ', '• Experience in model validation techniques, model testing and continuous monitoring of model performance ', '• Utilizes industry-leading standards for working with very large datasets to extract meaningful business information using statistics, machine learning, and predictive analytics. ', '• 2+ years of Data Modeling or similar experience. ', '• 3+ years of experience with predictive modeling (classification, regression, parameter tuning, optimization criteria, feature selection), preferably with multiple techniques ', '•Strong knowledge in SAS, R, Python or another platform to develop and implement predictive models. ', '• Prepare data for modelling and make best/creative use of applicable and available internal or external data ', '• Ability and willingness to quickly gain knowledge of SAS enterprise guide and enterprise miner. ', 'Responsibilities ', 'Would you like to join a growing Predictive Modeling team? You’ve landed on the right page! ', '• Demonstrated experience working with large relational data sets. ']",Mid-Senior level,Full-time,Information Technology,Staffing and Recruiting,2021-03-18 14:34:51
Data Engineer,DISH Network,"Englewood, CO",1 week ago,Be among the first 25 applicants,"['', ' Experience supporting and working with cross-functional teams in a dynamic environment ', ' Strong Software Engineering experience with proficiency in at least one of the following programming languages: Spark, Python, Scala or equivalent ', ' Experience with scheduling tools preferrable Control-M,Airflow or AWS Step functions. ', ' Experience with building streaming ingestion pipleline using Kafka streams, Apache Flink, or others ', ' Experience with building data ingestion pipelines both real time and batch using best practices ', ' Solve complex data integration problems ', ' Work with business analysts to understand business requirements and use cases', ' Manage day-to-day development activities for new data solutions and troubleshooting existing implementations. ', ' Experience with relational SQL and NoSQL databases, including Postgres, and Mongodb. ', 'Technical Requirements', ' 3+ years of experience working with distributed data technologies (e.g. Spark, Kafka, Flink etc) for building efficient, large-scale ‘big data’ pipelines; ', ' Develop and maintaining code for data ingestion and curation using databricks ', ' Work with product owners and technical leads to lead technical discussions and resolve technical issues ', ' Strong interpersonal, analytical, problem-solving, influencing, prioritization, decision- making and conflict resolution skills ', ' Experience with Cloud Computing platforms like Amazon AWS, Google Cloud etc. ', ' Lead teams in design, development and delivery of data solutions in wireless space using cloud data platforms ', ' Excellent written/verbal communication skills. ', ' Minimum of 5 years of experience delivering data solutions on a variety of data warehousing, big data and cloud data platforms.  3+ years of experience working with distributed data technologies (e.g. Spark, Kafka, Flink etc) for building efficient, large-scale ‘big data’ pipelines;  Strong Software Engineering experience with proficiency in at least one of the following programming languages: Spark, Python, Scala or equivalent  Experience with building data ingestion pipelines both real time and batch using best practices  Experience with building streaming ingestion pipleline using Kafka streams, Apache Flink, or others  Experience with Cloud Computing platforms like Amazon AWS, Google Cloud etc.  Experience supporting and working with cross-functional teams in a dynamic environment  Experience with relational SQL and NoSQL databases, including Postgres, and Mongodb.  Experience with change data capture tools (CDC) preferred such as Attunity/goldengate  Experience with scheduling tools preferrable Control-M,Airflow or AWS Step functions.  Strong interpersonal, analytical, problem-solving, influencing, prioritization, decision- making and conflict resolution skills  Excellent written/verbal communication skills. ', ' Minimum of 5 years of experience delivering data solutions on a variety of data warehousing, big data and cloud data platforms. ', ' Work with product vendors to identify and manage open product issues ', ""What You'll Do"", ' Apply best practices of data integration for data quality and automation ', ' Experience with change data capture tools (CDC) preferred such as Attunity/goldengate ', ' Lead teams in design, development and delivery of data solutions in wireless space using cloud data platforms  Manage day-to-day development activities for new data solutions and troubleshooting existing implementations.  Work with product owners and technical leads to lead technical discussions and resolve technical issues  Apply best practices of data integration for data quality and automation  Work with product vendors to identify and manage open product issues  Solve complex data integration problems  Develop and maintaining code for data ingestion and curation using databricks  Work with business analysts to understand business requirements and use cases']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Big Data and ETL Engineer,Rose International,"Home, KS",22 hours ago,Be among the first 25 applicants,"['', ' Design and build scalable infrastructure and platform to ingest, store and process very large amounts of data (structured, semi-structured and unstructured), including streaming and real-time into the cloud based Big Data Lake Build best in class ETL/ELT based solutions for effective data ingestion and transformation Collaborate with the various technical, platform and product development team on the use of various AWS services, including Amazon Elastic Compute Cloud (EC2), S3, Elastic Map Reduce (EMR), Glue, Athena etc. Work closely with the IT and the platform team to deliver technical solutions. This includes end to end data orchestration, data layering, data governance, metadata management and data cataloging Be an expert in all things data including data wrangling, aggregation, summarization and analysis Collaborate with the various data source teams both internal and external (Oracle, Teradata, MongoDB, cloud etc.) on effective strategies for data ingestion Partner with the data science, actuarial science and various business group on deep rooted data analysis including propensity scores, catastrophic modeling, time-series analysis and forecasting Be an evangelist for all things Big Data and Digital', ' Work closely with the IT and the platform team to deliver technical solutions. This includes end to end data orchestration, data layering, data governance, metadata management and data cataloging', ' Bachelor’s Degree in Computer Science, Mathematics, Engineering or Statistics. Master’s Degree a definite plus 5-10 years of relevant Data Ingestion/ETL/ELT, Big Data and Data Visualizations experience of mission critical platforms Experience in ETL/ELT and data wrangling using tools (Informatica, Talend etc.) and languages such as Python, R, Scala, Java, SQL and SAS Experience in Hadoop and Big Data platforms including cloud based platforms such as AWS, Azure & GCP. Experience in various services such as Hive, EMR, Juypter notebooks, Kinesis, SQS, Glue, Athena, Kafka, Redis, MongoDB etc.', ' 5-10 years of relevant Data Ingestion/ETL/ELT, Big Data and Data Visualizations experience of mission critical platforms', ' Design and build scalable infrastructure and platform to ingest, store and process very large amounts of data (structured, semi-structured and unstructured), including streaming and real-time into the cloud based Big Data Lake', ' Be an evangelist for all things Big Data and Digital', 'Responsibilities', ' Experience in various services such as Hive, EMR, Juypter notebooks, Kinesis, SQS, Glue, Athena, Kafka, Redis, MongoDB etc.', ' Bachelor’s Degree in Computer Science, Mathematics, Engineering or Statistics. Master’s Degree a definite plus', 'Position Description', ' Be an expert in all things data including data wrangling, aggregation, summarization and analysis', 'Qualifications', ' Collaborate with the various data source teams both internal and external (Oracle, Teradata, MongoDB, cloud etc.) on effective strategies for data ingestion Partner with the data science, actuarial science and various business group on deep rooted data analysis including propensity scores, catastrophic modeling, time-series analysis and forecasting', ' Experience in ETL/ELT and data wrangling using tools (Informatica, Talend etc.) and languages such as Python, R, Scala, Java, SQL and SAS', ' Collaborate with the various technical, platform and product development team on the use of various AWS services, including Amazon Elastic Compute Cloud (EC2), S3, Elastic Map Reduce (EMR), Glue, Athena etc.', 'Job Description', ' Experience in Hadoop and Big Data platforms including cloud based platforms such as AWS, Azure & GCP.', ' Build best in class ETL/ELT based solutions for effective data ingestion and transformation']",Entry level,Full-time,Engineering,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,MSX International,"Southfield, MI",4 weeks ago,51 applicants,"['', 'Other Competencies & Characteristics', 'Create and update documentation of process flows and business rules ', 'Able to complete advanced statistical analysis is required', 'Enthusiastic, highly motivated and ability to learn quick', ' Experience building Azure cloud data solutions and migrating from on-prem to cloud Previous external client facing experience  Proficiency in T-SQL Familiarity with administering MS SQL Server Strong communication skills and a working knowledge of agile development, including DevOps concepts Experience with data visualization tools (Tableau, PowerBI, etc.) ', 'Previous external client facing experience ', 'Proficiency in T-SQL', 'Minimum Qualifications', 'Job requires analyzing information and using logic to address work-related issues and problems', '1-3 years’ automotive operations experience, or similar business (quick-paced environment)', 'Maintain current workflows up to date and process large amounts of data as needed ', 'Performing data manipulation and statistical analysis', ' Design, code, test, correct and document programs and scripts using agreed standards and tools to achieve a well-engineered result Ensures data quality and implements tools and frameworks for automating the identification of data quality issues Profile data sources and develop ETL processes with knowledge of data modeling fundamentals, using both SQL and supporting ETL/ELT tools Assists management in creating estimates and proposals for clients  Plan effective data storage, security, sharing and publishing within the organization Create and update documentation of process flows and business rules  Maintain current workflows up to date and process large amounts of data as needed  ', 'Speed - Must be able to work quickly and efficiently without sacrificing the quality of their work', 'Solutions Oriented - Must be able to find workable solutions for any problems they might face', 'The noise level in the work environment is usually moderate', 'Office environment ', 'Strong communication skills and a working knowledge of agile development, including DevOps concepts', 'Profile data sources and develop ETL processes with knowledge of data modeling fundamentals, using both SQL and supporting ETL/ELT tools', 'While performing the duties of this job, the employee is regularly required to: Call, video, email, message and communicate with dealers and co-workers   ', 'WORK ENVIRONMENT ', 'Excellent Reasoning skills', ' Office environment  The noise level in the work environment is usually moderate ', 'Familiarity with administering MS SQL Server', 'Call, video, email, message and communicate with dealers and co-workers ', ' Knowledge Retention - In order to provide the highest quality of work, analysts must be able to learn and memorize a large amount of information Detail Oriented – Analysts must be able to focus on the details in order to identify and isolate issues Organizational Skills – Analysts must be organized and able to keep track of multiple projects at once  Flexibility - Analysts must be able to handle multiple tasks at once and follow changes in priority level Speed - Must be able to work quickly and efficiently without sacrificing the quality of their work Solutions Oriented - Must be able to find workable solutions for any problems they might face Full Professional Proficiency in English is required Able to complete advanced statistical analysis is required Job requires analyzing information and using logic to address work-related issues and problems While performing the duties of this job, the employee is regularly required to: Call, video, email, message and communicate with dealers and co-workers    ', 'Ensures data quality and implements tools and frameworks for automating the identification of data quality issues', 'Strong analytical skills with ability to understand and communicate the meaning of the data being presented ', 'Experience with data visualization tools (Tableau, PowerBI, etc.)', 'Knowledge Retention - In order to provide the highest quality of work, analysts must be able to learn and memorize a large amount of information', 'Design, code, test, correct and document programs and scripts using agreed standards and tools to achieve a well-engineered result', 'Bachelor’s Degree in computer science, statistics, mathematics or relevant discipline ', 'Summary', 'Plan effective data storage, security, sharing and publishing within the organization', 'Assists management in creating estimates and proposals for clients ', 'Essential Duties And Responsibilities', ' Call, video, email, message and communicate with dealers and co-workers  ', 'Organizational Skills – Analysts must be organized and able to keep track of multiple projects at once ', 'Excellent Mathematical/Statistical and Logic skills ', 'Experience building Azure cloud data solutions and migrating from on-prem to cloud', 'Detail Oriented – Analysts must be able to focus on the details in order to identify and isolate issues', ' Bachelor’s Degree in computer science, statistics, mathematics or relevant discipline  2+\u202fyears of data engineering and/or data warehousing experience 1-3 years’ automotive operations experience, or similar business (quick-paced environment) Experience in designing and developing ETL data pipelines (SSIS, KNIME, Alteryx). Should be proficient in writing Advanced SQLs, Expertise in performance tuning of SQLs Experience with complex data analysis Performing data manipulation and statistical analysis Strong analytical skills with ability to understand and communicate the meaning of the data being presented  Excellent Mathematical/Statistical and Logic skills  Excellent Reasoning skills Enthusiastic, highly motivated and ability to learn quick ', '2+\u202fyears of data engineering and/or data warehousing experience', 'Experience in designing and developing ETL data pipelines (SSIS, KNIME, Alteryx). Should be proficient in writing Advanced SQLs, Expertise in performance tuning of SQLs', 'Flexibility - Analysts must be able to handle multiple tasks at once and follow changes in priority level', 'Experience with complex data analysis', 'Preferred Qualifications', 'Full Professional Proficiency in English is required']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Big Data and ETL Engineer,"Software Guidance & Assistance, Inc. (SGA, Inc.)","Phoenix, AZ",2 days ago,Be among the first 25 applicants,"['', 'Required Skills', 'Experience in Commercial insurance industry is a definite plus', 'Work closely with the IT and the platform team to deliver technical solutions. This includes end to end data orchestration, data layering, data governance, metadata management and data cataloging', ""Bachelor's Degree in Computer Science, Mathematics, Engineering or Statistics. Master's Degree a definite plus"", 'RIGHT TO ', 'Engineer ', 'Collaborate with the various data source teams both internal and external(Oracle, Teradata, MongoDB, cloud etc.) on effective strategies for data ingestion Partner with the data science, actuarial science and various business group on deep rooted data analysis including propensity scores, catastrophic modeling, time-series analysis and forecasting', 'ETL', 'Experience in various services such as Hive, EMR, Juypter notebooks, Kinesis, SQS, Glue, Athena, Kafka, Redis, MongoDB etc.', 'Responsibilities', 'Design and build scalable infrastructure and platform to ingest, store and process very large amounts of data (structured, semi-structured and unstructured), including streaming and real-time into the cloud based Big Data Lake', 'Be an expert in all things data including data wrangling, aggregation, summarization and analysis', 'HIRE ', 'Prior hands-on experience delivering self-service big data and data visualization platforms', "" Bachelor's Degree in Computer Science, Mathematics, Engineering or Statistics. Master's Degree a definite plus 5-10 years of relevant Data Ingestion/ETL/ELT, Big Data and Data Visualizations experience of mission critical platforms Experience in ETL/ELT and data wrangling using tools (Informatica, Talend etc.) and languages such as Python, R, Scala, Java, SQL and SAS Experience in Hadoop and Big Data platforms including cloud based platforms such as AWS, Azure & GCP. Experience in various services such as Hive, EMR, Juypter notebooks, Kinesis, SQS, Glue, Athena, Kafka, Redis, MongoDB etc. Knowledge of Statistics, Operations Research and Machine/Deep Learning algorithms a definite plus Experience in working in fast-paced environment using scrum/agile methodology Experience in Commercial insurance industry is a definite plus Ability to work in a fast-paced environment both as an individual contributor and a technical lead Prior hands-on experience delivering self-service big data and data visualization platforms "", 'Ability to work in a fast-paced environment both as an individual contributor and a technical lead', 'Experience in working in fast-paced environment using scrum/agile methodology', 'Build best in class ETL/ELT based solutions for effective data ingestion and transformation', 'Collaborate with the various technical, platform and product development team on the use of various AWS services, including Amazon Elastic Compute Cloud (EC2), S3, Elastic Map Reduce (EMR), Glue, Athena etc.', '5-10 years of relevant Data Ingestion/ETL/ELT, Big Data and Data Visualizations experience of mission critical platforms', 'Services ', ' and Big Data ', ' Design and build scalable infrastructure and platform to ingest, store and process very large amounts of data (structured, semi-structured and unstructured), including streaming and real-time into the cloud based Big Data Lake Build best in class ETL/ELT based solutions for effective data ingestion and transformation Collaborate with the various technical, platform and product development team on the use of various AWS services, including Amazon Elastic Compute Cloud (EC2), S3, Elastic Map Reduce (EMR), Glue, Athena etc. Work closely with the IT and the platform team to deliver technical solutions. This includes end to end data orchestration, data layering, data governance, metadata management and data cataloging Be an expert in all things data including data wrangling, aggregation, summarization and analysis Collaborate with the various data source teams both internal and external(Oracle, Teradata, MongoDB, cloud etc.) on effective strategies for data ingestion Partner with the data science, actuarial science and various business group on deep rooted data analysis including propensity scores, catastrophic modeling, time-series analysis and forecasting Be an evangelist for all things Big Data and Digital ', 'Knowledge of Statistics, Operations Research and Machine/Deep Learning algorithms a definite plus', 'Experience in Hadoop and Big Data platforms including cloud based platforms such as AWS, Azure & GCP.', 'Phoenix, AZ or New York, NY', 'Experience in ETL/ELT and data wrangling using tools (Informatica, Talend etc.) and languages such as Python, R, Scala, Java, SQL and SAS', 'Job Description', 'Be an evangelist for all things Big Data and Digital', 'Financial ']",Entry level,Full-time,Engineering,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Cerner Corporation,"Kansas City, MO",3 weeks ago,48 applicants,"['', 'Qualifications', ' Must be currently residing in or willing to relocate to the Kansas City metro area ', ' 3 years of Software engineering work experience ', ' Bachelors degree in Computer Science, Computer Engineering or Information Systems or related field, or equivalent relevant work experience ', ' Must be currently residing in or willing to relocate to the Kansas City metro area  Willing to work additional or irregular hours as needed and allowed by local regulations  Work in accordance with corporate and organizational security policies and procedures, understand personal role in safeguarding corporate and client assets, and take appropriate action to prevent and report any compromises of security within scope of position', 'Expectations :', ' Bachelors degree in Computer Science, Computer Engineering or Information Systems or related field, or equivalent relevant work experience  3 years of Software engineering work experience  1 year of Big data or cloud technology work experience including data analysis, data ingestion, data modeling and/or machine learning ', ' Willing to work additional or irregular hours as needed and allowed by local regulations ', 'Basic Qualifications :', ' 1 year of Big data or cloud technology work experience including data analysis, data ingestion, data modeling and/or machine learning ', ' Work in accordance with corporate and organizational security policies and procedures, understand personal role in safeguarding corporate and client assets, and take appropriate action to prevent and report any compromises of security within scope of position']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
"Data Engineer, Data Warehouse",SoFi,"Murray, UT",3 weeks ago,Be among the first 25 applicants,"['', 'Data modelingBuild and maintain data structures and ETL/ELT data pipelinesProvision, optimize and maintain data feeds to external systemsWrite code to validate data quality and clean existing dataHelp analytics team, upstream engineering teams, as well as non-technical business users in understanding the Data WarehouseBe part of an on call support rotation to support the Data Warehouse and it’s automated processesCreating technical documentation ', 'Understanding of the software development lifecycle process', 'Experience writing SQL against several different database platforms', 'Experience using kafka', 'Strong business communication skills that can break down technical problems into business language for non-technical personnel', 'Why You’ll Love Working Here', 'Understands database architecture', 'Competitive salary packages and bonusesComprehensive medical, dental, vision and life insurance benefitsGenerous vacation and holidaysPaid parental leave for eligible employees401(k) and education on retirement planningTuition reimbursement on approved programsMonthly contribution up to $200 to help you pay off your student loansGreat health & well-being benefits including: telehealth parental support, subsidized gym programEmployer paid lunch program (except for remote employees)Fully stocked kitchen with snacks and drinks (When we’re back in the office of course)', 'Experience in docker', 'Experience creating data pipelines using Python scripting', 'Nice To Have', 'Experience in building data feeds and business reports', 'Comprehensive medical, dental, vision and life insurance benefits', 'Data modeling', 'Great health & well-being benefits including: telehealth parental support, subsidized gym program', 'Ability to work in a fast-paced environment, meet deadlines, and prioritize a workload', 'Write code to validate data quality and clean existing data', 'Creating technical documentation ', 'Responsibilities', 'What You’ll Need', 'Generous vacation and holidays', 'Working experience in the Python language with an emphasis on data', 'Proficient in writing and optimizing SQL scripts ', 'Experience using business intelligence reporting tools (Tableau, Looker, etc.)', 'Experience using cloud data technologies such as Redshift, Snowflake, or GCP', 'Description', 'Employer paid lunch program (except for remote employees)', 'Be part of an on call support rotation to support the Data Warehouse and it’s automated processes', 'Ability to bring new ideas and promote process improvement', 'Experience using AWS data technologies such as (S3, Glue, Kinesis, Lambda, etc.)', 'Paid parental leave for eligible employees', 'Fully stocked kitchen with snacks and drinks (When we’re back in the office of course)', 'Experience writing SQL against several different database platformsExperience creating data pipelines using Python scriptingExperience using cloud data technologies such as Redshift, Snowflake, or GCPExperience using AWS data technologies such as (S3, Glue, Kinesis, Lambda, etc.)Experience in dockerExperience using kafkaExperience in building data feeds and business reports', '401(k) and education on retirement planning', 'Competitive salary packages and bonuses', 'The Role', 'Provision, optimize and maintain data feeds to external systems', 'Skills and experience in finding, investigating, and resolving data quality issues', 'Position at SoFi', 'Working knowledge of some AWS data technologies', 'Build and maintain data structures and ETL/ELT data pipelines', 'Help analytics team, upstream engineering teams, as well as non-technical business users in understanding the Data Warehouse', 'Tuition reimbursement on approved programs', '3+ years working experience working with automated scripting, data modeling, and data architecture', 'Monthly contribution up to $200 to help you pay off your student loans', '3+ years working experience working with automated scripting, data modeling, and data architectureProficient in writing and optimizing SQL scripts Understands database architectureWorking experience in the Python language with an emphasis on dataWorking knowledge of some AWS data technologiesUnderstanding of the software development lifecycle processSkills and experience in finding, investigating, and resolving data quality issuesAbility to work in a fast-paced environment, meet deadlines, and prioritize a workloadAbility to bring new ideas and promote process improvementStrong business communication skills that can break down technical problems into business language for non-technical personnel']",Not Applicable,Full-time,Information Technology,Financial Services,2021-03-18 14:34:51
Lead Data Engineer,Maze,"Indiana, United States",4 days ago,Be among the first 25 applicants,"['', 'Great written and verbal english & great communication skillsExperience building data pipelineLeadership experienceExperience working for an early stage startup', 'Experience with Graph database, Neo4j and CypherExperience with AWS, Segment, Autopilot, Amplitude, Stripe, SalesforceExperience with JS environment', ""Consider this our wish-list. We know there will be great candidates that don't meet every one of these criteria—if you're passionate about the role and feel that your experience prepares you to do it, we'd love to hear from you."", 'Great written and verbal english & great communication skills', 'Laptop and accessories paid by Maze', '5x Growth:\xa0You will help us 5x MRR in 2021 by building key relationships with our partner ecosystem, launching integrations and managing ongoing partner marketing activities.', 'Experience building data pipeline', ""Category:\xa0We're on a mission to democratize user testing and are truly excited about defining a new space where anyone can test and learn rapidly."", '5x Growth:', 'Build self-serve dashboard to give access to that data to all department', 'Unlimited time off', ""We're on a mission to empower modern teams to build better user experiences by testing and learning rapidly with real users. To do this, we've partnered with the world's biggest design tools, including Adobe XD, Figma, InVision, Sketch, and Marvel. We\xa0power over 40,000 brands globally, including IBM, Logitech, Pipedrive, Uber, Greenpeace, and Braze."", 'Stand out of the pack with:', 'Benefits', 'Experience with Graph database, Neo4j and Cypher', ""Budget for training based on your and the company's needs"", ""Early-stage startup:\xa0You will join an early-stage startup with less than 50 employees. This means you'll have the unique opportunity to directly impact success and help shape the future of Maze."", ""We're a fully remote team working across 15 countries. You'll be joining a passionate team whose resumes include Canva, Figma, GitLab, InVision, Typeform and Zendesk."", 'About Maze', 'Twice a year company retreat for a week, fully paid by Maze (once COVID is over)', 'Early-stage startup:', 'Family leave: 14 weeks for birth or adoptive parents', ""Unlimited time offTwice a year company retreat for a week, fully paid by Maze (once COVID is over)Laptop and accessories paid by MazeFamily leave: 14 weeks for birth or adoptive parentsCo-working space and accounting fee paid up to a certain amount per monthBudget for training based on your and the company's needs"", 'Experience with AWS, Segment, Autopilot, Amplitude, Stripe, Salesforce', 'Audit / and improve all tracking, events, naming, key metrics, ...', 'Product-market Fit:', 'Leadership experience', ""Maze is a Series A funded startup building the future of rapid testing for modern teams. We're backed by some of the world's best funds and have an extensive advisory network."", 'Build a team to help get the above done', 'Why Maze is unique', 'Experience with JS environment', ""Product-market Fit:\xa0We already have a strong product-market fit for product designers and NPS of 60, and we're excited to build on top of this to reach PMF for our new user segments."", 'Key responsibilities:', 'WHAT WE ARE LOOKING FOR', 'Experience working for an early stage startup', 'Category:', 'WHAT YOU WILL DO', 'Support Sales & Marketing operations', 'Co-working space and accounting fee paid up to a certain amount per month', 'Build a system that gave us visibility and trust in our key metrics and user behaviors.', 'Audit / and improve all tracking, events, naming, key metrics, ...Implement event pipeline & data warehouse solutionBuild self-serve dashboard to give access to that data to all departmentSupport Sales & Marketing operationsBuild a team to help get the above done', ""Early-stage startup:\xa0You will join an early-stage startup with less than 50 employees. This means you'll have the unique opportunity to directly impact success and help shape the future of Maze.Category:\xa0We're on a mission to democratize user testing and are truly excited about defining a new space where anyone can test and learn rapidly.Product-market Fit:\xa0We already have a strong product-market fit for product designers and NPS of 60, and we're excited to build on top of this to reach PMF for our new user segments.5x Growth:\xa0You will help us 5x MRR in 2021 by building key relationships with our partner ecosystem, launching integrations and managing ongoing partner marketing activities."", 'Implement event pipeline & data warehouse solution']",Associate,Full-time,Information Technology,Design,2021-03-18 14:34:51
Data Engineer,CLEAR (clearme.com),"New York, NY",2 weeks ago,Be among the first 25 applicants,"['', 'Experience with stream-processing systems: Kafka, Storm, Spark Streaming', ' Build out our data pipeline architecture, and optimize data flow and collection for cross functional teams. Lead the development of a Data Lake solution that can be used for reporting and analytics across the entire organization. Work closely with our engineering teams to integrate data sources across a multitude of micro-services. Work with our Data Warehouse, Data Science, and Product teams to ensure that we have high quality data that meets the needs of the business. Drive data acquisition and technology improvements to help our systems evolve with our needs. ', 'Experience with queuing systems: SQS, RabbitMQ', 'Lead the development of a Data Lake solution that can be used for reporting and analytics across the entire organization.', 'You have experience with SQL Databases such as: Redshift, SQL Server, Snowflake, Big Query, Oracle, Postgres, MySQL', 'Experience with NoSQL databases such as Redis, Cassandra, CouchDB, MongoDB, Elasticsearch', '5+ years of experience with languages such as Python, Java, and Scala', 'Build out our data pipeline architecture, and optimize data flow and collection for cross functional teams.', 'Have experience developing against internal and external API’s to consume data from disparate structured and unstructured sources', 'Who You Are', ' You have 3+ years working in an AWS environment, with experience using one or more of the following: Kinesis, EMR, RDS, S3, Glue, Athena, DynamoDB Have experience developing against internal and external API’s to consume data from disparate structured and unstructured sources 5+ years of experience with languages such as Python, Java, and Scala You have experience with big data tools such as Hadoop, Spark, Hive, Hudi, Presto, Sqoop Experience with stream-processing systems: Kafka, Storm, Spark Streaming You have experience with SQL Databases such as: Redshift, SQL Server, Snowflake, Big Query, Oracle, Postgres, MySQL Experience with NoSQL databases such as Redis, Cassandra, CouchDB, MongoDB, Elasticsearch Experience with data pipeline and workflow management tools: Airflow, Luigi, Oozie, Azkaban, etc. Experience with queuing systems: SQS, RabbitMQ', 'You have 3+ years working in an AWS environment, with experience using one or more of the following: Kinesis, EMR, RDS, S3, Glue, Athena, DynamoDB', 'Drive data acquisition and technology improvements to help our systems evolve with our needs.', 'What You Will Do', 'Work closely with our engineering teams to integrate data sources across a multitude of micro-services.', 'Work with our Data Warehouse, Data Science, and Product teams to ensure that we have high quality data that meets the needs of the business.', 'You have experience with big data tools such as Hadoop, Spark, Hive, Hudi, Presto, Sqoop', 'Experience with data pipeline and workflow management tools: Airflow, Luigi, Oozie, Azkaban, etc.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Sr Data Engineer/Data Modeler,OneAmerica,"Indianapolis, IN",3 weeks ago,65 applicants,"['', 'Maintain conceptual data models of each line of business', 'Recommended Education and/or Certifications', 'Required Work Experience', '3+ years of experience in a data architect role', 'Job Summary', 'Disclaimer:\xa0OneAmerica is an equal opportunity employer and strictly prohibits unlawful discrimination based upon an individual’s race, color, religion, gender, sexual orientation, gender identity/expression, national origin/ancestry, age, mental/physical disability, medical condition, marital status, veteran status, or any other characteristic protected by law.', 'Help establish road maps for each line of business to achieve the Enterprise Data Strategy', 'Maintain a holistic view of the scope of all data projects across a line of business looking for potential project collisionsProvide project consultation Review Data Architecture overall approach at project levelLook for opportunities to provide data reuseSet Data Modeling standards and processesPerform data model audits for standards complianceMaintain conceptual data models of each line of businessUnderstand the complete path of data within the entire line of businessDevelop an in-depth understanding of the business data needs and the ability to align those business needs with the overall Enterprise data strategyPartner with the Enterprise Data Architect to influence, understand, and implement the Enterprise Data StrategyHelp establish road maps for each line of business to achieve the Enterprise Data StrategyParticipate in the Enterprise Data Governance CommitteeWork with Plan Organization to provide project estimationEvolve OneAmerica Ways for data to align to industry best practicesDirect/Develop/Execute upon continuous improvement effortsEnsure reduction of tech debt is built into projects and other efforts as appropriate to keep OneAmerica current on technological advancements as they align to architectural directionFollow and ensure compliance to OA data and security standards', 'Provide project consultation ', 'To learn more about our products, services, and the companies of OneAmerica, visit oneamerica.com/companies.', 'Job Requirements', 'Direct/Develop/Execute upon continuous improvement efforts', 'The Data Engineer Senior/Area Data Architect will provide strategic leadership and direction for data-related projects. The role will partner closely with at least one Line of Business to maintain their data model in alignment with standards and architecture. The Data Engineer Senior will review data models, evolve and implement OneAmerica Ways around data, and design and execute on continuous improvement efforts.', 'Maintain a holistic view of the scope of all data projects across a line of business looking for potential project collisions', 'Follow and ensure compliance to OA data and security standards', 'Required Education and/or Certifications', '*** This role has the possibility of being completely remote.***', 'Review Data Architecture overall approach at project level', 'Understand the complete path of data within the entire line of business', 'Look for opportunities to provide data reuse', 'Expert SQL skills and demonstrated ability to write complex SQL for purposes of analysis and translation of data needs into back end structures', 'CDMP certification, Mastery Level preferred', 'Leadership of focused technical teams', 'Bachelor’s Degree in computer science or a related discipline.', 'Partner with the Enterprise Data Architect to influence, understand, and implement the Enterprise Data Strategy', 'Set Data Modeling standards and processes', '5+ years of experience data modeling data solutions including but not limited to : ODS, Data Warehouse, Data Integration Hub, OLTP/OLAP, or equivalent knowledge', 'Ensure reduction of tech debt is built into projects and other efforts as appropriate to keep OneAmerica current on technological advancements as they align to architectural direction', 'Demonstrated experience in work estimation and management', '3+ years of experience in a data architect role5+ years of experience data modeling data solutions including but not limited to : ODS, Data Warehouse, Data Integration Hub, OLTP/OLAP, or equivalent knowledge5+ years of experience data modeling, querying, and performance tuning relational DBMS (Oracle, SQL Server, DB2)Experience with recent data technologies (NOSQL, Hadoop, RedShift, etc.) preferredExpert SQL skills and demonstrated ability to write complex SQL for purposes of analysis and translation of data needs into back end structuresDemonstrated experience in work estimation and managementLeadership of focused technical teams', 'Perform data model audits for standards compliance', 'Evolve OneAmerica Ways for data to align to industry best practices', 'Participate in the Enterprise Data Governance Committee', 'Develop an in-depth understanding of the business data needs and the ability to align those business needs with the overall Enterprise data strategy', '5+ years of experience data modeling, querying, and performance tuning relational DBMS (Oracle, SQL Server, DB2)', 'Experience with recent data technologies (NOSQL, Hadoop, RedShift, etc.) preferred', 'Work with Plan Organization to provide project estimation', 'Masters degree in Data Analytics or similar field preferredCDMP certification, Mastery Level preferred', 'Masters degree in Data Analytics or similar field preferred']",Mid-Senior level,Full-time,Information Technology,Financial Services,2021-03-18 14:34:51
Data Engineer,LatentView Analytics,"New York, United States",,N/A,"['', 'Scheduling and Monitoring of Hadoop and Spark jobs ', 'Ability to build applications using Python frontend and backend for the Django based internal tooling.Willing to learn Pyspark and ScalaDeep dives into specific customer data related issues and produces small reports to customer business\xa0performance', 'Deep dives into specific customer data related issues and produces small reports to customer business\xa0performance', 'Ability to build applications using Python frontend and backend for the Django based internal tooling.', 'Primary Responsibilities: ', '2+ years’ experience with data ingestion through batch and streaming methodologies using open source or public cloud tools like Kafka, Airflow.', 'Good understanding of Data Warehouse methodologies ', 'Willing to learn Pyspark and Scala', 'Knowledge of Hadoop M/R, Pig and Hive is a strong plus ', 'Additional Responsibilities: ', '3 to 8 years’ experience in developing Data Models, DB schemas, ETLs 2+ years’ experience with data ingestion through batch and streaming methodologies using open source or public cloud tools like Kafka, Airflow.2+ years’ experience working in one of three big public cloud ecosystems (i.e AWS, Azure or GCP)  Knowledge of Hadoop M/R, Pig and Hive is a strong plus Understanding of IN memory distributed computing frameworks like Spark (and/or DataBricks) and its parameter tuning, optimized query writing Scheduling and Monitoring of Hadoop and Spark jobs Good understanding of Data Warehouse methodologies Working knowledge of SQL Hands on experience in any of the programming languages (Shell scripting, Python, Scala, Java, etc)', '\xa0', 'Understanding of IN memory distributed computing frameworks like Spark (and/or DataBricks) and its parameter tuning, optimized query writing ', 'Working knowledge of SQL Hands on experience in any of the programming languages (Shell scripting, Python, Scala, Java, etc)', '2+ years’ experience working in one of three big public cloud ecosystems (i.e AWS, Azure or GCP)  ', '3 to 8 years’ experience in developing Data Models, DB schemas, ETLs ', 'Associate / Consultant (Data Engineering) - New York USA - Job Description']",Mid-Senior level,Full-time,Information Technology,Management Consulting,2021-03-18 14:34:51
Data Engineer,Excelion Partners,"Milwaukee, WI",5 days ago,Be among the first 25 applicants,"['', 'Bonuses based on data science and analytics practice performance.', 'Create, consult, and improve client’s data analytics and data science strategies and projects.', 'Build POCs supporting customer data needs & business strategies', 'Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.An understanding of data modeling patterns such as data vault and star schema.Strong analytic skills related to working with unstructured datasets.A successful history of manipulating, processing, and extracting value from disconnected datasets.Exposure to cloud data engineering tools.Working knowledge of message queuing and stream processingExperience supporting and working on data solutions for different business units.A desire to collaborate with data analysts and data scientists on client data solutions.', 'The position is critical to the organization’s long-term successDirect ability to make a substantial impact on organizational growth and performanceThe position has strong organizational commitment and supportYou will get exposure to new data technologies and techniques.Bonuses based on data science and analytics practice performance.', 'The position has strong organizational commitment and support', '1+ years of consulting experiencePrior experience with one or more of the following: Snowflake, Google BigQuery, Azure Synapse, or AWS Redshift.Working knowledge of PowerBI architecture and compute optimization.', 'Create, consult, and improve client’s data analytics and data science strategies and projects.Design and create cloud data architectures based on client requirements.Build POCs supporting customer data needs & business strategiesOptimize processing for data solutions ranging from dashboarding to machine learning.Create meaningful, digestible data visualizations to share findings, trends, and opportunities.Prepare and deliver reports and presentations to colleagues and client leadership based on findings.Give presentations and talks based on your experience to the data community.Develops relationships with vendors, partners, and clients to maintain a pulse on technology development and opportunities to leverage it.', 'Prepare and deliver reports and presentations to colleagues and client leadership based on findings.', 'Strong analytic skills related to working with unstructured datasets.', 'Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.', 'Create meaningful, digestible data visualizations to share findings, trends, and opportunities.', 'Exposure to cloud data engineering tools.', 'A desire to collaborate with data analysts and data scientists on client data solutions.', '1+ years of consulting experience', 'Qualifications & Experience', 'Direct ability to make a substantial impact on organizational growth and performance', 'Working knowledge of PowerBI architecture and compute optimization.', 'Preferred Experience', 'A successful history of manipulating, processing, and extracting value from disconnected datasets.', 'Experience supporting and working on data solutions for different business units.', 'Working knowledge of message queuing and stream processing', 'Give presentations and talks based on your experience to the data community.', 'Key Responsibilities', 'The position is critical to the organization’s long-term success', 'An understanding of data modeling patterns such as data vault and star schema.', 'Design and create cloud data architectures based on client requirements.', 'Optimize processing for data solutions ranging from dashboarding to machine learning.', 'Develops relationships with vendors, partners, and clients to maintain a pulse on technology development and opportunities to leverage it.', 'POSITION ATTRACTIONS', 'You will get exposure to new data technologies and techniques.', 'Prior experience with one or more of the following: Snowflake, Google BigQuery, Azure Synapse, or AWS Redshift.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
"Big Data Engineer, Job Ref #: 818297",PK,"Omaha, NE",22 hours ago,Be among the first 25 applicants,"['', 'Overview', 'Responsibilities', 'Qualifications']",Entry level,Full-time,Engineering,Information Technology and Services,2021-03-18 14:34:51
Data Engineer #106186,Contemporary Staffing Solutions,"Fort Washington, PA",2 days ago,Be among the first 25 applicants,"['', 'Experience in migrations from SQL Server and Postgresql to Snowflake.', 'Supporting the Data Warehouse strategy and Business Intelligence initiatives.', '100% Remote to Start - Onsite in Fort Washington, PA once restrictions are lifted', 'Experience in ETL and ELT workflow management (Fivetran, DBT, Matillion, Snowflake Snowpipe, Stored Procedures, and Streams a plus). ', '5+ years of experience driving adoption and automation of data management services and tools.', 'Expert knowledge in Snowflake/SQL Server/cloud-based data warehouse.', '7+ years experience as a Data Engineer.', 'Perform ad hoc data analysis to meet business unit data validation needs.', '7+ years experience as a Data Engineer.5+ years of experience driving adoption and automation of data management services and tools.3+ years of experience with API based ELT automation framework, data manager, interface design, development, and maintenance.Expert knowledge in Snowflake/SQL Server/cloud-based data warehouse.AWS cloud experience (EC2, S3, Lambda, SQS)Experience in ETL and ELT workflow management (Fivetran, DBT, Matillion, Snowflake Snowpipe, Stored Procedures, and Streams a plus). Demonstrate capabilities to provide continuous improvements and optimizations to both cost and performance on cloud-based technologies.Familiarity with AWS Data and Analytics technologies such as Glue, Athena, Spectrum, Data Pipeline.Experience with integration with 3rd party application using Python connector api Strong knowledge of Big Data concepts and working with both structured and unstructured data.Experience in migrations from SQL Server and Postgresql to Snowflake.Experience in B2C, Martech, and CDP technologies and environments.Hands on experience with SQL Server 2016, SSIS, SSAS.Integrate/export data following security guidelines.Supporting the Data Warehouse strategy and Business Intelligence initiatives.Perform ad hoc data analysis to meet business unit data validation needs.', 'Contract to Hire Opportunity', '3+ years of experience with API based ELT automation framework, data manager, interface design, development, and maintenance.', 'Data Engineer ', 'Integrate/export data following security guidelines.', 'Hands on experience with SQL Server 2016, SSIS, SSAS.', 'Experience in B2C, Martech, and CDP technologies and environments.', 'Job Description', 'Familiarity with AWS Data and Analytics technologies such as Glue, Athena, Spectrum, Data Pipeline.', 'Demonstrate capabilities to provide continuous improvements and optimizations to both cost and performance on cloud-based technologies.', 'Strong knowledge of Big Data concepts and working with both structured and unstructured data.', 'Experience with integration with 3rd party application using Python connector api ', 'AWS cloud experience (EC2, S3, Lambda, SQS)']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Chrome Hearts,"Los Angeles, CA",2 weeks ago,Be among the first 25 applicants,"['', 'Experience and mastery of SQL Analysis Services required; Azure Synapse and Azure Data Factory preferred', 'Strong understanding of the Microsoft Common Data Model and Data Flows', 'Able to work in a fast-paced, multi-site location, team-oriented environment', 'General Qualifications', 'Mastery of SQL relational databases and SQL queries', 'Responsibilities', 'Maintain day-to-day operation of data warehouse and related technologies (SQL Polybase, SQL Analysis Services, Azure Data Factory, Azure Data Lake, Synapse, Data Flow, etc.)', 'Diagnose and resolve performance issues in data systems', 'Design and Develop reports, dashboards, KPIs in partnership with Business Analysts using Power BI, Reporting Services, and other toolsProvide ongoing expert recommendations to management on data platform technologies, approaches to business application integrations to the platform, and other design/architectural requirements.Architect and design new data models and extensions to existing modelsDevelop ETL / ELT jobs in Azure Data Factory to move and integrate data from a wide range of data sources.Work closely with business analysts to refine data requirementsMaintain day-to-day operation of data warehouse and related technologies (SQL Polybase, SQL Analysis Services, Azure Data Factory, Azure Data Lake, Synapse, Data Flow, etc.)Collaborate and build reports and dashboards using Power BI and other toolsDiagnose and resolve performance issues in data systemsDocument designs and design changes', 'Collaborate and build reports and dashboards using Power BI and other tools', 'Some experience with Microsoft Dynamics 365 Finance and Operations (formerly Dynamics AX) preferred, but not mandatory', 'Excellent oral and written communication skills', 'Able to work in a fast-paced, multi-site location, team-oriented environmentAbility to communicate information to stakeholders at all levels in an effective and courteous mannerHigh level of attention to detailExcellent oral and written communication skillsStrong work ethic and a make-it-happen attitude', 'Experience using Microsoft Azure', '5 years of experience building analytical data models/data warehouse', 'Document designs and design changes', 'Technical Qualifications', '5 years of experience building analytical data models/data warehouseMastery of SQL relational databases and SQL queriesExperience and mastery of SQL Analysis Services required; Azure Synapse and Azure Data Factory preferredUnderstanding of Data Lake architecturesStrong understanding of the Microsoft Common Data Model and Data FlowsExperience with Power BI report and dashboard designSolid grasp of Data Modeling, Data Structures and AlgorithmsSome programming experience helpful in an Azure data-friendly language (C#, python, etc.)Experience using Microsoft AzureSome experience with Microsoft Dynamics 365 Finance and Operations (formerly Dynamics AX) preferred, but not mandatory', 'Strong work ethic and a make-it-happen attitude', 'Work closely with business analysts to refine data requirements', 'Understanding of Data Lake architectures', 'Ability to communicate information to stakeholders at all levels in an effective and courteous manner', 'Provide ongoing expert recommendations to management on data platform technologies, approaches to business application integrations to the platform, and other design/architectural requirements.', 'Experience with Power BI report and dashboard design', 'Develop ETL / ELT jobs in Azure Data Factory to move and integrate data from a wide range of data sources.', 'Architect and design new data models and extensions to existing models', 'Solid grasp of Data Modeling, Data Structures and Algorithms', 'High level of attention to detail', 'Design and Develop reports, dashboards, KPIs in partnership with Business Analysts using Power BI, Reporting Services, and other tools', 'Some programming experience helpful in an Azure data-friendly language (C#, python, etc.)']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Universal Tennis,"Orlando, FL",5 days ago,Be among the first 25 applicants,"['', 'Experience with data scraping, cleaning, and ETL processes preferred', 'Degree in Computer Science or related field desiredDesire and excitement for working on small teams with startup DNAAbility to work both collaboratively as part of a team and independently to dive deep and solve tough problemsPropensity to seek out, accept and integrate feedback professionallySelf-motivated, quick/continuous learner with a passion for innovationTennis player/fan is a plus', 'Experience with with asynchronous queues and/or publish/subscribe systems (ex: RabbitMQ, Azure Service Bus) preferred', 'Tennis player/fan is a plus', 'Additional Requirements', 'Experience with Python (numpy, pandas) a plus', 'Familiarity with ElasticSearch is a plus', 'Ability to work both collaboratively as part of a team and independently to dive deep and solve tough problems', 'Experience with cloud based application platforms (AWS, Azure)', '3+ years of backend/server web development', 'Experience with unit testing frameworks and understanding of unit testing fundamentals', 'Experience with Git and Pull-Requests', 'Self-motivated, quick/continuous learner with a passion for innovation', 'Requirements', 'Experience with .NET Core and C# preferred', 'Description', 'Strong knowledge of Relational Databases (SQL Server and/or MySQL)', 'Propensity to seek out, accept and integrate feedback professionally', 'Building an event and tournament management platform for clubs and organizers', 'Experience with API development and REST and JSON', 'Developing marketplace applications to search, find and connect with players, coaches, and events', 'Expanding UTR to millions of competitive and recreational players around the globeCollecting match results from hundreds of different sources with different data formatsEnabling any player or organization to post match results and track their performanceDeveloping marketplace applications to search, find and connect with players, coaches, and eventsBuilding an event and tournament management platform for clubs and organizers', 'Desire and excitement for working on small teams with startup DNA', 'Knowledgeable about sound engineering practices like continuous delivery, automated testing, (micro)services-based architecture, etc.', 'Here Are Some Of The Exciting Initiatives On Our Product Roadmap', 'Experience with Scrum, Kanban and other agile methodologies', 'Experience with Redis, cloud object storage, serverless functions, and containers a plus', 'Collecting match results from hundreds of different sources with different data formats', 'Enabling any player or organization to post match results and track their performance', 'Experience with ORM Framework such as Dapper or Entity Framework', 'Degree in Computer Science or related field desired', '3+ years of backend/server web developmentExperience with .NET Core and C# preferredExperience with API development and REST and JSONExperience with cloud based application platforms (AWS, Azure)Experience with Python (numpy, pandas) a plusExperience with data scraping, cleaning, and ETL processes preferredStrong knowledge of Relational Databases (SQL Server and/or MySQL)Experience with ORM Framework such as Dapper or Entity FrameworkExperience with with asynchronous queues and/or publish/subscribe systems (ex: RabbitMQ, Azure Service Bus) preferredKnowledgeable about sound engineering practices like continuous delivery, automated testing, (micro)services-based architecture, etc.Experience with Scrum, Kanban and other agile methodologiesExperience with unit testing frameworks and understanding of unit testing fundamentalsFamiliarity with ElasticSearch is a plusExperience with Redis, cloud object storage, serverless functions, and containers a plusExperience with Git and Pull-Requests', 'Expanding UTR to millions of competitive and recreational players around the globe']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Data Engineer (Azure),Tiger Analytics,"Dallas, TX",4 weeks ago,40 applicants,"['', 'Experience building data ingestion pipelines using Azure Data Factory to ingest structured and unstructured data ', 'Bachelor’s degree in Computer Science or similar field ', 'Experience in data wrangling, advanced analytic modeling is preferred', 'Benefits', '4+ years of experience in IT industry', 'Experience extracting/querying/joining large data sets at scale', ' Bachelor’s degree in Computer Science or similar field  4+ years of experience in IT industry Expertise in Python and Pyspark Experience building data pipelines using Azure stack 2+ years of experience using Apache spark Good working experience on Delta Lake and ETL processing Proficiency in SQL queries Prior experience of working in a Unix environment Experience in harmonizing raw data into a consumer-friendly format using Azure Databricks Experience extracting/querying/joining large data sets at scale Experience building data ingestion pipelines using Azure Data Factory to ingest structured and unstructured data  Experience in data wrangling, advanced analytic modeling is preferred Exposure to Java is a plus  Strong communication and organizational skills  ', 'Requirements', 'Description', 'Experience building data pipelines using Azure stack', 'Proficiency in SQL queries', 'Expertise in Python and Pyspark', '2+ years of experience using Apache spark', 'Good working experience on Delta Lake and ETL processing', 'Exposure to Java is a plus ', 'Strong communication and organizational skills ', 'Prior experience of working in a Unix environment', 'Experience in harmonizing raw data into a consumer-friendly format using Azure Databricks']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Two Six Technologies,"Arlington, VA",6 days ago,Be among the first 25 applicants,"['', ' Experience with modern stream-processing systems: Storm, Spark-Streaming, etc. ', ' Two Six Technologies ', ' Experience with Kubernetes, AWS, Docker, a plus', ' Bachelor’s degree in Computer Science or related field ', ' A self-driven work ethic and a willingness to jump in and solve hard problems ', ' U.S. Citizenship  Bachelor’s degree in Computer Science or related field  4+ years of relevant work experience in software development  Strong coding skills (e.g., Python, Java)  Experience with relational databases: Postgres  Experience with modern stream-processing systems: Storm, Spark-Streaming, etc.  Working knowledge of message queuing, stream processing, and highly scalable data stores  Excellent communication skills ', ' Adept at simplifying and communicating complex ideas ', ' Exposure to DevSecOps and Agile environments and processes ', ' Prior experience in cybersecurity ', ' An active DoD clearance (Secret or above)  Exposure to DevSecOps and Agile environments and processes  Proficient understanding of code versioning tools, such as Git  Experience optimizing code for performance, scalability, and stability  A self-driven work ethic and a willingness to jump in and solve hard problems  Adept at simplifying and communicating complex ideas  Prior experience in cybersecurity  Experience with Kubernetes, AWS, Docker, a plus', ' Excellent communication skills ', 'Responsibilities', 'An Ideal Candidate Will Also Have', ' Working knowledge of message queuing, stream processing, and highly scalable data stores ', ' Experience optimizing code for performance, scalability, and stability ', 'Qualifications', 'An Ideal Candidate Will Also Have An active DoD clearance (Secret or above)  Exposure to DevSecOps and Agile environments and processes  Proficient understanding of code versioning tools, such as Git  Experience optimizing code for performance, scalability, and stability  A self-driven work ethic and a willingness to jump in and solve hard problems  Adept at simplifying and communicating complex ideas  Prior experience in cybersecurity  Experience with Kubernetes, AWS, Docker, a plus', ' Minimum requirements: ', ' An active DoD clearance (Secret or above) ', ' U.S. Citizenship ', ' Proficient understanding of code versioning tools, such as Git ', 'Overview', ' Experience with relational databases: Postgres ', ' 4+ years of relevant work experience in software development ', ' Strong coding skills (e.g., Python, Java) ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Analytics Engineer,Vanguard,"Charlotte, NC",2 weeks ago,Be among the first 25 applicants,"['', 'Elevates code into the development, test, and Production environments on schedule. Provides follow up Production support when needed. Submits change control requests and documents. ', 'Writes programs and reports. ', 'Experience with ETL tools, serverless architecture is a huge plus ', 'Five years developer or systems analyst experience', 'Very Strong Java, Scala, PySpark, bash scripting, EMR. Strong analysis, development and debugging skills for the AWS platformExperience with ETL tools, serverless architecture is a huge plus Hands-on experience with Database structures, SQLMust be familiar with Security Concepts (IAM, data governance and data privacy, etc.)Familiarity with unit testing frameworks, oozie, control-M a plus ', 'Five years developer or systems analyst experienceStrong, demonstrated analysis and problem solving skillsStrong planning and organizational skillsStrong written and oral communication skillsAbility to work out of hours when needed, including weekends and eveningsAbility to lead small team on separate components or deliverables associated with projects', ""Develops understanding of client business functions and technology needs. Develops understanding of Vanguard's tools, technologies, and applications/databases, including those that interface with the business areas and other systems. "", 'Complies with Retail Systems policies and procedures including attendance and weekly time and status reporting. ', 'Develops code that reuses objects, is well structured, includes sufficient comments, and is easy to maintain. ', 'Demonstrated Knowledge Of The Following', 'Translates technical specifications, and/or design models into code for new or enhancement projects (for internal or external clients). ', 'Interfaces with cross functional teams, including associated Data & Mid Tier teams.', 'Complies with IT policies and procedures, especially those for quality and productivity standards that enable the team to meet established milestones. ', 'Understands and adheres to Vanguard’s Diversity mission and policies. ', 'Other Competencies', 'Must be familiar with Security Concepts (IAM, data governance and data privacy, etc.)', 'Strong, demonstrated analysis and problem solving skills', 'Writes the system/technical portion of assigned deliverables. Assists technical team members with the system/technical portion of their deliverables, e.g., systems testers with test plans. On small teams, the developer may write these items. ', 'About Vanguard', 'Performs unit testing and writes appropriate unit test plans to ensure requirements are satisfied. Assists in integration, systems, acceptance, and other related testing as needed. ', 'Participates in design, code, and test inspections throughout life cycle to identify issues and ensure methodology compliance. ', 'Ability to lead small team on separate components or deliverables associated with projects', 'Hands-on experience with Database structures, SQL', ""Provides intermediate level system analysis, design, development, and implementation of data analytics (ETL) solutions for AWS cloud environment.Thoroughly understands and applies Vanguard’s IT Navigator development methodology. Thoroughly understands and applies Vanguard’s Architecture standards, including all Information Security policies and procedures. Verifies all deliverables meet Methodology and Information Security requirements. Translates technical specifications, and/or design models into code for new or enhancement projects (for internal or external clients). Develops code that reuses objects, is well structured, includes sufficient comments, and is easy to maintain. Writes programs and reports. Elevates code into the development, test, and Production environments on schedule. Provides follow up Production support when needed. Submits change control requests and documents. Participates in design, code, and test inspections throughout life cycle to identify issues and ensure methodology compliance. Participates in systems analysis activities, including system requirements analysis and definition, e.g. prototyping. Participates in other meetings, such as those for use case creation and analysis.Writes the system/technical portion of assigned deliverables. Assists technical team members with the system/technical portion of their deliverables, e.g., systems testers with test plans. On small teams, the developer may write these items. Performs unit testing and writes appropriate unit test plans to ensure requirements are satisfied. Assists in integration, systems, acceptance, and other related testing as needed. Ensures developed code is optimized in order to meet Vanguard performance specifications associated with page rendering time by completing page performance tests.Develops understanding of client business functions and technology needs. Develops understanding of Vanguard's tools, technologies, and applications/databases, including those that interface with the business areas and other systems. Interfaces with cross functional teams, including associated Data & Mid Tier teams.Complies with IT policies and procedures, especially those for quality and productivity standards that enable the team to meet established milestones. Complies with Retail Systems policies and procedures including attendance and weekly time and status reporting. Understands and adheres to Vanguard’s Diversity mission and policies. Participates in special projects and performs other duties as assigned."", 'Ensures developed code is optimized in order to meet Vanguard performance specifications associated with page rendering time by completing page performance tests.', 'Inclusion Statement', 'Provides intermediate level system analysis, design, development, and implementation of data analytics (ETL) solutions for AWS cloud environment.', 'Strong written and oral communication skills', 'Very Strong Java, Scala, PySpark, bash scripting, EMR. ', 'Thoroughly understands and applies Vanguard’s IT Navigator development methodology. Thoroughly understands and applies Vanguard’s Architecture standards, including all Information Security policies and procedures. Verifies all deliverables meet Methodology and Information Security requirements. ', 'Participates in special projects and performs other duties as assigned.', 'Ability to work out of hours when needed, including weekends and evenings', 'Familiarity with unit testing frameworks, oozie, control-M a plus ', 'Participates in systems analysis activities, including system requirements analysis and definition, e.g. prototyping. Participates in other meetings, such as those for use case creation and analysis.', 'Strong planning and organizational skills', 'Strong analysis, development and debugging skills for the AWS platform']",Associate,Full-time,Information Technology,Financial Services,2021-03-18 14:34:51
"Data Engineer - SQL, Pipelines","Ursus, Inc. ","Washington, DC",3 weeks ago,Be among the first 25 applicants,"['', 'Summary: ', 'Experience with more than one coding language', 'Design, build, and launch extremely efficient and reliable data pipelines to move data across a number of platforms including Data Warehouse, online caches, and real-time systems', 'Ability to cultivate relationships and collaborate with cross-functional partners to shape, support, and execute business and product goals', ""Partner with program managers, engineers, investigators, data scientists, and cross-functional partners to understand data needs to support EDM's regulatory reporting requirements"", ""Partner with program managers, engineers, investigators, data scientists, and cross-functional partners to understand data needs to support EDM's regulatory reporting requirementsDesign, build, and launch extremely efficient and reliable data pipelines to move data across a number of platforms including Data Warehouse, online caches, and real-time systemsDeliver monthly reporting on the end-to-end management of EDM's investigations process and strategic outcomes, with a particular focus on EDM's regulatory reporting requirementsPartner with Case Managers to build data expertise and own data quality across all phases of EDM investigations and enforcementsDesign impact indicators and track progress towards goalsPerform analysis of data and trends to proactively deliver insights about operational performanceVisualize and help manage data in dynamic dashboards which support the ongoing tracking of EDM initiatives in real-timeUse your data and analytics experience to 'see what's missing', identifying and addressing gaps in existing processes"", 'Designing and implementing real-time pipelines', '5+ analytical problem solving', 'BA/BS or equivalent', 'Experience writing complex SQL queries to drive analysis and insights', '5+ years experience in quantitative role in data, reporting, and', 'Experience with Airflow or other workflow management platforms (Luigi, Prefect, Dagster, digdag.io, Google Cloud Composer, AWS Step Functions, Azure Data Factory, UC4, Control-M)', 'Ability to prioritize and handle multiple competing priorities simultaneously', 'Minimum Qualifications ', 'Experience with data analysis and visualizations tools (SQL, R, Tableau, SASS etc.)', 'Experience with data quality and validation', ""Deliver monthly reporting on the end-to-end management of EDM's investigations process and strategic outcomes, with a particular focus on EDM's regulatory reporting requirements"", '5+ years experience in quantitative role in data, reporting, and5+ analytical problem solvingExperience writing complex SQL queries to drive analysis and insightsExperience with data analysis and visualizations tools (SQL, R, Tableau, SASS etc.)Exceptional analytical skills with the ability to glean insights and share findings clearly and effectively', 'Job Title:  Data Engineer - SQL, PipelinesLocation:  Washington, DCDuration:  6 Months', ""Use your data and analytics experience to 'see what's missing', identifying and addressing gaps in existing processes"", 'Proven ability to excel in fast-paced environment with minimal guidance', 'Partner with Case Managers to build data expertise and own data quality across all phases of EDM investigations and enforcements', 'Exceptional analytical skills with the ability to glean insights and share findings clearly and effectively', 'Perform analysis of data and trends to proactively deliver insights about operational performance', 'Visualize and help manage data in dynamic dashboards which support the ongoing tracking of EDM initiatives in real-time', 'Experience with SQL performance tuning and end-to-end process optimization', 'Design impact indicators and track progress towards goals', 'BA/BS or equivalentExperience with SQL performance tuning and end-to-end process optimizationExperience with Airflow or other workflow management platforms (Luigi, Prefect, Dagster, digdag.io, Google Cloud Composer, AWS Step Functions, Azure Data Factory, UC4, Control-M)Experience with more than one coding languageDesigning and implementing real-time pipelinesExperience with data quality and validationProven ability to excel in fast-paced environment with minimal guidanceAbility to prioritize and handle multiple competing priorities simultaneouslyAbility to cultivate relationships and collaborate with cross-functional partners to shape, support, and execute business and product goals', 'Preferred Qualifications']",Entry level,Contract,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Senior Data Engineer/Data Analyst,Altman Solon,"Boston, MA",2 weeks ago,27 applicants,"['', 'The Senior Data Analyst / Data Engineer role is a mid-level position for applicants with a passion for working with large data sets and collaborating with diverse teams to solve an ever-changing set of problems. We seek specialists with strong problem-solving skills and a track record of achieving results, as well as a desire for the personal impact that can only be found within a boutique organization. Candidates should have the following qualifications:', 'Required Skills', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience using a workflow management tool such as Airflow, Luigi, Oozie, or Azkaban', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience exporting data to Excel and publishing to BI solution such as Tableau, Qlik, Looker, or Power BI for business users to view', 'To meet the demand of growth, Altman Solon is looking for an experienced Data Analyst or Data Engineer who can play several important roles:', '1.\xa0\xa0\xa0\xa0\xa0\xa0Guide other Data Analysts and Data Engineers to solve complex analytical problems', 'Altman Solon is a 300-person strategy consulting group that focuses exclusively on the telecommunications, media and technology (TMT) related sectors. As the largest TMT strategy consulting group in the world, we assist clients in fast, high-impact, confident decision making. We enable clients to seize new opportunities, improve performance, and increase shareholder value within complex and converging industries.', '5.\xa0\xa0\xa0\xa0\xa0\xa0Effectively present and communicate complex analytical or technical concepts to internal and external stakeholders', 'Please visit https://www.altmansolon.com/careers/join-us/ and select ""Apply Here"" under ""Application Process - Americas"". Please only submit your application through the Altman Solon portal. Applications via email or linkedin will not be accepted.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience using Apache Spark', 'Master’s Degree and 1-4 years’ experience in a data related field or Bachelor’s Degree and 3-6 years’ experience in a data related field.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa02+ years experience working with Python or another object-oriented language', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Attention to detail and time management delivering high quality work while meeting deadlines', 'Location:\xa0Boston, MA or New York, NY', '3.\xa0\xa0\xa0\xa0\xa0\xa0Develop quality audits to identify and report potential data quality issues and oversee data team’s work product', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0SQL mastery, including techniques for writing efficient code over large datasets', '2.\xa0\xa0\xa0\xa0\xa0\xa0Collaborate with Executives, Business Analysts, and Data Scientists to formulate hypotheses, prepare data, build data models, and visualize results to effectively connect data insights to complex client strategic questions', 'We believe that diversity, equity, and inclusion are key principles for the successful operation of any business, and especially ours. We are committed to ensuring that all employees, at all levels, feel supported, feel a sense of belonging, and are equally invested in the success of our shared work. This starts with ensuring that we draw the most talented people from all backgrounds. Altman Solon is an Equal Opportunity Employer and E-Verify user. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability, or protected veteran status.', 'Please note that we are unable to consider applicants who now or in the future require sponsorship for work visa status (e.g. F (OPT), H-1B)', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience working with GCP, AWS, or other cloud platforms', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience working with and running ETLs on traditional relational databases – MySQL, PostgreSQL, MSSQL, Oracle SQL', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience running ETLs on a data warehouse – AWS Redshift, Presto, Hive, Big Query, Teradata', 'Qualifications:', 'Experience/Education', '4.\xa0\xa0\xa0\xa0\xa0\xa0Build and maintain master databases / data warehouses for consulting teams by pulling together disparate data sources', 'Location:', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Ability to leverage critical data-driven thinking and enthusiasm for translating data into actionable insight to generate consistently accurate and useful analysis and models', 'Altman Solon delivers actionable, data-driven results to our clients. Our Analytics Innovation team is an internal analytics technology group that is focused on developing a suite of advanced analytics products and supporting business strategy consulting teams to meet the evolving needs of our clients.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Extensive experience with data preparation for statistical or machine learning models', 'The Analytics Innovation Team', 'Duties and Responsibilities', '\xa0', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa01-2 years of data project leadership experience']",Associate,Full-time,Information Technology,Management Consulting,2021-03-18 14:34:51
AWS Data Engineer,Brillio,"Santa Clara, CA",4 weeks ago,Over 200 applicants,"['', 'Brillio is the leader in global digital business transformation, applying technology with a human touch. We help businesses define internal and external transformation objectives and translate those objectives into actionable market strategies using proprietary technologies. With 2600+ experts and 13 offices worldwide, Brillio is the ideal partner for enterprises that want to quickly increase their core business productivity, and achieve a competitive edge, with the latest digital solutions', 'In-depth knowledge of AWS, especially data storage and processing technology stack like Athena, Glue, S3, EMR, Lambda, Cloud Front, Cloud Formation, Kinesis, Data Pipeline Services, AWS Batch Services, Dynamo DB, etc. Knowledge of tools like Apache Airflow. Expert in writing SQL and some experience in Python and shell scripting. Clear, professional written and verbal communication skills, ability to easily communicate complex ideas Leadership skills Project management understanding for smooth functioning. It would be exceptional if you also have this Knowledge of Big data Experience in Spark and real-time processing. Any AWS certification related to Data.\xa0', 'It would be exceptional if you also have this Knowledge of Big data Experience in Spark and real-time processing. ', 'Any AWS certification related to Data.\xa0', 'Glue, S3, EMR, Lambda, Cloud Front, Cloud Formation', 'Leadership skills Project management understanding for smooth functioning. ', 'Data Pipeline Services', 'Clear, professional written and verbal communication skills, ability to easily communicate complex ideas ', 'Expert in writing SQL and some experience in Python and shell scripting. ', 'Job Description:', 'Athena', 'In-depth knowledge of AWS, especially data storage and processing technology stack like Athena, Glue, S3, EMR, Lambda, Cloud Front, Cloud Formation, Kinesis, Data Pipeline Services, AWS Batch Services, Dynamo DB, etc. ', 'Knowledge of tools like Apache Airflow. ']",Mid-Senior level,Full-time,Engineering,Information Technology and Services,2021-03-18 14:34:51
"Data Engineer, Product Analytics",Carvana,"Phoenix, AZ",4 weeks ago,78 applicants,"['', 'You have solid understanding of indexing concepts', ' Search and browse our inventory of over 20,000 vehicles that we own and certify. Narrow down search results using highly intelligent filtering tools/components. View vehicle details, Carfax reports, and 360 rotating studio images for every vehicle. Secure financing in minutes using Carvana’s in-house service or their own bank. Interact with GUI components to easily customize loan length, down payment, and monthly payment. Generate, upload, and eSign all documents online (no ink necessary). Schedule front door delivery or pick up at one of our vending machines. Trade-in their existing vehicle or just sell it to Carvana (no purchase necessary). ', 'Utilize test and production environments, adhering to change management requirements for system implementations', 'You have an extensive background in SQL and relational databases', 'Take part in cross-functional project teams with data scientists, product engineers and analysts, among others.', 'You have a strong understanding of data integration concepts, business intelligence, data warehousing and working with large data', ' You hold (or have the equivalent experience to) a degree in Management Information Systems, Computer Science or a related field You have an extensive background in SQL and relational databases You have used Python for basic to advanced data processing tasks You have a strong understanding of data integration concepts, business intelligence, data warehousing and working with large data You have worked with and feel comfortable with MS SQL Server You know what a query plan is and how to view one You have solid understanding of indexing concepts You are excited for performance tuning and process streamlining (woo!) You actively keep up with industry best practices You’re a forward thinker; you will predict how future company evolution will affect processes implemented today You are fearless of being un-knowledgeable about a particular subject area/technology; you yearn to learn and ask questions, with a strong desire to grow through challenging work and new technologies You practice excellent written and verbal communication Your attitude rocks Bonus: ', 'Trade-in their existing vehicle or just sell it to Carvana (no purchase necessary).', '401K with company match.', 'Interact with GUI components to easily customize loan length, down payment, and monthly payment.', 'A multitude of perks including student loan payments, discounts on vehicles, benefits for your pets, and much more.', 'About Carvana', 'A great wellness program to keep you healthy and happy both physically and mentally.', 'Full-Time Position with a competitive salary.', 'You have experience working with streaming data pipelines', 'Move and shape structured and semi-structured data between various data platforms, including MS SQL Server, AWS S3, Azure DataLake, Azure Blob and Azure SQL DB', 'Use tools like SSDT, Python and Databricks to build data pipelines from our production systems into easily digestible objects for reporting', 'Generate, upload, and eSign all documents online (no ink necessary).', 'Work with Product and Analytics leaders to define business intelligence needs and translate them into functional requirements ', 'You are excited for performance tuning and process streamlining (woo!)', 'Access to opportunities to expand your skill set and share your knowledge with others across the organization.', 'What You Should Have', 'A seat in one of the fastest-growing companies in the country.', 'Medical, Dental, and Vision benefits.', 'You hold (or have the equivalent experience to) a degree in Management Information Systems, Computer Science or a related field', 'Legal stuff', 'Other duties as assigned.', 'You have experience coding in JAVA', 'What We’ll Offer In Return', 'Other Requirements', ' Use tools like SSDT, Python and Databricks to build data pipelines from our production systems into easily digestible objects for reporting Move and shape structured and semi-structured data between various data platforms, including MS SQL Server, AWS S3, Azure DataLake, Azure Blob and Azure SQL DB Become familiar with the multitude of ways data flows through our systems in order to contribute creative problem solving solutions, as well as to ensure the integrity of various data stores and ETLs Utilize test and production environments, adhering to change management requirements for system implementations Participate in team design sessions and code reviews Take part in cross-functional project teams with data scientists, product engineers and analysts, among others. Work with Product and Analytics leaders to define business intelligence needs and translate them into functional requirements  Work with end users and other IT teams to resolve operational issues and mitigate risks as applicable Help architect, design and prototype solutions to support business strategies and deliver business value Other duties as assigned. ', 'Work with end users and other IT teams to resolve operational issues and mitigate risks as applicable', 'View vehicle details, Carfax reports, and 360 rotating studio images for every vehicle.', 'You have used Python for basic to advanced data processing tasks', 'What You’ll Be Doing', 'Help architect, design and prototype solutions to support business strategies and deliver business value', 'Search and browse our inventory of over 20,000 vehicles that we own and certify.', 'A company culture of promotions from within, with a start-up atmosphere allowing for varied and rapid career development.', 'Your attitude rocks', ' You have experience coding in JAVA You have experience working with streaming data pipelines ', 'You’re a forward thinker; you will predict how future company evolution will affect processes implemented today', 'Narrow down search results using highly intelligent filtering tools/components.', 'You know what a query plan is and how to view one', 'Participate in team design sessions and code reviews', 'Bonus: ', 'You actively keep up with industry best practices', 'About The Team And Position', 'Must be able to read, write, speak and understand English.', 'Schedule front door delivery or pick up at one of our vending machines.', 'You have worked with and feel comfortable with MS SQL Server', 'You are fearless of being un-knowledgeable about a particular subject area/technology; you yearn to learn and ask questions, with a strong desire to grow through challenging work and new technologies', ' Full-Time Position with a competitive salary. Medical, Dental, and Vision benefits. 401K with company match. A multitude of perks including student loan payments, discounts on vehicles, benefits for your pets, and much more. A great wellness program to keep you healthy and happy both physically and mentally. Access to opportunities to expand your skill set and share your knowledge with others across the organization. A company culture of promotions from within, with a start-up atmosphere allowing for varied and rapid career development. A seat in one of the fastest-growing companies in the country. ', 'Become familiar with the multitude of ways data flows through our systems in order to contribute creative problem solving solutions, as well as to ensure the integrity of various data stores and ETLs', 'Additionally, You Will', ' Must be able to read, write, speak and understand English. ', 'You practice excellent written and verbal communication', 'Secure financing in minutes using Carvana’s in-house service or their own bank.']",Entry level,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Engineer,Second Front Systems,United States,1 week ago,Be among the first 25 applicants,"['', 'An active Security Clearance', 'Previous experience with:', 'WHO WE ARE LOOKING FOR', 'Building data-intensive applications using relational, NoSQL, and graph database systemsDeveloping and deploying data pipeline and workflow management tools to implement extract, transform, and load data functions ingesting a variety of 3rd party data sourcesDeveloping tests to ensure data quality and application reliabilityImplementing application telemetry and analysis to understand system performance and user behaviorsBuilding models and algorithms to generate insights from raw data\xa0', 'CI/CD tooling (Jenkins, Sonarqube, Anchore, Kubernetes)', 'Identify data needs and requirements, work with external data providers to identify solutions, and implement reliable and efficient pipelines to fulfill those requirements', 'Developing and deploying data pipeline and workflow management tools to implement extract, transform, and load data functions ingesting a variety of 3rd party data sources', 'Previous experience working in national security', 'Implementing application telemetry and analysis to understand system performance and user behaviors', 'Implement alerting and testing to ensure the accuracy of these pipelines\xa0', 'Developing tests to ensure data quality and application reliability', 'Using Git, following a branching strategy, within a CI/CD pipeline', 'QUALIFICATIONS - WHAT YOU BRING TO THE ROLE', 'Design and maintain our data structures and models as the Second Front product evolves', 'The backbone of Second Front’s acquisitions platform is data. The quality of our data is crucial for our Department of Defense customers, who need accurate, timely, and relevant information to make critical decisions about technology acquisition. Your work building data models and pipelines for the Second Front application will be critical to our users.', 'Microservices', 'An active Security ClearancePrevious experience working in national security', 'Identify data needs and requirements, work with external data providers to identify solutions, and implement reliable and efficient pipelines to fulfill those requirementsDesign and maintain our data structures and models as the Second Front product evolvesImplement alerting and testing to ensure the accuracy of these pipelines\xa0Communicate with the principal engineer, devops engineers, designers, product manager, and third-party partners to ensure effective development and delivery of the product.', 'Excellent communication, documentation and presentation skills', 'You will work with a small team of engineers, designers, and product managers to build and deliver our data-driven web application for our Department of Defense customers, spending 80-90% of your time in hands-on agile development writing code.', 'BS in Computer Science, data science, or related field, or equivalent experienceExcellent communication, documentation and presentation skillsHigh energy, resourcefulness and ability to work in a fast-paced environment with limited direction', 'Communicate with the principal engineer, devops engineers, designers, product manager, and third-party partners to ensure effective development and delivery of the product.', 'Data Engineer ', 'Second Front Systems is seeking a highly skilled and motivated Data Engineer to join our growing technical team. We are a small team working to accelerate the deployment of emerging technology into national security use-cases. We are seeking technical professionals who want to operate on the front lines of an exciting and disruptive mission.', 'BS in Computer Science, data science, or related field, or equivalent experience', 'PREFERRED QUALIFICATIONS', 'CI/CD tooling (Jenkins, Sonarqube, Anchore, Kubernetes)Using Git, following a branching strategy, within a CI/CD pipelineAWS or GCP cloud infrastructure and servicesMongoDB, SQL, Neo4j, and related toolsMicroservices', 'Building models and algorithms to generate insights from raw data\xa0', 'MongoDB, SQL, Neo4j, and related tools', 'AWS or GCP cloud infrastructure and services', 'RESPONSIBILITIES - WHAT YOU WILL BE DOING\xa0', '5+ years of practical experience:', 'Building data-intensive applications using relational, NoSQL, and graph database systems', 'High energy, resourcefulness and ability to work in a fast-paced environment with limited direction', 'U.S. citizens only']",Entry level,Full-time,Information Technology,Defense & Space,2021-03-18 14:34:51
Data Engineer,Sevan Multi-Site Solutions,United States,2 weeks ago,117 applicants,"['', 'Exemplifies and promotes our values of integrity, respect, teamwork, excellence, and charity. ', 'Experience with data visualization tools (Power BI, Tableau)', 'Experience/Education:', 'Serves as a role model and promotes professional behavior.', 'Competency with web technologies including REST/SOAP APIs, Web Services, JSON, and OData', 'Author queries and pipelines for data extraction, movement, integration, and storage', 'Follow DevOps best practices throughout the solution development lifecycle, user acceptance testing, and production release', 'Essential Duties and Responsibilities include but are not limited to the following statements.', 'Essential Duties and Responsibilities ', 'Utilize appropriate data analysis and modeling tools and languages based on the given requirements; learn new data analysis languages and tools when necessary.', 'Understand, explain, develop, and modify relational and non-relational data models; including designing, creating, and customizing along with optimizing based on system performance', 'Hi-level of competency and experience with SQL and platform specific versions (e.g.: T-SQL)Competency with web technologies including REST/SOAP APIs, Web Services, JSON, and ODataExperience with OLTP/OLAP database technologies (MS SQL Server, MySQL, or similar)Experience with data visualization tools (Power BI, Tableau)Competency with non-relational database technologies (MongoDB, CosmosDB)Excellent communication skills Demonstrate professionalism, adaptability, and self-motivationProactive approach and capacity to work independentlyAbility taking technical information and adapting it for various audiences', 'Work directly with end users, analysts, and project managers to understand business requirements and develop technical software requirements alongside other engineers', 'Qualifications: To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The requirements listed below are representative of the knowledge, skill, and/or ability required.', 'Summary: The Data Engineer is an organized, self-motivated, and detail-oriented individual with the ability to prioritize and meet deadlines to multiple stakeholders and sponsors. DEs are responsible for developing data-driven reporting and solutions that meet requirements in an agile and efficient manner.\xa0Additionally, DEs support the deployment, training, and maintenance of solutions, reports, and integrations when needed.', 'Troubleshoot, diagnose, upgrade, and improve the performance of reporting and data solutions', 'Design, create and maintain solutions, extensions, and integrations for applications', 'Proactive approach and capacity to work independently', 'Ability taking technical information and adapting it for various audiences', 'Culture, Leadership and Employee Development', 'Communicates our vision and purpose through Service, Talent, and Choices. ', 'Analyze and understand business requirements and translate into logical data models', 'Additional duties and projects as assigned', 'Excellent communication skills ', 'Participates in personal career development through on-the-job training, attends training programs and assists in the development of interns / co-ops. ', 'Hi-level of competency and experience with SQL and platform specific versions (e.g.: T-SQL)', 'Azure Data Factory, SSIS, Hadoop, Azure Databricks', 'Power BI and Paginated Reports (SSRS)', 'Summary:', 'Analyze and understand business requirements and translate into logical data modelsAuthor queries and pipelines for data extraction, movement, integration, and storageDesign, create and maintain solutions, extensions, and integrations for applicationsUnderstand, explain, develop, and modify relational and non-relational data models; including designing, creating, and customizing along with optimizing based on system performanceDesign and build reporting solutions and dashboards from myriad disparate data sourcesTroubleshoot, diagnose, upgrade, and improve the performance of reporting and data solutionsWork directly with end users, analysts, and project managers to understand business requirements and develop technical software requirements alongside other engineersFollow DevOps best practices throughout the solution development lifecycle, user acceptance testing, and production releaseUtilize appropriate data analysis and modeling tools and languages based on the given requirements; learn new data analysis languages and tools when necessary.Work with technical team to prioritize technical needs of solutions and take appropriate action to fulfillAdditional duties and projects as assigned', 'Embrace key Sevan-wide initiatives, like Safety, Sustainability, and core Values.', 'Design and build reporting solutions and dashboards from myriad disparate data sources', 'Embrace key Sevan-wide initiatives, like Safety and Sustainability. ', 'Work with technical team to prioritize technical needs of solutions and take appropriate action to fulfill', 'Qualifications: ', 'Preferred Skills:', 'Exemplifies and promotes our values of integrity, respect, teamwork, excellence, and charity. Embrace key Sevan-wide initiatives, like Safety and Sustainability. Communicates our vision and purpose through Service, Talent, and Choices. Serves as a role model and promotes professional behavior.Participates in personal career development through on-the-job training, attends training programs and assists in the development of interns / co-ops. ', 'MS SQL Server, Azure Cosmos DB, Azure Logic Apps/FlowAzure Data Factory, SSIS, Hadoop, Azure DatabricksAzure SQL and Azure Cosmos DB Power BI and Paginated Reports (SSRS)', 'Competency with non-relational database technologies (MongoDB, CosmosDB)', 'Azure SQL and Azure Cosmos DB ', 'Sevan Multi-Site Solutions, Inc. is proud to be an equal opportunity employer committed to a diverse and inclusive work environment. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity or expression, national origin, age, marital status, genetics, disability, pregnancy, veteran status or any other basis protected by law.', ""Minimum bachelor's degree in Business Analytics, Data Science, Computer Science, Information Systems, or related areas/experience with 3-6 years of relevant experience in data modelling, ETL processing, data analysis and integration, and data visualization."", 'MS SQL Server, Azure Cosmos DB, Azure Logic Apps/Flow', 'Experience with OLTP/OLAP database technologies (MS SQL Server, MySQL, or similar)', 'Demonstrate professionalism, adaptability, and self-motivation']",Associate,Full-time,Information Technology,Construction,2021-03-18 14:34:51
Data Engineer,Omaze,United States,,N/A,"['', 'Working with engineers and data users to maintain and implement best practices for protecting PII data in transit and at rest.', 'Data Build Tool (DBT)', 'Work with the organization to ensure data is protected, secure.', 'Work with Legal, IT and Tech to establish “Information Security Policy and Standards” documentation.', 'Investigate potential security events and fraud escalations.', 'Experienced at applying strict regulatory, compliance, and security requirements to software systems.', 'Comfortable operating in environments subject to regulatory, compliance, and risk-based security requirements.', 'Who We Are:\xa0', 'Our Ideal Candidate:', 'Develop and document standards and best practices.', 'Public Key Encryption', 'AWS - SSE-S3, SSE-KMS, SSE-C', ""Omaze raises funds and awareness for charities by offering the chance to win once-in-a-lifetime experiences and dream-come-true prizes. We've offered everything from a double date with John Krasinski and Emily Blunt, to a walk-on role in Star Wars, to a brand new customized Mercedes Benz sprinter van. We've given over $130M to charities around the world, from donors in 180 countries. Our vision is to be the first for-profit company to give $1B to charity in a single year, and we’re building our team of dedicated and passionate people to help us get there. That’s where you come in!"", 'Experience with 1 or more of the following languages:  Python, R, Go or Java', 'Work with the data team to deliver quality data solutions to business users and stakeholders.', 'What You’ll Do', 'Establish standards and practices to achieve and maintain compliance with regulations and standards such as ISO27000, GDPR, CCPA, SOX and PCI-DSS.Work with Legal, IT and Tech to establish “Information Security Policy and Standards” documentation.Work with the SRE team and IT to ensure data standards are up to date, tracked and adhered to by associates and external partners.Identify threats and risk associated to system integrations.Work with the organization to ensure data is protected, secure.Ensure audit trails are established for access and usage.Identify and minimize risk to customer Personally Identifiable Information (PII).Working with engineers and data users to maintain and implement best practices for protecting PII data in transit and at rest.Ensure security of enterprise applications meet our Information Security Policy and Standards.Investigate potential security events and fraud escalations.Work with engineers to establish methods to detect, investigate, qualify and quantify threats with data.Develop algorithms and reports to detect anomalies and provide threat assessmentsWrite scripts and programs to mine data.Develop and document standards and best practices.Work with the data team to deliver quality data solutions to business users and stakeholders.', 'Ability to work effectively in teams of technical and non-technical individuals (product, marketing, subject matter experts, etc).', 'Ability to learn new languages and tools quickly', 'You are passionate about voicing your opinions and are able to be humble in receiving feedback.', 'Familiar with NoSQL solutions, such as Redis, ElasticSearch, or DynamoDB.', '5+ years data engineering experienceExperience in creating guidelines and policies in conjunction with legal and business stakeholdersSQL - AdvancedSnowflake Cloud Data WarehouseData Build Tool (DBT)Experience with 1 or more of the following languages:  Python, R, Go or JavaData Modeling, Data EncryptionIn Transit - SSL/TLSAWS - SSE-S3, SSE-KMS, SSE-CPublic Key EncryptionAbility to learn new languages and tools quicklyFamiliar with event-driven and streaming architectures, using tools like Kafka, Kinesis, or SNS.Experience with Amazon Web Services (AWS), RDS, Logging and Monitoring, IAMComfortable operating in environments subject to regulatory, compliance, and risk-based security requirements.Familiar with NoSQL solutions, such as Redis, ElasticSearch, or DynamoDB.Experienced at applying strict regulatory, compliance, and security requirements to software systems.Ability to work effectively in teams of technical and non-technical individuals (product, marketing, subject matter experts, etc).Engage engineers across our organization to support a culture of collaboration and inclusion through mentorship, respectful code review, and dedication to quality.You are excited to share and learn from your teammates about crafting amazing user experiences.You are passionate about voicing your opinions and are able to be humble in receiving feedback.You have a track record of trying and learning new things and are not afraid to learn through failures.', 'You are able to collaborate with both technical and non technical audiences. You believe everyone has something to contribute, value diversity and inclusion, and enjoy mentoring others to help them grow. You believe the quality of service we render has a direct impact on ourselves, surroundings and the world.', 'Develop algorithms and reports to detect anomalies and provide threat assessments', 'Experience in creating guidelines and policies in conjunction with legal and business stakeholders', 'Write scripts and programs to mine data.', 'You are excited to share and learn from your teammates about crafting amazing user experiences.', '5+ years data engineering experience', ""Who We're Seeking:"", 'Ensure audit trails are established for access and usage.', 'SQL - Advanced', 'The Data Engineering team at Omaze is responsible for creating information pipelines and data governance. As a Data Engineer - Security and Compliance, you have a broad knowledge of information system architectures. You have deep knowledge and experience with system integration, communication protocols and security controls. You have a working knowledge of cyber security best practices and compliance standards such as CCPA, GDPR, SOX and PCI-DSS. Your past experience communicating and collaborating with teams such as Legal, IT, Tech and Operations will be instrumental in the development of a comprehensive data security strategy and data protection framework. You have demonstrated the ability to secure databases, information pipelines, endpoints and applications. You are hands on and can write scripts to solve challenges, identify risk and research anomalies.', 'In Transit - SSL/TLS', 'Experience with Amazon Web Services (AWS), RDS, Logging and Monitoring, IAM', 'Data Modeling, Data Encryption', 'Work with the SRE team and IT to ensure data standards are up to date, tracked and adhered to by associates and external partners.', 'Identify and minimize risk to customer Personally Identifiable Information (PII).', 'Establish standards and practices to achieve and maintain compliance with regulations and standards such as ISO27000, GDPR, CCPA, SOX and PCI-DSS.', 'Identify threats and risk associated to system integrations.', 'You have a track record of trying and learning new things and are not afraid to learn through failures.', 'Ensure security of enterprise applications meet our Information Security Policy and Standards.', 'Familiar with event-driven and streaming architectures, using tools like Kafka, Kinesis, or SNS.', 'Engage engineers across our organization to support a culture of collaboration and inclusion through mentorship, respectful code review, and dedication to quality.', 'Snowflake Cloud Data Warehouse', 'Work with engineers to establish methods to detect, investigate, qualify and quantify threats with data.']",Mid-Senior level,Full-time,Information Technology,Fund-Raising,2021-03-18 14:34:51
Data Engineer,Centene Corporation,"Charlotte, NC",3 weeks ago,Be among the first 25 applicants,"['', 'Contribute to the development and maintenance of real-time processing applicationsContribute to the creation and maintenance of optimal data pipeline architecturesConduct maintenance and support for core infrastructure health, system upgrades, monitoring, CI/CD and loggingResearch streaming best practices and proper stream architectureCollaborate with team members to better understand existing data requirements and validation rulesAnalyze trends in data sets and contribute to the development of algorithms in order to improve upon the usefulness of raw dataThis position will provide the Health Plan Systems with a platform for real-time stream processing by performing application and production support to help ensure the availability of streaming services and the continuous flow of data.', 'Conduct maintenance and support for core infrastructure health, system upgrades, monitoring, CI/CD and logging', 'Analyze trends in data sets and contribute to the development of algorithms in order to improve upon the usefulness of raw dataThis position will provide the Health Plan Systems with a platform for real-time stream processing by performing application and production support to help ensure the availability of streaming services and the continuous flow of data.', 'Contribute to the creation and maintenance of optimal data pipeline architectures', 'Research streaming best practices and proper stream architecture', 'Education/Experience', 'Collaborate with team members to better understand existing data requirements and validation rules', 'Centene is an equal opportunity employer that is committed to diversity, and values the ways in which we are different. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, veteran status, or other characteristic protected by applicable law.', 'Position Purpose', 'Contribute to the development and maintenance of real-time processing applications']",Not Applicable,Full-time,Information Technology,Hospital & Health Care,2021-03-18 14:34:51
Data Engineer,Herman Miller,"Zeeland, MI",2 weeks ago,38 applicants,"['', 'Employee Status', 'Our Values', 'Your Day-to-day Work Will Involve', 'Travel', ' Ability in managing and communicating date warehouse plans to internal clients. ', 'Schedule', ' Architecting and developing in AWS Services like AWS Glue, Lambda, Step Functions, S3, EMR, Cloudwatch, CloudTrail, Lake Formation, Cloudformation, etc. ', ' Experience working with AWS cloud services and Redshift data warehouse. ', 'What You Bring', 'Primary Location', 'Shift', ' Performing data analysis required to troubleshoot data related issues and assist in the resolution of data issues. ', ' A Bachelor’s degree in Computer Science or a related technical field. ', ' Implementing processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it. ', 'Who We Hire', ' Designing data integrations and data quality framework. ', '  Cleaning, normalizing, and aggregating IoT sensor telemetry data for Herman Miller Live Platform reporting and analytics.   Providing support to functional areas by acting as an expert in tools and methods for accessing, analyzing, and reporting corporate data.   Collaborating with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.   Designing data integrations and data quality framework.   Developing and maintains scalable data pipelines and builds out new API integrations to support continuing increases in data volume and complexity.   Implementing processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it.   Performing data analysis required to troubleshoot data related issues and assist in the resolution of data issues.  ', ' Excellent oral and written communication skills with a keen of customer service. ', ' Collaborating with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization. ', 'Inside the Job', 'Why Join Us?', ' Providing support to functional areas by acting as an expert in tools and methods for accessing, analyzing, and reporting corporate data. ', ' Developing and maintains scalable data pipelines and builds out new API integrations to support continuing increases in data volume and complexity. ', 'Work Schedule', '  A Bachelor’s degree in Computer Science or a related technical field.   Fully proficient data engineering abilities typically gained through 5+ years of working with Python/Java, SQL, and working schema design and dimensional data modeling.   Experience designing, building and maintaining data processing systems.   Experience working with AWS cloud services and Redshift data warehouse.   Architecting and developing in AWS Services like AWS Glue, Lambda, Step Functions, S3, EMR, Cloudwatch, CloudTrail, Lake Formation, Cloudformation, etc.   Ability in managing and communicating date warehouse plans to internal clients.   Implementing and maintaining large date warehouse.   Experience in implementing ETL using one of the following languages: Python, Scala, or Java   Excellent oral and written communication skills with a keen of customer service.  ', ' Implementing and maintaining large date warehouse. ', ' Fully proficient data engineering abilities typically gained through 5+ years of working with Python/Java, SQL, and working schema design and dimensional data modeling. ', ' Experience designing, building and maintaining data processing systems. ', ' Experience in implementing ETL using one of the following languages: Python, Scala, or Java ', ' Cleaning, normalizing, and aggregating IoT sensor telemetry data for Herman Miller Live Platform reporting and analytics. ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
"Data Engineer – Fairfax, VA",Vibrent Health,"Fairfax, VA",7 days ago,Be among the first 25 applicants,"['', 'Collaborate with the Data Analysts, Application Developers, Report Developers, Architects, Product Manager and other stakeholders to provide visibility into the data layer and related pipelines', 'Experience with Test Driven Development and continuous integration', 'Bachelor’s Degree with 4+ years of experience or equivalent', 'Developing working PoCs along with architects.', 'Experience with using AWS based solutions (e.g EMR, Glue) for data management', 'Design, code, test, debug, document, and implement changes to new and existing data pipelines', 'Responsibilities', 'Bachelor’s Degree with 4+ years of experience or equivalentExtensive, advanced knowledge of Python 3, JavaExperience with using AWS based solutions (e.g EMR, Glue) for data managementExperience with Kubernetes and Kubernetes based tools like Helm.Experience deploying data lake solutions like Hadoop, Hive, Spark, DatabricksExperience with Test Driven Development and continuous integrationExpertise with SQLProven track record of working independently to deliver high quality codeStrong analytical, verbal and written communications skillsMust be a critical thinker and self-starterAbility to work in fast-paced nature of a high-growth organization', 'Proven track record of working independently to deliver high quality code', 'Ability to work in fast-paced nature of a high-growth organization', 'Must be a critical thinker and self-starter', 'Managing production releases Understand and track operating costs of production code and be able to take corrective action', 'Manage your own time and work well both independently and as part of a team', 'Design, code, test, debug, document, and implement changes to new and existing data pipelinesEnsure code is high quality through automated tests, production monitoring, alerting and fixing the issues quickly.Ensure code is self tested through a CI pipeline, can be deployed using Kubernetes and can be managed by the development team (through logs and alerts)Managing production releases Understand and track operating costs of production code and be able to take corrective actionHandle critical production issues.Guiding, assisting and training remote developers and contractors on the project.Evaluating cloud based and open source solutions for complexity and costsDeveloping working PoCs along with architects.Collaborate with the Data Analysts, Application Developers, Report Developers, Architects, Product Manager and other stakeholders to provide visibility into the data layer and related pipelinesIdentify opportunities to leverage data and create innovative solutionsKeep up-to-date with latest technology trendsWork as a team member in a highly creative and collaborative environmentManage your own time and work well both independently and as part of a team', 'Qualifications', 'Description', 'Handle critical production issues.', 'Ensure code is self tested through a CI pipeline, can be deployed using Kubernetes and can be managed by the development team (through logs and alerts)', 'Experience deploying data lake solutions like Hadoop, Hive, Spark, Databricks', 'Ensure code is high quality through automated tests, production monitoring, alerting and fixing the issues quickly.', 'Extensive, advanced knowledge of Python 3, Java', 'Experience with Kubernetes and Kubernetes based tools like Helm.', 'Identify opportunities to leverage data and create innovative solutions', 'Evaluating cloud based and open source solutions for complexity and costs', 'Keep up-to-date with latest technology trends', 'Guiding, assisting and training remote developers and contractors on the project.', 'Expertise with SQL', 'Work as a team member in a highly creative and collaborative environment', 'Strong analytical, verbal and written communications skills']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Atlantic Group,Greater Syracuse-Auburn Area,3 weeks ago,Be among the first 25 applicants,"['Distribution of cloud architecture to back new distributed computing solutions that often span the full array of cloud services. This will consist of migration of active applications and enhancement of new applications using cloud services.', 'Own the Insights gathered by the formation of advanced technology roadmaps. Share real world implementations and propose new resources that would streamline adoption and drive greater value.', 'Master’s or PhD in Computer Science, Physics, Engineering or Math, with high certifications are highly preferred.', 'Hands on experience leading enterprise-wide data engineering, warehousing and analytics projects', 'Hands on experience with Apache', 'Will consider a bachelor’s degree', 'Responsibilities', ' Strategize and carry out intricate data analyses utilizing SQL and other tools Distribution of cloud architecture to back new distributed computing solutions that often span the full array of cloud services. This will consist of migration of active applications and enhancement of new applications using cloud services. Own the Insights gathered by the formation of advanced technology roadmaps. Share real world implementations and propose new resources that would streamline adoption and drive greater value. Innovate and engage with key technology stakeholders to create a compelling vision of a data-driven enterprise environment and the impact it will have on their teams, their projects and their outcomes. ', 'Extensive, demonstrated expertise with Python', 'Demonstrated industry efficiency in the fields of database, data warehousing or data sciences', 'Demonstrated experience with distributed computing', 'Requirements', 'Strategize and carry out intricate data analyses utilizing SQL and other tools', 'Understanding of database and analytical technologies in the industry including MPP and NoSQL databases, Data Warehouse design, PostgreSQL, BI reporting and Dashboard development', 'Note: Qualified candidates will be contacted within 2 business days of application. If an applicant does not meet the above criteria, we will keep your resume on file for future opportunities and may contact you for further discussions.', 'Innovate and engage with key technology stakeholders to create a compelling vision of a data-driven enterprise environment and the impact it will have on their teams, their projects and their outcomes.', 'Experience with R helpful', 'Implementing AWS services in a variety of distributed computing, enterprise environments', 'Expertise with GCP resources including BigQuery and GCS', 'Expertise with AWS services such as Athena, Glue, Lambda, S3, DynamoDB, NoSQL, Relational Database Service (RDS), Amazon EMR and Amazon Redshift.', ' Master’s or PhD in Computer Science, Physics, Engineering or Math, with high certifications are highly preferred. Will consider a bachelor’s degree Hands on experience leading enterprise-wide data engineering, warehousing and analytics projects Understanding of database and analytical technologies in the industry including MPP and NoSQL databases, Data Warehouse design, PostgreSQL, BI reporting and Dashboard development Extensive, demonstrated expertise with Python Hands on experience with Apache Demonstrated experience with distributed computing Expertise with AWS services such as Athena, Glue, Lambda, S3, DynamoDB, NoSQL, Relational Database Service (RDS), Amazon EMR and Amazon Redshift. Experience with R helpful Expertise with GCP resources including BigQuery and GCS Demonstrated industry efficiency in the fields of database, data warehousing or data sciences Implementing AWS services in a variety of distributed computing, enterprise environments ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,DigitalOcean,"New York, NY",4 weeks ago,192 applicants,"['', 'History of proactively identifying forward-looking data engineering strategies, utilizing appropriate technologies, and implementing at scale', 'We value development. You will work with some of the smartest and most interesting people in the industry. We are a high-performance organization that is always challenging ourselves to continuously grow. We maintain a growth mindset in everything we do and invest deeply in employee development through formalized mentorship, LinkedIn Learning tracks, and other internal programs. We also provide all employees with reimbursement for relevant conferences, training, and education.', 'Experience in custom ETL design, implementation and maintenance', 'Track record of developing in complex data environments and intelligence platforms for business users', 'Pioneer initiatives around data quality, integrity, security and governance', 'We care about your physical, financial and mental well-being. We offer competitive health, dental, and vision benefits for employees and their dependents, a monthly gym reimbursement to support your physical health, and a commute or internet allowance to make your trips to your office or your desk easier. We offer generous parental leave with transition time built-in upon return to work. We offer competitive compensation and a 401k plan with up to a 4% employer match. ', 'Experience implementing dimensional modeling in a configuration tool like dbt or LookML a plus', 'Experience interacting with key stakeholders in different fields, interpreting challenges and opportunities into actionable engineering strategies', 'Experience with ephemeral/streaming metrics services (Prometheus, DataDog, etc) and with SLO/SLI driven analytics a big plus.', 'We value development', ' We value development. You will work with some of the smartest and most interesting people in the industry. We are a high-performance organization that is always challenging ourselves to continuously grow. We maintain a growth mindset in everything we do and invest deeply in employee development through formalized mentorship, LinkedIn Learning tracks, and other internal programs. We also provide all employees with reimbursement for relevant conferences, training, and education. We care about your physical, financial and mental well-being. We offer competitive health, dental, and vision benefits for employees and their dependents, a monthly gym reimbursement to support your physical health, and a commute or internet allowance to make your trips to your office or your desk easier. We offer generous parental leave with transition time built-in upon return to work. We offer competitive compensation and a 401k plan with up to a 4% employer match.  We support our remote employee experience. While we have great office spaces in NYC, Cambridge and Palo Alto, we’re very distributed—we use a number of communication tools to connect across the company—and all remote employees have the opportunity to visit our offices and meet their teams face-to-face at team offsites. We also have an annual company offsite, Shark Week, to get quality in-person time with the entire company at least once a year. We also allow employees to outfit their workstations to meet their needs—whether remote or in office. We value diversity and inclusivity. We are an equal opportunity employer and we do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. ', ' Bachelor’s degree in Computer Science, Math, Statistics, Economics, or other quantitative field; or equivalent experience.  Experience in custom ETL design, implementation and maintenance Track record of developing in complex data environments and intelligence platforms for business users Demonstrable ability to relate high-level business requirements to technical ETL and data infrastructure needs, including underlying data models and scripts History of proactively identifying forward-looking data engineering strategies, utilizing appropriate technologies, and implementing at scale Extensive hands-on experience with schema design and dimensional data modeling Experience interacting with key stakeholders in different fields, interpreting challenges and opportunities into actionable engineering strategies Experience with analytics databases like Snowflake, Redshift, or BigQuery.  Advanced SQL and relational database knowledge (MySQL, PostgreSQL) in addition to warehousing and dimension modeling Experience scripting in Go or Python or a similar scripting language. Effective communication and interpersonal skills Experience implementing dimensional modeling in a configuration tool like dbt or LookML a plus Experience designing and building dashboards in BI tools like Looker, Tableau, or PowerBI a plus. Experience with job schedulers (Airflow, Luigi, Azkaban, etc.) a plus Experience with ephemeral/streaming metrics services (Prometheus, DataDog, etc) and with SLO/SLI driven analytics a big plus. ', 'Have you ever wondered what happens inside the cloud?', 'We support our remote employee experience. While we have great office spaces in NYC, Cambridge and Palo Alto, we’re very distributed—we use a number of communication tools to connect across the company—and all remote employees have the opportunity to visit our offices and meet their teams face-to-face at team offsites. We also have an annual company offsite, Shark Week, to get quality in-person time with the entire company at least once a year. We also allow employees to outfit their workstations to meet their needs—whether remote or in office.', 'Experience with analytics databases like Snowflake, Redshift, or BigQuery. ', 'Advanced SQL and relational database knowledge (MySQL, PostgreSQL) in addition to warehousing and dimension modeling', ""What You'll Be Doing"", 'Effective communication and interpersonal skills', ' Develop and implement metrics and dimensions for powering analytical use cases across the company, incorporating a wide variety of data sources across the company at varying levels of complexity and scale Focus on data quality of the data environment and data products being delivered to the business, and effectively communicate to internal user base regarding production status Interface closely with data infrastructure, engineering and technical operations teams to ensure correctness and soundness of metrics built in the data environment and availability of data product services Pioneer initiatives around data quality, integrity, security and governance Work closely with data stakeholders across the company, both technical and non-technical, to understand evolving needs as more complex data models are introduced for reporting and data science ', 'We want people who are passionate about creating experiences that our customers will love.', 'Why You’ll Like Working For DigitalOcean', 'Develop and implement metrics and dimensions for powering analytical use cases across the company, incorporating a wide variety of data sources across the company at varying levels of complexity and scale', 'Focus on data quality of the data environment and data products being delivered to the business, and effectively communicate to internal user base regarding production status', 'Bachelor’s degree in Computer Science, Math, Statistics, Economics, or other quantitative field; or equivalent experience. ', 'Demonstrable ability to relate high-level business requirements to technical ETL and data infrastructure needs, including underlying data models and scripts', 'Extensive hands-on experience with schema design and dimensional data modeling', 'We care about your physical, financial and mental well-being', ""What We'll Expect From You"", 'Work closely with data stakeholders across the company, both technical and non-technical, to understand evolving needs as more complex data models are introduced for reporting and data science', 'We value diversity and inclusivity. We are an equal opportunity employer and we do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.', 'Interface closely with data infrastructure, engineering and technical operations teams to ensure correctness and soundness of metrics built in the data environment and availability of data product services', 'Experience with job schedulers (Airflow, Luigi, Azkaban, etc.) a plus', 'We value diversity and inclusivity', 'Experience designing and building dashboards in BI tools like Looker, Tableau, or PowerBI a plus.', 'We support our remote employee experience', 'Experience scripting in Go or Python or a similar scripting language.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Lead Data Engineer,All Turtles,"New York, NY",1 week ago,29 applicants,"['', 'Flexibility and comfort working in a dynamic, fast-growing environment with minimal oversight, documentation and process.', 'Working with business and product leaders to understand the questions they need to be able to answer.', ""You'll be responsible for:"", 'Designing, implementing and maintaining the analytics infrastructure that supports our product & business decision making.', 'Experience building batch data pipelines and data warehouses on cloud computing platforms such as AWS or GCP, working with MPP databases such as BigQuery, and using workflow management tools such as Apache Airflow.', '5+ years of data engineering experience.', 'Expert level skills in Python and SQL. Experience with Jupyter Notebook or similar tools is a plus.', 'Working with product development teams to instrument their applications to gather the required data.', ' Working with business and product leaders to understand the questions they need to be able to answer. Designing, implementing and maintaining the analytics infrastructure that supports our product & business decision making. Working with product development teams to instrument their applications to gather the required data. Creating dashboards and visualizations. ', ' 5+ years of data engineering experience. Expert level skills in Python and SQL. Experience with Jupyter Notebook or similar tools is a plus. Extensive experience with data warehousing and data visualization tools. Experience building batch data pipelines and data warehouses on cloud computing platforms such as AWS or GCP, working with MPP databases such as BigQuery, and using workflow management tools such as Apache Airflow. A solid foundation in statistics. Excellent communications skills. Flexibility and comfort working in a dynamic, fast-growing environment with minimal oversight, documentation and process. ', 'Extensive experience with data warehousing and data visualization tools.', 'Creating dashboards and visualizations.', ""Ideally, you'll have:"", 'Excellent communications skills.', 'A solid foundation in statistics.']",Associate,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Data Analytics Engineer,"KCF Technologies, Inc.","Pittsburgh, PA",4 days ago,Be among the first 25 applicants,"['', ""We are looking for an individual who not only has an aptitude for data analytics, statistical analysis, and pattern recognition, but also has a passion for learning and understanding how things work. You consider yourself to be Independently motivated and an analytically deductive problem solver, with a factual and to-the-point communication style. You're a case matter expert on things that draw your interest, in addition to being extremely disciplined with perfectionist tendencies."", 'Produce models for edge and/or cloud (AWS) execution', 'Design and implement algorithms tailored to industrial machine health diagnostics, prognostics, and root-cause analysisProduce models for edge and/or cloud (AWS) executionIntegrate analytics solutions within the KCF SMARTdiagnostics platformConstruct data mining tools for acquiring training and validation data sets', 'KCF Technologies is an exciting and rapidly growing technology company dedicated to putting innovative solutions to work in the Industrial World. SmartDiagnostics, our IIOT Technology, supports our mission to Transform American Industry by helping our customers eliminate downtime, eradicate waste, increase safety, and elevate manufacturing jobs across the United States. To accomplish this epic technical revolution, we integrate our core values into our everyday actions. Learn more at www.kcftech.com ', 'At KCF, we are an equal opportunity employer. The only things we require for employment, compensation, advancement and benefits are performance and a good team attitude. No one will be denied opportunities or benefits, and no employment decisions will be made, on the basis of race, religion/creed, national origin, ancestry, sex, sexual orientation, gender, gender identity, age, disability that does not prohibit performance of essential job functions, protected veteran status, medical condition, marital status, pregnancy, genetic information, possession of a general education development certificate (GED) as compared to a high school diploma, or any other characteristic protected by applicable federal or state laws. KCF complies with applicable state and local laws governing nondiscrimination in employment in every location in which KCF has facilities.', '*Position has ability to be fully remote', 'Able to work in a rapid-paced environment, managing and tracking multiple tasks with speed and accuracy', 'Four weeks Paid Time Off, in addition to paid Holidays', 'KCF Technologies is looking for a talented and motivated Data Analytics Engineer to help develop computer algorithms and models trained to ingest sensor and machine process data and diagnose fault conditions, prognose time-to-failure, provide recommended actions, and perform root-cause analysis.', 'We know it takes competitive benefits and development opportunities to fuel a team that exhibits our values of Smarts, Grit, Drive, Autonomy, and Responsibility. At KCF, we provide perks that are focused on bringing out the best in you:', 'Integrate analytics solutions within the KCF SMARTdiagnostics platform', ""Bachelor's degree in Computational Science; Statistics; Computer, Mechanical, or Industrial Engineering; or similar field of study"", 'Company provided Vision and Dental plan', 'Strong organizational, time management, and prioritization abilities ', 'Essential Functions', 'Adaptable', 'Detail-oriented', 'Self-motivated', 'Must exemplify the following KCF cultural values: Smarts, Grit, Drive ', 'Exciting opportunities for growth and professional development', ""Bachelor's degree in Computational Science; Statistics; Computer, Mechanical, or Industrial Engineering; or similar field of studyStrong mathematical background (linear algebra, calculus, probability and statistics)Machine Learning experience (regression and classification, supervised, and unsupervised learning)At least 2 years of experience applying data analytics techniques to solve real-world problemsStrong algorithm design skillsProgramming experience with Python (Numpy, Scipy, Scikit-Learn, Matplotlib, TensorFlow, etc.), MATLAB, R, or similar languagesCommunicates verbally and in writing in a clear and professional mannerAble to work in a rapid-paced environment, managing and tracking multiple tasks with speed and accuracyHighly service-oriented disposition with aptitude in problem-solving Must exemplify the following KCF cultural values: Smarts, Grit, Drive Strong organizational, time management, and prioritization abilities Should be able to deal with difficult, sensitive, and confidential issues For the purposes of determining compliance with U.S. export control regulations, all applicants will be required to answer yes or no to the following question: Are you a U.S. Citizen, a lawful permanent resident of the U.S. (i.e., a green card holder), an asylee, or a refugee?"", 'At least 2 years of experience applying data analytics techniques to solve real-world problems', '401(k) retirement plan with up to 4% KCF match', 'For the purposes of determining compliance with U.S. export control regulations, all applicants will be required to answer yes or no to the following question: Are you a U.S. Citizen, a lawful permanent resident of the U.S. (i.e., a green card holder), an asylee, or a refugee?', 'Industry leading Medical Insurance - 100% Company Paid for you and your family', 'Highly service-oriented disposition with aptitude in problem-solving ', 'Where You Come In', 'Qualifications', 'Industry leading Medical Insurance - 100% Company Paid for you and your familyCompany provided Vision and Dental planCompetitive compensation plus quarterly bonus opportunities401(k) retirement plan with up to 4% KCF matchFour weeks Paid Time Off, in addition to paid HolidaysAnnual Fitness Reimbursement program and generous travel stipendsExciting opportunities for growth and professional developmentHybrid Remote Work Model - Work from Home, Work from Anywhere Ability to work in one of the most exciting fields in technology', 'Passionate for solving real-world problems', 'Strong mathematical background (linear algebra, calculus, probability and statistics)', 'Competitive compensation plus quarterly bonus opportunities', 'Our ideal candidate exemplifies our cultural values of Smarts, Grit, and Drive, and considers him or herself to be:', 'Self-motivatedPassionate for solving real-world problemsHard-workingAdaptableDetail-orientedAccepting of the unknownIf this sounds like you, we encourage you to keep reading!', 'Annual Fitness Reimbursement program and generous travel stipends', 'Perks & Benefits', 'Hybrid Remote Work Model - Work from Home, Work from Anywhere ', 'Design and implement algorithms tailored to industrial machine health diagnostics, prognostics, and root-cause analysis', 'Should be able to deal with difficult, sensitive, and confidential issues ', 'Communicates verbally and in writing in a clear and professional manner', 'Programming experience with Python (Numpy, Scipy, Scikit-Learn, Matplotlib, TensorFlow, etc.), MATLAB, R, or similar languages', 'Machine Learning experience (regression and classification, supervised, and unsupervised learning)', 'Why KCF Technologies?', 'Hard-working', 'Ability to work in one of the most exciting fields in technology', 'Construct data mining tools for acquiring training and validation data sets', 'Strong algorithm design skills', 'Accepting of the unknown', '#PM19', 'If this sounds like you, we encourage you to keep reading!']",Entry level,Full-time,Information Technology,Computer Hardware,2021-03-18 14:34:51
Data Engineer I,"Acumen, LLC","Burlingame, CA",3 weeks ago,101 applicants,"['', 'Due to the sensitive nature of much of our work, all Acumen employees must undergo a background check. Your employment will be contingent upon your completing, and Acumen reviewing to its satisfaction, a mandatory background check. ', 'Familiarity with one or more computer programming languages', 'Master’s in Information Management Systems, Statistics, Mathematics, Operations Research, Economics, Public Health, a related field with quantitative emphasis, or 2+ years of work experience in a field with quantitative emphasis', 'Ensure data inventory is complete and accurate through application design, including fault analysis and detection, quality control, and the development of tracking systems.', 'Previous experience in a Data Analyst/Data Engineer position', 'Qualifications Desired', 'Employees who work with particularly sensitive information may be asked to undergo an additional background check after starting work.', 'Extract, transform, and load (ETL) big data.', 'Collaborate with other Data Engineers and in-house researchers to maintain systems, produce documentation, and educate internal and external users about company resources.', 'Team player with strong interpersonal skills', '1+ years of experience working with databases or data pipelining tools ', 'Perform validation checks across multiple sources to verify data integrity as needed.', 'Excellent written and oral communication skills', ' A Bachelor’s in Computer Science, Statistics, Mathematics, Operations Research, Economics, Public Health, or related field with quantitative emphasis Strong organizational, planning, and problem solving skills Team player with strong interpersonal skills Excellent written and oral communication skills Familiarity with one or more computer programming languages ', ' Master’s in Information Management Systems, Statistics, Mathematics, Operations Research, Economics, Public Health, a related field with quantitative emphasis, or 2+ years of work experience in a field with quantitative emphasis Interest in big data Interest in making an impact in the field of healthcare policy research Previous experience in a Data Analyst/Data Engineer position 1+ years of experience working with programming languages such as SAS, SQL, Python, or R 1+ years of experience working with databases or data pipelining tools  ', 'Develop data structures, databases, and querying programs which facilitate efficient data access.', ' Extract, transform, and load (ETL) big data. Develop complex data processing algorithms that combine multiple data sources, while optimizing run-time efficiency. Develop data structures, databases, and querying programs which facilitate efficient data access. Develop data structures from claims and enrollment data which support research and analytic activities of in-house analysts as well as congressional and federal agencies. Ensure data inventory is complete and accurate through application design, including fault analysis and detection, quality control, and the development of tracking systems. Collaborate with other Data Engineers and in-house researchers to maintain systems, produce documentation, and educate internal and external users about company resources. Perform validation checks across multiple sources to verify data integrity as needed. Perform other duties and responsibilities as assigned. ', 'The Data Engineer Will', 'Interest in big data', 'Perform other duties and responsibilities as assigned.', 'Develop complex data processing algorithms that combine multiple data sources, while optimizing run-time efficiency.', 'Qualifications Required', 'Interest in making an impact in the field of healthcare policy research', 'Job Description', 'A Bachelor’s in Computer Science, Statistics, Mathematics, Operations Research, Economics, Public Health, or related field with quantitative emphasis', 'Strong organizational, planning, and problem solving skills', '1+ years of experience working with programming languages such as SAS, SQL, Python, or R', 'Develop data structures from claims and enrollment data which support research and analytic activities of in-house analysts as well as congressional and federal agencies.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,HealthPartners,"Richfield, MN",1 month ago,Be among the first 25 applicants,"['', 'Collaborate closely with analysts, architects, and engineers on our agile team, as well as other teams and in other disciplines.', 'Additional Information', 'Ability to coordinate projects from the concept stage to completion, leading cross functional teams as an active contributor.', 'Familiarity with DevOps and DataOps principles', 'Excellent communication skills.', 'Must have hands-on experience with SQL and Oracle.', 'Effectively work with teams across multiple lines of business to examine and identify database structural and functional needs by evaluating their operations, applications, and programming.', 'Present solutions and cost estimates to stakeholders, working with them to build a delivery approach and timeline for implementation.', 'BA/BS in computer science, data science, management information systems, mathematics, statistics, health care management, business, economics, finance or related field.', 'Help our teams identify and solve for other use cases as needed.', 'Familiarity with relational and non-relational database technologies.', 'A minimum of 7 years of direct experience working on the development of complex data design / architecture for decision support.', 'Must have experience working in an enterprise environment, effectively communicating and working across multiple business teams.', 'Good working knowledge of medical terminology, medical billing (or claims), and medical coding systems: ICD-10, CPT4, DRG, GPI.', 'Accountabilities', 'Lead our teams towards master data management to solve for the following use case: identifying the source of truth/system of record for core data elements such as contacts (i.e. email address) and how to keep them in sync on a timely basis.', 'BA/BS in computer science, data science, management information systems, mathematics, statistics, health care management, business, economics, finance or related field.A minimum of 7 years of direct experience working on the development of complex data design / architecture for decision support.Must have familiarity with event-driven architecture and messaging platforms.Strong knowledge of Oracle, SQL, ETL, DataStage, rules engines, stream-processing platforms such as Kafka.Strong knowledge of data analysis and statistical tools such as SAS and R.Working knowledge of data lake technology including HDFS, Hive, Impala, Python, Spark, Scala, and Nifi.Must have hands-on experience with SQL and Oracle.Ability to coordinate projects from the concept stage to completion, leading cross functional teams as an active contributor.Must have experience working in an enterprise environment, effectively communicating and working across multiple business teams.Excellent communication skills.Ability to work effectively in a collaborative agile environment.Strong understanding of industry privacy and security standards.', 'Define and help successfully plan, build and implement a ‘publish/subscribe’ messaging platform to build out a resilient and scalable event driven architecture that will ultimately be used across the organization.', 'Develop guidelines and patterns for usage of data streaming, data virtualization, change data capture and direct data access accounting for various use cases ranging from real time data needs to legacy more static data warehouse architecture and techniques.', 'Strong knowledge of Oracle, SQL, ETL, DataStage, rules engines, stream-processing platforms such as Kafka.', '5+ years of experience in health care and/or health research settings, working with health care claims and / or patient data.', 'Familiarity with Web Services, SQL, Java, and other technologies.', 'Strong understanding of industry privacy and security standards.', 'Master’s degree in computer science, management information systems, data science, mathematics, statistics, health care management, business, economics, finance or related field.Good working knowledge of medical terminology, medical billing (or claims), and medical coding systems: ICD-10, CPT4, DRG, GPI.5+ years of experience in health care and/or health research settings, working with health care claims and / or patient data.Agile experience, Scrum and Kanban preferredFamiliarity with relational and non-relational database technologies.Familiarity with Web Services, SQL, Java, and other technologies.Familiarity with DevOps and DataOps principles', 'Required Qualifications', 'Strong knowledge of data analysis and statistical tools such as SAS and R.', 'Define and help successfully plan, build and implement a ‘publish/subscribe’ messaging platform to build out a resilient and scalable event driven architecture that will ultimately be used across the organization.Develop guidelines and patterns for usage of data streaming, data virtualization, change data capture and direct data access accounting for various use cases ranging from real time data needs to legacy more static data warehouse architecture and techniques.Effectively work with teams across multiple lines of business to examine and identify database structural and functional needs by evaluating their operations, applications, and programming.Lead our teams towards master data management to solve for the following use case: identifying the source of truth/system of record for core data elements such as contacts (i.e. email address) and how to keep them in sync on a timely basis.Help our teams identify and solve for other use cases as needed.Prepare accurate database design and architectural diagrams for management and executive teams.Present solutions and cost estimates to stakeholders, working with them to build a delivery approach and timeline for implementation.Coordinate and work closely with IS&T teams to define and document a comprehensive and coherent summary of the current-state data architecture and it’s dependencies across business platforms, assessing opportunities for improvement.Evaluate business needs to help define data solutions, preparing design reports and manage the implementation of agreed upon solutions.Collaborate closely with analysts, architects, and engineers on our agile team, as well as other teams and in other disciplines.Inventory existing data assets and organizational needs and use cases for future applications', 'Master’s degree in computer science, management information systems, data science, mathematics, statistics, health care management, business, economics, finance or related field.', 'Ability to work effectively in a collaborative agile environment.', 'Prepare accurate database design and architectural diagrams for management and executive teams.', 'Job Description', 'Agile experience, Scrum and Kanban preferred', 'Must have familiarity with event-driven architecture and messaging platforms.', 'Working knowledge of data lake technology including HDFS, Hive, Impala, Python, Spark, Scala, and Nifi.', 'Inventory existing data assets and organizational needs and use cases for future applications', 'Preferred Qualifications', 'Evaluate business needs to help define data solutions, preparing design reports and manage the implementation of agreed upon solutions.', 'Coordinate and work closely with IS&T teams to define and document a comprehensive and coherent summary of the current-state data architecture and it’s dependencies across business platforms, assessing opportunities for improvement.']",Entry level,Full-time,Information Technology,Insurance,2021-03-18 14:34:51
Data DevOps Engineer,Versa Networks,"Santa Clara County, CA",3 weeks ago,92 applicants,"['', 'Experience in setting up multitenant Spark & Kafka clusters. ', 'Exhibit strong skills in at least Python, Scala or Java.', 'Experience in setting up multitenant Spark & Kafka clusters. Experience running infrastructure automation using Jenkins/Ansible/Terraform or similar pipelining tools.Exhibit strong skills in at least Python, Scala or Java.Experience in cleaning, processing, managing, analyzing & search vast collections of data.Experience in Apache Spark, Apache Kafka and related toolset.Some experience with database technologies is a must. Should know when to use a NoSQL, a regular SQL DB or a cloud SQL DB.Should be team player with the ability to handle multiple projects at the same time.', 'Experience running infrastructure automation using Jenkins/Ansible/Terraform or similar pipelining tools.', 'Some experience with database technologies is a must. Should know when to use a NoSQL, a regular SQL DB or a cloud SQL DB.', 'Roles and Responsibilities', 'Experience in cleaning, processing, managing, analyzing & search vast collections of data.', 'Versa is unique among software-defined networking vendors, providing an end-to-end solution that both simplifies and secures the WAN/branch office network. Based completely on software, Versa’s Cloud IP Platform delivers a broad set of capabilities for building agile and secure enterprise networks, as well as highly efficient managed service offerings.', 'Should be team player with the ability to handle multiple projects at the same time.', 'Experience', 'Experience in Apache Spark, Apache Kafka and related toolset.', 'We are looking for Data DevOps Engineers with at least 8+ years of relevant experience.']",Not Applicable,Full-time,Information Technology,Computer Networking,2021-03-18 14:34:51
Cloud Data Engineer,Boston Energy Trading and Marketing,"Boston, MA",3 weeks ago,39 applicants,"['', 'Required Experience and Skills:', 'Supportive of a culture of continuous process and organizational improvement', 'Incorporate new data sources from external vendors using streams, flat files, APIs, and databases.', 'Plan & execute using agile methodologies, developing & delivering within predefined sprints', '\xa0Work directly with analysts to gain insight into existing data sets.\xa0\xa0Evaluate use cases for new data turning requests into actionable designs.', 'Essential Duties & Responsibilities:', 'Maintain and provide support for the existing data pipelines using Python, ADF, and SQL', 'Develop real-time/near real-time data ingestion solutions from web API.', 'Leverage data modeling skills to design, and develop dimensional model (Facts, Dimensions)', '4+ years with relational databases (Microsoft SQL Server, MySQL, Oracle)', 'Build and maintain serverless data ingestion & refresh pipelines in terabyte-scale using Azure cloud services – Azure Data Factory, Azure Logic Apps, Azure Functions, Python, Databricks with Snowflake Azure Data Lake Store,\xa0and others', 'If You are…', '2+ years working with\xa0Spark\xa0or other distributed computing frameworks (e.g.:\xa0Hadoop,\xa0Cloudera, Databricks)', '2+ years with Azure services including: Logic Apps, Azure Functions, DevOps', 'Bachelor’s degree in computer science, engineering, or data science', 'Passionate about technology and excel in cloud platforms, implementation, and troubleshooting', 'Identify & deploy appropriate file formats for data ingestion into various storage and/or compute services via ADF for multiple use cases', 'Experience with contemporary data file formats like Apache Parquet, Avro', 'The Cloud Data Engineer will help transform our cloud data systems by deploying architectures to drive greater analytical and business value from a wide range of data sources. This role will work with Energy Analysts, Traders and IT team members to design and develop high-performance, resilient, automated data pipelines, and data transformation applications. They will adapt technologies for ingesting, transforming, classifying, cleansing, and exposing data using creative designs to support the needs of our business solutions. The Cloud Data Engineer’s broad experience with data management technologies will enable matching the right technologies to the required schemas and workloads. ', 'Experience analyzing data for data quality and supporting the use of data in an enterprise setting.', '…we would love to hear from you!', ' …we would love to hear from you!', 'Develop and implement tests to ensure data quality across all integrated data sources.', 'An excellent communicator, both oral and written, with an understanding of programming concepts, operating systems and/or networking', 'Intermediate/advanced programming experience in\xa0Python (Anaconda/Data Science/Data Acquisition),\xa0and\xa0SQL (Complex ANSI-SQL)', 'Distributed SQL query engines (e.g.:\xa0Hive,\xa0Azure SQL DW, Presto) a plus', 'Boston Energy Trading & Marketing (BETM), located in the financial district, is undergoing a digital transformation.\xa0We’re building a highly energetic, talented team to help realize our vision of creating a next-generation digital platform that enables growth.\xa0As a hybrid-startup, we’re recruiting talent with the vision, passion and attitude to accelerate delivery, but who also have the experience to effectively manage the digital operation we create. If you have those traits and capabilities, and you are ready to join our Boston-based IT team, we’d love to hear from you!', '\xa0Work directly with analysts to gain insight into existing data sets.\xa0\xa0Evaluate use cases for new data turning requests into actionable designs.Build and maintain serverless data ingestion & refresh pipelines in terabyte-scale using Azure cloud services – Azure Data Factory, Azure Logic Apps, Azure Functions, Python, Databricks with Snowflake Azure Data Lake Store,\xa0and othersIncorporate new data sources from external vendors using streams, flat files, APIs, and databases.Leverage data modeling skills to design, and develop dimensional model (Facts, Dimensions)Maintain and provide support for the existing data pipelines using Python, ADF, and SQLPlan & execute using agile methodologies, developing & delivering within predefined sprintsIdentify & deploy appropriate file formats for data ingestion into various storage and/or compute services via ADF for multiple use casesDevelop real-time/near real-time data ingestion solutions from web API.Develop and implement tests to ensure data quality across all integrated data sources.', '\xa0', 'Experience with Snowflake solutions desired.', '3+ years of experience designing and developing enterprise data warehouses (Kimball star schema model)', 'Bachelor’s degree in computer science, engineering, or data scienceIntermediate/advanced programming experience in\xa0Python (Anaconda/Data Science/Data Acquisition),\xa0and\xa0SQL (Complex ANSI-SQL)4+ years with relational databases (Microsoft SQL Server, MySQL, Oracle)3+ years of experience designing and developing enterprise data warehouses (Kimball star schema model)2+ years working with\xa0Spark\xa0or other distributed computing frameworks (e.g.:\xa0Hadoop,\xa0Cloudera, Databricks)2+ years with Azure services including: Logic Apps, Azure Functions, DevOpsExperience with contemporary data file formats like Apache Parquet, AvroExperience analyzing data for data quality and supporting the use of data in an enterprise setting.Experience with Snowflake solutions desired.Distributed SQL query engines (e.g.:\xa0Hive,\xa0Azure SQL DW, Presto) a plus', 'Our Team’s Vision:', 'Passionate about technology and excel in cloud platforms, implementation, and troubleshootingAn excellent communicator, both oral and written, with an understanding of programming concepts, operating systems and/or networkingSupportive of a culture of continuous process and organizational improvement']",Mid-Senior level,Full-time,Information Technology,Oil & Energy,2021-03-18 14:34:51
Data Engineer,Catapult Solutions Group,"Plano, TX",2 weeks ago,27 applicants,"['', '5+ years of experience in generating and presenting data-driven insights from enterprise data', 'Build automated reports and dashboards in industry-leading BI tools (Qlik)', 'Must-Haves/ Top 3 skills  1 - Qliksense  2 - SQL  3 - Qlikview     Come join the ""Global Sales Analytics"" as a data analyst, to work on exciting analytically problems. We are looking for creative problem solvers with a passion to deliver data-driven insights. This position works closely with sales teams to deliver business results using data for reporting, insights, and optimization. ', 'Ability to ""dive into"" systems and formulate ideas on how to better support stakeholders', '     ', 'Strong analytical problem solving, priority setting, facilitation, multi-tasking, analytical, and collaboration skills', 'Advanced SQL skills to perform data querying and summarization; Experience in advanced analytics and optimization a plus', '5+ years of experience in generating and presenting data-driven insights from enterprise dataBuild automated reports and dashboards in industry-leading BI tools (Qlik)Ability to ""dive into"" systems and formulate ideas on how to better support stakeholdersStrong analytical problem solving, priority setting, facilitation, multi-tasking, analytical, and collaboration skillsOutstanding communication skills with the ability to influence decision-makers and build consensus with teamsWillingness and ability to adapt to the rapid business and organizational changes that accompany a high-growth environment.Work effectively with cross-functional teams to document requirements and ensure consistency across systems and business processes.Ability to be multi-task effectively and thrive in a fast-paced, dynamic environment without close guidance or supervisionAdvanced SQL skills to perform data querying and summarization; Experience in advanced analytics and optimization a plusFamiliarity with programming languages and stats packages (e.g. python, R)Undergrad degree in a quantitative field (Masters preferred)', 'Familiarity with programming languages and stats packages (e.g. python, R)', 'Work effectively with cross-functional teams to document requirements and ensure consistency across systems and business processes.', 'Desired Skills and Experience', 'Willingness and ability to adapt to the rapid business and organizational changes that accompany a high-growth environment.', 'Ability to be multi-task effectively and thrive in a fast-paced, dynamic environment without close guidance or supervision', 'Outstanding communication skills with the ability to influence decision-makers and build consensus with teams', 'Undergrad degree in a quantitative field (Masters preferred)']",Mid-Senior level,Full-time,Information Technology,Banking,2021-03-18 14:34:51
Software Data Engineer,Smart Data,"Louisville, KY",2 weeks ago,97 applicants,"['', 'Create automation systems and tools to configure, monitor, and orchestrate our…', 'Minimum Qualifications:', 'Front-end frameworks (Angular, React, Backbone, etc)', 'Strong coding skills in one or more modern programming languages, Python preferred, but Java or C# are acceptable.', 'Design, develop, and implement…Create automation systems and tools to configure, monitor, and orchestrate our…Evaluate new technologies for continuous improvements in our…Collaborate closely with the product/applications teams to build out new…Data infrastructure and pipelines that collect, connect, centralize, and curate data from various internal and external data sources.Data delivery and data visualization applications using state-of-the-art web technologies.Work with data scientists and data analysts to implement descriptive, forecasting and predictive algorithms and models using the latest technologies.Bring an entrepreneurial mindset, openness, transparency, and collegiality to your everyday work.', 'Atleast one popular back-end framework (Java/Spring/Hibernate, Nodejs/Express, .Net, Python/Flask/SqlAlchemy). Genscape uses .Net extensively but not exclusively.', 'Design, develop, and implement…', 'Bring an entrepreneurial mindset, openness, transparency, and collegiality to your everyday work.', 'Distributed systems for data processing, including tools such as Spark, Kafka, Kubernetes, etc.\xa0', 'BS in Computer Science or related degrees.\xa0', 'Work with data scientists and data analysts to implement descriptive, forecasting and predictive algorithms and models using the latest technologies.', 'Evaluate new technologies for continuous improvements in our…', 'Data delivery and data visualization applications using state-of-the-art web technologies.', 'Collaborate closely with the product/applications teams to build out new…', 'General Role Description:', 'Fluency with SQL and Relational Databases, like PostgreSQL, SQL Server, etc.', '2+ [5+ for Senior] years of experience ', 'DevOps mindset, experience with continuous integration and automation tools and processes, like TeamCity/Jenkins/Bamboo.\xa0', 'Data infrastructure and pipelines that collect, connect, centralize, and curate data from various internal and external data sources.', 'BS in Computer Science or related degrees.\xa0Strong coding skills in one or more modern programming languages, Python preferred, but Java or C# are acceptable.2+ [5+ for Senior] years of experience Experience with developing:Backendsubsystems of large web applications or complex ETL processing systemsDistributed systems for data processing, including tools such as Spark, Kafka, Kubernetes, etc.\xa0Atleast one popular back-end framework (Java/Spring/Hibernate, Nodejs/Express, .Net, Python/Flask/SqlAlchemy). Genscape uses .Net extensively but not exclusively.Front-end frameworks (Angular, React, Backbone, etc)Fluency with SQL and Relational Databases, like PostgreSQL, SQL Server, etc.DevOps mindset, experience with continuous integration and automation tools and processes, like TeamCity/Jenkins/Bamboo.\xa0', 'Experience with developing:', 'Software Data Engineer Job Description', 'Backendsubsystems of large web applications or complex ETL processing systems']",Mid-Senior level,Full-time,Engineering,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,TDECU,"Sugar Land, TX",2 weeks ago,Be among the first 25 applicants,"['', 'Disclaimer ', '\uf0b7 Ability to work both independently and within teams.', '\uf0b7 Ability to perform business analysis and documentation.', 'Knowledge, Skills, and Abilities ', 'Integration Services ETL (Extract, Transform, Load):', '\uf0b7 Excellent written and oral communication skills to Identifies, researches, and resolves technical problem.', 'Stays abreast of changes within the credit union relating to services and procedures.', '\uf0b7 Ability to do data integrity and validation; and manage multiple priorities simultaneously.', 'While performing the essential duties of this position, an employee would frequently be required to stand, walk, and sit. An employee must frequently lift and/or move up to 10 pounds and may occasionally lift and/or move up to 25 pounds to perform essential position functions.', '(Education, Experience, Knowledge, Skills, and Abilities) Essential education: \uf0b7 Bachelor’s degree required or equivalent years of experience.\uf0a7 A minimum of 5 years experience in integration services, reporting services, business documentation, and T-SQL systems.', '(Education, Experience, Knowledge, Skills, and Abilities) ', 'Job Description ', 'Essential education: ', 'Reporting Services: Develop and deploy reports to the self-service portal. Maintain folder level security based on active directory groups. High level summary and self-service reporting and dashboards. Reporting against, multiple data warehouses, source systems (SQL Server and Oracle), and SSAS database in multidimensional mode and Power BI mode.', '\uf0a7 A minimum of 5 years experience in integration services, reporting services, business documentation, and T-SQL systems.', '\uf0a7 A minimum of 3 years experience in Analysis Services.', '\uf0b7 Experience in Team Foundation Server or other source control system.', '(The physical demands and work environment characteristics described herein are representative of those that must be met by an employee to successfully perform essential functions of this position and/or may be encountered while performing essential functions. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.) ', 'Physical Demands and Work Environment: ', 'Minimum Qualifications: ', 'Business Requirements Documentation', '(The physical demands and work environment characteristics described herein are representative of those that must be met by an employee to successfully perform essential functions of this position and/or may be encountered while performing essential functions. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.) While performing the essential duties of this position, an employee would frequently be required to stand, walk, and sit. An employee must frequently lift and/or move up to 10 pounds and may occasionally lift and/or move up to 25 pounds to perform essential position functions.Specific vision abilities required by this position include close vision, distance vision, and the ability to adjust focus. The noise level in the work environment is usually moderate.', '\uf0b7 Competent with MS Office (Word, Excel, PowerPoint, Access, Visio)', 'Texas Dow Employees Credit Union is an equal opportunity employer, dedicated to a policy of non-discrimination in employment on any basis including race, color, age, protected veteran status, sex, religion, disability, genetic information, national origin or other status protected by federal, state or local law. Consistent with the American Disabilities Act, applicants may request accommodations needed to participate in the application process.', 'Implements processes as required by the Bank Secrecy Act and TDECU policies and procedures including but not limited to completing and submitting Currency Transaction Reports, being knowledgeable of signs of unusual financial activities, and reporting signs of unusual activity and/or completing reporting for Suspicious Activity Report as appropriate per procedure.', 'Business Requirements Documentation: Responsible for ticketing system report requests with detail from all the various data sources, requirements, and time estimates. Maintain DBA Tools database with reports and extract information as well as Integration Services packages. Work with project management teams for documentation details. Technical report documentation of data extracts, automation processes, and data analytics.', 'The Business Intelligence Specialist role will be responsible for identify business intelligence, reporting, and data analysis needs. They will work closely with end-users and business units to turn data into critical information and knowledge that can be used to make sound business decisions.', '\ufeffEssential Duties and Responsibilities: ', 'Integration Services ETL (Extract, Transform, Load): Responsible for design, testing, and implementation of ETL processes for delivering data extracts and final data reports as well as loading fact and dimension tables in the data warehouse. Monitors integration services database to determine job status, troubleshoot, and resolve any issues that occur while running ETL jobs. Responsible for Data Warehouse Star Schema design.Reporting Services: Develop and deploy reports to the self-service portal. Maintain folder level security based on active directory groups. High level summary and self-service reporting and dashboards. Reporting against, multiple data warehouses, source systems (SQL Server and Oracle), and SSAS database in multidimensional mode and Power BI mode.Business Requirements Documentation: Responsible for ticketing system report requests with detail from all the various data sources, requirements, and time estimates. Maintain DBA Tools database with reports and extract information as well as Integration Services packages. Work with project management teams for documentation details. Technical report documentation of data extracts, automation processes, and data analytics.Analysis Services: Design and implement multidimensional mode database and Power BI mode data model including stand alone and through SharePoint.Completes all mandatory compliance testing within designated time frame and other compliance assignments including Office of Foreign Assets Control (OFAC), Unusual Activity Reports (UAR), and Security processes.Implements processes as required by the Bank Secrecy Act and TDECU policies and procedures including but not limited to completing and submitting Currency Transaction Reports, being knowledgeable of signs of unusual financial activities, and reporting signs of unusual activity and/or completing reporting for Suspicious Activity Report as appropriate per procedure.Stays abreast of changes within the credit union relating to services and procedures.', 'Analysis Services: Design and implement multidimensional mode database and Power BI mode data model including stand alone and through SharePoint.', '\uf0b7 Working knowledge of Microsoft Business Intelligence stack (Integration Services, Reporting Services, and Analysis Services)', '\uf0b7 C# knowledge for SSIS scripting', '\uf0b7 In depth knowledge of SISS and its capabilities.', 'The above statements are intended to describe the general nature and level of work being performed by people assigned to this job. They are not intended to be an exhaustive list of all responsibilities, duties, and skills required of personnel so classified. ', 'Analysis Services', 'Position Summary: ', 'Completes all mandatory compliance testing within designated time frame and other compliance assignments including Office of Foreign Assets Control (OFAC), Unusual Activity Reports (UAR), and Security processes.', '\uf0a7 A minimum of 3 years experience in Analysis Services.\uf0a7 Experience in a financial institution preferredKnowledge, Skills, and Abilities \uf0b7 Competent with MS Office (Word, Excel, PowerPoint, Access, Visio)\uf0b7 Working knowledge of Microsoft Business Intelligence stack (Integration Services, Reporting Services, and Analysis Services)\uf0b7 C# knowledge for SSIS scripting\uf0b7 In depth knowledge of SISS and its capabilities.\uf0b7 Knowledge of T-SQL and ANSI SQL stored procedures with multiple parameters.\uf0b7 Ability to perform business analysis and documentation.\uf0b7 Experience in Team Foundation Server or other source control system.\uf0b7 Ability to do data integrity and validation; and manage multiple priorities simultaneously.\uf0b7 Excellent written and oral communication skills to Identifies, researches, and resolves technical problem.\uf0b7 Ability to work both independently and within teams.', '\uf0b7 Bachelor’s degree required or equivalent years of experience.', 'The above statements are intended to describe the general nature and level of work being performed by people assigned to this job. They are not intended to be an exhaustive list of all responsibilities, duties, and skills required of personnel so classified. Texas Dow Employees Credit Union is an equal opportunity employer, dedicated to a policy of non-discrimination in employment on any basis including race, color, age, protected veteran status, sex, religion, disability, genetic information, national origin or other status protected by federal, state or local law. Consistent with the American Disabilities Act, applicants may request accommodations needed to participate in the application process.', 'Integration Services ETL (Extract, Transform, Load): Responsible for design, testing, and implementation of ETL processes for delivering data extracts and final data reports as well as loading fact and dimension tables in the data warehouse. Monitors integration services database to determine job status, troubleshoot, and resolve any issues that occur while running ETL jobs. Responsible for Data Warehouse Star Schema design.', 'Specific vision abilities required by this position include close vision, distance vision, and the ability to adjust focus. The noise level in the work environment is usually moderate.', '\uf0b7 Knowledge of T-SQL and ANSI SQL stored procedures with multiple parameters.', '\uf0a7 Experience in a financial institution preferred', 'Reporting Services:']",Associate,Full-time,Information Technology,Financial Services,2021-03-18 14:34:51
Data Engineer,Twistle,"Albuquerque, NM",1 month ago,Be among the first 25 applicants,"['', 'Experience working in an Agile/Scrum development process.', 'Participate in code reviews with languages like LookerML, Python, Django, JavaScript', 'BS/MS degree in Computer Science, Engineering, and/or related Healthcare experience.', '3 years of experience in working in multi-tenant SaaS applications and services.Experience working in an Agile/Scrum development process.', 'Build analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition.', 'Strong working knowledge of tools like JIRA, Asana, Confluence, and Tettra.', '3 years of experience in working in multi-tenant SaaS applications and services.', 'Strong knowledge of and experience with reporting software such as Looker, BusinessObjects, Power BI, Tableau, etc.', 'Maintain both test and production library of interfaces, applying appropriate methods, procedures, and safeguards to protect the integrity of interfaces, ensuring their recoverability.', 'Strong working knowledge of AWS services including Redshift, RDS, EMR and EC2.', 'Automate and monitor business critical applications and troubleshoot or escalate appropriately when issues arise.', 'Strong experience demonstrating and understanding tools like Kafka, Spark and Hadoop; relational NoSQL and SQL databases including Cassandra and PostgreSQL. ', 'Identify, design, and implement process improvements related to (data mapping, optimizing data delivery, and scalability of transformations while automating manual processes.', 'Collaborate with other engineers, implementation, customer success managers throughout the development process to release functional, performant and secure data on a regular basis.', 'Hands on experience with SQL, developing stored procedures, functions, views and triggers, while validating and analyzing data integrity. ', 'Demonstrate experience working with various payloads including (JSON, XML APIs, Web Services, ETL, and File Transfers', 'Build required infrastructure for optimal extraction, transformation and loading of data from various data sources using AWS and SQL technologies.', 'Automate and monitor business critical applications and troubleshoot or escalate appropriately when issues arise.Participate in code reviews with languages like LookerML, Python, Django, JavaScriptCollaborate with other engineers, implementation, customer success managers throughout the development process to release functional, performant and secure data on a regular basis.', 'Monitor, triage, and modify configuration for integrated healthcare messaging within Twistle Platform.', '5+ years’ experience in healthcare integrations, with at least 2+ years in Cloud applications.', 'Familiarity with one or more of the following development languages: Python, C#, Java.', ' Responsibilities ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
"Data Engineer, Consultant",Blue Shield of California,"Oakland, CA",3 days ago,Be among the first 25 applicants,"['', 'Hands-on experience in Python coding and familiarity with Python libraries used in data engineering and machine learning like Pandas, NumPy etc.', 'Job Summary', ' 8+ years of proven hands-on experience in data integration, data warehouse, BI solution design, development, and implementation. 5+ years of hands-on experience in at least one relational data source such as Oracle, SQL Server and Data warehouse MPP applications such Netezza, Teradata etc. Experience working in a cloud environment – AWS, Azure or GCP, related cloud specific flavor of big data/Hadoop/MPP platforms and other technology components. Experience with AI-ML tools like RapidMiner, Databricks or H20.AI. Experience with NOSQL databases like MongoDB, Cassandra or MarkLogic Hands-on experience in Python coding and familiarity with Python libraries used in data engineering and machine learning like Pandas, NumPy etc. 5+ years of hands-on experience with Data Integration, Data Quality and Data Virtualization tools like Informatica PowerCenter, Informatica Data Quality and Golden Gate Replication. Proven expertise in writing efficient advanced and analytical SQL and Unix shell scripts. Hands-on experience with source version control, continuous integration, and experience with release/change management delivery tools /methodology. Extensive experience in triaging data issues, analyzing end to end data pipelines and in working with business users in resolving issues. Prior experience in leading a technical team, providing technical direction and mentoring junior data integration developer. ', 'Excellent communications and client interfacing skills as with an ability to work in a highly collaborative environment.', 'Extensive experience in triaging data issues, analyzing end to end data pipelines and in working with business users in resolving issues.', 'Proven expertise in writing efficient advanced and analytical SQL and Unix shell scripts.', 'Prior experience in leading a technical team, providing technical direction and mentoring junior data integration developer.', 'Qualifications', ' Minimum Bachelor’s Degree in Computer Science or other related disciplines.', 'Experience working in a cloud environment – AWS, Azure or GCP, related cloud specific flavor of big data/Hadoop/MPP platforms and other technology components.', '8+ years of proven hands-on experience in data integration, data warehouse, BI solution design, development, and implementation.', 'Work closely with Operations Production Support team in resolving escalated high priority incidents and the development coding issues.', 'Knowledge And Experience', 'Experience with AI-ML tools like RapidMiner, Databricks or H20.AI.', 'Prior experience in healthcare Payor/provider space.', 'Design and develop reusable components, code, and document custom automation frameworks, maintain scripts, and update these items as needed to build continuous delivery pipelines, ensuring rapid availability of the product.', '5+ years of hands-on experience in at least one relational data source such as Oracle, SQL Server and Data warehouse MPP applications such Netezza, Teradata etc.', '5+ years of hands-on experience with Data Integration, Data Quality and Data Virtualization tools like Informatica PowerCenter, Informatica Data Quality and Golden Gate Replication.', 'Experience with NOSQL databases like MongoDB, Cassandra or MarkLogic', 'Hands-on experience with source version control, continuous integration, and experience with release/change management delivery tools /methodology.', ' Expert grasp of data warehouse design techniques including slowly changing dimensions, aggregation, partitioning and indexing strategies.  Excellent communications and client interfacing skills as with an ability to work in a highly collaborative environment. Prior experience in healthcare Payor/provider space. Design and develop reusable components, code, and document custom automation frameworks, maintain scripts, and update these items as needed to build continuous delivery pipelines, ensuring rapid availability of the product. Work closely with Operations Production Support team in resolving escalated high priority incidents and the development coding issues.  Minimum Bachelor’s Degree in Computer Science or other related disciplines.', 'Expert grasp of data warehouse design techniques including slowly changing dimensions, aggregation, partitioning and indexing strategies. ']",Not Applicable,Full-time,Information Technology,Hospital & Health Care,2021-03-18 14:34:51
Data Engineer - Denver,GavinHeath,"Denver, CO",2 weeks ago,Be among the first 25 applicants,"['', 'Qualifications', 'Opportunity for career advancement ', 'Write maintainable code and apply automated testing where applicable', 'Responsibilities', 'Willingness to jump in and get the job done no matter how big or small', 'Embed reporting and data visualizations within company application', 'Focus on continuous improvement', 'Expert level SQL knowledge with extensive ETL experience and the ability to diagnose and solve DB performance issues', 'Be a key contributor to cloud-based applications and services using SQL, Azure Data Factory, Rest APIs, SQL Server/Synapse and Power Bi', ""Develop solutions that support customers self-service reporting needs and streamline the implementation team's workflows"", 'Work with a team to make architectural decisions', ' 1-5+ years of experience building solutions in Power BI and Azure Synapse or a comparable business intelligence solution Expert level SQL knowledge with extensive ETL experience and the ability to diagnose and solve DB performance issues Willingness to jump in and get the job done no matter how big or small Focus on continuous improvement Opportunity for career advancement  ', '1-5+ years of experience building solutions in Power BI and Azure Synapse or a comparable business intelligence solution', "" Be a key contributor to cloud-based applications and services using SQL, Azure Data Factory, Rest APIs, SQL Server/Synapse and Power Bi Embed reporting and data visualizations within company application Develop solutions that support customers self-service reporting needs and streamline the implementation team's workflows Write maintainable code and apply automated testing where applicable Work with a team to make architectural decisions Deliver value to the business in a fast-paced agile environment "", 'Deliver value to the business in a fast-paced agile environment']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,ANTENNA,"New York, NY",3 weeks ago,71 applicants,"['', '401K', 'You love data – and enjoy pushing the boundaries of what insights can be gleaned from it.', 'Proficiency in Kubernetes, Spark, and Google Cloud Platform.', ' You love data – and enjoy pushing the boundaries of what insights can be gleaned from it. Proficiency in Python / Django / PostgreSQL. Proficiency in Kubernetes, Spark, and Google Cloud Platform. Basic front end knowledge (e.g. HTML, JavaScript) Strong background in distributed data processing, software engineering design, and data modeling concepts. :3+ years of experience building complex data pipelines and products. Quick to pick up new technologies. Strong in written and verbal communication. ', ':3+ years of experience building complex data pipelines and products.', 'Wellness stipend', 'Basic front end knowledge (e.g. HTML, JavaScript)', 'Design, build, and deploy streaming and batch data pipelines capable of processing and storing data quickly and reliably.', 'Meaningful equity compensation', 'About ANTENNA', 'And more!', ""ANTENNA's Benefits"", 'Quick to pick up new technologies.', 'Unlimited PTO', 'Maintain an efficient data infrastructure.', ' Create reliable data pipelines and build intuitive data products that allow our Data Analysts and Data Scientists to easily leverage data in a self-service manner. Design, build, and deploy streaming and batch data pipelines capable of processing and storing data quickly and reliably. Integrate with a variety of data sources such as email receipt data, credit card data, and more. Ensure data quality and build tools to detect and alert our team of any anomalies. Maintain an efficient data infrastructure. ', 'Create reliable data pipelines and build intuitive data products that allow our Data Analysts and Data Scientists to easily leverage data in a self-service manner.', 'Competitive base salary', 'Full healthcare benefits', 'Strong background in distributed data processing, software engineering design, and data modeling concepts.', 'Workstation stipend + work from anywhere', ""At ANTENNA, you'll work hand-in-hand with the world's leading consumer subscription businesses to help build incredible, sustainable companies."", ' Meaningful equity compensation Competitive base salary Full healthcare benefits Unlimited PTO 401K Wellness stipend Workstation stipend + work from anywhere And more!', 'Proficiency in Python / Django / PostgreSQL.', 'Integrate with a variety of data sources such as email receipt data, credit card data, and more.', 'Strong in written and verbal communication.', 'Ensure data quality and build tools to detect and alert our team of any anomalies.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Quantitative Data Engineer,JCW,"New York, NY",3 weeks ago,Over 200 applicants,"['The Data Engineer will be working alongside the Data Analytics and Strategy team. The qualified candidate will be a  hands-on programmer and will be knowledgeable in data modeling. ', '', 'Margarita Romero recruits for Risk and Analytics professionals across the financial services sector and would be happy to provide further details about this or other positions. You can reach her at (646) 564 – 3594 or at margarita.romero@jcwresourcing.com.', 'Details ', 'JCW is currently working on a search for a Junior/ Senior Data Engineer. The qualified candidate will join a multi-strategy investment manager here in NYC. ', ' ', 'Details', 'Python is required, SQL and C++ are a plus. ', 'Requirements', 'Requirements ', 'Python is required, SQL and C++ are a plus. Degree in Computer Science, Financial Engineering, Statistics, or related fields.  5+ years experience managing data for quant trading purposes   ', ' 5+ years experience managing data for quant trading purposes   ', 'Degree in Computer Science, Financial Engineering, Statistics, or related fields. ']",Associate,Full-time,Analyst,Financial Services,2021-03-18 14:34:51
Cloud Data Engineer,Merck,"Madison, NJ",1 month ago,Be among the first 25 applicants,"['', 'Employee Status', 'Developing Data Lake solutions on Redshift and developing data ingestion pipelines through AWS S3 using python.', 'Preferably', ""Perform data management releases following our company's standard SDLC including hyper-care and follow-ups"", 'Partner with domain experts to verify and drive model capabilities and translate modeling outputs into business language', 'AWS Solution Architect Associate Certified', 'Proficiency to create and run AWS Glue,  Electronic Medical Record (EMR) , StreamSets, DataBricks or other Extract,Transform,Load (ETL) toolsets. Multiple toolsets knowledge is preferable.', 'Design and implement Data Lake technologies across Animal Health division focused on Cloud Architecture best practices', 'B.S. in Computer Science, Math, Business or related field3+ plus years of experience developing database loads and ETL tools 3+ plus Experience in developing and / or deploying complex Business Analytics solutions2+ years of delivering end-to-end data analysis including working with large data setsMust have worked with Business Intelligence / Data Warehouse projectsAWS Solution Architect Associate Certified', 'Experience with AWS cloud implementation using EC2 instance, S3 storage, Redshift and different services', 'What We Look For …', 'INSPIRE.', 'B.S. in Computer Science, Math, Business or related field', 'Hazardous Material(s):', '3+ plus years of experience developing database loads and ETL tools ', 'Inventing For Life, Impacting Lives', 'Partnering with business partners organization to build cloud native solutions for Animal Health users both internal and external.', 'Actively listen, and collaboratively translate commercial and business questions into data analysis and commercial solutions', 'INVENT.', 'Valid Driving License', 'Competencies And Skills', 'Experience with Enterprise data sets – Across Value chain of Commercial, Supply chain, and ResearchManaged vendor teams that handle technical development and supportBI development experience for Online Analytical Processing (OLAP) cubes with Spotfire / Qlik / PowerBI, etc.', 'Requisition ID:', 'Shift', 'BI development experience for Online Analytical Processing (OLAP) cubes with Spotfire / Qlik / PowerBI, etc.', '2+ years of delivering end-to-end data analysis including working with large data sets', 'Engineering & Management of AWS platform to stay current with internal policies & procedure and advancement of the platform to support current & emerging business needsPartnering with business partners organization to build cloud native solutions for Animal Health users both internal and external.Collaborate with our AWS Cloud engineering team to ensure appropriate security controls are in place for AWS platform and solutions leveraging cloud servicesDesign and implement Data Lake technologies across Animal Health division focused on Cloud Architecture best practicesDriving Infrastructure as code for service deployment as well as configuration managementDeveloping and maintain Software Development Lifecycle (SDLC) documentation for the platformActively participating in promoting best practices through community of practice and learning sessions', 'Relocation:', 'Actively participating in promoting best practices through community of practice and learning sessions', 'Demonstrate ability to design and develop complex database loads and maintenance process', 'Collaborate with our AWS Cloud engineering team to ensure appropriate security controls are in place for AWS platform and solutions leveraging cloud services', 'Experience with Enterprise data sets – Across Value chain of Commercial, Supply chain, and Research', 'Travel Requirements', 'Developing and maintain Software Development Lifecycle (SDLC) documentation for the platform', ""Experience with AWS cloud implementation using EC2 instance, S3 storage, Redshift and different servicesProficiency to create and run AWS Glue,  Electronic Medical Record (EMR) , StreamSets, DataBricks or other Extract,Transform,Load (ETL) toolsets. Multiple toolsets knowledge is preferable.Demonstrate ability to design and develop complex database loads and maintenance processDeveloping Data Lake solutions on Redshift and developing data ingestion pipelines through AWS S3 using python.Highly proficient in the use of SQL for developing complex Stored Procedures, Triggers, Functions, Performance Tuning and Query Optimization.Deliver on data-driven projects through organization of team efforts and hands-on approach workingActively listen, and collaboratively translate commercial and business questions into data analysis and commercial solutionsPerform data management releases following our company's standard SDLC including hyper-care and follow-upsPartner with domain experts to verify and drive model capabilities and translate modeling outputs into business language"", 'Education And Experience', 'VISA Sponsorship', ', ', 'Inspiring Your Career Growth', 'Who We Are …', 'NOTICE FOR INTERNAL APPLICANTS', 'Flexible Work Arrangements', 'US And Puerto Rico Residents Only', 'Must have worked with Business Intelligence / Data Warehouse projects', '3+ plus Experience in developing and / or deploying complex Business Analytics solutions', 'Driving Infrastructure as code for service deployment as well as configuration management', 'Job Description', 'IMPACT.', 'Search Firm Representatives Please Read Carefully ', 'Highly proficient in the use of SQL for developing complex Stored Procedures, Triggers, Functions, Performance Tuning and Query Optimization.', 'Managed vendor teams that handle technical development and support', 'Deliver on data-driven projects through organization of team efforts and hands-on approach working', 'Engineering & Management of AWS platform to stay current with internal policies & procedure and advancement of the platform to support current & emerging business needs', 'Number Of Openings']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Criteria Corp,"West Hollywood, CA",3 weeks ago,28 applicants,"['Excellent communication skills to work with Executive, Product, and Engineering teams ', 'Experience building and refining data architectures and pipelines ', 'Expertise in modern cloud computing and storage solutions (we’re on AWS) ', 'Criteria is looking for an experienced and talented Data Engineer to join our small, but growing Engineering team. We have some exciting data-focused projects on our Roadmap, so we need someone to help build the data infrastructure and capability to create powerful and useful tools for our customers. You will be the first and foundational hire in our Data team! ', 'Ability to create internal tools for analytics and data scientist team members to inform the direction of, and to optimize our product ', 'Experience working with structured and unstructured data stores ', 'Strong commitment to data quality ', 'Expertise working with SQL ', '3-5 years of experience working as a Data Engineer ', 'The position is full-time and based in our corporate office on San Vicente in West Hollywood, CA. Currently, we’re all working remote. ', 'Ability to build and optimize ELT and ETL processes from a variety of sources ', 'Working knowledge of keeping data separated and secure across multiple data centers in various regions ', 'Ability to work in an Agile environment. We generally run two-week sprints to keep things organized and moving along. The ideal candidate will be able to thrive in this type of environment ', '3-5 years of experience working as a Data Engineer Experience building and refining data architectures and pipelines Expertise in modern cloud computing and storage solutions (we’re on AWS) Ability to build and optimize ELT and ETL processes from a variety of sources Experience working with structured and unstructured data stores Strong commitment to data quality Expertise working with SQL Working knowledge of keeping data separated and secure across multiple data centers in various regions Ability to create internal tools for analytics and data scientist team members to inform the direction of, and to optimize our product Ability to work in an Agile environment. We generally run two-week sprints to keep things organized and moving along. The ideal candidate will be able to thrive in this type of environment Excellent communication skills to work with Executive, Product, and Engineering teams Strong desire to learn new skills and improve existing ones', 'Strong desire to learn new skills and improve existing ones', '\xa0 ', 'If you want to know more about what it’s like to work here check us out on Glassdoor:  https://www.glassdoor.com/Overview/Working-at-Criteria-EI_IE1146223.11,19.htm ', 'Top candidates will have these skills: ']",Mid-Senior level,Full-time,Analyst,Computer Software,2021-03-18 14:34:51
Data Engineer,Canary,"Seattle, WA",1 week ago,36 applicants,"['', ' 5+ years of experience as a Data Engineer, BI Engineer or related field in a company with large, complex data sources.', ' Hands-on experience and advanced knowledge of SQL', ' Enable more efficient adhoc queries & analysis', ' Raise the bar on the importance of data within the operations and analytics teams', ' Solid understanding of data design approaches (and how to best use them)', ' Explore and learn the latest AWS technologies to provide new capabilities and increase efficiencies', ' Experience working with AWS big data technologies (Redshift, S3, EMR)', ' Create a new cluster for Capacity for the US and CA', ' Data modeling to support realtime & batch monitoring & alerts,', 'This Individual Will Be Responsible For Driving', ' Masters in computer science, mathematics, statistics, economics, or other quantitative fields. 5+ years of experience as a Data Engineer, BI Engineer or related field in a company with large, complex data sources. Experience working with AWS big data technologies (Redshift, S3, EMR) Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy. Familiarity with data quality automation.', ' Ensure consistency between various platform, operational, and analytic data sources to enable faster and more efficient detection and resolution of issues', "" Bachelor's degree in Computer Science, Engineering, Mathematics, or a related technical discipline 4+ years of industry experience in software development, data engineering, business intelligence, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets Hands-on experience and advanced knowledge of SQL Experience in Data Modeling, ETL Development, and Data Warehousing Solid understanding of data design approaches (and how to best use them) Knowledge of data management fundamentals and data storage principles Knowledge of distributed systems as it pertains to data storage and computing"", ' Build data pipelines to feed machine learning models and decision engines for real-time and large-scale offline use cases', "" Bachelor's degree in Computer Science, Engineering, Mathematics, or a related technical discipline"", 'Preferred Qualification', ' 4+ years of industry experience in software development, data engineering, business intelligence, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets', ' Knowledge of data management fundamentals and data storage principles', ' Masters in computer science, mathematics, statistics, economics, or other quantitative fields.', ' Experience in Data Modeling, ETL Development, and Data Warehousing', ' Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations', ' Data modeling to support realtime & batch monitoring & alerts, Enable more efficient adhoc queries & analysis Build data pipelines to feed machine learning models and decision engines for real-time and large-scale offline use cases Create a new cluster for Capacity for the US and CA Work closely with analytics and supply chain leaders to scale data Ensure consistency between various platform, operational, and analytic data sources to enable faster and more efficient detection and resolution of issues Explore and learn the latest AWS technologies to provide new capabilities and increase efficiencies Raise the bar on the importance of data within the operations and analytics teams Support the teams through our 3 year planning journey, helping design our future data architecture', 'Basic Qualifications', ' Familiarity with data quality automation.', ' Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy.', ' Knowledge of distributed systems as it pertains to data storage and computing', ' Support the teams through our 3 year planning journey, helping design our future data architecture', ' Work closely with analytics and supply chain leaders to scale data']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer 2,The Kraft Group & Affiliates,"Foxborough, MA",3 days ago,Be among the first 25 applicants,"['', ' This company is an equal opportunity employer. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, veteran status, and other legally protected characteristics.', ' Handle multiple projects and meet deadlines', ' Sitting for extended periods of time', ' SUMMARY ', '  Bachelors degree in Information Systems, Computer Science, or related field  2-3 years of experience working with data using SQL or similar technology  Very high attention to detail  Familiarity with a data integration platform, such as Snaplogic, SSIS, or Informatica  Familiarity with BI Visualization tools, such as Tableau  Ability to work on multiple projects in a fast-paced environment  Strong communication skills to all levels of technical expertise ', 'Other Duties', ' Data Integration  Using cloud technology, combine data from various sources, cloud and on-premise, based on requirements  Perform data cleansing and standardization  Load data into a cloud data warehouse as projects dictate  Using the enterprise ETL tool, create modify, and improve integration pipelines  Translate business requirements into data warehouse pipelines using ETL/ELT methodologies  Extract and load many disparate systems into a centralized data warehouse  ', ' Ongoing Responsibilities  Import and integrate new data sources based on business need  Proactively identify potential data problems, but react as needed to unexpected issues  Improve existing processes to streamline efforts  Handle multiple projects and meet deadlines  Monitor, schedule, and maintain existing integrations  ', ' Load data into a cloud data warehouse as projects dictate', ' Extract and load many disparate systems into a centralized data warehouse', ' Translate business requirements into data warehouse pipelines using ETL/ELT methodologies', ' WORK ENVIRONMENT ', ' Improve existing processes to streamline efforts', ' Import and integrate new data sources based on business need', ' Bachelors degree in Information Systems, Computer Science, or related field', ' Collaborate with analysts to come up with creative solutions to data challenges', ' Ability to work on multiple projects in a fast-paced environment', '  Import and integrate new data sources based on business need  Proactively identify potential data problems, but react as needed to unexpected issues  Improve existing processes to streamline efforts  Handle multiple projects and meet deadlines  Monitor, schedule, and maintain existing integrations ', '  Data Integration  Using cloud technology, combine data from various sources, cloud and on-premise, based on requirements  Perform data cleansing and standardization  Load data into a cloud data warehouse as projects dictate  Using the enterprise ETL tool, create modify, and improve integration pipelines  Translate business requirements into data warehouse pipelines using ETL/ELT methodologies  Extract and load many disparate systems into a centralized data warehouse    Business Intelligence & Data Analysis  Assist with preparing and loading data for Analysis and BI reports and dashboards  Identify opportunities for new data sources  Collaborate with analysts to come up with creative solutions to data challenges    Ongoing Responsibilities  Import and integrate new data sources based on business need  Proactively identify potential data problems, but react as needed to unexpected issues  Improve existing processes to streamline efforts  Handle multiple projects and meet deadlines  Monitor, schedule, and maintain existing integrations   ', ' Very high attention to detail', 'Duties And Responsibilities', ' Assist with preparing and loading data for Analysis and BI reports and dashboards', ' Perform data cleansing and standardization', ' The employee is occasionally required to reach with hands and arms', 'Qualifications', ' Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions', 'Description', '  Assist with preparing and loading data for Analysis and BI reports and dashboards  Identify opportunities for new data sources  Collaborate with analysts to come up with creative solutions to data challenges ', ' Fast paced office environment', ' Strong communication skills to all levels of technical expertise', '  Using cloud technology, combine data from various sources, cloud and on-premise, based on requirements  Perform data cleansing and standardization  Load data into a cloud data warehouse as projects dictate  Using the enterprise ETL tool, create modify, and improve integration pipelines  Translate business requirements into data warehouse pipelines using ETL/ELT methodologies  Extract and load many disparate systems into a centralized data warehouse ', ' Identify opportunities for new data sources', ' Familiarity with a data integration platform, such as Snaplogic, SSIS, or Informatica', ' The employee frequently is required to talk or hear', ' CERTIFICATES, LICENSES, REGISTRATIONS ', ' Monitor, schedule, and maintain existing integrations', 'Supervisory Responsibilities', ' 2-3 years of experience working with data using SQL or similar technology', ' Ability to work nights and weekends as business dictates', ' Using cloud technology, combine data from various sources, cloud and on-premise, based on requirements', ' Using the enterprise ETL tool, create modify, and improve integration pipelines', '  The noise level in the work environment is usually moderate  Fast paced office environment  Ability to work nights and weekends as business dictates ', ' PHYSICAL DEMANDS ', ' Familiarity with BI Visualization tools, such as Tableau', ' SKILLS AND QUALIFICATIONS ', ' Business Intelligence & Data Analysis  Assist with preparing and loading data for Analysis and BI reports and dashboards  Identify opportunities for new data sources  Collaborate with analysts to come up with creative solutions to data challenges  ', ' The noise level in the work environment is usually moderate', ' Dexterity of hands and fingers to operate a computer keyboard, mouse, and other computing equipment', '  Sitting for extended periods of time  Dexterity of hands and fingers to operate a computer keyboard, mouse, and other computing equipment  The employee frequently is required to talk or hear  The employee is occasionally required to reach with hands and arms  Specific vision abilities required by this job include close vision, distance vision, color vision, peripheral vision, depth perception, and ability to adjust focus  Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions ', ' Proactively identify potential data problems, but react as needed to unexpected issues', ' Specific vision abilities required by this job include close vision, distance vision, color vision, peripheral vision, depth perception, and ability to adjust focus']",Associate,Full-time,Information Technology,Construction,2021-03-18 14:34:51
Data Engineer,TekMasters,"McLean, VA",2 days ago,Be among the first 25 applicants,"['', 'Must possess an ACTIVE TS/SCI clearance with a Full Scope Polygraph, sponsorship is not available at this time.', 'Expert level MS Excel, PowerPoint skills', 'Working with a successful team developing designing and developing databases and other applications. ', 'Ability to communicate clearly, concisely, and with technical accuracy.', "" Must possess an ACTIVE TS/SCI clearance with a Full Scope Polygraph, sponsorship is not available at this time. 10 years of related work experience Bachelor's degree or higher 5 years working in the IC with knowledge and experience in one or more intelligence disciplines (INTs) Willingness and ability to travel, CONUS and OCONUS, to conduct assessments. Expert level MS Excel, PowerPoint skills Ability to communicate clearly, concisely, and with technical accuracy. "", 'Employer-paid disability and group life insurance', 'Continued appreciation and recognition throughout your service, not just when you retire', 'TekMasters’ Impact', 'Competitive salaries with weekly pay', 'Developing custom database scripts. ', 'Dollar for Dollar 401k match with immediate investment', 'TekMasters’ goal', 'What You Will Be Doing As a Data Engineer', 'Industry best Medical options, including employer-paid options', ' Working with a successful team developing designing and developing databases and other applications.  Developing custom database scripts.  Organizing data while watching for updates and errors that may impact the customer.  ', 'Must Haves', 'Willingness and ability to travel, CONUS and OCONUS, to conduct assessments.', ""Bachelor's degree or higher"", '10 years of related work experience', '5 years working in the IC with knowledge and experience in one or more intelligence disciplines (INTs)', 'Organizing data while watching for updates and errors that may impact the customer. ', ' Industry best Medical options, including employer-paid options Dollar for Dollar 401k match with immediate investment Employer-paid disability and group life insurance Competitive salaries with weekly pay Continued appreciation and recognition throughout your service, not just when you retire ']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer II (Remote),Black Knight,"Philadelphia, PA",1 month ago,Be among the first 25 applicants,"['', 'Knowledge of financial services industry a plus', 'Outstanding verbal and written communication skills to technical and non-technical audiences of various levels in the organization (e.g., executive, management, individual contributors)', 'Ability to estimate work effort for project sub-plans or small projects and ensure the project is successfully completed', 'EDUCATIONAL GUIDELINES', 'Experience with AWS cloud services: S3, Glue, Athena, Kinesis, DynamoDB, Sagemaker, EMR, RDS, Redshift a plus', 'Experience building production quality cloud products preferred', 'Experience building and optimizing ‘big data’ data pipelines, architectures and data sets', 'JOB FAMILY LEVEL', 'Agile (Scaled Agile Framework)', 'Monitor system operation to detect potential problems.', 'Experience building and optimizing ‘big data’ data pipelines, architectures and data setsExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvementAdvanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databasesExperience with build processes supporting data transformation, data structures, metadata, dependency and workload managementExperience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.Experience with big data tools such as Hadoop, Spark, Kafka, etc. strongly preferredExperience with relational SQL and NoSQL databases, including Postgres and Cassandra preferredExperience with data pipeline and workflow management tools such as Azkaban, Luigi, Airflow, etc.Experience with AWS cloud services: S3, Glue, Athena, Kinesis, DynamoDB, Sagemaker, EMR, RDS, Redshift a plusExperience with stream-processing systems such as Storm, Spark-Streaming, etc. a plusStrong analytic skills related to working with unstructured datasetsA successful history of manipulating, processing and extracting value from large disconnected datasetsWorking knowledge of message queuing, stream processing, and highly scalable ‘big data’ data storesExperience building production quality cloud products preferredExperience with AWS , DevOPs , CI/CD preferredKnowledge of financial services industry a plusKnowledge of banking practices, regulations and operations within the assigned line(s) of business a plusOutstanding verbal and written communication skills to technical and non-technical audiences of various levels in the organization (e.g., executive, management, individual contributors)Excellent analytical, decision-making, problem-solving, team, and time management skillsAbility to estimate work effort for project sub-plans or small projects and ensure the project is successfully completedPositive outlook, strong work ethic, and responsive to internal and external clients and contactsWillingly and successfully fulfills the role of teacher, mentor and coach', 'Machine Learning ( Natural Language Processing ,Vision , Classification , Search)', 'Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement', 'Cloud (AWS)', 'Designs scalable, highly available, fault tolerant and resilient data processing infrastructure, assembled from microservices in a Continuous Integration Continuous Delivery environment.', 'Time Type:', 'Experience with big data tools such as Hadoop, Spark, Kafka, etc. strongly preferred', 'Experience with build processes supporting data transformation, data structures, metadata, dependency and workload management', 'Experience with data pipeline and workflow management tools such as Azkaban, Luigi, Airflow, etc.', 'DevOps ( Infrastructure as Code , Continuous Integration and Continuous Delivery)', 'Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores', 'General Knowledge, Skills & Abilities', 'Experience with relational SQL and NoSQL databases, including Postgres and Cassandra preferred', 'Willingly and successfully fulfills the role of teacher, mentor and coach', 'General Duties & Responsibilities', 'Strong analytic skills related to working with unstructured datasets', 'Participates in project meetings with other technical staff, business owners and subject matter experts.', 'Job Description:', 'Performs additional related duties as assigned.', 'Languages (Java , AngularJS , Python)', 'Job Family Description', 'Experience with AWS , DevOPs , CI/CD preferred', 'Interacts with product managers and/or users to define system requirements and/or necessary modifications.', 'Verify stability, interoperability, portability, security, or scalability of system architecture.', 'Position:', 'Agile (Scaled Agile Framework)Machine Learning ( Natural Language Processing ,Vision , Classification , Search)DevOps ( Infrastructure as Code , Continuous Integration and Continuous Delivery)Behavioral Driven DevelopmentDesign and ArchitectureCloud (AWS)Languages (Java , AngularJS , Python)', 'Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases', 'Excellent analytical, decision-making, problem-solving, team, and time management skills', 'Design and Architecture', 'Location:', 'Behavioral Driven Development', 'Builds streaming and batch data extraction transformation and load processes using AWS Serverless technologies.', 'A successful history of manipulating, processing and extracting value from large disconnected datasets', 'Knowledge of banking practices, regulations and operations within the assigned line(s) of business a plus', 'Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.', 'Experience with stream-processing systems such as Storm, Spark-Streaming, etc. a plus', 'Reviews application in progress of development to ensure compliance with overall design parameters and corporate development standards.', 'Participates in project meetings with other technical staff, business owners and subject matter experts.Designs scalable, highly available, fault tolerant and resilient data processing infrastructure, assembled from microservices in a Continuous Integration Continuous Delivery environment.Builds streaming and batch data extraction transformation and load processes using AWS Serverless technologies.Interacts with product managers and/or users to define system requirements and/or necessary modifications.Assesses and develops design requirements for the project and communicates in writing or in meetings with development team while assessing detailed specifications against design requirements.Develops and/or reviews development of test protocols for testing application before user acceptance.Reviews application in progress of development to ensure compliance with overall design parameters and corporate development standards.Verify stability, interoperability, portability, security, or scalability of system architecture.Monitor system operation to detect potential problems.Document design specifications, installation instructions, and other system-related information.Performs additional related duties as assigned.', 'Document design specifications, installation instructions, and other system-related information.', 'Assesses and develops design requirements for the project and communicates in writing or in meetings with development team while assessing detailed specifications against design requirements.', ' Black Knight is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, disability, age, and protected veteran or military family status. Our employees’ diversity is our strength, and when we embrace our differences, it makes us better and brighter. Black Knight’s commitment to inclusion is at the core of who we are, and motivates us in how we do business each and every day. ', 'Develops and/or reviews development of test protocols for testing application before user acceptance.', 'Positive outlook, strong work ethic, and responsive to internal and external clients and contacts']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,PEAK6 ,"Dallas, TX",2 weeks ago,37 applicants,"['', 'Proficient with version control systems, ideally GitHub', ' Bachelor’s degree in Computer Science, Computer Engineering or a similar field 2+ years of Microsoft SQL Development Experience 2+ years of experience in data warehousing and ETL processes 1+ years of experience with Python Advanced problem-solving, debugging, and troubleshooting skills Excellent client support skills Proficient with version control systems, ideally GitHub Ability and willingness to learn new things (languages, tools, frameworks) quickly Experience with the Microsoft BI Stack (SSIS, SSAS, SSRS) preferred Experience with NoSQL technologies preferred Financial services background preferred ', 'We see tech differently. You’ll work with people who are leaders in the tech industry. We are passionate engineers dedicated to finding new and different ways to use technology to solve our customers’ problems.', 'And a few reasons why you may not like working for us:', 'You’re a tech snob. We deal with all technology – Linux and Windows; .NET, Java, and Python; SQL and noSQL (mongoDB); RabbitMQ and other open source tools and libraries. We love technology and want to work with all of it. If you’re wed to a particular tech, you may not like working for us.', 'Be a data wizard. Create, manipulate, access, and deliver data in the most efficient ways possible to business users, end-customers, 3rd-party vendors, and application developers', 'We’re a leader in the space. Apex is recognized for disrupting the financial services industry, enabling fintech standouts like Stash, Webull and Betterment. We’ve got an amazing track record of success and we foster ongoing innovation. So you get all the benefits of a proven, growing company, while enjoying a very entrepreneurial culture', 'Experience with NoSQL technologies preferred', 'What You’ll Do All Day', 'Strives for frictionless IT – You understand the importance of building great partnerships. You promote a seamless, smooth, user friendly and reliable environment.', 'Is passionate', 'Show off your work. ', 'Live our culture. Embrace Apex’s values as our differentiator and be an example of them every day.', 'Bachelor’s degree in Computer Science, Computer Engineering or a similar field', '1+ years of experience with Python', 'Excellent client support skills', 'You’re not the collaborative type', 'You’re a tech snob', 'We see tech differently.', 'Is collaborative. You’re excited to work with fellow engineers and big thinkers. You know how to collaborate not only within the department, but also across the organization', 'Make us better. Identify, advocate for, and implement solutions to improve performance and efficiencies across systems, APIs, and overnight batch processing. Develop quality code that is maintainable and avoids problems. Promote a culture for effective documentation and lessons-learned.', 'We’re Looking For Someone Who', 'Is passionate. You have a genuine passion for technology. You love using technology differently to maximize opportunity and impact for customers and you have a way of bringing out that same fire in the people you work with', 'See the data.', 'Your work will have immediate impact. You’ll be able to see your direct impact on our tech department, our business, and with our clients. You won’t be just another talented engineer.', ' Is passionate. You have a genuine passion for technology. You love using technology differently to maximize opportunity and impact for customers and you have a way of bringing out that same fire in the people you work with Is motivated. You’re driven to be the best – whether that’s decreasing system down time or making an innovative change to “how it’s always been done” resulting in a more efficient way of supporting the customer. You challenge yourself by setting goals and exceeding them Is collaborative. You’re excited to work with fellow engineers and big thinkers. You know how to collaborate not only within the department, but also across the organization Wants to make an impact. You’re looking to do amazing work. You value preventing problems from occurring over being caught in the chaos zone putting out fires and looking for the “hero” spotlight Strives for frictionless IT – You understand the importance of building great partnerships. You promote a seamless, smooth, user friendly and reliable environment. ', ' We’re a leader in the space. Apex is recognized for disrupting the financial services industry, enabling fintech standouts like Stash, Webull and Betterment. We’ve got an amazing track record of success and we foster ongoing innovation. So you get all the benefits of a proven, growing company, while enjoying a very entrepreneurial culture We see tech differently. You’ll work with people who are leaders in the tech industry. We are passionate engineers dedicated to finding new and different ways to use technology to solve our customers’ problems. Your work will have immediate impact. You’ll be able to see your direct impact on our tech department, our business, and with our clients. You won’t be just another talented engineer. ', 'Wants to make an impact', 'Be a great team member.', 'A Few Reasons Why You Might Love Us', 'Ability and willingness to learn new things (languages, tools, frameworks) quickly', 'Is motivated', 'Learn the data.', 'Live our culture.', '2+ years of Microsoft SQL Development Experience', 'Wants to make an impact. You’re looking to do amazing work. You value preventing problems from occurring over being caught in the chaos zone putting out fires and looking for the “hero” spotlight', 'Be a data wizard.', 'We’re a leader in the space. ', 'Advanced problem-solving, debugging, and troubleshooting skills', 'You’re not the collaborative type. We work together to ensure the best possible solutions for our customers. We think two brains are better than one so we do most of our work together. Team work makes the dream work on this team.', 'Financial services background preferred', 'The Skills You’ll Need To Succeed', 'Learn the data. Utilize excellent analytical and problem solving techniques to understand our complex data structures and put the data to work. Participate in all phases of the development process.', 'See the data. Build reports, analytics and visualizations to help meet business initiatives and make decisions.', 'Experience with the Microsoft BI Stack (SSIS, SSAS, SSRS) preferred', 'Strives for frictionless IT ', 'Work the data. Create processes to load, transform, and deliver data to business users, end-customers, 3rd party vendors, and application developers.', 'Make us better.', 'Is motivated. You’re driven to be the best – whether that’s decreasing system down time or making an innovative change to “how it’s always been done” resulting in a more efficient way of supporting the customer. You challenge yourself by setting goals and exceeding them', 'Is collaborative. ', '2+ years of experience in data warehousing and ETL processes', 'Show off your work. Embrace transparency and share metrics around our levels of service with the rest of the company and our customers.', 'Be a great team member. Work as a member of an agile software development team to rapidly produce software. Balance both project-based and day-to-day support tasks. ', 'Work the data.', 'Your work will have immediate impact.', ' You don’t like change. This is not a job for someone who likes “predictable.” The job is based on the unknown which inevitably means change. If you like to know what you’re going to do every day, you may not like working on this team. You have to go with the flow here. You’re not the collaborative type. We work together to ensure the best possible solutions for our customers. We think two brains are better than one so we do most of our work together. Team work makes the dream work on this team. You’re a tech snob. We deal with all technology – Linux and Windows; .NET, Java, and Python; SQL and noSQL (mongoDB); RabbitMQ and other open source tools and libraries. We love technology and want to work with all of it. If you’re wed to a particular tech, you may not like working for us. ', 'You don’t like change. This is not a job for someone who likes “predictable.” The job is based on the unknown which inevitably means change. If you like to know what you’re going to do every day, you may not like working on this team. You have to go with the flow here.', ' Be a data wizard. Create, manipulate, access, and deliver data in the most efficient ways possible to business users, end-customers, 3rd-party vendors, and application developers Work the data. Create processes to load, transform, and deliver data to business users, end-customers, 3rd party vendors, and application developers. Learn the data. Utilize excellent analytical and problem solving techniques to understand our complex data structures and put the data to work. Participate in all phases of the development process. See the data. Build reports, analytics and visualizations to help meet business initiatives and make decisions. Make us better. Identify, advocate for, and implement solutions to improve performance and efficiencies across systems, APIs, and overnight batch processing. Develop quality code that is maintainable and avoids problems. Promote a culture for effective documentation and lessons-learned. Be a great team member. Work as a member of an agile software development team to rapidly produce software. Balance both project-based and day-to-day support tasks.  Show off your work. Embrace transparency and share metrics around our levels of service with the rest of the company and our customers. Live our culture. Embrace Apex’s values as our differentiator and be an example of them every day. ', 'You don’t like change']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Senior Data Engineer,iQmetrix,"Davidson County, NC",3 days ago,Be among the first 25 applicants,"['', ' Experience building end-to-end data pipelines capable of handling high throughput from scratch.', ' Experience in Data Modeling, ETL Development, and Data Warehousing. Experience dealing with Highly Scalable Data systems like Apache Hadoop, Azure Data Lake, Apache Spark or others. As well as relational or NoSQL languages. Experience using business intelligence reporting tools (Power BI, Tableau, SI Sense, Looker, etc.). Experience using big data technologies (Hadoop, Spark, Hive, Hbase, etc.). Experience with CDC logging, databricks, and Kafka. 2+ years Public Cloud Development Experience. Knowledge of Data Management fundamentals and Data Storage principles. Experience coding and automating processes using Python or R. Strong customer focus, ownership, urgency, and drive. Excellent communication skills and the ability to work well in a team. Effective analytical, troubleshooting, and problem-solving skills. Experience building end-to-end data pipelines capable of handling high throughput from scratch.', ' 3+ years of experience using big data technologies including Hadoop and Spark.', ' Identify ways to improve data reliability, system efficiency and quality.', ' Experience in Data Modeling, ETL Development, and Data Warehousing.', ' Experience using big data technologies (Hadoop, Spark, Hive, Hbase, etc.).', ' Experience with clustered column store data warehouses such as Synapse and Redshift.', ' Experience providing technical leadership and educating other engineers for best practices on data engineering.', ' Develop, construct, test and maintain architecture.', ' Working with the iQmetrix data science team.', "" Communication skills and emotional intelligence are key. iQmetrix is looking for someone who can listen to others and take multiple perspectives into consideration when making decisions decision. People who love what they do; they're passionate about their work. With interpersonal skills, a team member can build strong relationships from scratch. We love that. Be humble. We're on the hunt for, someone who is not afraid to admit their mistakes and work towards preventing them in the future. Thinks about the health of the company before personal agendas. Profit and loss are major influencers on organizational decisions and our team must have the big picture in mind-always. Being a self-starter is a huge asset. This means being able to dive into the work without fear of making a mistake. Flexibility, and the ability to adapt to an evolving environment, will go a long way at iQmetrix. Keep in mind: the iQmetrix team works in open, collaborative office environments. Successful people can thrive in this style of workspace. People who can stick by their decisions, since they did what they thought was best, will fit well into the iQmetrix team. These people are also not"", ' Experience coding and automating processes using Python or R.', "" People who love what they do; they're passionate about their work."", ' 2+ years Public Cloud Development Experience.', ' Excellent communication skills and the ability to work well in a team.', ' Communication skills and emotional intelligence are key. iQmetrix is looking for someone who can listen to others and take multiple perspectives into consideration when making decisions decision.', ' People who can stick by their decisions, since they did what they thought was best, will fit well into the iQmetrix team. These people are also not', ' Experience using business intelligence reporting tools (Power BI, Tableau, SI Sense, Looker, etc.).', ' Collaborating with architecture to develop pipelines for predictive and prescriptive modelling.', ' Facilitate deeper analysis for reporting.', ' Background in Big Data, non-relational databases, Machine Learning and Data Mining is a plus.', ' Fully understanding client challenges and business needs so that these can be considered during the development of this new platform.', 'Accountabilities', ' Experience with CDC logging, databricks, and Kafka.', ' Thinks about the health of the company before personal agendas. Profit and loss are major influencers on organizational decisions and our team must have the big picture in mind-always.', ' Fully understanding client challenges and business needs so that these can be considered during the development of this new platform. Develop, construct, test and maintain architecture. Align the architecture to business and client requirements. Develop data set pipelines and processes including data integrity and quality testing. Identify ways to improve data reliability, system efficiency and quality. Facilitate deeper analysis for reporting. Working with the iQmetrix data science team. Collaborating with architecture to develop pipelines for predictive and prescriptive modelling.', ' Knowledge of Data Management fundamentals and Data Storage principles.', ' Align the architecture to business and client requirements.', ' Strong customer focus, ownership, urgency, and drive.', ' Masters in computer science, mathematics, statistics, economics, or other quantitative fields.', 'Basic Qualifications', ' 3+ years of experience using big data technologies including Hadoop and Spark. Experience working with Microsoft big data technologies (Synapse, Data Factory, data lake). Experience with clustered column store data warehouses such as Synapse and Redshift. Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy. Experience providing technical leadership and educating other engineers for best practices on data engineering. Background in Big Data, non-relational databases, Machine Learning and Data Mining is a plus. Masters in computer science, mathematics, statistics, economics, or other quantitative fields. Experience with managing a team.', ' Experience dealing with Highly Scalable Data systems like Apache Hadoop, Azure Data Lake, Apache Spark or others. As well as relational or NoSQL languages.', ' Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy.', "" Be humble. We're on the hunt for, someone who is not afraid to admit their mistakes and work towards preventing them in the future."", ' Develop data set pipelines and processes including data integrity and quality testing.', ' Effective analytical, troubleshooting, and problem-solving skills.', ' Experience working with Microsoft big data technologies (Synapse, Data Factory, data lake).', ' Experience with managing a team.', ' Being a self-starter is a huge asset. This means being able to dive into the work without fear of making a mistake.', 'Preferred Qualifications', ' Flexibility, and the ability to adapt to an evolving environment, will go a long way at iQmetrix. Keep in mind: the iQmetrix team works in open, collaborative office environments. Successful people can thrive in this style of workspace.', ' With interpersonal skills, a team member can build strong relationships from scratch. We love that.']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Turnberry Solutions,"Philadelphia, PA",1 month ago,Be among the first 25 applicants,"['', 'Interview Logistics: ', '3+ years of experience with Python', '1+ years of experience with AWS EMR, AWS S3 service.', 'Eager to both review peer code and have your code reviewed', 'Leverage distributed computing and serverless architecture such as AWS EMR & AWS Lambda, to develop pipelines for transforming data', 'Comfortable on the command line and consider it an essential tool', 'Ingesting and managing billions of healthcare records from a wide variety of partners', ' 3+ years of work experience 3+ years of experience with Python 3+ years of experience with PySpark and Spark-SQL (writing, testing, debugging spark routines) 1+ years of experience with AWS EMR, AWS S3 service. Comfortable using AWS CLI and boto3 Comfortable using *nix command line (shell scripting, AWK, SED) Experience with MySQL and Postgres ', 'Building a culture that supports rapid iteration and new possibilities', ' Work with the team to load data into data warehouse Troubleshoot and resolve issues relating to data integrity Help establish procedures and best practices for transforming and storing data Lead requirements gathering around data pipeline automation improvements Work with some of the most exciting open-source tools like Spark, Hadoop, Docker, Airflow, Zeppelin Leverage distributed computing and serverless architecture such as AWS EMR & AWS Lambda, to develop pipelines for transforming data Marvel at the speed with which your creation makes it into production Research and implement new technologies with a team of developers to execute strategies and implement solutions Produce peer reviewed quality software Solve complex problems related to the real-time discovery of large data ', 'Experience with MySQL and Postgres', 'Help establish procedures and best practices for transforming and storing data', 'hysical Environment and Working Conditions:', 'Required Skills And Experience', 'Work with some of the most exciting open-source tools like Spark, Hadoop, Docker, Airflow, Zeppelin', 'Business Problem', '3+ years of work experience', 'Required Skills Set: ', 'Empowering clients with highly rewarding data discovery and licensing tools', ' Empowering clients with highly rewarding data discovery and licensing tools Ingesting and managing billions of healthcare records from a wide variety of partners Standardizing on common data models across data types Orchestrating an industry-leading HIPAA privacy layer Innovating our proprietary de-identification and data science algorithms Building a culture that supports rapid iteration and new possibilities ', 'Produce peer reviewed quality software', 'Comfortable using AWS CLI and boto3', ' Experience with Apache Airflow Experience with Apache Zeppelin Experience with healthcare data ', 'Additional Preferred Skills', 'Data Engineer in Philadelphia, PA 19103', 'Experience with healthcare data', 'Physical Environment And Working Conditions', ""Confident in SQL, you know it, write smart queries, it's no big deal"", 'Data driven, testing and measuring as much as you can', 'Research and implement new technologies with a team of developers to execute strategies and implement solutions', 'Experience with Apache Airflow', 'Work with the team to load data into data warehouse', 'Marvel at the speed with which your creation makes it into production', 'Experience with Apache Zeppelin', 'Troubleshoot and resolve issues relating to data integrity', 'Experienced in writing scalable applications on distributed architectures', 'Project Description:', 'Solve complex problems related to the real-time discovery of large data', "" Experienced in writing scalable applications on distributed architectures Data driven, testing and measuring as much as you can Eager to both review peer code and have your code reviewed Comfortable on the command line and consider it an essential tool Confident in SQL, you know it, write smart queries, it's no big deal "", 'Job Description', 'Innovating our proprietary de-identification and data science algorithms', 'Education Required:', 'Years of Experience:', '3+ years of experience with PySpark and Spark-SQL (writing, testing, debugging spark routines)', 'Standardizing on common data models across data types', 'Orchestrating an industry-leading HIPAA privacy layer', 'Comfortable using *nix command line (shell scripting, AWK, SED)', 'Lead requirements gathering around data pipeline automation improvements']",Mid-Senior level,Full-time,Other,Information Technology and Services,2021-03-18 14:34:51
Data Engineer I,MediQuant,"Brecksville, OH",2 weeks ago,Be among the first 25 applicants,"['', 'Ability to solve problems using logical thought processes and devising creative solutions', 'Additional Eligibility Qualifications', ' Uses MQ scoping language from the contract and determines if the dataset delivered falls within the expected scope, or if the dataset delivered does not match the expected deliverable.', ' Manages a fluctuating workload and prioritizes workload during times of peak demand and conflicting priorities. Knows when to seek assistance to ensure deadlines are met and quality is delivered', ' Effectively checks work for accuracy, giving specific attention to error zones. Takes ownership to ensure own work is error-free. Conducts independent research when issues are identifies; and problem-solves. Recognizes when additional help is needed and proactively reaches out to mentors/leaders for help.', "" Extracts all data from legacy database systems. (*) Under the guidance of DE II or DE III, can establish connectivity to client legacy database Establishes basic ODBC connectivity to various database types. Kicks-off extract process and monitors the process during runtime Identifies extract errors and performs simple troubleshooting steps. Uses Structured Query Language (SQL) commands needed to browse the database and view data before extraction Compares layout of the legacy database with the layout of the extraction to ensure that all tables and expected row counts match Performs quality assurance checks on extraction to ensure work is accurate and reliable before it is passed on to the next team Loads various discrete and non-discrete data sources into a Microsoft SQL Server environment. (*) Under guidance of DE II or DE III, loads flat file data into a SQL Server Database. Exports large directories of files, and extracts metadata stored in the directory listing. Resulting output is in a tabular format, linking the metadata to the corresponding file pathii. Using scripting tools (i.e. Python, Java, C#, PowerShell/Bash, etc.), writes scripts to read, write, copy, rename, move, sort, or otherwise process many non-discrete data files. Reviews incoming datasets to check for errors and ensure validity. Acts as a second validator to review database layouts, and verifies that no data is missing from the legacy system. Uses MQ scoping language from the contract and determines if the dataset delivered falls within the expected scope, or if the dataset delivered does not match the expected deliverable. Collaborates with both the Client/Extractor and the DM/IC to ensure a smooth transition of the data between the extraction and the modeling Responsible for the location of the data. Communicates clearly to the team as the location of the data changes during the review process Runs detailed comparisons between table lists from the legacy system and the extracted dataset. Works with other team members to compare to past extract projects throughout MediQuant history. Troubleshooting and Problem Solving. (*) - Identifies problems and proactively intervenes to mitigate or eliminate potential for negative impact. Effectively checks work for accuracy, giving specific attention to error zones. Takes ownership to ensure own work is error-free. Conducts independent research when issues are identifies; and problem-solves. Recognizes when additional help is needed and proactively reaches out to mentors/leaders for help.ii. Uses strong data analysis skills to debug data anomaliesiii. Applies technical knowledge and seeks to fully understand the client’s expectations by asking questions. Works collaboratively with project teams to innovate and find solutions to complex issues. Documents issues and resolutions for and “lessons learned” to avoid repeating. Liaise with application/tool developers, PMO, IT, or other sources to create tools needed to streamline workflows Organization and Time Management. (*) - Handles multiple projects and tasks and prioritizes deadlines. Identifies and utilizes all resources available when priorities conflict or when external challenges are lining up against the deadline. Must be able to clearly communicate with the PM and Lead, at the time conflicting priorities threaten delivery of scheduled tasks. Manages a fluctuating workload and prioritizes workload during times of peak demand and conflicting priorities. Knows when to seek assistance to ensure deadlines are met and quality is deliveredii. Maintains organized notes and uses project management tools to keep project team updated with the latest statuses and roadblocksQualificationsCompetenciesCollaboration SkillsOrganizational SkillsInterpersonal and Communication SkillsCritical ThinkingProblem Solving/AnalysisTeam PlayerSelf-Motivated/Self-StarterTime ManagementRequired Education And ExperienceBachelor’s degree in Computer Science, Computer Engineering, Software Engineering, preferred. High School Diploma or GED with 1-3 years of applicable experience in computer science related field required.Programming Skills Basic knowledge of SQL Scripting skills, able to query and transform data Proficiency in one of the following programming languages: Python, Java, C# Familiar with PowerShell/Bash scriptingExcellent organizational and documentation skillsAble to understand database catalog, schema, and table layoutsBasic understanding of healthcare data terminology (Clinical, Financial, Accounts, Visits, Charges, etc.) to help with scope review and understanding the datasets that are under review.Additional Eligibility Qualifications1-3 years’ experience in Healthcare IT or clinical environments, preferredPast experience extracting Healthcare data, preferredAbility to solve problems using logical thought processes and devising creative solutionsHigh attention to accuracy and detailAbility and desire to work in a strong team cultureKnowledge with HIPAA regulations involving PHI and HITECHStrong customer service and information technology acumenOther Duties. Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.Work Authorization/Security Clearance (if applicable). In compliance with Federal employment laws, MediQuant will verify the identity and employment authorization of each person hired.AAP/EEO Statement. MediQuant, Inc. is an equal opportunity employer.Work Environment. This job operates in a professional office environment. This role routinely uses standard office equipment such as computers, phones, photocopiers, filing cabinets and fax machines. While performing some duties of this job, the employee is exposed to dust or other airborne particles. The noise level on the job site is typical of an office environment.Physical Demands. While performing the duties of this job, the employee is regularly required to talk or hear. The employee frequently is required to sit for long periods of time, stand; walk; use hands to finger, handle or feel; and reach with hands and arms. The employee is occasionally required to climb or balance; and stoop, kneel, crouch or crawl. The employee must occasionally lift and/or move up to 10 pounds and occasionally lift and/or move up to 25 pounds. Specific vision abilities required by this job include close vision, distance vision, color vision, peripheral vision, depth perception and ability to adjust focus.Position Type and Expected Hours of Work. This is a full-time position, and hours of work and days are typically Monday through Friday, 8:30 a.m. to 5 p.m. Some flexibility in hours is allowed, but the employee must be available during the “core” work hours of 9:00 a.m. to 3:30 p.m. and must work 40 hours each week to maintain full-time status. Occasional evening and weekend work may be required as job duties demand.Travel. Little to no travel is expected for this position.Founded in 1999, MediQuant's initial objective was to provide innovative, dependable revenue cycle technology to the healthcare market. Led by seasoned healthcare and IT management professionals, the company's purpose has widened in scope since its early days. Today, MediQuant provides comprehensive data life cycle management technology, working in close partnership with healthcare providers and other vendors to optimize data management, including revenue cycles.The MediQuant team, a core group composed of highly experienced IT professionals supported by knowledgeable administrative staff, boasts extensive experience in healthcare software development, EDI, clinical services, data management and other specialties."", 'Critical Thinking', 'Collaboration Skills', 'High attention to accuracy and detail', 'AAP/EEO Statement. ', 'Position Summary', ' Acts as a second validator to review database layouts, and verifies that no data is missing from the legacy system.', '1-3 years’ experience in Healthcare IT or clinical environments, preferred', "" Organization and Time Management. (*) - Handles multiple projects and tasks and prioritizes deadlines. Identifies and utilizes all resources available when priorities conflict or when external challenges are lining up against the deadline. Must be able to clearly communicate with the PM and Lead, at the time conflicting priorities threaten delivery of scheduled tasks. Manages a fluctuating workload and prioritizes workload during times of peak demand and conflicting priorities. Knows when to seek assistance to ensure deadlines are met and quality is deliveredii. Maintains organized notes and uses project management tools to keep project team updated with the latest statuses and roadblocksQualificationsCompetenciesCollaboration SkillsOrganizational SkillsInterpersonal and Communication SkillsCritical ThinkingProblem Solving/AnalysisTeam PlayerSelf-Motivated/Self-StarterTime ManagementRequired Education And ExperienceBachelor’s degree in Computer Science, Computer Engineering, Software Engineering, preferred. High School Diploma or GED with 1-3 years of applicable experience in computer science related field required.Programming Skills Basic knowledge of SQL Scripting skills, able to query and transform data Proficiency in one of the following programming languages: Python, Java, C# Familiar with PowerShell/Bash scriptingExcellent organizational and documentation skillsAble to understand database catalog, schema, and table layoutsBasic understanding of healthcare data terminology (Clinical, Financial, Accounts, Visits, Charges, etc.) to help with scope review and understanding the datasets that are under review.Additional Eligibility Qualifications1-3 years’ experience in Healthcare IT or clinical environments, preferredPast experience extracting Healthcare data, preferredAbility to solve problems using logical thought processes and devising creative solutionsHigh attention to accuracy and detailAbility and desire to work in a strong team cultureKnowledge with HIPAA regulations involving PHI and HITECHStrong customer service and information technology acumenOther Duties. Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.Work Authorization/Security Clearance (if applicable). In compliance with Federal employment laws, MediQuant will verify the identity and employment authorization of each person hired.AAP/EEO Statement. MediQuant, Inc. is an equal opportunity employer.Work Environment. This job operates in a professional office environment. This role routinely uses standard office equipment such as computers, phones, photocopiers, filing cabinets and fax machines. While performing some duties of this job, the employee is exposed to dust or other airborne particles. The noise level on the job site is typical of an office environment.Physical Demands. While performing the duties of this job, the employee is regularly required to talk or hear. The employee frequently is required to sit for long periods of time, stand; walk; use hands to finger, handle or feel; and reach with hands and arms. The employee is occasionally required to climb or balance; and stoop, kneel, crouch or crawl. The employee must occasionally lift and/or move up to 10 pounds and occasionally lift and/or move up to 25 pounds. Specific vision abilities required by this job include close vision, distance vision, color vision, peripheral vision, depth perception and ability to adjust focus.Position Type and Expected Hours of Work. This is a full-time position, and hours of work and days are typically Monday through Friday, 8:30 a.m. to 5 p.m. Some flexibility in hours is allowed, but the employee must be available during the “core” work hours of 9:00 a.m. to 3:30 p.m. and must work 40 hours each week to maintain full-time status. Occasional evening and weekend work may be required as job duties demand.Travel. Little to no travel is expected for this position.Founded in 1999, MediQuant's initial objective was to provide innovative, dependable revenue cycle technology to the healthcare market. Led by seasoned healthcare and IT management professionals, the company's purpose has widened in scope since its early days. Today, MediQuant provides comprehensive data life cycle management technology, working in close partnership with healthcare providers and other vendors to optimize data management, including revenue cycles.The MediQuant team, a core group composed of highly experienced IT professionals supported by knowledgeable administrative staff, boasts extensive experience in healthcare software development, EDI, clinical services, data management and other specialties."", ' Collaborates with both the Client/Extractor and the DM/IC to ensure a smooth transition of the data between the extraction and the modeling', 'Collaboration SkillsOrganizational SkillsInterpersonal and Communication SkillsCritical ThinkingProblem Solving/AnalysisTeam PlayerSelf-Motivated/Self-StarterTime Management', 'POSITION TITLE: Data Engineer I', ' Under the guidance of DE II or DE III, can establish connectivity to client legacy database Establishes basic ODBC connectivity to various database types. Kicks-off extract process and monitors the process during runtime Identifies extract errors and performs simple troubleshooting steps. Uses Structured Query Language (SQL) commands needed to browse the database and view data before extraction Compares layout of the legacy database with the layout of the extraction to ensure that all tables and expected row counts match Performs quality assurance checks on extraction to ensure work is accurate and reliable before it is passed on to the next team', ' Identifies and utilizes all resources available when priorities conflict or when external challenges are lining up against the deadline. Must be able to clearly communicate with the PM and Lead, at the time conflicting priorities threaten delivery of scheduled tasks.', 'Ability and desire to work in a strong team culture', 'Work Environment. ', ' Documents issues and resolutions for and “lessons learned” to avoid repeating.', ' Basic knowledge of SQL Scripting skills, able to query and transform data', ' Extracts all data from legacy database systems. (*) Under the guidance of DE II or DE III, can establish connectivity to client legacy database Establishes basic ODBC connectivity to various database types. Kicks-off extract process and monitors the process during runtime Identifies extract errors and performs simple troubleshooting steps. Uses Structured Query Language (SQL) commands needed to browse the database and view data before extraction Compares layout of the legacy database with the layout of the extraction to ensure that all tables and expected row counts match Performs quality assurance checks on extraction to ensure work is accurate and reliable before it is passed on to the next team', ' Familiar with PowerShell/Bash scripting', ' Works collaboratively with project teams to innovate and find solutions to complex issues. Documents issues and resolutions for and “lessons learned” to avoid repeating. Liaise with application/tool developers, PMO, IT, or other sources to create tools needed to streamline workflows', ' Works collaboratively with project teams to innovate and find solutions to complex issues.', 'Programming Skills', 'Travel. ', 'Extracts all data from legacy database systems. (*)', ' Runs detailed comparisons between table lists from the legacy system and the extracted dataset. Works with other team members to compare to past extract projects throughout MediQuant history.', 'Physical Demands. ', 'Qualifications', ' Conducts independent research when issues are identifies; and problem-solves. Recognizes when additional help is needed and proactively reaches out to mentors/leaders for help.', 'Required Education And Experience', ' Loads various discrete and non-discrete data sources into a Microsoft SQL Server environment. (*) Under guidance of DE II or DE III, loads flat file data into a SQL Server Database. Exports large directories of files, and extracts metadata stored in the directory listing. Resulting output is in a tabular format, linking the metadata to the corresponding file pathii. Using scripting tools (i.e. Python, Java, C#, PowerShell/Bash, etc.), writes scripts to read, write, copy, rename, move, sort, or otherwise process many non-discrete data files.', ' Reviews incoming datasets to check for errors and ensure validity. Acts as a second validator to review database layouts, and verifies that no data is missing from the legacy system. Uses MQ scoping language from the contract and determines if the dataset delivered falls within the expected scope, or if the dataset delivered does not match the expected deliverable. Collaborates with both the Client/Extractor and the DM/IC to ensure a smooth transition of the data between the extraction and the modeling Responsible for the location of the data. Communicates clearly to the team as the location of the data changes during the review process Runs detailed comparisons between table lists from the legacy system and the extracted dataset. Works with other team members to compare to past extract projects throughout MediQuant history.', 'Interpersonal and Communication Skills', 'Self-Motivated/Self-Starter', ' Uses Structured Query Language (SQL) commands needed to browse the database and view data before extraction', 'Team Player', 'Position Type and Expected Hours of Work. ', ' Compares layout of the legacy database with the layout of the extraction to ensure that all tables and expected row counts match', 'Past experience extracting Healthcare data, preferred', 'JOB DUTIES and ESSENTIAL FUNCTIONS: ', ' Under the guidance of DE II or DE III, can establish connectivity to client legacy database', ' Proficiency in one of the following programming languages: Python, Java, C#', ' Identifies extract errors and performs simple troubleshooting steps.', ' Acts as a second validator to review database layouts, and verifies that no data is missing from the legacy system. Uses MQ scoping language from the contract and determines if the dataset delivered falls within the expected scope, or if the dataset delivered does not match the expected deliverable. Collaborates with both the Client/Extractor and the DM/IC to ensure a smooth transition of the data between the extraction and the modeling Responsible for the location of the data. Communicates clearly to the team as the location of the data changes during the review process Runs detailed comparisons between table lists from the legacy system and the extracted dataset. Works with other team members to compare to past extract projects throughout MediQuant history.', ' Liaise with application/tool developers, PMO, IT, or other sources to create tools needed to streamline workflows', ' Under guidance of DE II or DE III, loads flat file data into a SQL Server Database. Exports large directories of files, and extracts metadata stored in the directory listing. Resulting output is in a tabular format, linking the metadata to the corresponding file path', ' Responsible for the location of the data. Communicates clearly to the team as the location of the data changes during the review process', 'Bachelor’s degree in Computer Science, Computer Engineering, Software Engineering, preferred. High School Diploma or GED with 1-3 years of applicable experience in computer science related field required.Programming Skills Basic knowledge of SQL Scripting skills, able to query and transform data Proficiency in one of the following programming languages: Python, Java, C# Familiar with PowerShell/Bash scriptingExcellent organizational and documentation skillsAble to understand database catalog, schema, and table layoutsBasic understanding of healthcare data terminology (Clinical, Financial, Accounts, Visits, Charges, etc.) to help with scope review and understanding the datasets that are under review.', ' Establishes basic ODBC connectivity to various database types.', 'Organizational Skills', 'Organization and Time Management. (*) ', 'Time Management', 'Knowledge with HIPAA regulations involving PHI and HITECH', ' Performs quality assurance checks on extraction to ensure work is accurate and reliable before it is passed on to the next team', 'Loads various discrete and non-discrete data sources into a Microsoft SQL Server environment. (*)', 'Excellent organizational and documentation skills', 'Problem Solving/Analysis', 'Strong customer service and information technology acumen', 'Other Duties. ', 'Able to understand database catalog, schema, and table layouts', '(', ' Exports large directories of files, and extracts metadata stored in the directory listing. Resulting output is in a tabular format, linking the metadata to the corresponding file path', '1-3 years’ experience in Healthcare IT or clinical environments, preferredPast experience extracting Healthcare data, preferredAbility to solve problems using logical thought processes and devising creative solutionsHigh attention to accuracy and detailAbility and desire to work in a strong team cultureKnowledge with HIPAA regulations involving PHI and HITECHStrong customer service and information technology acumen', 'Work Authorization/Security Clearance (if applicable). ', ' Kicks-off extract process and monitors the process during runtime', 'Reviews incoming datasets to check for errors and ensure validity.', 'Troubleshooting and Problem Solving. (*) - ', ' Under guidance of DE II or DE III, loads flat file data into a SQL Server Database.', ' Effectively checks work for accuracy, giving specific attention to error zones. Takes ownership to ensure own work is error-free.', 'Basic understanding of healthcare data terminology (Clinical, Financial, Accounts, Visits, Charges, etc.) to help with scope review and understanding the datasets that are under review.', 'Competencies', 'Bachelor’s degree in Computer Science, Computer Engineering, Software Engineering, preferred. High School Diploma or GED with 1-3 years of applicable experience in computer science related field required.', ' Troubleshooting and Problem Solving. (*) - Identifies problems and proactively intervenes to mitigate or eliminate potential for negative impact. Effectively checks work for accuracy, giving specific attention to error zones. Takes ownership to ensure own work is error-free. Conducts independent research when issues are identifies; and problem-solves. Recognizes when additional help is needed and proactively reaches out to mentors/leaders for help.ii. Uses strong data analysis skills to debug data anomaliesiii. Applies technical knowledge and seeks to fully understand the client’s expectations by asking questions. Works collaboratively with project teams to innovate and find solutions to complex issues. Documents issues and resolutions for and “lessons learned” to avoid repeating. Liaise with application/tool developers, PMO, IT, or other sources to create tools needed to streamline workflows', ' Identifies and utilizes all resources available when priorities conflict or when external challenges are lining up against the deadline. Must be able to clearly communicate with the PM and Lead, at the time conflicting priorities threaten delivery of scheduled tasks. Manages a fluctuating workload and prioritizes workload during times of peak demand and conflicting priorities. Knows when to seek assistance to ensure deadlines are met and quality is delivered']",Entry level,Part-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer (m/f/d),Springer Nature Technology and Publishing Solutions,"Springer, NM",1 week ago,Be among the first 25 applicants,"['Ability to adapt and drive change', 'What We Would Appreciate', 'Experience with different DB systems (e.g. MS SQL server, Postgres, Vertica)', ' Analyze, normalize and transform data from various in-house systems Implement data pipelines and data models for process mining Monitoritor the data pipelines, ensure their reliability and help troubleshoot any issues. ', 'You will join a new team working at the vibrant pulse of Springer Nature’s publishing workflows processing millions of articles. We combine data from a variety of systems to mine process flows and facilitate granular end-to-end analysis. We are an enabler for continuous improvement and transformation monitoring.', 'Experience with Google Cloud Platform technologies like Big Query, Cloud Composer or Data Flow', 'Project management skills', ' Maintain and expand existing reporting systems, both proprietary an third-party Develop best practices and blueprints that can be used across projects ', 'Experience with data and process analysis tools like the Celonis Execution Management System', 'Develop best practices and blueprints that can be used across projects', 'Solution-oriented problem-solver, proficient and experienced with SQL and Python', 'Experience with data pipelines and ETL processes, ideally for larger volumes of data', ' We are looking forward to your online application using our online application system (SuccessFactors) stating your first possible date of joining and your salary expectations. ', 'Knowledge of scientific publishing processes', ' Project management skills Experience with Google Cloud Platform technologies like Big Query, Cloud Composer or Data Flow Experience with data and process analysis tools like the Celonis Execution Management System Knowledge of scientific publishing processes ', 'We take our work seriously but not ourselves, placing more importance on collaboration and relationship building than on solo heroics. Our mission is to perform together to lay the foundation for analyzing today’s processes to make tomorrow’s publishing more efficient.', 'i', 'Implement data pipelines and data models for process mining', ' University degree in the area of computer science, engineering or a comparable education Solution-oriented problem-solver, proficient and experienced with SQL and Python Experience with different DB systems (e.g. MS SQL server, Postgres, Vertica) Experience with data pipelines and ETL processes, ideally for larger volumes of data Strong analytical, organizational and problem-solving skills Ability to adapt and drive change Excellent command (business fluency) of English and German ', 'Excellent command (business fluency) of English and German', 'Visit the Springer Nature Editorial and Publishing website at', 'Who You Are', 'University degree in the area of computer science, engineering or a comparable education', 'Maintain and expand existing reporting systems, both proprietary an third-party', 'Strong analytical, organizational and problem-solving skills', 'What We Expect', 'Who We Are', ' Your contact person for the location Heidelberg is Ms Birgit Kolb, HR Director Heidelberg. ', 'Springer Nature opens the doors to discovery for researchers, educators, clinicians and other professionals. Every day, around the globe, our imprints, books, journals, platforms and technology solutions reach millions of people. For over 175 years our brands and imprints have been a trusted source of knowledge to these communities and today, more than ever, we see it as our responsibility to ensure that fundamental knowledge can be found, verified, understood and used by our communities – enabling them to improve outcomes, make progress, and benefit the generations that follow. ', 'Monitoritor the data pipelines, ensure their reliability and help troubleshoot any issues.', 'This position can be filled at the following locations: Heidelberg, Dordrecht', 'Youare as much into working with data, processes and software solutions as you are into working with people. You would describe yourself as a get-it-done and out-of-the-box person who appreciates the cultural diversity of a global company. You have already gained professional experience in international projects, you value self-dependency and reliability as much as we do, and you would be ready to get trained on the job.', 'What You Will Do', ' Visit our website at ', 'Analyze, normalize and transform data from various in-house systems', ' Excited by the prospect of joining us? ']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Celonis Data Engineer,ThreeBridge Solutions,"Bridgewater, NJ",2 weeks ago,67 applicants,"['', ' Map business goals with data Work with the business to understand what data they need to analyze their process. Write SQL / ML code to connect a process to Celonis Configure data jobs, loads and data models Validate data ', 'New data models / processes', 'Integration Of New Data - Tables', ' Create a new view that than can be integrated into the data model with the correct join definition predefined in the SQL code to keep out unnecessary data out of the front end', ' Adapt the current data extractions with the table to be added as well as correct filtering over global Datapool-Parameters', 'Recommended Know-How', 'IT-affinity', 'Schedulers (Transformations, Extractions, ML Workbench)', 'Creation of a new data model', 'Derive technical requirements from the business requirements given', ' Additional meta-data Further activities Change in activity logics New data models / processes New systems ', 'Celonis Applications', 'Map business goals with data', 'Additional meta-data', 'Process Automation', 'App Store', 'Write SQL / ML code to connect a process to Celonis', 'Change Request Can Be', 'Scheduler Executions (Delta Loads, Full Loads, Extractions, Machine Learning Workbench)', 'Duration:', 'Change Request Handling', 'Further activities', ' Add the table to the data model and trigger a new load', 'Vertica SQL skills', 'Machine Learning Workbench', ' Scheduler Executions (Delta Loads, Full Loads, Extractions, Machine Learning Workbench) Data Model Loads Overall System Health APC-Consumption License Consumption (Business Users, Analysts) ', 'Monitoring', 'Overall System Health', 'Validate data', 'The Data Engineer Tasks Include', 'Delta Loads', 'APC-Consumption', 'Integration of all events together with their SQL logic', ' Getting information on the table names, description and name mapping from the source system', 'Responsibilities:', 'License Consumption (Business Users, Analysts)', 'Data Connections', ' Understand the current SQL logic to derive how the activity is created', ' Know-how of data structures Vertica SQL skills Experience with ETL processes IT-affinity ', 'The Data Engineer Uses Celonis To', ' Process Analytics Event Collection App Store Process Automation Action Engine Machine Learning Workbench', 'Data Pool Parameters (Start Dates, Document Types, etc.)', 'SQL Adaption / Transformation Customization / Activity Creation', 'Scoping of the new project and estimation of a technical implementation timeline', 'Event Collection', 'Build up of the data model with all joins and necessary data', 'Change Request handling (e.g. scoping)', 'Adaption of technical analysis features', 'Configuration Of The Complete Event Collection', 'Work with the business to understand what data they need to analyze their process.', 'Change in activity logics', 'Data Pools', 'Data Model Loads', 'Connecting And Creating New Processes / Data Models', ' Create / Adapt the activity and integrate it into the according event log and transformation logic', 'Experience with ETL processes', 'Process Analytics', 'Connecting and creating new processes / data models', 'Steering the technical integration of the project', ' Schedulers (Transformations, Extractions, ML Workbench) Data Pool Parameters (Start Dates, Document Types, etc.) Data Connections Data Pools Integration Servers Data Model Loads Delta Loads ', 'Integration Servers', 'Integration of new tables', 'Configure data jobs, loads and data models', ' Scoping of the new project and estimation of a technical implementation timeline Derive technical requirements from the business requirements given Steering the technical integration of the project Creation of a new data model Integration of all events together with their SQL logic Build up of the data model with all joins and necessary data ', 'Know-how of data structures', 'Configuration of the complete event collection', 'Action Engine', 'SQL adaption / transformation customization', ' Gather source system knowledge to derive the new SQL logic', 'New systems', ' Change Request handling (e.g. scoping) Integration of new tables SQL adaption / transformation customization Configuration of the complete event collection Adaption of technical analysis features Connecting and creating new processes / data models Monitoring ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Analytics Engineer,"BDO USA, LLP","Chicago, IL",4 weeks ago,Be among the first 25 applicants,"['', ' Partakes in technology training to learn various Data Analytics / Business Intelligence technologies', "" Bachelor's degree from an accredited university, required Bachelor’s degree in Computer Science or a related field from an accredited university, preferred"", ' Participates in support activities for existing software', ' Amazon Web Services (AWS) with Redshift', ' Data visualization using PowerBI or Tableau for data analysis and reporting', ' Interacts with customers and project leaders to define and document project specifications', ' Semantic Models', ' Analysis Services (SSAS – Multidimensional and Tabular cubes)', ' Other duties as required', 'Responsibilities', ' SSRS', ' SSIS SSAS SSRS Stream Analytics or Data Lake Analytics Microsoft Azure SQL or SQL Data Warehouse Azure Data Factory PowerBI or Tableau AWS with Redshift', 'Education', "" Bachelor's degree from an accredited university, required"", ' Two (2) or more years of Software Development experience within Data Analytics / Business Intelligence development using Microsoft technologies, required', 'Qualifications', ' Experience with the following technologies, preferred:', ' Strong SQL Server skills is a must along with SQL queries and stored procedures Excellent organizational and time management skills Strong written and verbal communication skills Experience with the following technologies, preferred: Semantic Models Integration Services (SSIS) Analysis Services (SSAS – Multidimensional and Tabular cubes) Reporting Services (SSRS) Machine Learning Stream Analytics or Data Lake Analytics Microsoft Azure SQL or SQL Data Warehouse Azure Data Factory PowerBI or Tableau Amazon Web Services (AWS) with Redshift Must be open to local travel to client sites, if needed', ' SSAS', ' Azure Data Factory', ' Must be open to local travel to client sites, if needed', ' Strong SQL Server skills is a must along with SQL queries and stored procedures', 'Other Knowledge, Skills & Abilities', ' Reporting Services (SSRS)', ' SSIS', ' Excellent organizational and time management skills', 'Experience', 'Software', ' Strong written and verbal communication skills', ' AWS with Redshift', ' Microsoft Azure SQL or SQL Data Warehouse', ' PowerBI or Tableau', ' Bachelor’s degree in Computer Science or a related field from an accredited university, preferred', ' Designs, develops, tests, and implements data analytics or business intelligence solutions', ' Integration Services (SSIS)', ' Interacts with customers and project leaders to define and document project specifications Designs, develops, tests, and implements data analytics or business intelligence solutions Participates in support activities for existing software Data visualization using PowerBI or Tableau for data analysis and reporting Partakes in technology training to learn various Data Analytics / Business Intelligence technologies Other duties as required', ' Stream Analytics or Data Lake Analytics', ' Machine Learning']",Not Applicable,Full-time,Finance,Accounting,2021-03-18 14:34:51
Data Engineer,ROSEN,"Columbus, OH",1 week ago,Be among the first 25 applicants,"['', ' Passion for data and for the development of tools to master them ', 'New Challenge:', ' Fusion of extremely structured, semi-structured and unstructured data sources(e.g. historical data, metadata, simulation data, etc.) ', ' Development or application of suitable tools for data access ', ' Creation of ETL pipelines to validate, enrich and persist data ', ' Knowledge of industry wide technology trends and best practices ', 'University degree in computer science, commercial information technology, physics or equivalent5+ years relevant work experience in the field of data engineering', 'ROSEN USA offers an exceptional working environment, salary commensurate with experience and incredible benefits package.', 'Responsibilties', 'Education and Experience:', ' Successfully completed a degree in computer science or other area with strong programming background ', ' Definition of data requirements together with Data Scientists ', 'Requirements', 'Description', 'Qualification Or Skills', 'To become part of the ROSEN family', ' Maintenance and continuous improvement of machine learning models ', ' Provision of data sources for consumers in the form of interfaces (APIs) ', ' Successfully completed a degree in computer science or other area with strong programming background  Mastering of a programming language like python and database systems  Good grasp of big data infrastructures and relevant big data tools  Passion for data and for the development of tools to master them  Good grasp of containerization technology and Linux  Knowledge of industry wide technology trends and best practices  Good teamwork skills in interdisciplinary teams ', ' Good grasp of big data infrastructures and relevant big data tools ', ' Visit of national and international conferences on a regular basis ', '5+ years relevant work experience in the field of data engineering', ' Design of a big data infrastructure and big data platform (storage, formats, interfaces, etc.) ', ' Design of a big data infrastructure and big data platform (storage, formats, interfaces, etc.)  Creation of ETL pipelines to validate, enrich and persist data  Provision of data sources for consumers in the form of interfaces (APIs)  Fusion of extremely structured, semi-structured and unstructured data sources(e.g. historical data, metadata, simulation data, etc.)  Development or application of suitable tools for data access  Maintenance and continuous improvement of machine learning models  Definition of data requirements together with Data Scientists  Visit of national and international conferences on a regular basis ', ' Mastering of a programming language like python and database systems ', ' Good teamwork skills in interdisciplinary teams ', 'OUR OFFER', 'University degree in computer science, commercial information technology, physics or equivalent', 'Be part of a new team, shape the team and your future and enjoy working in a multi-cultural and multi-national team. ', 'New Team, New Office! Protect Our Environment By Your Algorithms', ' Good grasp of containerization technology and Linux ']",Entry level,Full-time,Information Technology,Electrical/Electronic Manufacturing,2021-03-18 14:34:51
Data Platform Engineer - Experimentation,Stitch Fix,"San Francisco, CA",7 days ago,Be among the first 25 applicants,"['', 'Be involved in the day-to-day operations of the team, including maintaining and improving our current platform and supporting full-stack data scientists', 'The ability to effectively communicate with technical and non-technical business partners alike, and who are interested in building strong cross-functional relationships', 'In This Role You Can Expect To', 'Contribute to a culture of technical collaboration and scalable resilient systems', 'A desire to question the status quo and promote innovative solutions to challenging problems', 'We offer competitive compensation packages and comprehensive health benefits', 'Be an owner of a highly visible and high-traffic platform', ' Help shape how experimentation is done at Stitch Fix Be an owner of a highly visible and high-traffic platform Have the opportunity to work on all aspects of the experimentation system, from services to backend data stores to analytics and logging Share the responsibility of directing the team’s investment in impactful directions Contribute to a culture of technical collaboration and scalable resilient systems Work with bright and kind colleagues who are passionate about their craft ', 'We take what we do seriously. We don’t take ourselves seriously', 'We believe in autonomy & taking initiative', 'Exceptional coding and design skills', 'We have a smart, experienced leadership team that wants to do it right & is open to new ideas', 'Help shape how experimentation is done at Stitch Fix', 'A rigorous focus on simple solutions - more complex is not always better', 'Have the opportunity to work on all aspects of the experimentation system, from services to backend data stores to analytics and logging', 'You’re Excited About This Opportunity Because You Will...', 'You will be proud to say that you work for Stitch Fix and will know that the work you do brings joy to our clients every day', 'Experience working autonomously and taking ownership of projects', 'Work with bright and kind colleagues who are passionate about their craft', 'About The Role', 'We are a group of bright, kind and goal oriented people. You can be your authentic self here, and are empowered to encourage others to do the same!', 'About Stitch Fix', 'We Get Excited About Candidates Who Have…', 'We love solving problems, thinking creatively and trying new things', 'About The Team', 'Share the responsibility of directing the team’s investment in impactful directions', 'We are a technologically and data-driven business', 'Work with business partners to identify new opportunities for experimentation support', 'We are challenged, developed and have meaningful impact', 'A robust focus on business impact', 'We are committed to our clients and connected through our vision of “Transforming the way people find what they love”', ' Experience working autonomously and taking ownership of projects Strong experience building out scalable distributed and production systems A rigorous focus on simple solutions - more complex is not always better Exceptional coding and design skills A robust focus on business impact The ability to effectively communicate with technical and non-technical business partners alike, and who are interested in building strong cross-functional relationships A desire to question the status quo and promote innovative solutions to challenging problems ', ' We are a group of bright, kind and goal oriented people. You can be your authentic self here, and are empowered to encourage others to do the same! We are a successful, fast-growing company at the forefront of tech and fashion, redefining retail for the next generation We are a technologically and data-driven business We are committed to our clients and connected through our vision of “Transforming the way people find what they love” We love solving problems, thinking creatively and trying new things We believe in autonomy & taking initiative We are challenged, developed and have meaningful impact We take what we do seriously. We don’t take ourselves seriously We have a smart, experienced leadership team that wants to do it right & is open to new ideas We offer competitive compensation packages and comprehensive health benefits You will be proud to say that you work for Stitch Fix and will know that the work you do brings joy to our clients every day ', 'Build and own large aspects to the experimentation platform, with a focus on scalable, resilient infrastructure', ' Build and own large aspects to the experimentation platform, with a focus on scalable, resilient infrastructure Help us curate a consistent set of interfaces Be involved in the day-to-day operations of the team, including maintaining and improving our current platform and supporting full-stack data scientists Work with business partners to identify new opportunities for experimentation support ', 'Help us curate a consistent set of interfaces', 'We are a successful, fast-growing company at the forefront of tech and fashion, redefining retail for the next generation', ""Please Review Stitch Fix's Recruiting Privacy Policy Here"", 'Strong experience building out scalable distributed and production systems', 'YOU’LL LOVE WORKING AT STITCH FIX BECAUSE…']",Entry level,Full-time,Research,Apparel & Fashion,2021-03-18 14:34:51
Data Solutions - Data Engineer I,ArcBest Technologies,"Fort Smith, AR",10 hours ago,Be among the first 25 applicants,"['', ' ArcBest Technologies Data Solutions ', 'Other duties and projects, as assigned.', 'Ready to apply? Before doing so, please make sure you meet the minimum requirements:', 'ArcBest ', 'Data Engineer I', 'Discover, research, and integrate new data sources.', 'ArcBest', 'Collaborate with various teams, including functional stakeholders, Technical Services, Advanced Analytics and Business Intelligence to ensure actionable insights are securely enabled by appropriate data technologies.', 'Fort Smith, AR', ' ArcBest Technologies ', 'Implement ETL/ELT flows in accordance with established best practices.', 'Ensure data is properly catalogued.', 'Data Solutions - Data Engineer I', 'Your Contribution May Include', 'Monitor existing ETL/ELT jobs to ensure efficiency of process and correctness of results, including on-call responsibilities.', 'Implement ETL/ELT flows in accordance with established best practices.Monitor existing ETL/ELT jobs to ensure efficiency of process and correctness of results, including on-call responsibilities.Collaborate with various teams, including functional stakeholders, Technical Services, Advanced Analytics and Business Intelligence to ensure actionable insights are securely enabled by appropriate data technologies.Ensure data is properly catalogued.Discover, research, and integrate new data sources.Other duties and projects, as assigned.', 'Other Requirements: ', 'Data Solutions - Data Engineer I ', 'Education: ', 'Computer Skills: ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer- eCommerce SaaS,TEEMA,"Los Angeles, CA",,N/A,"['', 'Preferred Qualifications:', 'Experience working with digital marketing data e.g. Google Analytics, Heap', 'An experienced “full-stack” analyst, with 3+ years of experience in business intelligence, data science, data analysis, data engineering, or a relevant analytical/quantitative field', 'Working knowledge with Amazon Web Services (inc. EC2, S3, Lambda)', 'Develop actionable insights through the rigorous assessment of multiple data sources', 'Work with the engineering team to diagnose and fix data discrepancies and maintain transparent code to ensure business requirements and consistent ETL logic', 'Provide consistent, accurate, and timely reporting', 'Expertise with BI visualization tools, e.g., Tableau, PowerBI, Periscope Data/Sisense, Looker.', 'Support and contribute to the analytics layer of our team’s data environment to make data standardized and easily accessible to end-users', '\xa0Does a Data Engineer role working for a high-growth eCommerce SaaS interest you?', 'Experience working with transaction/user-level data in retail/ eCommerce', 'Build self-serve modules for data needs', 'Perform diagnostic analysis of varied data to identify issues and promote opportunities for improvement', 'Responsibilities:', 'Review and validate customer data (transactions, marketing platforms, purchase flow, etc.)', 'Excellent communication and documentation skills—being someone who values building relationships with colleagues and enjoys explaining complex topics in simple terms', 'We are using a number of tools to make sense of our data:  AWS Redshift, Stitch, DBT, Fivetran, Looker, Google Analytics, and Postgres to name a few :)', 'Integrate new data sources and build ingestion pipelines for our data warehouse', 'Bachelor’s degree in a Computer Science, Engineering, Math, Finance, Statistics or related discipline', 'Collaborate with stakeholders in our marketing, planning, digital product, and finance & operations departments to define business questions and identify opportunities for data-driven improvements', 'Required Responsibilities:', 'Experience with git and version control', 'Knowledge of data warehousing best practices', 'Expertise with Data Engineering Tools, e.g. DBT, Fivetran, Stitch, Funnel.', 'As a Data Engineer, you will support all analyses and reporting to drive the business forward. The ideal candidate is a self-motivated, highly detail-oriented team-player with a positive drive to strategize, implement and own business intelligence solutions that enable the business to derive valuable insights. You will work to apply software engineering best practices to the production and maintenance of analytics code and transform raw data into consumable information and business logic. This role will be pivotal in working with senior stakeholders, providing key insights quickly, and impacting the business meaningfully.', 'Look no further :)  This role will allow you to grow and learn new technology as they grow their team.  This role has a little bit of everything, Data analysis, Analytics, Engineering, and Science :)', 'Experience with cloud-based data warehousing systems (e.g., AWS Redshift, Snowflake, BigQuery, Azure SQL Warehouse)', 'Advanced SQL skills (3+ years proficiency)', 'Masters and/or MBA a plus', '\xa0', 'Create and maintain reporting and dashboards using SQL to deliver insights', 'This position will report to the Director of Analytics.']",Mid-Senior level,Full-time,Information Technology,Staffing and Recruiting,2021-03-18 14:34:51
Data Engineer,"OM1, Inc.","Boston, MA",1 week ago,71 applicants,"['', 'Analyze and interpret data flows, attributes, and quality', 'Direct applicants only - No agencies', 'Code, test and deliver data-oriented product features', 'Build, automate, and improve big-data pipelines', 'Solid understanding of software engineering principles', 'Data processing experience with Python, Scala, Spark, and/or Airflow', 'Responsibilities', 'Comfort working in an Agile/Scrum environment with continuous delivery', 'Requirements', 'A strong desire to advance healthcare and improve patient outcomes', 'Experience with CI/CD tools such as Jenkins', 'Build, automate, and improve big-data pipelinesCode, test and deliver data-oriented product featuresIntegrate new data sources into our data assetModel, enhance and enrich database schemas and underlying dataCraft scalable backend software and tools using modern software engineering practicesAnalyze and interpret data flows, attributes, and quality', 'Description', 'Integrate new data sources into our data asset', 'Craft scalable backend software and tools using modern software engineering practices', 'Familiarity with cloud-based platforms such as AWS, Databricks, and/or Snowflake DB', 'Strong programming skills, preferably Python, Scala, Java, or similarExcellent SQL skillsSolid understanding of software engineering principlesComfort working in an Agile/Scrum environment with continuous deliveryA strong desire to advance healthcare and improve patient outcomesExcellent attention to detail', 'Model, enhance and enrich database schemas and underlying data', 'Excellent SQL skills', 'Experience working on systems with strong security and privacy requirements', 'Familiarity with cloud-based platforms such as AWS, Databricks, and/or Snowflake DBData processing experience with Python, Scala, Spark, and/or AirflowAutomated unit and integration testing experienceExperience with CI/CD tools such as JenkinsBackground in medical records and health insurance claimsExperience working on systems with strong security and privacy requirements', 'Strong programming skills, preferably Python, Scala, Java, or similar', 'Background in medical records and health insurance claims', 'Excellent attention to detail', 'Automated unit and integration testing experience']",Entry level,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
"Data Engineer, Intern","Qorvo, Inc.","Greensboro, NC",2 days ago,99 applicants,"['', 'Experience with or completed coursework related to data pipeline automation.', 'Relocation Eligible: ', 'Soft Skills', 'Position Location: ', 'Support pipeline operations.', 'Collaborate with BI users, applications developers, and data scientists to establish and refine data requirements.', 'Implement data movement and transformations in the context of an automated data pipeline.', ' Use SQL, Python / Pandas, Apache Spark, and other data manipulation tools to develop, construct, test, and maintain databases and big data processing systems in support of analytics and applications. Unify data from disparate sources while assessing and improving data quality and usability. Prepare data for use in diagnostic, predictive, and prescriptive modeling. Collaborate with BI users, applications developers, and data scientists to establish and refine data requirements. Implement data movement and transformations in the context of an automated data pipeline. Support pipeline operations. Assist data scientists in feature engineering and model deployment  ', 'Experience with or completed course work related to cloud-based data storage solutions.', 'Responsibilities', 'Curious', 'Desired Degree: ', 'Unify data from disparate sources while assessing and improving data quality and usability.', 'Excellent communication skills', ' Greensboro, NC', 'Qualifications', 'Effective collaborative skills', 'Familiarity with mathematical and statistical software tools and programming/scripting languages.', 'Driven', 'Assist data scientists in feature engineering and model deployment ', 'Strong process-driven problem-solving skills.', ' Computer science, engineering, or another quantitative degree, or equivalent coursework and/or work experience. Experience with or completed course work related to cloud-based data storage solutions. Experience with or completed coursework related to data pipeline automation. Familiarity with mathematical and statistical software tools and programming/scripting languages. Strong process-driven problem-solving skills. ', 'Computer science, engineering, or another quantitative degree, or equivalent coursework and/or work experience.', ' Excellent communication skills Effective collaborative skills Innovative Curious Driven ', 'Citizenship', 'Innovative', '3 months', 'Prepare data for use in diagnostic, predictive, and prescriptive modeling.', 'Use SQL, Python / Pandas, Apache Spark, and other data manipulation tools to develop, construct, test, and maintain databases and big data processing systems in support of analytics and applications.']",Not Applicable,Internship,Information Technology,Consumer Electronics,2021-03-18 14:34:51
"Sr Data Engineer, Data Science",Twitter,"Seattle, WA",3 weeks ago,66 applicants,"['', 'As such, you will ', 'Design, develop, and launch extremely efficient and reliable data pipelines to move data and to provide intuitive analytics to our partner teams.Make Twitter-scale data more discoverable and easy to use for Data Scientists and Analysts across the company.Collaborate with other engineers and Data Scientists to discover the best solutions.Support your colleagues by reviewing code and designs.Diagnose and solve issues in our existing data pipelines and envision and build their successors.', 'Company Description', 'Job Description', 'Collaborate with other engineers and Data Scientists to discover the best solutions.', 'Design, develop, and launch extremely efficient and reliable data pipelines to move data and to provide intuitive analytics to our partner teams.', 'Make Twitter-scale data more discoverable and easy to use for Data Scientists and Analysts across the company.', 'Support your colleagues by reviewing code and designs.', 'Diagnose and solve issues in our existing data pipelines and envision and build their successors.']",Not Applicable,Full-time,Information Technology,Internet,2021-03-18 14:34:51
Staff Data Engineer,JLL,"Chicago, IL",3 days ago,Be among the first 25 applicants,"['', ' Independent and able to manage, prioritize & lead workload ', ' Develop good understanding of how data will flow & stored through an organization across multiple applications such as CRM, Broker & Sales tools, Finance, HR etc  Unify, enrich, and analyze variety of data to derive insights and opportunities  Design & develop data management and data persistence solutions for application use cases leveraging relational, non-relational databases and enhancing our data processing capabilities  Develop POCs to influence platform architects, product managers and software engineers to validate solution proposals and migrate  Develop data lake solution to store structured and unstructured data from internal and external sources and provide technical guidance to help migrate colleagues to modern technology platform ', ' JLL Privacy Notice ', ' Develop data lake solution to store structured and unstructured data from internal and external sources and provide technical guidance to help migrate colleagues to modern technology platform ', ' 3+ years of experience working with source code control systems and Continuous Integration/Continuous Deployment tools ', ' Design, Architect, and Develop solutions leveraging cloud big data technology to ingest, process and analyze large, disparate data sets to exceed business requirements  Develop systems that ingest, cleanse and normalize diverse datasets, develop data pipelines from various internal and external sources and build structure for previously unstructured data  Interact with internal colleagues and external professionals to determine requirements, anticipate future needs, and identify areas of opportunity to drive data development ', ' Hands-on engineer who is curious about technology, should be able to quickly adopt to change and one who understands the technologies supporting areas such as Cloud Computing (AWS, Azure(preferred), etc.), Micro Services, Streaming Technologies, Network, Security, etc.  3 or more years of active development experience as a data developer using Python-spark, Spark Streaming, Azure SQL Server, Cosmos DB/Mongo DB, Azure Event Hubs, Azure Data Lake Storage, Azure Search etc.  Design & develop data management and data persistence solutions for application use cases leveraging relational, non-relational databases and enhancing our data processing capabilities  Build, test and enhance data curation pipelines integration data from wide variety of sources like DBMS, File systems, APIs and streaming systems for various KPIs and metrics development with high data quality and integrity  Maintain the health and monitoring of assigned data engineering capabilities that span analytic functions by triaging maintenance issues; ensure high availability of the platform; monitor workload demands; work with Infrastructure Engineering teams to maintain the data platform; serve as an SME of one or more application ', ' Mentor other members in the team and organization and contribute to organizations’ growth. ', ' 6 + years’ work experience and bachelor’s degree in Information Science, Computer Science, Mathematics, Statistics or a quantitative discipline in science, business, or social science.\u202f ', ' As a Staff Data Engineer at JLL Technologies, you will- ', ' Contribute and adhere to CI/CD processes, development best practices and strengthen the discipline in Data Engineering Org  Mentor other members in the team and organization and contribute to organizations’ growth. ', 'California Residents only', 'Required Skills-', 'What This Job Involves', ' Build, test and enhance data curation pipelines integration data from wide variety of sources like DBMS, File systems, APIs and streaming systems for various KPIs and metrics development with high data quality and integrity ', ' Interact with internal colleagues and external professionals to determine requirements, anticipate future needs, and identify areas of opportunity to drive data development ', 'Supplemental Privacy Statement', 'What You Can Expect From Us-', ' Hands-on engineer who is curious about technology, should be able to quickly adopt to change and one who understands the technologies supporting areas such as Cloud Computing (AWS, Azure(preferred), etc.), Micro Services, Streaming Technologies, Network, Security, etc. ', ' Design & develop data management and data persistence solutions for application use cases leveraging relational, non-relational databases and enhancing our data processing capabilities ', ' Develop good understanding of how data will flow & stored through an organization across multiple applications such as CRM, Broker & Sales tools, Finance, HR etc ', ' Develop systems that ingest, cleanse and normalize diverse datasets, develop data pipelines from various internal and external sources and build structure for previously unstructured data ', 'About JLL And JLL Technologies', ' Design, Architect, and Develop solutions leveraging cloud big data technology to ingest, process and analyze large, disparate data sets to exceed business requirements ', 'What We Are Looking For-', ' Develop POCs to influence platform architects, product managers and software engineers to validate solution proposals and migrate ', ' Maintain the health and monitoring of assigned data engineering capabilities that span analytic functions by triaging maintenance issues; ensure high availability of the platform; monitor workload demands; work with Infrastructure Engineering teams to maintain the data platform; serve as an SME of one or more application ', ' Team player, Reliable, self-motivated, and self-disciplined individual capable of executing on multiple projects simultaneously within a fast-paced environment working with cross functional teams ', ' 3 or more years of active development experience as a data developer using Python-spark, Spark Streaming, Azure SQL Server, Cosmos DB/Mongo DB, Azure Event Hubs, Azure Data Lake Storage, Azure Search etc. ', ' Contribute and adhere to CI/CD processes, development best practices and strengthen the discipline in Data Engineering Org ', ' Team player, Reliable, self-motivated, and self-disciplined individual capable of executing on multiple projects simultaneously within a fast-paced environment working with cross functional teams  3+ years of experience working with source code control systems and Continuous Integration/Continuous Deployment tools  Independent and able to manage, prioritize & lead workload ', 'Optional Skills-', ' Unify, enrich, and analyze variety of data to derive insights and opportunities ', 'JLL is an Equal Opportunity Employer']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Lead Data Engineer,Aegon,"Denver, CO",5 days ago,Be among the first 25 applicants,"['', 'Please note that the compensation information that follows is a good faith estimate for this position only and is provided pursuant to the Colorado Equal Pay for Equal Work Act and Equal Pay Transparency Rules. It is estimated based on what a successful Colorado applicant might be paid. It assumes that the successful candidate will be in Colorado or perform the position from Colorado. Similar positions located outside of Colorado will not necessarily receive the same compensation. **CompensationThe Salary for this position generally ranges between $93,450 - $154,350. This range is an estimate, based on potential employee qualifications, operational needs and other considerations permitted by law. The range may vary above and below the stated amounts, as permitted by Colorado Equal Pay Transparency Rule 4.1.2.Bonus EligibilityThis position is also typically eligible for an Annual Bonus based on the Company Bonus Plan/Individual Performance and is at Company Discretion at a rate of 15%.What You ReceiveA Comprehensive Wealth + Health package. It’s our passion to empower people, and especially our employees, to add years to their lives and more life to their years. That means a healthy account balance and a healthy body to match. As you’ll come to discover, Wealth + Health is a central part of everything we do!Wealth Benefits; Competitive Pay, Bonus, and Benefits Package; Pension Plan, 401k Match, Employee Stock Purchase Plan, Tuition Reimbursement, Disability Insurance, Stock Purchase Plan, Employee Discounts, Career Training & Development Opportunities, Certification SponsorshipHealth and Work/Life Balance Benefits; Be Well Company sponsored holistic wellness program which includes Wellness Coaching and reward dollars, Parental Leave, Adoption Assistance, Employee Assistance Program, College Coach Program, Back-up Care Program, Paid Time Off to Volunteer, Employee Matching Gifts Program, Employee Resource Groups, Inclusion and Diversity Programs, Employee Recognition ProgramOur commitment to inclusion & diversity means that we value differences. We encourage the unique perspectives of individuals and are dedicated to creating a respectful and inclusive work environment.', 'Bonus Eligibility', 'Be Well', 'Understanding of big data and real time streaming analytics processing architecture and ecosystems', 'Strong technical communication skills', 'Experience with data analysis and using data visualization tools to describe data', 'ResponsibilitiesWork collaboratively with other engineers, data scientists, analytics teams, and business product owners in an agile environment:Architect, build, and support the operation of Cloud and On-Premises enterprise data infrastructure and tools.Design robust, reusable and scalable data driven solutions and data pipeline frameworks to automate the ingestion, processing and delivery of both structured and unstructured batch and real-time streaming data.Lead the development of data APIs and data delivery services to support critical operational processes, analytical models and machine learning applications.Assist the selection and integration of data related tools, frameworks and applications required to expand platform capabilities.Understand and implement best practices in management of enterprise data, including master data, reference data, metadata, data quality and lineage.QualificationsBachelor’s degree in computer science, math, engineering, or relevant technical fieldFive years of collective experience in the application of data engineering, data analytics, data warehousing, business intelligence, database administration and data integration concepts and methodologiesFour years of experience architecting, building, and administering big data and real-time streaming analytics architectures in on-premises and cloud environmentsThree years of experience architecting, building and administering large-scale distributed applicationsThree years of experience with Linux operations and development, including basic commands and shell scriptingThree years of experience executing DevOps methodologies and continuous integration/continuous deliveryDemonstrated skills in delivery of high quality technical project solutionsExpertise in SQL for data profiling, analysis, and extractionFamiliarity with data science techniques and frameworksCreative thinker with strong analytical skillsResults oriented with a strong customer focusAbility to work in a team environmentStrong technical communication skillsAbility to prioritize work to meet tight deadlinesAbility to learn and keep pace with the latest technology advances and quickly grasp new technologies to support the environment and contribute to project deliverablesPreferred QualificationsMaster’s degree in a technical field (e.g. computer science, math, engineering) Understanding of big data and real time streaming analytics processing architecture and ecosystemsExperience with data warehousing architecture and implementation, including hands on experience with source to target mappings and developing ETL codeExperience with advanced analytics and machine learning concepts and technology implementationsExperience with data analysis and using data visualization tools to describe dataRelevant technology or platform certification (AWS, Microsoft, etc.) Software development experience in relevant programming languages (e.g. Java, Python, Scala, Node.js)Working ConditionsOffice environmentOccasional travelPlease note that the compensation information that follows is a good faith estimate for this position only and is provided pursuant to the Colorado Equal Pay for Equal Work Act and Equal Pay Transparency Rules. It is estimated based on what a successful Colorado applicant might be paid. It assumes that the successful candidate will be in Colorado or perform the position from Colorado. Similar positions located outside of Colorado will not necessarily receive the same compensation. **CompensationThe Salary for this position generally ranges between $93,450 - $154,350. This range is an estimate, based on potential employee qualifications, operational needs and other considerations permitted by law. The range may vary above and below the stated amounts, as permitted by Colorado Equal Pay Transparency Rule 4.1.2.Bonus EligibilityThis position is also typically eligible for an Annual Bonus based on the Company Bonus Plan/Individual Performance and is at Company Discretion at a rate of 15%.What You ReceiveA Comprehensive Wealth + Health package. It’s our passion to empower people, and especially our employees, to add years to their lives and more life to their years. That means a healthy account balance and a healthy body to match. As you’ll come to discover, Wealth + Health is a central part of everything we do!Wealth Benefits; Competitive Pay, Bonus, and Benefits Package; Pension Plan, 401k Match, Employee Stock Purchase Plan, Tuition Reimbursement, Disability Insurance, Stock Purchase Plan, Employee Discounts, Career Training & Development Opportunities, Certification SponsorshipHealth and Work/Life Balance Benefits; Be Well Company sponsored holistic wellness program which includes Wellness Coaching and reward dollars, Parental Leave, Adoption Assistance, Employee Assistance Program, College Coach Program, Back-up Care Program, Paid Time Off to Volunteer, Employee Matching Gifts Program, Employee Resource Groups, Inclusion and Diversity Programs, Employee Recognition ProgramOur commitment to inclusion & diversity means that we value differences. We encourage the unique perspectives of individuals and are dedicated to creating a respectful and inclusive work environment.', 'Design robust, reusable and scalable data driven solutions and data pipeline frameworks to automate the ingestion, processing and delivery of both structured and unstructured batch and real-time streaming data.', 'Three years of experience executing DevOps methodologies and continuous integration/continuous delivery', 'Master’s degree in a technical field (e.g. computer science, math, engineering) Understanding of big data and real time streaming analytics processing architecture and ecosystemsExperience with data warehousing architecture and implementation, including hands on experience with source to target mappings and developing ETL codeExperience with advanced analytics and machine learning concepts and technology implementationsExperience with data analysis and using data visualization tools to describe dataRelevant technology or platform certification (AWS, Microsoft, etc.) Software development experience in relevant programming languages (e.g. Java, Python, Scala, Node.js)', 'Assist the selection and integration of data related tools, frameworks and applications required to expand platform capabilities.', 'Occasional travel', 'What You Receive', 'Bachelor’s degree in computer science, math, engineering, or relevant technical fieldFive years of collective experience in the application of data engineering, data analytics, data warehousing, business intelligence, database administration and data integration concepts and methodologiesFour years of experience architecting, building, and administering big data and real-time streaming analytics architectures in on-premises and cloud environmentsThree years of experience architecting, building and administering large-scale distributed applicationsThree years of experience with Linux operations and development, including basic commands and shell scriptingThree years of experience executing DevOps methodologies and continuous integration/continuous deliveryDemonstrated skills in delivery of high quality technical project solutionsExpertise in SQL for data profiling, analysis, and extractionFamiliarity with data science techniques and frameworksCreative thinker with strong analytical skillsResults oriented with a strong customer focusAbility to work in a team environmentStrong technical communication skillsAbility to prioritize work to meet tight deadlinesAbility to learn and keep pace with the latest technology advances and quickly grasp new technologies to support the environment and contribute to project deliverables', 'Software development experience in relevant programming languages (e.g. Java, Python, Scala, Node.js)', 'Master’s degree in a technical field (e.g. computer science, math, engineering) ', 'Qualifications', 'Five years of collective experience in the application of data engineering, data analytics, data warehousing, business intelligence, database administration and data integration concepts and methodologies', 'Four years of experience architecting, building, and administering big data and real-time streaming analytics architectures in on-premises and cloud environments', 'Experience with data warehousing architecture and implementation, including hands on experience with source to target mappings and developing ETL code', 'ResponsibilitiesWork collaboratively with other engineers, data scientists, analytics teams, and business product owners in an agile environment:Architect, build, and support the operation of Cloud and On-Premises enterprise data infrastructure and tools.Design robust, reusable and scalable data driven solutions and data pipeline frameworks to automate the ingestion, processing and delivery of both structured and unstructured batch and real-time streaming data.Lead the development of data APIs and data delivery services to support critical operational processes, analytical models and machine learning applications.Assist the selection and integration of data related tools, frameworks and applications required to expand platform capabilities.Understand and implement best practices in management of enterprise data, including master data, reference data, metadata, data quality and lineage.QualificationsBachelor’s degree in computer science, math, engineering, or relevant technical fieldFive years of collective experience in the application of data engineering, data analytics, data warehousing, business intelligence, database administration and data integration concepts and methodologiesFour years of experience architecting, building, and administering big data and real-time streaming analytics architectures in on-premises and cloud environmentsThree years of experience architecting, building and administering large-scale distributed applicationsThree years of experience with Linux operations and development, including basic commands and shell scriptingThree years of experience executing DevOps methodologies and continuous integration/continuous deliveryDemonstrated skills in delivery of high quality technical project solutionsExpertise in SQL for data profiling, analysis, and extractionFamiliarity with data science techniques and frameworksCreative thinker with strong analytical skillsResults oriented with a strong customer focusAbility to work in a team environmentStrong technical communication skillsAbility to prioritize work to meet tight deadlinesAbility to learn and keep pace with the latest technology advances and quickly grasp new technologies to support the environment and contribute to project deliverablesPreferred QualificationsMaster’s degree in a technical field (e.g. computer science, math, engineering) Understanding of big data and real time streaming analytics processing architecture and ecosystemsExperience with data warehousing architecture and implementation, including hands on experience with source to target mappings and developing ETL codeExperience with advanced analytics and machine learning concepts and technology implementationsExperience with data analysis and using data visualization tools to describe dataRelevant technology or platform certification (AWS, Microsoft, etc.) Software development experience in relevant programming languages (e.g. Java, Python, Scala, Node.js)Working ConditionsOffice environmentOccasional travel', 'Three years of experience with Linux operations and development, including basic commands and shell scripting', 'Experience with advanced analytics and machine learning concepts and technology implementations', 'Office environment', 'Work collaboratively with other engineers, data scientists, analytics teams, and business product owners in an agile environment:', 'Bachelor’s degree in computer science, math, engineering, or relevant technical field', 'Our commitment to inclusion & diversity means that we value differences. We encourage the unique perspectives of individuals and are dedicated to creating a respectful and inclusive work environment.', 'Results oriented with a strong customer focus', 'Compensation', 'Demonstrated skills in delivery of high quality technical project solutions', 'Ability to work in a team environment', 'Expertise in SQL for data profiling, analysis, and extraction', 'Ability to learn and keep pace with the latest technology advances and quickly grasp new technologies to support the environment and contribute to project deliverables', 'Architect, build, and support the operation of Cloud and On-Premises enterprise data infrastructure and tools.', 'Wealth + Health', 'Understand and implement best practices in management of enterprise data, including master data, reference data, metadata, data quality and lineage.', 'Familiarity with data science techniques and frameworks', 'Lead the development of data APIs and data delivery services to support critical operational processes, analytical models and machine learning applications.', 'Three years of experience architecting, building and administering large-scale distributed applications', 'Work collaboratively with other engineers, data scientists, analytics teams, and business product owners in an agile environment:Architect, build, and support the operation of Cloud and On-Premises enterprise data infrastructure and tools.Design robust, reusable and scalable data driven solutions and data pipeline frameworks to automate the ingestion, processing and delivery of both structured and unstructured batch and real-time streaming data.Lead the development of data APIs and data delivery services to support critical operational processes, analytical models and machine learning applications.Assist the selection and integration of data related tools, frameworks and applications required to expand platform capabilities.Understand and implement best practices in management of enterprise data, including master data, reference data, metadata, data quality and lineage.', 'Office environmentOccasional travel', 'Ability to prioritize work to meet tight deadlines', 'Relevant technology or platform certification (AWS, Microsoft, etc.) ', 'Preferred Qualifications', 'Creative thinker with strong analytical skills']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Zachary Piper Solutions,"Raleigh, NC",2 weeks ago,Be among the first 25 applicants,"['', 'Qualifications For The Data Engineer Include']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Staff Data Engineer | Big Data | Autonomous Driving  ,Glocomms,"Newark, CA",,N/A,"['', 'Staff Data Engineer | Big Data | Autonomous Driving ', 'Mentoring is fine with you. ', 'Whom are you joining?', 'Containerization is second nature - Docker, Kubernetes etc... ', 'Extensive programming and software engineering experience', 'Who are you?', 'Staff level, ready to lead and architect highly scalable systems to ingest and progress petabytes of data per day.', ""Staff level, ready to lead and architect highly scalable systems to ingest and progress petabytes of data per day.You want to be designing hands-on applications for pipeline and data management. You're familiar with big data tools such as Spark, Kafka, Cassandra to name a few. Mentoring is fine with you. You are a leader - you're going to be setting standards and driving best practice. you're going to be implementing ML pipelines for the Data Science team too."", 'Changing the face of the automotive industry, ', ""You're familiar with big data tools such as Spark, Kafka, Cassandra to name a few. "", ""you're going to be implementing ML pipelines for the Data Science team too."", ""You've been involved with the development of large scale data platforms for a number of yearsContainerization is second nature - Docker, Kubernetes etc... PySpark, Spark is something you're well atune with. Data streaming platforms is also something you've been doing day-to-day. Extensive programming and software engineering experience"", ""PySpark, Spark is something you're well atune with. Data streaming platforms is also something you've been doing day-to-day. "", ""You've been involved with the development of large scale data platforms for a number of years"", ""You are a leader - you're going to be setting standards and driving best practice. "", ""Not only the next biggest name in the world of autonomous driving, but you're also joining a business committed to sustainable energy by creating electric vehicles centered around the human experience. "", 'You want to be designing hands-on applications for pipeline and data management. ', 'Typical experience looks like... ']",Director,Full-time,Information Technology,Automotive,2021-03-18 14:34:51
Data Engineer,LightBox,"Irvine, CA",1 month ago,Be among the first 25 applicants,"['', ""Bachelor's degree or better in Computer Science or equivalentMust have an excellent academic record with a good grounding in Software Engineering theory including at least one modern programming languageFamiliar with Standards, concepts, practices, and procedures within the field of Computer Science"", ""Bachelor's degree or better in Computer Science or equivalent"", 'Data Lake & Warehouse Modeling', 'Create data pipelines to prepare and maintain data to solve specific use cases', 'Experience building scalable and maintainable data intensive applications', 'Driven to continually learn about and incorporate new technologies', 'Pipeline orchestration technology, such as Prefect, Luigi or Airflow', 'Create data pipelines to prepare and maintain data to solve specific use casesModel and architect data to solve business problemsAdhere to high-quality development principles while delivering solutions on-time and on-budgetAnalyze and resolve technical and application problemsMigrate existing workflows to newer infrastructuresAnalyze use cases and propose solutions to meet business objectives', 'Who you are:', 'Understanding and integrating human and machine workflows', 'Data catalog platforms, such as Amundsen', 'Any RDBMS, such as MySQL, MS-SQL', '2 to 5 years’ experience as a software engineer', 'Other Desirable Attributes', 'Cloud infrastructure, such as Azure, AWS, or Google Cloud', 'Migrate existing workflows to newer infrastructures', 'At least one modern programming language, preferably PythonKnowledge and experience with OOP design patternsAny RDBMS, such as MySQL, MS-SQLExperience building scalable and maintainable data intensive applicationsPipeline orchestration technology, such as Prefect, Luigi or AirflowPipeline transformation tools, preferably dbtBig data technologies, such as Hadoop & Spark, ideally including Python pandas and Dask', 'Position Overview', 'Key Knowledge & Skills', 'Education', 'Familiar with Standards, concepts, practices, and procedures within the field of Computer Science', 'Core Competencies', 'Keen interest in data engineering and a “tinkering” mindsetDriven to continually learn about and incorporate new technologiesThrive in a self-driven environmentUnderstanding and integrating human and machine workflowsData Lake & Warehouse ModelingGit, Docker, KubernetesCloud infrastructure, such as Azure, AWS, or Google CloudDevelopment on Linux', 'Location – Spatial Data', 'NO TELEPHONE CALLS OR AGENCY SOLICITATION PLEASE.', 'Development on Linux', 'Big data technologies, such as Hadoop & Spark, ideally including Python pandas and Dask', 'Keen interest in data engineering and a “tinkering” mindset', 'Distributed systems – storage, compute & access patterns', 'Graph database, such as Neo4j', 'Experience', 'Git, Docker, Kubernetes', 'Analyze and resolve technical and application problems', 'Pipeline transformation tools, preferably dbt', 'Thrive in a self-driven environment', 'Distributed systems – storage, compute & access patternsUnstructured Data extraction – pdf, scrapingGraph database, such as Neo4jLocation – Spatial DataData catalog platforms, such as Amundsen', 'Unstructured Data extraction – pdf, scraping', 'At least one modern programming language, preferably Python', 'Model and architect data to solve business problems', 'Analyze use cases and propose solutions to meet business objectives', 'Knowledge and experience with OOP design patterns', 'Adhere to high-quality development principles while delivering solutions on-time and on-budget', 'What You Will Do And Achieve', 'Must have an excellent academic record with a good grounding in Software Engineering theory including at least one modern programming language']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,"Redaptive, Inc",United States,4 weeks ago,26 applicants,"['', 'Volunteer Activities\xa0', 'Architect, design, build and deploy tooling and infrastructure to ensure reliable, high-quality meter and billing data is collected, stored, and served via REST APIs.', 'Experience in Agile software development methods and tooling, such as Jira ticketing, Kanban process, etc.', '3+ years of software development experience.BS degree preferred in Computer Science or related technical field.\xa0Equivalent practical experience on the job as a data or software engineer will also qualify candidates, depending on level and type of experience.Experience with an open-source oriented stack, ideally at least two of the following languages: Java / Spring,\xa0Node.js, Python.Experience with cloud computing, preferably the AWS suite. For example: IOT, S3, Lambdas, SQS, RDBMS (PostgreSQL), API Gateway, Sagemaker.Experience in Agile software development methods and tooling, such as Jira ticketing, Kanban process, etc.Self-starter that thrives in a fast-paced environment with changing requirements.Domain knowledge in energy efficiency, solar/storage, or electric utilities is a plus.\xa0.\xa0', '401(k) Match\xa0', 'Experience with an open-source oriented stack, ideally at least two of the following languages: Java / Spring,\xa0Node.js, Python.', 'Self-starter that thrives in a fast-paced environment with changing requirements.', 'Equity\xa0', 'Job Requirements/Qualifications:', 'Permanent authorization to work in the United States is required at this time.', 'Build and maintain models of energy savings estimations and calculations based on underlying timeseries and equipment data.', 'Fully remote-within-USA work environment\xa0', 'Monthly Virtual Happy Hour', 'Equity\xa0Benefits start your first day:\xa0Medical, Dental, Vision, Life Insurance\xa0Flexible\xa0Spending Accounts: Healthcare and Dependent Care401(k) Match\xa0Monthly Internet and Cell Phone Allowance\xa0Flexible Time Away\xa0Fully remote-within-USA work environment\xa0Volunteer Activities\xa0Monthly Virtual Happy Hour', 'Develop data pipelines to bring real-time data from a variety of IOT and operational systems into Redaptive’s data lake.', 'Domain knowledge in energy efficiency, solar/storage, or electric utilities is a plus.\xa0.\xa0', 'About Redaptive Inc.', 'Experience with cloud computing, preferably the AWS suite. For example: IOT, S3, Lambdas, SQS, RDBMS (PostgreSQL), API Gateway, Sagemaker.', 'Our core platform is a meter data backhaul and management system gathering energy data from IoT devices in real-time with the ultimate goal of enabling energy savings for our customers. This platform powers numerous energy savings models, processes, and dashboards.', 'Flexible Time Away\xa0', 'Job Responsibilities:', 'The Perks!', 'Redaptive is the leading Efficiency-as-a-Service company that enables businesses to meet largescale energy efficiency and strategic technology objectives. With over $1 billion in project lending capacity, Redaptive deploys best-in-class energy efficiency technologies across large commercial real estate portfolios through a unique, data driven shared savings model. Redaptive is backed by CarVal, ENGIE New Ventures, Linse Capital, CBRE, and Evergy Ventures.', '3+ years of software development experience.', 'Develop performance monitoring and anomaly detection systems to flag performance outliers across a variety of energy conservation technologies, including lighting, HVAC, solar, etc.', 'Benefits start your first day:\xa0Medical, Dental, Vision, Life Insurance\xa0', 'Our company culture is fun, collaborative, and fast paced. We are passionate about changing the world and helping our customers to become more environmentally sustainable and profitable. We are looking for team members who are driven, passionate, and want to take on a diverse set of challenges to help grow a great company. Redaptive, Inc is an equal employment opportunity employer and all qualified applicants will receive consideration for employment.', 'Monthly Internet and Cell Phone Allowance\xa0', 'Work closely with front-end engineers to develop the APIs that feed internal and external user interfaces such as Network Operations Center / Meter Reliability, customer-facing dashboards, invoice / payment systems, and presentation of new customer opportunities.', 'Redaptive is seeking a Data Engineer to join our Engineering team.\xa0As a data engineer, you will be working with Engineering Management, DevOps, Data Science, Product Managers, and QA to develop a highly scalable and real-time energy management solution.', '\xa0', 'This role can be based remotely anywhere in the USA and will work closely with international teammates.\xa0This position reports to the Senior Manager of Engineering & Data Science.', 'Work closely with internal BI / Strategy teams to provide underlying data models, infrastructure, and tooling to enable internal intelligence.', 'BS degree preferred in Computer Science or related technical field.\xa0Equivalent practical experience on the job as a data or software engineer will also qualify candidates, depending on level and type of experience.', 'Flexible\xa0Spending Accounts: Healthcare and Dependent Care', 'Architect, design, build and deploy tooling and infrastructure to ensure reliable, high-quality meter and billing data is collected, stored, and served via REST APIs.Develop data pipelines to bring real-time data from a variety of IOT and operational systems into Redaptive’s data lake.Build and maintain models of energy savings estimations and calculations based on underlying timeseries and equipment data.Develop performance monitoring and anomaly detection systems to flag performance outliers across a variety of energy conservation technologies, including lighting, HVAC, solar, etc.Work closely with front-end engineers to develop the APIs that feed internal and external user interfaces such as Network Operations Center / Meter Reliability, customer-facing dashboards, invoice / payment systems, and presentation of new customer opportunities.Work closely with internal BI / Strategy teams to provide underlying data models, infrastructure, and tooling to enable internal intelligence.']",Associate,Full-time,Engineering,Renewables & Environment,2021-03-18 14:34:51
Data Integration Engineer- Midwest/Southwest Region,Navitus Health Solutions,United States,2 days ago,Be among the first 25 applicants,"[' Ability to effectively prioritize and execute tasks in a high-pressure environment.', ' Participate and provide technical leadership in all phases of a project from discovery and planning through implementation and delivery.', ' Other duties as assigned/required.', ' Event Driven Architecture', "" Identify roadblocks or areas of concern and bring potential solutions to management's attention."", ' Learn new things, and grow rapidly from constant exposure to innovative ideas, concepts and patterns.', ' Excellent understanding of coding methods and best practices', 'Education', ' Define and maintain standards for data object naming and other data administration standards.', ' Flexible and adaptable in regards to learning and understanding new technologies.', ' Design and manage conceptual, logical, and physical data models, including versioning and governance.', ' Contribute to a positive team atmosphere.', ' Fundamental knowledge of working with structured, unstructured, and semi structured data.', ' Informatica', 'Experience', ' Identify and document requirements for database best practices and efficiencies.', ' Ensure efforts delight the customer.', ' Work with other technical teams to define and identify cross-functional solutions for integration with new or existing technologies.', ""  Ensure efforts delight the customer.  Provide superior customer service utilizing a high-touch, customer centric approach focused on collaboration and communication.  Contribute to a positive team atmosphere.  Innovate and create value for the customer.  Collaborate with business teams to identify business needs and translate into technical requirements.  Work with stakeholders to design and develop data ingest, validation, and transform pipelines.  Work with other technical teams to define and identify cross-functional solutions for integration with new or existing technologies.  Lead the effort for design, development and implementation on a project-by-project basis to deliver solutions which are easily maintainable and scalable.  Participate and provide technical leadership in all phases of a project from discovery and planning through implementation and delivery.  Research and identify new technologies and technology upgrades which will enhance the ability to deliver robust solutions.  Wear many hats and gain experience with tools, technologies and platforms across many technology stacks.  Learn new things, and grow rapidly from constant exposure to innovative ideas, concepts and patterns.  Create new data pipelines, while maintaining and continuously improving existing pipelines.  Participate in data strategy and road map planning, data architecture definitions, centralized data platform design, and implementation.  Mentor less experienced team members as necessary.  Code reviews on an as needed basis to ensure conformity to company development guidelines.  Interface with business units to remediate and prevent future data quality issues  Generate and implement data quality rules to ensure accuracy and excellence throughout the data landscape.  Collaborate closely with IT development teams and Analytics to implement advanced strategies for gathering, reviewing and analyzing data requirements.  Identify and document requirements for database best practices and efficiencies.  Develop and maintain user manuals and guidelines.  Estimate effort required for the design, development and implementation on a project-by-project basis.  Identify roadblocks or areas of concern and bring potential solutions to management's attention.  Support various technologies deployed at Navitus regardless of development platform.  Ensure the data strategies and architectures are in regulatory compliance.  Assist in the development and maintenance of a data catalog.  Ability to diagnose and troubleshoot data issues, recognizing common data integration and transformation patterns.  Maintain enterprise metadata so that it can be reported and leveraged for re-use.  Provide data analyst services that facilitate an efficient and accurate data load into integrated schemas within the different databases.  Have an overall view of where data is being used and assess impact of changes across systems and processes.  Define and maintain standards for data object naming and other data administration standards.  Design and manage conceptual, logical, and physical data models, including versioning and governance.  Participate in, adhere to, and support compliance program objectives.  Other duties as assigned/required. "", ' Participate in data strategy and road map planning, data architecture definitions, centralized data platform design, and implementation.', ' Experience with NoSQL such as MongoDB is desired', ' Interface with business units to remediate and prevent future data quality issues', ' Knowledge of healthcare industry practices, HIPAA, and applicable data privacy practices preferred.', ' Application Program Interface (API)', ' Estimate effort required for the design, development and implementation on a project-by-project basis.', ' Create new data pipelines, while maintaining and continuously improving existing pipelines.', ' Participate in, adhere to, and support compliance program objectives.', ' Ability to work both independently and in a team-oriented, collaborative environment.', ' Able to communicate effectively with members of interdisciplinary teams.', ""This role is intended to be filled by a resident of the Midwest or Southwest of the United States and is open to a fully remote work environment*The Data Integration Engineer ensures efforts are in alignment with the Enterprise Architecture team to support customer-focused objectives and the IT Vision, a collaborative partner delivering innovative ideas, solutions and services to simplify people's lives. The Data Integration Engineer will help define and grow our data centric architecture. The Data Integration Engineer is responsible for implementing data ingestion, validation, and transformation pipelines. The Data Integration engineer will design and maintain batch and streaming integrations across a variety of data domains and platforms. The Data Integration Engineer will be responsible for designing streaming capabilities for the internal and external movement of data to support our customer's and client needs. The Data Integration Engineer will align our data structures, designs, and capabilities with the needs of the organization and in support of our strategic direction. The ideal candidate is experienced in Domain driven development, event driven architecture with real time data transformations.Job Responsibilities  Ensure efforts delight the customer.  Provide superior customer service utilizing a high-touch, customer centric approach focused on collaboration and communication.  Contribute to a positive team atmosphere.  Innovate and create value for the customer.  Collaborate with business teams to identify business needs and translate into technical requirements.  Work with stakeholders to design and develop data ingest, validation, and transform pipelines.  Work with other technical teams to define and identify cross-functional solutions for integration with new or existing technologies.  Lead the effort for design, development and implementation on a project-by-project basis to deliver solutions which are easily maintainable and scalable.  Participate and provide technical leadership in all phases of a project from discovery and planning through implementation and delivery.  Research and identify new technologies and technology upgrades which will enhance the ability to deliver robust solutions.  Wear many hats and gain experience with tools, technologies and platforms across many technology stacks.  Learn new things, and grow rapidly from constant exposure to innovative ideas, concepts and patterns.  Create new data pipelines, while maintaining and continuously improving existing pipelines.  Participate in data strategy and road map planning, data architecture definitions, centralized data platform design, and implementation.  Mentor less experienced team members as necessary.  Code reviews on an as needed basis to ensure conformity to company development guidelines.  Interface with business units to remediate and prevent future data quality issues  Generate and implement data quality rules to ensure accuracy and excellence throughout the data landscape.  Collaborate closely with IT development teams and Analytics to implement advanced strategies for gathering, reviewing and analyzing data requirements.  Identify and document requirements for database best practices and efficiencies.  Develop and maintain user manuals and guidelines.  Estimate effort required for the design, development and implementation on a project-by-project basis.  Identify roadblocks or areas of concern and bring potential solutions to management's attention.  Support various technologies deployed at Navitus regardless of development platform.  Ensure the data strategies and architectures are in regulatory compliance.  Assist in the development and maintenance of a data catalog.  Ability to diagnose and troubleshoot data issues, recognizing common data integration and transformation patterns.  Maintain enterprise metadata so that it can be reported and leveraged for re-use.  Provide data analyst services that facilitate an efficient and accurate data load into integrated schemas within the different databases.  Have an overall view of where data is being used and assess impact of changes across systems and processes.  Define and maintain standards for data object naming and other data administration standards.  Design and manage conceptual, logical, and physical data models, including versioning and governance.  Participate in, adhere to, and support compliance program objectives.  Other duties as assigned/required. Responsibilities (including One Or More Of The Following)  Oracle  MS SQL  Informatica  Automic  .NET Core  Microservices Architecture  Event Driven Architecture  Confluent/Kafka  Application Program Interface (API)  Qlik Replicate (CDC) EducationCollege degree or equivalent experience required in the field of computer science, information systems or software engineering and/or a minimum of 5+ years of professional experience.Experience  Excellent understanding of coding methods and best practices  Proven experience collaborating with business teams to analyze business requirements and develop technical specifications.  Proven experience designing complex data flows with end-to-end automation of complex requirements.  Proven experience with streaming platforms such as Kafka.  Proven experience developing test cases and test plans.  Demonstrated understanding of Software Development Life Cycle.  Experience with relational databases.  Background with unit and integration testing basics  Fundamental knowledge of distributed data processing and storage  Fundamental knowledge of working with structured, unstructured, and semi structured data.  Fundamental knowledge of API frameworks  Knowledge of healthcare industry practices, HIPAA, and applicable data privacy practices preferred.  Experience developing data exchange pipelines using HL7 FHIR, SOA, XML / JSON, and REST API would be a plus.  Experience with NoSQL such as MongoDB is desired Key Skills/Competencies  Able to communicate effectively with members of interdisciplinary teams.  Flexible and adaptable in regards to learning and understanding new technologies.  Strong written, oral, and interpersonal communication skills.  Ability to conduct research into software-related issues and products.  Highly self-motivated and directed.  Keen attention to detail.  Proven analytical and problem-solving abilities.  Ability to effectively prioritize and execute tasks in a high-pressure environment.  Ability to work both independently and in a team-oriented, collaborative environment.  On-call availability.  Some travel may be required.  The ability to consistently interact cooperatively and respectfully with other employees. Equal Opportunity Employer Minorities/Women/Protected Veterans/Disabled"", ' Have an overall view of where data is being used and assess impact of changes across systems and processes.', ' Proven experience collaborating with business teams to analyze business requirements and develop technical specifications.', ' Proven experience developing test cases and test plans.', ' Ensure the data strategies and architectures are in regulatory compliance.', ' Confluent/Kafka', ' Innovate and create value for the customer.', ' Maintain enterprise metadata so that it can be reported and leveraged for re-use.', ' Code reviews on an as needed basis to ensure conformity to company development guidelines.', ' Oracle', ' Highly self-motivated and directed.', ' Automic', 'Job Responsibilities', ' The ability to consistently interact cooperatively and respectfully with other employees.', 'Responsibilities (including One Or More Of The Following)', ' Experience with relational databases.', ' Generate and implement data quality rules to ensure accuracy and excellence throughout the data landscape.', ' On-call availability.', '  Able to communicate effectively with members of interdisciplinary teams.  Flexible and adaptable in regards to learning and understanding new technologies.  Strong written, oral, and interpersonal communication skills.  Ability to conduct research into software-related issues and products.  Highly self-motivated and directed.  Keen attention to detail.  Proven analytical and problem-solving abilities.  Ability to effectively prioritize and execute tasks in a high-pressure environment.  Ability to work both independently and in a team-oriented, collaborative environment.  On-call availability.  Some travel may be required.  The ability to consistently interact cooperatively and respectfully with other employees. ', ' Proven experience designing complex data flows with end-to-end automation of complex requirements.', ' Research and identify new technologies and technology upgrades which will enhance the ability to deliver robust solutions.', ' Demonstrated understanding of Software Development Life Cycle.', ' Keen attention to detail.', ' Assist in the development and maintenance of a data catalog.', ' Ability to diagnose and troubleshoot data issues, recognizing common data integration and transformation patterns.', ' Proven experience with streaming platforms such as Kafka.', ' Background with unit and integration testing basics', ' Some travel may be required.', ' Collaborate with business teams to identify business needs and translate into technical requirements.', ' Work with stakeholders to design and develop data ingest, validation, and transform pipelines.', ' MS SQL', ' Develop and maintain user manuals and guidelines.', ' Fundamental knowledge of API frameworks', ' Ability to conduct research into software-related issues and products.', '  Oracle  MS SQL  Informatica  Automic  .NET Core  Microservices Architecture  Event Driven Architecture  Confluent/Kafka  Application Program Interface (API)  Qlik Replicate (CDC) ', ' Wear many hats and gain experience with tools, technologies and platforms across many technology stacks.', ' Support various technologies deployed at Navitus regardless of development platform.', ' Proven analytical and problem-solving abilities.', ' Qlik Replicate (CDC)', '  Excellent understanding of coding methods and best practices  Proven experience collaborating with business teams to analyze business requirements and develop technical specifications.  Proven experience designing complex data flows with end-to-end automation of complex requirements.  Proven experience with streaming platforms such as Kafka.  Proven experience developing test cases and test plans.  Demonstrated understanding of Software Development Life Cycle.  Experience with relational databases.  Background with unit and integration testing basics  Fundamental knowledge of distributed data processing and storage  Fundamental knowledge of working with structured, unstructured, and semi structured data.  Fundamental knowledge of API frameworks  Knowledge of healthcare industry practices, HIPAA, and applicable data privacy practices preferred.  Experience developing data exchange pipelines using HL7 FHIR, SOA, XML / JSON, and REST API would be a plus.  Experience with NoSQL such as MongoDB is desired ', ' Fundamental knowledge of distributed data processing and storage', ' Collaborate closely with IT development teams and Analytics to implement advanced strategies for gathering, reviewing and analyzing data requirements.', ' .NET Core', 'Key Skills/Competencies', ' Provide data analyst services that facilitate an efficient and accurate data load into integrated schemas within the different databases.', ' Provide superior customer service utilizing a high-touch, customer centric approach focused on collaboration and communication.', ' Strong written, oral, and interpersonal communication skills.', ' Lead the effort for design, development and implementation on a project-by-project basis to deliver solutions which are easily maintainable and scalable.', ' Microservices Architecture', ' Experience developing data exchange pipelines using HL7 FHIR, SOA, XML / JSON, and REST API would be a plus.', ' Mentor less experienced team members as necessary.']",Not Applicable,Full-time,Information Technology,Hospital & Health Care,2021-03-18 14:34:51
Data Engineer (Remote) - LAAgencia,LAAgencia ,United States,5 days ago,Be among the first 25 applicants,"['', 'Must be within +/- 2 hours of Eastern Standard Timezone (NYC)', 'Deploying and scaling machine learning models in production.', 'BA/BS degree in Computer Science or a related technical field, or equivalent practical experience.', '+ 3 years of Experience in Amazon AWS, DevOps, and Automation (Cloud Formation)', 'Architecting, building, and maintaining modern, scalable data architectures on AWSBuilding resilient production ETL pipelines using workflow orchestration tools such as Airflow, Prefect, AWS Step FunctionsDeploying and scaling machine learning models in production.Data exploration, analysis, and reporting with an eye towards developing a narrative using Notebooks.', 'Resume must demonstrate English ability (English fluently speaking is necessary)', 'BA/BS degree in Computer Science or a related technical field, or equivalent practical experience.Advanced experience in Python with an excellent understanding of computer science fundamentals, complex data structure, data processing, data quality, data lifecycle, and algorithms.+ 3 years of Experience in Amazon AWS, DevOps, and Automation (Cloud Formation)AWS certification, or progress toward, at the associate level (Solutions Architect or Developer), or specialty (Big Data) a strong advantage.Enjoys collaborating with other engineers on architecture and sharing designs with the teamExcellent verbal and written English communication.Interacts with others using sound judgment, good humor, and consistent fairness in a fast-paced environment', 'Building resilient production ETL pipelines using workflow orchestration tools such as Airflow, Prefect, AWS Step Functions', 'Job Description', 'Enjoys collaborating with other engineers on architecture and sharing designs with the team', 'Advanced experience in Python with an excellent understanding of computer science fundamentals, complex data structure, data processing, data quality, data lifecycle, and algorithms.', 'Data exploration, analysis, and reporting with an eye towards developing a narrative using Notebooks.', 'Interacts with others using sound judgment, good humor, and consistent fairness in a fast-paced environment', 'AWS certification, or progress toward, at the associate level (Solutions Architect or Developer), or specialty (Big Data) a strong advantage.', 'Architecting, building, and maintaining modern, scalable data architectures on AWS', 'Excellent verbal and written English communication.', 'Requirements']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,"TracFone Wireless, Inc.","Miami, FL",3 weeks ago,Be among the first 25 applicants,"['', 'The position is responsible for constructing and optimizing our data lake and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. Incumbent is expected to be experienced as a data pipeline builder and a data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data-analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. Individual must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives. The incumbent will stage and make the data ready for Data Science, Analytics, Artificial Intelligence and Machine Learning purposes.', 'Culture:\xa0An entrepreneurial focus, where ownership and ingenuity are expected.', 'Requirements:', 'Strong knowledge of AWS data related services (DMS, Glue, EMR, S3, Athena, Lambda, Redshift, DynamoDB, KMS).', 'Must have 5+ years of Data Sytems/Warehouse/Lakes/Equivalent sytem experience with multiple OS platforms.', 'Strong analytical and problem solving skills.', '\xa0Must possess or develop ability to converse with the business, development, operations, carriers, vendors, etc.', 'Benefits:\xa0', 'Shell scripting, SQL Stored Procedures IBM/other databases and JAVA skills are desired.', 'Must possess or develop business knowledge of how customer transactions reflect the business logic that drive the existing or future code.', '\ufeffWhat you will do:', 'Must have 3 + years of Python and Pyspark.', 'Strong experience of different architectural components comprising the middle-ware is required.', 'Work on T-shaped skills across Data Lake/Data Pipeline to take data across from the source all the way to the consumption layer.', 'Benefits:\xa0Excellent health benefits, Matching 401K, and education reimbursement.', 'Responsible for maintaining integrity between multiple databases, including but not limiting to, CLARIFY, BI, BRM, OFS (Inbound/Outbound Process) and MAX.', 'Strong knowledge of Unix/AIX and Windows operating systems, standard concepts, practices, and procedures within the relational database field.', 'Teamwork:', 'Recognized leader:\xa0Rated #1 Prepaid Wireless Provider in the U.S.Technology driven:\xa0Opportunity to work with state-of-the-art technologyTeamwork:\xa0A supportive team environment that thrives on innovation.Culture:\xa0An entrepreneurial focus, where ownership and ingenuity are expected.Benefits:\xa0Excellent health benefits, Matching 401K, and education reimbursement.', 'Strong database/relational/non-relational concepts required.', 'A bachelor’s degree from an accredited college in Computer Science or equivalent. ', 'A bachelor’s degree from an accredited college in Computer Science or equivalent. Strong knowledge of AWS Cloud Systems.Strong knowledge of AWS data related services (DMS, Glue, EMR, S3, Athena, Lambda, Redshift, DynamoDB, KMS).Strong knowledge of Python and Pyspark (Hive).Strong knowledge of Unix/AIX and Windows operating systems, standard concepts, practices, and procedures within the relational database field.Strong database/relational/non-relational concepts required.Strong analytical and problem solving skills.Hadoop, Kafka, Spark, Scala (preferred).Must have 5+ years of Data Sytems/Warehouse/Lakes/Equivalent sytem experience with multiple OS platforms.Must have 3 + years of Python and Pyspark.Must have 3 + years of AWS or alternative cloud systems.Must possess or develop business knowledge of how customer transactions reflect the business logic that drive the existing or future code.\xa0Must possess or develop ability to converse with the business, development, operations, carriers, vendors, etc.Shell scripting, SQL Stored Procedures IBM/other databases and JAVA skills are desired.Strong experience of different architectural components comprising the middle-ware is required.', 'Strong knowledge of AWS Cloud Systems.', 'Must have 3 + years of AWS or alternative cloud systems.', 'Responsibilities:', 'Culture:\xa0', 'Individual must be able to work well with others in a team environment and across departments. The incumbent should mentor junior staff in business knowledge and technical skills.', 'Technology driven:\xa0Opportunity to work with state-of-the-art technology', 'Design/Implement QA framework within the Data-Lake. Design and implement test strategies, write test cases, design/implement test automation', 'The position is responsible for data analysis, trending & root-cause-identification of customer impacting issues pertaining to customer impacting systems across all the enterprise on all channels for all brands. Individual must be very strong in logical & analytical thinking with experience in SQL, Oracle SQL (other SQLs desired), shell script with attention to detail.\xa0Ability to understand data patterns from large sets of structured and unstructured data. Monitor success & failure rates on all channels for all key provisioning process such as Activation, Reactivation, Purchases, Redemptions, Ports and Upgrades. ', 'Responsible to write infra structure as code using Terraform', 'Position Highlights:', 'Tracfone Wireless is an Equal Employment Opportunity employer. We embrace diversity and do not discriminate based on race, religion, color, national origin, sex, sexual orientation, age, veteran status, disability status, or any other applicable characteristics protected by law', 'Recognized leader:\xa0Rated #1 Prepaid Wireless Provider in the U.S.', 'Pipeline data using Cloud or on-premises technologies: AWS Big Data Services, Hadoop, HDFS, AI/Deep Learning API’s, SQL/NOSQL, Unstructured Databases, etc.', 'Build data streams to ingest, load, transform, group, logically join and assemble data ready for data analysis / analytics/ reporting / next best action / next-best-offer, build data streams to ingest, load, transform, group, logically join and assemble data ready for data analysis / analytics/ reporting / next best action / next best offer.Pipeline data using Cloud or on-premises technologies: AWS Big Data Services, Hadoop, HDFS, AI/Deep Learning API’s, SQL/NOSQL, Unstructured Databases, etc.Responsible to write infra structure as code using TerraformDesign/Implement QA framework within the Data-Lake. Design and implement test strategies, write test cases, design/implement test automationResponsible for maintaining integrity between multiple databases, including but not limiting to, CLARIFY, BI, BRM, OFS (Inbound/Outbound Process) and MAX.Work on T-shaped skills across Data Lake/Data Pipeline to take data across from the source all the way to the consumption layer.Maintain knowledge and proficiency of current and upcoming hardware/software technologies. Mentor junior staff in ramping up analytical and technical skills.', 'Hadoop, Kafka, Spark, Scala (preferred).', 'Recognized leader:', '\xa0', 'Technology driven:', 'Maintain knowledge and proficiency of current and upcoming hardware/software technologies. Mentor junior staff in ramping up analytical and technical skills.', 'Teamwork:\xa0A supportive team environment that thrives on innovation.', 'Strong knowledge of Python and Pyspark (Hive).', 'Build data streams to ingest, load, transform, group, logically join and assemble data ready for data analysis / analytics/ reporting / next best action / next-best-offer, build data streams to ingest, load, transform, group, logically join and assemble data ready for data analysis / analytics/ reporting / next best action / next best offer.']",Director,Full-time,Information Technology,Telecommunications,2021-03-18 14:34:51
Data Engineer,Recorded Future,"Somerville, MA",1 week ago,46 applicants,"['', 'What You Should Bring To The Data Engineer Role', ' Data: You have some experience working with databases and are comfortable manipulating heterogeneous data sets. ', 'Why should you join Recorded Future?', ' Thoroughly understand the data we give to clients. Collaborate with your teammates to identify opportunities to improve the data. ', '  Build code to condense massive amounts of data into understandable and actionable summaries at scale.   Thoroughly understand the data we give to clients. Collaborate with your teammates to identify opportunities to improve the data.   Suggest and drive projects to expand the use of modern best practices in our codebase. Work across programming languages to take advantage of the right tools for each job.  ', ' Curiosity: You enjoy puzzles and are invigorated by the challenge of understanding what a complex piece of code does. ', '  Programming: You are comfortable writing production code in Python and JavaScript. You are familiar with best practices in software engineering and are excited about spreading their use. Bonus for experience with Java-backend JavaScript technologies such as TypeScript, NodeJS, Nashorn, GraalVM, or Rhino.   Curiosity: You enjoy puzzles and are invigorated by the challenge of understanding what a complex piece of code does.   Data: You have some experience working with databases and are comfortable manipulating heterogeneous data sets.   Excellent communication: Your clarity of thought is always apparent in your crisp and articulate emails, Slack chats, phone calls, and in-person conversations.  ', ' Build code to condense massive amounts of data into understandable and actionable summaries at scale. ', 'What You’ll Do As a Data Engineer', ' Programming: You are comfortable writing production code in Python and JavaScript. You are familiar with best practices in software engineering and are excited about spreading their use. Bonus for experience with Java-backend JavaScript technologies such as TypeScript, NodeJS, Nashorn, GraalVM, or Rhino. ', ' Excellent communication: Your clarity of thought is always apparent in your crisp and articulate emails, Slack chats, phone calls, and in-person conversations. ', ' Suggest and drive projects to expand the use of modern best practices in our codebase. Work across programming languages to take advantage of the right tools for each job. ']",Entry level,Full-time,Information Technology,Computer & Network Security,2021-03-18 14:34:51
Data Engineer,Resource Logistics Inc.,"Houston, TX",6 days ago,Be among the first 25 applicants,"['', 'Location Houston, TX', 'Data Engineer', 'Microsoft Power BI,', 'Azure function, Azure service apps, HDInsight clusters or similar concept in other cloud platforms', 'Duration 12 months', ', pyspark', 'Background in Designing and building Distributed systems and data pipelines such as Data factory']",Entry level,Contract,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,GTL,"Pittsburgh, PA",3 weeks ago,Be among the first 25 applicants,"['', 'Write GitLab CI/CD Pipelines for infrastructure and for unit testing and deploying code', ' Partner with internal business stakeholders to engineer batch and real-time streaming data pipelines Partner with DBA and other developers to harness application data and design database structures to obtain best results from the data Develop and maintain data pipelines in Apache Airflow and Apache Spark Write GitLab CI/CD Pipelines for infrastructure and for unit testing and deploying code Assist in maintaining efficient operation of reports, visualizations and ETL processes within the GTL back-office Resolve defects/bugs during QA testing, pre-production, production, and post-release patches Have a quality mindset and work hard to prevent bugs through unit testing, test-driven development, version control, continuous integration and deployment Troubleshoot data issues, validate result sets, and implement process improvements Use acquired information to help build out a big data schema and infrastructure Contribute to the design and architecture of the project Conduct design and code reviews  Operate within Agile Development environment and apply the methodologies Generate tracking and monitoring tools to validate data Review and respond to system alerts in internal and external alerting and metrics systems Document your work, system architecture and processes in JIRA, Confluence and other documentation tools. Respond to user requests and document work done on service tickets in ticketing system Support proper data governance processes and policies; implement and/or validate data lineage, quality checks, classification, etc. ', 'Troubleshoot data issues, validate result sets, and implement process improvements', 'Experience in orchestrating data pipelines using Apache Airflow, Snowflake and Spark', 'Knowledge of Containerization and experience in Kubernetes.', 'Assist in maintaining efficient operation of reports, visualizations and ETL processes within the GTL back-office', 'Experience with Informatica ETL software', 'Experience in developing and maintaining ETL processes', 'Develop and maintain data pipelines in Apache Airflow and Apache Spark', 'Experience with AWS services like S3, EMR, EC2 or Azure equivalents', 'Excellent presentation and communication skills', 'Experience working with Business Intelligence software such as Looker, Business Objects and PowerBI', ' 4 Year College Degree in Computer Science, Mathematics, Statistics or a similar quantitative field or comparable experience in lieu of degree A minimum of 2 years of related experience, to include a minimum of 1 year of work as a Data Engineer, BI Engineer, or Data Warehouse Engineer Proficient to expert level skills in SQL Proficient to expert level skills in Python Experience with relational databases like Oracle, Microsoft SQL Server, MySQL Experience in developing and maintaining ETL processes Experience with AWS services like S3, EMR, EC2 or Azure equivalents Solid analytical and organizational skills, with ability to analyze processes Excellent presentation and communication skills Familiarity with Business Intelligence concepts Solid knowledge of distributed computing ', 'Responsibilities Include', 'Use acquired information to help build out a big data schema and infrastructure', ' Experience in orchestrating data pipelines using Apache Airflow, Snowflake and Spark Knowledge of Containerization and experience in Kubernetes. Experience building APIs and Webhooks Experience with columnar databases like Snowflake, Redshift Experience working with Business Intelligence software such as Looker, Business Objects and PowerBI Experience with Informatica ETL software ', 'Experience with columnar databases like Snowflake, Redshift', 'Falls Church, VA', 'Contribute to the design and architecture of the project', 'Experience building APIs and Webhooks', 'Experience with relational databases like Oracle, Microsoft SQL Server, MySQL', 'A minimum of 2 years of related experience, to include a minimum of 1 year of work as a Data Engineer, BI Engineer, or Data Warehouse Engineer', 'Proficient to expert level skills in Python', 'Desired Qualifications', 'Respond to user requests and document work done on service tickets in ticketing system', 'office', 'Document your work, system architecture and processes in JIRA, Confluence and other documentation tools.', 'Generate tracking and monitoring tools to validate data', '4 Year College Degree in Computer Science, Mathematics, Statistics or a similar quantitative field or comparable experience in lieu of degree', 'GTL, an innovation leader in correctional technology, education solutions that assist in rehabilitating inmates, and payment services solutions for government.\u202f GTL leads the fields of correctional technology, education, and government payment services with visionary solutions and customized products that integrate seamlessly to deliver security, financial value, and operational efficiencies while aiding inmate rehabilitation and reducing recidivism rates.\u202f ', 'Partner with DBA and other developers to harness application data and design database structures to obtain best results from the data', 'Have a quality mindset and work hard to prevent bugs through unit testing, test-driven development, version control, continuous integration and deployment', 'Solid knowledge of distributed computing', 'Proficient to expert level skills in SQL', 'Solid analytical and organizational skills, with ability to analyze processes', 'Required Qualifications', 'Pittsburg, PA office', 'Operate within Agile Development environment and apply the methodologies', 'Review and respond to system alerts in internal and external alerting and metrics systems', 'Conduct design and code reviews ', 'Familiarity with Business Intelligence concepts', 'GTL is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee on the basis of race, color, religion, creed, national origin or ancestry, sex, pregnancy or pregnancy-related condition, age, physical or mental disability, veteran or military status, genetic information, sexual orientation, marital status, or any other legally recognized protected basis under federal, state or local laws, regulations or ordinances. The information collected by this application is solely to determine suitability for employment, verify identity and maintain employment statistics on applicants.', 'Partner with internal business stakeholders to engineer batch and real-time streaming data pipelines', 'Support proper data governance processes and policies; implement and/or validate data lineage, quality checks, classification, etc.', 'Resolve defects/bugs during QA testing, pre-production, production, and post-release patches']",Mid-Senior level,Full-time,Information Technology,Telecommunications,2021-03-18 14:34:51
"Data Engineer - Atlanta, GA",Tekberry,"Atlanta, GA",2 weeks ago,63 applicants,"['DATA ENGINEER', 'Hours:', '', 'Experience with some storage systems such as Apache Parquet, PostgreSQL, MongoDB, DynamoDB, file systems, etc. ', ""The successful candidate understands Data Analytics alternatives well enough to find the pipeline design that best fits the partners'/customers' overall dataflow. "", 'Job #:', 'Type:', 'Summary', 'Data Engineer in Atlanta, GA', 'Work with partners/customers to identify the requests and develop high-quality and reusable software components/libraries for them. ', 'Location:', 'The deployment of this data pipeline should cover on-premises, cloud (AWS or Azure), or hybrid cases. So, you should also be familiar with REST API design. ', 'Deliver your code in various programming languages (e.g., C# on .NET core, Python, C++, etc.) to meet different requirements. ', 'Experience with pre-packaged tools like Tableau or toolkits for building algorithms to fit the domain would be desired for this position.', ""Experience with some storage systems such as Apache Parquet, PostgreSQL, MongoDB, DynamoDB, file systems, etc. Work with partners/customers to identify the requests and develop high-quality and reusable software components/libraries for them. The deployment of this data pipeline should cover on-premises, cloud (AWS or Azure), or hybrid cases. So, you should also be familiar with REST API design. Deliver your code in various programming languages (e.g., C# on .NET core, Python, C++, etc.) to meet different requirements. Parse/clean/normalize multiple formats of data (e.g., spec/test vector files, ASCII, CSV, etc.) in the pipeline. The successful candidate understands Data Analytics alternatives well enough to find the pipeline design that best fits the partners'/customers' overall dataflow. Experience with pre-packaged tools like Tableau or toolkits for building algorithms to fit the domain would be desired for this position."", 'Rate:', 'Requirements', 'Parse/clean/normalize multiple formats of data (e.g., spec/test vector files, ASCII, CSV, etc.) in the pipeline. ']",Entry level,Contract,Information Technology,Electrical/Electronic Manufacturing,2021-03-18 14:34:51
Data Engineer (contract),Capgemini,"Philadelphia, PA",1 month ago,Be among the first 25 applicants,"['', 'Data Engineering background 8-10 yearsPython DevelopmentData Bricks - Disparate Data sourceBuild the Delta lakesUpload the data AWS Graph DB NeptuneGraphQL knowledge', 'Build the Delta lakes', 'Requirement', 'Data Engineering background 8-10 years', 'Upload the data AWS Graph DB Neptune', 'GraphQL knowledge', 'Data Bricks - Disparate Data source', 'Python Development']",Associate,Contract,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,HighPoint Global,"Washington, DC",1 week ago,Be among the first 25 applicants,"['', 'Collect requirements, plan implementations and integrate data sources in collaboration with stakeholders.', 'General knowledge of machine learning techniques and methodologies', 'Bachelor’s degree in Computer Science, Statistics, related field or equivalent experience', 'Job Responsibilities', 'Should be well organized, thorough, and able to handle competing priorities', 'Ability to start, manage and conclude projects independently', 'Experience in financial industry a plus', 'Experience with cloud service providers (AWS, Azure)', 'Significant experience with the following languages and tools: Python, R, scikit-learn', 'Experience with scripting tools such as Python, unix shell scripts', 'Positive attitude, attention to detail and great problem-solving skills', 'Facilitate major machine learning projects from conception to conclusion.', 'Description', 'Significant experience with JupyterHub, Python and R languagesSignificant experience with the following languages and tools: Python, R, scikit-learnAnaconda, SQL Server, RStudio, Spyder experience preferredExperience with scripting tools such as Python, unix shell scriptsExperience with big data platforms (Hadoop, Spark, etc)Experience with cloud service providers (AWS, Azure)General knowledge of machine learning techniques and methodologiesShould be well organized, thorough, and able to handle competing prioritiesAbility to start, manage and conclude projects independentlyAbility to adjust to changes in requirements, scope and directionPositive attitude, attention to detail and great problem-solving skillsExperience in financial industry a plusAbility to obtain position of Public Trust designation', 'Work onsite 100%', 'Implement, architect, and administer a JupyterHub server environment.', 'Significant experience with JupyterHub, Python and R languages', 'Ability to obtain position of Public Trust designation', 'Create understanding and build consensus with stakeholders to define project security, data architecture and operational requirements.', 'Data Engineer', 'Implement, architect, and administer a JupyterHub server environment.Facilitate major machine learning projects from conception to conclusion.Collect requirements, plan implementations and integrate data sources in collaboration with stakeholders.Create understanding and build consensus with stakeholders to define project security, data architecture and operational requirements.Demonstrate strong oral and written communication skills, with the ability to communicate on technical and methodological matters with audiences of various expertise.Work onsite 100%', 'Experience with big data platforms (Hadoop, Spark, etc)', 'Ability to adjust to changes in requirements, scope and direction', 'Education And Years Of Experience Requirements', 'About Highpoint', 'Anaconda, SQL Server, RStudio, Spyder experience preferred', 'Knowledge And Skills Requirements', 'Demonstrate strong oral and written communication skills, with the ability to communicate on technical and methodological matters with audiences of various expertise.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Senior Data Engineer,Sinclair Broadcast Group,"Hunt Valley, MD",4 days ago,Be among the first 25 applicants,"['', 'Fundamental understanding of Data Lakes, Data Catalogs, Hardware and Network Topology', 'Azure Data Lake (Analytics and Storage), Data Warehouse / Synapse Analytics / Amazon Redshift, Data Factory, Logic Apps, Data Bricks', 'About Us', ' Algorithmic concepts from at least one information-centric discipline (e.g. statistics, machine learning, information processing, natural language processing, etc.) Gathering data requirements from stakeholders Evaluating data collection for accuracy Data Governance concepts Mindful of practices, procedures and legal guidelines that govern PII and other sensitive data Ability to tackle complex problems with creative solutions when the path may not be clear. ', 'Mindful of practices, procedures and legal guidelines that govern PII and other sensitive data', 'Ability to tackle complex problems with creative solutions when the path may not be clear.', 'Experience creating ETL/ELT processes to move data between internal and external sources using APIs, ETL. ', 'Experience using data reporting and visualization tools (e.g. Cognos, SSRS, Qlik, Data Studio, Power BI, Tableau, etc.)', 'Familiar with the modern cloud data platforms (Azure preferred but AWS is ok). In particular, experience in some of the following is preferred: Azure Data Lake (Analytics and Storage), Data Warehouse / Synapse Analytics / Amazon Redshift, Data Factory, Logic Apps, Data Bricks Other preferred technologies include: Advanced T-SQL (SQL Server), REST API concepts, Powershell, SSIS, PowerBI  ', 'We Are Looking For The Following Skills And Experience', 'Algorithmic concepts from at least one information-centric discipline (e.g. statistics, machine learning, information processing, natural language processing, etc.)', 'Evaluating data collection for accuracy', 'About The Team', 'Experience in database design, architecture and warehousing', 'Gathering data requirements from stakeholders', ' Familiar with the modern cloud data platforms (Azure preferred but AWS is ok). In particular, experience in some of the following is preferred: Azure Data Lake (Analytics and Storage), Data Warehouse / Synapse Analytics / Amazon Redshift, Data Factory, Logic Apps, Data Bricks Other preferred technologies include: Advanced T-SQL (SQL Server), REST API concepts, Powershell, SSIS, PowerBI   Experience in database design, architecture and warehousing Experience creating ETL/ELT processes to move data between internal and external sources using APIs, ETL.  Fundamental understanding of Data Lakes, Data Catalogs, Hardware and Network Topology Experience with Object-Oriented Programming Languages such as Python, JavaScript, Java, C#, etc.  Experience using data reporting and visualization tools (e.g. Cognos, SSRS, Qlik, Data Studio, Power BI, Tableau, etc.) ', 'About', 'Data Governance concepts', 'Data Engineer', 'Experience with Object-Oriented Programming Languages such as Python, JavaScript, Java, C#, etc. ', 'Job Description', 'Other preferred technologies include: Advanced T-SQL (SQL Server), REST API concepts, Powershell, SSIS, PowerBI', ' Azure Data Lake (Analytics and Storage), Data Warehouse / Synapse Analytics / Amazon Redshift, Data Factory, Logic Apps, Data Bricks Other preferred technologies include: Advanced T-SQL (SQL Server), REST API concepts, Powershell, SSIS, PowerBI ']",Associate,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Data Center Engineer,"CPS, Inc.","Aurora, IL",6 days ago,28 applicants,"['', 'Replacing hard drives/memory/transceivers and other hardware related tasks as required by our server and network teams', 'our responsibilities will include overall management of our US data centers, this will include much of the following:', 'Managing power and space including monitoring and capacity planning', 'Assisting with planning moves or new build-outs', ' Installing and cabling switches, servers, and appliances in our on site data center Installation of servers and network equipment with smart hands in remote co-locations Managing power and space including monitoring and capacity planning Maintaining a consistent and clean cable management - keep it tidy Replacing hard drives/memory/transceivers and other hardware related tasks as required by our server and network teams Assisting with planning moves or new build-outs', 'Maintaining a consistent and clean cable management - keep it tidy', 'Installation of servers and network equipment with smart hands in remote co-locations', 'Installing and cabling switches, servers, and appliances in our on site data center']",Entry level,Full-time,Information Technology,Food & Beverages,2021-03-18 14:34:51
Data Infrastructure Engineer,Bolt,"San Francisco, CA",3 weeks ago,27 applicants,"['', 'Jenkins & CircleCI for build pipelines', 'Flexible Paid Time Off', 'Participate in creating and executing a roadmap for all things data infrastructure.', 'Gym and wellness subsidy', 'Our Stack', 'Awesome teammates!', 'Commuter Benefits + Safe rides programs', 'Paid parental leave', 'Comfortable thinking about infrastructure as code ', 'Strong knowledge of SQL and at least one programming language', 'Partner with our ML team to make it easier to do feature engineering and build a reliable data stack to power Bolt’s ML products', 'Postgres RDS is our application database', 'Own data reliability and help assess and determine which warehousing technologies to use, when to move data between data stores, and help productionize models.', 'Create fault-tolerant, timely, and optimized pipelines for data ingestion powering the company’s business analytics. Evangelize best practices with the engineering, infra, analyst teams for building data models, pipelines, and materialized views. Standardize access to data across teams, and build tooling to reuse queries.', 'Step functions and Lambdas to coordinate workflows', 'Experience with Terraform or similar tools', 'AWS Redshift as the data warehouse + Spark on EMR for doing ETL', '4+ years of managing data infrastructure / distributed systemsStrong knowledge of SQL and at least one programming languageMust be willing to both architect solutions & get deep into the weeds of delivering solutionsExperience with big data technologies such as Redshift, EMR + Spark, S3, Glue, Kinesis Firehose, Lambda, etc. Comfortable thinking about infrastructure as code Experience with Terraform or similar toolsNice to haves: NoSQL tech like DynamoDb, Kafka, Kinesis Data Streams. Familiarity with BI tools Eg: Quicksight, Metabase, etc.', 'Nice to haves: NoSQL tech like DynamoDb, Kafka, Kinesis Data Streams. Familiarity with BI tools Eg: Quicksight, Metabase, etc.', 'AWS Lambda for data ingestion and Kinesis Firehose to transport them into S3Step functions and Lambdas to coordinate workflowsAWS Redshift as the data warehouse + Spark on EMR for doing ETLTerraform for maintaining infraJenkins & CircleCI for build pipelinesPostgres RDS is our application databaseOur code base is primarily in Golang & Typescript. However, our data bits are mostly in Python.', 'Experience with big data technologies such as Redshift, EMR + Spark, S3, Glue, Kinesis Firehose, Lambda, etc. ', 'Perks', 'Must be willing to both architect solutions & get deep into the weeds of delivering solutions', 'Build scalable systems that effectively store and crunch tons of data.', 'Ideal Profile', 'Monthly team events', 'In Addition To General Problem-solving You Will', 'Cell phone reimbursement', 'Terraform for maintaining infra', 'Comprehensive health coverage: Medical, Dental and Vision', 'Our code base is primarily in Golang & Typescript. However, our data bits are mostly in Python.', 'What You Will Do', 'We are at the intersection of e-commerce and payments. We collect a lot of data that is key to decision-making at Bolt. ', 'AWS Lambda for data ingestion and Kinesis Firehose to transport them into S3', 'Retirement plans', 'Competitive compensation', 'Work with cross-functional partners (product, infra, security, analysts) to power data-driven products. ', '4+ years of managing data infrastructure / distributed systems', 'Competitive compensationFlexible Paid Time OffComprehensive health coverage: Medical, Dental and VisionRetirement plansCommuter Benefits + Safe rides programsGym and wellness subsidyCell phone reimbursementMonthly team eventsPaid parental leaveAwesome teammates!']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Data Integration Engineer,Covestic,"Seattle, WA",2 weeks ago,71 applicants,"['Advanced understanding for Data Warehousing concepts and data modeling.', 'Experience with GDPR compliant ecosystems.', 'Develop tools to monitor, debug, and analyze data pipelines.', 'Experience with prioritization of projects that mitigate trade-offs with ROI and business value.', '\xa0Education: ', '4+ years of development and/or DBA experience in Relational Database Management Systems (RDBMS). 3+ years’ experience with SQL, cloud platforms, data warehouses, and integration tools with Azure and Microsoft integration experience.\xa0ETL processing experience using logic written in Python and standard toolsets. Advanced understanding for Data Warehousing concepts and data modeling.Experience using Azure Data Services (SQL DB, Blob Storage, Data Lakes, Synapse Analytics, Data Factory, Analysis Services)Experience with orchestration toolsets, such as Airflow or Control-M, Azure Data Factory, or other industry standard tools. Hands-on experience in the implementation and performance tuning, of relational or MPP databases (Teradata, Redshift, Snowflake, or relevant MPP system).\xa0Experience designing database environments, analyzing production deployments, and making recommendations to optimize performance.Proficiency with Agile/Scrum software development framework and methodology, and shipping features every two weeks.', '3+ years’ experience with SQL, cloud platforms, data warehouses, and integration tools with Azure and Microsoft integration experience.\xa0', 'Required Skills: ', 'Responsible for designing, building & managing the EDW and advanced analytics platform, with a focus on net-new builds, and self-healing, resilient, data integration pipelines.Build and operate stable, reliable data pipelines.Evangelist for data management practices.\xa0\xa0Strong point-of-view on dimensional modeling, and associated tradeoffs, as you will be facilitating workshops, and designing and implementing data schemas and models, to deliver enterprise perspectives. Collaborate with senior management, product management, and other engineers, in the development of\xa0optimal\xa0data products.Develop tools to monitor, debug, and analyze data pipelines.', 'Experience with eDiscovery.', ""Bachelor's Degree in Computer Science, Data Engineering, Data Analytics, Information Systems, Mathematics or equivalent professional or military experience."", 'Hands-on experience in the implementation and performance tuning, of relational or MPP databases (Teradata, Redshift, Snowflake, or relevant MPP system).\xa0', 'Responsible for designing, building & managing the EDW and advanced analytics platform, with a focus on net-new builds, and self-healing, resilient, data integration pipelines.', '4+ years of development and/or DBA experience in Relational Database Management Systems (RDBMS). ', 'Azure or AWS big data or architecture certification, or relevant open source contributions. ', 'Covestic is looking for a Sr Data Integration Engineer to support the evolution of an enterprise data warehouse program.\xa0This role will be instrumental in designing, building and delivering\xa0on\xa0our client’s next generation enterprise data platform and will contribute to the delivery of a highly scalable, modern enterprise data warehouse.\xa0It will leverage a “best of breed” technology stack through an EDW Azure ecosystem using Talend and Azure SQL custom toolsets.', 'Evangelist for data management practices.\xa0\xa0', 'Proficiency with Agile/Scrum software development framework and methodology, and shipping features every two weeks.', 'Experience in working with Data Scientists, to operationalize machine learning models.', 'Knowledge of various data science techniques, and experience implementing models developed with these techniques into a production environment.', 'Experience with containers, and container orchestration, such as Kubernetes.', 'Desired Skills:\xa0', 'ETL processing experience using logic written in Python and standard toolsets. ', 'Responsibilities: ', 'Experience designing database environments, analyzing production deployments, and making recommendations to optimize performance.', 'Strong point-of-view on dimensional modeling, and associated tradeoffs, as you will be facilitating workshops, and designing and implementing data schemas and models, to deliver enterprise perspectives. ', 'Experience using Azure Data Services (SQL DB, Blob Storage, Data Lakes, Synapse Analytics, Data Factory, Analysis Services)', 'Cloud CI/CD experience with Terraform, cloud formation, Maven/Jenkins/Arti-factory, or similar technologies.', 'Cloud CI/CD experience with Terraform, cloud formation, Maven/Jenkins/Arti-factory, or similar technologies.Experience working with Talend ETL platform establishing data pipes and ensuring high data quality.Experience with GDPR compliant ecosystems.Experience with eDiscovery.Knowledge of various data science techniques, and experience implementing models developed with these techniques into a production environment.Azure or AWS big data or architecture certification, or relevant open source contributions. Experience in working with Data Scientists, to operationalize machine learning models.Experience with containers, and container orchestration, such as Kubernetes.Experience with prioritization of projects that mitigate trade-offs with ROI and business value.', 'Collaborate with senior management, product management, and other engineers, in the development of\xa0optimal\xa0data products.', 'Build and operate stable, reliable data pipelines.', 'Experience working with Talend ETL platform establishing data pipes and ensuring high data quality.', 'Experience with orchestration toolsets, such as Airflow or Control-M, Azure Data Factory, or other industry standard tools. ']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-03-18 14:34:51
"Senior Data Engineer, Data Platform",Chainalysis Inc.,"New York, NY",2 weeks ago,32 applicants,"['', 'Advanced skills in SQL, ETL, DB tuning and optimization (e.g. Redshift, PostgreSQL, Athena/Glue)', '(BTC, ETH, and others)', 'We have increased usage of our Data Warehouse while controlling overall cost', 'A background like this helps: ', 'Ability to work across the organization to identify high-friction areas or data gaps', 'but not required)', 'Sentiment from the business teams is that data is much easier to access, reliable and of high quality and we can prove that with metrics', 'An ownership mindset - acts and makes decisions on behalf of the company, not just the team. ', ""In one year you'll know you were successful if..."", '(e.g. Tableau, AWS Quicksight)', 'Experience in inventing new metrics, conducting root cause analysis, testing hypotheses and developing reporting/visualization solutions (e.g. Tableau, AWS Quicksight)', 'Excellent organizational skills including prioritization of multiple concurrent projects while still delivering timely and accurate results.', 'We have successfully migrated our data science environment from our data center into AWS', 'You have built tools and mechanisms for consolidating and updating data regularly to make it easily accessible to all stakeholders in the company', "" We have increased usage of our Data Warehouse while controlling overall cost Sentiment from the business teams is that data is much easier to access, reliable and of high quality and we can prove that with metrics We have successfully migrated our data science environment from our data center into AWS You have built tools and mechanisms for consolidating and updating data regularly to make it easily accessible to all stakeholders in the company You've treated our internal data as a product and helped to scale the company's analytics platforms, trained staff to leverage BI tools, and provided your colleagues with easier access to data assets "", 'Infrastructure as code (Terraform) and CI/CD (Docker, EKS, Helm) ', 'Educational background in Computer Science, Engineering, Math, Statistics, Economics or a quantitative discipline preferred (but not required)', 'Experience with technologies such as Airflow, Kafka, Hadoop or Spark', 'The following would differentiate you:', 'Knowledge and experience with data pipeline technologies, data modeling (star schemas, snowflake, aggregation tables), basic data architectures (compute and reporting clusters)', ' Understanding of blockchain data structures and interfaces (BTC, ETH, and others)  Experience with technologies such as Airflow, Kafka, Hadoop or Spark ', '(Docker, EKS, Helm) ', '(Terraform)', '(e.g. Redshift, PostgreSQL, Athena/Glue)', 'Self-sufficient, and a strong bias for action', "" Educational background in Computer Science, Engineering, Math, Statistics, Economics or a quantitative discipline preferred (but not required) You've been successful building Data Warehouse/BI and Data Science enablement solutions on AWS or similar platforms.  Advanced skills in SQL, ETL, DB tuning and optimization (e.g. Redshift, PostgreSQL, Athena/Glue) Infrastructure as code (Terraform) and CI/CD (Docker, EKS, Helm)  Knowledge and experience with data pipeline technologies, data modeling (star schemas, snowflake, aggregation tables), basic data architectures (compute and reporting clusters) Experience in inventing new metrics, conducting root cause analysis, testing hypotheses and developing reporting/visualization solutions (e.g. Tableau, AWS Quicksight) Ability to work across the organization to identify high-friction areas or data gaps Excellent organizational skills including prioritization of multiple concurrent projects while still delivering timely and accurate results. Self-sufficient, and a strong bias for action An ownership mindset - acts and makes decisions on behalf of the company, not just the team.  "", 'You belong here. ', ""You've been successful building Data Warehouse/BI and Data Science enablement solutions on AWS or similar platforms. "", ""You've treated our internal data as a product and helped to scale the company's analytics platforms, trained staff to leverage BI tools, and provided your colleagues with easier access to data assets"", 'Understanding of blockchain data structures and interfaces (BTC, ETH, and others) ']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Cloud Data Engineer,"Essex Management, LLC","Rockville, MD",1 week ago,Be among the first 25 applicants,"['', 'Bachelor’s Degree in a Technical or Scientific Discipline (or professional certifications(s)) and a minimum 5 years technical experience (required); Master’s degree or greater (preferred)', 'Strong SQL and RDBMS skills (required)', 'Write Python that interfaces with SQL and NoSQL databases (e.g. Amazon Aurora, PostgreSQL, MySQL, DynamoDB, MongoDB)', 'Familiarity with the agile software development process (e.g. SCRUM) (preferred)', 'Write Python that interfaces with AWS (e.g. Step Functions, Lambdas, S3, DMS, Glue)', 'Position Summary', 'Experience with object-oriented programming (required). Python (preferred)', 'Understanding of NoSQL Database concepts (preferred)', 'Education and Experience Guidelines:', 'Essex Management, LLC is an Affirmative Action and Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or protected veteran status and will not be discriminated against on the basis of disability. Equal Employment Opportunity Posters If you’d like to view a copy of the company’s affirmative action plan or policy statement, please email ', 'Produce high-quality engineering design documents', 'Design ERDs, including Relational and Star schemas', ' Develop ETL processes  Write Python that interfaces with AWS (e.g. Step Functions, Lambdas, S3, DMS, Glue) Write Python that interfaces with SQL and NoSQL databases (e.g. Amazon Aurora, PostgreSQL, MySQL, DynamoDB, MongoDB) Develop and perform complex SQL-based queries Design ERDs, including Relational and Star schemas Produce high-quality engineering design documents Collaborate and communicate (written and verbal) with other team members towards the development of high-quality technical deliverables ', ' . This telephone line and email address is reserved solely for job seekers with disabilities requesting accessibility assistance or an accommodation in the job application process. Please do not call about the status of your job application if you do not require accessibility assistance or an accommodation. Messages left for other purposes, such as following up on an application or non-disability related technical issues, will not receive a response.', 'Develop ETL processes ', 'Collaborate and communicate (written and verbal) with other team members towards the development of high-quality technical deliverables', 'Develop and perform complex SQL-based queries', 'Requirements Essential Functions', 'Experience with AWS (Step Functions, Lambdas, S3, DMS, Glue (preferred)', 'Experience with data warehouse concepts and implementing ETL processes (required)', ' . If you are an individual with a disability and would like to request a reasonable accommodation as part of the employment selection process, please contact Essex HR Department at 301-760-7527 or ', 'hr@essexmanagement.com', 'PLEASE NOTE THAT THIS POSITION REQUIRES ELIGIBILITY FOR PUBLIC TRUST CLEARANCE.', ' Bachelor’s Degree in a Technical or Scientific Discipline (or professional certifications(s)) and a minimum 5 years technical experience (required); Master’s degree or greater (preferred) Experience with data warehouse concepts and implementing ETL processes (required) Strong SQL and RDBMS skills (required) Experience with object-oriented programming (required). Python (preferred) Understanding of NoSQL Database concepts (preferred) Familiarity with the agile software development process (e.g. SCRUM) (preferred) Experience with AWS (Step Functions, Lambdas, S3, DMS, Glue (preferred) ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,AppLovin,"Portland, OR",4 weeks ago,Be among the first 25 applicants,"['', 'About The Role', 'Bonus Points For', 'About You']",Mid-Senior level,Full-time,Information Technology,Computer Games,2021-03-18 14:34:51
Junior NLP Data engineer-Chatbot/AI/WIT.AI- No visas please,Comrise,"New York, NY",5 hours ago,129 applicants,"['', '2+ years of experience as an NLP engineer/data scientist', 'Experience with CoreNLP and Wit.ai', '732-203-6038', 'Proficiency developing and productionizing NLP algorithms using Python.', 'Renu Goel', 'Track record of extracting text and unstructured data.', 'renu.goel@comrise.com']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Certilytics,"Louisville, KY",3 weeks ago,Be among the first 25 applicants,"['', ' Provable SQL knowledge and experience working with a variety of databases Successful history of manipulating, transforming, and extracting value from large data sets Strong project management and organizational skills Knowledge of programming languages (Python and Java) Ability to work with various functional teams in a dynamic environment Excellent written and verbal communication Strong collaborative drive Experience designing, building, and maintaining big data pipelines Ability to function autonomously and with partners and teammates across several time zones Technical mindset and desire to learn new tools and approaches Experience working with healthcare data desired Experience with big data tools: Hadoop, Hive, Spark, Yarn desired ', 'Required Skills', 'Evaluate logs across a wide range of complex systems', 'Ability to function autonomously and with partners and teammates across several time zones', 'Experience designing, building, and maintaining big data pipelines', 'Create and maintain data pipelines that support internal and external clients', 'Technical mindset and desire to learn new tools and approaches', 'Strong collaborative drive', 'Strong project management and organizational skills', 'Provable SQL knowledge and experience working with a variety of databases', 'Ability to work with various functional teams in a dynamic environment', 'Work with data experts to strive for greater functionality in our data platform', 'Sedentary work that primarily involves sitting/standing in-front of a computer', 'Responsibilities/Accountabilities', ' Ability to work remotely without interruptions Sedentary work that primarily involves sitting/standing in-front of a computer ', 'Experience with big data tools: Hadoop, Hive, Spark, Yarn desired', 'Excellent written and verbal communication', 'Other duties as assigned', ' Create and maintain data pipelines that support internal and external clients Optimize and troubleshoot complex joins across massive data sets (billions of records) Identify, design, implement, and own internal process improvements Automate manual processes, and optimize data delivery and ingestion Respond to operational data pipeline failures with a sense of ownership and an eye toward continuous improvement Assemble accurate large, complex data sets that meet business requirements Evaluate logs across a wide range of complex systems Work with data experts to strive for greater functionality in our data platform Generate accurate and effective documentation Other duties as assigned ', 'Successful history of manipulating, transforming, and extracting value from large data sets', 'Automate manual processes, and optimize data delivery and ingestion', 'Experience working with healthcare data desired', 'Generate accurate and effective documentation', 'Ability to work remotely without interruptions', 'Optimize and troubleshoot complex joins across massive data sets (billions of records)', 'Identify, design, implement, and own internal process improvements', 'Respond to operational data pipeline failures with a sense of ownership and an eye toward continuous improvement', 'Knowledge of programming languages (Python and Java)', 'Assemble accurate large, complex data sets that meet business requirements']",Not Applicable,Full-time,Engineering,Mechanical or Industrial Engineering,2021-03-18 14:34:51
Data Engineer/Data Engineering Manager,Lawrence Harvey,"Chicago, IL",3 weeks ago,95 applicants,"['', 'My client is a world-leading Fortune Global 500 consultancy with leading capabilities in Digital, Cloud and Security. They employ over 500,000 people and serve clients in more than 120 countries. As of 2020, they work with 91 of the Fortune Global 100 and have appearances on Fortune’s “World’s Most Admired Companies” numerous times.', 'Translate objectives into a scalable solution that meets end customers’ needs within deadlines.', 'Provide architecture support to the data scientists.', 'Develop use cases that drive business value for clients', 'Experience using Python, Spark, pySpark, Java, or Scala; either on AWS or Azure', 'Experience designing and building Big Data ETL pipelines using Talend or Informatica technologies', 'Data Engineering Consultant', ""Bachelor's or Master’s in Computer Science, Engineering, Technical Science2+ years designing, implementing large scale data pipelines for data curation, feature engineering and machine learningExperience using Python, Spark, pySpark, Java, or Scala; either on AWS or AzureMinimum 1 year of designing and building streaming data ingestion, analysis and processing pipelines using Kafka, Kafka Streams, Spark Streaming and similar cloud native technologies."", 'Design and build real-time analytics solutions and work alongside data architects', 'Prototype and Test end to end data supply chain', 'Design and build real-time analytics solutions and work alongside data architectsDevelop Cloud Native architecturePrototype and Test end to end data supply chainDevelop use cases that drive business value for clientsProvide architecture support to the data scientists.Translate objectives into a scalable solution that meets end customers’ needs within deadlines.', 'Responsibilities:', 'Qualifications', 'Also there Is a Data Manager position available as well!', ""Bachelor's or Master’s in Computer Science, Engineering, Technical Science"", 'Minimum 1 year of designing and building streaming data ingestion, analysis and processing pipelines using Kafka, Kafka Streams, Spark Streaming and similar cloud native technologies.', 'Experience building Business Data Catalogs or Data Marketplaces for powering business analytics using technologies such as Alation, Collibra, Informatica or custom solutions.', 'Bonus points if:', 'They are looking for a Spark or Cloud Data engineering professionals who are also client-facing. ', 'Bonus points if:\xa0', 'Develop Cloud Native architecture', '2+ years designing, implementing large scale data pipelines for data curation, feature engineering and machine learning', 'AWS Solution Architect or Azure Architect or GCP architect certification.Experience designing and building Big Data ETL pipelines using Talend or Informatica technologiesExperience building Business Data Catalogs or Data Marketplaces for powering business analytics using technologies such as Alation, Collibra, Informatica or custom solutions.', '\xa0', 'AWS Solution Architect or Azure Architect or GCP architect certification.', 'They are looking for a Data Engineering Consultant to join their Applied Intelligence team. The team is one of the world’s largest team of data scientists, data engineers, and experts in machine learning and AI.\xa0']",Associate,Full-time,Information Technology,Management Consulting,2021-03-18 14:34:51
Reporting & Data Engineer,"ClubReady, LLC","Missouri City, TX",3 weeks ago,Be among the first 25 applicants,"['Job Overview', '', ' MS SQL Server SQL Server Reporting Services (SSRS) T-SQL and Stored Procedures Data Transformation Services (DTS) SQL Server Integration Services (SSIS) ', 'MS SQL Server', ' Reports using SQL Server Reporting Services (SSRS) Advanced TSQL and Stored Procedures SQL Server Integration Services (SSIS) packages ', "" A Bachelor's degree in Computer Science, Engineering (any discipline), or closely related field of study is preferred 3+ years’ experience with Microsoft technologies: MS SQL Server SQL Server Reporting Services (SSRS) T-SQL and Stored Procedures Data Transformation Services (DTS) SQL Server Integration Services (SSIS)   3+ years overall software and/or application development experience performing data analyst tasks 3+ years’ experience working with and managing large volumes of data within a SQL Server environment Experience working within the Scrum framework Experience in SQL Performance Tuning using query plan analysis"", ""A Bachelor's degree in Computer Science, Engineering (any discipline), or closely related field of study is preferred"", 'Responsible for providing accurate and timely communication of the status of assignment, job responsibilities, and the resolution of problems to the appropriate parties following established policies and procedures.', 'T-SQL and Stored Procedures', 'Experience working within the Scrum framework', 'Design, Build and Maintain Reports using SQL Server Reporting Services (SSRS) Advanced TSQL and Stored Procedures SQL Server Integration Services (SSIS) packages  ', 'Qualifications And Experience Required', 'Exhibit ability to plan and manage multiple deliverables in a highly energized and fast-paced environment.', 'SQL Server Integration Services (SSIS) packages', '3+ years overall software and/or application development experience performing data analyst tasks', '3+ years’ experience working with and managing large volumes of data within a SQL Server environment', 'Essential Duties', 'Data Transformation Services (DTS)', ' Be driven, organized, & detail oriented. Have excellent communication and technical writing skills. Work well in a team environment and brainstorm with other developers/senior developers on ideas, workflows, and technologies Have strong technical, logical, analytical, and problem-solving skills Ability to step into large and complicated system and quickly start making an impact Exhibit ability to plan and manage multiple deliverables in a highly energized and fast-paced environment. Have excellent interpersonal skills with the ability to develop cohesive working relationships with internal and external clients. Have the flexibility to adapt to change and willing to learn and develop new skill sets as applicable. ', 'Analyze the client requirements and design technical solutions based on those requirements.', 'Have excellent communication and technical writing skills.', '3+ years’ experience with Microsoft technologies: MS SQL Server SQL Server Reporting Services (SSRS) T-SQL and Stored Procedures Data Transformation Services (DTS) SQL Server Integration Services (SSIS)  ', 'SQL Server Integration Services (SSIS)', 'Ability to step into large and complicated system and quickly start making an impact', 'Have excellent interpersonal skills with the ability to develop cohesive working relationships with internal and external clients.', 'Experience in SQL Performance Tuning using query plan analysis', 'Work with the product owner in an agile (scrum) development to ensure adequate requirements are available to scope, estimate, and plan work.', 'Provide technical leadership by making recommendations for enhancement to the existing solution or to resolve production issues.', 'Have the flexibility to adapt to change and willing to learn and develop new skill sets as applicable.', 'Have strong technical, logical, analytical, and problem-solving skills', 'Work well in a team environment and brainstorm with other developers/senior developers on ideas, workflows, and technologies', 'Advanced TSQL and Stored Procedures', 'Perform day to day troubleshooting of reporting and ETL issues.', 'Reports using SQL Server Reporting Services (SSRS)', ' Design, Build and Maintain Reports using SQL Server Reporting Services (SSRS) Advanced TSQL and Stored Procedures SQL Server Integration Services (SSIS) packages   Analyze the client requirements and design technical solutions based on those requirements. Work with the product owner in an agile (scrum) development to ensure adequate requirements are available to scope, estimate, and plan work. Support and maintenance of the reporting and Datamart environments. Perform day to day troubleshooting of reporting and ETL issues. Develop testing and implementation strategy, conduct appropriate functional and performance testing to identify and resolve process bottlenecks and data quality issues. Provide technical leadership by making recommendations for enhancement to the existing solution or to resolve production issues. Responsible for providing accurate and timely communication of the status of assignment, job responsibilities, and the resolution of problems to the appropriate parties following established policies and procedures. ', 'Successful Candidates Will', 'Support and maintenance of the reporting and Datamart environments.', 'Develop testing and implementation strategy, conduct appropriate functional and performance testing to identify and resolve process bottlenecks and data quality issues.', 'Be driven, organized, & detail oriented.', 'SQL Server Reporting Services (SSRS)']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Lead Data Engineer,BigCommerce,"Austin, TX",2 weeks ago,Be among the first 25 applicants,"['', 'Estimate work items and help maintain a scrum backlog', 'Focused developer with a strong sense of ownership', 'Experience with monitoring near real time scalable data pipelines', 'BS or MS in Computer Science or equivalent experience or field', 'Experience with Terraform, Puppet, and Jenkins', 'Diversity, Equity & Inclusion at BigCommerce', 'Actively participate with our development team in all phases of the software development lifecycle, including requirements gathering, functional and technical design, development, testing and roll-out, and support', ' Build scalable near real time data pipelines on AWS and GCP that collects, transforms, loads and curates data from various internal and external data sources Build a scalable distributed data store that will be central source of truth Create scalable and low latency solutions Implement machine learning pipelines and algorithms Evaluate new technologies and build prototypes for continuous improvements in Data Engineering Contribute to setting standards and development principles for the team and influence across the larger organization ', '4+ years of experience in building data pipelines in the cloud with tools like S3, Hadoop, Hive; or their equivalents', 'Plays a key role in defining data engineering strategy and communicating priorities to others', 'Build performance and load tests written from scalability and resiliency standpoint', 'Strong experience with SQL, ETL, Data Warehousing and Data Orchestration', 'Capable of working closely across multiple teams to ensure data solutions line up with business initiatives and are of high quality', 'Strong experience with AWS and/or GCP', 'Implement machine learning pipelines and algorithms', '4+ years of experience in Java, Scala and/or Python', 'Build a scalable distributed data store that will be central source of truth', 'Experience with shell scripting', 'Excellent written and verbal communication skills', 'Distill and present research findings to both technical and non-technical leaders ', 'Experience with scheduling frameworks, preferably Airflow', 'Experience with of all aspects of data systems including database design, ETL, aggregation strategy, performance optimization, and technology trade-offs', 'Write automated unit, integration and acceptance tests to support our continuous integration pipelines', 'Partners with engineering management to determine hiring needs and helps with recruiting activities', 'Contribute to setting standards and development principles for the team and influence across the larger organization', ' Write automated unit, integration and acceptance tests to support our continuous integration pipelines Build performance and load tests written from scalability and resiliency standpoint Participate in peer code reviews and advocate for the best coding practices and principles Partners with Infrastructure and Engineering teams to ensure instrumentation, logging and monitoring is in place ', 'Lead collaboration across different teams', 'Build scalable near real time data pipelines on AWS and GCP that collects, transforms, loads and curates data from various internal and external data sources', 'Partners with Infrastructure and Engineering teams to ensure instrumentation, logging and monitoring is in place', 'Coach and mentor others, models the company values', 'Create scalable and low latency solutions', 'Evaluate new technologies and build prototypes for continuous improvements in Data Engineering', '8+ years developing data and software solutions', '4+ years of experience in streaming technology such as Kafka', ' BS or MS in Computer Science or equivalent experience or field 8+ years developing data and software solutions 4+ years of experience in Java, Scala and/or Python 4+ years of experience in building data pipelines in the cloud with tools like S3, Hadoop, Hive; or their equivalents 4+ years of experience in streaming technology such as Kafka Strong experience with AWS and/or GCP Strong experience with SQL, ETL, Data Warehousing and Data Orchestration Experience with scheduling frameworks, preferably Airflow Experience with monitoring near real time scalable data pipelines Experience with of all aspects of data systems including database design, ETL, aggregation strategy, performance optimization, and technology trade-offs Experience with Terraform, Puppet, and Jenkins Experience with shell scripting Focused developer with a strong sense of ownership Ability to independently drive projects consisting of many stories from inception through to completion and production deployment Capable of working closely across multiple teams to ensure data solutions line up with business initiatives and are of high quality Excellent written and verbal communication skills ', ' Actively participate with our development team in all phases of the software development lifecycle, including requirements gathering, functional and technical design, development, testing and roll-out, and support Estimate work items and help maintain a scrum backlog Lead collaboration across different teams Distill and present research findings to both technical and non-technical leaders  Plays a key role in defining data engineering strategy and communicating priorities to others Coach and mentor others, models the company values Partners with engineering management to determine hiring needs and helps with recruiting activities ', 'Participate in peer code reviews and advocate for the best coding practices and principles', 'Ability to independently drive projects consisting of many stories from inception through to completion and production deployment']",Associate,Full-time,Engineering,Computer Software,2021-03-18 14:34:51
Data Engineer,Base-2 Solutions,"Bethesda, MD",5 days ago,Be among the first 25 applicants,"['', 'No', ' 2 years of experience with big data tools: Hadoop, Spark, Kafka, NiFi, Pulsar', 'You Will Work Closely With The Chief Architect, Systems Engineers, Software Engineers, And Data Scientists On The Following Key Tasks', ' Experience transforming data in various formats, including JSON, XML, CSV, and zipped files', ' 2 years of experience with and managing data across relational SQL and NoSQL databases like MySQL, Postgres, Cassandra, HDFS, Redis, and Elasticsearch', ' Experience developing data ingest workflows with stream-processing systems: Spark-Streaming, Kafka Streams and/or Flink', ' Design and implement high-volume data ingestion and streaming pipelines using Open Source frameworks like Apache Spark, Flink, Nifi, and Kafka on AWS Cloud Leverage strategic and analytical skills to understand and solve customer and business centric questions Create prototypes and proofs of concept for iterative development Learn new technologies and apply the knowledge in production systems Monitor and troubleshoot performance issues on the enterprise data pipelines and the data lake Partner with various teams to define and execute data acquisition, transformation, processing and make data actionable for operational and analytics initiatives', ' Experience developing flexible ontologies to fit data from multiple sources and implementing the ontology in the form of database mappings / schemas', ' Good interpersonal and communication skills necessary to work effectively with customers and other team members.', ' Monitor and troubleshoot performance issues on the enterprise data pipelines and the data lake', ' Knowledge about security and best practices.', ' Experience developing flexible data ingest and enrichment pipelines, to easily accommodate new and existing data sources', 'None', ' Experience with Jira, Confluence and extensive experience with Agile methodologies.', ' 2 years of experience working in a Linux environment', ' Learn new technologies and apply the knowledge in production systems', ' Must have an active Top Secret security clearance and able to obtain a TS/SCI with Polygraph.', 'Full Time', ' BS in Computer Science, Systems Engineering, or related technical field or equivalent experience with at least 8+ years in systems engineering or administration (6+ years with a MS/MIS Degree). (4 years for mid-level)', ' 2 years of experience with object-oriented/object function scripting languages: Python (preferred) and/or Java', 'Fun stuff you will do on the job', 'Reston, VA (preferred) or Bethesda, MD.', ' Leverage strategic and analytical skills to understand and solve customer and business centric questions', ' Experience with software configuration management tools such as Git/Gitlab, Salt, Confluence, etc.', 'Maryland', ' Data engineering experience in Intelligence Community or other government agencies Experience with Microservices architecture components, including Docker and Kubernetes. Experience with AWS cloud services: EC2, S3, EMR, RDS, Redshift, Athena and/or Glue Experience with Jira, Confluence and extensive experience with Agile methodologies. Knowledge about security and best practices. Experience developing flexible data ingest and enrichment pipelines, to easily accommodate new and existing data sources Experience with software configuration management tools such as Git/Gitlab, Salt, Confluence, etc. Experience with continuous integration and deployment (CI/CD) pipelines and their enabling tools such as Jenkins, Nexus, etc. Detailed oriented/self-motivated with the ability to learn and deploy new technology quickly', ' Create prototypes and proofs of concept for iterative development', ' Design and implement high-volume data ingestion and streaming pipelines using Open Source frameworks like Apache Spark, Flink, Nifi, and Kafka on AWS Cloud', ' BS in Computer Science, Systems Engineering, or related technical field or equivalent experience with at least 8+ years in systems engineering or administration (6+ years with a MS/MIS Degree). (4 years for mid-level) Must have an active Top Secret security clearance and able to obtain a TS/SCI with Polygraph. 2 years of experience with big data tools: Hadoop, Spark, Kafka, NiFi, Pulsar 2 years of experience with object-oriented/object function scripting languages: Python (preferred) and/or Java 2 years of experience with and managing data across relational SQL and NoSQL databases like MySQL, Postgres, Cassandra, HDFS, Redis, and Elasticsearch 2 years of experience working in a Linux environment Experience working with and designing REST APIs Experience developing data ingest workflows with stream-processing systems: Spark-Streaming, Kafka Streams and/or Flink Experience transforming data in various formats, including JSON, XML, CSV, and zipped files Experience developing flexible ontologies to fit data from multiple sources and implementing the ontology in the form of database mappings / schemas Good interpersonal and communication skills necessary to work effectively with customers and other team members.', ' Partner with various teams to define and execute data acquisition, transformation, processing and make data actionable for operational and analytics initiatives', ' Experience working with and designing REST APIs', ' Experience with continuous integration and deployment (CI/CD) pipelines and their enabling tools such as Jenkins, Nexus, etc.', 'Data Engineer', '(4 years for mid-level)', ' Detailed oriented/self-motivated with the ability to learn and deploy new technology quickly', 'Top Secret', 'This is you', ' Data engineering experience in Intelligence Community or other government agencies', 'You Will Wow Us If You Have These Skills', ' Experience with AWS cloud services: EC2, S3, EMR, RDS, Redshift, Athena and/or Glue', 'Bethesda', ' Experience with Microservices architecture components, including Docker and Kubernetes.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
"Manager, Data Engineer",Ally,"Charlotte, NC",1 week ago,Be among the first 25 applicants,"['', 'Strong verbal and written communication skills', 'Ally and Your Career', 'Solutions analysis and design, balancing technical and business factors.', 'The Work Itself', 'The Skills You Bring', 'Work-Life Integration: other benefits including LifeMatters® Employee Assistance Program, subsidized and discounted Weight Watchers® program and other employee discount programs.', 'Strong knowledge and experience with one or more SDLC methodologies and/or principles such as Agile Scrum or Kanban, or other related Agile methods', 'Strong requirements gathering and design abilities', 'Building a Family: adoption, surrogacy, and fertility support as well as parental and caregiver leave, back-up child and adult/elder day care program and child care discounts.', 'Must have experience building Data Warehouse based on DV-2 Data Vaulting model', 'Experience related to data warehousing design and development', 'Planning for the Future: plan for the near and long term with an industry-leading 401K retirement savings plan with matching and company contributions, student loan and 529 educational assistance programs, tuition reimbursement, and other financial well-being programs.', 'Vendor management for service and software delivery.', 'Work-Life Integration: ', ' Time Away: competitive holiday and flexible paid-time-off, including time off for volunteering and voting. Planning for the Future: plan for the near and long term with an industry-leading 401K retirement savings plan with matching and company contributions, student loan and 529 educational assistance programs, tuition reimbursement, and other financial well-being programs. Supporting your Health & Well-being: flexible health and insurance options including dental and vision, pre-tax Health Savings Account with employer contributions and a total well-being program that helps you and your family stay on track physically, socially, emotionally and financially. Building a Family: adoption, surrogacy, and fertility support as well as parental and caregiver leave, back-up child and adult/elder day care program and child care discounts. Work-Life Integration: other benefits including LifeMatters® Employee Assistance Program, subsidized and discounted Weight Watchers® program and other employee discount programs. ', 'Planning for the Future:', 'Management of team capacity, talent, learning & growth, and financials.', 'Managing engineers and analytics professionals to design and deliver solutions in an Agile context. ', "" Bachelor's degree in Computer Science, Information Systems, or other relevant field of study. 5+ years of data engineering and data analysis experience.Must have experience building Data Warehouse based on DV-2 Data Vaulting model Experience related to data warehousing design and development Strong knowledge and experience with one or more SDLC methodologies and/or principles such as Agile Scrum or Kanban, or other related Agile methods Strong knowledge of DevSecOps pipeline creation, maturity, and operations Ability to lead, mentor and manage individual contributors Strong requirements gathering and design abilities Strong analytic and problem-solving skills Strong verbal and written communication skills Ability to multi-task and manage through complexity "", 'High-level and low-level design of real time and batch system interfaces and data enrichment pipelines in a cloud-native data ecosystem.', 'Time Away', 'The Opportunity', 'Strong analytic and problem-solving skills', ' Managing engineers and analytics professionals to design and deliver solutions in an Agile context.  Management of team progression and maturity and Agile delivery and DevOps/CI-CD automation. Solutions analysis and design, balancing technical and business factors. High-level and low-level design of real time and batch system interfaces and data enrichment pipelines in a cloud-native data ecosystem. Troubleshooting and support of data & analytics solutions end-to-end. Promoting and enforcing design and development standards and best practices. Mid-level business partner engagement; management of relationship and inter-group planning among technology leadership/peers. Vendor management for service and software delivery. Management of team capacity, talent, learning & growth, and financials. ', 'How We’ll Have Your Back', 'Troubleshooting and support of data & analytics solutions end-to-end.', ""Bachelor's degree in Computer Science, Information Systems, or other relevant field of study."", 'Who We Are', '5+ years of data engineering and data analysis experience.', 'Ability to lead, mentor and manage individual contributors', 'Mid-level business partner engagement; management of relationship and inter-group planning among technology leadership/peers.', 'Promoting and enforcing design and development standards and best practices.', 'Supporting your Health & Well-being:', 'Time Away: competitive holiday and flexible paid-time-off, including time off for volunteering and voting.', 'Strong knowledge of DevSecOps pipeline creation, maturity, and operations', 'Management of team progression and maturity and Agile delivery and DevOps/CI-CD automation.', 'Building a Family:', 'Ability to multi-task and manage through complexity', 'Supporting your Health & Well-being: flexible health and insurance options including dental and vision, pre-tax Health Savings Account with employer contributions and a total well-being program that helps you and your family stay on track physically, socially, emotionally and financially.']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Sr. Big Data Engineer - Remote,NAVA Software Solutions,"Raleigh, NC",4 days ago,Be among the first 25 applicants,"['', 'Responsibilities', 'Required Skills']",Associate,Full-time,Engineering,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Apple,"Cupertino, CA",1 day ago,Be among the first 25 applicants,"['', 'Description', 'Education & Experience', 'Summary', 'Key Qualifications']",Not Applicable,Full-time,Information Technology,Consumer Electronics,2021-03-18 14:34:51
Data Engineer,CVS Health,"Hartford, CT",23 hours ago,Be among the first 25 applicants,"['', 'Business Overview', 'Assists in the development of large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs.', 'Builds data marts and data models to support clients and other internal customers.', 'Writes ETL (Extract / Transform / Load) processes, designs database systems and develops tools for real-time and offline analytic processing.', 'Has strong knowledge of large scale search applications and building high volume data pipelines.', 'Collaborates with client team to transform data and integrate algorithms and models into automated processes.', 'Applies understanding of key business drivers to accomplish own work.', 'Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources.', 'Uses expertise, judgment and precedents to contribute to the resolution of moderately complex problems.', 'Experience building data transformation and processing solutions.', 'Assists in the development of large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs.Applies understanding of key business drivers to accomplish own work.Uses expertise, judgment and precedents to contribute to the resolution of moderately complex problems.Leads portions of initiatives of limited scope, with guidance and direction.Writes ETL (Extract / Transform / Load) processes, designs database systems and develops tools for real-time and offline analytic processing.Collaborates with client team to transform data and integrate algorithms and models into automated processes.Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines.Uses programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems.Builds data marts and data models to support clients and other internal customers.', 'Strong collaboration and communication skills within and across teams.', 'Ability to understand complex systems and solve challenging analytical problems.', '3 or more years of progressively complex data related experience.', 'Education', 'Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment.', 'Leads portions of initiatives of limited scope, with guidance and direction.', 'Experience with bash shell scripts, UNIX utilities & UNIX Commands.', 'Strong problem solving skills and critical thinking ability.Strong collaboration and communication skills within and across teams.3 or more years of progressively complex data related experience.Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources.Ability to understand complex systems and solve challenging analytical problems.Experience with bash shell scripts, UNIX utilities & UNIX Commands.Knowledge in Java, Python, Hive, Cassandra, Pig, MySQL or NoSQL or similar.Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment.Experience building data transformation and processing solutions.Has strong knowledge of large scale search applications and building high volume data pipelines.', 'Uses programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems.', 'Strong problem solving skills and critical thinking ability.', 'Required Qualifications', 'Job Description', 'Knowledge in Java, Python, Hive, Cassandra, Pig, MySQL or NoSQL or similar.', 'Preferred Qualifications', 'Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Riskalyze,"Philadelphia, PA",2 weeks ago,80 applicants,"['', 'Work with leaders across the company to develop a strong understanding of their needs for data sets in Salesforce, Domo and other tools for our customer-facing teams. Work with engineering teams in the R&D organization to understand the data schemas and structures.', 'Fully stocked drink fridges.', 'Annual bonus subject to company/individual performance.', 'Build monitoring and alerting tools to notify multiple people on the Revenue Operations team when our core systems or products have issues producing data, our data pipelines have issues extracting and transforming data, or our customer-facing systems have issues receiving data.', 'Coordinate the validation of data quality by requiring both the R&D stakeholders who have knowledge of data structure, and the customer-facing teams who have knowledge of customer activities and business operations, to review the outputs of the data pipeline and ensure it is accurate and complete.', '5+ years of experience working with at least half of the following technologies SQL, cURL, JSON, Javascript, Kotlin, Apex and/or Python. Clear ability to rapidly extend your natural skills to encompass the other half.', 'Benefits', 'Medical, dental and vision with access to HSA or FSA depending on chosen medical plan.', '401(k) Retirement savings with employer matching dollar-for-dollar, up to 4%.', 'Responsibilities', 'Strong communication, collaboration, and presentation skills.', 'On-site financial planning with a registered financial advisor.', 'Requirements', 'Strong experience with ETL tools, databases, data warehousing solutions', 'Available pet insurance.', 'Design and build automated and scalable data pipelines to extract that data from our core databases and products, transform that data into usable datasets for our customer-facing teams, and load that data into Salesforce, Domo and other tools.', 'Elite at constructing data schemas and structuring datasets for ease of use by our customer-facing teams. Ability to distill down complex concepts and communicate them to non-technical teams.', 'Experience working independently on data pipeline and integration projects; owning and leading a function without a large team to fall back on; willingness to be entrepreneurial and scrappy in creating infrastructure from scratch. Strong systems thinker.', 'All hands team meetings every 6 weeks with catering.', ' Work with leaders across the company to develop a strong understanding of their needs for data sets in Salesforce, Domo and other tools for our customer-facing teams. Work with engineering teams in the R&D organization to understand the data schemas and structures. Design and build automated and scalable data pipelines to extract that data from our core databases and products, transform that data into usable datasets for our customer-facing teams, and load that data into Salesforce, Domo and other tools. Coordinate the validation of data quality by requiring both the R&D stakeholders who have knowledge of data structure, and the customer-facing teams who have knowledge of customer activities and business operations, to review the outputs of the data pipeline and ensure it is accurate and complete. Build monitoring and alerting tools to notify multiple people on the Revenue Operations team when our core systems or products have issues producing data, our data pipelines have issues extracting and transforming data, or our customer-facing systems have issues receiving data. Work with stakeholders to assist with data-related technical issues and support data infrastructure needs. Consult with R&D Product Managers to ensure that new products and features are developed to feed data seamlessly into the data pipeline. Mentor other RevOps team members on data pipelines and architecture to create resilience in our resourcing and systems. ', 'Consult with R&D Product Managers to ensure that new products and features are developed to feed data seamlessly into the data pipeline.', '3 weeks Vacation & 1 week of sick time per year + 11 paid holidays.', 'In office snacks 3x per week.', 'Mentor other RevOps team members on data pipelines and architecture to create resilience in our resourcing and systems.', ' Medical, dental and vision with access to HSA or FSA depending on chosen medical plan. Available pet insurance. 401(k) Retirement savings with employer matching dollar-for-dollar, up to 4%. Annual bonus subject to company/individual performance. On-site financial planning with a registered financial advisor. 3 weeks Vacation & 1 week of sick time per year + 11 paid holidays. All hands team meetings every 6 weeks with catering. Fully stocked drink fridges. In office snacks 3x per week. ', 'Work with stakeholders to assist with data-related technical issues and support data infrastructure needs.', ' Experience working independently on data pipeline and integration projects; owning and leading a function without a large team to fall back on; willingness to be entrepreneurial and scrappy in creating infrastructure from scratch. Strong systems thinker. Strong experience with ETL tools, databases, data warehousing solutions 5+ years of experience working with at least half of the following technologies SQL, cURL, JSON, Javascript, Kotlin, Apex and/or Python. Clear ability to rapidly extend your natural skills to encompass the other half. Elite at constructing data schemas and structuring datasets for ease of use by our customer-facing teams. Ability to distill down complex concepts and communicate them to non-technical teams. Strong communication, collaboration, and presentation skills. ']",Mid-Senior level,Full-time,Engineering,Computer Software,2021-03-18 14:34:51
Data Engineer,The Walt Disney Company,"Santa Monica, CA",6 days ago,137 applicants,"['', 'Create runbooks and actionable alerts as part of the development process', 'Partner with technical and non-technical colleagues to understand data and reporting requirements.Work with Engineering teams to collect required data from internal and external systems.Design table structures and define ETL strategy to build performant Data solutions that are reliable and scalable in a fast growing data ecosystem.Develop Data Quality checks for source and target data sets. Develop UAT plans and conduct QA.Develop and maintain ETL routines using ETL and orchestration tools such as Airflow, Luigi and Jenkins.Document and publish Metadata and table designs to facilitate data adoption.Perform ad hoc analysis as necessary.Perform SQL and ETL tuning as necessary.Develop and maintain Dashboards/reports using Tableau and LookerCoordinate and resolve escalated production support incidents in Tier 2 support rotationCreate runbooks and actionable alerts as part of the development process', 'Familiarity with data exploration / data visualization tools like Tableau, Looker, Chartio, etc.', 'Comfortable working in a fast-paced and highly collaborative environment.', 'Degree in an analytical field such as economics, mathematics, or computer science is desired.', '1+ years of experience with workflow management tools (Airflow, Oozie, Azkaban, UC4)', 'Coordinate and resolve escalated production support incidents in Tier 2 support rotation', '2+ years of relevant Professional experience.1+ years work experience implementing and reporting on business key performance indicators in data warehousing environments. Strong understanding of data modeling principles including Dimensional modeling, data normalization principles etc.1 + years experience using analytic SQL, working with traditional relational databases and/or distributed systems such as Hadoop / Hive, BigQuery, Redshift.Experience programming languages (e.g. Python, R, bash) preferred.1+ years of experience with workflow management tools (Airflow, Oozie, Azkaban, UC4)Good understanding of SQL Engines and able to conduct advanced performance tuningExperience with Hadoop (or similar) Ecosystem (MapReduce, Yarn, HDFS, Hive, Spark, Presto, Pig, HBase)Familiarity with data exploration / data visualization tools like Tableau, Looker, Chartio, etc.Ability to think strategically, analyze and interpret market and consumer information.Strong communication skills – written and verbal presentations.Excellent conceptual and analytical reasoning competencies.Degree in an analytical field such as economics, mathematics, or computer science is desired.Comfortable working in a fast-paced and highly collaborative environment.Process-oriented with phenomenal documentation skills', 'Document and publish Metadata and table designs to facilitate data adoption.', 'Perform SQL and ETL tuning as necessary.', 'What To Bring', 'Perform ad hoc analysis as necessary.', 'Develop and maintain ETL routines using ETL and orchestration tools such as Airflow, Luigi and Jenkins.', '2+ years of relevant Professional experience.', 'Work with Engineering teams to collect required data from internal and external systems.', '1+ years work experience implementing and reporting on business key performance indicators in data warehousing environments. Strong understanding of data modeling principles including Dimensional modeling, data normalization principles etc.', 'Ability to think strategically, analyze and interpret market and consumer information.', 'Develop and maintain Dashboards/reports using Tableau and Looker', 'Summary', 'Strong communication skills – written and verbal presentations.', 'Excellent conceptual and analytical reasoning competencies.', 'Design table structures and define ETL strategy to build performant Data solutions that are reliable and scalable in a fast growing data ecosystem.', 'Partner with technical and non-technical colleagues to understand data and reporting requirements.', ""What You'll Do"", 'Experience with Hadoop (or similar) Ecosystem (MapReduce, Yarn, HDFS, Hive, Spark, Presto, Pig, HBase)', 'Experience programming languages (e.g. Python, R, bash) preferred.', '1 + years experience using analytic SQL, working with traditional relational databases and/or distributed systems such as Hadoop / Hive, BigQuery, Redshift.', 'Process-oriented with phenomenal documentation skills', 'Develop Data Quality checks for source and target data sets. Develop UAT plans and conduct QA.', 'Good understanding of SQL Engines and able to conduct advanced performance tuning']",Mid-Senior level,Full-time,Business Development,Marketing and Advertising,2021-03-18 14:34:51
SQL Data Engineer,Tesla,"Fremont, CA",3 weeks ago,Over 200 applicants,"['', 'Qualifications', 'Develop collaborative relationships with key business sponsors and IT resources for the efficient resolution of work requests.', 'Work in a time constrained environment to analyze, design, develop and deliver Enterprise Data Warehouse solutions for Tesla’s Finance and Accounting teams', 'Responsibilities', 'Communicate technical and business topics, as appropriate, in a 360-degree fashion, when required; communicate using written, verbal and/or presentation materials as necessary.', 'Develop, enforce, and recommend enhancements to Applications in the area of standards, methodologies, compliance, and quality assurance practices; participate in design and code walkthroughs.', 'Work on ETL SSIS, Business Intelligence & Reporting tools like SSRS, SSAS (Multidimensional and Tabular) and Tableau', 'Minimum Qualifications:', 'Provide timely and accurate estimates for newly proposed functionality enhancements', 'Work with systems that handle sensitive data with strict SOX controls and change management processes', 'Utilize technical and domain knowledge to develop and implement effective solutions; provide hands on mentoring to team members through all phases of the Systems Development Life Cycle (SDLC) using Agile practices.', 'critical situation', 'The Role', 'Preferred Qualifications', 'Work in a time constrained environment to analyze, design, develop and deliver Enterprise Data Warehouse solutions for Tesla’s Finance and Accounting teamsWork on ETL SSIS, Business Intelligence & Reporting tools like SSRS, SSAS (Multidimensional and Tabular) and TableauWork with systems that handle sensitive data with strict SOX controls and change management processesDevelop collaborative relationships with key business sponsors and IT resources for the efficient resolution of work requests.Provide timely and accurate estimates for newly proposed functionality enhancementscritical situationCommunicate technical and business topics, as appropriate, in a 360-degree fashion, when required; communicate using written, verbal and/or presentation materials as necessary.Develop, enforce, and recommend enhancements to Applications in the area of standards, methodologies, compliance, and quality assurance practices; participate in design and code walkthroughs.Utilize technical and domain knowledge to develop and implement effective solutions; provide hands on mentoring to team members through all phases of the Systems Development Life Cycle (SDLC) using Agile practices.']",Entry level,Full-time,Information Technology,Automotive,2021-03-18 14:34:51
Junior Data Engineer ,Brooksource,"Charlotte, NC",,N/A,"['', 'Communicate with business and other technology teams onshore and offshore', 'Present data stories to business leaders', '1+ years utilizing Python to develop scripts', 'Build frameworks and reusable components for data acquisition and integration going across structured and unstructured data sources', 'POSITION SUMMARY:', 'DESIRED SKILLS:', 'Knowledge of Spark, Hadoop, Kafka, and Streaming Components', 'B.S. or M.S. in Computer Science, Data Science or similar field or equivalent experience and work record', 'Create data standards and patterns for others to use across the company', 'Experience building data integration and ETL components using DataStage', 'Strong problem solving and troubleshooting skills with the ability to exercise mature judgment', '1+ years building Tableau dashboards and visuals to create data stories', 'Cloud knowledge and/or experience ', 'B.S. or M.S. in Computer Science, Data Science or similar field or equivalent experience and work record1+ years working within a SQL database1+ years of Application Development experience with Java or a similar Object-Oriented Programming Language1+ years utilizing Python to develop scripts1+ years of data mining or data comparison experienceStrong problem solving and troubleshooting skills with the ability to exercise mature judgmentAbility to manage multiple projects without continuous directionAbility to function effectively and proficiently in an environment of ongoing changeProactive self-starter with the ability to work well under pressure, prioritize and multitask.Experience working directly with technology, business stakeholders/leadership in formulating technical requirements', 'Director level exposure and mentorshipClose-knit junior level team', 'REQUIRED SKILLS:', 'Use data to find new efficiencies and decrease data duplication across the company', 'Director level exposure and mentorship', 'Assist with troubleshooting issues and providing end to end technical solutions quickly and accurately', 'Experience in the financial services industry, particularly within a data environmentKnowledge in designing and building SQL apps on Oracle, Teradata, HBase and Hive/ImpalaKnowledge of Spark, Hadoop, Kafka, and Streaming ComponentsExperience building data integration and ETL components using DataStageApplication integration experience using API/MulesoftCloud knowledge and/or experience 1+ years building Tableau dashboards and visuals to create data stories', 'Brooksource is seeking an Junior Data Engineer within the Chief Data Office to join our Fortune 100 financial services client in the Charlotte, NC area. As the digital transformation continues to accelerate, this team is responsible for creating a digital footprint for the enterprise . This team is responsible for figuring out how much data is in the enterprise, who is using it, where it is located, etc. An ideal candidate is highly organized with good communication skills who can manage multiple tasks simultaneously and to be able to respond to and manage unplanned priorities in a fast-paced environment.', 'KEY RESPONSIBILITIES AND DUTIES:', 'Ability to manage multiple projects without continuous direction', '1+ years working within a SQL database', 'Experience working directly with technology, business stakeholders/leadership in formulating technical requirements', 'VALUE ADD TO YOU:', 'Build frameworks and reusable components for data acquisition and integration going across structured and unstructured data sourcesUse data to find new efficiencies and decrease data duplication across the companyScan assets, pulling payment information, and inputting into corresponding applicationsCreate data standards and patterns for others to use across the companyCommunicate with business and other technology teams onshore and offshorePresent data stories to business leadersMonitor and analyze existing processes to identify enhancement opportunitiesAssist with troubleshooting issues and providing end to end technical solutions quickly and accuratelyHandle complex operational tasks and recommends processing and tech changes with minimal supervisionMaintain knowledge of emerging technologies', 'Application integration experience using API/Mulesoft', 'Handle complex operational tasks and recommends processing and tech changes with minimal supervision', 'Close-knit junior level team', 'Monitor and analyze existing processes to identify enhancement opportunities', 'Knowledge in designing and building SQL apps on Oracle, Teradata, HBase and Hive/Impala', 'Scan assets, pulling payment information, and inputting into corresponding applications', '\xa0', '1+ years of Application Development experience with Java or a similar Object-Oriented Programming Language', 'Proactive self-starter with the ability to work well under pressure, prioritize and multitask.', 'Brooksource provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, national origin, age, sex, citizenship, disability, genetic information, gender, sexual orientation, gender identity, marital status, amnesty or status as a covered veteran in accordance with applicable federal, state, and local laws.', 'Maintain knowledge of emerging technologies', 'Ability to function effectively and proficiently in an environment of ongoing change', '1+ years of data mining or data comparison experience', 'Experience in the financial services industry, particularly within a data environment']",Entry level,Contract,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Accenture,"New York, NY",19 hours ago,Be among the first 25 applicants,"['', 'Preferred Qualifications:', 'Create a value chain to help address the challenges of acquiring data, evaluating its value, distilling & analyzing ', 'Database design and performance optimization with multiple databases (relational data stores - RDS, Aurora; NoSQL data stores - DynamoDB; data warehouses -Teradata, Redshift, snowflake; graph databases)', 'For now, all Accenture business travel, international and domestic, is currently restricted to client-essential sales/delivery activity only.Please note: The safety and well-being of our people continues to be the top priority, and our decisions around travel are informed by government COVID-19 response directives, recommendations from leading health authorities and guidance from a number of infectious disease experts.', 'Basic Qualifications:', 'Work in an agile CI/CD environment, (Jenkins/Ansible experience a plus)', 'Business Translator (identifying business problem, initiative, analytics intervention, data science management, data science interpretation, storytelling)', 'Understand and used Object Orientated design techniques', '2 years of Experience writing REST or GraphQL APIs', 'Data & Analytics Transformation (current state assessment, strategy development, value case, roadmap, and blueprint)', 'Experience working with large data sets in distributed data environments ', 'Data architecture (Understanding logical ways of organizing and analyzing data and how this affects building databases, APIs, and UIs)', 'Experience with containerization platforms a plus, such as Docker, Kubernetes ', 'Process Automation, Machine Learning, and Artificial Intelligence practices (knowledge of how advancing digital tools and techniques are applied in enterprise data and analytics strategies and roadmaps)', 'Minimum of two years of experience in one or more of the following areas: ', 'Minimum of 3 years of combined data, analytics and strategic consulting experience', 'A Bachelor’s Degree or equivalent work experience (12 years) or an Associate’s Degree with 6 years of work experience', 'Experience using TDD and unit testing as part of normal software development using packages such as Postman, Jest, JUnit, PyUnit, Swagger, etc.', 'Lead data modeling activities to capture and model data requirements, business rules, and logical and physical models', 'Experience in data migration from on-prem to cloud. Multi-Cloud experience - AWS/Azure/Google a plus', 'Minimum of 3 years of hands-on experience on one or more of these technologies -Python, Scala, Spark, PySpark', 'Infrastructure as Code (Terraform or similar technology)']",Mid-Senior level,Full-time,Strategy/Planning,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Collabera Inc.,"McLean, VA",,N/A,"['', 'Utilize Enterprise Technology assets, when applicable.', 'Must Have ', 'Generally work is self-directed and not prescribed.', 'Experienced in building Real-time/Batch Ingestion and data processing pipelines built using Spark/Databricks', 'You will be responsible to', 'Works with less structured, more complex issues.', '6+ years of experience in software development', 'Hiring Data Engineer for 100% Remote Role', 'Description: ', 'Present solutions to teams and stakeholders and document design decisions', 'Set up CI/CD Pipelines and maintain.', 'Prerequisite', 'Skilled in coding in Python or Java', 'Adopt Enterprise Architecture guidelines to store and manage Big Data.', 'Work with Product owner to develop stories and user flows.', 'Strong leadership, organizational and time management skills', '2+ years of experience leading small/medium teams', 'In the role of Lead Software Engineer, Come and join our teams in building our Clinical Products and other applications. You will work independently and in a team as needed, in developing new features to build new products and enhance our applications as needed. Our platforms consume and generate a great volume of data.', ""A bachelor's degree in Computer Sciences or related field, or equivalent work experience; master's degree preferred"", 'Able to contribute towards test automation and DevOps pipelines', 'Sense of urgency in daily work ethic', 'This is Contract to Hire role with decent Salary plus benefits on conversion.Currently, not open for individuals who needs a sponsorship or has visa dependencies.', 'This is Contract to Hire role with decent Salary plus benefits on conversion.', 'Adopt Enterprise Architecture guidelines to store and manage Big Data.Develop data pipelines using Spark/DatabricksPresent solutions to teams and stakeholders and document design decisionsUtilize Enterprise Technology assets, when applicable.Work with Product owner to develop stories and user flows.Set up CI/CD Pipelines and maintain.Generally work is self-directed and not prescribed.Works with less structured, more complex issues.', ""A bachelor's degree in Computer Sciences or related field, or equivalent work experience; master's degree preferred6+ years of experience in software development2+ years of experience leading small/medium teamsExperienced in building Real-time/Batch Ingestion and data processing pipelines built using Spark/DatabricksSkilled in coding in Python or JavaAble to contribute towards test automation and DevOps pipelinesStrong interpersonal skills, coupled with equally strong Team Building and CommunicationSense of urgency in daily work ethicStrong leadership, organizational and time management skills"", 'Currently, not open for individuals who needs a sponsorship or has visa dependencies.', 'Develop data pipelines using Spark/Databricks', 'Strong interpersonal skills, coupled with equally strong Team Building and Communication']",Executive,Contract,Information Technology,Pharmaceuticals,2021-03-18 14:34:51
Data Engineer Intern - Data Science,LinkedIn,"Sunnyvale, CA",2 weeks ago,Over 200 applicants,[''],Internship,Internship,Engineering,Internet,2021-03-18 14:34:51
Data Engineer,Amazon,"Seattle, WA",1 day ago,Be among the first 25 applicants,"['', ' Effective troubleshooting and problem solving skills', ' Professional experience as a Data Engineer or related specialty (e.g., Software Engineer, Business Intelligence Engineer, Data Scientist) with a track record of manipulating, processing, and extracting value from large datasets', ' Familiar with computer science fundamentals including object-oriented design, data structures, algorithm design, problem solving, and complexity analysis', ' Degree in Computer Science, Engineering, Mathematics, or a related field and 5+ years of professional experience', ' Experience in data modeling, ETL development and data warehousing', ' Experience with Big Data Technologies (Hadoop, Hive, Hbase, Pig, Spark, etc.)', ' Professional experience as a Data Engineer or related specialty (e.g., Software Engineer, Business Intelligence Engineer, Data Scientist) with a track record of manipulating, processing, and extracting value from large datasets Knowledge of AWS Technologies Knowledge in using OLAP technologies and BI Analytics. Oracle, Redshift, Linux, OBIEE experience Experience with multiple database platforms Familiar with computer science fundamentals including object-oriented design, data structures, algorithm design, problem solving, and complexity analysis Coding proficiency in at least one modern programming language (, , , etc) Query tuning skills', ' Degree in Computer Science, Engineering, Mathematics, or a related field and 5+ years of professional experience Experience in data modeling, ETL development and data warehousing Data warehousing experience with Redshift, Teradata, etc. Experience with Big Data Technologies (Hadoop, Hive, Hbase, Pig, Spark, etc.) Excellent verbal and written communication Effective troubleshooting and problem solving skills Strong customer focus, ownership, urgency and drive. Excellent verbal and written communication skills and the ability to work well in a team.', 'Description', ' Knowledge in using OLAP technologies and BI Analytics.', 'Company', ' Oracle, Redshift, Linux, OBIEE experience', 'Basic Qualifications', ' Coding proficiency in at least one modern programming language (, , , etc)', ' Excellent verbal and written communication skills and the ability to work well in a team.', ' Excellent verbal and written communication', ' Strong customer focus, ownership, urgency and drive.', ' Experience with multiple database platforms', ' Knowledge of AWS Technologies', 'Preferred Qualifications', ' Data warehousing experience with Redshift, Teradata, etc.', ' Query tuning skills']",Not Applicable,Full-time,Strategy/Planning,Computer Software,2021-03-18 14:34:51
"Software Engineer, Data Pipelines",GitHub,"Washington, DC",19 hours ago,Be among the first 25 applicants,"['', 'Preferred Qualifications:', 'You have operational experience with data pipelines and distributed systems', 'Why You Should Join:', ""Share responsibility for the availability and performance of our team's systems"", ""You're passionate about software design and have good testing skills"", 'Collaborate with teammates to design data pipelines and APIs, seeking and offering feedback along the way', 'You have experience with Kafka, Java and the JVMYou have DevOps experience\xa0', 'Build high-volume event collection, processing and storage systems', ""You're able to empathize with a diverse set of engineers"", 'Responsibilities:', ""Build high-volume event collection, processing and storage systemsWork with application engineers to build product features that use GitHub's dataCollaborate with teammates to design data pipelines and APIs, seeking and offering feedback along the wayDevelop foundational data infrastructure, enabling other teams to build data-centric featuresWork across many languages including Go, Java, Ruby and PythonShare responsibility for the availability and performance of our team's systems"", ""Work with application engineers to build product features that use GitHub's data"", 'You have DevOps experience\xa0', ""You've built software using several different programming languages"", 'Minimum Qualifications:\xa0', 'Work across many languages including Go, Java, Ruby and Python', ""You have strong written and verbal communication skillsYou've built software using several different programming languagesYou're passionate about software design and have good testing skillsYou have operational experience with data pipelines and distributed systemsYou're able to empathize with a diverse set of engineers"", 'Develop foundational data infrastructure, enabling other teams to build data-centric features', 'Who We Are:', 'You have experience with Kafka, Java and the JVM', 'You have strong written and verbal communication skills', 'Leadership Principles:']",Associate,Full-time,Engineering,Computer Software,2021-03-18 14:34:51
Data Engineer - 100% REMOTE,Sonatype,"Atlanta, GA",3 days ago,Be among the first 25 applicants,"['', ""The opportunity to be part of an incredible, high-growth company, working on a team of experienced colleaguesCompetitive salary packageMedical/Dental/Vision benefitsPaid Parental LeavePaid Volunteer Time Off (VTO)Business casual dressFlexible work schedules that ensure time for you to be you2019 Best Places to Work Washington Post and Washingtonian2019 Wealthfront Top Career Launch CompanyEY Entrepreneur of the Year 2019Fast Company Top 50 Companies for InnovatorsGlassdoor ranking of 4.9Come see why we've won all of these awards"", 'Database and data manipulation skills working with relational or non-relational models.', 'Experience working in a highly distributed environment, using modern collaboration tools to facilitate team communication.', 'Required Skills And Experience', 'Business casual dress', 'Knowledge and experience with large scale data tools and techniques (i.e. MapReduce, Hadoop, Hive, Spark).', 'Deep software engineering experience; we primarily use Java.Database and data manipulation skills working with relational or non-relational models.Strong ability to select and integrate appropriate tools, frameworks, systems to build great solutions.Deep curiosity for how things work and desire to make them better.Currently reside in either Canada, Colombia, or the United States of America and are legally authorized to work without sponsorship in the corresponding country.', 'What We Offer', 'Glassdoor ranking of 4.9', 'Desired Skills And Experience', 'Knowledge and experience with AWS Big Data services (i.e. EMR, ElasticSearch).', 'The opportunity to be part of an incredible, high-growth company, working on a team of experienced colleagues', 'Knowledge and experience with non-relational databases (i.e. Hbase, MongoDB, Cassandra).', 'Competitive salary package', '2019 Wealthfront Top Career Launch Company', 'Fast Company Top 50 Companies for Innovators', '2019 Best Places to Work Washington Post and Washingtonian', 'Medical/Dental/Vision benefits', 'Strong ability to select and integrate appropriate tools, frameworks, systems to build great solutions.', 'Currently reside in either Canada, Colombia, or the United States of America and are legally authorized to work without sponsorship in the corresponding country.', ""Come see why we've won all of these awards"", 'EY Entrepreneur of the Year 2019', 'Paid Volunteer Time Off (VTO)', 'Deep curiosity for how things work and desire to make them better.', 'Paid Parental Leave', 'Degree in Computer Science, Engineering, or another quantitative field.', 'Flexible work schedules that ensure time for you to be you', 'Degree in Computer Science, Engineering, or another quantitative field.Knowledge and experience with non-relational databases (i.e. Hbase, MongoDB, Cassandra).Knowledge and experience with large scale data tools and techniques (i.e. MapReduce, Hadoop, Hive, Spark).Knowledge and experience with AWS Big Data services (i.e. EMR, ElasticSearch).Experience working in a highly distributed environment, using modern collaboration tools to facilitate team communication.', 'Deep software engineering experience; we primarily use Java.']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer (Experienced),Salesforce,"Los Angeles, CA",1 day ago,Be among the first 25 applicants,"['', ' Clearly articulate pros and cons of various technologies and platforms in open source and proprietary products. Execute proof of concept on new technology and tools to help the organization pick the best tools and solutions. ', ' Communicate with product owners and analysts to clarify requirements. Craft technical solutions and assemble design artifacts (functional design documents, data flow diagrams, data models, etc.). ', 'Salesforce.org', 'Accommodations ', 'Posting Statement', ' Own the technical solution design, lead the technical architecture and implementation of data acquisition and integration projects, both batch and real time. Define the overall solution architecture needed to implement a layered data stack that ensures a high level of data quality and timely insights. ', ' Strong SQL optimization and performance tuning experience in a high volume data environment that utilizes parallel processing. Hadoop, Spark, Teradata platform experience a plus. ', ' 5+ years of experience working with ETL tools, specifically creating data driven orchestration and transformation jobs and user and project administration. A strong Python scripting knowledge including hands-on experience in building packages for ETL is required. Advanced Matillion developer is a plus. ', ' 4+ years of experience with Data Warehouse including knowledge of Stored Procedures, tasks and streams. Must be an expert in writing complex SQL queries and understand the methodologies to tune/improve query performance. ', ' Build data pipelines data processing tools and technologies in open source and proprietary products. ', ' Own the technical solution design, lead the technical architecture and implementation of data acquisition and integration projects, both batch and real time. Define the overall solution architecture needed to implement a layered data stack that ensures a high level of data quality and timely insights.  Communicate with product owners and analysts to clarify requirements. Craft technical solutions and assemble design artifacts (functional design documents, data flow diagrams, data models, etc.).  Build data pipelines data processing tools and technologies in open source and proprietary products.  Serve the team as a subject matter expert & mentor for ETL design, and other related big data and programming technologies.  Identify incomplete data, improve quality of data, and integrate data from several data sources.  Proactively identify performance & data quality problems and drive the team to remediate them. Advocate architectural and code improvements to the team to improve execution speed and reliability.  Design and develop tailored data structures in database and Hadoop.  Quickly create functioning ETL prototypes to address quickly changing business needs.  Revamp prototypes to create production-ready data flows.  Support Data Science research by designing, developing, and maintaining all parts of the Big Data pipeline for reporting, statistical and machine learning, and computational requirements.  Perform data profiling, complex sampling, statistical testing, and testing of reliability on data.  Clearly articulate pros and cons of various technologies and platforms in open source and proprietary products. Execute proof of concept on new technology and tools to help the organization pick the best tools and solutions.  Harness operational excellence & continuous improvement with a can do leadership attitude. ', ' Identify incomplete data, improve quality of data, and integrate data from several data sources. ', 'To get the best candidate experience, please consider applying for a maximum of 3 roles within 12 months to ensure you are not duplicating efforts.', ' Knowledge of data modeling techniques and high-volume ETL/ELT design. ', 'Responsibilities', ' Support Data Science research by designing, developing, and maintaining all parts of the Big Data pipeline for reporting, statistical and machine learning, and computational requirements. ', 'Job Requirements', 'Salesfore.com', ' Ability to research, analyze, interpret, and produce accurate results within reasonable turnaround times with an iterative mindset with rapid prototyping designs. ', ' Familiarity with scrum/agile project management methodologies and SDLC stages required. ', ' Strong problem solving with acute attention to detail and ability to meet tight deadlines and project plans. ', 'Job Details', 'Accommodations  - ', ' Proactively identify performance & data quality problems and drive the team to remediate them. Advocate architectural and code improvements to the team to improve execution speed and reliability. ', 'Salesforce.com', ' Design and develop tailored data structures in database and Hadoop. ', ' Harness operational excellence & continuous improvement with a can do leadership attitude. ', ' Experience with version control systems (Github, Subversion) and deployment tools (e.g. continuous integration) required. ', ' If you are currently in college/ grad school or have less than a year of experience - please check out FutureForce job opportunities at Salesforce:', ' Previous projects should display technical leadership with an emphasis on data lake, data warehouse solutions, business intelligence, big data analytics, enterprise-scale custom data products. ', ' Quickly create functioning ETL prototypes to address quickly changing business needs. ', ' Revamp prototypes to create production-ready data flows. ', ' Hands-on on Salesforce.com knowledge of product and functionality a plus. ', ' BS/MS degree in Computer Science, Engineering, Mathematics, Physics, or equivalent/related degree.  5+ years of experience working with ETL tools, specifically creating data driven orchestration and transformation jobs and user and project administration. A strong Python scripting knowledge including hands-on experience in building packages for ETL is required. Advanced Matillion developer is a plus.  4+ years of experience with Data Warehouse including knowledge of Stored Procedures, tasks and streams. Must be an expert in writing complex SQL queries and understand the methodologies to tune/improve query performance.  Previous projects should display technical leadership with an emphasis on data lake, data warehouse solutions, business intelligence, big data analytics, enterprise-scale custom data products.  Familiarity with new big data management techniques of schema on read, search analytics, graph analytics, semantic data lakes, linked data, etc.  Knowledge of data modeling techniques and high-volume ETL/ELT design.  Strong SQL optimization and performance tuning experience in a high volume data environment that utilizes parallel processing. Hadoop, Spark, Teradata platform experience a plus.  Experience with version control systems (Github, Subversion) and deployment tools (e.g. continuous integration) required.  Experience with programming languages like Java, Scala & scripting in Python, Perl, Bash.  Experience working with Public Cloud platforms like GPC, AWS, or Azure.  Familiarity with scrum/agile project management methodologies and SDLC stages required.  Hands-on on Salesforce.com knowledge of product and functionality a plus.  Ability to work effectively in an unstructured and fast-paced environment both independently and in a team setting,with a high degree of self-management with clear communication and commitment to delivery timelines.  Strong problem solving with acute attention to detail and ability to meet tight deadlines and project plans.  Ability to research, analyze, interpret, and produce accurate results within reasonable turnaround times with an iterative mindset with rapid prototyping designs. ', 'Role Description', ' Experience working with Public Cloud platforms like GPC, AWS, or Azure. ', ' BS/MS degree in Computer Science, Engineering, Mathematics, Physics, or equivalent/related degree. ', ' Familiarity with new big data management techniques of schema on read, search analytics, graph analytics, semantic data lakes, linked data, etc. ', 'Job Category', ' Experience with programming languages like Java, Scala & scripting in Python, Perl, Bash. ', 'https://www.salesforce.com/company/careers/university-recruiting/', ' Ability to work effectively in an unstructured and fast-paced environment both independently and in a team setting,with a high degree of self-management with clear communication and commitment to delivery timelines. ', ' Perform data profiling, complex sampling, statistical testing, and testing of reliability on data. ', ' Serve the team as a subject matter expert & mentor for ETL design, and other related big data and programming technologies. ']",Not Applicable,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Engineer,Lockheed Martin,"Orlando, FL",4 days ago,Be among the first 25 applicants,"['', 'Desired Skills', '2+ Years of experience working with Big Data Technologies such as Spark, Kafka, & Presto', ' Ability to translate customer needs into Data Quality solutions', 'Ingesting Data from Batch and Streaming processes to load it into our Data Lake', 'Developing CI/CD pipelines to manage effective changes to multiple infrastructure foot prints', 'Experience Level', ' Ability to focus on needed deliverables, working within a DevSecOps team Ability to translate customer needs into Data Quality solutions Experience working with Big Data Technologies such as Spark, Kafka, & Presto Has a strong teaming mentality, can work collaboratively to develop optimum solutions Knowledge of various data processing architectural design patterns Past experience working in a multi-functional team in an applied machine learning context involving streaming and batch processing.', ' Past experience working in a multi-functional team in an applied machine learning context involving streaming and batch processing.', 'Experience with DevOps tools: Docker, Git [GitLab, GitHub], Continuous Integration [CI], Continuous Deployment [CD]', 'Bachelor’s Degree in Engineering, Computer Science, or other related discipline', 'Ability to focus on needed deliverables, working within a DevSecOps team', ' Experience working with Big Data Technologies such as Spark, Kafka, & Presto', '1+ Years of Experience working with Kubernetes', 'Knowledgeable in multiple programming languages (Python, Bash, etc.)Desired Skills', '1+ Years writing microservices', 'Job.Qualifications', 'Managing and creating ETL processes to prepare data for consumption', 'Bachelor’s Degree in Engineering, Computer Science, or other related discipline2+ Years of experience working with Big Data Technologies such as Spark, Kafka, & PrestoExperience with DevOps tools: Docker, Git [GitLab, GitHub], Continuous Integration [CI], Continuous Deployment [CD]Knowledgeable in multiple programming languages (Python, Bash, etc.)Desired SkillsAbility to focus on needed deliverables, working within a DevSecOps teamHas a strong teaming mentality, can work collaboratively to develop optimum solutions1+ Years writing microservices1+ Years of Experience working with KubernetesKnowledge of various data processing architectural design patternsPast experience working in a multi-functional team in an applied machine learning context involving streaming and batch process', ' Knowledge of various data processing architectural design patterns', ' Ability to focus on needed deliverables, working within a DevSecOps team', 'Past experience working in a multi-functional team in an applied machine learning context involving streaming and batch process', ' Has a strong teaming mentality, can work collaboratively to develop optimum solutions', 'Lockheed Martin is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.', 'Basic Qualifications', 'Description:', ' Develop and implement automated regression test plans', 'Creating monitoring capabilities to ensure data streams are effective and active', 'Knowledge of various data processing architectural design patterns', 'Has a strong teaming mentality, can work collaboratively to develop optimum solutions', 'BASIC QUALIFICATIONS:', 'Ingesting Data from Batch and Streaming processes to load it into our Data LakeManaging and creating ETL processes to prepare data for consumptionCreating monitoring capabilities to ensure data streams are effective and activeDeveloping CI/CD pipelines to manage effective changes to multiple infrastructure foot prints Develop and implement automated regression test plans']",Entry level,Full-time,Information Technology,Construction,2021-03-18 14:34:51
Data Engineer,Radancy,"Fayetteville, GA",2 days ago,Be among the first 25 applicants,"['', ' Build monitoring dashboards and automate data quality testing ', ' Assist with the development and review of technical and end user documentation including ETL workflows, research, and data analysis ', 'About The Job', ' Product / reporting suite experience ', ' Detail oriented and strong communicator ', ' Develop and lead the testing and fixing of new or enhanced solutions for data products and reports, including automating ETL testing ', ' Responsible for daily integrity checks, performing deployments and releases ', ' Bachelors or Masters degree in Computer Science or other related field ', ' AdTech experience preferred ', ' Radancy Data Engineering works on data services across product organizations within Radancy, and supports building a customer facing data visualization product. The Data Engineering team supports an enterprise grade recruitment platform focusing on talent acquisition and job opportunity exploration. ', ' Ingest and aggregate data from both internal and external data sources to build our world class datasets ', ' Exposure / familiarity with Google Cloud Platform / BigQuery / Amazon Redshift ', 'The Team', ' Work with Cloud Computing Platforms (GCP/AWS), Luigi, Kafka and other open-source technologies ', ' Own meaningful parts of our service, have an impact, grow with the company ', ' Radancy Data Engineering works on data services across product organizations within Radancy, and supports building a customer facing data visualization product. The Data Engineering team supports an enterprise grade recruitment platform focusing on talent acquisition and job opportunity exploration.  The team has extensive experience in ETL development, works with large scale data in real time, and cross collaborates with other engineering teams across the organization. ', 'Desired Technical Qualifications', ' Build and maintain ETL pipelines utilizing Python that connect 1st and 3rd party data  Work with Cloud Computing Platforms (GCP/AWS), Luigi, Kafka and other open-source technologies  Conduct data modeling, schema design, and SQL development  Ingest and aggregate data from both internal and external data sources to build our world class datasets  Develop and lead the testing and fixing of new or enhanced solutions for data products and reports, including automating ETL testing  Collaborate with Product Owner and domain experts to recognize and help adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation  Assist with the development and review of technical and end user documentation including ETL workflows, research, and data analysis  Work with Product team to define data collection and engineering frameworks  Build monitoring dashboards and automate data quality testing  Responsible for daily integrity checks, performing deployments and releases  Own meaningful parts of our service, have an impact, grow with the company ', ' Build and maintain ETL pipelines utilizing Python that connect 1st and 3rd party data ', ' Collaborate with Product Owner and domain experts to recognize and help adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation ', ' Work with Product team to define data collection and engineering frameworks ', ' 2+ years of Python, SQL, and ETL development ', 'Overview', 'Flexible Location (Remote Hubs): ', ' Enthusiastic about working with and exploring new data sets ', ' Exposure to front end development: HTML, JavaScript, jQuery, Angular or similar libraries ', ' Radancy', ' 2+ years of Python, SQL, and ETL development  Bachelors or Masters degree in Computer Science or other related field  Product / reporting suite experience  Familiarity with C#, .Net, Kafka, Docker  Exposure to front end development: HTML, JavaScript, jQuery, Angular or similar libraries  Exposure / familiarity with Google Cloud Platform / BigQuery / Amazon Redshift  AdTech experience preferred  Enthusiastic about working with and exploring new data sets  Detail oriented and strong communicator ', ' The team has extensive experience in ETL development, works with large scale data in real time, and cross collaborates with other engineering teams across the organization. ', ' Familiarity with C#, .Net, Kafka, Docker ', ' Conduct data modeling, schema design, and SQL development ']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
"Data Engineer III, Tech",Walmart,"Bentonville, AR",2 days ago,49 applicants,"['', 'Data Modeling: Analyzes complex data elements, systems, data flows, dependencies, and relationships to contribute to conceptual, physical, andlogical data models. Develops the Logical Data Model and Physical Data Models including data warehouse and data mart designs. Defines relationaltables, primary and foreign keys, and stored procedures to create a data model structure. ', 'Provides and supports the implementation of business solutions by building relationships and partnerships with key stakeholders; identifying business needs; determining and carrying out necessary processes and practices; monitoring progress and results; recognizing and capitalizing on improvement opportunities; and adapting to competing demands, organizational changes, and new responsibilities.', 'Data Strategy: Understands, articulates, and applies principles of the defined strategy to routine business problems that involve a single function.', 'Evaluates existing data models and physical databases forvariances and discrepancies. Develops efficient data flows. Analyzes data-related system integration challenges and proposes appropriate solutions.', ""Problem Formulation: Identifies possible options to address the business problems within one's discipline through analytics, big data analytics, andautomation."", 'Applied Business Acumen: Supports the development of business cases and recommendations. Owns delivery of project activity and tasks assignedby others. Supports process updates and changes. Solves business issues.Data Governance: Supports the documentation of data governance processes. Supports the implementation of data governance practices.Data Strategy: Understands, articulates, and applies principles of the defined strategy to routine business problems that involve a single function.Data Transformation and Integration: Extracts data from identified databases. Creates data pipelines and transform data to a structure that is relevantto the problem by selecting appropriate techniques. Develops knowledge of current data science and analytics trends.Data Source Identification: Supports the understanding of the priority order of requirements and service level agreements. Helps identify the mostsuitable source for data that is fit for purpose. Performs initial data quality checks on extracted data.Data Modeling: Analyzes complex data elements, systems, data flows, dependencies, and relationships to contribute to conceptual, physical, andlogical data models. Develops the Logical Data Model and Physical Data Models including data warehouse and data mart designs. Defines relationaltables, primary and foreign keys, and stored procedures to create a data model structure. ', 'Understands and translates business and functional needs to produce clean datasets; designs and develops scalable ETL solutions in cloud; works closely with data scientists and machine learning engineers to develop data pipelines; collaborates with development teams to test and deploy large scale data processing pipelines; creates metrics to continuously evaluate the performance of ETL solutions; maintains and improves the performance of existing solutions; ensures adherence to performance standards and compliance to data security requirements; keeps abreast of new tools, algorithms, and techniques in data engineering and works to implement them in the organization.', 'Creates training documentation and trains end-users on data modeling. Oversees the tasks of less experienced programmers and stipulates systemtroubleshooting supports.Code Development and Testing: Writes code to develop the required solution and application features by determining the appropriate programminglanguage and leveraging business, technical, and data requirements. Creates test cases to review and validate the proposed solution design. Createsproofs of concept. Tests the code using the appropriate testing approach. Deploys software to production servers. Contributes code documentation,maintains playbooks, and provides timely progress updates.Demonstrates up-to-date expertise and applies this to the development, execution, and improvement of action plans by providing expert advice andguidance to others in the application of information and best practices; supporting and aligning efforts to meet customer and business needs; andbuilding commitment for perspectives and rationales.Provides and supports the implementation of business solutions by building relationships and partnerships with key stakeholders; identifying business needs; determining and carrying out necessary processes and practices; monitoring progress and results; recognizing and capitalizing on improvement opportunities; and adapting to competing demands, organizational changes, and new responsibilities.Models compliance with company policies and procedures and supports company mission, values, and standards of ethics and integrity by incorporating these into the development and implementation of business plans; using the Open Door Policy; and demonstrating and assisting others with how to apply these in executing business processes and practices.', 'Outlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications. ', 'Data Source Identification: Supports the understanding of the priority order of requirements and service level agreements. Helps identify the mostsuitable source for data that is fit for purpose. Performs initial data quality checks on extracted data.', 'Demonstrates up-to-date expertise and applies this to the development, execution, and improvement of action plans by providing expert advice andguidance to others in the application of information and best practices; supporting and aligning efforts to meet customer and business needs; andbuilding commitment for perspectives and rationales.', 'Data Transformation and Integration: Extracts data from identified databases. Creates data pipelines and transform data to a structure that is relevantto the problem by selecting appropriate techniques. Develops knowledge of current data science and analytics trends.', 'About Global Tech', 'Minimum Qualifications...', 'Code Development and Testing: Writes code to develop the required solution and application features by determining the appropriate programminglanguage and leveraging business, technical, and data requirements. Creates test cases to review and validate the proposed solution design. Createsproofs of concept. Tests the code using the appropriate testing approach. Deploys software to production servers. Contributes code documentation,maintains playbooks, and provides timely progress updates.', 'Applied Business Acumen: Supports the development of business cases and recommendations. Owns delivery of project activity and tasks assignedby others. Supports process updates and changes. Solves business issues.', 'Creates training documentation and trains end-users on data modeling. Oversees the tasks of less experienced programmers and stipulates systemtroubleshooting supports.', 'Data Governance: Supports the documentation of data governance processes. Supports the implementation of data governance practices.', 'Models compliance with company policies and procedures and supports company mission, values, and standards of ethics and integrity by incorporating these into the development and implementation of business plans; using the Open Door Policy; and demonstrating and assisting others with how to apply these in executing business processes and practices.', ""Position Summary... What You'll Do..."", 'Outlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications. ']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Tata Consultancy Services,"Charlotte, NC",2 days ago,Be among the first 25 applicants,[''],Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer 3,PayPal,"Chandler, AZ",4 weeks ago,Be among the first 25 applicants,"['', 'Experience in handling petabyte scale data ', 'Master’s degree or equivalent in Computer Science or related field with a minimum of 5+ years of hands-on software engineering experience. 5+ years of experience building large scalable and reliable enterprise technology platforms using Big Data open-source technologies such as Hadoop, HBase, Spark, Kafka and Elastic Search / Solr At least 3 years’ experience in large scale, production, server-side development on the JVM, Scala, Python and/or GO 3+ years of experience with SQL and relational database systems like Oracle, PostgreSQL or MySQL is preferable Experience with building RESTful API’s is preferred.Experience with cloud native technologies is a plus.Hands on experience with engineering developer tools- GIT, Dockerization, Continuous Integration frameworks etc. Passionate about Software Quality and ensuring that Secure Development Lifecycle best practices are followed Ability to adapt to new development environments, changing business requirements and learning new systems highly desired. Good team player, strong communication skills and able to effectively work across multiple teams on solutions that have complex dependencies and requirements in a fast-paced environment Experience working in agile software development model preferred.Experience with Tableau or other visualization tools is a plus.Experience with open-source contribution is a plus', 'Pro-active response in identifying and troubleshooting integration or technical issues. ', 'At least 3 years’ experience in large scale, production, server-side development on the JVM, Scala, Python and/or GO ', 'Great verbal, communication and follow through skills.', 'Ability to prototype ideas and demonstrate pros and cons and make recommendations using demos and slides. ', 'Ability to adapt to new development environments, changing business requirements and learning new systems highly desired. ', 'Job Qualifications:', '3+ years of experience with SQL and relational database systems like Oracle, PostgreSQL or MySQL is preferable ', 'Contribute to innovation with a passion for delivering leading edge Big Data Lake solutions to our customers.', '5+ years of experience building large scalable and reliable enterprise technology platforms using Big Data open-source technologies such as Hadoop, HBase, Spark, Kafka and Elastic Search / Solr ', 'Job Description:', 'Nice-to-haves:', 'Master’s degree or equivalent in Computer Science or related field with a minimum of 5+ years of hands-on software engineering experience. ', 'Hands on experience with engineering developer tools- GIT, Dockerization, Continuous Integration frameworks etc. ', 'Prior experience building/operating large-scale distributed systems and services ', '\u202fAs a member of the team, lead a component design/development, work with the development and big data teams to design scalable big data platform framework and services. ', 'Collaborate with management to define and set standards for engineering rigor and help cultivate the culture in the team.', 'Contributions to open-source projects preferably in\u202fthe platform space ', 'Ability to use data to drive products and decisions. Be able to explain complex technical concepts to management, product managers, support, and other engineers.', 'Job Description Summary:', '\u202fAs a member of the team, lead a component design/development, work with the development and big data teams to design scalable big data platform framework and services.  Identify, design and build automated test frameworks to enable validation of the solution for consistency, accuracy and repeatability @scale and TTM.Ability to prototype ideas and demonstrate pros and cons and make recommendations using demos and slides. Ability to use data to drive products and decisions. Be able to explain complex technical concepts to management, product managers, support, and other engineers.Collaborate with management to define and set standards for engineering rigor and help cultivate the culture in the team.Contribute to innovation with a passion for delivering leading edge Big Data Lake solutions to our customers.Good team player, able to effectively work across multiple teams on solutions that have complex dependencies and requirements in a fast-paced environment.Ability to assess new technologies and make pragmatic choices that points towards our long-term vision.Pro-active response in identifying and troubleshooting integration or technical issues. Great verbal, communication and follow through skills.', 'Good team player, able to effectively work across multiple teams on solutions that have complex dependencies and requirements in a fast-paced environment.', 'Experience with cloud native technologies is a plus.', 'Passionate about Software Quality and ensuring that Secure Development Lifecycle best practices are followed ', 'Experience working in agile software development model preferred.', 'Experience with open-source contribution is a plus', 'Responsibilities Include:', 'Ability to assess new technologies and make pragmatic choices that points towards our long-term vision.', 'Experience with Tableau or other visualization tools is a plus.', 'Good team player, strong communication skills and able to effectively work across multiple teams on solutions that have complex dependencies and requirements in a fast-paced environment ', 'Experience with building RESTful API’s is preferred.', ' Identify, design and build automated test frameworks to enable validation of the solution for consistency, accuracy and repeatability @scale and TTM.', 'Prior experience building/operating large-scale distributed systems and services Contributions to open-source projects preferably in\u202fthe platform space Experience in handling petabyte scale data ']",Not Applicable,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Engineer,Bloomberg LP,"New York, NY",2 weeks ago,100 applicants,"['Grow our business: Approach each day knowing this role is mission-critical to the success of the firm. We will rely on your expertise, and our flat structure allows for you to make real impact, real quick', 'We""ll trust you to:', 'A BA/BS degree or higher in Computer Science, Mathematics, or relevant data technology field, or equivalent professional work experience in software development, data engineering, data science or information technology2+ years of Python programming and scripting in a production environment2+ years of experience working with restful APIs and data modeling within SQL and NoSQL databases  Deep understanding of large-scale, distributed systemsLegal authorization to work full-time in the United States without requiring visa sponsorship', 'Apply your coding skills: automate the influx of data and build flexible solutions for data acquisition, ETL and machine learning pipelines, and human-in-the-loop data processing to drive successful product adoptionInspire and impact our business: act as an internal consultant to influence and implement more efficient products through analysis, dashboards, web apps, and user documentationDevelop your career: sharpen your technical skills and strengthen relationships through project management, partnering with stakeholders across the firm, and establishing scalable architectureChampion improvements: identify strategic technical gaps in our ecosystem and advocate for solutions over workaroundsGrow our business: Approach each day knowing this role is mission-critical to the success of the firm. We will rely on your expertise, and our flat structure allows for you to make real impact, real quick', '2+ years of experience working with restful APIs and data modeling within SQL and NoSQL databases  ', 'You""ll need to have:', 'A BA/BS degree or higher in Computer Science, Mathematics, or relevant data technology field, or equivalent professional work experience in software development, data engineering, data science or information technology', 'Inspire and impact our business: act as an internal consultant to influence and implement more efficient products through analysis, dashboards, web apps, and user documentation', 'Apply your coding skills: automate the influx of data and build flexible solutions for data acquisition, ETL and machine learning pipelines, and human-in-the-loop data processing to drive successful product adoption', '2+ years of Python programming and scripting in a production environment', 'Legal authorization to work full-time in the United States without requiring visa sponsorship', 'Deep understanding of large-scale, distributed systems', 'Does this sound like you?', 'Develop your career: sharpen your technical skills and strengthen relationships through project management, partnering with stakeholders across the firm, and establishing scalable architecture', 'Champion improvements: identify strategic technical gaps in our ecosystem and advocate for solutions over workarounds']",Not Applicable,Full-time,Information Technology,Financial Services,2021-03-18 14:34:51
Data Engineer,"NBCUniversal Media, LLC","Universal City, CA",7 days ago,Be among the first 25 applicants,"['', 'Country', ' Media-focused – Strong knowledge/passion for media including broadcast TV, digital, and film Direct experience working with sources like Nielsen, Adobe Analytics, comScore, and other media/entertainment industry datasets a plus ', "" Action-oriented – You're constantly figuring out new problems and are regularly showing results with a positive attitude, always displaying ethical behavior, integrity, and building trust Strong understanding of Agile principles and best practices  You’ve dealt with ambiguity and can make quality decisions in a dynamic, fast-paced environment "", 'Notices', 'About Us', "" 5+ years of experience in a data engineering role  Direct experience with data modeling, ETL/ELT development principles, and data warehousing concepts  Knowledge of data management fundamentals and data storage principles  Experience in building data pipelines using Python/SQL or similar programming languages  Demonstratable experience in Airflow, Luigi or similar orchestration engines  General understanding of cloud data engineering design patterns and use cases  Bachelor's degree in Computer Science, Data Science, Statistics, Informatics, Information Systems or related field. "", ' Understanding of big data technology stacks (Hive / Spark etc) is a plus ', ' Strong understanding of Agile principles and best practices  You’ve dealt with ambiguity and can make quality decisions in a dynamic, fast-paced environment ', ' Build and manage relationships with supporting IT teams in order to effectively deliver work products to production ', ' General understanding of cloud data engineering design patterns and use cases ', 'Career Level', ' 5+ years of experience in a data engineering role ', ' Direct experience with data modeling, ETL/ELT development principles, and data warehousing concepts ', 'Sub-Business', 'Responsibilities', ' Experience with Snowflake, Amazon Web Services, or related cloud platforms a plus  Understanding of big data technology stacks (Hive / Spark etc) is a plus ', ' Participate in development sprints, demos, and retrospectives, as well as release and deployment ', 'City', ' Design, build, and scale data pipelines across a variety of source systems and streams (internal, third-party, as well as cloud-based), distributed/elastic environments, and downstream applications and/or self-service solutions ', ' Collaborate with business leaders, engineers, and product managers to understand data needs.  Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using cloud-native data engineering principles  Design, build, and scale data pipelines across a variety of source systems and streams (internal, third-party, as well as cloud-based), distributed/elastic environments, and downstream applications and/or self-service solutions  Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.  Implement the appropriate design patterns while optimizing performance, cost, security, and scale and end user experience  Participate in development sprints, demos, and retrospectives, as well as release and deployment  Build and manage relationships with supporting IT teams in order to effectively deliver work products to production ', ' Demonstratable experience in Airflow, Luigi or similar orchestration engines ', "" Analytical – You have experience in delivering data analytics solutions that promote data discovery Experience with Snowflake, Amazon Web Services, or related cloud platforms a plus  Understanding of big data technology stacks (Hive / Spark etc) is a plus  Media-focused – Strong knowledge/passion for media including broadcast TV, digital, and film Direct experience working with sources like Nielsen, Adobe Analytics, comScore, and other media/entertainment industry datasets a plus  Communicator – You have excellent verbal and written skills with the ability to communicate ideas effectively across all levels of the organization, both technical and non-technical  Action-oriented – You're constantly figuring out new problems and are regularly showing results with a positive attitude, always displaying ethical behavior, integrity, and building trust Strong understanding of Agile principles and best practices  You’ve dealt with ambiguity and can make quality decisions in a dynamic, fast-paced environment "", 'Qualifications', ' Analytical – You have experience in delivering data analytics solutions that promote data discovery Experience with Snowflake, Amazon Web Services, or related cloud platforms a plus  Understanding of big data technology stacks (Hive / Spark etc) is a plus ', ' Communicator – You have excellent verbal and written skills with the ability to communicate ideas effectively across all levels of the organization, both technical and non-technical ', ' Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. ', ' Desired Characteristics: ', ' Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using cloud-native data engineering principles ', ' Experience with Snowflake, Amazon Web Services, or related cloud platforms a plus ', ' Knowledge of data management fundamentals and data storage principles ', ' Experience in building data pipelines using Python/SQL or similar programming languages ', ' Implement the appropriate design patterns while optimizing performance, cost, security, and scale and end user experience ', ' You’ve dealt with ambiguity and can make quality decisions in a dynamic, fast-paced environment ', 'State/Province', 'Qualifications/Requirements', 'Multiple Locations', "" Bachelor's degree in Computer Science, Data Science, Statistics, Informatics, Information Systems or related field. "", ' Direct experience working with sources like Nielsen, Adobe Analytics, comScore, and other media/entertainment industry datasets a plus ', ' Strong understanding of Agile principles and best practices ', ' Collaborate with business leaders, engineers, and product managers to understand data needs. ']",Not Applicable,Full-time,Information Technology,Broadcast Media,2021-03-18 14:34:51
Data Engineer,neteffects,"Charlotte, NC",2 days ago,88 applicants,"['', ' data Analysis.', 'No C2C', ' ETL tools and automation', '\ufeff', 'Python', 'Healthcare experience preferred.', 'Required skills:', ' Data Engineer ', '\ufeffAt Neteffects, we are looking for a Data Engineer for our Direct client. The position is remote for now.', 'Excellent communication skills both written and oral and documentation skills.', 'SQL (', 'Hands-on experience with ETL tools and automation and perform complex data Analysis.', 'SQL (Oracle, MSSQL, MySQL, etc.) and NoSQL (Mongo)', 'Strong understanding of data modeling, algorithms, and data transformation techniques', '3+ years of experience in Computer Engineering, Software Development', 'NoSQL (Mongo', 'SQL (Oracle, MSSQL, MySQL, etc.) and NoSQL (Mongo)Scripting languages such as PythonStrong understanding of data modeling, algorithms, and data transformation techniques3+ years of experience in Computer Engineering, Software DevelopmentHands-on experience with ETL tools and automation and perform complex data Analysis.Healthcare experience preferred.Excellent communication skills both written and oral and documentation skills.', 'Scripting languages such as Python']",Mid-Senior level,Contract,Information Technology,Staffing and Recruiting,2021-03-18 14:34:51
Data Integration Engineer,Peloton Interactive,"Santa Clara, CA",19 hours ago,Be among the first 25 applicants,"['', 'You are a proactive problem-solver, even in areas of uncertainty and ambiguity.', 'supply chain systems data integrations', 'Strong verbal and written communication skills', 'Communicate and collaborate effectively with technical peers and business users.', 'Provide guidance on the monitoring, troubleshooting, and resolution of data quality issues in collaboration with the appropriate stakeholders; and', ' Integration Engineer', 'Ability to work well under pressure; manage tight deadlines and situations where conflicting priorities arise.', 'Hands-on experience on Integration Platforms such as Boomi, MuleSoft; AnyPoint, Pentaho, and Informatica', 'Experience with performance tuning optimization within Boomi', 'Own the Boomi development process from requirements gathering to full implementation.', 'Minimum of 3 years experience working with integration platforms (Boomi, MuleSoft etc.).', 'Dell Boomi ', 'Job Responsibilities', 'Document and analyze current business processes and underlying systems/applications.', 'Functional experience with ERP systems (i.e., NetSuite, SAP)', 'Proven ability to implement at least 1 to 2 full integration projects involving on-premise and Cloud applications.', 'Monitor, troubleshoot, and resolve problems with integrations.', 'Basic Job Requirements', ' Bachelor’s degree Strong verbal and written communication skills Strong analytical and critical thinking skills Adapt and proactive at problem-solving and conflict resolution. Minimum of 3 years experience working with integration platforms (Boomi, MuleSoft etc.). Proven ability to implement at least 1 to 2 full integration projects involving on-premise and Cloud applications. Functional experience with ERP systems (i.e., NetSuite, SAP) Experience with REST and SOAP web services SQL language proficiency with at least one of the common database platforms such as SQL Server, Oracle, MySQL, and/or DB2 database management systems Ability to work well under pressure; manage tight deadlines and situations where conflicting priorities arise. ', 'SQL language proficiency with at least one of the common database platforms such as SQL Server, Oracle, MySQL, and/or DB2 database management systems', ' You have the ability to analyze business problems and develop both long-term sustainable solutions and also quick tactical solutions. You can articulate complex concepts in a way that is understandable to non-technical stakeholders. You have excellent analytical and critical reasoning skills. You are a proactive problem-solver, even in areas of uncertainty and ambiguity. You possess strong collaboration skills and approach problems with positive intent while driving towards resolution. ', 'Strong understanding of integration architecture options such as SoA and APIs', 'You possess strong collaboration skills and approach problems with positive intent while driving towards resolution.', 'Who You Are', 'You have the ability to analyze business problems and develop both long-term sustainable solutions and also quick tactical solutions.', 'Adapt and proactive at problem-solving and conflict resolution.', 'Lead the research, development & implementation of special projects, as needed.', 'Preferred Experience', 'About Peloton', 'You have excellent analytical and critical reasoning skills.', 'Collaborate with the development team to architect efficient and stable integrations.', ' Boomi Developer/Architect certified Experience developing applications that utilize Boomi Integration Strong understanding of integration architecture options such as SoA and APIs Experience with performance tuning optimization within Boomi Functional experience with ERP systems (i.e., NetSuite, SAP) Hands-on experience on Integration Platforms such as Boomi, MuleSoft; AnyPoint, Pentaho, and Informatica ', 'Experience with REST and SOAP web services', 'Boomi Developer/Architect certified', 'Bachelor’s degree', ' Own the Boomi development process from requirements gathering to full implementation. Monitor, troubleshoot, and resolve problems with integrations. Collaborate with the development team to architect efficient and stable integrations. Document and analyze current business processes and underlying systems/applications. Lead the research, development & implementation of special projects, as needed. Provide guidance on the monitoring, troubleshooting, and resolution of data quality issues in collaboration with the appropriate stakeholders; and Communicate and collaborate effectively with technical peers and business users. ', 'Experience developing applications that utilize Boomi Integration', 'You can articulate complex concepts in a way that is understandable to non-technical stakeholders.', 'Strong analytical and critical thinking skills']",Entry level,Full-time,Information Technology,"Health, Wellness and Fitness",2021-03-18 14:34:51
Data Engineer,Dice,"McLean, VA",19 hours ago,Be among the first 25 applicants,[''],Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
"Data Engineer, Analytics",Facebook,"New York, NY",2 days ago,Be among the first 25 applicants,"['', 'Experience working with either a MapReduce or an MPP system.', '5+ years experience with object-oriented programming languages.', 'Collaborate with engineers, product managers, and data scientists to understand data needs, representing key data insights visually in a meaningful way.', 'Create and contribute to frameworks that improve the efficacy of logging data, while working with data infrastructure to triage issues and resolve.', '5+ years experience in the data warehouse space.', 'Experience analyzing data to identify gaps and inconsistencies.', 'Mentor team members by giving/receiving actionable feedback.', 'Knowledge and practical application of Python.', 'Experience working autonomously in global teams.', '5+ years experience with schema design and dimensional data modeling.', 'Responsibilities', 'Experience managing and communicating data warehouse plans to internal clients.', 'Conceptualize and own the data architecture for multiple large-scale projects, while evaluating design and operational cost-benefit tradeoffs within systems.Create and contribute to frameworks that improve the efficacy of logging data, while working with data infrastructure to triage issues and resolve.Collaborate with engineers, product managers, and data scientists to understand data needs, representing key data insights visually in a meaningful way.Define and manage SLA for all data sets in allocated areas of ownership.Determine and implement the security model based on privacy requirements, confirm safeguards are followed, address data quality issues, and evolve governance processes within allocated areas of ownership.Design, build, and launch collections of sophisticated data models and visualizations that support multiple use cases across different products or domains.Solve our most challenging data integration problems, utilizing optimal ETL patterns, frameworks, query techniques, sourcing from structured and unstructured data sources.Assist in owning existing processes running in production, optimizing complex code through advanced algorithmic concepts.Optimize pipelines, dashboards, frameworks, and systems to facilitate easier development of data artifacts.Influence product and cross-functional teams to identify data opportunities to drive impact.Mentor team members by giving/receiving actionable feedback.', '5+ years experience in custom ETL design, implementation and maintenance.', 'Influence product and cross-functional teams to identify data opportunities to drive impact.', 'Minimum Qualification', 'BS/BA in Technical Field, Computer Science or Mathematics.', 'Define and manage SLA for all data sets in allocated areas of ownership.', 'Conceptualize and own the data architecture for multiple large-scale projects, while evaluating design and operational cost-benefit tradeoffs within systems.', 'Preferred Qualification', 'Experience influencing product decisions with data.', 'Design, build, and launch collections of sophisticated data models and visualizations that support multiple use cases across different products or domains.', 'Determine and implement the security model based on privacy requirements, confirm safeguards are followed, address data quality issues, and evolve governance processes within allocated areas of ownership.', 'BS/BA in Technical Field, Computer Science or Mathematics.Experience working with either a MapReduce or an MPP system.Knowledge and practical application of Python.Experience working autonomously in global teams.Experience influencing product decisions with data.', 'Optimize pipelines, dashboards, frameworks, and systems to facilitate easier development of data artifacts.', '5+ years experience in writing SQL statements.', '5+ years experience in the data warehouse space.5+ years experience in custom ETL design, implementation and maintenance.5+ years experience with object-oriented programming languages.5+ years experience with schema design and dimensional data modeling.5+ years experience in writing SQL statements.Experience analyzing data to identify gaps and inconsistencies.Experience managing and communicating data warehouse plans to internal clients.', 'Assist in owning existing processes running in production, optimizing complex code through advanced algorithmic concepts.', 'Solve our most challenging data integration problems, utilizing optimal ETL patterns, frameworks, query techniques, sourcing from structured and unstructured data sources.']",Not Applicable,Full-time,Information Technology,Internet,2021-03-18 14:34:51
Data Engineer - Intelligent Forecasting,DICK'S Sporting Goods,"Coraopolis, PA",2 days ago,Be among the first 25 applicants,"['', '3+ years of experience being close to the business and delivering value through it as part of a team.', 'Experience with message queuing, stream processing, and highly scalable ‘big data’ data stores (Kafka, Pub/Sub)', 'Work with stakeholders including the product, data and architecture teams to assist with data-related technical issues and support their data infrastructure needs.', 'At Dicks Sporting Goods, we are creating the future of sport driven by powerful data products and platforms that serve our Athletes and Teammates.', 'Experience with relational databases (Oracle, SQL Server, etc..) as well as NoSQL database technologies (MongoDB, BigTable, Cassandra, etc..)', ""Bachelor's Degree in\xa0Computer Science, Software Engineering, Information Systems or Information Technology or related field required, or equivalent experience"", ""Bachelor's Degree in\xa0Computer Science, Software Engineering, Information Systems or Information Technology or related field required, or equivalent experienceThree to five years of experience in\xa0Data Engineering, AI/ML Engineer Integration, Data ModelingAny Public Cloud certification focused on Data Engineering or Data ScienceAny Public Cloud certification focused on Cloud EngineeringExperience building data pipelines on modern public cloud services like Snowflake, AWS, GCP, or Azure.Experience with message queuing, stream processing, and highly scalable ‘big data’ data stores (Kafka, Pub/Sub)Proficient with SQL, Spark, and other common Query languagesExperience with Cloud Identity and Access Management for data on public cloud.Proficient with object-oriented programming and scripting languages (Python, Java, etc..)Experience with continuous integration/continuous delivery (CI/CD) pipelines (Jenkins, Concourse, Azure DevOps)Experience with relational databases (Oracle, SQL Server, etc..) as well as NoSQL database technologies (MongoDB, BigTable, Cassandra, etc..)Experience with AI/ML workflow frameworks like Kubeflow, Azure AI/ML, AWS Sagemaker, etc.Proficient in developing, maintaining and interacting with APIsExperience with Agile Development and Agile Deployment tools and versioning using Git or similar tools\xa0Proficient in Linux/Unix environments"", 'Hands-on experience building, managing, and automating data pipelines', 'Experience with Cloud Identity and Access Management for data on public cloud.', 'Work as part of a team building the data ingestion, products, pipelines, and tooling supporting our data products driving merchandising, supply-chain, pricing, and product development initiaves.', 'Proficient in Linux/Unix environments', 'Any Public Cloud certification focused on Cloud Engineering', 'You understand the importance of Kafka, Snowpipe, or Kinesis for real-time needs', 'Provide proactive design, operational support, and governance for privacy and security policy for data.', 'Experience and love for Python, Spark, SQL, or other standard data scripting languagesHands-on experience building, managing, and automating data pipelinesFamiliarity and appreciation for modern public cloud data services from AWS, GCP, or Azure3+ years of experience being close to the business and delivering value through it as part of a team.Some knowledge or exposure to supporting AI/ML engineering and integrating data to model development, management and serving.A good grip of data structures, relationships, integration patterns, and algorithms.You understand the importance of Kafka, Snowpipe, or Kinesis for real-time needsSome experience applying security and privacy to how you manage data.A grasp of the importance of common data platform patterns and how they relate like Data Lake, Data Mesh, Data Catalog, Tagging, Stream Processing, etc.', 'We are looking for a Data Engineer who wants to be a part of a team solving the ingestion, enrichment, and activation of data to drive better development, forecasting, allocation, and insights into the products we sell. In this role, you will work with the core of how we use inventory, transaction, and historical forecast data working with modern cloud and open source technology to transform the future of sport.', 'Qualifications', 'Experience and love for Python, Spark, SQL, or other standard data scripting languages', 'Any Public Cloud certification focused on Data Engineering or Data Science', 'A good grip of data structures, relationships, integration patterns, and algorithms.', 'Proficient with object-oriented programming and scripting languages (Python, Java, etc..)', 'Experience with AI/ML workflow frameworks like Kubeflow, Azure AI/ML, AWS Sagemaker, etc.', 'Proficient in developing, maintaining and interacting with APIs', 'Experience with Agile Development and Agile Deployment tools and versioning using Git or similar tools\xa0', 'A grasp of the importance of common data platform patterns and how they relate like Data Lake, Data Mesh, Data Catalog, Tagging, Stream Processing, etc.', 'Familiarity and appreciation for modern public cloud data services from AWS, GCP, or Azure', 'You would partner with merchandising, supply-chain, pricing, and product development business teams to build the next-generation of data insights. Work alongside data science teams building AI/ML models, platform engineers creating services that support you, and software developers creating unique experiences.', 'Some knowledge or exposure to supporting AI/ML engineering and integrating data to model development, management and serving.', 'Some experience applying security and privacy to how you manage data.', 'We are open to this role being remote.\xa0', 'Experience building data pipelines on modern public cloud services like Snowflake, AWS, GCP, or Azure.', 'Proficient with SQL, Spark, and other common Query languages', 'Three to five years of experience in\xa0Data Engineering, AI/ML Engineer Integration, Data Modeling', 'Job Duties & Responsibilities', 'Experience with continuous integration/continuous delivery (CI/CD) pipelines (Jenkins, Concourse, Azure DevOps)', 'What you will bring:', 'This role builds new data products, pipelines, APIs, materialized views, services, and tools. They are fascinated with data and the modern ways of creating rich, scalable data products driving how we work.', 'Work as part of a team building the data ingestion, products, pipelines, and tooling supporting our data products driving merchandising, supply-chain, pricing, and product development initiaves.Work with stakeholders including the product, data and architecture teams to assist with data-related technical issues and support their data infrastructure needs.Provide proactive design, operational support, and governance for privacy and security policy for data.']",Associate,Full-time,Information Technology,Retail,2021-03-18 14:34:51
Data Engineer (100% remote!),Optello,"St Louis, MO",4 days ago,51 applicants,"['', ' Collaborating with distributed team of engineers and product owners', 'Optello is proud to be an Equal Opportunity Employer', ' Azure App Functions', ' Apache PIG', ' Relational databases', ' Prior experience in a similar role, ideally within the health tech space but one where analytics was the focal point of the technology Experience with the following languages and technologies: Relational databases SQL ETL (Azure Data Factory / SSIS, Databricks) Azure App Functions Azure Data lake Spark Apache PIG', ' Enhancing the data pipeline and platform using architectural and design patterns such as data lake', ' Practicing general software engineering principles (SOLID principles), best practices (code review), engineering patterns (e.g. design patterns), and Test Driven Development', ' Enhancing the data pipeline and platform using architectural and design patterns such as data lake Implementing scalable data services using serverless Azure resources such as Data Factory, Synapse, Databricks, Azure Functions and traditional SQL Collaborating with data scientists to implement AI and machine learning solutions, and DevOps to automate workflows and processes Contributing to building new predictive and prescriptive analytics to compliment existing descriptive and diagnostic analytics Contributing to product development efforts with goals of reliable delivery, high quality (eg. bug free), technical excellence and continuous delivery Implementing technical design for feature enhancements using appropriate design patterns, data and object models Collaborating with distributed team of engineers and product owners Practicing general software engineering principles (SOLID principles), best practices (code review), engineering patterns (e.g. design patterns), and Test Driven Development', ' ETL (Azure Data Factory / SSIS, Databricks)', 'Your Right to Work', ' Contributing to building new predictive and prescriptive analytics to compliment existing descriptive and diagnostic analytics', ' Competitive compensation + benefits Generous annual bonus structure', ' SQL', ' Generous annual bonus structure', 'Email Your Resume In Word To', ' Competitive compensation + benefits', ' Experience with the following languages and technologies:', ' Azure Data lake', ' Spark', ' Implementing scalable data services using serverless Azure resources such as Data Factory, Synapse, Databricks, Azure Functions and traditional SQL', ' Collaborating with data scientists to implement AI and machine learning solutions, and DevOps to automate workflows and processes', 'Please do NOT change the email subject line in any way. You must keep the JobID: linkedin : JG9-1621153 -- in the email subject line for your application to be considered.***', ' Implementing technical design for feature enhancements using appropriate design patterns, data and object models', ' Contributing to product development efforts with goals of reliable delivery, high quality (eg. bug free), technical excellence and continuous delivery', ' Prior experience in a similar role, ideally within the health tech space but one where analytics was the focal point of the technology']",Entry level,Full-time,Information Technology,Construction,2021-03-18 14:34:51
Software Engineer - Data,Twitch,"San Francisco, CA",2 weeks ago,79 applicants,"['', '  3+ years of software development experience .  2+ years experience in Go or Python. 1+ years working with distributed, highly available systems. ', ' Medical, Dental, Vision & Disability Insurance  401(k) , Maternity & Parental Leave  Flexible PTO  Commuter Benefits  Amazon Employee Discount Monthly Contribution & Discounts for Wellness Related Activities & Programs (e.g., gym memberships, off-site massages), Breakfast, Lunch & Dinner Served Daily Free Snacks & Beverages  ', "" You've made petabytes of data usable. You've worked with Amazon Web Services (AWS). You contribute to open source. "", 'Bonus Points', 'You contribute to open source.', ' 2+ years experience in Go or Python.', 'About Us', 'Free Snacks & Beverages ', ""You've worked with Amazon Web Services (AWS)."", 'Medical, Dental, Vision & Disability Insurance', 'About The Role', ' Develop new capabilities in our data warehouses and pipelines. Improve the reliability, flexibility , and scalability of our existing tools. Collaborate on our vision as we scale to our next petabyte. ', 'Flexible PTO', 'Monthly Contribution & Discounts for Wellness Related Activities & Programs (e.g., gym memberships, off-site massages),', 'Develop new capabilities in our data warehouses and pipelines.', 'Breakfast, Lunch & Dinner Served Daily', 'Perks', ""You've made petabytes of data usable."", 'You Will', 'Amazon Employee Discount', ' 401(k) , Maternity & Parental Leave ', ' 3+ years of software development experience .', 'Collaborate on our vision as we scale to our next petabyte.', 'You Have:', ' Commuter Benefits ', '1+ years working with distributed, highly available systems.', 'Improve the reliability, flexibility , and scalability of our existing tools.']",Not Applicable,Full-time,Engineering,Internet,2021-03-18 14:34:51
Senior Data Engineer,Fidelity Investments,"Durham, NC",2 days ago,Be among the first 25 applicants,"['', 'Sophisticated experience with PL/SQL and complex queries, views, packages etc.', 'Experience with cloud native data warehousing and data lake solutions using Redshift, Snowflake, etc.', 'Your experience in executing projects in an Agile environment.', 'Understanding of Cloud Computing and DevOps concepts including CI/CD pipelines using Git and Jenkins', 'Your sophisticated skills in data intensive application development, data integration, and data pipeline design patterns on a distributed platform.', 'Your ability to collaborate with other technical and business specialists in the team', 'Bachelor’s Degree or equivalent in a technology related field (e.g. Computer Science, Engineering, etc.) required.', 'Experience with modern Object-Oriented Programming Languages like Java, Scala, Python would be a huge plus', 'The Team', 'Basic understanding of NoSQL and BigData technologies such as e.g. Hadoop, HBase, MongoDB, Cassandra, etc.', 'Bachelor’s Degree or equivalent in a technology related field (e.g. Computer Science, Engineering, etc.) required.Strong proven understanding of Relational Databases like Oracle or PostgresSophisticated experience with PL/SQL and complex queries, views, packages etc.6-8 years of proven experience working with ETL data integration and data movement design patterns using InformaticaExperience with modern Object-Oriented Programming Languages like Java, Scala, Python would be a huge plusYour sophisticated skills in data intensive application development, data integration, and data pipeline design patterns on a distributed platform.Knowledge of batch job scheduling and dependency management using BMC Control-M for distributed systemsUnderstanding of Cloud Computing and DevOps concepts including CI/CD pipelines using Git and JenkinsBasic understanding of NoSQL and BigData technologies such as e.g. Hadoop, HBase, MongoDB, Cassandra, etc.Experience with cloud native data warehousing and data lake solutions using Redshift, Snowflake, etc.Your experience in executing projects in an Agile environment.Your ability to collaborate with other technical and business specialists in the teamYour ability to learn and experiment with new technologies and patternsYour passion to follow modern test driven and automation driven software development methodologies', 'Strong proven understanding of Relational Databases like Oracle or Postgres', 'The Role', 'The Expertise And Skills You Bring', 'Your ability to learn and experiment with new technologies and patterns', 'Your passion to follow modern test driven and automation driven software development methodologies', '6-8 years of proven experience working with ETL data integration and data movement design patterns using Informatica', 'Job Description', 'Knowledge of batch job scheduling and dependency management using BMC Control-M for distributed systems', 'Certifications']",Mid-Senior level,Full-time,Quality Assurance,Financial Services,2021-03-18 14:34:51
Data Engineer - Data Services,American Express,"Phoenix, AZ",6 days ago,114 applicants,"['', 'Build a strong infrastructure and a solid career.', 'Millions of customers depend on our databases.', ' Agile Practices Emerging Technologies Programming Languages and Frameworks Programming/Software Development System/Platform Domain Knowledge ', 'Programming/Software Development', 'Support initiatives that deliver workable end-to-end database infrastructure solutions', 'Here’s Just Some Of What You’ll Do', 'Develop automation tools to improve the time to market and time to repair systems', 'At the core of Infrastructure Engineering', 'Contribute to the analysis of business, application, and technical infrastructure requirements.', 'System/Platform Domain Knowledge', 'Design, build, enhance and integrate the infrastructure required to support various database platforms and our business portfolio', 'Agile Practices', 'Are you up for the challenge?', 'Minimum Qualifications', ' Develop automation tools to improve the time to market and time to repair systems Contribute to the analysis of business, application, and technical infrastructure requirements. Design, build, enhance and integrate the infrastructure required to support various database platforms and our business portfolio Support initiatives that deliver workable end-to-end database infrastructure solutions ', 'Emerging Technologies', 'Programming Languages and Frameworks']",Associate,Full-time,Information Technology,Financial Services,2021-03-18 14:34:51
Data Engineer,N/A,"Norfolk, VA",2 days ago,Be among the first 25 applicants,"['', ' ', 'Bachelor’s degree in Computer Science, Engineering, or another technical field 5+ years of industry experience in database engineering Must meet DoD 8570.01 Certification Requirements for IAT Level II CompTIA Security+ CE certification or equivalent (required) MCSA: Windows Server 2016 certification or similar (able to obtain within a month) Team player with experience working in an agile development culture Highly organized with excellent problem-solving skills Strong written and verbal communication skills Expert level proficiency with SQL and its variation among popular databases Experience with some of the modern relational databases Secret clearance must possess and maintain at or above this level ', 'ProModel Government Solutions is an Equal Opportunity Employer/Veterans/Disabled', 'Work with other engineers to keep our systems up-to-date and conduct regular system diagnostics and security checks ', 'Required: ', 'Potential Work Locations Hampton Roads, VA (Norfolk, Suffolk, VA Beach) Travel Required N/A Work Schedule (Shift etc.): Day Shift Authorization to work in US required ', 'Team player with experience working in an agile development culture ', 'Bachelor’s degree in Computer Science, Engineering, or another technical field ', 'Preferred: ', 'Our decision support platform is enabled by leading-edge technology and leverages advanced modeling/optimization capabilities join us as we build the future of BI, analytics, AI/ML, and data insights for the DoD. Our users need a rich, immersive experience that helps them effortlessly gain insights into complex problems, rapidly run scenarios and make sense of the immense amounts of data being generated across the DoD. As a database engineer, you’ll perform a critical role in configuring, updating, and architecting the platform backend that powers our decision support capability. ', 'Handle common database procedures such as upgrade, backup, recovery, migration, etc. ', 'Travel Required N/A ', 'Required:', 'Manage the Extract, Transform, Load (ETL) processes ', 'Responsibilities:', 'Position Details ', 'Preferred:', 'Work in close unison with various members of the DevOps, Development, and QA teams to deploy new software releases, configure websites, update ETL engines, monitor and troubleshoot web service connections, and respond quickly to on-demand software releases and data requests to meet our customers rapidly evolving real-world needs ', 'Develop database solutions to implement the defined architecture ', 'CompTIA Security+ CE certification or equivalent (required) ', 'Highly organized with excellent problem-solving skills ', 'Qualifications:', 'Experience with some of the modern relational databases ', 'Position Details', 'About ProModel Government Solutions, a BigBear.ai company \xa0 ', 'Expert level proficiency with SQL and its variation among popular databases ', 'Strong written and verbal communication skills ', 'Qualifications: ', 'Secret clearance must possess and maintain at or above this level ', 'Must meet DoD 8570.01 Certification Requirements for IAT Level II ', 'BigBear.ai is seeking a Data Engineer\xa0to support the next generation of application platforms for our customers. This position will support ProModel Government Solutions, a BigBear.ai company. ', 'Top Secret clearance ', 'Responsibilities: ', 'Authorization to work in US required ', '5+ years of industry experience in database engineering ', 'Support an engineering team in the maintenance, configuration, and transformation of our database servers Work with other engineers to keep our systems up-to-date and conduct regular system diagnostics and security checks Handle common database procedures such as upgrade, backup, recovery, migration, etc. Manage the Extract, Transform, Load (ETL) processes Define models and standards by which data is managed and stored Develop database solutions to implement the defined architecture Work in close unison with various members of the DevOps, Development, and QA teams to deploy new software releases, configure websites, update ETL engines, monitor and troubleshoot web service connections, and respond quickly to on-demand software releases and data requests to meet our customers rapidly evolving real-world needs ', 'Support an engineering team in the maintenance, configuration, and transformation of our database servers ', 'MCSA: Windows Server 2016 certification or similar (able to obtain within a month) ', 'Familiarity with tools that can aid with profiling server resource usage and optimization ', 'Working knowledge of virtualization (VMWare, Cohesity) Familiarity with tools that can aid with profiling server resource usage and optimization Top Secret clearance ', 'Data Engineer', 'About ProModel Government Solutions, a BigBear.ai company \xa0', '\xa0\xa0 ', 'ProModel Government Solutions, a BigBear.ai company, is an agile provider of mission-critical predictive and prescriptive analytic software solutions for decision support to the Department of Defense and U.S. Government. For more than 25 years, ProModel Government Solutions has built innovative and adaptable custom model-based software solutions to visualize complex and disparate data, synchronize operational needs, mitigate risk and optimize resources to support strategic and tactical decisions for the Department of Defense and other Federal Government agencies. ', 'Define models and standards by which data is managed and stored ', 'Potential Work Locations Hampton Roads, VA (Norfolk, Suffolk, VA Beach) ', 'Work Schedule (Shift etc.): Day Shift ', 'Working knowledge of virtualization (VMWare, Cohesity) ', 'You’ll be a part of an agile team composed of talented developers, testers, DBAs, other server administrators, technical writers, and business analysts. Your skills will make us a better team, and you will love what you will gain from being among technical, talented, and driven individuals. ']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Remote SQL Data Engineer,Apex Systems,"Atlanta, GA",2 days ago,91 applicants,"['', 'Talend/Python (tools they use, more the better) – substitute (Datastage, Informatica would suffice)', 'Job Overview:', 'Apex Systems, the 2nd largest IT staffing firm, is looking to hire a Remote SQL Data Engineer for one of our clients in the Healthcare Industry! This assignment is anticipated to be a 6-12 month contract to hire; however, Apex Systems cannot guarantee the length of this assignment. This will also have a first shift Monday-Friday schedule. If you are interested, please apply and we will reach out to you right away!', 'Advanced SQL Skills – ELT (Database driven coding) Snowflake (DW Platform) Great at ETL, Python Talend/Python (tools they use, more the better) – substitute (Datastage, Informatica would suffice)', 'Advanced SQL Skills – ELT (Database driven coding) ', 'Primary focus will be on the data migration (on-prem Netezza and Datastage) once the migration is complete in 6 months, they will be building out Data marts. Position is for anyone looking to expand the skillset. The client has a research wing utilizing cloud platform. The hospital systems being revamped, Data pipelines for IoT, digital patient workflows expanding telehealth. There are a lot of opportunities to work with technology that’s going to be changing the landscape. Especially in the cloud area. Just starting data science team too and will be areas of growth into that space. ', 'Snowflake (DW Platform) Great at ETL, Python ', 'Primary focus will be on the data migration (on-prem Netezza and Datastage) once the migration is complete in 6 months, they will be building out Data marts. ', 'Position is for anyone looking to expand the skillset. The client has a research wing utilizing cloud platform. The hospital systems being revamped, Data pipelines for IoT, digital patient workflows expanding telehealth. There are a lot of opportunities to work with technology that’s going to be changing the landscape. Especially in the cloud area. Just starting data science team too and will be areas of growth into that space. ', 'Required Technologies:']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Backend/Data Engineer - Personalization (Remote Eligible - Americas),Spotify,"Boston, MA",3 weeks ago,73 applicants,"['', 'We are a distributed workforce enabling our band members to find a work mode that is best for them!', 'You are experienced in building distributed, high-volume Java backend services. In addition to Java, you have worked in production with some of the following (or an equivalent): Kubernetes, Cloud Bigtable, Cloud SQL, Postgres, Memcached, ElasticSearch.You know how to work with high volume heterogeneous data, and have production-level experience with one or more higher-level JVM-based data processing frameworks such as Beam, Dataflow, Crunch, Scalding, Storm, Spark, etc. (but not just Pig/Hive/BigQuery/other SQL-like abstractions).You are knowledgeable about data access and data storage techniques.Bonus if you have any level of experience with modern web development, particularly JavaScript coding, testing, debugging, and automation techniques, and/or TypeScript, React, Styled Components, and GraphQL.You have a quality-mindset and are interested in all parts of software development: coding, testing, deployment, monitoring. You demonstrate a growth mentality ー you welcome challenges, seek feedback from others, show perseverance on long-term goals, and generally go above and beyond in your work. A common theme throughout our teams is a genuine interest in music, so you might play an instrument and/or love listening to music whenever you get a chance!', 'Working hours? We operate within the Eastern Standard time zone for collaboration and ask that all be located that time zone. ', 'Be a strong participant in the Spotify-wide web developer community affecting and driving web engineering across the company.', 'Collaborate and pair program in a multi-functional team, where the focus is on backend services and batch data pipelines, but some web work will also be encouraged. We believe that multi-functional teams work better when everyone is T-shaped. Be a strong participant in the Spotify-wide web developer community affecting and driving web engineering across the company.Be a technical leader within your immediate team and within Spotify in general.Hack on new insights during regular hack days and the yearly Spotify-wide hack week!', 'You are experienced in building distributed, high-volume Java backend services. In addition to Java, you have worked in production with some of the following (or an equivalent): Kubernetes, Cloud Bigtable, Cloud SQL, Postgres, Memcached, ElasticSearch.', 'We are a distributed workforce enabling our band members to find a work mode that is best for them!Where in the world? For this role, it can be within the Americas region in which we have a work location and is within working hours. Working hours? We operate within the Eastern Standard time zone for collaboration and ask that all be located that time zone. Prefer an office to work from home instead? Not a problem! We have plenty of options for your working preferences. Find more information about our Work From Anywhere options here .', 'You demonstrate a growth mentality ー you welcome challenges, seek feedback from others, show perseverance on long-term goals, and generally go above and beyond in your work. ', 'Who You Are', 'Collaborate and pair program in a multi-functional team, where the focus is on backend services and batch data pipelines, but some web work will also be encouraged. We believe that multi-functional teams work better when everyone is T-shaped. ', 'Prefer an office to work from home instead? Not a problem! We have plenty of options for your working preferences. Find more information about our Work From Anywhere options here .', 'A common theme throughout our teams is a genuine interest in music, so you might play an instrument and/or love listening to music whenever you get a chance!', 'Be a technical leader within your immediate team and within Spotify in general.', 'Bonus if you have any level of experience with modern web development, particularly JavaScript coding, testing, debugging, and automation techniques, and/or TypeScript, React, Styled Components, and GraphQL.', ""What You'll Do"", 'You know how to work with high volume heterogeneous data, and have production-level experience with one or more higher-level JVM-based data processing frameworks such as Beam, Dataflow, Crunch, Scalding, Storm, Spark, etc. (but not just Pig/Hive/BigQuery/other SQL-like abstractions).', 'Where in the world? For this role, it can be within the Americas region in which we have a work location and is within working hours. ', ""Where You'll Be"", 'You are knowledgeable about data access and data storage techniques.', 'Hack on new insights during regular hack days and the yearly Spotify-wide hack week!', 'You have a quality-mindset and are interested in all parts of software development: coding, testing, deployment, monitoring. ']",Not Applicable,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Data Factory Data Engineer,Ford Motor Company,"Dearborn, MI",21 hours ago,Be among the first 25 applicants,"['', 'Two or more years of experience in a Data Management role including running queries and compiling data for analytics', 'Support the data requirements of the different functional teams like MS&S, PD, Quality, etc. and all the regional KPI / Metrics initiatives -Evaluate, explore and select the right data platform technologies including Big Data, RDBMS & NoSQL to meet the analytics requirements', 'Our Preferred Requirements', 'Strategic and clear thinking to translate discreet and complex ideas to business-driven results', 'Other Information', 'Serve as data subject matter expert and demonstrate an understanding of key data management principles and data use', 'Work with technology team to model data, create data flow diagrams, load, and transformation', 'Continuously increase Data Coverage by working closely with stakeholders and Data Scientists, understanding and evaluating their data requirements to create meaningful, organized and structured “information”', 'Ability to write complex SQL queries needed to query & analyze data', '5 years of experience in a Data Management role including running queries and compiling data for analytics', 'Strong team player, with the ability to collaborate well with others, to solve problems and actively incorporate input from various sources', 'Demonstrated experience building visualizations using Tableau/Qlikview', 'Drive the Data Strategy and align it with continuously changing business landscape', 'Work on all the data types across the enterprise including Customer, Dealer, Vehicle, Manufacturing, etc.', 'Superior organization, coaching and interpersonal skills, combined with effective leadership, decision-making, and communication', 'Drive the Data Strategy and align it with continuously changing business landscapeWork on all the data types across the enterprise including Customer, Dealer, Vehicle, Manufacturing, etc.Continuously increase Data Coverage by working closely with stakeholders and Data Scientists, understanding and evaluating their data requirements to create meaningful, organized and structured “information”Support the data requirements of the different functional teams like MS&S, PD, Quality, etc. and all the regional KPI / Metrics initiatives -Evaluate, explore and select the right data platform technologies including Big Data, RDBMS & NoSQL to meet the analytics requirementsProvide visibility to Data Quality issues and work with the business owners to fix the issues -Implement an Enterprise Data Governance model and actively promote the concept of data standardization, integration, fusion and qualityWork with technology team to model data, create data flow diagrams, load, and transformationBuild the Metadata model & Business Glossary by gathering information from multiple sources: business users, existing data sources, databases and other relevant documents and systemsServe as data subject matter expert and demonstrate an understanding of key data management principles and data use', 'Detail-oriented with a strong drive to enforce common', 'Ability to communicate complex solution concepts in simple terms', 'Experience programming and producing working models or transformations with modern programming languages ', 'Ability to anticipate obstacles and develop plans to resolve those obstacles', 'Bachelor’s degree in Data Science, Computer Science, or Data Engineering', 'Ability to quickly comprehend the functions and capabilities of new technologies', 'Resourceful and quick learner, with the ability to efficiently seek out, learn, and apply new areas of expertise, as needed ', 'Minimum of 3 years of experience in data design, data architecture and data modeling (both transactional and analytic)', 'Bachelor’s degree in Data Science, Computer Science, or Data EngineeringTwo or more years of experience in a Data Management role including running queries and compiling data for analyticsTwo or more years of experience in data design, data architecture and data modeling (both transactional and analytic)', 'Strong analytical and problem solving skills, with the ability to communicate in a clear and succinct manner and effectively evaluates information / data to make decisions', 'Strong interpersonal, and leadership skills, with proven abilities to communicate complex topics to leaders and peers in a simple, clear, plan oriented manner', 'Ability to apply multiple solutions to business problems', 'What you’ll receive in return:', 'Strong team player, with the ability to collaborate well with others, to solve problems and actively incorporate input from various sourcesDemonstrated customer focus, with the ability to evaluate decisions through the eyes of the customer, build strong customer relationships, and create processes with customer viewpointStrong analytical and problem solving skills, with the ability to communicate in a clear and succinct manner and effectively evaluates information / data to make decisionsStrong interpersonal, and leadership skills, with proven abilities to communicate complex topics to leaders and peers in a simple, clear, plan oriented mannerAbility to anticipate obstacles and develop plans to resolve those obstaclesChange oriented, with the ability to actively generates process improvements, support and drives change, and confront difficult circumstances in creative ways Resourceful and quick learner, with the ability to efficiently seek out, learn, and apply new areas of expertise, as needed Highly self-motivated, with the ability to work independentlySuperior organization, coaching and interpersonal skills, combined with effective leadership, decision-making, and communicationStrong oral and written communication skills (English)Strategic and clear thinking to translate discreet and complex ideas to business-driven resultsDetail-oriented with a strong drive to enforce common', 'Provide visibility to Data Quality issues and work with the business owners to fix the issues -Implement an Enterprise Data Governance model and actively promote the concept of data standardization, integration, fusion and quality', 'Change oriented, with the ability to actively generates process improvements, support and drives change, and confront difficult circumstances in creative ways ', 'Knowledge of data management standards, data governance practices and data quality', 'What You’ll Be Able To Do', 'Build the Metadata model & Business Glossary by gathering information from multiple sources: business users, existing data sources, databases and other relevant documents and systems', 'Two or more years of experience in data design, data architecture and data modeling (both transactional and analytic)', 'Minimum of 2 years of experience in Big Data / NoSQL technologies including Hadoop (HDFS, MapReduce, Hive, Scala, Spark etc.), especially command line experience with loading and manipulating files within HDFS Strong oral and written communication skills', 'Highly self-motivated, with the ability to work independently', 'Demonstrated customer focus, with the ability to evaluate decisions through the eyes of the customer, build strong customer relationships, and create processes with customer viewpoint', 'Strong oral and written communication skills (English)', ""Master's degree in Data Science, Computer Science, or Data Engineering5 years of experience in a Data Management role including running queries and compiling data for analyticsMinimum of 3 years of experience in data design, data architecture and data modeling (both transactional and analytic)Minimum of 2 years of experience in Big Data / NoSQL technologies including Hadoop (HDFS, MapReduce, Hive, Scala, Spark etc.), especially command line experience with loading and manipulating files within HDFS Strong oral and written communication skillsExperience programming and producing working models or transformations with modern programming languages Demonstrated experience building visualizations using Tableau/QlikviewAbility to write complex SQL queries needed to query & analyze dataKnowledge of data management standards, data governance practices and data qualityAbility to communicate complex solution concepts in simple termsAbility to apply multiple solutions to business problemsAbility to quickly comprehend the functions and capabilities of new technologiesStrong Oral and written communication skills"", 'Strong Oral and written communication skills', ""Master's degree in Data Science, Computer Science, or Data Engineering"", 'The Minimum Requirements We Seek']",Entry level,Full-time,Information Technology,Automotive,2021-03-18 14:34:51
Data Engineer - Remote - 125K,Jefferson Frank,"Fairfax, VT",2 days ago,Be among the first 25 applicants,"['', ' SQL, (Postgres, MySQL)', ' Solid mission, VERY strong funding Never stopped hiring during COVID-19, very stable Really strong benefits', ' Kubernetes/Kubernetes tools like Helm (Docker experience is OK)', ' APIs exp a plus', 'Skills', 'Benefits', ' 135 employees', ' They are a health technology company that uses evidence-based data to solve health challenges. They come up with research-related products that can be utilized in the health care field employees', ' Up to 10% bonus for each role, to be paid out quarterly', ' Extensive Python exp', ' Experience creating Data Pipelines & Data Lakes', ' 80% health, dental & vision coverage for individual AND families', ' Up to 10% bonus for each role, to be paid out quarterly 80% health, dental & vision coverage for individual AND families 401k - 100% match up to 6%. Fully vested after first year Unlimited sick leave, 3 weeks PTO', 'About My Client', ' Unlimited sick leave, 3 weeks PTO', ' 4+ yrs exp', ' Never stopped hiring during COVID-19, very stable', ' Data experience in Hadoop, Spark, HIVE', ' AWS experience required (Redshift required)', ' Solid mission, VERY strong funding', ' Really strong benefits', ' 401k - 100% match up to 6%. Fully vested after first year', ' 135 employees HQ in Fairfax, VA. They are remote now but want the candidates in-office once they return They are a health technology company that uses evidence-based data to solve health challenges. They come up with research-related products that can be utilized in the health care field employees', ' 4+ yrs exp AWS experience required (Redshift required) Extensive Python exp Experience creating Data Pipelines & Data Lakes Data experience in Hadoop, Spark, HIVE Kubernetes/Kubernetes tools like Helm (Docker experience is OK) SQL, (Postgres, MySQL) APIs exp a plus', ' HQ in Fairfax, VA. They are remote now but want the candidates in-office once they return']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Capital One,"McLean, VA",3 weeks ago,29 applicants,"['', '1+ years of experience with Ansible / Terraform', ' slides 76-91', '3+ years of experience in application development', '2+ years of experience with UNIX/Linux including basic commands and shell scripting', '2+ years of experience developing Java based software solutions ', '1+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)', '1+ year experience working on streaming data applications (Spark Streaming, Kafka, Kinesis, and Flink', 'Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Snowflake', 'Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community', 'What You’ll Do', 'Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologiesWork with a team of developers with deep experience in machine learning, distributed microservices, and full stack systemsUtilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as SnowflakeShare your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering communityCollaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowermentPerform unit tests and conducting reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance', ""Master's Degree 3+ years of experience in application development1+ year experience working on streaming data applications (Spark Streaming, Kafka, Kinesis, and Flink1+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)1+ years of experience with Ansible / Terraform2+ years of experience with Agile engineering practices 2+ years in-depth experience with the Hadoop stack (MapReduce, Pig, Hive, Hbase) 2+ years of experience with NoSQL implementation (Mongo, Cassandra) 2+ years of experience developing Java based software solutions 2+ years of experience in at least one scripting language (Python, Perl, JavaScript, Shell) 2+ years of experience developing software solutions to solve complex business problems2+ years of experience with UNIX/Linux including basic commands and shell scripting"", 'Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems', 'At least 1 years of experience in big data technologies (Cassandra, Accumulo, HBase, Spark, Hadoop, HDFS, AVRO, MongoDB, or Zookeeper)', 'Capital One Data Engineer', '2+ years of experience with Agile engineering practices ', '#lifeatcapitalone', 'Bachelor’s Degree At least 2 years of experience in application developmentAt least 1 years of experience in big data technologies (Cassandra, Accumulo, HBase, Spark, Hadoop, HDFS, AVRO, MongoDB, or Zookeeper)', 'inclusive,', '2+ years in-depth experience with the Hadoop stack (MapReduce, Pig, Hive, Hbase) ', '2+ years of experience in at least one scripting language (Python, Perl, JavaScript, Shell) ', 'Bachelor’s Degree ', 'diversity & inclusion', 'Perform unit tests and conducting reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance', 'Basic Qualifications', ""Master's Degree "", 'At least 2 years of experience in application development', '2+ years of experience with NoSQL implementation (Mongo, Cassandra) ', 'Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies', '2+ years of experience developing software solutions to solve complex business problems', 'Preferred Qualifications', 'Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment']",Entry level,Full-time,Information Technology,Banking,2021-03-18 14:34:51
SQL Data Engineer,Jettison,"Hoffman Estates, IL",6 days ago,122 applicants,"['', 'Jettison is representing a client who is searching for a\xa0SQL Data Engineer\xa0in the Hoffman Estates, Illinois area.\xa0\xa0This position is a contract to hire role and will be onsite in the near future.', 'Must have VERY strong SQL knowledge\xa0', 'Great verbal and written communication skills\xa0', 'Jettison is an Equal Opportunity/Affirmative Action employer', 'Work experience using PowerBI\xa0', 'Demonstrated ability to walk thru raw SQL', 'Hands on work with Azure and Python\xa0', 'Machine learning experience\xa0', 'Required:\xa0', 'Ability analyze data using T-SQL', '3+ years of SQL Data Engineer experience', 'Knowledge of SSIS and SSRS', 'Bachelors Degree']",Mid-Senior level,Full-time,Marketing,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,State Farm ®,"Dunwoody, GA",3 days ago,62 applicants,"['', 'Qualifications', 'Computer Science background (Bachelor degree or higher) with a minimum of 2 years of experience in an IT related field Possess strong business acumen and the technical ability to acquire, transform and interpret complex data in order to answer ad hoc questions often coming from top executivesExperience with gathering and creating analytic business requirements, researching potential data sources (both internal and external sources), designing, developing and maintaining data assetsExperience with data governance policies, including the implementation of data security strategiesExcellent communication skills and the ability to work with multiple, diverse stakeholders across business areas and leadership levelsTechnical expertise with multiple compute environments, including at least two of the following: Linux, Hadoop, Mainframe, and AWSExperience with building and maintaining data pipelinesFamiliarity with building SQL and No-SQL queriesFamiliarity with one of the following languages: Python, R, or SASKnowledge of version control and DevOps tools, such as GitLab Knowledge of work prioritization using the Agile framework', 'Responsibilities', 'Overview', 'Experience with data governance policies, including the implementation of data security strategies', 'Experience with building and maintaining data pipelines', ' Possess strong business acumen and the technical ability to acquire, transform and interpret complex data in order to answer ad hoc questions often coming from top executives', 'Excellent communication skills and the ability to work with multiple, diverse stakeholders across business areas and leadership levels', 'Familiarity with building SQL and No-SQL queries', 'Familiarity with one of the following languages: Python, R, or SAS', ' Knowledge of work prioritization using the Agile framework', 'Experience with gathering and creating analytic business requirements, researching potential data sources (both internal and external sources), designing, developing and maintaining data assets', 'Computer Science background (Bachelor degree or higher) with a minimum of 2 years of experience in an IT related field', 'Technical expertise with multiple compute environments, including at least two of the following: Linux, Hadoop, Mainframe, and AWS', 'Knowledge of version control and DevOps tools, such as GitLab']",Not Applicable,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer Intermediate (work from home Mid-Atlantic US resident),Geisinger,"Harrisburg, PA",4 days ago,Be among the first 25 applicants,"['', 'Writes code for parallel computing.', 'Job Summary', 'Our Purpose & Values', 'Job Duties', 'LEARNING', 'Provides preliminary code review, testing, debugging, and general testing instructions.', 'Develops new programs and responsible for moving existing code to high performance distributed systems code.', 'KINDNESS', 'About Geisinger', 'Coordinates projects and responsible for timely and accurate execution.', 'Education', 'Works closely with data architects to define and execute an enterprise data architecture for complex healthcare data flows.', 'INNOVATION', 'EXCELLENCE', 'Builds data ingestion pipelines for the Big Data Hadoop environment.Programming for a Big Data distributed computing environment using Java, Scala or similar object oriented programming languages.Programming data processing and integration algorithms on the Apache Hadoop, HBase stack.Coordinates projects and responsible for timely and accurate execution.Collaborates and participates in the design and implementation of various projects.Involves high-level participation in the design and management of a computational infrastructure for different purposes including applied medical research.Collaborates with other technology team members, clinicians and researchers on projects requiring data and analytic services.Works closely with data architects to define and execute an enterprise data architecture for complex healthcare data flows.Writes code for parallel computing.Develops new programs and responsible for moving existing code to high performance distributed systems code.Responsible to document all changes completed on the system within designated timeframes.Responsible for following department coding/programming guidelines to produce efficient routines.Provides preliminary code review, testing, debugging, and general testing instructions.', 'SAFETY', 'Responsible for following department coding/programming guidelines to produce efficient routines.', 'Builds data ingestion pipelines for the Big Data Hadoop environment.', 'Programming for a Big Data distributed computing environment using Java, Scala or similar object oriented programming languages.', 'Involves high-level participation in the design and management of a computational infrastructure for different purposes including applied medical research.', 'Collaborates with other technology team members, clinicians and researchers on projects requiring data and analytic services.', 'Experience', 'Responsible to document all changes completed on the system within designated timeframes.', 'Collaborates and participates in the design and implementation of various projects.', 'Programming data processing and integration algorithms on the Apache Hadoop, HBase stack.']",Entry level,Full-time,Information Technology,Nonprofit Organization Management,2021-03-18 14:34:51
Data Engineer,VanderHouwen,"Portland, OR",6 days ago,90 applicants,"['', 'Description', 'Benefits', 'Required Technical Skills', 'About VanderHouwen', 'Preferred Technical Skills', 'Data Engineer ', 'Data Engineer Qualifications']",Entry level,Contract,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer - Austin,Ace Technologies,"Austin, TX",24 hours ago,Be among the first 25 applicants,"['', ' Experience with Azure Stream Analytics, AzureDataFactory or ETL Tools like Informatica Power Centre', ' Build, design and launch data pipelines, landing zones and datalakes that store, transform and move data', ' Preferred experience with R and Python', ' Good understanding of a tech stack such as Microsoft (AzureDataLake Storage, AzureDataFactory, Azure Streams,Databricks) orCloudlie AWS/Google', ' Collect, combine and integrate data from our omni and multi-channel footprints using our enterprise tech-stack that intuitively conveys insights to the business regarding data trends and consumer behaviour', ' Design, Build and write code for cloud-compatible CI/CD frameworks to deploy solutions onCloudDataPlatforms', ' Data Engineer with Azure', ' Experience with Streaming Data, Azure Datastack development (AzureDataLake Storage, AzureDataLake Analytics, AzureDataFactory,Databricks)', ' Monitor dataquality processes and compliance processes in accordance with industry and data governance COP best practices', ' DWH Concept and good to have experience with MSSQL or Oracledatabase', ' Harness, model and transform data(structured, unstructured) from several sources that empowers users to collaborate and analyzation different ways leading to better and faster decision making']",Entry level,Full-time,Information Technology,Computer Hardware,2021-03-18 14:34:51
Data Engineer,Motion Recruitment,"Los Angeles, CA",2 days ago,Be among the first 25 applicants,"['', '401(k)', 'Required Skills & Experience', 'Basic understanding of Database, Data Modeling', 'Python', ""Someone who's close to Analytics (BI reporting), not infrastructure"", 'Snowflake', 'SQL ', 'Paid Sick Time Leave', 'You will receive the following benefits:', 'Medical Insurance & Health Savings Account (HSA)', 'Competitive Salary: Up to $XXK/year, DOE', 'Pre-tax Commuter Benefit', "" SQL  Python Database experience  Basic understanding of Database, Data Modeling Someone who's close to Analytics (BI reporting), not infrastructure "", 'Big Data (Hive, Hadoop) ', ' Competitive Salary: Up to $XXK/year, DOE You will receive the following benefits: Medical Insurance & Health Savings Account (HSA) 401(k) Paid Sick Time Leave Pre-tax Commuter Benefit ', 'Database experience ', 'Plusses', ' Big Data (Hive, Hadoop)  Snowflake ', 'Job Description']",Entry level,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Engineer,Eliassen Group,"New York, NY",3 days ago,179 applicants,"['', 'AWS Certification (Solution Architect/Develop/SysOps)Docker/KubernetesREST API', 'Requirements:', 'Ability to build and manage resilient and scalable infrastructure on AWS', 'Hands on programming language experience: Scala/Python', 'CICD pipeline automation with Jenkins/Github.', 'REST API', 'Object oriented programming concepts', 'Strong AWS service experience :', 'Experience with infrastructure tuning and deployment at enterprise scale.', 'Are you starting your job search and don’t know where to begin? Have you been looking for a position change and just can’t seem to find the next right fit? Eliassen Group might just have a position for you! We are partnered with a large FinTech organization who is looking for an experienced Data Engineer to support a large enterprise project in a dynamic and Agile technical environment.\xa0Their projects are tasked with bringing data from a third party host transforming the work from the raw format to meet the existing data set and make it available to various LOBs through Rest APIs, into a centralized data lake, and an analytical environment using data warehousing tools that run on AWS to help keep the infrastructure in place.\xa0', 'S3/RDS/IAM/EMR/EC2/ECS/CloudWatch', 'Ability to build and manage resilient and scalable infrastructure on AWSHands on programming language experience: Scala/PythonObject oriented programming conceptsExperience with infrastructure tuning and deployment at enterprise scale.Strong AWS service experience :S3/RDS/IAM/EMR/EC2/ECS/CloudWatchCICD pipeline automation with Jenkins/Github.Able to collaborate with multiple teams and adapt with shifting business priority.', 'AWS Certification (Solution Architect/Develop/SysOps)', 'Able to collaborate with multiple teams and adapt with shifting business priority.', 'Docker/Kubernetes', 'Plus:', 'Take a look at the specific expectations and requirements for this position and submit an inquiry for more information:']",Mid-Senior level,Contract,Information Technology,Staffing and Recruiting,2021-03-18 14:34:51
Data Engineer,EyeCare Partners,"Ballwin, MO",1 day ago,Be among the first 25 applicants,"['', 'Development and implementation of scripts for datahub maintenance, monitoring, performance tuning', 'Design, build and maintain data pipelines from various source systems into SnowflakeAnalyze data elements from various systems, data flow, dependencies, relationships and assist in designing conceptual physical and logical data modelsDesign, build and maintain complex data sets designed to meet various business needs in the areas of reporting, advanced analytics and ad-hoc analysisCoordinate the build and maintenance of data pipelines by third party service providersEnabling and executing data migrations across systems (e.g. SQL server to Snowflake or other cloud data platforms)Development and implementation of scripts for datahub maintenance, monitoring, performance tuningWork with data and business analysts to deploy and support a robust data quality platformWork with data and business analysts to deploy and support a robust data cataloging strategyWork with various business and technical stakeholders and assist with data-related technical needs and issuesWork with data and analytics teams and drive greater value from our data and analytics investmentsWork closely with cross-functional teams to understand and transform business requirements into scalable and manageable solutionsPresent solutions and options to leadership, project teams and other stakeholders adapting style to both technical and non-technical audiencesEnsures teams adhere to documented design and development patterns and standardsProactively monitor and resolve on-going production issuesWork closely with various technical teams to ensure consistency, quality of solutions and knowledge sharing across the enterpriseEducate organization on available and emerging tool setsEnsure adherence to the approach of self-service data solutions and enable other teams with analytics solutions delivery via Data as a Service model', 'Experience working with multiple ETL/ELT tools and cloud based data hubs', 'A self-motivated personality with a passion for working in a fast-paced environment', 'Work with various business and technical stakeholders and assist with data-related technical needs and issues', 'Work closely with cross-functional teams to understand and transform business requirements into scalable and manageable solutions', 'Proactively monitor and resolve on-going production issues', 'Demonstrated problem solving', 'Work closely with various technical teams to ensure consistency, quality of solutions and knowledge sharing across the enterprise', 'Experience with AWS cloud services: EC2, EMR, RDS, DMS', ""Bachelor's degree in Computer Science, Information Systems or equivalent + 3 years related experience3+ years of hands-on-experience in the design, development, and implementation of data solutionsAdvanced SQL knowledge with strong query writing, stored procedures skillsExperience with Snowflake development and supportExperience with object-oriented/object function scripting languages: Python, Java, Scala, etc.Experience with AWS cloud services: EC2, EMR, RDS, DMSExperience with relational databases such as SQL Server and object relational databases such as PostgreSQLExperience with stream-processing systems: Storm, Spark-Streaming, etc.Experience with data analysis, ETL, and workflow automationExperience working with multiple ETL/ELT tools and cloud based data hubsDemonstrated problem solvingDemonstrated ability to think and work with a proactive mindsetA self-motivated personality with a passion for working in a fast-paced environment"", 'Enabling and executing data migrations across systems (e.g. SQL server to Snowflake or other cloud data platforms)', 'Work with data and business analysts to deploy and support a robust data quality platform', '3+ years of hands-on-experience in the design, development, and implementation of data solutions', 'Advanced SQL knowledge with strong query writing, stored procedures skills', 'Requirements', 'Design, build and maintain data pipelines from various source systems into Snowflake', ""Bachelor's degree in Computer Science, Information Systems or equivalent + 3 years related experience"", 'Experience with object-oriented/object function scripting languages: Python, Java, Scala, etc.', 'Present solutions and options to leadership, project teams and other stakeholders adapting style to both technical and non-technical audiences', 'Ensures teams adhere to documented design and development patterns and standards', 'Essential Responsibilities', 'Experience with Snowflake development and support', 'Coordinate the build and maintenance of data pipelines by third party service providers', 'Work with data and analytics teams and drive greater value from our data and analytics investments', 'Experience with data analysis, ETL, and workflow automation', 'Design, build and maintain complex data sets designed to meet various business needs in the areas of reporting, advanced analytics and ad-hoc analysis', 'Experience with stream-processing systems: Storm, Spark-Streaming, etc.', 'Experience with relational databases such as SQL Server and object relational databases such as PostgreSQL', 'Demonstrated ability to think and work with a proactive mindset', 'Ensure adherence to the approach of self-service data solutions and enable other teams with analytics solutions delivery via Data as a Service model', 'Analyze data elements from various systems, data flow, dependencies, relationships and assist in designing conceptual physical and logical data models', 'Work with data and business analysts to deploy and support a robust data cataloging strategy', 'Educate organization on available and emerging tool sets']",Entry level,Full-time,Quality Assurance,Information Technology and Services,2021-03-18 14:34:51
Associate Data Analytics Engineer,Northwestern Mutual,"Milwaukee, WI",1 week ago,41 applicants,"['', 'Novice programming skills', 'This job is not covered by the existing Collective Bargaining Agreement.', 'Apply engineering best practices in order to analyze, design, develop, deploy and support data analytics products. ', 'Python and cloud compute skills', ""We're strong and growing."", 'Acquire, analyze, combine, synthesize, and structure data with clear definitions and sources for analytical consumption.Participate in consultations with data consumers to identify meaningful datasets for analytical consumption.Develop data products using continuous deployment and integration practicesApply engineering best practices in order to analyze, design, develop, deploy and support data analytics products. Participate in agile story authoring, sizing, and demo sessions for product features Participate in code reviews and learn from them.', 'Build an understanding of the data flow so that data can be shared across the company with one source of truth being the goal.', 'Develop data products using continuous deployment and integration practices', 'At Northwestern Mutual, we are strong, innovative and growing. We invest in our people. We care and make a positive difference.', 'Computer programming classes.', 'Participate in agile story authoring, sizing, and demo sessions for product features ', 'Exceptional analytical, conceptual, and problem-solving skills.', 'At Northwestern Mutual, we believe relationships are built on trust.', ""What's the role?"", 'Ability to identify data quality issues and their root causes. Propose fixes and design data audits. ', 'Excellent written and verbal communication skills', 'Qualifications', 'Grow your career with a best-in-class company that puts our client’s interests at the center of all we do. Get started now! ', ' We are an equal opportunity/affirmative action employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, gender identity or expression, sexual orientation, national origin, disability, age or status as a protected veteran, or any other characteristic protected by law.', 'Build an understanding of data validation routines and purpose', 'Who We Are', 'Preferred Skills And Abilities', 'Associate Data Analytics Engineer Job Description', 'Participate in consultations with data consumers to identify meaningful datasets for analytical consumption.', 'We care. ', 'Novice programming skillsPython and cloud compute skillsSQL skills and background in ETLBuild the understanding of the business domain and the meaning of the data in it to accurately identify key and associated data which can be extracted, transformed and loaded for effective consumption downstream.Build an understanding of the data flow so that data can be shared across the company with one source of truth being the goal.Build an understanding of data validation routines and purposeAbility to identify data quality issues and their root causes. Propose fixes and design data audits. Excellent written and verbal communication skillsExceptional analytical, conceptual, and problem-solving skills.Self-motivated and seeking to learn', 'Build the understanding of the business domain and the meaning of the data in it to accurately identify key and associated data which can be extracted, transformed and loaded for effective consumption downstream.', 'Required Certifications', 'Participate in code reviews and learn from them.', 'SQL skills and background in ETL', 'Responsibilities Include But Are Not Limited To', 'Acquire, analyze, combine, synthesize, and structure data with clear definitions and sources for analytical consumption.', 'Role And Responsibilities', 'Self-motivated and seeking to learn', 'We invest in our people. ', 'At Northwestern Mutual, we are strong, innovative and growing. We invest in our people. We care and make a positive difference. ', 'We are an equal opportunity/affirmative action employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, gender identity or expression, sexual orientation, national origin, disability, age or status as a protected veteran, or any other characteristic protected by law.']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Labcorp,"Tampa, FL",6 days ago,29 applicants,"['', ' Continuously fine tune and optimize data integration systems to promote stability and eliminate exceptions requiring human intervention.', 'What You Will Be Doing', ' 5+ years working in information systems development roles or proven technical skills coupled with a degree in Computer Science, Information Systems or similar field. ', ' 3+ years working in a data engineering capacity.', ' Quickly identify and resolve production issues related to data matching, ingestion merge or reporting export.', ' Collaborate with application development team on database architecture and processes to support various LabCorp Employer Services internal and client facing applications.', ' Experience with Azure Cloud service offerings such as: Azure Data Factory, Azure Databricks, Azure Functions, Azure Synapse.', ' Highly proficient with SQL Server Integration Services, SQL Server Agent automation.', ' Solid understanding of flat file formats and file format conversion.', ' Ability to identify and diagnose ETL and database related issues, perform root cause analysis, and recommend corrective actions.', ' Build data systems and processes to enable easy troubleshooting and exception reconciliation by the Data Ops team.', ' Develop procedures to enhance reporting and query capabilities to improve efficiency and accuracy.', ' Ability to work with complex business requirements in developing SQL stored procedures.', ' Design and build data visualizations, interactive dashboards and report extracts to support the internal business and clients.', 'Requirements', ' Work with business and client liaisons to create standardized approaches to data sharing and integration.', ' Build automated data ingestion and ETL processes utilizing SSIS, T-SQL, SQL agent jobs, Python and Azure Cloud Services. ', '  Design and develop data report extracts, processes and comprehensive systems to support the needs of the business and functionality of web applications.  Build automated data ingestion and ETL processes utilizing SSIS, T-SQL, SQL agent jobs, Python and Azure Cloud Services.   Develop and maintain complex matching logic systems and stored procedures for merging inbound patient results data from flat files or staging tables to existing patient profile records.   Continuously fine tune and optimize data integration systems to promote stability and eliminate exceptions requiring human intervention.  Work with business and client liaisons to create standardized approaches to data sharing and integration.  Quickly identify and resolve production issues related to data matching, ingestion merge or reporting export.  Document data workflow diagrams for major services.  Build and manage monitoring systems and tests for production data quality.  Build data systems and processes to enable easy troubleshooting and exception reconciliation by the Data Ops team.  Develop procedures to enhance reporting and query capabilities to improve efficiency and accuracy.  Design and build data visualizations, interactive dashboards and report extracts to support the internal business and clients.  Collaborate with application development team on database architecture and processes to support various LabCorp Employer Services internal and client facing applications. ', ' Solid communication skills for documenting complex processes and oral communication skills in communicating with technical and non-technical employees.', ' Build and manage monitoring systems and tests for production data quality.', 'Location:', '  Experience with Agile/Scrum methodologies and working as part of a cross-functional delivery team.  Experience with Azure Cloud service offerings such as: Azure Data Factory, Azure Databricks, Azure Functions, Azure Synapse. ', ' Develop and maintain complex matching logic systems and stored procedures for merging inbound patient results data from flat files or staging tables to existing patient profile records. ', '  5+ years working in information systems development roles or proven technical skills coupled with a degree in Computer Science, Information Systems or similar field.   3+ years working in a data engineering capacity.  Exceptional skills in relational database systems - MS SQL Server and Azure SQL Server.  T-SQL, dynamic SQL, scripting, reporting services and Integration Services, flat file automation (pull, post).  Highly proficient with SQL Server Integration Services, SQL Server Agent automation.  Ability to identify and diagnose ETL and database related issues, perform root cause analysis, and recommend corrective actions.  Python experience in the context of data orchestration, automation and analysis.  Data visualization design and development capability in Tableau or other tools/languages.  Ability to work with complex business requirements in developing SQL stored procedures.  Solid understanding of flat file formats and file format conversion.  Understanding of EDI file formats.  Solid communication skills for documenting complex processes and oral communication skills in communicating with technical and non-technical employees.  Data workflow mapping in Visio or other similar tool. ', ' Exceptional skills in relational database systems - MS SQL Server and Azure SQL Server.', 'Your Background and Experience ', ' T-SQL, dynamic SQL, scripting, reporting services and Integration Services, flat file automation (pull, post).', 'Role Description', ' Data visualization design and development capability in Tableau or other tools/languages.', 'Strongly Preferred', ' Experience with Agile/Scrum methodologies and working as part of a cross-functional delivery team.', ' Data workflow mapping in Visio or other similar tool.', ' Understanding of EDI file formats.', 'Department', ' Design and develop data report extracts, processes and comprehensive systems to support the needs of the business and functionality of web applications.', ' Document data workflow diagrams for major services.', ' Python experience in the context of data orchestration, automation and analysis.', 'Job Title']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Junior Data Engineer,Invesco US,"Atlanta, GA",1 week ago,48 applicants,"['', 'Complete all tasks related to technical analysis, building and unit testing, quality assurance, system test and implementation in accordance with the Technology development life cycle.', '401(K) matching of 100% up to the first 6% with additional supplemental contribution', 'Design and/or understand complex data models with strong SQL skills', 'Health & wellbeing benefits', 'Maintains a broad understanding of implementation, integration, and inter-connectivity issues with emerging technologies to define data strategies.', 'Comfortable working with ambiguity (e.g. imperfect data, loosely defined concepts, ideas, or goals) and translating these into more tangible outputs.', 'Experience with troubleshooting issues in the software and bug-fixes', 'Work with development teams and other project leaders/stakeholders to provide technical solutions that enable business capabilities.', ' Work with development teams and other project leaders/stakeholders to provide technical solutions that enable business capabilities. Complete all tasks related to technical analysis, building and unit testing, quality assurance, system test and implementation in accordance with the Technology development life cycle. Design and/or understand complex data models with strong SQL skills Assist with data modeling skills and architecture Analyze requirements, design, build and test system components. Maintains a broad understanding of implementation, integration, and inter-connectivity issues with emerging technologies to define data strategies. Execute strategies that inform data design and architecture partnering with enterprise standard. Understand client business and make recommendations and technology suggestions to solve problems and improve efficiency. Understand architecture of applications to effectively troubleshoot problems and develop more efficient production processes. Be prepared to carry out business analysis tasks to ensure that the development/change meets user requirements and expectations. Prepare for and support user acceptance testing. Provide post implementation support. ', 'Experience with Agile methodology and working on scrum teams.', '1- 2 years of experience in Autosys job scheduler', ' No more than 8 bullet points of 1-2 sentences max ', ' 1- 2 years of experience working in an Oracle, Informatica, UNIX production data warehousing environment as a Data Warehouse developer 1- 2 years of experience in Autosys job scheduler 1 - 2 years of experience designing and implementing ETL processes using Informatica PowerCenter 1- 2 years of experience with Oracle and/or SQL server database and deep knowledge in SQL Understanding of relational database design and development life cycle principles and standard methodologies Experience using GitHub, Bit Bucket, or other code repository solution Experience with Agile methodology and working on scrum teams. Familiarity with shell/python scripting languages Experience with troubleshooting issues in the software and bug-fixes Experience in data warehousing concepts Financials services/asset management industry experience is a plus ', '1- 2 years of experience working in an Oracle, Informatica, UNIX production data warehousing environment as a Data Warehouse developer', 'Open minded, flexible and willing to listen for other people’s opinions.', 'Enjoy challenging and thought provoking work and have a strong desire to learn and progress', 'Your Role', '1 - 2 years of experience designing and implementing ETL processes using Informatica PowerCenter', 'Understanding of relational database design and development life cycle principles and standard methodologies', 'Execute strategies that inform data design and architecture partnering with enterprise standard.', 'Employee stock purchase plan', 'The Department', 'Assist with data modeling skills and architecture', 'Flexible time off and opportunities for a flexible work schedule', 'Understand architecture of applications to effectively troubleshoot problems and develop more efficient production processes.', 'What’s in it for you?', 'Understand client business and make recommendations and technology suggestions to solve problems and improve efficiency.', 'Be prepared to carry out business analysis tasks to ensure that the development/change meets user requirements and expectations.', 'Prepare for and support user acceptance testing.', 'Financials services/asset management industry experience is a plus', 'Familiarity with shell/python scripting languages', 'Able to work independently or as a team player and ramp up on new technologies', 'Parental Leave benefits', 'Skills / Other Personal Attributes Required:', 'Strong data analytical skills', 'Self-motivated. Capable of working with little or no supervision', ' Strong written, verbal communication and presentation skills Able to work in a global, multicultural environment Self-motivated. Capable of working with little or no supervision Ability to react positively under pressure to meet tight deadlines Able to work independently or as a team player and ramp up on new technologies Open minded, flexible and willing to listen for other people’s opinions. Strong data analytical skills Enjoy challenging and thought provoking work and have a strong desire to learn and progress Comfortable working with ambiguity (e.g. imperfect data, loosely defined concepts, ideas, or goals) and translating these into more tangible outputs. ', 'No more than 8 bullet points of 1-2 sentences max', ' Flexible time off and opportunities for a flexible work schedule 401(K) matching of 100% up to the first 6% with additional supplemental contribution Health & wellbeing benefits Parental Leave benefits Employee stock purchase plan ', '1- 2 years of experience with Oracle and/or SQL server database and deep knowledge in SQL', 'The experience you bring:', 'Provide post implementation support.', 'Experience using GitHub, Bit Bucket, or other code repository solution', 'Strong written, verbal communication and presentation skills', 'Able to work in a global, multicultural environment', 'Experience in data warehousing concepts', 'Ability to react positively under pressure to meet tight deadlines', 'Analyze requirements, design, build and test system components.']",Not Applicable,Full-time,Information Technology,Investment Management,2021-03-18 14:34:51
Data Engineer,Storable,"Texas, United States",3 weeks ago,153 applicants,"['', 'Engage remotely! Since mid-March we transitioned to a fully remote environment and we run regular contests, trivia games and remote happy hours!', 'Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using various SQL and ‘big data’ technologies', 'About Us:', 'Assemble large, complex data sets that meet both functional and non-functional requirements', 'Create and maintain optimal data pipeline architecture', 'Get active in the community by joining one of our many quarterly offsite volunteer and community service events.', 'Create and maintain optimal data pipeline architectureAssemble large, complex data sets that meet both functional and non-functional requirementsIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using various SQL and ‘big data’ technologiesBuild analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metricsWork closely with stakeholders including the Executive, Product, Data and Design teams to design and deliver products and functionality to address analytical and functional data needsCreate data tools for analytics and data scientist and business operations team members that assist them in building and optimizing our productWork with data and analytics experts to strive for greater functionality in our data systems', 'Storable is looking for a Data Engineer to join our Data Architecture team in Austin. This team member will be responsible for expanding and optimizing our cloud native big data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing distributed systems as well as building them from the ground up.\xa0', 'Work with data and analytics experts to strive for greater functionality in our data systems', 'Computer science degree or equivalent experience3+\xa0 years experience in software development, data engineering, BI development, and / or data architectureExperience with Python, SQL, Airflow, AWS, RESTful APIs, and Tableau or other data visualization toolsConsistent track record of leading successful delivery for a large-scale project or being a key contributor on multiple projectsConsistent track record of positively influencing project direction and contributions to cross-functional and/or cross-organizational collaborations', 'Benefits and Perks:', 'Instant-Access & Flexible vacation. We trust you, so we have a ‘take what you need’ vacation policy. No waiting to use it or need for accruals.', 'Do More, Be More – Company rewards and recognition that add up and can be redeemed for once-in-a-lifetime Bucketlist experiences!', 'All applicants must be currently authorized to work in the United States on a full-time basis.', 'Must\xa0be located on:\xa0TX, KS, NC, MO, CO, PA, IL, IN', 'Create data tools for analytics and data scientist and business operations team members that assist them in building and optimizing our product', 'Computer science degree or equivalent experience', 'Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics', 'Experience with Python, SQL, Airflow, AWS, RESTful APIs, and Tableau or other data visualization tools', 'Must', 'The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.', 'We leverage our platform in combination with our over 25 years of storage industry expertise to help our thousands of storage customers achieve their tenant experience and operational efficiency objectives every single day.', 'Location:\xa0Remote', 'Consistent track record of leading successful delivery for a large-scale project or being a key contributor on multiple projects', 'What you’ll do everyday:', 'Consistent track record of positively influencing project direction and contributions to cross-functional and/or cross-organizational collaborations', 'Generous health coverage for you and your family, including short- and long-term disability coverage, 401(k) and HSA matching, company-provided life insurance, and more.Engage remotely! Since mid-March we transitioned to a fully remote environment and we run regular contests, trivia games and remote happy hours!Instant-Access & Flexible vacation. We trust you, so we have a ‘take what you need’ vacation policy. No waiting to use it or need for accruals.Do More, Be More – Company rewards and recognition that add up and can be redeemed for once-in-a-lifetime Bucketlist experiences!Get active in the community by joining one of our many quarterly offsite volunteer and community service events.Fun company events, including Halloween costume contests, ugly sweater competitions, baseball game outings, ice cream socials, food trucks, and more.', 'Storable is committed to providing equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, Storable will provide reasonable accommodations for qualified individuals with disabilities.', 'Generous health coverage for you and your family, including short- and long-term disability coverage, 401(k) and HSA matching, company-provided life insurance, and more.', '3+\xa0 years experience in software development, data engineering, BI development, and / or data architecture', 'At Storable, we believe storage operators should have one partner they can trust to help you get the results they need for their business. That’s why we’ve built the industry’s first fully integrated platform that offers facility management software, facility websites, marketing programs and services, payments, and deeply integrated tenant insurance capabilities all in one solution.\xa0', 'Fun company events, including Halloween costume contests, ugly sweater competitions, baseball game outings, ice cream socials, food trucks, and more.', 'Data Engineer', 'What you need to bring to the table:', 'Work closely with stakeholders including the Executive, Product, Data and Design teams to design and deliver products and functionality to address analytical and functional data needs', 'The Data Engineer will support our software developers, database architects, data analysts and data scientists on stakeholder initiatives and ensure delivered architecture is consistent and supportable throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products.\xa0', 'Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.']",Mid-Senior level,Full-time,Information Technology,Internet,2021-03-18 14:34:51
Data Engineer,Kforce Inc,"Beaverton, OR",1 day ago,Be among the first 25 applicants,"['', 'Required Skills', ' Builds windows, screens and reports', ' Participates in quality assurance and develops test application code in client server environment', ' Database', ' Prototypes', 'Additional Skills', ' Business Requirements', ""  Establishes database management systems, standards, guidelines and quality assurance for database deliverables, such as conceptual design, logical database, capacity planning, external data interface specification, data loading plan, data maintenance plan and security policy  Documents and communicates database design  Evaluates and installs database management systems  Codes complex programs and derives logical processes on technical platforms  Builds windows, screens and reports  Assists in the design of user interface and business application prototypes  Participates in quality assurance and develops test application code in client server environment  Provides expertise in devising, negotiating and defending the tables and fields provided in the database  Adapts business requirements, developed by modeling/development staff and systems engineers, and develops the data, database specifications, and table and element attributes for an application  At more experienced levels, helps to develop an understanding of client's original data and storage mechanisms  Determines appropriateness of data for storage and optimum storage organization  Determines how tables relate to each other and how fields interact within the tables for a relational model  This is a typical office job, with no special physical requirements or unusual work environment "", ' Database Management', 'Responsibilities', ' Capacity Planning', ' Any relevant education and/or training will be considered a plus', ' User Interface', ' Establishes database management systems, standards, guidelines and quality assurance for database deliverables, such as conceptual design, logical database, capacity planning, external data interface specification, data loading plan, data maintenance plan and security policy', ' Switch Capacity', ' Maintenance', ' Candidates qualifying for this position will have similar experience along with the skills and abilities to perform the duties mentioned', ' Adapts business requirements, developed by modeling/development staff and systems engineers, and develops the data, database specifications, and table and element attributes for an application', ' Languages: English (Speak, Read, Write)', ' Determines how tables relate to each other and how fields interact within the tables for a relational model', '  Database  Business Requirements  Capacity Planning  Conceptual Design  Database Design ', '  Database Management  Databases  Maintenance  Prototype  Prototypes  Quality Assurance  Switch Capacity  User Interface  Languages: English (Speak, Read, Write)  Candidates qualifying for this position will have similar experience along with the skills and abilities to perform the duties mentioned  Any relevant education and/or training will be considered a plus ', 'REQUIREMENTS:', ' This is a typical office job, with no special physical requirements or unusual work environment', "" At more experienced levels, helps to develop an understanding of client's original data and storage mechanisms"", ' Conceptual Design', ' Databases', ' Assists in the design of user interface and business application prototypes', ' Provides expertise in devising, negotiating and defending the tables and fields provided in the database', ' Evaluates and installs database management systems', ' Codes complex programs and derives logical processes on technical platforms', ' Prototype', ' Quality Assurance', ' Database Design', ' Determines appropriateness of data for storage and optimum storage organization', ' Documents and communicates database design']",Associate,Contract,Information Technology,Consumer Electronics,2021-03-18 14:34:51
Data Engineer,StockX,"Detroit, MI",6 days ago,Be among the first 25 applicants,"['', ""3+ years' experience with AWS or engineering in other cloud environments"", 'Experience providing technical leadership and mentoring other engineers for best practices on data engineering', 'Masters in Computer Science, Physics, Mathematics, Statistics or other Engineering disciplines', 'Work collaboratively with business analysts, product managers, data scientists as well as business partners and actively participate in design thinking session', 'Help continually improve ongoing reporting and analysis processes, simplifying self-service support for business stakeholders', 'Optimize the data pipeline to support ML workloads and use cases', 'Design and build mission critical data pipelines with a highly scalable distributed architecture - including data ingestion (streaming, events and batch), data integration, data curation', 'Nice To Have', "" 7+ years’ experience in data warehouse / data lake technical architecture Minimum 3 years of Big Data and Big Data tools in one or more of the following: Kafka, MapReduce, Spark or Python, Hadoop 3+ years' experience with AWS or engineering in other cloud environments Experience with Database Architecture/Schema design Strong familiarity with batch processing and workflow tools such as AirFlow, NiFi Ability to work independently with business partners and management to understand their needs and exceed expectations in delivering tools/solutions Strong interpersonal, verbal and written communication skills and ability to present complex technical/analytical concepts to executive audience Strong business mindset with customer obsession; ability to collaborate with business partners to identify needs and opportunities for improved data management and delivery Experience providing technical leadership and mentoring other engineers for best practices on data engineering BS/BA degree in Computer Science, Physics, Mathematics, Statistics or other Engineering disciplines "", 'Experience with Hadoop implementation', 'Automation of end to end data pipeline with metadata, data quality checks and audit ', 'Responsibilities', 'Build and support a big data platform on the cloud', 'Experience with Database Architecture/Schema design', 'Experience with data visualization tools such as Tableau, Looker, PowerBI', 'Define and implement automation of jobs and testing', 'Strong familiarity with batch processing and workflow tools such as AirFlow, NiFi', ' Masters in Computer Science, Physics, Mathematics, Statistics or other Engineering disciplines Experience with data visualization tools such as Tableau, Looker, PowerBI Experience with Hadoop implementation ', 'Qualifications', 'Minimum 3 years of Big Data and Big Data tools in one or more of the following: Kafka, MapReduce, Spark or Python, Hadoop', '7+ years’ experience in data warehouse / data lake technical architecture', 'Participate in design and code reviews', 'Build and support reusable framework to ingest, integration and provision data', 'Strong interpersonal, verbal and written communication skills and ability to present complex technical/analytical concepts to executive audience', 'Ability to work independently with business partners and management to understand their needs and exceed expectations in delivering tools/solutions', 'Strong business mindset with customer obsession; ability to collaborate with business partners to identify needs and opportunities for improved data management and delivery', 'Support mission critical applications and near real time data needs from the data platform', 'Capture and publish metadata and new data to subscribed users', 'BS/BA degree in Computer Science, Physics, Mathematics, Statistics or other Engineering disciplines', ' Design and build mission critical data pipelines with a highly scalable distributed architecture - including data ingestion (streaming, events and batch), data integration, data curation Help continually improve ongoing reporting and analysis processes, simplifying self-service support for business stakeholders Build and support reusable framework to ingest, integration and provision data Automation of end to end data pipeline with metadata, data quality checks and audit  Build and support a big data platform on the cloud Define and implement automation of jobs and testing Optimize the data pipeline to support ML workloads and use cases Support mission critical applications and near real time data needs from the data platform Capture and publish metadata and new data to subscribed users Work collaboratively with business analysts, product managers, data scientists as well as business partners and actively participate in design thinking session Participate in design and code reviews Motivate, coach, and serve as a role model and mentor for other development team associates/members that leverage the platform  ', 'Motivate, coach, and serve as a role model and mentor for other development team associates/members that leverage the platform ', 'We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. This job description is intended to convey information essential to understanding the scope of the job and the general nature and level of work performed by job holders within this job. However, this job description is not intended to be an exhaustive list of qualifications, skills, efforts, duties, responsibilities or working conditions associated with the position. StockX reserves the right to amend this job description at any time.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Hinge Health,"Portland, OR",4 weeks ago,Be among the first 25 applicants,"['', 'Lead migration of the legacy data environment to cloud data services', 'Design and build the structures necessary for the Business Intelligence team to work autonomously', 'Participate in hiring and mentoring of team members', 'Monthly wellness benefit', 'Trust', 'Create tooling to automate data compliance', 'BONUS POINTS', 'Establish data governance tools and practices in a HIPAA environment', 'Programming expertise with Python, with demonstrated knowledge of software engineering best-practice development (e.g. linting, testing)', 'Ability to gather and correlate data across disparate sources and file formats', ""What We're Looking For"", 'We celebrate diversity and are committed to creating an inclusive environment for all employees.', 'Prior DBA experience', 'Excellent communication skills, both written and verbal', 'Learn-it-all (vs know-it-all): We’re always willing to learn. ', '401K match ', 'Prior experience with healthcare data (PHI/PII/HIPAA requirements)', 'FSA & HSA accounts', 'Mastery of SQL', '3 months paid parental leave', 'Professional Development budget ', 'Expertise in Go, Java, or RubyPrior DBA experiencePrior experience with healthcare data (PHI/PII/HIPAA requirements)Prior experience working with KubernetesExtensive NoSQL knowledgeBig Data technologies', 'Family & fertility benefit through Maven Clinic', 'Create the strategy, tooling, processes, and coaching that enables service and application teams to take full ownership of their data in a growing organizationLead migration of the legacy data environment to cloud data servicesCreate scalable tools and processes to propagate OLTP data to the data warehouseParticipate in hiring and mentoring of team membersEstablish data governance tools and practices in a HIPAA environmentCreate tooling to automate data complianceDesign and build the structures necessary for the Business Intelligence team to work autonomouslyAssist and coach teams to optimize poorly performing queries or overly-complicated modelsWork with SRE team establish best practices around database monitoring, alerting and availability', ""What You'll Love About Us"", 'Generous mental health stipend', 'Trust: We trust our teammates to always act in the team and company’s best interest. Hustle: We’re creative, we’re unrelenting, we find a way.Effective communication: We’re prompt and concise. Learn-it-all (vs know-it-all): We’re always willing to learn. Frugal: We don’t waste money and especially not time. ', '6 years of software engineering experience, 4+ in data engineering', 'Big Data technologies', ""What You'll Accomplish"", 'Competitive compensation with meaningful stock optionsMedical, Dental, Vision, Disability and Life Insurance (We cover 100% of your premium and 75% for your dependents) FSA & HSA accountsFamily & fertility benefit through Maven Clinic401K match 3 months paid parental leaveProfessional Development budget Monthly wellness benefitGenerous mental health stipendNoise-cancelling headphonesWork from home policyOpportunity to join a fantastically talented, diverse, and passionate team at a pivotal time in the company’s lifecycle', 'Noise-cancelling headphones', 'Hustle', 'Ability to collaborate and problem solve across teams', 'Work from home policy', 'Creation of ETL and data pipelines using code', 'Frugal: We don’t waste money and especially not time. ', 'Extensive NoSQL knowledge', 'Competitive compensation with meaningful stock options', 'Opportunity to join a fantastically talented, diverse, and passionate team at a pivotal time in the company’s lifecycle', 'Work with SRE team establish best practices around database monitoring, alerting and availability', 'Medical, Dental, Vision, Disability and Life Insurance (We cover 100% of your premium and 75% for your dependents) ', 'Create scalable tools and processes to propagate OLTP data to the data warehouse', 'Familiarity with the command line and Docker containers', 'What Shapes Our Company', 'Effective communication: We’re prompt and concise. ', 'Effective communication', 'Trust: We trust our teammates to always act in the team and company’s best interest. ', 'OLTP and data warehouse modeling best practices', 'Prior experience working with Kubernetes', 'Frugal: ', 'Create the strategy, tooling, processes, and coaching that enables service and application teams to take full ownership of their data in a growing organization', 'Hustle: We’re creative, we’re unrelenting, we find a way.', 'Learn-it-all (vs know-it-all)', 'Expertise in Go, Java, or Ruby', 'Assist and coach teams to optimize poorly performing queries or overly-complicated models', 'Mastery of SQLOLTP and data warehouse modeling best practicesProgramming expertise with Python, with demonstrated knowledge of software engineering best-practice development (e.g. linting, testing)Creation of ETL and data pipelines using codeFamiliarity with the command line and Docker containersAbility to gather and correlate data across disparate sources and file formatsAbility to collaborate and problem solve across teamsExcellent communication skills, both written and verbal6 years of software engineering experience, 4+ in data engineeringBachelor’s degree in C.S. or equivalent experience', 'Bachelor’s degree in C.S. or equivalent experience']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,PRI Global,"St Louis, MO",2 days ago,67 applicants,"['', '  SQL (Oracle, MSSQL, MySQL, etc) and NoSQL (Mongo)', 'Day to Day Responsibilities of this Position and Description of Project:\xa0Project involves defining required data types, identifying and tracing data between source and destination systems for a regulatory related initiative.\xa0Designing architecture of a given platform, documenting data flow, performing complex data analysis, and ETL.', '2+ years of experience in Computer Engineering, Software Development, System Administration, Linux Administration, DevOps and Site Reliability Engineering. ', '   Hands-on experience with ETL tools & automation', 'Education/Experience:', ' ', ""Bachelor's degree in Computer Science, Computer Engineering, Software Engineering, related field or equivalent experience. Master’s degree in Computer Science or Computer Engineering preferred. "", '100% remote', '   Scripting languages such as Python', 'Experience with the following programs/platforms is preferred: Streaming Frameworks - Apache Spark, Apache Storm, Kafka K-Streams, Apache Samza, Apache Flink, Apache Apex, Apache Nifi and Apache Pulsar; Storage Technologies - MongoDB, Oracle, Teradata, Hadoop, DocumentDB, SQL Server, Volt DB, Apache Ignite, CockroachDB, Amazon Aurora and Redis; Linux – RedHat; Continuous Integration and Deployment – CloudBees, Jenkins, Ansible Tower, Puppet and Chef.', ""Bachelor's degree in Computer Science, Computer Engineering, Software Engineering, related field or equivalent experience. Master’s degree in Computer Science or Computer Engineering preferred. 2+ years of experience in Computer Engineering, Software Development, System Administration, Linux Administration, DevOps and Site Reliability Engineering. Experience with the following programs/platforms is preferred: Streaming Frameworks - Apache Spark, Apache Storm, Kafka K-Streams, Apache Samza, Apache Flink, Apache Apex, Apache Nifi and Apache Pulsar; Storage Technologies - MongoDB, Oracle, Teradata, Hadoop, DocumentDB, SQL Server, Volt DB, Apache Ignite, CockroachDB, Amazon Aurora and Redis; Linux – RedHat; Continuous Integration and Deployment – CloudBees, Jenkins, Ansible Tower, Puppet and Chef."", '   Strong understanding of data modeling, algorithms, and data transformation techniques', '   3+ years of experience in Computer Engineering, Software Development']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-03-18 14:34:51
"Software Engineer, Data",Knock,"Seattle, WA",1 day ago,Be among the first 25 applicants,"['', 'We encourage you to apply even if you don’t have every listed requirement. ', 'Have experience building data pipelines and utilizing programming tools to do so. Here Knock we use Apache Spark, Scala, Rust, Go, Python, and Rest APIs - but you can learn these technologies on the job', 'You should be versed in developing APIs to serve data produced by ETL jobs', 'This position is in the continental United States.', ' Knock is a 100% remote, work from home culture and has been since our inception in 2015  100% employee covered medical, dental, & vision premiums  Unlimited PTO (2 weeks mandatory) + flexible work schedules  Paid parental leave  $1,000 each year for education, training, and professional development  Salary + Equity. We offer equitable compensation based market data, accomplishments, experience, & your location ', 'Design data schemas and optimize internal data warehouses, augmenting data from multiple sources.', 'Design, build, and maintain REST APIs to serve data to customers', 'All of our teams have a strong customer-first mindset and data-driven approach to our work, and that should be your approach as well', '$1,000 each year for education, training, and professional development ', 'Unlimited PTO (2 weeks mandatory) + flexible work schedules ', '100% employee covered medical, dental, & vision premiums ', 'Build data pipelines and aggregate data.', ' Have experience building data pipelines and utilizing programming tools to do so. Here Knock we use Apache Spark, Scala, Rust, Go, Python, and Rest APIs - but you can learn these technologies on the job You should be versed in developing APIs to serve data produced by ETL jobs You have a desire to work at a rapidly growing startup and make it a success, and are comfortable learning new technologies and tools Your experience in SQL, MySQL, or Postgres will be valuable here to identify slow queries and debugging All of our teams have a strong customer-first mindset and data-driven approach to our work, and that should be your approach as well Believe in creating diverse, equitable, and inclusive practices and programs that will further Knock’s commitment to making an impact, learning, putting people first, being open, and courageous. Have proven success working 100% remote in prior positions & are experienced working with a distributed, national team.  We encourage you to apply even if you don’t have every listed requirement.  ', 'Salary + Equity. We offer equitable compensation based market data, accomplishments, experience, & your location', 'Monitor and troubleshoot operational or data issues in the data pipelines.', 'Knock is a 100% remote, work from home culture and has been since our inception in 2015 ', 'Be committed to good engineering practice of testing, logging, alerting and deployment processes.', 'Believe in creating diverse, equitable, and inclusive practices and programs that will further Knock’s commitment to making an impact, learning, putting people first, being open, and courageous.', 'Understand the data that powers our applications, and be able to propose appropriate data models for new features.', 'Benefits, Perks, & Enjoying Life', ' Build data pipelines and aggregate data. Design data schemas and optimize internal data warehouses, augmenting data from multiple sources. Design, build, and maintain REST APIs to serve data to customers Cross-functionally collaborate with our Data Science and Machine Learning teams.  Understand the data that powers our applications, and be able to propose appropriate data models for new features. Be committed to good engineering practice of testing, logging, alerting and deployment processes. Monitor and troubleshoot operational or data issues in the data pipelines. Drive architectural plans and implementation for future data storage, reporting, and analytic solutions. ', 'Drive architectural plans and implementation for future data storage, reporting, and analytic solutions.', 'Please no recruitment firm or agency inquiries, you will not receive a reply from us.', 'Cross-functionally collaborate with our Data Science and Machine Learning teams. ', 'Paid parental leave ', 'You have a desire to work at a rapidly growing startup and make it a success, and are comfortable learning new technologies and tools', 'Have proven success working 100% remote in prior positions & are experienced working with a distributed, national team. ', 'Your experience in SQL, MySQL, or Postgres will be valuable here to identify slow queries and debugging']",Mid-Senior level,Full-time,Engineering,Internet,2021-03-18 14:34:51
Data Engineer,Literati,"Austin, TX",3 weeks ago,195 applicants,"['', 'Coaching and teaching others how to do great data engineering', 'Innovating on ways to build automated pipelines and transformations to keep pace with the businesses growth', 'Working collaboratively with analytics team to build a self-service analytics infrastructure', 'Being part of the team, doing what’s needed for the company to succeed', 'Reworking existing data structures into more logical and scalable forms', 'Building data objects, procedures, and data streams in Snowflake ', 'Qualifications', 'Building new tables, schemas in the data warehouse to increase consistency and accuracy of the businesses reporting and analysis', 'Company Description', 'Solid ability to design an enterprise data warehouse model, expert knowledge a strong plus', 'Expert knowledge in SQL, and in python and/or javascript', 'Expert knowledge in SQL, and in python and/or javascriptExtensive knowledge in Snowflake and FiveTranSolid ability to design an enterprise data warehouse model, expert knowledge a strong plusPassion towards making others better', 'You’re Good At', 'WE’RE GOOD AT:', 'Data Engineer', 'Building data pipelines from various internal and external sources using FiveTran and custom python ', 'You Will Need', 'Working collaboratively with analytics team to build a self-service analytics infrastructureBuilding new tables, schemas in the data warehouse to increase consistency and accuracy of the businesses reporting and analysisBuilding new pipelines to bulk up the data lake for greater analyticsInnovating on ways to build automated pipelines and transformations to keep pace with the businesses growth', 'Your Day-to-day Will Look Like', 'Job Description', 'Architecting great data structures in various forms (3NF, Dimensional, semi-structured)', 'Building data pipelines from various internal and external sources using FiveTran and custom python Architecting great data structures in various forms (3NF, Dimensional, semi-structured)Reworking existing data structures into more logical and scalable formsBuilding data objects, procedures, and data streams in Snowflake Coaching and teaching others how to do great data engineeringBeing part of the team, doing what’s needed for the company to succeed', 'Extensive knowledge in Snowflake and FiveTran', 'Building new pipelines to bulk up the data lake for greater analytics', 'Passion towards making others better']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Empower Professionals Inc,"Malvern, PA",3 days ago,Be among the first 25 applicants,[''],Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Orchard,New York City Metropolitan Area,6 days ago,117 applicants,"['', ""We're proud to be recognized by Inc. Magazine, Fast Company and Forbes as one of the best workplaces of 2020. We also have a 4.9 Glassdoor rating! Orchard is building the first one-stop-shop in real estate and we’re bringing together the most innovative professionals across real estate, business, marketing, technology and design. We also have some pretty great perks:"", 'Design, create, and continuously improve upon reports, dashboards, etc. to help company stakeholders measure performance and make informed decisions; closely collaborate with functional group leads throughout the reporting lifecycle, from initial scoping to deployment to maintenance', 'Exceptional dimensional data modeling skills. Knowledgeable about data warehouse technical architecture, ETL frameworks, and OLAP principles.', 'Spearhead business intelligence infrastructure efforts, including owning the design and implementation of a modular, parallelized codebase of SQL scripts necessary for long-term business intelligence scalabilityDive into and deeply understand new data sources and their underlying data libraries to transform, integrate, and make them accessible for self-directed analysis by stakeholders across the business, including team leads from sales, marketing, product, and operationsDesign, create, and continuously improve upon reports, dashboards, etc. to help company stakeholders measure performance and make informed decisions; closely collaborate with functional group leads throughout the reporting lifecycle, from initial scoping to deployment to maintenance', 'Orchard is radically simplifying the way people buy and sell their homes. For the average American, the home purchase and sale process takes months, creates anxiety, and is filled with uncertainty and hassle. Orchard has reimagined the end-to-end experience of buying and selling, from innovative home search tools to find the perfect home to the ability to buy a new home before selling your current one. Orchard customers manage the entire experience through a personalized online dashboard, while also getting the support of best-in-class Orchard real estate agents.\xa0', 'Why Orchard', 'About the Role', 'Equity participationFlexible PTOUp to 18 weeks of paid family leaveEmployee discount on Orchard’s services', 'Spearhead business intelligence infrastructure efforts, including owning the design and implementation of a modular, parallelized codebase of SQL scripts necessary for long-term business intelligence scalability', 'Experience driving fast-paced projects from scratch to completion (e.g. building a new code base to tackle a complex problem) in a highly organized manner', '4+ years of experience in a business intelligence, analytics, data science or engineering role. Experience working at a high-growth technology company a plus.BA/BS degree in a quantitative discipline (Computer Science, Math, Statistics, Physics or Engineering)Experience working in SQL and Python. Familiarity with Postgres, Redshift, Airflow or Looker is a plus.Exceptional dimensional data modeling skills. Knowledgeable about data warehouse technical architecture, ETL frameworks, and OLAP principles.Experience prioritizing, building, and deploying code using business intelligence reporting tools; Looker proficiency a plus.Experience driving fast-paced projects from scratch to completion (e.g. building a new code base to tackle a complex problem) in a highly organized manner', 'Experience working in SQL and Python. Familiarity with Postgres, Redshift, Airflow or Looker is a plus.', 'As a Data Engineer, you will be involved in creating, updating and maintaining the full business intelligence stack. Focusing on building scalable back-end data architecture but also producing high impact front-end dashboards. Working closely with the Product and Technology teams, you will extract and transform data from new products and build & own the analytics layer of the company’s data environment to support our business intelligence tooling. You will help lead the charge to surface critical data to end users and enable leadership to make fast, data-informed decisions.\xa0', 'Equity participation', 'Dive into and deeply understand new data sources and their underlying data libraries to transform, integrate, and make them accessible for self-directed analysis by stakeholders across the business, including team leads from sales, marketing, product, and operations', 'Flexible PTO', 'This is a full-time role reporting to our Head of Engineering, and will be based out of our New York City office.', 'What You’ll Do Here:', 'Experience prioritizing, building, and deploying code using business intelligence reporting tools; Looker proficiency a plus.', 'About Orchard', 'We’d Love to Hear From You if You Have:', 'Employee discount on Orchard’s services', '4+ years of experience in a business intelligence, analytics, data science or engineering role. Experience working at a high-growth technology company a plus.', 'Headquartered in New York City and with offices in Austin, Denver and Atlanta, Orchard has over 200 employees and has grown 10x year over year. We have raised over $130 million in equity financing from top-tier investors including Revolution, Firstmark, Accomplice, Navitas and Juxtapose. Our investors have also backed the likes of Pinterest, AirBnb, Shopify and Sweetgreen.', 'Up to 18 weeks of paid family leave', 'Orchard is proud to be an equal opportunity employer. We provide employment opportunities without regard to age, race, color, ancestry, national origin, religion, disability, sex, gender identity or expression, sexual orientation, veteran status, or any other protected status in accordance with applicable law.', 'BA/BS degree in a quantitative discipline (Computer Science, Math, Statistics, Physics or Engineering)']",Associate,Full-time,Engineering,Real Estate,2021-03-18 14:34:51
Data Engineer,W2O Group,"Austin, TX",2 days ago,39 applicants,"['', 'Extensive, demonstrated expertise with Python. should be generally comfortable in different categories (machine learning, development, scripting, etc.)', 'Hands on experience leading enterprise-wide data engineering, warehousing and analytics projects', '5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative fieldAdvanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.Extensive, demonstrated expertise with Python. should be generally comfortable in different categories (machine learning, development, scripting, etc.)Experience in building real time streaming data ingestion and processing pipeline using Apache Beam (running on either Google Dataflow or Apache (Apex Flink, or Spark) or Kafka in an analytics or data science use caseBuild processes supporting data transformation, data structures, metadata, dependency and workload management.Strong analytic skills related to working with unstructured datasets.Hands on experience leading enterprise-wide data engineering, warehousing and analytics projectsExperience with data pipeline and workflow management tools: Airflow, Oozie etc.Ability to think strategically about business, product, and technical challenges in an enterprise environmentUnderstanding of database and analytical technologies in the industry including MPP and NoSQL databases, Data Warehouse design, PostgreSQL, BI reporting and Dashboard developmentDemonstrated industry efficiency in the fields of database, data warehousing or data sciencesCustomer facing skills with the ability to drive discussions with senior leadership regarding trade-offs, best practices and risk mitigationDesire and ability to interact with all levels of the organization', 'Own the Insights gleaned by the creation of advanced technology roadmaps. Share real world implementations and recommend new capabilities that would simplify adoption and drive greater value.', 'Strong analytic skills related to working with unstructured datasets.', 'Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.', 'Design and implement data ingestion solutions on GCP using GCP native services ', 'Responsibilities', 'Experience in building real time streaming data ingestion and processing pipeline using Apache Beam (running on either Google Dataflow or Apache (Apex Flink, or Spark) or Kafka in an analytics or data science use case', '5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field', 'Build processes supporting data transformation, data structures, metadata, dependency and workload management.', 'Innovate and engage with key technology stakeholders to create a compelling vision of a data-driven enterprise environment and the impact it will have on their teams, their projects and their outcomes.Requirements5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative fieldAdvanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.Extensive, demonstrated expertise with Python. should be generally comfortable in different categories (machine learning, development, scripting, etc.)Experience in building real time streaming data ingestion and processing pipeline using Apache Beam (running on either Google Dataflow or Apache (Apex Flink, or Spark) or Kafka in an analytics or data science use caseBuild processes supporting data transformation, data structures, metadata, dependency and workload management.Strong analytic skills related to working with unstructured datasets.Hands on experience leading enterprise-wide data engineering, warehousing and analytics projectsExperience with data pipeline and workflow management tools: Airflow, Oozie etc.Ability to think strategically about business, product, and technical challenges in an enterprise environmentUnderstanding of database and analytical technologies in the industry including MPP and NoSQL databases, Data Warehouse design, PostgreSQL, BI reporting and Dashboard developmentDemonstrated industry efficiency in the fields of database, data warehousing or data sciencesCustomer facing skills with the ability to drive discussions with senior leadership regarding trade-offs, best practices and risk mitigationDesire and ability to interact with all levels of the organizationW2O Group offers a comprehensive benefit program and perks, including flexible PTO, expanded paid leave for new parents including a 4th Trimester program that helps new parents transition back to work, and a five-week sabbatical program. Other perks include Income Protection, Retirement plans/401(k) match, and cell phone savings plans. Learn more about our great benefits and perks at: http://www.w2ogroup.com/W2O Group is an Equal Opportunity Employer. We foster an environment that embraces diversity. We are stronger with a wider range of opinions, strengths, and backgrounds to achieve our goals.W2O Group offers a comprehensive benefit program and perks, including flexible PTO, expanded paid leave for new parents including a 4th Trimester program that helps new parents transition back to work, and a five-week sabbatical program. Other perks include Income Protection, Retirement plans/401(k) match, and cell phone savings plans. Learn more about our great benefits and perks at: http://www.w2ogroup.com/W2O Group is committed to being an Equal Opportunity employer. As such, we seek motivated and qualified applicants without regard to race, color, religion, sex (including pregnancy), sexual orientation, gender identity/expression, ethnic or national origin, age, physical or mental disability, genetic information, marital information, or any other characteristic protected by federal, state, or local employment discrimination laws where W2O operates. We strive to employ, motivate, advance and reasonably accommodate any qualified employees and applicants. We believe diversity of persons and ideas forms the most comprehensive, forward-looking company.', 'Demonstrated industry efficiency in the fields of database, data warehousing or data sciences', 'Demonstrated experience with distributed computing', 'Requirements', 'Delivery of cloud architecture to support new distributed computing solutions that often span the full array of cloud services. This will include migration of existing applications and development of new applications using cloud services.', 'Customer facing skills with the ability to drive discussions with senior leadership regarding trade-offs, best practices and risk mitigation', 'Design and implement data ingestion solutions on GCP using GCP native services Demonstrated experience with distributed computingDesign and optimize data models on GCP cloud using GCP data stores such as BigQueryIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.Delivery of cloud architecture to support new distributed computing solutions that often span the full array of cloud services. This will include migration of existing applications and development of new applications using cloud services.Own the Insights gleaned by the creation of advanced technology roadmaps. Share real world implementations and recommend new capabilities that would simplify adoption and drive greater value.', 'Experience with data pipeline and workflow management tools: Airflow, Oozie etc.', 'Understanding of database and analytical technologies in the industry including MPP and NoSQL databases, Data Warehouse design, PostgreSQL, BI reporting and Dashboard development', 'Role', '4th Trimester', 'Desire and ability to interact with all levels of the organization', 'Design and optimize data models on GCP cloud using GCP data stores such as BigQuery', 'Ability to think strategically about business, product, and technical challenges in an enterprise environment', 'Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.', 'Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Senior Data Engineer,Atyeti Inc,New York City Metropolitan Area,2 days ago,27 applicants,"['We are looking for a Senior Data Engineer for one of our Investment Banking Client - This is a long term contract.', '', 'Job Overview:', 'Must have expertise on RDBMS solutions such as Oracle, SQL Server, Azure SQL DB etc..', 'Please email me your updated resume along with your salary/rate expectation to sangitha.ganesh(at)atyeti.com/ 919 583 2695', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Excellent Python programming skills and experience delivering projects using PySpark is a must. Scala programming experience is nice to have.', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience using Cloud data warehouse technologies such as Snowflake is highly desired', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience leveraging Azure Storage and Data Lake Store Gen 2 in Azure based big data projects along with\xa0integration to other Azure Data services', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience designing and developing Airflow DAGs, Operators etc.…', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Demonstrable experience designing and developing big data applications using Apache Spark and Airflow', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience leveraging Databricks to develop Spark based applications along with very good understanding about data engineering capabilities provided by the service', '•\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience designing and developing real time data processing applications using Spark streaming, Kafka, Event Hub etc.. ']",Not Applicable,Contract,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,DPR Construction,"Richmond, VA",2 days ago,Be among the first 25 applicants,"['', 'Secure the movement of sensitive information in a manner consistent with company policy and management expectations', 'Control integration quality and develop ways to detect and correct anomalies with data exchange', ' DPR has been nationally recognized for its strong company culture, based on a well-defined purpose “We Exist to Build Great Things,” and four core values: integrity, enjoyment, uniqueness and ever forward. A flat, title-less organization that empowers people at all levels to make decisions, DPR ranked on FORTUNE’s “100 Best Companies to Work For” list for five consecutive years. For more information, visit http://www.dpr.com .', 'Create and maintain optimal data pipeline architecture', 'Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.', 'Enable data access, data processing, and data products by architecting, maintaining, scaling, monitoring and securing Data Warehouse, EL & ETL system, and data pipelines and BI systems', 'Ability to understand, consume and use API’s, JSON, Webservices for Data pipelines.', 'Experience in Software development.', 'Assemble large, complex data sets that meet functional / non-functional business requirements.', ' to success in this role', 'Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.', 'Define and lead API integration strategies and for the enterprise', 'Identify, design, and implement internal process improvements, automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.', 'Ability to work effectively with others who are in remote locations and varying time zones', 'Motivation to continually learn and take on added responsibilities while maintaining a positive attitude', 'Work with data and analytics experts to strive for greater functionality in our data systems.', 'Seek and Embrace Change – Continuously improve work processes rather than accepting the status quo', 'Position Summary', 'Responsibilities', 'Strong with SQL development knowledge for Relational Databases', 'Growth and Development – Know or learn what is needed to deliver results and successfully compete', 'Implement enterprise integrations that result in a scalable, flexible, and highly available solutions that perform well under high traffic', 'Excellent knowledge of EL and ELT, Datawarehousing, and cloud-based tools', 'Business and Technical Analysis skills', 'Qualifications', 'Create and maintain optimal data pipeline architectureAssemble large, complex data sets that meet functional / non-functional business requirements.Enable data access, data processing, and data products by architecting, maintaining, scaling, monitoring and securing Data Warehouse, EL & ETL system, and data pipelines and BI systemsIdentify, design, and implement internal process improvements, automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.Keep our data separated and secureCreate data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.Work with data and analytics experts to strive for greater functionality in our data systems.Define and lead API integration strategies and for the enterpriseImplement enterprise integrations that result in a scalable, flexible, and highly available solutions that perform well under high trafficSecure the movement of sensitive information in a manner consistent with company policy and management expectationsControl integration quality and develop ways to detect and correct anomalies with data exchange', 'Resourceful creative approach to problem-solving is expected', 'Key', 'Strong communication skills, with the ability to work both independently and in project teams', 'Keep our data separated and secure', 'Ability to adapt quickly to change & deep curiosity to learn new tools and technologies and apply them', 'Experience in scripting languages like Batch, Shell in Unix environment', 'Solid understanding of database engineering and design (Relational, De-normalized, Data Lakes, etc.)Knowledge of AWS and Azure platformsExperience in Software development.Ability to understand, consume and use API’s, JSON, Webservices for Data pipelines.Excellent knowledge of EL and ELT, Datawarehousing, and cloud-based toolsStrong with SQL development knowledge for Relational DatabasesBusiness and Technical Analysis skillsExperience in scripting languages like Batch, Shell in Unix environmentExperience with integration of data from multiple data sources like API’s, JSON and any other databases, Flat-files, Spreadsheets.Experience in Data Mapping, XML/JSON, and web service', 'Experience with integration of data from multiple data sources like API’s, JSON and any other databases, Flat-files, Spreadsheets.', 'Experience in Data Mapping, XML/JSON, and web service', 'Solid understanding of database engineering and design (Relational, De-normalized, Data Lakes, etc.)', 'Knowledge of AWS and Azure platforms', 'Job Description', 'Ability to adapt quickly to change & deep curiosity to learn new tools and technologies and apply themAbility to work with and collaborate across the team and work effectively with others to identify the impact on the company’s business processes, other applications, network, etc.Strong analytical and problem-solving abilities.Seek and Embrace Change – Continuously improve work processes rather than accepting the status quoGrowth and Development – Know or learn what is needed to deliver results and successfully competeAbility to work effectively with others who are in remote locations and varying time zonesResourceful creative approach to problem-solving is expectedStrong communication skills, with the ability to work both independently and in project teamsMotivation to continually learn and take on added responsibilities while maintaining a positive attitude', 'Ability to work with and collaborate across the team and work effectively with others to identify the impact on the company’s business processes, other applications, network, etc.', 'Strong analytical and problem-solving abilities.', 'Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.', 'Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.']",Entry level,Full-time,Information Technology,Construction,2021-03-18 14:34:51
Data Engineer - REMOTE,Jobot,"New York, NY",2 days ago,Be among the first 25 applicants,"['', ' We believe that kindness is still completely relevant.', ' Competitive Base Salary!', 'A Bit About Us', ' At least 1 year of web development experience', ' We take the job very seriously but do not take ourselves seriously.', 'Are you a data driven decision maker? Can you help bridge the gap between software engineering and data analysis? Come join us!', ' At least 3 years of experience building data pipelines in Python, Spark, or similar technologies', 'Job Details', ' Accelerated Career Growth!', ' Competitive Base Salary! Extremely Fun and Passionate Culture! Flexible Work Schedules! Accelerated Career Growth!', ' Extremely Fun and Passionate Culture!', ' Flexible Work Schedules!', ' We believe transparency and a strong team brings the best results for everyone.', ' At least 3 years of experience building data pipelines in Python, Spark, or similar technologies At least 2 years of experience with a analytics dashboard, e.g. Tableau, Looker, etc. At least 1 year of web development experience Experience with AWS, specifically Redshift, is a plus', ' We take the job very seriously but do not take ourselves seriously. We believe that kindness is still completely relevant. We believe transparency and a strong team brings the best results for everyone. Our Get a Job Give a Job program helps increase employment across the globe.', ' At least 2 years of experience with a analytics dashboard, e.g. Tableau, Looker, etc.', ' Experience with AWS, specifically Redshift, is a plus', 'Why join us?', ' Our Get a Job Give a Job program helps increase employment across the globe.']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Amazon Web Services (AWS),"Herndon, VA",5 days ago,36 applicants,"['', ' Bachelor’s Degree in Computer Science, Information Systems, Data Analytics, or related technical/engineering field', ' Take ownership of data reliability by, among other things, performing deep-dives to find root causes of potential data anomalies, and taking subsequent action to address these anomalies.', ' Collaborate with Business Intelligence Engineering team members, engineering stakeholders, partner technical teams, and business stakeholders, to gather business and functional requirements, and translate these requirements into a robust, scalable, and operable data infrastructure that works well within the overall AWS data architecture, and leads to improved engineering decisions.', ' Meets/exceeds Amazon’s functional/technical depth and complexity for this role', 'DESCRIPTION', ' 2+ years of experience with Python or other relevant scripting language', ' 1+ years of experience in preparing data for direct use in visualization tools, such as Salesforce, Tableau, or Amazon QuickSight', ' Insist on the highest standards by recognizing and adopting best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation.', 'Responsibilities', ' Continuously optimize the performance of data queries, and address extract, transform, load (ETL) procedures.', ' 1+ years of experience with the AWS tech stack – Glue, Redshift, EMR, S3, EC2, and Lambda will be used regularly in this role', 'Description', ' 1+ years of experience with Data Architecture and Design', ' Experience in documenting technical/data systems for technical and business leaders', ' Develop a deep understanding of our vast data sources, and provide continuous recommendations for use to solve specific business problems.', ' Proficient in Scala/Spark/Hadoop', ' Experience working with data scientists on research and machine learning problems', ' 2+ years of experience with data warehouse technical architecture, infrastructure components, and extract, transform, load (ETL) procedure', 'Company', ' Develop a deep understanding and awareness of operational data from the AWS fleet, and build mechanisms for retrieving and aggregating such data for use by downstream business intelligence solutions.', ' Collaborate with Business Intelligence Engineering team members, engineering stakeholders, partner technical teams, and business stakeholders, to gather business and functional requirements, and translate these requirements into a robust, scalable, and operable data infrastructure that works well within the overall AWS data architecture, and leads to improved engineering decisions. Develop a deep understanding and awareness of operational data from the AWS fleet, and build mechanisms for retrieving and aggregating such data for use by downstream business intelligence solutions. Develop a deep understanding of our vast data sources, and provide continuous recommendations for use to solve specific business problems. Take ownership of data reliability by, among other things, performing deep-dives to find root causes of potential data anomalies, and taking subsequent action to address these anomalies. Continuously optimize the performance of data queries, and address extract, transform, load (ETL) procedures. Insist on the highest standards by recognizing and adopting best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation.', 'Basic Qualifications', ' Expert-level knowledge of SQL', ' Be self-driven, detail-oriented, and show ability to deliver on ambiguous projects with incomplete or dirty data', ' Meets/exceeds Amazon’s leadership principles requirements for this role', ' Expert-level knowledge of SQL Proficient in Scala/Spark/Hadoop Experience in documenting technical/data systems for technical and business leaders Experience working with data scientists on research and machine learning problems Be self-driven, detail-oriented, and show ability to deliver on ambiguous projects with incomplete or dirty data Meets/exceeds Amazon’s leadership principles requirements for this role Meets/exceeds Amazon’s functional/technical depth and complexity for this role', ' 3+ years Structured Query Language (SQL) experience', ' Bachelor’s Degree in Computer Science, Information Systems, Data Analytics, or related technical/engineering field 3+ years Structured Query Language (SQL) experience 2+ years of experience with Python or other relevant scripting language 2+ years of experience with data warehouse technical architecture, infrastructure components, and extract, transform, load (ETL) procedure 1+ years of experience with the AWS tech stack – Glue, Redshift, EMR, S3, EC2, and Lambda will be used regularly in this role 1+ years of experience with Data Architecture and Design 1+ years of experience in preparing data for direct use in visualization tools, such as Salesforce, Tableau, or Amazon QuickSight', 'Preferred Qualifications']",Not Applicable,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Engineer,Launch Consulting Group,"Bellevue, WA",1 week ago,97 applicants,"['', 'Excellent communication skills both written and oral ', 'Bachelor’s Degree in Computer Sciences, Analytics, Systems Eng., Statistics or related field ', 'Experience with Teradata', 'Required Experience/Qualifications', 'Help continually improve ongoing data platform, analysis and reporting processes, automating or simplifying self-service support for customers ', 'Design and implement data load processes from Cloud or On Premises data sources into Azure & Snowflake Cloud solutions. ', 'Responsibilities', 'Migrate existing processes and data from On Premises SQL Server and other environments to Azure and/or Snowflake storage solutions coupled with Azure or AWS compute. ', 'Collaborate with Data Scientists on Data Provenance and Lineage. Design & implement processes/tasks with a focus on data quality, master data management, and data lineage as components of the overall data ecosystem. ', 'We are Navigators in the Age of Transformation.', 'Participate in implementation, and support of a data warehouse/data base and analytics platform utilizing Azure cloud technology', 'Experience with Snowflake and Matillion orchestration ', '2+ years of Spark, Python and PowerShell development or equivalent ', 'Hands-on experience working with CICD technologies in the data engineering space ', 'Participate in data governance; ensure all existing data is created in the right way, and that new data is created according to appropriate standards and with proper documentation ', ' Participate in implementation, and support of a data warehouse/data base and analytics platform utilizing Azure cloud technology Design and implement data load processes from Cloud or On Premises data sources into Azure & Snowflake Cloud solutions.  Migrate existing processes and data from On Premises SQL Server and other environments to Azure and/or Snowflake storage solutions coupled with Azure or AWS compute.  Collaborate with Data Scientists on Data Provenance and Lineage. Design & implement processes/tasks with a focus on data quality, master data management, and data lineage as components of the overall data ecosystem.  Explore and learn the latest Cloud technologies to provide new capabilities and increase efficiency with a focus on Snowflake, Azure, and AWS. Work with top-notch technical professionals developing complex systems at scale and with a focus on sustained operational excellence  Collaborate with Data Analysts & Data Scientists; for products that require reporting data or AI/ML models to ensure that datasets are in place and are used consistently internally/externally  Participate in data governance; ensure all existing data is created in the right way, and that new data is created according to appropriate standards and with proper documentation  Read, write, and configure code for end-to-end service telemetry, alerting and self-healing capabilities  Strive for continuous improvement of code quality and development practices  Help continually improve ongoing data platform, analysis and reporting processes, automating or simplifying self-service support for customers  ', '3+ years of experience in working as an data engineering or analytics team member working with cross functional teams ', 'Strong attention to detail and sense of urgency ', 'Explore and learn the latest Cloud technologies to provide new capabilities and increase efficiency with a focus on Snowflake, Azure, and AWS.', 'Read, write, and configure code for end-to-end service telemetry, alerting and self-healing capabilities ', 'DATA ENGINEER', ' 3+ years of experience in working as an data engineering or analytics team member working with cross functional teams  3+ years of SQL DW/DB development or equivalent  3+ years of experience designing solutions in Azure specializing in any of the following technologies: Azure Data Factory, Azure Data Lake, Azure SQL & Azure SQL Data Warehouse, Azure Functions  Experience with Teradata Experience with Snowflake and Matillion orchestration  2+ years of Spark, Python and PowerShell development or equivalent  Bachelor’s Degree in Computer Sciences, Analytics, Systems Eng., Statistics or related field  Strong attention to detail and sense of urgency  Excellent communication skills both written and oral  Hands-on experience working with CICD technologies in the data engineering space  ', 'Collaborate with Data Analysts & Data Scientists; for products that require reporting data or AI/ML models to ensure that datasets are in place and are used consistently internally/externally ', 'Strive for continuous improvement of code quality and development practices ', '3+ years of SQL DW/DB development or equivalent ', '3+ years of experience designing solutions in Azure specializing in any of the following technologies: Azure Data Factory, Azure Data Lake, Azure SQL & Azure SQL Data Warehouse, Azure Functions ', 'Please refer to job #4356', 'Work with top-notch technical professionals developing complex systems at scale and with a focus on sustained operational excellence ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer - McKinsey Digital,McKinsey & Company,"New York, NY",4 weeks ago,135 applicants,"['', ""Bachelor's degree in computer science or engineering; master's degree preferredExperienced on Big Data platforms and tools like Hadoop, hbase, CouchDB, hive, Pig, Spark, etc.Experienced with data modeling, design patterns, building highly scalable and secured solutions, distributed systemsSolid grasp of ETL across various platformsKnowledge of agile software development process and familiarity with performance metric toolsKnowledge of web application development technologies such as Ruby on Rails, Java, UNIX, HTML, CSS, Perl, or PHP is a plusStrong analytical and problem-solving skills paired with the ability to develop creative and efficient solutions; tolerance in dealing with bad quality dataProficiency in visualization tool (Tableau, D3, etc.)Distinct customer focus and quality mindsetAbility to see from and sell to multiple viewpointsAbility to work at an abstract level and build consensus; ability to work both independently and with a teamAbility to work efficiently with a solid sense for setting prioritiesAbility to guide own learning and contribute to domain knowledge buildingExcellent interpersonal, leadership and communication skillsStrong command of English language (both verbal and written)Willingness to travel"", 'Experienced on Big Data platforms and tools like Hadoop, hbase, CouchDB, hive, Pig, Spark, etc.', 'Strong command of English language (both verbal and written)', 'Knowledge of web application development technologies such as Ruby on Rails, Java, UNIX, HTML, CSS, Perl, or PHP is a plus', 'Excellent interpersonal, leadership and communication skills', ""Bachelor's degree in computer science or engineering; master's degree preferred"", 'Solid grasp of ETL across various platforms', 'Proficiency in visualization tool (Tableau, D3, etc.)', 'Qualifications', 'Ability to work at an abstract level and build consensus; ability to work both independently and with a team', 'Distinct customer focus and quality mindset', 'Willingness to travel', ""Who You'll Work With"", 'Ability to see from and sell to multiple viewpoints', ""What You'll Do"", 'Ability to guide own learning and contribute to domain knowledge building', 'Strong analytical and problem-solving skills paired with the ability to develop creative and efficient solutions; tolerance in dealing with bad quality data', 'Experienced with data modeling, design patterns, building highly scalable and secured solutions, distributed systems', 'Ability to work efficiently with a solid sense for setting priorities', 'Knowledge of agile software development process and familiarity with performance metric tools']",Associate,Full-time,Consulting,Computer Software,2021-03-18 14:34:51
Sr Data Engineer (Remote),PRECISION SERVICES SYSTEM IT,"Colorado Springs, CO",21 hours ago,Be among the first 25 applicants,"['We never submit your resume without your permission, and we never give your resume to other staffing companies', 'Familiarity with big data technology including PDW and Hadoop', 'We are up front about pay rates and the interviewing process, and provide you with constant feedback. Our Responsibility. We take seriously our obligations to our consultants.', 'We offer highly competitive pay rates, as well as an opportunity to participate in our group health insurance, long term disability and 401(k) plans.', ""We actually understand your skills. We'll make sure that you are right for the job, and that the job is right for you. Our Reputation for Quality. Clients favor Precision's candidates."", 'Training in lean and agile delivery methodologies Please click the Apply Now button to apply for the job. We will review your resume and call if you are qualified. Resumes will NOT be sent to clients without your approval. REFERRALS WANTED', 'We have dedicated HR personnel that will answer the phone when you call. THE STAFFING FIRM YOU WORK WITH MATTERS. WORK WITH PRECISION. Job ID 4345DI-2355', ""We're the top provider of contract IT professionals to many of our clients Our Experience. We know how to sell our candidates to clients."", 'Experience working with a financial ERP product like Oracle, SAP or Workday Plusses', 'Familiarity with cloud data pipelines and products such as AWS, S3, Lambda, Snowflake', 'We know what it is like to be an IT consultant working in a large organization.', 'We take the time to learn about you so that we can best match you to our openings and sell you to our clients.', ""Our clients know from experience that Precision's consultants are the most qualified and the best fit for their project teams. Our Ethics. We pride ourselves on our ethical business practices."", ""We've been in business for over 20 years, and have made thousands of successful placements Our Background. Precision is run by former IT professionals."", 'We are direct vendors to Fortune 500 companies across North America', 'When required, we carefully manage H1-B and applications, and involve you at every step.', '1000 REWARD! Refer a colleague to us, and Precision will give you 1000 if we find a job for that person! (The fine print The referred candidate must be previously unknown to us. Start date must be within 6 months of referral.) THERE ARE MANY IT STAFFING FIRMS YOU CAN WORK WITH. WHY WORK WITH PRECISION? Our Clients. We work with the best.', 'We always pay on time', 'Experience building and supporting data pipelines and products using SQL, SSIS, SSAS, C, .Net']",Associate,Contract,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,"McCarthy Building Companies, Inc.","Omaha, NE",23 hours ago,Be among the first 25 applicants,"['', 'Participate in code and design review to ensure alignment to standards and best practices', 'Azure DevOps', 'Azure Data Factory (including pipelines and mapping & wrangling data flows)Azure SQL (nice to have: Azure Data Warehouse/Synapse)Azure Data Lake Gen2Azure DevOps', 'Mature understanding of data warehouse and data lake concepts and design (including star schemas)', 'Experience building data pipelines to ingest unstructured and streaming dataExperience preparing data for Data Science and Machine Learning use casesExperience with master data managementExperience designing reports using Power BI, Power Query, and DAX', 'Design, implement, and support ETL and ELT pipelines using Azure Data Factory pipelines and dataflows (both mapping and wrangling)', ' 5+ years of data solution delivery experience with 1+ years on the Microsoft Azure platform:Azure Data Factory (including pipelines and mapping & wrangling data flows)Azure SQL (nice to have: Azure Data Warehouse/Synapse)Azure Data Lake Gen2Azure DevOps', 'Preferred', 'Extensive experience querying API endpoints', 'Extensive experience with T-SQL (queries and DDL)', 'McCarthy is proud to be an equal opportunity and affirmative action employer regardless of race, color, gender, age, sexual orientation, gender identity, religious beliefs, marital status, genetic information, national origin, disability or protected veteran status.', 'Experience preparing data for Data Science and Machine Learning use cases', 'Experience with Agile/Scrum methodology preferred', 'Experience developing and promoting work through devops pipelines and utilizing source control, preferably GIT, and Visual Studio’s database tools', 'Required', 'Research, analyze, recommend and select technical approaches for solving challenging and complex development and integration problems', 'Experience building data pipelines to ingest unstructured and streaming data', 'Promote ADF and database objects through environments using Azure DevOps and Visual Studio', 'Excellent analytical, conceptual, and problem-solving abilities', 'Experience designing reports using Power BI, Power Query, and DAX', 'Azure Data Lake Gen2', 'Mature understanding of data warehouse and data lake concepts and design (including star schemas) 5+ years of data solution delivery experience with 1+ years on the Microsoft Azure platform:Azure Data Factory (including pipelines and mapping & wrangling data flows)Azure SQL (nice to have: Azure Data Warehouse/Synapse)Azure Data Lake Gen2Azure DevOpsExtensive experience with T-SQL (queries and DDL)Extensive experience querying API endpointsExperience M and Power Query data wranglingExperience developing and promoting work through devops pipelines and utilizing source control, preferably GIT, and Visual Studio’s database toolsExperience with Agile/Scrum methodology preferredExcellent analytical, conceptual, and problem-solving abilitiesEntrepreneurial attitude with a passion to deliver value for the organization and a desire to foster and develop team members ', 'Azure SQL (nice to have: Azure Data Warehouse/Synapse)', 'Create and run testing protocols for data solutions', 'Experience with master data management', 'Skills & QualificationsMature understanding of data warehouse and data lake concepts and design (including star schemas) 5+ years of data solution delivery experience with 1+ years on the Microsoft Azure platform:Azure Data Factory (including pipelines and mapping & wrangling data flows)Azure SQL (nice to have: Azure Data Warehouse/Synapse)Azure Data Lake Gen2Azure DevOpsExtensive experience with T-SQL (queries and DDL)Extensive experience querying API endpointsExperience M and Power Query data wranglingExperience developing and promoting work through devops pipelines and utilizing source control, preferably GIT, and Visual Studio’s database toolsExperience with Agile/Scrum methodology preferredExcellent analytical, conceptual, and problem-solving abilitiesEntrepreneurial attitude with a passion to deliver value for the organization and a desire to foster and develop team members PreferredExperience building data pipelines to ingest unstructured and streaming dataExperience preparing data for Data Science and Machine Learning use casesExperience with master data managementExperience designing reports using Power BI, Power Query, and DAXMcCarthy is proud to be an equal opportunity and affirmative action employer regardless of race, color, gender, age, sexual orientation, gender identity, religious beliefs, marital status, genetic information, national origin, disability or protected veteran status.', 'Key Responsibilities', 'Assure development work accords with best practices including security and data quality', 'Design, implement, and support modern data solutions with Azure Data Factory, Azure Data Lake, and Azure SQLDesign, implement, and support ETL and ELT pipelines using Azure Data Factory pipelines and dataflows (both mapping and wrangling)Create and run testing protocols for data solutionsPromote ADF and database objects through environments using Azure DevOps and Visual StudioAssure development work accords with best practices including security and data qualityParticipate in code and design review to ensure alignment to standards and best practicesResearch, analyze, recommend and select technical approaches for solving challenging and complex development and integration problems', 'Experience M and Power Query data wrangling', 'Design, implement, and support modern data solutions with Azure Data Factory, Azure Data Lake, and Azure SQL', 'Azure Data Factory (including pipelines and mapping & wrangling data flows)', 'Entrepreneurial attitude with a passion to deliver value for the organization and a desire to foster and develop team members ']",Entry level,Full-time,Information Technology,Construction,2021-03-18 14:34:51
Data Engineer (Remote) - Advanced Technology Group,Cognizant,"Overland Park, KS",2 days ago,Be among the first 25 applicants,"['', 'Work with project management to provide regular status updates to internal and external stakeholders.', 'Key Experience', 'Work collaboratively with a project and client team in an agile environment to support our cloud initiatives', 'Knowledge of Quote to Cash process (CPQ, Order Management, Billing, and CRM) a plus', '4-year degree in a technical or business field of study preferred', 'Willingness and ability to travel up to 50% (subject to change at any time based off project demands)', 'Leverage system and process analysis skills to produce data strategies that create a seamless and uninterrupted experience for our clients.', ' Participate in discovery sessions with project teams to understand data requirements needed to support solution implementations. Work collaboratively with a project and client team in an agile environment to support our cloud initiatives Leverage system and process analysis skills to produce data strategies that create a seamless and uninterrupted experience for our clients. Collaborate with clients to generate data mapping documents between target and source systems. Leverage industry-leading tools to extract, transform and load data into source systems in an automated fashion to support data migrations. Perform data profiling tasks to collect statistics, trends, impacts, and summaries that lead to a structured and successful implementation. Provide key insights based on data analytics to aid the client in drawing conclusions as to the business value and impact of their data and best approaches for migrating and transforming data to support new system architectures. Work with project management to provide regular status updates to internal and external stakeholders. ', ' A minimum of 1 years’ experience in a data related position  Experience with ETL (Extract-Transform-Load) concepts and platforms Experience with at least one database platform such as Microsoft SQL Server, MySQL, Oracle, etc Experience with scripting languages such as JavaScript or Python Experience working in cloud transformation projects within CRM, CPQ, and/or finance systems Experience working in an Agile (Scrum) environment desired ', 'Excellent verbal and written communication skills. ', 'Experience working in an Agile (Scrum) environment desired', 'Experience working in cloud transformation projects within CRM, CPQ, and/or finance systems', 'About Cognizant', 'Experience with scripting languages such as JavaScript or Python', ' 4-year degree in a technical or business field of study preferred Knowledge of Quote to Cash process (CPQ, Order Management, Billing, and CRM) a plus ', 'Position Overview', ' Bachelor’s degree and interest in learning common database tools, ETL products, and working with data in relational databases. Ability to effectively manage time and prioritize tasks Ability to work with ambiguity Capability to take on tasks with little direction and produce high-value deliverables in a fast-paced environment Excellent verbal and written communication skills.  Ability to work with technical and non-technical users, other departments, and clients Organization skills - ability to work in a highly dynamic environment with shifting priorities ', 'Education', 'Participate in discovery sessions with project teams to understand data requirements needed to support solution implementations.', 'Ability to work with technical and non-technical users, other departments, and clients', 'Ability to effectively manage time and prioritize tasks', 'Location and Travel:', 'Collaborate with clients to generate data mapping documents between target and source systems.', 'Desired locations are ATG Delivery Centers, located in Missoula, MT, Cincinnati, OH, Kansas City, MO or St. Louis, MO, however for senior experienced professionals working virtual may be an option.', 'Preferred Knowledge And Skills', 'Key Responsibilities', 'Relevant Technologies', ' Willingness and ability to travel up to 50% (subject to change at any time based off project demands) Desired locations are ATG Delivery Centers, located in Missoula, MT, Cincinnati, OH, Kansas City, MO or St. Louis, MO, however for senior experienced professionals working virtual may be an option. ', 'Experience with at least one database platform such as Microsoft SQL Server, MySQL, Oracle, etc', 'A minimum of 1 years’ experience in a data related position ', 'Perform data profiling tasks to collect statistics, trends, impacts, and summaries that lead to a structured and successful implementation.', 'Provide key insights based on data analytics to aid the client in drawing conclusions as to the business value and impact of their data and best approaches for migrating and transforming data to support new system architectures.', 'Ability to work with ambiguity', 'Leverage industry-leading tools to extract, transform and load data into source systems in an automated fashion to support data migrations.', 'Organization skills - ability to work in a highly dynamic environment with shifting priorities', 'Capability to take on tasks with little direction and produce high-value deliverables in a fast-paced environment', 'Bachelor’s degree and interest in learning common database tools, ETL products, and working with data in relational databases.', 'Experience with ETL (Extract-Transform-Load) concepts and platforms']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer - Lead Level,USAA,"Plano, TX",1 day ago,Be among the first 25 applicants,"['', 'Identifies and manages existing and emerging risks that stem from business activities and the job role.Ensures risks associated with business activities are effectively identified, measured, monitored, and controlled. Follows written risk and compliance policies and procedures for business activities.Design and implement complex technical solutions.Provides guidance to teams to help design, build and manage data pipelines for data structures encompassing data transformation, data models, schemas, metadata, data quality, and workload management.Participate in daily standups and lead design reviews.Breakdown business features into technical stories and approaches.Identify and re-engineer data processes to improve overall efficiency and business value.Create proof of concepts and prototypes.Implement efficient defect management, root cause analysis, and resolution processes.Ensure that team builds processes that support effective data management, metadata management, and good data quality.Mentor and coach junior / experienced engineers.Manage projects within team and work effectively with cross-functional teams.Provide guidance and work with team members to ensure successful completion of tasks', 'Minimum Requirements', 'Preferred', 'Ensure that team builds processes that support effective data management, metadata management, and good data quality.Mentor and coach junior / experienced engineers.', ""Bachelor's degree in related field of study, OR 4 additional years of related experience beyond the minimum required.And, 8 years of data management experience implementing data solutions demonstrating depth of technical understanding within a specific discipline(s)/technology(s) to include 2 years demonstrated technical leadership experience and/or leading teams.Deep experience with and contributes to multiple technologies and product lines across a company."", 'Ensures risks associated with business activities are effectively identified, measured, monitored, and controlled. ', 'Manage projects within team and work effectively with cross-functional teams.', 'Experience leading data engineering and solution design across multiple projects and functional areas', 'Design and implement complex technical solutions.', 'Ability to model and design modern data structures, SQL/NoSQL databases, Data Lakes, Cloud Data Warehouses (SnowFlake preferred)', 'Experience with designing, implementing, monitoring modern data platforms', 'Benefits', 'And, 8 years of data management experience implementing data solutions demonstrating depth of technical understanding within a specific discipline(s)/technology(s) to include 2 years demonstrated technical leadership experience and/or leading teams.', 'to the opening is 3/22/21 by 11:59 pm CST time.', 'Experience in a software engineer role, leveraging Java, Python, Scala or C++', 'About USAA', 'For Internal Candidates', 'Geographical Differential', ""Bachelor's degree in related field of study, OR 4 additional years of related experience beyond the minimum required."", 'Shift premium', 'Relocation', 'Experience with performance optimizations and best practices for scalable data models, pipelines and queries', 'Breakdown business features into technical stories and approaches.', 'Comprehensive knowledge of data management / data governance principles ', 'available', 'Implement efficient defect management, root cause analysis, and resolution processes.', 'Identifies and manages existing and emerging risks that stem from business activities and the job role.', 'Compensation', 'Participate in daily standups and lead design reviews.', 'Provide guidance and work with team members to ensure successful completion of tasks', 'USAA Total Rewards', '$$117,600 - $211,700.', 'Provides guidance to teams to help design, build and manage data pipelines for data structures encompassing data transformation, data models, schemas, metadata, data quality, and workload management.', 'Identify and re-engineer data processes to improve overall efficiency and business value.', 'Experience applying DevOps and DataOps concepts to improve quality and time to market for data and analytics products ', 'not', 'Create proof of concepts and prototypes.', 'Experience with cloud-based data offerings (Amazon AWS, Google GCP, Microsoft Azure)', 'Deep experience with and contributes to multiple technologies and product lines across a company.', 'Last day for internal candidates to apply ', 'Experience with designing, implementing, monitoring modern data platformsExperience leading data engineering and solution design across multiple projects and functional areasExperience applying DevOps and DataOps concepts to improve quality and time to market for data and analytics products Ability to model and design modern data structures, SQL/NoSQL databases, Data Lakes, Cloud Data Warehouses (SnowFlake preferred)Experience with performance optimizations and best practices for scalable data models, pipelines and queriesExperience in a software engineer role, leveraging Java, Python, Scala or C++Experience with cloud-based data offerings (Amazon AWS, Google GCP, Microsoft Azure)Comprehensive knowledge of data management / data governance principles ', 'Follows written risk and compliance policies and procedures for business activities.']",Not Applicable,Full-time,Information Technology,Financial Services,2021-03-18 14:34:51
Data Engineer (Python/Spark),Averity,United States,,N/A,"['We are one of the leading reinsurance companies in the world. We are hiring a Data Engineer for our new Data Science Center of Excellence team, that will own the build out of our infrastructure and implementation of data solutions.', '', 'Who Are We?', '• Knowledge of CI/CD Principles and experience', ""What's in it for you?"", 'Compensation:', ' ', 'We are a reinsurance company, the leading global risk and reinsurance specialist, that is located in Midtown Manhattan. We deliver expertise, strategic advise and industry-leading analytics for our clients around the world.', '• Experience building & deploying ETL Data Pipelines', '• $120,000 - $180,000 Base Salary', 'What Skills Do You Need?', 'Nice to have:', '• Spark, Python and SQL ', ""What's The Job?"", 'This is a great opportunity for someone to join a large, publicly traded company with over $1 Billion in revenue. This is also a great opportunity to join a new growing data driven team in a large organization.', '*We are big proponents of diversity, and encourage diverse applicants / candidates with diverse backgrounds to apply.', '• Outstanding Benefits Package.', ""As a Data Engineer, you will collaborate with Data Scientists, Product Managers, Business Analysts, and Stakeholders on Data Projects including Data Collection, Data Analysis and Data Products for our clients. You\xa0will lead and will own the design, development, and delivery of the Data Projects. You'll also report directly to senior-level management."", '• Bonus of 5-20%', 'We are looking for someone with strong\xa0analytical skills and\xa0interpersonal skills,\xa0that\xa0wants to be part of an Agile team that operates in a Startup environment within a larger organization.\xa0It would be great if you come from a Startup with consulting background, AdTech, or were part of a technology group that acted as internal consultants within internal organizations.', '• Computer Science or Data Science Degree', '• Azure Databricks ', '• 401(k)', '• Cloud AWS preferred', '\xa0', '• Excellent Vacation/Sick Day Policy (25+)', ""Day to day you'll be working with large datasets to build and deploy custom Data Pipelines using Python and Spark. You should\xa0have a solid experience with Big Data, ETL Frameworks, Data Streaming, CI/CD Principles and Cloud Deployment.""]",Mid-Senior level,Full-time,Engineering,Staffing and Recruiting,2021-03-18 14:34:51
Intern - Data Engineer,American Medical Association,"Chicago, IL",2 days ago,Be among the first 25 applicants,"['', 'Any knowledge/interest in implementing data management systems, ETL development, or master data management solutions is highly desirable.', 'Experience and interest in presenting analytic findings to business customers. Familiar with reporting and visualization tools, such as Tableau, Power BI, Business Objects, etc.', 'Any experience with or basic understanding of newer database structures and models such as NoSQL, Hadoop, Marklogic and Cassandra, a plus.Programming skills in Python, Java highly desirable.', 'Basic analysis skills; familiar with data analysis tools and techniques, such as SAS, R, Python, SPSS, text analytics, NLP; Able to manage and integrate insights and establish monitoring around multiple internal data sources, such as AMA Masterfile, Enterprise Data Warehouse, customer database and purchased/appended data if available.', 'Familiar with SQL in the extraction and manipulation of datasets. General knowledge of transactional data processing, ETL, data warehouse, data mart, and operational reporting solutions a plus.Any experience with the following tools is desirable: Aqua Data Studio, IBM DataStage / QualityStage, Information Analyzer, Informatica Business Glossary and Powercenter.', 'What Puts You Over The Top', 'Experience and interest in deeper analysis of data subjects, potentially spanning over a timeline of several months.', 'Responsibilities', 'Be working towards a BS or MS degree in Data Science, Statistics, Analytics, Computer Science, Information Systems, or a related degree.', 'Support the overall effort to modernize and enhance AMA data management architecture. Assist in development and testing of innovative analytical approaches, including but not limited to Big Data, A.I., machine learning, text analytics, and Natural Language Processing (NLP). Assist in vendor integration, data management work flow evaluation and optimization, vendor service level adherence monitoring activities, and requirements gathering around internal data management processes as effort moves forward.', 'Some practical experience working on AWS or another cloud provider ', 'Support the Data Management group and overall Health Solutions team through insightful data analysis and the development of analytical reporting, including subject areas of data collection, data quality, business rule adherence, and optimal choice of data fitness. Tailor analytical/reporting output to be digestible given the audience through usage of visualization and dashboards. Utilize technical acumen to automate reports over time and eliminate unnecessary manual processes. Support the overall effort to modernize and enhance AMA data management architecture. Assist in development and testing of innovative analytical approaches, including but not limited to Big Data, A.I., machine learning, text analytics, and Natural Language Processing (NLP). Assist in vendor integration, data management work flow evaluation and optimization, vendor service level adherence monitoring activities, and requirements gathering around internal data management processes as effort moves forward.Respond to data analysis needs and help deliver data analysis projects through the appropriate choice of front-end error detection and correction, process control and improvement, or process design strategies. Develop testing and processes to ensure data integrity and accuracy of the data, as well as proof of concept matching exercise to vet external data sources and gauge benefit. Follow all processes and procedures and provide documentation on all work. Support the ongoing effort to master AMA data assets through the implementation of an enterprise wide physician Masterfile strategy, work closely with technology partners to leverage technology tools to the utmost and understand internal data flow and ETL activities between current AMA systems and future platforms. ', 'Requirements', 'Experience with various batch matching methodologies. Willingness to learn and work with disparate data sets of varying structures and quality and interested in creating ad hoc methodologies to facilitate matching on these data sets, often without the benefit of common keys.', 'Some practical experience working on AWS or another cloud provider Some practical experience developing with Apache Spark and/or Hive Good knowledge of SQL and experience with columnar datastoresYou are working on your Masters degree', 'Ingestion, standardization, metadata management, business rule curation, data enhancement, and statistical computation against data sources that include relational, XML, JSON, streaming, REST API, and unstructured data.', 'THE AMA IS COMMITTED TO IMPROVING THE HEALTH OF THE NATION', 'Data Science InternChicago, IL', 'Respond to data analysis needs and help deliver data analysis projects through the appropriate choice of front-end error detection and correction, process control and improvement, or process design strategies. Develop testing and processes to ensure data integrity and accuracy of the data, as well as proof of concept matching exercise to vet external data sources and gauge benefit. Follow all processes and procedures and provide documentation on all work. ', 'May include other responsibilities as assigned. ', 'You are working on your Masters degree', 'Understanding of orchestration and scheduling tooling such as Jenkins/Airflow/Rundeck', 'Support the ongoing effort to master AMA data assets through the implementation of an enterprise wide physician Masterfile strategy, work closely with technology partners to leverage technology tools to the utmost and understand internal data flow and ETL activities between current AMA systems and future platforms. ', 'Support the Data Management group and overall Health Solutions team through insightful data analysis and the development of analytical reporting, including subject areas of data collection, data quality, business rule adherence, and optimal choice of data fitness. Tailor analytical/reporting output to be digestible given the audience through usage of visualization and dashboards. Utilize technical acumen to automate reports over time and eliminate unnecessary manual processes. ', 'Some practical experience developing with Apache Spark and/or Hive ', 'Good knowledge of SQL and experience with columnar datastores']",Internship,Internship,Information Technology,Nonprofit Organization Management,2021-03-18 14:34:51
Data Science Engineer,WorkBoard Inc.,"Denver, CO",20 hours ago,Be among the first 25 applicants,"['', ""Within One Month, You'll"", 'Leverage your mastery of business, communication, and technical skills to consult with experts in most successful enterprises in the business world. ', ' Bring at least 5 years of combined experience in Business Intelligence, Data Science and Data Warehousing Software development expertise in Python. Expertise in SQL and experience with large relational database systems. Experience creating an Events framework to enable better data analytics Experience with Data Visualization standard methodologies Experience working with the Product team to define and create Product Success metrics and engagement drivers. BS in Computer Science, Engineering or related technical or equivalent experience Have worked at a fast growing SaaS organization where you’ve demonstrated personal accountability and willingness to go above and beyond the job description. You are genuine, warm, positive, empathetic, and engaging with a passion for technology, and customer success. ', 'Humble ', ' Fully understand the Schema for our entire Relational Database Understand and be a point of contact for our event logging pipeline and the business logic associated with the events Ability to have client-facing conversations on our published metrics and help them understand the product logic Maintain / Enhance our internal Dashboard in Python and Sigma Work with App Engineering, Product and Customer Success teams to roll out in-product data focused features Dig deep into data to find levers to increase adoption and success and work cross functionally to drive awareness of these levers ', 'Fully understand the Schema of our core product tables', 'happy ', 'THE WORKBOARD STORY', 'honest ~ ', 'THE TEAM', 'Experience working with the Product team to define and create Product Success metrics and engagement drivers.', 'Have a deep understanding of the problem space we are building the platform for, and how are various offerings are interconnected to provide a comprehensive business solution', 'Demonstrate 100% understanding of WorkBoard’s underlying data structure and it’s mapping to our UI Objects', 'Work with App Engineering, Product and Customer Success teams to roll out in-product data focused features', ' Become a certified OKR Coach and WorkBoard Expert! Demonstrate 100% understanding of WorkBoard’s underlying data structure and it’s mapping to our UI Objects Have action plans in place to achieve your Key Results! ', '401K with employer matching', ""Within Three Months, You'll"", 'Understand our user engagement and product analytics for 3 areas better than anyone', 'Understand our current Analytics events structure and framework', 'OUR VALUES - WE LIVE BY THE 4 Hs', ""Within Six Months, You'll"", 'Ability to have client-facing conversations on our published metrics and help them understand the product logic', 'Have worked at a fast growing SaaS organization where you’ve demonstrated personal accountability and willingness to go above and beyond the job description.', 'Understand and be a point of contact for our event logging pipeline and the business logic associated with the events', 'Paid holidays', 'Experience creating an Events framework to enable better data analytics', 'THE OPPORTUNITY', 'Hungry ', 'Have action plans in place to achieve your Key Results!', 'WorkBoard', 'Maintain / Enhance our internal Dashboard in Python and Sigma', 'Dig deep into data to find levers to increase adoption and success and work cross functionally to drive awareness of these levers', 'COMING IN', 'Become a certified OKR Coach and WorkBoard Expert!', 'Health insurance ', 'Bring at least 5 years of combined experience in Business Intelligence, Data Science and Data Warehousing', 'a Few Of Our Awesome Benefits', 'Experience with Data Visualization standard methodologies', 'Software development expertise in Python.', 'Expertise in SQL and experience with large relational database systems.', 'You are genuine, warm, positive, empathetic, and engaging with a passion for technology, and customer success.', 'Be part of the foundational team with great people, who have an entrepreneurial mindset and bring their absolute best every day.', 'Rich, deep data set to draw insights from and influence internal and product and engineering leadership', 'Provide customer feedback and work with the engineering team to translate the feedback into product features. ', 'BS in Computer Science, Engineering or related technical or equivalent experience', 'Flexible PTO & sick days', 'And much more!', ' Rich, deep data set to draw insights from and influence internal and product and engineering leadership Be part of the foundational team with great people, who have an entrepreneurial mindset and bring their absolute best every day. Leverage your mastery of business, communication, and technical skills to consult with experts in most successful enterprises in the business world.  Provide customer feedback and work with the engineering team to translate the feedback into product features.  ', 'Fully understand the Schema for our entire Relational Database', ' Have a deep understanding of the problem space we are building the platform for, and how are various offerings are interconnected to provide a comprehensive business solution Fully understand the Schema of our core product tables Understand our current Analytics events structure and framework Understand our user engagement and product analytics for 3 areas better than anyone ', ' Flexible PTO & sick days Paid holidays Health insurance  401K with employer matching Quarterly All-Hands Meetings And much more! ', 'Quarterly All-Hands Meetings']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
"Data Engineer, Business Intelligence",Essence,New York City Metropolitan Area,2 days ago,Be among the first 25 applicants,"['', 'Previous experience working with data and technology', 'Become a proficient user of Google Cloud Platform and associated technologies, ensuring your work incorporates industry best practice', 'Analytically minded, enabling you to understand and overcome technically complex challenges, and to tell compelling stories with data', 'An ability to demonstrate a passion for the digital marketing ecosystem, and an understanding of the role that data plays within it', 'The Role:', 'This role forms part of a globally distributed business intelligence team, whose objective is to ensure that one of our most important global accounts have access to the right data and insights in order to inform their marketing decisions.\xa0\xa0', 'Support the translation of user requirements and business needs into technical specifications', 'Experience building underlying data pipelines and ETL, particularly useful if done using Google Cloud Platform, Airflow, DBT etc.', 'Experience using or building reports with business intelligence software, ideally Google DataStudio', 'Visit essenceglobal.com for more information and follow us on Twitter at @essenceglobal.', 'Assist the development of technical solutions, in line with specifications,\xa0 that collect, store and transform disparate data sources', 'Assist the development and maintenance of automated jobs that ensure required data is made available in an efficient and scalable way as possible', 'About Essence', 'An understanding of how data is tracked and exchanged in the process of digital advertising (e.g. role of ad servers and other third-party tech vendors)', 'Some of the things we’d like you to do:', 'Attend internal stakeholder meetings, presenting your solutions and providing updates on your work.', 'related company (e.g. publisher, ad tech, client marketing org)', 'Experience with digital marketing platforms and the data they generate, in particular Google Marketing Platform, Facebook, Twitter etc.\xa0 Knowledge of their API’s a plus.', 'Required', 'As Data Engineer your primary responsibility will be to support a Senior Data Engineer to create and maintain underlying data infrastructure that provides the wider team with the data they need to provide timely, accurate and meaningful deliverables & reporting.\xa0 In doing so you will gain a foundational understanding of cloud technology and key data engineering skills and knowledge to help you build a career in this fast evolving, and in demand, industry.', 'Monitor automated jobs, troubleshooting data issues as-and-when they arise', 'Desirable', 'Assist the development and maintenance of data quality checks and procedures, helping account teams overcome data issues before they impact the quality of the reporting solution', 'A bit about yourself:', 'Support other members of the team responsible for “last mile” transformation and visualization of data within Google Data Studio reports and dashboards', 'What you can expect from Essence', 'Provide hands-on support to users of reportion solutions, helping the wider team triage and respond to user queries in a timely manner', 'Experience with programming and/or statistical languages (e.g. SQL, Python)', 'Strong spoken and written communication skills, ensuring your thoughts and needs are heard and understood', 'Essence’s mission is to make advertising more valuable to the world.\xa0 We do this by employing the world’s very best talent to solve some of the toughest challenges of today’s digital marketing landscape.\xa0 It’s important that we hire people whose values reflect those of our own: genuine, results-focused, daring and insightful.\xa0 As an Essence employee, we promise you a workplace that invests in your career, cares for you and is fun and engaging.\xa0 We believe these factors create a workplace where you can be yourself and do amazing work.', 'The team’s primary responsibility is to maintain a global Google Cloud based reporting solution, which automates the collection and transformation of disparate marketing data into a single source of truth.\xa0 Not only does that mean running and maintaining the solution that already exists, but also continually improving it to incorporate new data sources, and to derive new insights, to support ever-evolving business demands.', 'Assist the development of technical solutions, in line with specifications,\xa0 that collect, store and transform disparate data sourcesAssist the development and maintenance of automated jobs that ensure required data is made available in an efficient and scalable way as possibleAssist the development and maintenance of data quality checks and procedures, helping account teams overcome data issues before they impact the quality of the reporting solutionSupport the translation of user requirements and business needs into technical specificationsBecome a proficient user of Google Cloud Platform and associated technologies, ensuring your work incorporates industry best practiceMonitor automated jobs, troubleshooting data issues as-and-when they ariseSupport other members of the team responsible for “last mile” transformation and visualization of data within Google Data Studio reports and dashboardsProvide hands-on support to users of reportion solutions, helping the wider team triage and respond to user queries in a timely mannerAttend internal stakeholder meetings, presenting your solutions and providing updates on your work.Support the development strong working relationships with third-party data providers that we rely on for access to necessary data', 'Support the development strong working relationships with third-party data providers that we rely on for access to necessary data', 'Strong organizational skills and attention to detail, including the ability to manage multiple tasks in a fairly autonomous way', 'Work experience within a marketing organization, preferably at a media agency or\xa0', 'Experience building underlying data pipelines and ETL, particularly useful if done using Google Cloud Platform, Airflow, DBT etc.Experience with digital marketing platforms and the data they generate, in particular Google Marketing Platform, Facebook, Twitter etc.\xa0 Knowledge of their API’s a plus.An understanding of how data is tracked and exchanged in the process of digital advertising (e.g. role of ad servers and other third-party tech vendors)Experience using or building reports with business intelligence software, ideally Google DataStudioWork experience within a marketing organization, preferably at a media agency or\xa0', ""Essence, part of GroupM, is a global data and measurement-driven media agency whose mission is to make brands more valuable to the world. Clients include Google, Flipkart, NBCUniversal, L'Oréal and the Financial Times. The agency is more than 2,000 people strong, manages $4.5B in annualized media spend, and deploys campaigns in 121 markets via 22 offices in APAC, EMEA and the Americas.\xa0"", 'Delivers best results when working in a team environment, and an ability to partner effectively with people of varying degrees of technical capability', 'Previous experience working with data and technologyExperience with programming and/or statistical languages (e.g. SQL, Python)Analytically minded, enabling you to understand and overcome technically complex challenges, and to tell compelling stories with dataStrong organizational skills and attention to detail, including the ability to manage multiple tasks in a fairly autonomous wayStrong spoken and written communication skills, ensuring your thoughts and needs are heard and understoodAn ability to demonstrate a passion for the digital marketing ecosystem, and an understanding of the role that data plays within itDelivers best results when working in a team environment, and an ability to partner effectively with people of varying degrees of technical capability']",Mid-Senior level,Full-time,Engineering,Marketing and Advertising,2021-03-18 14:34:51
eCom Data Engineer,PepsiCo,"New York, NY",3 weeks ago,42 applicants,"['', ' Kubernetes', 'Collaborate with business teams to design and implement a solution', 'Write production-grade code using standard software engineering methodologiesImplement data solutions with software engineering best practicesOwn data pipelines end-to-endCollaborate with business teams to design and implement a solutionContribute to a data driven culture where everyone is valuedAnalyze large data sets and develop custom models to uncover trends, patterns and insightsWork with technologists both inside and outside DSA to identify long-term, sustainable solutionsProvide critical thought leadership to enhance organizational capabilities by utilizing big and small data', 'Motivated to learn and collaborate in a large team settingDrive to build sustainable and reliable solutionsBS or MS in Computer Science or related fieldKnowledge of Python, SQL, GitHub, AWS, Docker, etc.Knowledge of best-practices in distributed systems and large-scale data processingAbility to effectively and concisely communicate with both business and technical audiences', ' AWS services like s3, Sagemaker, lambda, SQS, SNS, EMR, ECR, EKS, etc.', ' ETL tools like Airflow, Celery, Luigi, etc.', 'Responsibilities', 'Drive to build sustainable and reliable solutions', 'Nice to have:', ' MPP cloud data warehouses like Snowflake, Redshift, etc.', 'Work with technologists both inside and outside DSA to identify long-term, sustainable solutions', 'BS or MS in Computer Science or related field', ' Distributed data processing systems like Spark, Hadoop, etc.', ' ML frameworks (sklearn, Tensorflow, PyTorch, MXNet or similar)', 'Implement data solutions with software engineering best practices', 'Knowledge of Python, SQL, GitHub, AWS, Docker, etc.', 'Analyze large data sets and develop custom models to uncover trends, patterns and insights', ' Knowledge of ML pipeline frameworks like kubeflow, MLflow, etc.', 'Own data pipelines end-to-end', 'Relocation Eligible:', 'Provide critical thought leadership to enhance organizational capabilities by utilizing big and small data', ' Knowledge of ML pipeline frameworks like kubeflow, MLflow, etc. ML frameworks (sklearn, Tensorflow, PyTorch, MXNet or similar) Kubernetes ETL tools like Airflow, Celery, Luigi, etc. AWS services like s3, Sagemaker, lambda, SQS, SNS, EMR, ECR, EKS, etc. Distributed data processing systems like Spark, Hadoop, etc. MPP cloud data warehouses like Snowflake, Redshift, etc.', 'Auto req ID:', 'Qualifications/Requirements', 'Job Description', 'Ability to effectively and concisely communicate with both business and technical audiences', 'Write production-grade code using standard software engineering methodologies', 'Job Type:', 'Knowledge of best-practices in distributed systems and large-scale data processing', 'Motivated to learn and collaborate in a large team setting', 'Contribute to a data driven culture where everyone is valued']",Mid-Senior level,Full-time,Information Technology,Consumer Goods,2021-03-18 14:34:51
Core Data Engineer,Eight Eleven Group,"Louisville, KY",1 day ago,Be among the first 25 applicants,"['', '<< Return to Search Results', '(Location)', 'Azure Data Factory (ETL)', ""What You'll Be Responsible For"", '(Education Required) -', 'Job Title:', 'Good understanding of requirements around data loads. ', 'Capable of planning and executing on both short-term and long-term goals individually and with the team.', 'Azure SQL DB', 'Benefits Of Working With Brooksource', 'Brooksource provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, national origin, age, sex, citizenship, disability, genetic information, gender, sexual orientation, gender identity, marital status, amnesty or status as a covered veteran in accordance with applicable federal, state, and local laws.JO-2101-107498', 'Very strong experience in performance tuning SQL and ETL pipelines in Enterprise Data Warehouse environments', 'Data Modeling', 'Azure Data Factory (ETL)Azure SQL DBSQL DeveloperData Modeling', 'Working with the line of business to understand requests pertaining to data', 'Excellent written and verbal communication and interpersonal skills, able to effectively collaborate with technical and business partners', 'Previous experience working with this client and placing both permanent employees and contractors.Direct communication with the hiring manager, which allows us to have a deep understanding of the timeline and move you through the interview process faster.Dedication to keep an open line of communication and provide full visibility.Brooksource is an equal opportunity employer.', 'SQL Developer', 'Job Description:', 'You will move that data through various data models', '(Daily) – ', 'Requirements', ""Working with the line of business to understand requests pertaining to dataUtilizing SQL/ETL processes to grab data from the client's Data LakeYou will move that data through various data modelsEnsuring data is clean/structured to be able to be consumed and visualized "", 'Previous experience working with this client and placing both permanent employees and contractors.', '(Certification Required)', '5+ years’ work experience with:Azure Data Factory (ETL)Azure SQL DBSQL DeveloperData Modeling', ""Utilizing SQL/ETL processes to grab data from the client's Data Lake"", '5+ years’ work experience with:Azure Data Factory (ETL)Azure SQL DBSQL DeveloperData ModelingVery strong experience in performance tuning SQL and ETL pipelines in Enterprise Data Warehouse environmentsProblem solver with excellent interpersonal skills with ability to make sound complex decisions in a fast-paced, technical environment.Ability to work on multiple areas like Data Lakes, Data pipeline ETL, Data modeling & design, writing complex SQL queries etc.Any experience with Data Visualization through PowerBI is helpfulCapable of planning and executing on both short-term and long-term goals individually and with the team.Excellent written and verbal communication and interpersonal skills, able to effectively collaborate with technical and business partnersGood understanding of requirements around data loads. ', 'Job Location:', 'Dedication to keep an open line of communication and provide full visibility.', 'Direct communication with the hiring manager, which allows us to have a deep understanding of the timeline and move you through the interview process faster.', 'Ability to work on multiple areas like Data Lakes, Data pipeline ETL, Data modeling & design, writing complex SQL queries etc.', 'Any experience with Data Visualization through PowerBI is helpful', 'Brooksource is an equal opportunity employer.', 'Problem solver with excellent interpersonal skills with ability to make sound complex decisions in a fast-paced, technical environment.', 'Ensuring data is clean/structured to be able to be consumed and visualized ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
"Data Engineer (Python, SQL)",Huxley,"Chicago, IL",1 week ago,Over 200 applicants,"['Supporting the data quality program as well as operations, and technical documentations', 'Competitive compensation plus 15% bonus that is paid out bi-annually', 'The ability to work with company with an award winning positive cultureCompetitive compensation plus 15% bonus that is paid out bi-annuallyGenerous PTO and paid holidays', ' ', 'Experience with cloud technologies like GCP is a plus', 'Design and build out ETL Pipelines and toolsWorking in collaboration with other members from the data team implementing new ideasSupporting the data quality program as well as operations, and technical documentations', 'The ability to work with company with an award winning positive culture', 'Experience building out data pipelines from scratch using PythonSQL development experienceExperience with cloud technologies like GCP is a plus', ""What's in it for you?!"", 'Sthree US is acting as an Employment Agency in relation to this vacancy.', 'Experience building out data pipelines from scratch using Python', 'Generous PTO and paid holidays', 'My client is looking to grow their Data team notorious for their Award Winning Culture. They are currently building out their analytical warehouse and real-time data that overlooks their ecommerce platform. Experience building out data pipelines and ETL tools is a must, which seems to align with your background!', 'Working in collaboration with other members from the data team implementing new ideas', 'Must have Qualifications:', 'SQL development experience', 'Responsibilities:', 'Design and build out ETL Pipelines and tools']",Entry level,Full-time,Engineering,Computer Software,2021-03-18 14:34:51
Data Engineer,Charles Schwab,"Westlake, TX",6 days ago,32 applicants,"['', ' Mentoring, motivating, and supporting the team to achieve organizational objectives and goals ', ' 2+ years of experience building data pipelines and interfaces with object oriented languages (.Net, Java, Python) ', ' 2+ years of experience working on agile teams delivering data solutions ', ' Designing, implementing, and maintaining data warehouses and near real-time data pipelines via the practical application of existing and new data engineering techniques ', ' 1+ years of experience modeling star schema data warehouses using the Kimball dimensional modeling techniques ', ' Exceptional interpersonal skills, including teamwork, communication, and negotiation', 'What You Are Good At', ' 3+ years of experience designing, building, and supporting near real-time data pipelines and analytical solutions using Hadoop, Teradata, MS SQL Server, Talend, Informatica, and/or SSIS ', ' 1+ years of experience delivering solutions on public cloud platforms (Google Cloud preferred) ', 'What You Have', ' Ensuring consistency with published development, coding and testing standards ', ' Advocating for agile practices to increase delivery throughput ', ' 3+ years of experience designing, building, and supporting near real-time data pipelines and analytical solutions using Hadoop, Teradata, MS SQL Server, Talend, Informatica, and/or SSIS  2+ years of experience working on agile teams delivering data solutions  2+ years of experience building data pipelines and interfaces with object oriented languages (.Net, Java, Python)  1+ years of experience modeling star schema data warehouses using the Kimball dimensional modeling techniques  1+ years of experience delivering solutions on public cloud platforms (Google Cloud preferred)  Basic understanding of at least one IT Management frameworks such as ITIL or COBiT  Experience writing automated unit, integration, and acceptance tests for data interfaces & data pipelines  Ability to quickly learn & become proficient with new technologies  Exceptional interpersonal skills, including teamwork, communication, and negotiation', ' Collaborating directly with business and technology stakeholders to define future-state business capabilities & requirements, and translating those into transitional and target state data architectures  Analyzing the current technology environment to detect critical deficiencies, and recommend solutions for improvement  Designing, implementing, and maintaining data warehouses and near real-time data pipelines via the practical application of existing and new data engineering techniques  Developing continuous integration and continuous deployment pipelines for data solutions that include automated unit & integration testing  Mentoring, motivating, and supporting the team to achieve organizational objectives and goals  Advocating for agile practices to increase delivery throughput  Ensuring consistency with published development, coding and testing standards ', ' Ability to quickly learn & become proficient with new technologies ', 'Your Opportunity ', ' Developing continuous integration and continuous deployment pipelines for data solutions that include automated unit & integration testing ', ' Analyzing the current technology environment to detect critical deficiencies, and recommend solutions for improvement ', ' Experience writing automated unit, integration, and acceptance tests for data interfaces & data pipelines ', ' Collaborating directly with business and technology stakeholders to define future-state business capabilities & requirements, and translating those into transitional and target state data architectures ', ' Basic understanding of at least one IT Management frameworks such as ITIL or COBiT ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Skillz Inc.,"Las Vegas, NV",2 days ago,29 applicants,"['', 'Familiarity with Kinesis, Lamda', 'Building data integration toolkit for backend services', 'Who We’re Looking For', 'Build new systems to provide real-time streaming analytics and event processing pipeline based on fast data architecture', 'Familiarity with Alooma, Snowflakes', ' Familiarity with Agile engineering practices 1+ years experience on Kubernete, Helm chart 1+ years of experience with Spark, Scala and/or Akka 1+ years of experience with Spark Streaming, Storm, Flink, or other Stream Processing technologies 1+ years of experience working with Kafka or similar data pipeline backbone 1+ years of experience with Unix/Linux systems with scripting experience in Shell, Perl or Python 1+ years’ experience with NoSQL implementation (ElasticSearch, Cassandra, etc. a plus) At least 1 year of experience with Unix/Linux systems with scripting experience Familiarity with Alooma, Snowflakes Familiarity with Kinesis, Lamda Prior experience in gaming Prior experience in finance ', '1+ years of experience with Spark Streaming, Storm, Flink, or other Stream Processing technologies', '1+ years experience on Kubernete, Helm chart', 'What You’ll Do', 'Bonus', 'Build enterprise grade data lake to support both business analytical needs and next generation data infrastructure', 'Find better ways to move massive amounts of data from a variety of sources to formats consumable by reporting systems and people', '1+ years of experience with Unix/Linux systems with scripting experience in Shell, Perl or Python', ' At least 1+ years of experience in Scala/Java or Python programming AWS data products (Data pipelines, Athena, Pinpoint, S3, etc) Experience deploying data infrastructure Experience with recognized industry patterns, methodologies, and techniques ', 'At least 1+ years of experience in Scala/Java or Python programming', '1+ years of experience working with Kafka or similar data pipeline backbone', 'Prior experience in finance', '1+ years’ experience with NoSQL implementation (ElasticSearch, Cassandra, etc. a plus)', 'Familiarity with Agile engineering practices', '1+ years of experience with Spark, Scala and/or Akka', 'About Skillz', 'Prior experience in gaming', 'Improve monitoring and alarms that impact data integrity replication lag', 'Support our data science team in deploying new algorithms for matchmaking, fraud and cheat detection', 'AWS data products (Data pipelines, Athena, Pinpoint, S3, etc)', 'Basic Qualifications', 'Experience with recognized industry patterns, methodologies, and techniques', 'Support our product development team in creating new events to measure/track', 'At least 1 year of experience with Unix/Linux systems with scripting experience', 'Experience deploying data infrastructure', 'Your Skillz:', ' Build new systems to provide real-time streaming analytics and event processing pipeline based on fast data architecture Build enterprise grade data lake to support both business analytical needs and next generation data infrastructure Building data integration toolkit for backend services Support our data science team in deploying new algorithms for matchmaking, fraud and cheat detection Find better ways to move massive amounts of data from a variety of sources to formats consumable by reporting systems and people Improve monitoring and alarms that impact data integrity replication lag Support our product development team in creating new events to measure/track ']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Associate Data Engineer,The Hartford,"Remote, OR",2 weeks ago,42 applicants,"['', 'Prefer working knowledge of ETL process', 'Experience in Tableau is a plus', 'Experience in Data Analysis and/or Data Engineer competencies (e.g. SQL Server, Oracle)', 'Knowledge of relational database design and concepts', 'Responsibilities', 'Solve a range of core business and technical questions through data analysis', 'Design and develop data solutions extracting data from Oracle and SQL ServerSolve a range of core business and technical questions through data analysisSupport the development of advanced monitoring metrics, which includes becoming the subject matter expert of the data and ownership of metric definitions.Collaboration within the team to support data needs and team members’ analyses.Extend data analytics using tools like Python', 'Extend data analytics using tools like Python', 'Design and develop data solutions extracting data from Oracle and SQL Server', 'Bachelor degree or equivalent experience in related field required Knowledge of relational database design and conceptsExperience in Data Analysis and/or Data Engineer competencies (e.g. SQL Server, Oracle)Experience writing SQL queries', 'Prefer working knowledge of ETL processExperience in Tableau is a plus', 'Preferred Experience', 'Support the development of advanced monitoring metrics, which includes becoming the subject matter expert of the data and ownership of metric definitions.', 'Collaboration within the team to support data needs and team members’ analyses.', 'Experience writing SQL queries', 'Requirements', 'Bachelor degree or equivalent experience in related field required ']",Associate,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Engineer,Hired Recruiters,"New York, NY",1 day ago,Be among the first 25 applicants,"['', 'You have worked with a variety of cloud and data solutions, such as: AWS / Azure / Google Cloud, SnowFlake, SnowPlow, Kafka, Segment.io, DataBricks, Trifecta, Hadoop, Spark, Airflow', 'Design, implement, and operate stable, scalable, low cost solutions to flow data from production systems into the data warehouse and into end-user facing BI applications', 'Help lead the team to build our data infrastructure and work with emerging technologies such as Snowflake and associated cloud services', 'Implement processes and systems to monitor data quality, ensuring production data is always accurate and available', 'Experience operating a production solution which supports the business', ' You have 5+ years of experience in the field of data engineering or related engineering experience You have recent experience building large-scale production data solutions You are familiar with data driven marketing and integrating into marketing automation solutions. You have a command of various programming languages to collect and manipulate data such as Python, R, and SQL. Experience operating a production solution which supports the business You have strong solution architecture skills and a passion for building data solutions that power the future business. You have worked with a variety of cloud and data solutions, such as: AWS / Azure / Google Cloud, SnowFlake, SnowPlow, Kafka, Segment.io, DataBricks, Trifecta, Hadoop, Spark, Airflow We are looking for people that will thrive in the collaborative environment while also being strong individual contributors with a proactive mindset. ', 'You have 5+ years of experience in the field of data engineering or related engineering experience', 'You are familiar with data driven marketing and integrating into marketing automation solutions.', 'You have strong solution architecture skills and a passion for building data solutions that power the future business.', 'What You’ll Do', 'You have recent experience building large-scale production data solutions', 'Running machine learning experiments using best-in-class ML platforms', 'Who You Are', ' Help lead the team to build our data infrastructure and work with emerging technologies such as Snowflake and associated cloud services Design, implement, and operate stable, scalable, low cost solutions to flow data from production systems into the data warehouse and into end-user facing BI applications Implement processes and systems to monitor data quality, ensuring production data is always accurate and available Running machine learning experiments using best-in-class ML platforms Automate & optimize everything Leverage best in industry practices to build the next generation data ecosystem to collect, move, store and analyze data ', 'We are looking for people that will thrive in the collaborative environment while also being strong individual contributors with a proactive mindset.', 'Automate & optimize everything', 'Leverage best in industry practices to build the next generation data ecosystem to collect, move, store and analyze data', 'You have a command of various programming languages to collect and manipulate data such as Python, R, and SQL.']",Mid-Senior level,Full-time,Engineering,Computer Software,2021-03-18 14:34:51
Data Engineer,Princess Polly,"West Hollywood, CA",2 weeks ago,54 applicants,"['', ' Amazing Employee Discount Program (40%)', ' Individual & Team Based Leadership Development Programs', ' Qualifications ', ' Experience Using Dbt (strongly Preferred) ', ' Expert SQL Skills (required) ', ' Company Paid Life, Short Term Disability, Long Term Disability, & Employee', ' Company Sponsored Medical (HMO & PPO Options), Dental, & Vision Plans', ' Flexible working arrangements', ' Flexible working arrangements Amazing Employee Discount Program (40%) Company Sponsored Medical (HMO & PPO Options), Dental, & Vision Plans Company Paid Life, Short Term Disability, Long Term Disability, & Employee', ' Positive Company Culture that Celebrates both Personal & Company Milestones', ' 401(k) Program (100% Match Up to 5% of Pay) Individual & Team Based Leadership Development Programs Positive Company Culture that Celebrates both Personal & Company Milestones 15 Vacation Days + 10 Sick Days + 10 Holidays', ' 15 Vacation Days + 10 Sick Days + 10 Holidays', ' Responsibilities ', ' 401(k) Program (100% Match Up to 5% of Pay)']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
"Sr. Data Engineer, Data Platform","Metasys Technologies, Inc.","Bristol, CT",1 day ago,Be among the first 25 applicants,"['', 'Bachelors degree or better in Computer Science or a related technical field or equivalent job experience.', 'Robust data analysis and root cause analysis skills', ' Bachelors degree or better in Computer Science or a related technical field or equivalent job experience. ', 'Preferred', 'The Sr. Data Engineer will work directly on the Oracle enterprise data warehouse (EDW) to deliver batch and realtime data for analytics and reporting capabilities to the Linear Ad Sales business unit.', 'Job Summary', '5+ years of experience working as an Oracle / Snowflake database developer (Oracle 11g or greater)', 'Provide expert level advice to data scientists, data engineers, and operations to deliver high quality analytics via machine learning and deep learning via data pipelines and APIs.', 'Help understand our data by performing exploratory and quantitative analytics, data mining, and discovery.', 'Ability to develop, implement and maintain standards established by the architecture and Development teams.', 'Required Education, Experience/Skills/Training: Bachelors degree or better in Computer Science or a related technical field or equivalent job experience.  Preferred Good to have Media Ad Sales and Finance experience. Metasys Technologies is an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identify, national origin, veteran or disability status.', 'At least 5+ years with data warehouse design, development in an Oracle, Snowflake database developer (Oracle 11g or greater) or similar database technologies', '5+ years of working in a data warehousing / big data environment', 'Self-motivated independent thinker and collaborative team member.', 'Required', ' At least 5+ years with data warehouse design, development in an Oracle, Snowflake database developer (Oracle 11g or greater) or similar database technologies ', ' 5+ years of experience working as an Oracle / Snowflake database developer (Oracle 11g or greater) 5+ years of working in a data warehousing / big data environment SQL ETL/ELT development and performance tuning Ability to develop, implement and maintain standards established by the architecture and Development teams. Robust data analysis and root cause analysis skills Self-motivated independent thinker and collaborative team member. Experience in using CI/CD pipeline (Gitlab) ', 'Think of new ways to help make our data platform more scalable, resilient and reliable and then work across our team to put your ideas into action.', 'Mentor other software engineers by developing re-usable frameworks. Review design and code produced by other engineers.', 'Help us stay ahead of the curve by working closely with product, data modelers, API developers, DevOps team, and analysts to design systems which can scale elastically', 'SQL ETL/ELT development and performance tuning', 'Work across the different stages of the EDW data pipeline, using tools such as Oracle ExaCC, SAP Data Services, Oracle GoldenGate, Oracle BigData Extension, and Kalido', ' The Sr. Data Engineer will work directly on the Oracle enterprise data warehouse (EDW) to deliver batch and realtime data for analytics and reporting capabilities to the Linear Ad Sales business unit. Work across the different stages of the EDW data pipeline, using tools such as Oracle ExaCC, SAP Data Services, Oracle GoldenGate, Oracle BigData Extension, and Kalido Help understand our data by performing exploratory and quantitative analytics, data mining, and discovery. Think of new ways to help make our data platform more scalable, resilient and reliable and then work across our team to put your ideas into action. Ensure performance is optimized for realtime data by implementing and refining robust data processing across landing, integration and data mart layers Help us stay ahead of the curve by working closely with product, data modelers, API developers, DevOps team, and analysts to design systems which can scale elastically Mentor other software engineers by developing re-usable frameworks. Review design and code produced by other engineers. Provide expert level advice to data scientists, data engineers, and operations to deliver high quality analytics via machine learning and deep learning via data pipelines and APIs. ', 'Experience in using CI/CD pipeline (Gitlab)', 'Basic Requirements', 'Ensure performance is optimized for realtime data by implementing and refining robust data processing across landing, integration and data mart layers']",Entry level,Contract,Information Technology,Information Technology and Services,2021-03-18 14:34:51
"Associate Data Engineer - Minnetonka, MN",Optum,"Minnetonka, MN",7 days ago,Be among the first 25 applicants,"['', 'Bachelor’s degree in data science or information technology or equivalent years of data science experience ', 'The right candidate will have strong prioritization skills, ability to manage ad hoc requests in parallel with ongoing projects', '2+ years hands on development with Python ', 'Primary Responsibilities', 'You will have the opportunity to work with open-source distributed data processing frameworks, such as Apache Spark and build scalable machine learning applications and deploy them in production', ' 2+ years of experience working in a healthcare environment Preferred Google Cloud Data Science Certification  ', '3+ years of experience working in cloud environments like AWS and GCP.', 'UnitedHealth Group is an essential business.', 'Ability to create detailed technical design specifications and documentations.', 'UnitedHealth', 'Ability to effective communications with all levels of internal and external customers and staff, including issue resolution, negotiation of project related priorities, and completion dates. ', ' Bachelor’s degree in data science or information technology or equivalent years of data science experience  3+ years of experience working in cloud environments like AWS and GCP. 3+ years of developing complex SQLs. 2+ years hands on development with Python  2+ years of hands on experience with Google Big Query (such as Data Fusion, Data Flow, Composer, Data Catalog) 1+ years of experience in applications systems analysis and/or development and/or support. 2+ year of experience working in ETL data pipeline and automation using tools like Talend, airflow, SSIS, etc. Ability to deliver functionality in two or more programming languages within a single development environment. Such as SQL, Python, JavaScript or Java. Ability to create detailed technical design specifications and documentations. Ability to effective communications with all levels of internal and external customers and staff, including issue resolution, negotiation of project related priorities, and completion dates.  You will be asked to perform this role in an office setting or other company location. If you need to enter a work site for any reason, you will be required to screen for symptoms using the ProtectWell mobile app, Interactive Voice Response (i.e., entering your symptoms via phone system) or a similar UnitedHealth Group-approved symptom screener. When in a UnitedHealth Group building, employees are required to wear a mask in common areas, In addition, employees must comply with any state and local masking orders ', 'Specifically, as a member of our collaborative team you will be working with structured and unstructured data to explore the data, develop models, and performing research analytics. ', 'You would be participating in the development of our end-to-end NLP pipeline, building descriptive and predictive analytics and producing research validations.', 'Associate Data Engineer', '2+ year of experience working in ETL data pipeline and automation using tools like Talend, airflow, SSIS, etc.', 'If you need to enter a work site for any reason, you will be required to screen for symptoms using the ProtectWell mobile app, Interactive Voice Response (i.e., entering your symptoms via phone system) or a similar UnitedHealth Group-approved symptom screener. When in a UnitedHealth Group building, employees are required to wear a mask in common areas, In addition, employees must comply with any state and local masking orders', 'UnitedHealth Group', 'You will be asked to perform this role in an office setting or other company location.', 'Diversity creates a healthier atmosphere: UnitedHealth Group is an Equal Employment Opportunity/Affirmative Action employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law.', 'UnitedHealth Group is a drug-free workplace. Candidates are required to pass a drug test before beginning employment.', '3+ years of developing complex SQLs.', '2+ years of experience working in a healthcare environment', 'Required Qualifications', ' We are looking for a data scientist who is eager to tackle the challenges of extracting insights from vast amounts of EHR data originating from multiple sources.  You will work with a diverse set of Optum researchers, clinicians, internal and external clients to define the projects and see them through to completion that cover a range of clinical focus. You would be participating in the development of our end-to-end NLP pipeline, building descriptive and predictive analytics and producing research validations. Specifically, as a member of our collaborative team you will be working with structured and unstructured data to explore the data, develop models, and performing research analytics.  You will have the opportunity to work with open-source distributed data processing frameworks, such as Apache Spark and build scalable machine learning applications and deploy them in production The right candidate will have strong prioritization skills, ability to manage ad hoc requests in parallel with ongoing projects ', '2+ years of hands on experience with Google Big Query (such as Data Fusion, Data Flow, Composer, Data Catalog)', 'Group', 'You will work with a diverse set of Optum researchers, clinicians, internal and external clients to define the projects and see them through to completion that cover a range of clinical focus.', 'Ability to deliver functionality in two or more programming languages within a single development environment. Such as SQL, Python, JavaScript or Java.', 'Preferred Google Cloud Data Science Certification ', 'Preferred Qualifications', 'We are looking for a data scientist who is eager to tackle the challenges of extracting insights from vast amounts of EHR data originating from multiple sources. ', '1+ years of experience in applications systems analysis and/or development and/or support.']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Jerry.ai,"Palo Alto, CA",3 weeks ago,163 applicants,"['', '2+ years of data engineering experience within a rigorous engineering environment', 'Rockstar teammates. You will be working with a strong team with prior work experience at Amazon, Microsoft, NVIDIA, Alibaba, etc.', 'Consistently evolve data model & data schema based on business and engineering needs', 'Locations ', 'Experience with BI software (preferably Metabase or Tableau).', 'Experience with deploying and maintaining data infrastructure in the cloud (experience with AWS preferred).', ' Owner of the core company data pipeline, responsible for scaling up data processing flow to meet the rapid data growth Consistently evolve data model & data schema based on business and engineering needs Implement systems tracking data quality and consistency Develop tools supporting self-service data pipeline management (ETL) SQL and MapReduce job tuning to improve data processing performance ', 'Responsibilities', 'Owner of the core company data pipeline, responsible for scaling up data processing flow to meet the rapid data growth', 'Proficient in SQL, specially with Postgres dialect.', 'Meritocracy - we promote based on performance, not tenure', 'Expertise in Python for developing and maintaining data pipeline code.', 'Experience with Apache Spark and PySpark library (experience with AWS extension of PySpark is a plus).', 'Toronto', 'About The Role', 'Requirements', 'Boston', 'About Jerry.ai', 'Palo Alto', 'Experience with Hadoop (or similar) Ecosystem.', ' 2+ years of data engineering experience within a rigorous engineering environment Proficient in SQL, specially with Postgres dialect. Expertise in Python for developing and maintaining data pipeline code. Experience with Apache Spark and PySpark library (experience with AWS extension of PySpark is a plus). Experience with BI software (preferably Metabase or Tableau). Experience with Hadoop (or similar) Ecosystem. Experience with deploying and maintaining data infrastructure in the cloud (experience with AWS preferred). Comfortable working directly with data analytics to bridge business requirements with data engineering ', 'Start-up energy working with a brilliant and passionate team', ' Toronto Boston Palo Alto', 'We’d love to hear from you if you like ', 'Comfortable working directly with data analytics to bridge business requirements with data engineering', 'Exponential growth (5 straight quarters of 50-100%+ quarter over quarter growth)', 'SQL and MapReduce job tuning to improve data processing performance', ' Start-up energy working with a brilliant and passionate team Exponential growth (5 straight quarters of 50-100%+ quarter over quarter growth) Flat structure and access to senior leadership for continuous mentorship Meritocracy - we promote based on performance, not tenure Rockstar teammates. You will be working with a strong team with prior work experience at Amazon, Microsoft, NVIDIA, Alibaba, etc. ', 'Flat structure and access to senior leadership for continuous mentorship', 'Implement systems tracking data quality and consistency', 'Develop tools supporting self-service data pipeline management (ETL)', 'personal concierge for your car and home']",Associate,Full-time,Engineering,Marketing and Advertising,2021-03-18 14:34:51
Data Engineer,DC Public Charter School Board,"Washington, DC",1 week ago,Be among the first 25 applicants,"['', 'About The Dc Public Charter School Board (dc Pcsb)', 'TO APPLY', 'Competencies and Qualifications:', 'Compensation', 'About The Role', 'DC PCSB is an equal opportunity employer committed to building a culturally diverse staff. We strive to foster an environment where everyone feels included. We believe that when people bring their unique identities, backgrounds, perspectives, and experiences to our community, we are able to truly achieve excellence in our work.']",Mid-Senior level,Full-time,Information Technology,Nonprofit Organization Management,2021-03-18 14:34:51
Data Engineer,Fluence,San Francisco Bay Area,1 week ago,136 applicants,"['', 'AWS\xa0', 'Here at Fluence, we strive to continuously improve, be intellectually curious and be adaptive to our customers and employee’s needs. Collaboration is key, both in our partnerships with our customers, and with each other. Fluence prioritizes the most critical efforts that allow for the greatest impact. To be successful in this role you have:', 'Cassandra\xa0', 'Fluence is defined by its unwavering commitment to safety, quality, and integrity. We take personal ownership in what we do, developing trust in our relationships with internal and external stakeholders. We firmly believe in having honest, forthcoming, and fair communications. You will work within the growing Software Development team, along-side data scientists, product managers, and subject matter experts to shape the evolution of our company and the future of the electricity grid.\xa0In this role you will: ', 'Strong computer science fundamentals, including knowledge of data structures and\xa0algorithms\xa0\xa0', 'Fluence IS AN EQUAL OPPORTUNITY EMPLOYER and fully subscribes to the principles of Equal Employment Opportunity to ensure that all applicants and employees are considered for hire, promotion, and job status without regard to race, color, religion, sex, national origin, age, disability, veteran status, sexual orientation, marital or familial status.', 'Postgres\xa0\xa0', 'GET IN TOUCH', 'Leading ', 'Proven ability to meet deadlines and deliver solutions quickly at high\xa0quality\xa0\xa0', 'Working on transforming a fundamental part of our society is exciting and fulfilling. It requires creativity, diversity of ideas and backgrounds, and building trust to effect change and move with speed. We respect our coworkers and customers. We listen to what others have to say, and we are inclusive. Technologies you will likely use in the role:', 'Organized and detail-oriented, able to work well under deadlines in a changing environment and perform multiple tasks effectively and\xa0concurrently\xa0\xa0', 'Passionate about learning, and tackling new and exciting technical\xa0challenges\xa0\xa0', 'Build, test, scale, and refine\xa0data ingestion, warehousing and analytical components across our energy market\xa0platforms', 'Experience with AWS or other cloud-based development\xa0\xa0', 'Build, test, scale, and refine\xa0data ingestion, warehousing and analytical components across our energy market\xa0platformsBe an owner of what you build, managing the entire product\xa0lifecycle\xa0\xa0Tackle a wide variety of challenges throughout the stack and contribute to all parts of our code\xa0base\xa0\xa0Be a team player in a small, flat-structured, highly collaborative\xa0environment\xa0\xa0', 'Fluence, a Siemens and AES company, is the global market leader in energy storage technology solutions and services, combining the agility of a technology company with the expertise, vision and financial backing of two well-established and respected industry giants. Building on the pioneering work of AES Energy Storage and Siemens energy storage, our goal is to create a more sustainable future by transforming the way we power our world. Providing design, delivery and integration, Fluence offers proven energy storage technology solutions that address the diverse needs and challenges of customers in a rapidly transforming energy landscape. ', 'Fluence currently has more than 2.4 gigawatts of projects in operation or awarded across 24 countries and territories worldwide. We topped the Navigant Research utility-scale energy storage leaderboard in 2018 and were named one of Fast Company’s Most Innovative Companies in 2019. In 2020, our sixth-generation Tech Stack won Commercial Technology of the Year at the 22nd annual S&P Global Platts Global Energy Awards.', 'Fun', 'Kotlin\xa0\xa0Python\xa0\xa0React,\xa0Redux\xa0\xa0Postgres\xa0\xa0Cassandra\xa0Redis\xa0\xa0Kubernetes\xa0AWS\xa0', 'Responsible', 'This\xa0Data\xa0engineer\xa0will be a part of the new Fluence Digital business unit, formed following Fluence’s acquisition of San Francisco-based start-up AMS. Fluence Digital’s technology uses artificial intelligence, advanced price forecasting, portfolio optimization and market bidding to ensure energy storage and flexible generation assets are responding optimally to price signals sent by the market.\xa0', 'Location: San Francisco, CA, or continental US', 'Do others come to you for your subject matter expertise? Are you excited by the challenge of working in a start-up atmosphere with a purpose? ', 'Please send your resume and cover letter to careers@fluenceenergy.com.', 'React,\xa0Redux\xa0\xa0', 'Redis\xa0\xa0', 'Be an owner of what you build, managing the entire product\xa0lifecycle\xa0\xa0', 'Expert knowledge of database languages, data flow architectures and modeling techniques\xa0\xa0', 'Agile', 'Data Engineer ', '\xa05+ years of professional software experience\xa0\xa0Proven ability to meet deadlines and deliver solutions quickly at high\xa0quality\xa0\xa0Expert knowledge of database languages, data flow architectures and modeling techniques\xa0\xa0Passionate about learning, and tackling new and exciting technical\xa0challenges\xa0\xa0Strong computer science fundamentals, including knowledge of data structures and\xa0algorithms\xa0\xa0Organized and detail-oriented, able to work well under deadlines in a changing environment and perform multiple tasks effectively and\xa0concurrently\xa0\xa0Experience with AWS or other cloud-based development\xa0\xa0Enjoys pair\xa0programming\xa0\xa0Previous experience working at a\xa0startup\xa0\xa0', 'Python\xa0\xa0', 'Previous experience working at a\xa0startup\xa0\xa0', 'Be a team player in a small, flat-structured, highly collaborative\xa0environment\xa0\xa0', 'Kubernetes\xa0', 'Enjoys pair\xa0programming\xa0\xa0', 'Kotlin\xa0\xa0', '\xa0', 'ABOUT FLUENCE', '\xa05+ years of professional software experience\xa0\xa0', 'Tackle a wide variety of challenges throughout the stack and contribute to all parts of our code\xa0base\xa0\xa0']",Mid-Senior level,Full-time,Information Technology,Renewables & Environment,2021-03-18 14:34:51
Data Engineer,Tradeswell,"Baltimore, MD",3 weeks ago,36 applicants,"['', 'Strong engineering background and understanding of Python, Spark, and Airflow, Data Lakes & Warehousing, AWS Step Functions', ' Integrate with 3rd party systems to ingest and normalize data for customers Design, build, and maintain business critical data infrastructure Collaborate with other engineers, product managers, and data scientists to solve problems for internal and external customers ', 'Experience managing managing infrastructure for data pipelines, ETL process, and data warehouses', 'Integrate with 3rd party systems to ingest and normalize data for customers', 'A desire to continually grow, learn, and iterate on the product and yourself', 'Any unsolicited resumes/candidate profiles submitted through our website or to personal email accounts of employees of Tradeswell are considered property of Tradeswell and are not subject to payment of agency fees.', 'Design, build, and maintain business critical data infrastructure', ' An passion for data, data engineering, and data science Strong engineering background and understanding of Python, Spark, and Airflow, Data Lakes & Warehousing, AWS Step Functions Experience managing managing infrastructure for data pipelines, ETL process, and data warehouses A desire to continually grow, learn, and iterate on the product and yourself ', 'Collaborate with other engineers, product managers, and data scientists to solve problems for internal and external customers', 'An passion for data, data engineering, and data science']",Entry level,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
"Data Engineer, Power BI",Softchoice,"Malvern, PA",1 week ago,Be among the first 25 applicants,"['', 'Apache Spark', 'Leading by collaboration: Working with software developers, application architects, QA engineers, and operations teams to deliver on – and exceed – client expectations. You will also team up with SME’s, practice leads and vendors to fine tune our strategy and overcome obstacles to adoption at every step.', 'Bonus points for having multiple related certifications for Azure, AWS or GCP.', ' Great communication and consultative skills, with the ability to occasionally work onsite with customers and be able to present technically advanced topics to large groups of people with confidence and impact. End to end ability to chart a strategy and identify the solution, while also being able to deliver it and support the client through adoption and maturity Experience working in professional services and/or software engineering for clients in a provider to client relationship. Bonus if experience in a consulting firm. ', ' interview & employment accommodatio', 'Delivering excellence: Developing, testing and managing Data cloud native and 3rd party tooling and automation technologies, delivered through code, templates and scripts to ensure top-quality client experiences, delivered with consistency and efficiency. ', 'Foundational cloud IaaS and PaaS technologies', 'Database tuning and optimization', 'Proven experience driving results with relevant technologies and methods, strong familiarity, and knowledge of cloud platforms such as Azure.', 'Diversity, Inclusion & Belonging', ' Delivering excellence: Developing, testing and managing Data cloud native and 3rd party tooling and automation technologies, delivered through code, templates and scripts to ensure top-quality client experiences, delivered with consistency and efficiency.  Leading by collaboration: Working with software developers, application architects, QA engineers, and operations teams to deliver on – and exceed – client expectations. You will also team up with SME’s, practice leads and vendors to fine tune our strategy and overcome obstacles to adoption at every step. Evangelizing the New: You aren’t just deeply familiar with cutting-edge concepts and technologies. You know how to tailor your insights to customers, occasionally working onsite with customers as a consultant to explore their needs to provide a compelling vision forward. Identify business use cases, gather requirements to satisfy use case, analyze data sources to solve business case. Use a variety of data analysis and organizational tools to uncover insights. ', ""What you'll bring to the table:"", 'Some reasons why our employees love working here:\u202f ', 'Our commitment to your experience: ', 'Evangelizing the New: You aren’t just deeply familiar with cutting-edge concepts and technologies. You know how to tailor your insights to customers, occasionally working onsite with customers as a consultant to explore their needs to provide a compelling vision forward.', 'Solution design and data migration', 'Why you’ll love Softchoice: ', 'Experience in the following areas are an asset: Power BI Report Server Power BI Embedded Integration Services: Azure Data Factory Azure Cosmos DB Azure Synapse Oracle Database tuning and optimization Apache Spark GIT  ', 'The impact you will have: ', 'An appreciation of report design and the user experience.', 'Working knowledge of SQL Server, Azure SQL Database and Amazon RDS.', 'Power BI Embedded', 'Experience working in professional services and/or software engineering for clients in a provider to client relationship. Bonus if experience in a consulting firm.', 'Our team members have 2 paid volunteer days per year to give back to a cause of their choice. ', 'Use a variety of data analysis and organizational tools to uncover insights.', 'Excellent communication skills, ability to work in an agile environment.', 'Prior to commencing employment: ', 'Experience in designing and implementing BI solutions with expertise in the following areas: Solution design and data migration Security and Compliance Reporting Tools: Power BI, SSRS, etc. Working knowledge of TSQL, PLSQL and DAX Foundational cloud IaaS and PaaS technologies  ', 'Identify business use cases, gather requirements to satisfy use case, analyze data sources to solve business case.', 'Power BI Report Server', 'Softchoice has been recognized as a Best Workplace in Canada by the Great Place to Work Institute for 15 consecutive years. ', 'Security and Compliance', 'Knowledge of AWS data services (i.e. Redshift, S3, RDS)', 'We offer an opportunity to build a career in the technology industry. ', 'We have raised over $3 Million through our team member run charity Softchoice Cares. ', 'Expert-level understanding as a Power BI Developer and for data analysis.', 'GIT', 'You will have the opportunity to take an ownership position here at Softchoice.\u202f\u202f ', 'Good understanding about DevOps culture, methodologies, coding and automation.', ' Experience in designing and implementing BI solutions with expertise in the following areas: Solution design and data migration Security and Compliance Reporting Tools: Power BI, SSRS, etc. Working knowledge of TSQL, PLSQL and DAX Foundational cloud IaaS and PaaS technologies   Experience in the following areas are an asset: Power BI Report Server Power BI Embedded Integration Services: Azure Data Factory Azure Cosmos DB Azure Synapse Oracle Database tuning and optimization Apache Spark GIT   Knowledge of AWS data services (i.e. Redshift, S3, RDS) Bonus points for having multiple related certifications for Azure, AWS or GCP. ', 'Azure Cosmos DB', 'Oracle', 'End to end ability to chart a strategy and identify the solution, while also being able to deliver it and support the client through adoption and maturity', ' Power BI Report Server Power BI Embedded Integration Services: Azure Data Factory Azure Cosmos DB Azure Synapse Oracle Database tuning and optimization Apache Spark GIT ', 'Great communication and consultative skills, with the ability to occasionally work onsite with customers and be able to present technically advanced topics to large groups of people with confidence and impact.', 'Require an accommodation? We are ready to help: ', 'At least 3 years of relevant experience designing and implementing Power BI desktop and service.', 'Softchoice has been certified as a Great Place to Work in the United States for several years. ', ' Expert-level understanding as a Power BI Developer and for data analysis. Good understanding about DevOps culture, methodologies, coding and automation. Proven experience driving results with relevant technologies and methods, strong familiarity, and knowledge of cloud platforms such as Azure. Excellent communication skills, ability to work in an agile environment. At least 3 years of relevant experience designing and implementing Power BI desktop and service. An appreciation of report design and the user experience. Working knowledge of SQL Server, Azure SQL Database and Amazon RDS. ', 'Azure Synapse', 'Working knowledge of TSQL, PLSQL and DAX', ' Solution design and data migration Security and Compliance Reporting Tools: Power BI, SSRS, etc. Working knowledge of TSQL, PLSQL and DAX Foundational cloud IaaS and PaaS technologies ', 'Reporting Tools: Power BI, SSRS, etc.', ""What you'll do:"", 'Inclusion & Equal opportunity employment: ', ' Softchoice has been recognized as a Best Workplace in Canada by the Great Place to Work Institute for 15 consecutive years.  Softchoice has been certified as a Great Place to Work in the United States for several years.  We offer meaningful work that drives professional development.  Our team members have 2 paid volunteer days per year to give back to a cause of their choice.  We offer an opportunity to build a career in the technology industry.  We have raised over $3 Million through our team member run charity Softchoice Cares.  You will have the opportunity to take an ownership position here at Softchoice.\u202f\u202f  ', 'Integration Services: Azure Data Factory', 'We offer meaningful work that drives professional development. ']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Associate Data Engineer - Blue Bell,Signant Health,"Blue Bell, PA",2 days ago,Be among the first 25 applicants,"['', ' Diversity and Inclusion Competencies ', ' Understands and follows coding standards.', ' Efficiently and effectively diagnoses and resolves defects.', ' Dedication and commitment to promote diversity, multiculturalism and inclusion in all work activities', ' Performs all work in accordance with documented Standard Operating Procedures (SOPs), Working Instructions.', ' Ability to meet established timelines.', ' Ability to quickly learn and apply new skills, procedures and approaches.', ' Dedication and commitment to promote diversity, multiculturalism and inclusion in all work activities  Ability to collaborate in diverse teams to foster productive outcomes.', ' Working with development team, integrates implemented code and database objects into release application. Performs smoke testing for released application as required.', 'Key Accountabilities/Decision Making & Influence', ' Develops and maintains application components (under supervision).  Working with Business System Analysts (BSA), gains an understanding of the business requirements for assigned tasks.  Working with Technical Lead, Team Lead and/or Software Development Manager to gain an understanding of design patterns, principles and standards to be followed in implementing assigned tasks.  Understands and follows coding standards.  Working with development team, integrates implemented code and database objects into release application. Performs smoke testing for released application as required.  Efficiently and effectively diagnoses and resolves defects.  Identifies areas of the application impacted by the resolution and works with BSA and/or Software Test Engineer to define test cases required for the resolution.  Prepare all required change control documentation including updates to design and other technical documents as required.  Performs all work in accordance with documented Standard Operating Procedures (SOPs), Working Instructions.  Adheres to Good Clinical Practices (GCP), 21 CFR Part 11 and other regulatory requirements as required.', ' Working with Technical Lead, Team Lead and/or Software Development Manager to gain an understanding of design patterns, principles and standards to be followed in implementing assigned tasks.', ' Adheres to Good Clinical Practices (GCP), 21 CFR Part 11 and other regulatory requirements as required.', 'Knowledge, Skills & Attributes', ' Identifies areas of the application impacted by the resolution and works with BSA and/or Software Test Engineer to define test cases required for the resolution.', ' Ability to work in a fast paced environment.', 'Role Overview', ' Ability to collaborate in diverse teams to foster productive outcomes.', ' Strong verbal and written communication skills.', ' Ability to complete high quality technical documentation.', ' Develops and maintains application components (under supervision).', ' Strong verbal and written communication skills.  Ability to complete high quality technical documentation.  Ability to quickly learn and apply new skills, procedures and approaches.  Ability to meet established timelines.  Ability to work in a fast paced environment.', ' Prepare all required change control documentation including updates to design and other technical documents as required.', ' Working with Business System Analysts (BSA), gains an understanding of the business requirements for assigned tasks.']",Associate,Full-time,Information Technology,Biotechnology,2021-03-18 14:34:51
Data Engineer,TEKsystems,"Greenwood Village, CO",3 days ago,Be among the first 25 applicants,"['', 'Top Skills', 'Description', ' Expertise in SQL Expertise in Object Oriented languages such as Scala, Python (currently using Scala) building data pipelines Experience utilizing S3 and PostgreSQL as data storage locations', ' Expertise in Object Oriented languages such as Scala, Python (currently using Scala) building data pipelines', 'Additional Skills & Qualifications', ' Expertise in SQL', 'Experience Level', 'About TEKsystems', ' Experience utilizing S3 and PostgreSQL as data storage locations']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer - Monitoring Solutions,Nokia,N/A,3 weeks ago,Be among the first 25 applicants,"['', ' 3+ years of experience with schema design and dimensional data modeling ', ' 2+ years of Python or Java development experience ', ' Extensive experience using Python/Java including a strong grasp of object-oriented programming (OOP) fundamentals ', 'Key Responsibilities / Functions', ' Ability in managing and communicating data warehouse plans to internal clients ', ' Experience with predictive modeling and dissemination of research results; ', 'Infrastructure team', 'Schedule', ' Good communication and writing skills to facilitate productive collaboration with other team members and business units; ', 'Primary Location', 'Required Minimum Qualifications (Education, Technical Skills/Knowledge)', 'About Nokia IT', '  Extensive experience using Python/Java including a strong grasp of object-oriented programming (OOP) fundamentals   Extensive experience analyzing data using SQL   Experience designing and customizing databases   Experience/Knowledge of building, distrubuting and running container  ', ' Extensive experience analyzing data using SQL ', ' Experience/Knowledge of building, distrubuting and running container ', ' Strong knowledge of project management principles and concepts; ', 'Nokia ', ' 2+ years of SQL experience (NoSQL experience is a plus) ', ' Experience designing and customizing databases ', ' 3+ years of relevant experience such as implementing statistical analysis, developing cloud-based data lakes / data warehouses, managing data science projects, developing APIs, developing machine learning models, creating advanced data visualizations. ', 'Job', ' Experience solving problems with an emphasis on product development ', '  2+ years of Python or Java development experience   2+ years of SQL experience (NoSQL experience is a plus)   3+ years of experience with schema design and dimensional data modeling   Ability in managing and communicating data warehouse plans to internal clients   3+ years of relevant experience such as implementing statistical analysis, developing cloud-based data lakes / data warehouses, managing data science projects, developing APIs, developing machine learning models, creating advanced data visualizations.   Good communication and writing skills to facilitate productive collaboration with other team members and business units;   Strong knowledge of project management principles and concepts;   Experience solving problems with an emphasis on product development   Experience with predictive modeling and dissemination of research results;  ']",Not Applicable,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,The Bachrach Group,New York City Metropolitan Area,6 days ago,102 applicants,"['', 'Strong Data Development skills in Oracle SQL and SSIS, ETL, Linux/UNIX, Data Warehousing, and Data ModelingDatabase engineering experience with Microsoft SQL Server.Database experience in Oracle (10g) and Microsoft SQL Server (2005 or 2008)Experience with ETL products like SSIS or Clover ETL.Scripting in Unix or a Windows environmentExperience with various structured data storage methods.You are solution-driven with an ability to understand the big picture.You have great analytical and problem-solving skills.You have strong communication skills and an ability to synthesize information.Experience with IBM Infosphere Master Data Management is a plusUS Citizens and Green Card Holders (Permanent Residents) can only be considered.We are UNABLE to provide sponsorship to anyone.', 'You must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products. The right candidate will be excited by the prospect of optimizing and/or re-designing a data architecture to support the next generation of products and data initiatives.', 'Strong Data Development skills in Oracle SQL and SSIS, ETL, Linux/UNIX, Data Warehousing, and Data Modeling', 'Experience with ETL products like SSIS or Clover ETL.', 'Database engineering experience with Microsoft SQL Server.', 'Experience with IBM Infosphere Master Data Management is a plus', 'US Citizens and Green Card Holders (Permanent Residents) can only be considered.', 'WHAT WE ARE LOOKING FOR:', 'Data Engineer', 'You have great analytical and problem-solving skills.', 'You are solution-driven with an ability to understand the big picture.', 'Database experience in Oracle (10g) and Microsoft SQL Server (2005 or 2008)', 'We are UNABLE to provide sponsorship to anyone.', 'Scripting in Unix or a Windows environment', 'We are looking for a self-motivated Data Engineer to join a Data Management Team. You will be responsible for expanding and optimizing a Master Data Management Platform and ETL architecture, as well as optimizing data flow and collection for cross-functional teams. ', 'Experience with various structured data storage methods.', 'You have strong communication skills and an ability to synthesize information.']",Associate,Full-time,Information Technology,Hospital & Health Care,2021-03-18 14:34:51
Data Engineer,TuneCore,"Brooklyn, NY",7 days ago,Be among the first 25 applicants,"['', 'Be able to apply their skills in ingenious ways to solve new and existing problems.', 'Be highly proficient in SQL & Python with an ability to work with large and highly complex datasetsHave a thorough understanding of building data pipelines for RDBMS systems and various cloud-based data sources.Be comfortable working in a complex data environment involving in-depth data discovery, exploration and troubleshooting.Be able to apply their skills in ingenious ways to solve new and existing problems.Be open to exploring and learning new technologies on the go.Have the desire to lead and own the data engineering component of our new enterprise cloud-based data architecture.Be comfortable working in a cross-functional responsibility.Ensure adequate and thorough documentation of all existing and new processes.', ""TuneCore is going through some amazing changes. In partnership with our parent company, Believe International, we are expanding rapidly with an emphasis on teamwork and career-growth. Our focus continues to lie in building a workplace that incorporates respect, fairness, transparency and growth in every step of our employees' journey and we intend to continue working tirelessly to create a culture of positivity, diversity and kindness in an enjoyable workplace. "", 'Comprehensive knowledge of data-modeling principles specifically with a focus on data analytics', 'Ensure adequate and thorough documentation of all existing and new processes.', 'In-depth knowledge of RDBMS systems such as MySQL, MariaDB, PostgreSQL as well as cloud-based data warehousing solutions such as RedShift & Snowflake.', 'Strong ETL proficiency using GUI-based tools or code-based patterns.', 'Be comfortable working in a complex data environment involving in-depth data discovery, exploration and troubleshooting.', 'Be open to exploring and learning new technologies on the go.', 'Snowflake', 'Matillion / Fivetran / MDM', 'Experience with software engineering practicesMatillion / Fivetran / MDMAWSSnowflakeTableau', 'Be highly proficient in SQL & Python with an ability to work with large and highly complex datasets', 'Qualifications', 'Have a thorough understanding of building data pipelines for RDBMS systems and various cloud-based data sources.', 'Highly Desired Skills', 'Company Description', 'Must have at least 5 years of demonstrable work experience as a Data Engineer or Data Architect in an enterprise environment.In-depth knowledge of RDBMS systems such as MySQL, MariaDB, PostgreSQL as well as cloud-based data warehousing solutions such as RedShift & Snowflake.Comprehensive knowledge of data-modeling principles specifically with a focus on data analyticsStrong ETL proficiency using GUI-based tools or code-based patterns.Excellent communication and interpersonal skills with the ability to liaise with multiple teams.', 'Be comfortable working in a cross-functional responsibility.', 'TuneCore is seeking a seasoned Data Engineer who thrives in a dynamic and fast-paced environment and is able to work alongside our Software Engineering, Dev-Ops and Data Analytics teams to provide impactful solutions for all our data-driven business needs. ', 'Excellent communication and interpersonal skills with the ability to liaise with multiple teams.', 'AWS', 'Job Description', 'Candidates Should', 'Experience with software engineering practices', 'Tableau', 'Have the desire to lead and own the data engineering component of our new enterprise cloud-based data architecture.', 'Must have at least 5 years of demonstrable work experience as a Data Engineer or Data Architect in an enterprise environment.']",Mid-Senior level,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Data Engineer,Verizon,"Temple Terrace, FL",3 weeks ago,73 applicants,"['', 'Experience working in a network operations center environment.', 'Bachelor’s degree or four or more years of work experience.', 'Equal Employment Opportunity', 'Explore suitable options and designs for specific analytical solutions.', 'Experience knitting disperate data sources together', 'Master’s degree in Computer Science, Engineering, Statistics, IT, or related field.', 'Diversity and Inclusion at Verizon', 'When you join Verizon', 'Experience on Big Data platforms (i.e., Hadoop, Map/Reduce, Spark, HBase, CouchDB, Hive, etc.)', 'Gather requirements, assess gaps, and build roadmaps and architectures to help the analytics driven organization achieve its goals.Work closely with Data Analysts to ensure data quality and availability for analytical modelling.Explore suitable options and designs for specific analytical solutions.Define extract, load, and transform (ELT) based on jointly defined requirements.Prepare, clean, and massage data for use in modeling and prototypesIdentify gaps and implement solutions for data security, quality, and automation of processes.Support maintenance, bug fixes and, performance analysis along data pipeline.', 'What You’ll Be Doing...', 'Define extract, load, and transform (ELT) based on jointly defined requirements.', 'Four or more years of experience building data pipelines', 'Even Better If You Have', 'Work closely with Data Analysts to ensure data quality and availability for analytical modelling.', 'Identify gaps and implement solutions for data security, quality, and automation of processes.', 'Bachelor’s degree or four or more years of work experience.Four or more years of experience as a data engineerFour or more years of experience finding, cleaning, and preparing data for use by Data ScientistsExperience knitting disperate data sources togetherFour or more years of experience building data pipelinesExperience using SQL (i.e., PL/SQL or T-SQL with RDBMSs like Teradata, MS SQL Server, Oracle, etc.)Experience in data engineering, databases, and data warehouses.Strong experience with data engineering in Python.Ability to travel occasionally', ""You'll Need To Have"", 'Support maintenance, bug fixes and, performance analysis along data pipeline.', 'Gather requirements, assess gaps, and build roadmaps and architectures to help the analytics driven organization achieve its goals.', 'Prepare, clean, and massage data for use in modeling and prototypes', 'Four or more years of experience finding, cleaning, and preparing data for use by Data Scientists', 'Experience as an open source Contributor.', 'Master’s degree in Computer Science, Engineering, Statistics, IT, or related field.Experience with Scala, Julia, R, Python or other machine learning programming languageExperience on Big Data platforms (i.e., Hadoop, Map/Reduce, Spark, HBase, CouchDB, Hive, etc.)Strong analytical and problem-solving skills.Experience working in a network operations center environment.Experience as an open source Contributor.', 'diversity and inclusion', 'Experience with Scala, Julia, R, Python or other machine learning programming language', 'Experience in data engineering, databases, and data warehouses.', 'Experience using SQL (i.e., PL/SQL or T-SQL with RDBMSs like Teradata, MS SQL Server, Oracle, etc.)', 'Ability to travel occasionally', 'Strong experience with data engineering in Python.', 'Four or more years of experience as a data engineer', 'What we’re looking for...', 'Strong analytical and problem-solving skills.']",Mid-Senior level,Full-time,Information Technology,Telecommunications,2021-03-18 14:34:51
Data Engineer-Azure,"Vizient, Inc","Irving, TX",2 days ago,Be among the first 25 applicants,"['', ' Play an active role in story breakup and refinement sessions. ', ' Experience with cloud platform technologies such as Microsoft Azure or other PaaS technologies is preferred. ', 'Equal Opportunity Employer: Females/Minorities/Veterans/Individuals with Disabilities', ' Data analysis, data modeling, and data integration using Azure technologies like Azure Data Factory (ADF) is required. ', ' Design, develop, enhance, code, test, deliver and debug software independently.  Implement larger, more complex, or new stories for your product.  Play an active role in story breakup and refinement sessions.  Drive and lead story level architecture/design sessions.  Participate in feature level architecture/design sessions.  Recommend actions to improve procedures and standards.  Stay up to date on technical trends and emerging technology. ', ' Experience working with enterprise solution delivery in a large-scale distributed software design environment is preferred. ', ' Participate in feature level architecture/design sessions. ', ' Experience with PowerBI, Microstrategy, Java, C#, .NET, Typescript, JavaScript and Angular development is preferred. ', ' 2 or more years of experience in a software development or software engineer or data engineer role is required. ', ' Stay up to date on technical trends and emerging technology. ', 'Responsibilities', ' Hands on experience with Azure Synapse, Azure Stream Analytics, Azure Event Hubs, Azure Event Grid, Databricks, ADLS Gen 2, & Logic Apps is strongly preferred. ', ' Strong analytical and conceptual thinking along with database design skills is preferred. ', 'Qualifications', ' Relevant degree preferred.  2 or more years of experience in a software development or software engineer or data engineer role is required.  Data analysis, data modeling, and data integration using Azure technologies like Azure Data Factory (ADF) is required.  Strong aptitude and experience in writing and troubleshooting SQL and T-SQL is preferred.  Experience with Azure SQL and cloud data solutions within a data warehouse environment, using multiple data sources, data lake, etc. is strongly preferred.  Strong analytical and conceptual thinking along with database design skills is preferred.  Hands on experience with Azure Synapse, Azure Stream Analytics, Azure Event Hubs, Azure Event Grid, Databricks, ADLS Gen 2, & Logic Apps is strongly preferred.  Experience working with enterprise solution delivery in a large-scale distributed software design environment is preferred.  Experience working in an Agile based development environment, using Agile concepts such as Continuous Integration, TDD, and Paired Programming is preferred.  Experience with cloud platform technologies such as Microsoft Azure or other PaaS technologies is preferred.  Experience with Octopus deploy, and Jenkins is preferred.  Experience with PowerBI, Microstrategy, Java, C#, .NET, Typescript, JavaScript and Angular development is preferred.  Microsoft Azure certifications in database and business intelligence technologies is preferred. ', 'Summary', ' Experience with Octopus deploy, and Jenkins is preferred. ', ' Design, develop, enhance, code, test, deliver and debug software independently. ', ' Strong aptitude and experience in writing and troubleshooting SQL and T-SQL is preferred. ', ' Implement larger, more complex, or new stories for your product. ', ' Drive and lead story level architecture/design sessions. ', ' Experience working in an Agile based development environment, using Agile concepts such as Continuous Integration, TDD, and Paired Programming is preferred. ', ' Relevant degree preferred. ', ' Recommend actions to improve procedures and standards. ', ' Microsoft Azure certifications in database and business intelligence technologies is preferred. ', ' Experience with Azure SQL and cloud data solutions within a data warehouse environment, using multiple data sources, data lake, etc. is strongly preferred. ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Senior Data Engineer - Operations Data and Analytics,Travelers,"Hartford, CT",2 days ago,Be among the first 25 applicants,"['', 'Develops and maintains relationships across the enterprise.', 'Must demonstrate a proactive nature with willingness to contribute, collaborate and work in an Lean-Agile team environment.', 'Primary Job Duties & Responsibilities', 'Education, Work Experience, & Knowledge', 'Develops process to acquire and integrate data.', 'Operationalizes and automates more complex (more systems, data sets and streams, size of data sets more substantial) products into business.', 'Company Summary', 'Experience with AWS', 'Strong SQL skills', 'Experience putting applications in Production at Travelers, preferred.', 'Minimum Qualifications', 'Visualization platforms: QlikView, Tableau, MicroStrategy and Qlik Sense', 'Python programming skills (or another programming language)Experience building data productsExperience building data pipelinesStrong SQL skillsExperience in code repository tool GitHubExperience with AWSExperience putting applications in Production at Travelers, preferred.Must demonstrate a proactive nature with willingness to contribute, collaborate and work in an Lean-Agile team environment.', 'Proactively looks to improve and optimize data products.', 'Employment Practices', 'Performs analysis of complex (type, quality, volume) sources to determine value and use. Determines and recommends data to include in analytical projects.', 'Degree in STEM related field', '5 years of relevant experience with data tools, techniques, and manipulation preferred.', 'Ensures customer satisfaction through professional communication, follow-up, and responsiveness to issues.', 'Job Description Summary', 'Reviews unfamiliar data sources. Connects to value for business. Able to link to systems.Develops process to acquire and integrate data.Independently reviews, prepares, designs and integrates complex (type, quality, volume) data correcting problems and recommends data cleansing/quality solutions to major issues.Works within Travelers standards, processes, and protocols.Leads medium scale projects and coordinates aspects of larger projects with limited supervision.Performs analysis of complex (type, quality, volume) sources to determine value and use. Determines and recommends data to include in analytical projects.Utilizes business knowledge to explain technical activities in business terms.Actively seeks opportunities to expand technical knowledge and capabilities.Develops and maintains relationships across the enterprise.Operationalizes and automates more complex (more systems, data sets and streams, size of data sets more substantial) products into business.Proactively looks to improve and optimize data products.Applies knowledge of current industry trends and techniques to formulate solutions within the context of assigned projects and/or enhancements.Ensures customer satisfaction through professional communication, follow-up, and responsiveness to issues.Builds effective relationships with stakeholders.Provides guidance and mentorship to lower level technical employees.', 'Leads medium scale projects and coordinates aspects of larger projects with limited supervision.', 'Experience in code repository tool GitHub', 'Actively seeks opportunities to expand technical knowledge and capabilities.', 'Experience building data pipelines', 'Job Specific Technical Skills & Competencies', 'Target Openings', 'Provides guidance and mentorship to lower level technical employees.', 'Builds effective relationships with stakeholders.', 'Applies knowledge of current industry trends and techniques to formulate solutions within the context of assigned projects and/or enhancements.', 'Reviews unfamiliar data sources. Connects to value for business. Able to link to systems.', 'Python programming skills (or another programming language)', 'Independently reviews, prepares, designs and integrates complex (type, quality, volume) data correcting problems and recommends data cleansing/quality solutions to major issues.', 'Advanced knowledge of data tools, techniques, and manipulation preferred. E', 'Programming languages - SAS, SQL, Spark, Python, Hive, AWS', 'Degree in STEM related fieldAdvanced knowledge of data tools, techniques, and manipulation preferred. EProgramming languages - SAS, SQL, Spark, Python, Hive, AWSVisualization platforms: QlikView, Tableau, MicroStrategy and Qlik Sense5 years of relevant experience with data tools, techniques, and manipulation preferred.', '4 years of relevant experience with data tools, techniques, and manipulation required.', 'Experience building data products', 'Works within Travelers standards, processes, and protocols.', 'Utilizes business knowledge to explain technical activities in business terms.']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Bayside Solutions,"Seattle, WA",2 days ago,25 applicants,"['', 'Our Company Bio:', ' Education & Experience:', 'Seattle, WA', 'Passionate about latest big data technologies, open source community presence is a big plus', 'Duration:', '.', 'Data Engineer\xa0', '5+ years of experience in big data ecosystem. Example technologies include batch and stream processing (e.g. Spark, Hive, Flink, Beam), analytical engines (e.g. Presto, Druid), search platform (e.g. Solr/Lucene), tooling (e.g. Airflow, Jupyter, Superset, Tableau), and storage format (e.g. Iceberg)Excellent verbal and written communication skills, able to collaborate cross-functionally with data science, machine learning, data platform and analytics teamsCustomer-focused mindset, with emphasis on user experience and satisfactionSuperb problem-solving skills, and able to thrive in a fast-paced and dynamic environmentHands-on in designing, building, scaling, and troubleshooting solutions to big data problemsMust be self-driven, and able to provide advice and support to users to properly integrate with our data platformProgramming experience in Java, Python, Scala, or similar languagesPassionate about latest big data technologies, open source community presence is a big plusExperience with AWS, Kubernetes, Infrastructure-as-code, and data privacy & compliance is a big plus', 'Customer-focused mindset, with emphasis on user experience and satisfaction', 'Key Qualifications:', 'Superb problem-solving skills, and able to thrive in a fast-paced and dynamic environment', 'Must be self-driven, and able to provide advice and support to users to properly integrate with our data platform', 'www.baysidesolutions.com', 'Excellent verbal and written communication skills, able to collaborate cross-functionally with data science, machine learning, data platform and analytics teams', 'Programming experience in Java, Python, Scala, or similar languages', 'Description:', 'Hands-on in designing, building, scaling, and troubleshooting solutions to big data problems', 'Experience with AWS, Kubernetes, Infrastructure-as-code, and data privacy & compliance is a big plus', '5+ years of experience in big data ecosystem. Example technologies include batch and stream processing (e.g. Spark, Hive, Flink, Beam), analytical engines (e.g. Presto, Druid), search platform (e.g. Solr/Lucene), tooling (e.g. Airflow, Jupyter, Superset, Tableau), and storage format (e.g. Iceberg)']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Slalom,"Atlanta, GA",3 days ago,106 applicants,"['', 'Developing ', ""Slalom is a modern consulting firm focused on strategy, technology, and business transformation. In 39 markets around the world, Slalom's\u202fteams have autonomy to move fast and do what's right. They are backed by\u202fregional innovation hubs, a global culture of collaboration, and partnerships with the world's top technology providers. Founded in 2001 and headquartered in Seattle, Slalom has organically grown to over 8,000 employees. Slalom has been named one of Fortune's 100 Best Companies to Work For five years running and is regularly recognized by\u202femployees as a best place to work. Learn more at slalom.com. "", 'Architecting', 'What You’ll Do', 'Slalom is an equal opportunity employer that is committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veterans status, or any other characteristic protected by federal, state, or local laws.\u202f', 'About Us', 'Implementing', 'Working', 'Who You’ll Work With', 'Slalom prides itself on helping team members thrive in their work and life. As a result, Slalom is proud it invest in benefits that include: meaningful time off and paid holidays, parental leave, 401(k) with a match, a range of choices for highly subsidized health, dental, & vision coverage, adoption and fertility assistance, and short/long-term disability. We also offer additional benefits such as a yearly $350 reimbursement account for any well-being related expenses as well as discounted home, auto, and pet insurance. ', 'Solving', 'Being', 'For recruiters posting internal and external jobs in Colorado - to comply with State law, benefit and pay ranges must disclosed within the job postings. Please change the above US benefit statement to include pay range. Please refer to Slalom comp ranges for the role you are posting; however, if you have a more specific target salary range for your role you can include this range.', 'What You’ll Bring']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,REI Systems,"Sterling, VA",4 days ago,Be among the first 25 applicants,[''],Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data/Analytics Engineer,Addepar,"Salt Lake City, UT",5 days ago,Be among the first 25 applicants,"['', 'You have built Data Lakes in S3 or Hadoop', 'Experience tuning SQL queries and data pipelines', 'Exposure to data streaming tools and technologies like kafka', 'Champion the strategy to build data security framework to store, move and access data', 'Establish best data practices and formalizing data governance framework ', 'Building reports and dashboards using BI tools', 'Ability to multi-task and change priorities', 'Nice To Have', 'Provide production support for data integration and transformation pipelines', 'Build and maintain data reconciliation processes', 'Responsibilities', 'Build and maintain scalable ELT/ETL processes using workflow automation tools ', 'Experience working with and administering one or more columnar storage systems (Preferred: Snowflake OR Redshift, Bigquery, etc)', ' 5+ years of experience as a data engineer and/or software engineer in a SaaS or financial services business 2+ years of experience working with workflow orchestration tools( Preferred: Airflow, Prefect OR Luigi, autosys, etc) 4+ years of experience building ELT/ETL processes using one or more tools(E.g. Python, Informatica, SSIS, Pentaho, etc) Experience working with and administering one or more columnar storage systems (Preferred: Snowflake OR Redshift, Bigquery, etc) Knowledge and skills with Looker will be important, not critical  Must have completed at least one full cycle of building a data lake and/or a data warehouse Exposure to data streaming tools and technologies like kafka Experience working in cloud data environments(Preferred: AWS OR GCP, Azure, etc) Strong software development skills in Python and SQL Experience tuning SQL queries and data pipelines Ability to multi-task and change priorities Stellar communications skills for requirements gathering ', 'Build realtime and batch data integrations to integrate data from disparate source systems into the DW', 'Stellar communications skills for requirements gathering', 'Designing dimensional models and be able to cleanse and transform raw data into structured format', 'Experience working with CRM systems like Salesforce', 'Requirements', 'Administer and maintain data pipeline tools and data warehouse', '4+ years of experience building ELT/ETL processes using one or more tools(E.g. Python, Informatica, SSIS, Pentaho, etc)', ' You have built Data Lakes in S3 or Hadoop Experience working with CRM systems like Salesforce Exposure to BI/reporting tools ( Preferred:Looker OR Tableau, Sisense/Periscope, Cognos, etc) Building reports and dashboards using BI tools Basic knowledge of statistics  Experience working with with SaaS or Fintech companies is a plus  ', 'Knowledge and skills with Looker will be important, not critical ', 'Basic knowledge of statistics ', ' Build realtime and batch data integrations to integrate data from disparate source systems into the DW Build and maintain scalable ELT/ETL processes using workflow automation tools  Designing dimensional models and be able to cleanse and transform raw data into structured format Administer and maintain data pipeline tools and data warehouse Provide production support for data integration and transformation pipelines Partner with analytics engineer and data analysts to build tooling to be able to effectively move data in and out of the data warehouse/data pond Champion the strategy to build data security framework to store, move and access data Establish best data practices and formalizing data governance framework  Scale and tune data pipelines, databases and SQL queries Build and maintain data reconciliation processes ', 'Strong software development skills in Python and SQL', 'Must have completed at least one full cycle of building a data lake and/or a data warehouse', '5+ years of experience as a data engineer and/or software engineer in a SaaS or financial services business', 'Scale and tune data pipelines, databases and SQL queries', 'Experience working in cloud data environments(Preferred: AWS OR GCP, Azure, etc)', 'Exposure to BI/reporting tools ( Preferred:Looker OR Tableau, Sisense/Periscope, Cognos, etc)', '2+ years of experience working with workflow orchestration tools( Preferred: Airflow, Prefect OR Luigi, autosys, etc)', 'Experience working with with SaaS or Fintech companies is a plus ', 'Partner with analytics engineer and data analysts to build tooling to be able to effectively move data in and out of the data warehouse/data pond']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Lead Data Engineer,Insight Global,"Portland, Oregon Metropolitan Area",,N/A,"['Desired Skills and Experience\xa0', '5+ yeas of Data Engineering experience utilizing SQL, Python, Spark, and AWS (EMR)', 'Experience with data visualization tools like Tableau', '', '2+ years of technical lead experience within a Data Engineering team', 'Plusses', 'Apache Nifi', '\xa0', 'Enterprise-level experience working with large consumer data sets', 'The Lead Data Engineer will be responsible for assisting a team of about 10 technical resources with technical monitoring and troubleshooting. The Lead will have a small programming responsibility, but the majority of their role will be centered around leading meetings, assisting engineers to get unblocked, expert code reviews, solution proposals, and creating technical user stories. Additional responsibilities include communicating with the supply chain business units and ensuring project timelines are maintained.', 'Day-to-Day\xa0', 'Supply chain analytics experience']",Mid-Senior level,Contract,Information Technology,Staffing and Recruiting,2021-03-18 14:34:51
Data Engineer,"Coalition, Inc.","Boston, MA",4 weeks ago,50 applicants,"['', 'Bonus Points', 'In-depth knowledge of AWS or other cloud-hosted platforms relevant to data engineering', 'Excellent oral and written communications skills at all levels', 'Why Coalition?', 'Implement risk models for various insurance products', 'About Us', 'Responsibilities', 'Expert-level knowledge of SQL, Python, R, or similar language used for data engineering', 'Bachelor’s degree in Computer Science or a related field preferred', 'Evaluate, recommend, and implement data pipelines for a variety of data sources used at Coalition', 'Deep understanding of ETL pipelines, statistical modeling, data analytics, and large scale data streaming', 'Requirements', '3+ years working with large disparate data sets', 'A proven track record of successfully automating business value from data insights', 'Explore new data sources and develop insights into existing data sources that improve business efficiency', 'Prior experience with insurance or network security technologies', 'Coalition Engineering', 'Recent press releases:', 'Experience with at least one big data search tool, such as Elastic', 'Experience with data visualization technologies', 'Deliver production-quality software implementations for ETL and streaming pipelines']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer - Marketing,Squarespace,"New York, NY",4 weeks ago,Be among the first 25 applicants,"['', 'Help guide the team towards best practices, and mentor teammates', 'Participate in architectural decisions across the organization, particularly with respect to the impact on data collection', 'Work with data consumers to identify self-service opportunities, and make sure they have access to the data they need to make decisions', 'Experience with Google Cloud Platform or other cloud providers is a plus', 'Identify, build and refine processes to make pipeline implementations faster, simpler, and less error-prone for Data Scientists and Analysts', 'Free lunch and snacks at all offices', 'About Squarespace', ' Experience building data pipelines and API integrations using Java, Scala, Python, or SQL Experience creating and/or modifying complex ETL jobs Strong systems design knowledge; you know how to architect data pipelines and how storage and compute fit together Prior experience making large datasets accessible; familiarity with Hive and Spark is a plus Experience with Google Cloud Platform or other cloud providers is a plus 4+ years of industry experience ', 'Diagramming and communicating process flows to identify and prioritize improvements', 'Build end-to-end data pipelines, consuming from a variety of sources', 'Strong systems design knowledge; you know how to architect data pipelines and how storage and compute fit together', 'Experience creating and/or modifying complex ETL jobs', 'Responsibilities', 'Equity plan for all employees', 'Coordinate with Data Scientists on advanced analytical models concerning marketing attribution, lifetime value, and other core marketing concepts', 'Up to 20 weeks of paid family leave', 'Retirement benefits with employer match', 'Education reimbursement', 'Qualifications', ' Health insurance with 100% premium covered for you and your dependent children Flexible vacation & paid time off Up to 20 weeks of paid family leave Equity plan for all employees Retirement benefits with employer match Fertility and adoption benefits Free lunch and snacks at all offices Education reimbursement Dog-friendly workplace in New York office Commuter benefit in the form of reduced tax (Ireland) and pretax (US) ', 'Flexible vacation & paid time off', ' Build end-to-end data pipelines, consuming from a variety of sources Identify, build and refine processes to make pipeline implementations faster, simpler, and less error-prone for Data Scientists and Analysts Diagramming and communicating process flows to identify and prioritize improvements Set timeliness and correctness goals for our data pipelines, maintain sufficient monitoring & coverage to ensure that incidents and outages are mitigated appropriately Work with data consumers to identify self-service opportunities, and make sure they have access to the data they need to make decisions Coordinate with Data Scientists on advanced analytical models concerning marketing attribution, lifetime value, and other core marketing concepts Contribute to a technical roadmap for the Marketing data program, and set priorities for work across the team Participate in architectural decisions across the organization, particularly with respect to the impact on data collection Help guide the team towards best practices, and mentor teammates ', 'Prior experience making large datasets accessible; familiarity with Hive and Spark is a plus', 'Experience building data pipelines and API integrations using Java, Scala, Python, or SQL', 'Health insurance with 100% premium covered for you and your dependent children', 'Today, more than a million people around the globe use Squarespace to share different perspectives and experiences with the world. Not only do we embrace and celebrate the diversity of our customer base, but we also strive for the same in our employees. At Squarespace, we are committed to equal employment opportunity regardless of race, color, ethnicity, ancestry, religion, national origin, gender, sex, gender identity or expression, sexual orientation, age, citizenship, marital or parental status, disability, veteran status, or other class protected by applicable law. We are proud to be an equal opportunity workplace.', 'Benefits & Perks', 'Commuter benefit in the form of reduced tax (Ireland) and pretax (US)', 'Dog-friendly workplace in New York office', 'Contribute to a technical roadmap for the Marketing data program, and set priorities for work across the team', 'Fertility and adoption benefits', '4+ years of industry experience', 'Set timeliness and correctness goals for our data pipelines, maintain sufficient monitoring & coverage to ensure that incidents and outages are mitigated appropriately']",Mid-Senior level,Full-time,Advertising,Computer Software,2021-03-18 14:34:51
Data Software Engineer II,Microsoft,"Redmond, WA",3 days ago,Be among the first 25 applicants,"['', 'Exposure to Agile software delivery and modern DevOps practices (including Git, CI/CD).', 'Solid understanding of Cloud centric data approach including SQL and NoSQL databases (ex. Azure SQL, Azure Cosmos DB).Proficiency in writing complex, highly optimized queries across large data sets.Experience working with Data Lake, Data Factory, Databricks or Apache Spark.Experience deploying cloud-based data services, including data pipeline orchestration using Infrastructure as Code (IaC).Experience with C#, SQL, Scala/Python/Java, T-SQL or similar scripting language.Exposure to Agile software delivery and modern DevOps practices (including Git, CI/CD).Data visualization and dashboard design experience is a plus but not required.', 'At least 3 years’ experience using large data systems (""Big Data"") and building databases, ETL, and reporting solutions.', 'Now let us talk about you', 'Required qalifications:', 'Experience working with Data Lake, Data Factory, Databricks or Apache Spark.', 'Data visualization and dashboard design experience is a plus but not required.', 'A minimum of a bachelor’s degree in Machine Learning/Data Science, Applied Statistics, Mathematics, Engineering or a related field, or equivalent alternative education, skills, and/or practical experience.', 'Responsibilities', 'Microsoft runs on trust. ', '3rd Party Compliance Engineering', 'Qualifications', 'Continuous learning and working closely with the latest technologies (AI/ML, Cloud Lakehouse/Synapse Analytics, etc.) in Microsoft.\u202f', 'Design, develop, and maintain data pipelines for real-time/batch analysis, reporting, optimization, data collection, and related functions.', 'Collaborate with colleagues including product owners, data scientists and other engineers to identify a business or engineering problem and translate it to a data science problem, dig out sources of data, conduct the analysis that would reveal useful insights, and help product teams to operationalize the solution.', 'Proficiency in writing complex, highly optimized queries across large data sets.', 'Design, develop, and maintain data pipelines for real-time/batch analysis, reporting, optimization, data collection, and related functions.Build and maintain data processing infrastructure that support complex analysis across our data science, product, and experimentation teams.Use large data sets to resolve major business and functional issues while improving data reliability, efficiency, and quality.Collaborate with colleagues including product owners, data scientists and other engineers to identify a business or engineering problem and translate it to a data science problem, dig out sources of data, conduct the analysis that would reveal useful insights, and help product teams to operationalize the solution.Continuous learning and working closely with the latest technologies (AI/ML, Cloud Lakehouse/Synapse Analytics, etc.) in Microsoft.\u202f', 'Experience deploying cloud-based data services, including data pipeline orchestration using Infrastructure as Code (IaC).', 'Experience with C#, SQL, Scala/Python/Java, T-SQL or similar scripting language.', 'A minimum of a bachelor’s degree in Machine Learning/Data Science, Applied Statistics, Mathematics, Engineering or a related field, or equivalent alternative education, skills, and/or practical experience.At least 3 years’ experience using large data systems (""Big Data"") and building databases, ETL, and reporting solutions.', 'Solid understanding of Cloud centric data approach including SQL and NoSQL databases (ex. Azure SQL, Azure Cosmos DB).', 'Build and maintain data processing infrastructure that support complex analysis across our data science, product, and experimentation teams.', 'Preferred Qualifications', 'Use large data sets to resolve major business and functional issues while improving data reliability, efficiency, and quality.']",Not Applicable,Full-time,Engineering,Computer Hardware,2021-03-18 14:34:51
Data Engineer - ou4Tefwp - IN0009511353,Ontario Systems,"Indianapolis, IN",2 days ago,Be among the first 25 applicants,[],Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Acquisition Engineer,HealthVerity,"Philadelphia, PA",3 weeks ago,Be among the first 25 applicants,"['', ' A self-starter who enjoys working in a small, rapidly changing, fast paced environment', ' Methodical, executing through several approaches to determine the best fit', ' Standardizing on common data models across data types', ' Knowledge of the healthcare industry and analytics utilized by pharmaceutical marketing teams', ' Energized by learning even if outside the scope of day-to-day responsibilities', ' Desired Skills And Experience ', ' About You ', ' Support Data Architect in the mapping and normalization process, providing expertise and authority on accurate meanings in source data', ' Empowering clients with highly rewarding data discovery and licensing tools Ingesting and managing billions of healthcare records from a wide variety of partners Standardizing on common data models across data types Orchestrating an industry-leading HIPAA privacy layer Innovating our proprietary de-identification and data science algorithms Building a culture that supports rapid iteration and new possibilities', ' Proven analytical, evaluative, and problem-solving abilities', ' Innovating our proprietary de-identification and data science algorithms', ' Provide expertise on all healthcare data types: medical claims transactions (e.g. 837 and 835), pharmacy claims (NCPDP D.0), EMR/EHR, lab transactions, and other emerging data assets', ' Empowering clients with highly rewarding data discovery and licensing tools', ' Source and oversee the lifecycle of reference data sources specified by the Data Architecture team', ' 4+ years’ experience in the healthcare data industry, preferably in a consulting environment', ' Comfortable working on several different tasks throughout your workday', ' Proficient in programming against large data assets with a working knowledge of SQL, preferably also knowledgeable in SAS and/or R', ' Ingesting and managing billions of healthcare records from a wide variety of partners', ' Analyze all vendor data assets and correct complex data anomalies', ' Confident enough to course correct a process or team when required', ' Analyze all vendor data assets and correct complex data anomalies Provide guidance and support during the vendor onboarding process which includes data ingestion, normalization, and QC activities Provide expertise on all healthcare data types: medical claims transactions (e.g. 837 and 835), pharmacy claims (NCPDP D.0), EMR/EHR, lab transactions, and other emerging data assets Source and oversee the lifecycle of reference data sources specified by the Data Architecture team Support Data Architect in the mapping and normalization process, providing expertise and authority on accurate meanings in source data', ' BS degree in math, statistics, or similar 4+ years’ experience in the healthcare data industry, preferably in a consulting environment Proficient in programming against large data assets with a working knowledge of SQL, preferably also knowledgeable in SAS and/or R Subject matter expertise in a wide variety of healthcare data assets Knowledge of the healthcare industry and analytics utilized by pharmaceutical marketing teams Proven analytical, evaluative, and problem-solving abilities Extensive experience working in a team-oriented, collaborative environment', ' BS degree in math, statistics, or similar', ' Provide guidance and support during the vendor onboarding process which includes data ingestion, normalization, and QC activities', ' Extensive experience working in a team-oriented, collaborative environment', ' Orchestrating an industry-leading HIPAA privacy layer', ' Building a culture that supports rapid iteration and new possibilities', ' Subject matter expertise in a wide variety of healthcare data assets', ' A data geek with enviable SQL skills and a passionate sense of ownership A self-starter who enjoys working in a small, rapidly changing, fast paced environment Confident enough to course correct a process or team when required Methodical, executing through several approaches to determine the best fit Energized by learning even if outside the scope of day-to-day responsibilities Comfortable working on several different tasks throughout your workday', ' A data geek with enviable SQL skills and a passionate sense of ownership']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Harnham,"New York, United States",1 week ago,50 applicants,"['$150,000 - $170,000', '', '401K', 'KEYWORDS', 'YOU WILL NEED:', 'Work with the product team to maintain digital analytics standards', 'THE ROLE – Data Engineer', 'Experience with AWS', 'Ensure that data is defined, transformed, optimized, and ready for analysts', 'Maintain/build AWS cloud environment for the digital marketing team', 'Digital Marketing', 'HOW TO APPLY', '$150,000 - $170,000 base salary', 'Must have experience with one or more tools like Facebook Ads, Adsense, AdWords, etc', ""Big data experience using tool's like Hadoop, Spark, or Kafka"", 'THE COMPANY:', 'Create and maintain data pipelines for the digital marketing teamWork with the product team to maintain digital analytics standardsEnsure that data is defined, transformed, optimized, and ready for analystsWork with digital marketing tools like Facebook Ads, AdSense, AdWords, etcMaintain/build AWS cloud environment for the digital marketing team', 'Health benefits', ""Commercial experience as a data engineer in a digital marketing analytics spaceMust have experience with one or more tools like Facebook Ads, Adsense, AdWords, etcAdvanced SQL knowledgeCommercial experience with Java, Scala, or PythonExperience with AWSBig data experience using tool's like Hadoop, Spark, or Kafka"", 'They are a technology company that grows awareness for local restaurants to compete against major corporate chains. The platform gives the user local recommendations based on their tastes. The food the user orders can be picked up or sent out for delivery.', 'Work with digital marketing tools like Facebook Ads, AdSense, AdWords, etc', 'Commercial experience with Java, Scala, or Python', 'PTO and sick time off', 'THE BENEFITS:', '$150,000 - $170,000 base salaryHealth benefits401KPTO and sick time off', 'Advanced SQL knowledge', 'NYC', 'Data Engineer', 'Python, Java, Scala, Data Engineering, Digital Marketing, NYC, Facebook Ads, AdWords, Adsense, Hadoop, Spark, Kafka, AWS, SQL', 'Please register your interest by sending your resume to Jacob Ragland via the Apply link on this page.', '\xa0', 'Commercial experience as a data engineer in a digital marketing analytics space', 'As a Data Engineer, you will be responsible for creating and maintaining a data pipeline driving Digital Marketing strategy. Having comical experience in digital marketing is a must! You will be building the end-user analytics tools so having this background is a must.', 'Create and maintain data pipelines for the digital marketing team']",Associate,Full-time,Engineering,Marketing and Advertising,2021-03-18 14:34:51
Clinical Data Engineer,ERT,"Boston, MA",1 day ago,Be among the first 25 applicants,"['', ' Collaborate closely with customers, clinical data suppliers, project managers, engineers, and regulatory experts to ensure proper data governance and project focus. ', ' 5+ years of experience working in the pharmaceutical industry in data management / analyst roles ', ' At least 3 years of experience in Project life cycle activities on development and maintenance projects. ', ' Analyze and interpret transactional clinical data systems, 3rd party clinical data suppliers from various trial sources (study protocols, trial repositories, data aggregators, etc.) and understand relevant study data and context for clinical research. ', ' Experience and desire to work in a Global environment ', "" Bachelor's degree in a Computer Science or related field. "", ' Basic knowledge of Thinking Skills', ' Proven experience working with interdisciplinary teams in a fast-paced environment. ', ' Basic knowledge of Organizational Awareness', ' 5+ years of experience working in the pharmaceutical industry in data management / analyst roles  5+ hands-on experience in SQL and data modeling.  Strong background in Python, R, SAS or similar language.  Comfort working in secure environments with restricted data.  Excellent written and verbal ability, including the ability to be persuasive to a technical or non-technical audience.  Proven experience working with interdisciplinary teams in a fast-paced environment.  Experience with CDISC (ODM, SDTM, ADaM) a plus.  Strong hands on experience in SQL.  Experience in working with different data formats - json, XML, parquet  At least 3 years of experience in software development life cycle.  At least 3 years of experience in Project life cycle activities on development and maintenance projects.  At least 3 years of experience in design and architecture review.  Ability to work in team in diverse/ multiple stakeholder environment  Strong Analytical skills  Experience and desire to work in a Global environment ', ' Strong background in Python, R, SAS or similar language. ', ' Basic knowledge of Project Management', 'Responsibilities', ' Ability to work in team in diverse/ multiple stakeholder environment ', ' Strong Analytical skills ', ' Basic knowledge of Leadership', ' Strong hands on experience in SQL. ', ' Architect data flows, sequence diagrams and data pipeline specifications for engineers to execute development of their components. ', ' 5+ hands-on experience in SQL and data modeling. ', 'Qualifications', ' Experience in working with different data formats - json, XML, parquet ', ' Candidate must be located within commuting distance of Boston, MA or be willing to relocate to the area. ', ' Excellent written and verbal ability, including the ability to be persuasive to a technical or non-technical audience. ', ' At least 3 years of experience in design and architecture review. ', ' Clean, standardize, and merge clinical trial data from disparate sources. ', ' Basic knowledge of Communication', ' Keeps current with applicable Standard Operating Procedures and associative training. ', ' An ability to work in a fast-paced environment where continuous innovation is occurring and ambiguity is the norm. ', 'Other Duties And Responsibilities', ' Basic knowledge of Interpersonal Relations', ' Experience with CDISC (ODM, SDTM, ADaM) a plus. ', 'Experience', ' At least 3 years of experience in software development life cycle. ', "" Bachelor's degree in a Computer Science or related field.  Candidate must be located within commuting distance of Boston, MA or be willing to relocate to the area. "", ' Comfort working in secure environments with restricted data. ', ' Triage many possible courses of action in a high-ambiguity environment, making use of both quantitative analysis and business judgment. ', 'Overview', ' Analyze raw clinical data in depth to identify common patterns, validate hypotheses, and integrate with external data sources. ', ' Basic knowledge of consultative/customer focus ', ' Build data models that convey complex information in digestible ways; present these insights to colleagues, management, and customers cleanly and confidently. ']",Entry level,Full-time,Research,Information Technology and Services,2021-03-18 14:34:51
Lead Data Engineer,General Motors,"Austin, TX",2 weeks ago,Be among the first 25 applicants,"['', 'Demonstrate mastery of many programming languages, tools and/or technologies with emphasis on ETL & Database development', 'Communicate and maintains Master Data, Metadata, Data Management Repositories, Logical Data Models, Data Standards', 'Preferred', 'Create and maintain optimal data pipeline architecture', 'You will identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.', 'Ability to tackle software engineering and data problems quickly and completely', 'Understand and maintain compliance with GM standards and industry standard methodology', 'Ability to identify tasks which require automation and automate them', 'In this role you will also lead junior developers, mentor, coach and help them to develop their software development skills in Data Engineering technologies.Demonstrate mastery of many programming languages, tools and/or technologies with emphasis on ETL & Database developmentUnderstand and maintain compliance with GM standards and industry standard methodology', ' Tuition assistance and student loan refinancing;', ' Paid time off including vacation days, holidays, and parental leave for mothers, fathers and adoptive parents;', 'As a Data Engineer, you will build industrialized data assets and optimize data pipelines in support of Vehicle Quality Business Intelligence and Advance Analytics objectives. You will work closely with our Quality Business teams, forward-thinking Data Scientists, BI Developers, System Architects and Data Architects to deliver value to our vision for the future.', 'Responsibilities', ' Company and matching contributions to 401K savings plan to help you save for retirement;', '7 or more years with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.', 'Communicate and maintains Master Data, Metadata, Data Management Repositories, Logical Data Models, Data StandardsCreate and maintain optimal data pipeline architectureYou will assemble large, complex data sets that meet functional / non-functional business requirementsYou will identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Build industrialized analytic datasets and delivery mechanisms that utilize the data pipeline to deliver actionable insights into vehicle quality, operational efficiency and other key business performance metricsWork with business partners on data-related technical issues and develop requirements to support their data infrastructure needsCreate highly consistent and accurate analytic datasets suitable for business intelligence and data scientist team members', ""At least 3 years of hands on experience with Big Data Tools: Hadoop, Spark, Kafka, etc.Master databases - Advanced SQL and NoSQL databases, including Postgres and Cassandra, Oracle and GreenplumData Wrangling and Preparation using GM's approved tools: PySpark, Jupyter etc.Stream-processing systems: Storm, Spark-Streaming, etc.Ability to tackle software engineering and data problems quickly and completelyAbility to identify tasks which require automation and automate themAbility to multi-task and stay organized in a dynamic work environment and work collaboratively with other IT organizations (GDAAS, Platform Engineering etc)"", 'For This Role You Will Be Responsible For', 'Requirements', ""Data Wrangling and Preparation using GM's approved tools: PySpark, Jupyter etc.Stream-processing systems: Storm, Spark-Streaming, etc."", 'Work with business partners on data-related technical issues and develop requirements to support their data infrastructure needs', 'About GM', 'In this role you will also lead junior developers, mentor, coach and help them to develop their software development skills in Data Engineering technologies.', 'developing data architecture and ETL solutions using sound, repeatable, industry standard methodologies. You will have the opportunity to work hands-on defining ETL solutions based on business requirements and system specifications.', ""Development activities will include enhancing existing data systems and optimizing ETL systems.Additionally you will lead development activities to migrate out of legacy technologies to GM's Big Data Platform utilizing new technologies such as, Kafka, Hadoop, PySpark, Greenplum and GM's internally developed Big Data tools."", 'Master databases - Advanced SQL and NoSQL databases, including Postgres and Cassandra, Oracle and Greenplum', 'Benefits Overview', 'In recent years, GM Information Technology has successfully executed the largest IT transformation in the history of the automotive industry, fully insourcing what once was a nearly completely outsourced IT function. Today GM IT is a dynamic and fast paced organization that designs, develops and maintains all IT infrastructure, applications and solutions enabling GM’s global operations. From designing and building the next generation of electric and other vehicles to developing a world-class GM experience for our dealers and customers, GM IT is driving real change in the most iconic automaker on the planet. Our team delivers unique enterprise-wide IT solutions in cutting-edge technologies such as mobility, telematics, mission-critical business systems, supercomputing, cloud, vehicle engineering and real-time computing. We offer challenging positions for passionate professionals looking to advance their careers and be a part of an IT organization focused on innovation, speed and business value.', 'We are not able to accommodate international relocation.', 'Build industrialized analytic datasets and delivery mechanisms that utilize the data pipeline to deliver actionable insights into vehicle quality, operational efficiency and other key business performance metrics', 'Create highly consistent and accurate analytic datasets suitable for business intelligence and data scientist team members', ' Global recognition program for peers and leaders to recognize and be recognized for results and behaviors that reflect our company values;', 'Ability to multi-task and stay organized in a dynamic work environment and work collaboratively with other IT organizations (GDAAS, Platform Engineering etc)', ' Healthcare (including a triple tax advantaged health savings account and wellness incentive), dental, vision and life insurance plans to cover you and your family;', 'Job Description', 'At least 3 years of hands on experience with Big Data Tools: Hadoop, Spark, Kafka, etc.', 'You will assemble large, complex data sets that meet functional / non-functional business requirements', ' Discount on GM vehicles for you, your family and friends.', ""developing data architecture and ETL solutions using sound, repeatable, industry standard methodologies. You will have the opportunity to work hands-on defining ETL solutions based on business requirements and system specifications.Development activities will include enhancing existing data systems and optimizing ETL systems.Additionally you will lead development activities to migrate out of legacy technologies to GM's Big Data Platform utilizing new technologies such as, Kafka, Hadoop, PySpark, Greenplum and GM's internally developed Big Data tools.""]",Not Applicable,Full-time,Information Technology,Automotive,2021-03-18 14:34:51
Data Engineer,ettain group,Raleigh-Durham-Chapel Hill Area,2 weeks ago,102 applicants,"['', '- BS in Math, Statistics, Data Science, Computer Science, Engineering, or Economics', '- Comfort manipulating and analyzing complex, high-volume, high dimensionality data from varying sources and Advanced SQL skills', '- Working knowledge of Python (not just programming) data manipulation and data wrangling techniques is a plus.', '- Self-driven individual, demonstrating continuous learning and creativity, and is naturally collaborative', 'W2 Only, C2C not eligible', '- 3+ years experience in a data engineering role', '- Experience with data warehousing and data integration tools, demonstrable proficiency in Python coding, and passion to build', '- A successful history of manipulating, processing, and extracting value from large disconnected datasets.', '- Good technical writing and verbal communication skills', '- Experience working with GCP', 'Job Title: Data Engineer', 'Our minimum requirements for this role:', '- Flexibility to adapt and interconnect different tools, including SnowFlake, Tableau Prep, Alteryx.', '- Experience collaborating cross-functionally to build and optimize big data sets, for both structured and unstructured data.', 'Job Description: Partner with sales leaders to enable data-driven go-to-market strategy and sales operations decisions. As a Data Engineer you will play an important role in developing and maintaining multiple data sets needed to prepare business insights, data science models, and data visualizations. You will wear multiple hats in this role, but much of your focus will be to use your superb Python and SQL skills to extract, integrate, and transform data from different systems into analytics-ready datasets. Beyond your technical prowess, you will need the soft-skills needed to effectively communicate and collaborate with analytics managers, business analysts, data scientists, and fellow data engineers on multiple projects.', '- Advanced working SQL knowledge and experience working with relational databases, query authoring as well as working familiarity with a variety of databases.', '- Experience solving analytical problems using quantitative approaches', '- Exposure to visualization tools such as Tableau and Excel']",Mid-Senior level,Contract,Information Technology,Staffing and Recruiting,2021-03-18 14:34:51
SQL Data Engineer,Sequoia Consulting Group,"Tempe, AZ",5 days ago,Be among the first 25 applicants,"['', 'Design and implement effective database solutions and models to store and retrieve company data ', 'Compensation & Benefits', 'Examine and identify database structural necessities by evaluating client operations, applications, and programming ', 'Required Skills & Experience', 'Knowledge of medical, dental, LTD/STD/life insurance concepts and data Understanding of various data extraction and transformation techniques Working familiarity with Salesforce (SFDC) Knowledge of Mulesoft is a bonus Familiar with data visualization standard methodologies\u202f ', 'Monitor the system performance by performing regular tests, fixing, and integrating new features ', 'Extensive hands-on experience working with SQL and Python for the purposes of data modeling and ETL ', 'Advanced working knowledge and ability to write complex SQL queries in MySQL, Snowflake, and Salesforce (SFDC) environments ', 'Soft Skills', 'Advanced working knowledge and ability to write complex SQL queries in MySQL, Snowflake, and Salesforce (SFDC) environments Extensive hands-on experience working with SQL and Python for the purposes of data modeling and ETL Strong familiarity with Kimball, OLAP, and EDW data design methodologies, especially for healthcare and benefits datasets 3+ years’ experience in data modeling and architecting, ETL, data engineering, or BI fields with concentration on data transformations and data modeling ', 'A champion of quality, able to QA and vouch for the integrity of the report output ', 'Design conceptual and logical data models and flowcharts ', 'What Does the Role Entail?', 'Lead all aspects of the migration of data from legacy systems to new solutions ', 'Install and organize information systems to guarantee company functionality ', 'Working familiarity with Salesforce (SFDC) ', 'Understanding of various data extraction and transformation techniques ', 'SQL', 'Knowledge of Mulesoft is a bonus ', '3+ years’ experience in data modeling and architecting, ETL, data engineering, or BI fields with concentration on data transformations and data modeling ', 'Assess fitness-for-purpose of existing data model and processes Design conceptual and logical data models and flowcharts Design and implement effective database solutions and models to store and retrieve company data Development of reporting solutions to meet the operational and executive needs of the platform Examine and identify database structural necessities by evaluating client operations, applications, and programming Optimize new and current database systems Assess database implementation procedures to ensure they follow internal and external regulations Install and organize information systems to guarantee company functionality Prepare accurate database design and architecture reports for management and executive teams Lead all aspects of the migration of data from legacy systems to new solutions Monitor the system performance by performing regular tests, fixing, and integrating new features Recommend solutions to improve new and existing database systems ', 'Assess fitness-for-purpose of existing data model and processes ', 'Maintaining business partner engagement and setting expectations ', 'Recommend solutions to improve new and existing database systems ', 'Translate business requirements into specifications that will be used to implement the required user-friendly environments, reports and dashboards, built from potentially multiple data sources ', 'Ability to succeed in a dynamic, Agile environment Strong prioritization and time-management skills Dedication to team goals that include support of live 24/7 production systems A consummate collaborator, able to establish good relationships with technical, product, and business owners A champion of quality, able to QA and vouch for the integrity of the report output Maintaining business partner engagement and setting expectations Assessing current processes and recommending changes as needed Documenting and communicating technical specifications to ensure that proper and optimized techniques, queries, data standards, and final outputs are understood and incorporated into data and analytics processes Participate in business analysis activities to gather required reporting and dashboard requirements Translate business requirements into specifications that will be used to implement the required user-friendly environments, reports and dashboards, built from potentially multiple data sources ', 'Development of reporting solutions to meet the operational and executive needs of the platform ', 'Knowledge of medical, dental, LTD/STD/life insurance concepts and data ', 'Prepare accurate database design and architecture reports for management and executive teams ', 'Useful Skills And Experience', 'Data Engineer ', 'A consummate collaborator, able to establish good relationships with technical, product, and business owners ', 'Participate in business analysis activities to gather required reporting and dashboard requirements ', 'Sequoia’s Culture – Our most important asset', 'Strong familiarity with Kimball, OLAP, and EDW data design methodologies, especially for healthcare and benefits datasets ', 'Assessing current processes and recommending changes as needed ', ""What You'll Do"", 'Familiar with data visualization standard methodologies\u202f ', 'Optimize new and current database systems ', 'Ability to succeed in a dynamic, Agile environment ', 'Assess database implementation procedures to ensure they follow internal and external regulations ', 'Strong prioritization and time-management skills ', 'Documenting and communicating technical specifications to ensure that proper and optimized techniques, queries, data standards, and final outputs are understood and incorporated into data and analytics processes ', 'Dedication to team goals that include support of live 24/7 production systems ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
"Data Engineer - SQL, Azure Cloud",recruitAbility,"Austin, TX",2 weeks ago,105 applicants,"['', 'Design and develop data pipelines to extract data from a wide variety of data sources using Azure, Snowflake Cloud, and cloud-native technologies.Build a data model to get actionable insights from data, operational efficiency, and other key business performance metrics.Design and manage inbound and outbound data processes and monitoring. Work with the data provider to bring in new feed into our data eco-system.Enjoy working in Agile as part of a scrum team and deliver high-quality products incrementally in an interactive manner.Write Test Driven Development based code to meet overall data quality standards as defined by the users.Automate the data testing processes and integrate them with monitoring systems.You are a team player who enjoys working with and supporting an Application Engineering team, DBA, Infrastructure, and Project Management Office.Analyze existing systems (including legacy) and data sets to help Business Analysts define the functional and non-functional requirements.Meet with the business users, assist with data-related technical issues, and support their data infrastructure needs', 'Preference for background in Financial Services, ideally in the Wealth Management/Independent Broker-Dealer/RIA industry', 'A solid compensation plan includes comprehensive benefits and a bonus plan', 'You are a team player who enjoys working with and supporting an Application Engineering team, DBA, Infrastructure, and Project Management Office.', 'Excellent salary and comprehensive benefits package for this full-time positionA world-class team of professionals, casual work environment, and rich cultureChallenging projects now and on the Technology Roadmap going out several yearsCareer path, training support, and opportunities for advancement withinAward-winning, a stable leader in their market space and still growingA solid compensation plan includes comprehensive benefits and a bonus planFull health, vision, dental. 401(k) plans along with a host of voluntary plans such as car insurance, legal services, and more.A brand new state of the art building in Southwest Austin with a basketball court, volleyball court, baseball field, walking trails, unlimited coffee, tea, and sparkling water', 'Our client, headquartered in the “Silicon Hills” of Austin, Texas, offers an experience as unique as the city in which it operates. The firm supports more than 1,700 independent financial advisors in delivering comprehensive securities and investment advisory services to their clients. With a culture rich in reinvention and advisor advocacy, they have developed integrated business management technology that, combined with its personalized consulting services, offers exceptional scale and efficiency', 'Build a data model to get actionable insights from data, operational efficiency, and other key business performance metrics.', 'A world-class team of professionals, casual work environment, and rich culture', 'Meet with the business users, assist with data-related technical issues, and support their data infrastructure needs', 'Working knowledge of Business Intelligence tools: Microsoft Integration Services, Reporting Services, and Analysis Services, as well as PowerBI.', 'Strong experience with NoSQL database, including PostgresBackground in working with Azure Cloud Services: Data Factory, SQL database, Functions, Data Lake, Databricks, Logic Apps, and Azure Automation.Fluent in object-oriented and functional script language: Python, Scala, and C#.Advanced working knowledge of SQL Server database - writing advanced SQL script, profiling, and optimization.Working knowledge of Business Intelligence tools: Microsoft Integration Services, Reporting Services, and Analysis Services, as well as PowerBI.Experience with other Big Data tools such as Spark, Snowflake, and KafkaPreference for background in Financial Services, ideally in the Wealth Management/Independent Broker-Dealer/RIA industryBachelors/Masters in Computer Science, MIS/Information Management, Engineering or related field', 'Enjoy working in Agile as part of a scrum team and deliver high-quality products incrementally in an interactive manner.', '\ufeffWhat’s in it for you as a Data Engineer with a growing company?', 'Design and manage inbound and outbound data processes and monitoring. Work with the data provider to bring in new feed into our data eco-system.', 'Primary Requirements for the Data Engineer', 'Design and develop data pipelines to extract data from a wide variety of data sources using Azure, Snowflake Cloud, and cloud-native technologies.', 'Full health, vision, dental. 401(k) plans along with a host of voluntary plans such as car insurance, legal services, and more.', 'Strong experience with NoSQL database, including Postgres', 'Award-winning, a stable leader in their market space and still growing', 'Analyze existing systems (including legacy) and data sets to help Business Analysts define the functional and non-functional requirements.', 'Automate the data testing processes and integrate them with monitoring systems.', 'Excellent salary and comprehensive benefits package for this full-time position', 'Bachelors/Masters in Computer Science, MIS/Information Management, Engineering or related field', 'Background in working with Azure Cloud Services: Data Factory, SQL database, Functions, Data Lake, Databricks, Logic Apps, and Azure Automation.', 'Career path, training support, and opportunities for advancement within', 'Challenging projects now and on the Technology Roadmap going out several years', 'Experience with other Big Data tools such as Spark, Snowflake, and Kafka', 'Advanced working knowledge of SQL Server database - writing advanced SQL script, profiling, and optimization.', 'Primary Responsibilities of the Data Engineer', 'A brand new state of the art building in Southwest Austin with a basketball court, volleyball court, baseball field, walking trails, unlimited coffee, tea, and sparkling water', 'Fluent in object-oriented and functional script language: Python, Scala, and C#.', 'They are looking for a passionate Data Engineer to join our growing Data and Analytics team. Join this exciting journey in modernizing their legacy solutions to the next-generation cloud platform. You will be responsible for working in a cross-functional team to expand, optimize, and improve overall data quality and set up next-generation data orchestration using modern cloud tools and technologies. As a Cloud Data Engineer, you will be designing and building secure and resilient architectures, with the goal of providing actionable insights to our Advisors for them to optimize their business.', 'Write Test Driven Development based code to meet overall data quality standards as defined by the users.']",Mid-Senior level,Full-time,Engineering,Financial Services,2021-03-18 14:34:51
Software/Data Engineer,TargetCW,San Francisco Bay Area,,N/A,"['', ' Experience working with Amazon Webservices, S3, EMR, Redshift etc.', 'San Francisco, CA (remote for now)', ' Be judicious about introducing dependencies.', ' Partner with Analytics, Product and Engineering teams to understand data needs.', ' Keep existing data sources fresh against data quality issues, design, develop and maintain data quality assurance framework and improve the processes for developing new ones raising the level of quality expected from our work. Conduct unit, integration, and system tests on our data sources to validate data against source systems, and optimize performance to improve query speed and reduce cost. Design, build and maintain a set of trusted data assets for a product or a group of products. Act as our team’s thought leader for defining data telemetry, storage and ETL processes. Partner with Analytics, Product and Engineering teams to understand data needs. Write software code and data solutions that are high quality and comprehensible. Have rigor around data architecture best practices: Balance customer requirements with technology requirements. Be proficient in a broad range of data design approaches. Be judicious about introducing dependencies. Create flexible data solutions without over-engineering. Understand how to be efficient with resource usage (e.g., system hardware, data storage, query optimization, AWS infrastructure etc.) Have knowledge of engineering and operational excellence best practices. Be able to make enhancements that improve data processes (e.g., data auditing solutions, management of manually maintained tables, automating, ad-hoc or manual operation steps).', ' 3+ years in relational database concepts with a solid knowledge of star schema, SQL, SQL Tuning, OLAP, Big Data technologies', ' Have knowledge of engineering and operational excellence best practices. Be able to make enhancements that improve data processes (e.g., data auditing solutions, management of manually maintained tables, automating, ad-hoc or manual operation steps).', ' Comfort working independently, prioritizing projects, and managing stakeholder expectations across teams.', ' Strong written and verbal communication skills.', ' Obsessed with data quality and a strong belief in test driven development', ' Conduct unit, integration, and system tests on our data sources to validate data against source systems, and optimize performance to improve query speed and reduce cost.', 'Our Ad Prod team is looking to hire an experienced data engineer. This position is focused on empowering staff throughout our organization to use and trust our advertisement data for financial reports. Your responsibilities may range from maintaining and enhancing our data warehouse which acts as sources of truth across the company, driving data quality across product departments and teams, building self-service business intelligence infrastructure for reporting,\xa0and connecting into data interfaces that allow finance and other functions to discover and analyze the data. In the process, you will work with technical and non-technical staff members throughout the company.', ' Keep existing data sources fresh against data quality issues, design, develop and maintain data quality assurance framework and improve the processes for developing new ones raising the level of quality expected from our work.', '18 Month Duration', ' Experience building aggregates, optimizing data workstreams and maintaining data pipelines', ' 5+ years of experience in data engineering, software engineering, or other related roles. Preferably in the consumer internet or gaming space, or working with a high-velocity, high-growth product / business. 3+ years in relational database concepts with a solid knowledge of star schema, SQL, SQL Tuning, OLAP, Big Data technologies 3+ years of experience maintaining data pipelines from multiple data sources, in collaboration with diverse partners. Experience with best practices for development including query optimization, version control, code reviews, and documentation. Proficient in SQL -- comfortable working with complex joins, window functions and writing SQL for aggregations. Experience working with Amazon Webservices, S3, EMR, Redshift etc. Experience in coding languages like Python/Java/Scala Experience building aggregates, optimizing data workstreams and maintaining data pipelines Comfort working independently, prioritizing projects, and managing stakeholder expectations across teams. Strong written and verbal communication skills. Obsessed with data quality and a strong belief in test driven development Experience in SAP integration is a big plus', '$150-200/hr DOE', ' Proficient in SQL -- comfortable working with complex joins, window functions and writing SQL for aggregations.', ' Understand how to be efficient with resource usage (e.g., system hardware, data storage, query optimization, AWS infrastructure etc.)', 'Full Time', ' Balance customer requirements with technology requirements.', 'You Will:', '*W2 ONLY, NO VISA SPONSORSHIP', 'Software/Data Engineer (W2 ONLY)', ' Experience with best practices for development including query optimization, version control, code reviews, and documentation.', ' Write software code and data solutions that are high quality and comprehensible.', ' Design, build and maintain a set of trusted data assets for a product or a group of products.', ' Act as our team’s thought leader for defining data telemetry, storage and ETL processes.', ' Have rigor around data architecture best practices:', ""We are the\xa0world's leading live streaming platform for gamers and the things we love. We make it possible to watch, play and chat with millions of other fans from around the world. We are looking for a Software/Data Engineer to join our team ASAP! "", ' 3+ years of experience maintaining data pipelines from multiple data sources, in collaboration with diverse partners.', ' Experience in coding languages like Python/Java/Scala', ' Be proficient in a broad range of data design approaches.', ' Experience in SAP integration is a big plus', '\xa0', 'You Have:', ' Create flexible data solutions without over-engineering.', 'PLEASE SUBMIT YOUR RESUME TO BE CONSIDERED!', ' 5+ years of experience in data engineering, software engineering, or other related roles. Preferably in the consumer internet or gaming space, or working with a high-velocity, high-growth product / business.']",Mid-Senior level,Full-time,Engineering,Computer Games,2021-03-18 14:34:51
Data Engineer,RemoteHub,"California, United States",6 days ago,Be among the first 25 applicants,"['', ' Data Studio Job', ' Data Quality management. Test Cases etc.', ' Apply for this Data Engineer position', ' Experience designing and building scalable and robust data pipelines to enable data-driven decisions for the business.', ' Develop, implement and tune ETL processes.', ' Gathering technical requirement from customer and enable the right team to develop and implement it. Experience designing and building scalable and robust data pipelines to enable data-driven decisions for the business. Understand business processes, logical data models and relational database implementations for data analysis. Build data expertise and implement own data quality test cases for required areas. Expert knowledge of SQL and of relational database systems and concepts. Expert knowledge in Azure Data Factory, Databricks Script, Scope Scripts, Cosmos etc. Demonstrated strength in data modeling, ETL development, and data warehousing Develop, implement and tune ETL processes. Experience analyzing data to discover opportunities and address gaps. Develop and maintain data pipelines including solutions for data collection, management, and usage. Develop and implement solutions for data quality validation and continuous improvement. Drive our data platform and help evolve our technology stack and development best practices Develop and unit test assigned features to meet product requirements. Working knowledge of data quality approaches and techniques. Programming language experience (C#, Python, Scala Spark.) is a plus. Build visualizations in PowerBI to help derive meaningful insights from data. Maintain and enhance our data and computation platform up and running. Accountable for ensuring that project tasks are accomplished to schedule at acceptable level of quality; or that appropriate contingencies are implemented. Ensure documentation of all project artefacts are accurate and current. Ensure all projects are delivered successfully and according to Engagement statement of work process, methodology, and quality standards.', ' Programming language experience (C#, Python, Scala Spark.) is a plus.', ' Accountable for ensuring that project tasks are accomplished to schedule at acceptable level of quality; or that appropriate contingencies are implemented.', ' Drive our data platform and help evolve our technology stack and development best practices', ' Databricks', ' Expert knowledge of SQL and of relational database systems and concepts.', ' Understand business processes, logical data models and relational database implementations for data analysis.', ' Scope Scripts, Cosmos', ' PowerBI', ' SQL Azure, Synapse (SQL Datawarehouse)', ' Working knowledge of data quality approaches and techniques.', ' Build data expertise and implement own data quality test cases for required areas.', ' Demonstrated strength in data modeling, ETL development, and data warehousing', ' Gathering technical requirement from customer and enable the right team to develop and implement it.', ' Develop and maintain data pipelines including solutions for data collection, management, and usage.', ' Kusto', ' Azure Data Factory', ' Experience analyzing data to discover opportunities and address gaps.', ' Expert knowledge in Azure Data Factory, Databricks Script, Scope Scripts, Cosmos etc.', ' Azure Data Lake, Blob Storage', ' Build visualizations in PowerBI to help derive meaningful insights from data.', ' Develop and implement solutions for data quality validation and continuous improvement.', ' Maintain and enhance our data and computation platform up and running.', 'Solid Familiarity On The Following Tools', ' Develop and unit test assigned features to meet product requirements.', ' SQL Azure, Synapse (SQL Datawarehouse) Azure Data Factory Kusto Scope Scripts, Cosmos PowerBI Azure Data Lake, Blob Storage Databricks Data Quality management. Test Cases etc. Data Studio Job', ' Ensure documentation of all project artefacts are accurate and current.', ' Ensure all projects are delivered successfully and according to Engagement statement of work process, methodology, and quality standards.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Remote Data Engineer Lead ,Vaco,"Phoenix, AZ",7 days ago,50 applicants,"['', ' ', 'Experience with delta lake is a plus', 'Design implement and champion the tooling required for data ingestion, transformation and orchestration.', 'Experience working in Azure environment', ""Work with Data Analysts, Business SME's in data modelling efforts"", 'Experience with SQL Server is a plus', ""Design implement and champion the tooling required for data ingestion, transformation and orchestration.Collaborate with Applications and Security TeamsWork with Data Analysts, Business SME's in data modelling effortsWork with Security/Infrastructure Architects and Data Engineers to implement security policiesCollaborate with Application teams to ingest change data streamsPerform code review and debug in case of process failures or data discrepanciesMonitor application performance and operations"", 'Hands on experience building big data pipelines using Python, Apache Spark, Apache Airflow', 'Monitor application performance and operations', 'Strong communication and analytical and problem-solving skills', '6+ years of experience in Data Engineering, Data Warehousing', '6+ years of experience in Data Engineering, Data WarehousingHands on experience building big data pipelines using Python, Apache Spark, Apache AirflowCommand in big data tech stack - Apache Kafka, HadoopExperience working in Azure environmentExperience with Azure DevOps is requiredExperience with delta lake is a plusExperience with SQL Server is a plusStrong communication and analytical and problem-solving skills', 'Collaborate with Applications and Security Teams', 'We are seeking a Senior Data Engineer to help build data infrastructure and data pipelines that power business data. Ideal candidate is expected to operate within a distributed, agile, cross-functional environment. This is an opportunity to create an enterprise-wide impact by providing normalized data to all stakeholders and downstream systems. Candidate is responsible for the ETL/ELT processes, following architecture guidelines, reliability, accuracy, monitoring, and infrastructure surrounding internal data processing.', 'Work with Security/Infrastructure Architects and Data Engineers to implement security policies', 'Perform code review and debug in case of process failures or data discrepancies', 'Experience with Azure DevOps is required', 'Collaborate with Application teams to ingest change data streams', 'Command in big data tech stack - Apache Kafka, Hadoop']",Associate,Contract,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer I,Thermo Fisher Scientific,"Alachua, FL",4 weeks ago,Be among the first 25 applicants,"['', 'Proficiency in using query languages such as SQL', 'Good scripting and programming skills', 'Strong communication skills and ability to work effectively across a matrix organization', '2-4 years of data engineering experience', 'Ability to interact professionally with a diverse group including VPs, directors, managers, subject matter experts and end-users.', 'Direct interaction with the business to understand and analyze business problems, derive insights and recommend solutions', 'Must be a business partner, not merely a technical expert – this position plays an active role providing actionable insight into the Clinical Trials Division', 'Extending company’s data with third party sources of information when needed', '2-4 years of data engineering experienceData-oriented personalityGreat communication skillsExperience with data visualization tools, such as PowerBI, Tableau or CognosProficiency in using query languages such as SQLExperience with NoSQL databasesGood applied statistics skills, such as distributions, statistical testing, regression, etc.Experience with common data science toolkitsGood scripting and programming skillsAbility to partner with management at all levels and to lead major projects and initiativesStrong communication skills and ability to work effectively across a matrix organizationBA/BS degree in finance, mathematics, computer science preferred or equivalent work experience ', 'Strong analytical skills – and ability to use those skills to influence and drive change', 'Minimum Requirements/Qualifications', 'Position Summary', 'Direct interaction with the business to understand and analyze business problems, derive insights and recommend solutionsWorking independently, or with functional analysts, to create reports, data sets and mechanisms to provide visibility to business dataExecuting ad-hoc analysis and presenting results in a clear mannerExtending company’s data with third party sources of information when neededCollaborate with IT and business partners to ensure data quality, integrity, and accuracy across the global CTD data storesEstablish good working relationships with peers in other divisions and explore joint system and process improvement opportunities.Maintain MS SQL Server and Oracle database environments (security, tables, views, packages, SQL Agent jobs, SSAS database, Integration Services)Develop/Maintain and Administer PowerBI dashboards, gateways and associated infrastructure', 'Good applied statistics skills, such as distributions, statistical testing, regression, etc.', 'Global experience', 'Develop/Maintain and Administer PowerBI dashboards, gateways and associated infrastructure', 'Executing ad-hoc analysis and presenting results in a clear manner', 'BA/BS degree in finance, mathematics, computer science preferred or equivalent work experience ', 'Collaborate with IT and business partners to ensure data quality, integrity, and accuracy across the global CTD data stores', 'Experience with data visualization tools, such as PowerBI, Tableau or Cognos', 'Maintain MS SQL Server and Oracle database environments (security, tables, views, packages, SQL Agent jobs, SSAS database, Integration Services)', 'Experience with common data science toolkits', '10% travel requirement', 'Key Success Factors', 'Great communication skills', 'Data-oriented personality', 'Strong organizational and communication skills, and proven ability to adapt style to different situations and peopleMust be a business partner, not merely a technical expert – this position plays an active role providing actionable insight into the Clinical Trials DivisionStrong analytical skills – and ability to use those skills to influence and drive changeExcellent interpersonal and communication skills (both verbal and written).Ability to interact professionally with a diverse group including VPs, directors, managers, subject matter experts and end-users.Self-motivated; bias for actionGlobal experience10% travel requirement', 'Key Responsibilities', 'Establish good working relationships with peers in other divisions and explore joint system and process improvement opportunities.', 'Strong organizational and communication skills, and proven ability to adapt style to different situations and people', 'Experience with NoSQL databases', 'Self-motivated; bias for action', 'Ability to partner with management at all levels and to lead major projects and initiatives', 'Excellent interpersonal and communication skills (both verbal and written).', 'Working independently, or with functional analysts, to create reports, data sets and mechanisms to provide visibility to business data']",Not Applicable,Full-time,Other,Pharmaceuticals,2021-03-18 14:34:51
Data Engineer - Newport Beach,Obsidian Security,"Newport Beach, CA",2 days ago,Be among the first 25 applicants,"['', 'You’re an engineer who is experienced designing, implementing and supporting new data processing, data sets and systems to support various advanced analytics needs from multiple sources.You enjoy solving large-scale problems and are comfortable doing incremental quality work while building brand new systems to enable future quality improvements', 'Cybersecurity experience is a plus', 'Support and build well-engineered data systems to support analytical needs for cloud based systems (AWS) and deliver data via APIs', 'You enjoy solving large-scale problems and are comfortable doing incremental quality work while building brand new systems to enable future quality improvements', 'Required Skills/experience', 'Familiarity with DevOps and AWS (S3, EMR, EC2 are a must)', 'BS/MS Computer Science or a highly quantitative discipline (MS Degree preferred)Ability with the following programming languages: Scala and PythonExtensive experience with the following software packages: SQL, NoSQL, Apache products (Spark, Hive, Airflow, Hadoop, etc.) and graph databasesFamiliarity with DevOps and AWS (S3, EMR, EC2 are a must)Successfully delivered and maintained a major cloud service with a large number of end users.Cybersecurity experience is a plus', 'Collaborate with the existing engineering team, data scientists, analysts and the business to understand requirements and data needs and definitions.', 'Who You Are', 'Participate in the engineering life-cycle at Obsidian, including designing distributed systems, writing production code, conducting code reviews and working alongside our infrastructure and reliability teams.Work in a big data ecosystem to develop and maintain an analytics pipeline for acquisition, storage, and processing data types of interest to feed real-time artificial intelligent system behaviors.Develop or implement tools to support analyst-driven machine learning analyses.Support and build well-engineered data systems to support analytical needs for cloud based systems (AWS) and deliver data via APIsCollaborate with the existing engineering team, data scientists, analysts and the business to understand requirements and data needs and definitions.', 'Participate in the engineering life-cycle at Obsidian, including designing distributed systems, writing production code, conducting code reviews and working alongside our infrastructure and reliability teams.', 'BS/MS Computer Science or a highly quantitative discipline (MS Degree preferred)', 'Ability with the following programming languages: Scala and Python', 'Develop or implement tools to support analyst-driven machine learning analyses.', 'Work in a big data ecosystem to develop and maintain an analytics pipeline for acquisition, storage, and processing data types of interest to feed real-time artificial intelligent system behaviors.', 'Successfully delivered and maintained a major cloud service with a large number of end users.', ""What You'll Do"", 'Extensive experience with the following software packages: SQL, NoSQL, Apache products (Spark, Hive, Airflow, Hadoop, etc.) and graph databases', 'You’re an engineer who is experienced designing, implementing and supporting new data processing, data sets and systems to support various advanced analytics needs from multiple sources.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Science Engineer,Stripe,"Seattle, WA",4 weeks ago,97 applicants,"['', 'Our stack spans tools in Scala, Python, R, Javascript, React, SQL ', 'Work on our centralized experimentation platform to pipeline experiment metrics and compute descriptive statistics', 'You Should Include These In Your Application', 'LinkedIn profile', 'We’re Looking For Someone Who Has', 'Help the Data Science team apply and generalize statistical and econometric models on large datasets', 'Resume', 'An inquisitive nature in diving into data inconsistencies to pinpoint issues', 'Identify data needs for business and product teams, understand their specific requirements for metrics and analysis, and build efficient and scalable data pipelines to enable data-driven decisions across Stripe', 'Prior experience with writing and debugging data pipelines using a distributed data framework (Hadoop/Spark/Pig etc…)', 'Improve our data visualization tooling and platform at Stripe to help the team create dynamic tools and reporting', '3+ Years of experience in a Data Engineering or Data Science role, with a focus on building data pipelines or conducting data intensive analysis. ', ' 3+ Years of experience in a Data Engineering or Data Science role, with a focus on building data pipelines or conducting data intensive analysis.  A strong engineering background and are interested in data Prior experience with writing and debugging data pipelines using a distributed data framework (Hadoop/Spark/Pig etc…) An inquisitive nature in diving into data inconsistencies to pinpoint issues Knowledge of a scientific computing language (such as R or Python) and SQL The ability to communicate cross-functionally, derive requirements and architect shared datasets ', 'Build data pipelines that track our marketing funnel from visits to onboarding to active usage of Stripe', 'A strong engineering background and are interested in data', 'The ability to communicate cross-functionally, derive requirements and architect shared datasets', ' Identify data needs for business and product teams, understand their specific requirements for metrics and analysis, and build efficient and scalable data pipelines to enable data-driven decisions across Stripe Design, develop, and own data pipelines and models that power internal analytics for product and business teams  Help the Data Science team apply and generalize statistical and econometric models on large datasets Drive the collection of new data and the refinement of existing data sources, develop relationships with production engineering teams to manage our data structures as the Stripe product evolves  Develop strong subject matter expertise and manage the SLAs for those data pipelines  ', 'Knowledge of a scientific computing language (such as R or Python) and SQL', 'You Will', 'Develop unified user data schemas and tables that provide a complete view of the business across our various products such as Stripe Connect, Atlas, or Sigma', 'Drive the collection of new data and the refinement of existing data sources, develop relationships with production engineering teams to manage our data structures as the Stripe product evolves ', 'Design, develop, and own data pipelines and models that power internal analytics for product and business teams ', 'Some Things You Might Work On', 'Develop strong subject matter expertise and manage the SLAs for those data pipelines ', ' Resume LinkedIn profile', ' Develop unified user data schemas and tables that provide a complete view of the business across our various products such as Stripe Connect, Atlas, or Sigma Build data pipelines that track our marketing funnel from visits to onboarding to active usage of Stripe Work on our centralized experimentation platform to pipeline experiment metrics and compute descriptive statistics Improve our data visualization tooling and platform at Stripe to help the team create dynamic tools and reporting Our stack spans tools in Scala, Python, R, Javascript, React, SQL  ']",Mid-Senior level,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Engineer,Modis,Detroit Metropolitan Area,3 weeks ago,169 applicants,"['', '2 + years. experience with either SQL, Oracle or DB2.\xa0 Expereince working with Java 1 + years. experience working with Sqoop, Hive, and Hbase etc.\xa0 Experience with data replication such as ETL or within Hadoop using Kafka, Scoop, etc.    3+ years experience designing, developing, and      implementing data pipelines and applications to stream and process      datasets at low latencies. 3+ years experience developing distributed systems and      data architecture.\xa0\xa0 Agile engineering practices ', ' ', 'Modis is looking for a Data Engineers for multiple roles in Metro Detroit. This will be working in a fast paced global enterprise environment, and will require the developer to be onsite. This position is direct W2, and a long term opportunity. Cannot work with 3rd party employers, but we can provide sponsorship.\xa0  ', 'Replicate data from hundreds of database sources within the company to the DSC Hadoop environment. ', 'Modis offers Visa Sponsorship and Green Card services Excellent compensation packages including Subsidized Medical Benefits, Paid Time Off and 401k No Third-Party inquiries', '3+ years experience designing, developing, and      implementing data pipelines and applications to stream and process      datasets at low latencies. ', 'Transform data to make it usable for data scientists. ', 'ADDITIONAL INFORMATION', 'POSITION DESCRIPTION', '3+ years experience developing distributed systems and      data architecture.\xa0\xa0 ', 'Expereince working with Java ', 'TECHNICAL SKILLS DESIRED', 'Replicate data from hundreds of database sources within the company to the DSC Hadoop environment. Work with Java in the Hadoop environment.  Transform data to make it usable for data scientists. Transform complex analytical models into      scalable, production-ready solutions. Continuously integrate and ship code into our      cloud Production environments. Work directly with Product Owners and      Developers to deliver data products in a collaborative and agile      environment. \xa0 Bring a passion to stay on top of tech trends,      experiment with and learn new technologies, participate in internal & external      technology communities, and mentor other members of the engineering      community. ', 'Experience with data replication such as ETL or within Hadoop using Kafka, Scoop, etc.    ', 'Bring a passion to stay on top of tech trends,      experiment with and learn new technologies, participate in internal & external      technology communities, and mentor other members of the engineering      community. ', 'Modis offers Visa Sponsorship and Green Card services ', '2 + years. experience with either SQL, Oracle or DB2.\xa0 ', 'POSITION DESCRIPTION\xa0\xa0 ', 'Excellent compensation packages including Subsidized Medical Benefits, Paid Time Off and 401k ', 'Work directly with Product Owners and      Developers to deliver data products in a collaborative and agile      environment. \xa0 ', 'Transform complex analytical models into      scalable, production-ready solutions. ', 'Agile engineering practices ', 'No Third-Party inquiries', '1 + years. experience working with Sqoop, Hive, and Hbase etc.\xa0 ', 'ADDITIONAL INFORMATION ', 'Work with Java in the Hadoop environment.  ', 'TECHNICAL SKILLS DESIRED ', 'Continuously integrate and ship code into our      cloud Production environments. ']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,E-Solutions,"Hartford, CT",2 days ago,Be among the first 25 applicants,"['', 'Advanced SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.', 'Translating data and technology requirements into our ETL / ELT architecture.Develop real-time and batch data ingestion and stream-analytic solutions leveraging technologies such as Kafka, Apache Spark, Java, NoSQL DBs, AWS EMR.Develop data driven solutions utilizing current and next generation technologies to meet evolving business needs.Develop custom cloud-based data pipeline.Provide support for deployed data applications and analytical models by identifying data problems and guiding issue resolution with partner data engineers and source data providers.Provide subject matter expertise in the analysis, preparation of specifications and plans for the development of data processes.', 'Experience with data modeling, data architecture design and leveraging large-scale data ingest from complex data sources', 'These projects will include designing and developing data ingestion and processing/transformation frameworks leveraging open source tools such as Python, Spark, pySpark, etc.', 'The senior data engineer will partner with the Data Analytics team to understand their data needs and build data pipelines using cutting edge technologies.They will perform hands-on development to create, enhance and maintain data solutions enabling seamless integration and flow of data across our data ecosystem.These projects will include designing and developing data ingestion and processing/transformation frameworks leveraging open source tools such as Python, Spark, pySpark, etc.', 'Translating data and technology requirements into our ETL / ELT architecture.', 'Strong knowledge of data pipelining software e.g., Talend, Informatica', 'The senior data engineer will partner with the Data Analytics team to understand their data needs and build data pipelines using cutting edge technologies.', 'They will perform hands-on development to create, enhance and maintain data solutions enabling seamless integration and flow of data across our data ecosystem.', 'Responsibilities:', 'Develop data driven solutions utilizing current and next generation technologies to meet evolving business needs.', 'Qualifications:', 'Provide support for deployed data applications and analytical models by identifying data problems and guiding issue resolution with partner data engineers and source data providers.', 'Develop custom cloud-based data pipeline.', ""Experience building and optimizing 'big data' data pipelines, architectures and data sets."", 'Data Engineer', 'Strong knowledge of analysis tools such as Python, R, Spark or SAS, Shell scripting, R/Spark on Hadoop or Cassandra preferred.', 'Disclaimer:\xa0E-Solutions provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, disability, genetic information, marital status, amnesty, or status as a covered veteran in accordance with applicable federal, state and local laws. We especially invite women, minorities, veterans, and individuals with disabilities to apply. EEO/AA/M/F/Vet/Disability.', ""Strong experience in data ingestion, gathering, wrangling and cleansing tools such as Apache NiFI, Kylo, Scripting, Power BI, Tableau and/or QlikExperience with data modeling, data architecture design and leveraging large-scale data ingest from complex data sourcesExperience building and optimizing 'big data' data pipelines, architectures and data sets.Advanced SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.Strong knowledge of analysis tools such as Python, R, Spark or SAS, Shell scripting, R/Spark on Hadoop or Cassandra preferred.Strong knowledge of data pipelining software e.g., Talend, Informatica"", 'Provide subject matter expertise in the analysis, preparation of specifications and plans for the development of data processes.', 'Develop real-time and batch data ingestion and stream-analytic solutions leveraging technologies such as Kafka, Apache Spark, Java, NoSQL DBs, AWS EMR.', 'Strong experience in data ingestion, gathering, wrangling and cleansing tools such as Apache NiFI, Kylo, Scripting, Power BI, Tableau and/or Qlik']",Entry level,Contract,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Cloud Data Engineer,Info Services,"Bristol, CT",2 days ago,58 applicants,"['', '\uf0b7 Have a technology toolbox – Hands on experience with newer technologies relevant', '\uf0b7 Creative thinking and motivated self-starter', 'other groups jealous', '\uf0b7 This candidate will work on building tools and a framework for data', 'stream processing specialists, API developers, our DevOps team, and', 'Required: ', 'services.', '\uf0b7 Experience working in AWS cloud environment.', 'landed data, and landed data with conformed data; Tools such as Springboot, Java, Kafka', '\uf0b7 Excellent communication skills and ability to interact with all levels of end users and', 'Location: Remote', '\uf0b7 Build software across our entire cutting-edge analytics platform, including', 'Preferred: ', 'Job Title :\xa0Cloud Data Engineer', '\uf0b7 Proficiency with agile development methodologies shipping features every two', 'Arun Reddy', 'experience with developing in a cloud native environment with many different', 'Summarize job responsibilities, core deliverables and major duties. What is required for the position to exist?\xa0\xa0', 'Info Services, LLC. is a registered “E-Verify” employer', 'technical resources', 'and Yarn/Kubernetes.', '--', 'Thanks & Regards,', 'technologies.', 'robust data processing, and caching technologies.', '“E-Verify” employer', 'data processing, storage, and APIs, with awesome cutting-edge', 'and snowflake will be used.', '\uf0b7 Experience with real-time and scalable systems are preferred.', 'operationalizing clusters in cloud environment.', '\uf0b7 Data and API ninja –You are also very handy with Streaming technologies such as', 'such as Kafka, KafkaStreams, Apache Flink and Java/Scala/Python will be used.', '\uf0b7 The data acquisition involves defining and creating schema registry of the source system', '\uf0b7 Expert knowledge of data systems and SQL', 'Job Summary:', 'reactive programming and dependency injection such as Spring to develop REST', '\uf0b7 Ensure performance isn’t our weakness by implementing and refining', '\uf0b7 Have 7+ years of experience developing with a mix of languages (Java, Scala, Python,', 'technologies. We operate in real-time with high-availability.', 'SQL, etc.) and frameworks to implement data acquisition, processing, and serving', 'reliable and then work across our team to put your ideas into action.', 'NOTE: Only on our W2 ', 'Cloud Data Engineer', '\uf0b7 Help us stay ahead of the curve by working closely with data architects,', 'Senior Recruiter', 'A ""Certified Minority Business Enterprise (MBE)""', '\uf0b7 Think of new ways to help make our platform more scalable, resilient and', 'Apache Kafka, Apache Flink; and framework such as Apache Spark. Understand', 'source contributions you are proud to share.', 'Hope you are doing well.', 'analysts to design systems which can scale elastically in ways which make', '\uf0b7 Experienced with Excel and data manipulation', '\uf0b7 The data validation involves creating a framework that validates the source data with', '\uf0b7 Prior experience building scalable platforms – handling large scale data,', 'Info Services LLC', 'Email : arunb@infoservicesllc.com', 'acquisition,\xa0conformance, and validation on the cloud.', '-Focus on major areas of work, typically 20% or more of role % of Time', 'Website : www.infoservicesllc.com', '\uf0b7 The data conformance involves creating a framework that use stream processing to', 'Job Description for your reference:', 'to the data space such as Spark, Kafka, and Snowflake. You’ll have plenty of', 'One of our Client is looking for Cloud Data Engineer. Below is the JD, please have a look and submit the resumes ASAP.', 'Basic Responsibilities:', 'consume the landed data, apply transformation and sink the data to snowflake; Tools', 'data using python or Java. Then automate the schema generation process and maintain.', 'weeks. It would be awesome if you have a robust portfolio on GitHub and / or open', '\uf0b7 Experience with open source such as Spring, Kafka, Flink, Spark, Snowflake, Cassandra', 'Hi All,', 'Describe what the person will do in the role - how he/she will impact the organization.']",Mid-Senior level,Contract,Engineering,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,"Docker, Inc","New York, NY",3 weeks ago,96 applicants,"['', ' Implement, document, oversee and evolve the Snowflake and ETL infrastructure Maintain the integrity of data within our data pipeline and warehouse Ensure quality of data and completeness of event logging across Docker codebase Integrate data from 3rd party services via ETL tools and custom pipelines Develop ETL jobs and tests to process, validate, transport, collate, aggregate, and distribute data Transform raw event logs into higher-order tables to make existing analysis easier and new analysis possible Creating automated reporting of weekly and monthly metrics and ROI for the executive management team and board Integrate emerging methodology, technology, and version control practices that best fit the team.  Design, build and automate business metrics into self-serve dashboards via Looker  Collaborate with product managers and analysts throughout the company to deliver reliable data that powers actionable insights Champion a data-informed mindset within our culture ', 'Champion a data-informed mindset within our culture', 'Strong verbal and written communication skills', 'At least 6 months of experience with Looker and LookML ', 'Familiarity with data warehousing concepts including data model design and query optimization strategies', 'Creating production-ready ETL scripts with Python and SQL', 'Experience using data collection platforms such as Segment, RudderStack, Fivetran etc. ', 'Responsibilities', 'Data Engineer (Remote)', 'Implement, document, oversee and evolve the Snowflake and ETL infrastructure', 'Develop ETL jobs and tests to process, validate, transport, collate, aggregate, and distribute data', 'Experience using data analysis and/or statistics to inform decisions ', 'Experience designing and deploying high-performance systems with reliable monitoring and logging practices', 'Maintain the integrity of data within our data pipeline and warehouse', 'Familiarity with at least one cloud ecosystem: AWS/Azure Infrastructure/Google Cloud', 'Qualifications', 'Integrate data from 3rd party services via ETL tools and custom pipelines', 'Transform raw event logs into higher-order tables to make existing analysis easier and new analysis possible', 'Design, build and automate business metrics into self-serve dashboards via Looker ', ' BS/MS in Computer Science, Math, Physics, or other technical fields At least 6 months of experience with Looker and LookML  Proficiency in building data pipelines leveraging Docker and orchestration frameworks such as Jenkins/Airflow/Luigi Experience designing and deploying high-performance systems with reliable monitoring and logging practices Proficiency working with a Data Warehouse platform (e.g. Snowflake, Redshift, BigQuery, AWS Athena, or Spectrum) Experience using data collection platforms such as Segment, RudderStack, Fivetran etc.  Familiarity with at least one cloud ecosystem: AWS/Azure Infrastructure/Google Cloud Experience of working in an agile environment and using tools such as JIRA/Asana/Trello  ', ' 2+ years of relevant industry experience Familiarity with data warehousing concepts including data model design and query optimization strategies Experience using and maintaining BI visualization tools such as (but not limited to) Looker, Tableau, or Power BI.  Creating production-ready ETL scripts with Python and SQL Experience with version control systems such as Github, Gitlab, Bitbucket etc.  Experience automating business and reporting processes Experience using data analysis and/or statistics to inform decisions  Strong verbal and written communication skills ', 'Experience automating business and reporting processes', 'Collaborate with product managers and analysts throughout the company to deliver reliable data that powers actionable insights', 'Experience using and maintaining BI visualization tools such as (but not limited to) Looker, Tableau, or Power BI. ', 'Experience with version control systems such as Github, Gitlab, Bitbucket etc. ', 'Proficiency in building data pipelines leveraging Docker and orchestration frameworks such as Jenkins/Airflow/Luigi', 'BS/MS in Computer Science, Math, Physics, or other technical fields', 'Ensure quality of data and completeness of event logging across Docker codebase', 'Integrate emerging methodology, technology, and version control practices that best fit the team. ', '2+ years of relevant industry experience', 'Experience of working in an agile environment and using tools such as JIRA/Asana/Trello ', 'Creating automated reporting of weekly and monthly metrics and ROI for the executive management team and board', 'Proficiency working with a Data Warehouse platform (e.g. Snowflake, Redshift, BigQuery, AWS Athena, or Spectrum)', 'Preferred Qualifications']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Marketing Data Engineer,PlayStation,"San Mateo, CA",3 weeks ago,81 applicants,"['', ""Bachelor’s or Master's degree in Engineering, Math, Statistics, Finance, Economics, or a related field"", 'A proactive problem solver and have good communication as well as project management skills to relay your findings and solutions across technical and non technical audiences', 'Continuously discover, transform, test, deploy and document data sources', ' ', 'Schedule data transformation and analysis pipelines using Airflow', 'PRIVACY NOTICE TO SIE LLC’S JOB APPLICANTS', 'Indirect identifiers such as a government ID, your Social Security, work permit or passport #.', 'Solid experience with ETL/ELT and scheduling tools (e.g. Talend, Airflow)', 'Work closely with data/business analysts to understand business requirements', 'Build and own SIE’s 1st party ID graph', ' Professional or job position-related information ', ' Non-public education information ', ' Non-public education information , including information about your education records, such as grades and transcripts.', 'You will be leading efforts in crafting high-performance, reusable, and scalable data models for our data warehouse to ensure our end-users get consistent and reliable answers when running their own analyses', 'Key Responsibilities:', 'Familiarity with customer, marketing and/or web data (e.g. Salesforce, Google Analytics, AdWords, YouTube etc)', 'Strong experience applying software engineering standard methodologies to analytics (e.g. version control, testing, and CI/CD)', 'Direct identifiers such as your first and last name.', 'Contact information such as your email address, mailing address, telephone number.', 'Experienced with data visualization tools and packages (e.g. Looker, Tableau, matplotlib)', 'Self- starter, motivated, responsible, innovative and technology-driven individual who performs well both independently and as a team member', 'Who You Are', 'Write sophisticated yet optimized data transformations in SQL/Python using dbt or similar technology', 'Categories of personal information we collect from you', 'Excellent SQL and Python knowledge strong hands-on data modeling and data warehousing skills and experience in transformations orchestrated through technologies such as dbt/cloud dataflow would be a plus', '  Identification and contact information  Direct identifiers such as your first and last name. Indirect identifiers such as a government ID, your Social Security, work permit or passport #. Contact information such as your email address, mailing address, telephone number.   Other information about you or that can be associated with you such as:  Sensitive/Protected Data. During the recruitment process, you may (voluntarily) provide us with your ethnicity, gender, military service information, or physical or mental health information, as well as your national origin and citizenship.  Professional or job position-related information , including your past professional experience, references; background verification; talent management and assessment; information regarding any conflicts of interests; and the terms and conditions of your job offer.  Non-public education information , including information about your education records, such as grades and transcripts.', ' Work closely with data/business analysts to understand business requirements You will be leading efforts in crafting high-performance, reusable, and scalable data models for our data warehouse to ensure our end-users get consistent and reliable answers when running their own analyses Write sophisticated yet optimized data transformations in SQL/Python using dbt or similar technology Schedule data transformation and analysis pipelines using Airflow Continuously discover, transform, test, deploy and document data sources Apply advanced aggregations and data wrangling techniques such as imputation for predictive analytics Work with wide range of tech stack at various levels such as Snowflake, dbt, Airflow, Fivetran, AWS, Spark, GCP Env:BigQuery, Cloud Composer, Cloud Dataflow etc Build and own SIE’s 1st party ID graph ', ' Sensitive/Protected Data. During the recruitment process, you may (voluntarily) provide us with your ethnicity, gender, military service information, or physical or mental health information, as well as your national origin and citizenship.', ' Other information about you or that can be associated with you such as: ', ' Sensitive/Protected Data. ', 'Work with wide range of tech stack at various levels such as Snowflake, dbt, Airflow, Fivetran, AWS, Spark, GCP Env:BigQuery, Cloud Composer, Cloud Dataflow etc', ' Identification and contact information ', 'Data Engineer - San Mateo', 'Apply advanced aggregations and data wrangling techniques such as imputation for predictive analytics', '  Other information about you or that can be associated with you such as:  Sensitive/Protected Data. During the recruitment process, you may (voluntarily) provide us with your ethnicity, gender, military service information, or physical or mental health information, as well as your national origin and citizenship.  Professional or job position-related information , including your past professional experience, references; background verification; talent management and assessment; information regarding any conflicts of interests; and the terms and conditions of your job offer.  Non-public education information , including information about your education records, such as grades and transcripts.', 'Power-user and specialist in building scalable data warehouses and pipelines using some of Cloud tools such as Snowflake, AWS, Google Cloud, Cloud ETL tools such as Databricks (Spark/Azure)', ' Professional or job position-related information , including your past professional experience, references; background verification; talent management and assessment; information regarding any conflicts of interests; and the terms and conditions of your job offer.', "" 5+ years working experience as a data or software engineer in a fast-paced growing company Excellent SQL and Python knowledge strong hands-on data modeling and data warehousing skills and experience in transformations orchestrated through technologies such as dbt/cloud dataflow would be a plus Strong experience applying software engineering standard methodologies to analytics (e.g. version control, testing, and CI/CD) Power-user and specialist in building scalable data warehouses and pipelines using some of Cloud tools such as Snowflake, AWS, Google Cloud, Cloud ETL tools such as Databricks (Spark/Azure) Solid experience with ETL/ELT and scheduling tools (e.g. Talend, Airflow) Experienced with data visualization tools and packages (e.g. Looker, Tableau, matplotlib) Familiarity with customer, marketing and/or web data (e.g. Salesforce, Google Analytics, AdWords, YouTube etc) Strong attention to details to highlight and address data quality issues Self- starter, motivated, responsible, innovative and technology-driven individual who performs well both independently and as a team member A proactive problem solver and have good communication as well as project management skills to relay your findings and solutions across technical and non technical audiences Bachelor’s or Master's degree in Engineering, Math, Statistics, Finance, Economics, or a related field Bonus points : Passionate about gaming and the digital entertainment industry "", 'Generally, We Obtain This Information Through Our Recruiting Team', '5+ years working experience as a data or software engineer in a fast-paced growing company', 'Strong attention to details to highlight and address data quality issues', 'Bonus points : Passionate about gaming and the digital entertainment industry']",Not Applicable,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Engineer - York,Compunnel ,"New York, NY",6 days ago,Be among the first 25 applicants,"['', ' Education:', ' Data Engineer Requirements:', 'Data Engineer - ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer - Big Data Platforms,"Lowe's Companies, Inc.",N/A,1 week ago,Be among the first 25 applicants,"['', 'Hadoop admin is responsible for capacity planning and estimating the requirements for lowering or increasing the capacity of the Hadoop cluster.', 'Understands Computer Science and/or Computer Engineering fundamentals; knows software architecture and readily applies this to Platform solutions', 'Supports systems integration testing (SIT) and user acceptance testing (UAT), provides insight into defining test plans, and ensures quality software deployment', '2 years of experience in Data, BI or Platform Engineering, Data Warehousing/ETL, or Software Engineering', '4 years of experience with technical documentation in a software development environment', ""Master\\'s degree in Computer Science, CIS, or related field"", 'Job Summary', '2-3 years of experience in Hadoop, NO-SQL, RDBMS or any Cloud Bigdata components, Teradata, MicroStrategy Expertise in Python, SQL, Scripting, Teradata, Hadoop utilities like Sqoop, Hive, Pig, Map Reduce, Spark, Ambari, Ranger, Kafka or equivalent Cloud Bigdata components', 'Platform Engineering Qualifications', 'Minimum Qualifications', ""Master\\'s degree in Computer Science, CIS, or related field2 years of IT experience developing and implementing business systems within an organization4 years of experience working with defect or incident tracking software4 years of experience with technical documentation in a software development environment2 years of experience working with an IT Infrastructure Library (ITIL) framework2 years of experience leading teams, with or without direct reportsExperience with application and integration middlewareExperience with database technologies"", '2 years of IT experience developing and implementing business systems within an organization', ""Bachelor\\'s degree in Engineering, Computer Science, CIS, or related field (or equivalent work experience in a related field)"", 'Expertise in Python, SQL, Scripting, Teradata, Hadoop utilities like Sqoop, Hive, Pig, Map Reduce, Spark, Ambari, Ranger, Kafka or equivalent Cloud Bigdata components', 'Implementing, managing and administering the overall Hadoop infrastructure.', 'About Lowe’s In The Community', 'About Lowe’s', 'Translates business requirements and specifications into logical program designs, modules, stable application systems, and data solutions with occasional guidance from senior colleagues; partners with Product Team to understand business needs and functional specificationsDevelops, configures, or modifies integrated business and/or enterprise application solutions within various computing environments by designing and coding component-based applications using various programming languagesConducts the implementation and maintenance of complex business and enterprise data solutions to ensure successful deployment of released applicationsSupports systems integration testing (SIT) and user acceptance testing (UAT), provides insight into defining test plans, and ensures quality software deploymentParticipates in the end-to-end product lifecycle by applying and sharing an in-depth understanding of company and industry methodologies, policies, standards, and controlsUnderstands Computer Science and/or Computer Engineering fundamentals; knows software architecture and readily applies this to Platform solutionsAutomates and simplifies team development, test, and operations processes; develops conceptual, logical and physical architectures consisting of one or more viewpoints (business, application, data, and infrastructure) required for business solution deliverySolves difficult technical problems; solutions are testable, maintainable, and efficient ', 'Translates business requirements and specifications into logical program designs, modules, stable application systems, and data solutions with occasional guidance from senior colleagues; partners with Product Team to understand business needs and functional specifications', 'Automates and simplifies team development, test, and operations processes; develops conceptual, logical and physical architectures consisting of one or more viewpoints (business, application, data, and infrastructure) required for business solution delivery', '2 years of experience leading teams, with or without direct reports', '1 year of experience working on project(s) involving the implementation of solutions applying development life cycles (SDLC)', 'Responsibilities of Hadoop admin include – deploying a Hadoop cluster, maintaining a Hadoop cluster, adding and removing nodes using cluster monitoring tools like Ganglia Nagios or Cloudera Manager, configuring the Name Node high availability and keeping a track of all the running Hadoop jobs.', 'Solves difficult technical problems; solutions are testable, maintainable, and efficient ', '2 years of experience working with an IT Infrastructure Library (ITIL) framework', 'Experience with database technologies', 'Conducts the implementation and maintenance of complex business and enterprise data solutions to ensure successful deployment of released applications', 'Responsibilities of Hadoop admin include – deploying a Hadoop cluster, maintaining a Hadoop cluster, adding and removing nodes using cluster monitoring tools like Ganglia Nagios or Cloudera Manager, configuring the Name Node high availability and keeping a track of all the running Hadoop jobs.Implementing, managing and administering the overall Hadoop infrastructure.Work closely with the data engineer, network, BI and application teams to make sure that all the big data applications are highly available and performing as expected.Hadoop admin is responsible for capacity planning and estimating the requirements for lowering or increasing the capacity of the Hadoop cluster.Setup/migrate job between on-prem to cloud and other way. Should have knowledge of cloud networking and IAM', 'Setup/migrate job between on-prem to cloud and other way. Should have knowledge of cloud networking and IAM', 'Platform Engineering Responsibilities', 'Key Responsibilities', '4 years of experience working with defect or incident tracking software', 'Experience with application and integration middleware', '2-3 years of experience in Hadoop, NO-SQL, RDBMS or any Cloud Bigdata components, Teradata, MicroStrategy ', 'Participates in the end-to-end product lifecycle by applying and sharing an in-depth understanding of company and industry methodologies, policies, standards, and controls', 'Develops, configures, or modifies integrated business and/or enterprise application solutions within various computing environments by designing and coding component-based applications using various programming languages', 'Preferred Qualifications', ""Bachelor\\'s degree in Engineering, Computer Science, CIS, or related field (or equivalent work experience in a related field)2 years of experience in Data, BI or Platform Engineering, Data Warehousing/ETL, or Software Engineering1 year of experience working on project(s) involving the implementation of solutions applying development life cycles (SDLC)"", 'Work closely with the data engineer, network, BI and application teams to make sure that all the big data applications are highly available and performing as expected.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Kofi Group,"Austin, TX",7 days ago,Be among the first 25 applicants,[''],Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,"ConsumerTrack, Inc.","Austin, TX",1 month ago,192 applicants,"['', 'Ability to operate in an agile, entrepreneurial start-up environment, and prioritize.', 'Learn More About What We Do', 'Knowledge for ETL/ELT tools like Informatica, IBM DataStage, or SaaS ETL tools is a plus.', '401(k) -- employees can start contributing immediately. After the first year, CTI matches your contribution up to 4% of your salary.', 'To keep our community of ConsumerTrackers engaged and connected, virtual team building events are held weekly and monthly.', 'For wellness and balance, weekly virtual fitness classes such as yoga are available.', ' Our teams are working remotely 100% for the foreseeable future and have flex time. We’re in the digital media space so we’re mobile and flexible!*Option to work from an office (if you need to get away!)  Tools & resources are available to keep our team connected across North America. (JIRA, Trello, Airtable, Slack, Zoom and so much more!) To keep our community of ConsumerTrackers engaged and connected, virtual team building events are held weekly and monthly. For wellness and balance, weekly virtual fitness classes such as yoga are available. To care for the local communities that we’re a part of across the U.S our team members host socially distanced philanthropic events every quarter. And most importantly, we’ve committed to consistent and transparent communication to help us all stay informed, engaged and to keep us on our path to success and #greatness. ', 'Build processes supporting data transformation, data structures metadata, and workload management.', 'Strong skills to write complex, highly-optimized SQL queries across large volumes of data.', 'Functions/Responsibilities', 'Contribution to student loan debt payments after the first year of employment.', ' Build and maintain multiple data pipelines to ingest new data sources (API and file-based) and support products used by both external users and internal teams. Optimize by building tools to evaluate and automatically monitor data quality, develop automated scheduling, testing, and distribution of feeds. Work with our data science and product management teams to design, rapid prototype, and productize new data product ideas and capabilities.  Work with the data engineering team to migrate and enhance our existing Pentaho-based ETL pipeline to a new ELT-based/SaaS Integration system. Conquer complex problems by finding new ways to solve with simple, efficient approaches with a focus on reliability, scalability, quality, and cost of our platforms. Build processes supporting data transformation, data structures metadata, and workload management. Collaborate with the team to perform root cause analysis and audit internal and external data and processes to help answer specific business questions. ', 'Experience with AWS infrastructure.', 'Benefits', 'Must have excellent troubleshooting and problem-solving skills.', 'Collaborate with the team to perform root cause analysis and audit internal and external data and processes to help answer specific business questions.', 'Paid maternity leave and paternity leave programs.', 'Build and maintain multiple data pipelines to ingest new data sources (API and file-based) and support products used by both external users and internal teams.', 'Paid vacation, sick days and holidays.', 'Comfortable working directly with data analytics to bridge business requirements with data engineering.', 'Experience with Tableau or other reporting tools is a plus.', ' Work with the data engineering team to migrate and enhance our existing Pentaho-based ETL pipeline to a new ELT-based/SaaS Integration system.', ' Master’s Degree (or a B.S. degree with relevant industry experience) in math, statistics, computer science, or equivalent technical field. 3+ years of professional Dimensional Data Warehousing/Data Modeling and ‘Big Data’ Experience. Strong skills to write complex, highly-optimized SQL queries across large volumes of data. Comfortable working directly with data analytics to bridge business requirements with data engineering. Experience with AWS infrastructure. Must have excellent troubleshooting and problem-solving skills. Ability to operate in an agile, entrepreneurial start-up environment, and prioritize. Excellent communication and teamwork, and a passion for learning. Curiosity and passion for data, visualization, and solving problems. Willingness to question the validity, accuracy of data and assumptions. ', 'Curiosity and passion for data, visualization, and solving problems.', 'Requirements', 'Competitive salary with excellent growth opportunity; we pride ourselves in having a team that exudes leadership, high initiative, creativity and passion.', 'Description', 'Excellent communication and teamwork, and a passion for learning.', ' Competitive salary with excellent growth opportunity; we pride ourselves in having a team that exudes leadership, high initiative, creativity and passion. Awesome medical, dental and vision plans with heavy employer contribution. Paid maternity leave and paternity leave programs. Paid vacation, sick days and holidays. Company funding for outside classes and conferences to help you improve your skills. Contribution to student loan debt payments after the first year of employment. 401(k) -- employees can start contributing immediately. After the first year, CTI matches your contribution up to 4% of your salary. ', 'Work with our data science and product management teams to design, rapid prototype, and productize new data product ideas and capabilities.', 'Our teams are working remotely 100% for the foreseeable future and have flex time. We’re in the digital media space so we’re mobile and flexible!*Option to work from an office (if you need to get away!) ', '3+ years of professional Dimensional Data Warehousing/Data Modeling and ‘Big Data’ Experience.', 'Master’s Degree (or a B.S. degree with relevant industry experience) in math, statistics, computer science, or equivalent technical field.', 'We are an equal-opportunity employer, and all qualified applicants will receive consideration for ', 'Conquer complex problems by finding new ways to solve with simple, efficient approaches with a focus on reliability, scalability, quality, and cost of our platforms.', 'employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law.', 'Awesome medical, dental and vision plans with heavy employer contribution.', 'Optimize by building tools to evaluate and automatically monitor data quality, develop automated scheduling, testing, and distribution of feeds.', 'Tools & resources are available to keep our team connected across North America. (JIRA, Trello, Airtable, Slack, Zoom and so much more!)', '*Option to work from an office (if you need to get away!)', ' Experience with Redshift, Snowflake, or other MPP databases is a plus. Knowledge for ETL/ELT tools like Informatica, IBM DataStage, or SaaS ETL tools is a plus. Experience with Tableau or other reporting tools is a plus. ', 'Willingness to question the validity, accuracy of data and assumptions.', 'And most importantly, we’ve committed to consistent and transparent communication to help us all stay informed, engaged and to keep us on our path to success and #greatness.', 'Basic Qualifications:', 'Experience with Redshift, Snowflake, or other MPP databases is a plus.', 'Preferred Qualifications', 'To care for the local communities that we’re a part of across the U.S our team members host socially distanced philanthropic events every quarter.', 'Company funding for outside classes and conferences to help you improve your skills.']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Data Engineer,Mindtree,"St Louis, MO",3 weeks ago,99 applicants,"['', 'Collaboration with tribes/dev teams on Reliability development (Fixes, Logging, Delivery Metrics)', 'Why Work with Us:', 'We are hiring for one of our strategic projects and looking for Data Engineer (DevOps) who are willing to join us as Fulltime Employees and interested in the below job description.', '• Recognized for our employee learning and talent development practices', 'Maintain services during deployment and in production by measuring and monitoring key performance and service level indicators including availability, latency, and overall system health', 'Company Location: Atlanta, GA', '• Values-driven and engaged leadership', 'Screening Highlights:', 'Ability to handle large volumes of proprietary data, introspect and provide insights', 'Experience in setting up and managing/modifying CI/CD pipelines using Jenkins.', '• Good work-life balance encouraged', '3+ years of hands-on experience in setting up and managing/modifying\xa0CI/CD\xa0pipelines using Jenkins preferably.2+ Hands-on experience with\xa0scripting languages\xa0such as Python and Bash.1+ years Hands-on experience with Hadoop infrastructure and Data Ingestion\xa0Ability to handle large volumes of proprietary data, introspect and provide insights1+ years of developing and/or administering software in\xa0public cloud\xa0(AWS, Azure or GCP)', 'Proficiency with continuous integration and continuous delivery tooling and practices', 'Build and manage CI/CD pipelines using Jenkins.', '2+ Hands-on experience with\xa0scripting languages\xa0such as Python and Bash.', 'Demonstrable cross-functional knowledge with systems, storage, networking, security and databases', 'System administration skills, including automation and orchestration of Linux/Windows using Chef, Puppet, Ansible, Salt Stack and/or containers (Docker, Kubernetes, etc.)', 'Roles & Responsibilities', 'Automate system scalability and continually work to improve system resiliency, performance and efficiency', 'About Mindtree', 'Provision and manage GCP infrastructure including Deploying and implementing Google Kubernetes Engine resources', 'public cloud', '3+\xa0years of experience developing and/or administering software in public\xa0cloud. Hands-on 6+ months in GCP.Experience in monitoring infrastructure and application uptime and availability to ensure functional and performance objectives.Experience in languages such as Python, Ruby, Bash, Java, Go, Perl, JavaScript and/or node.jsDemonstrable cross-functional knowledge with systems, storage, networking, security and databasesSystem administration skills, including automation and orchestration of Linux/Windows using Chef, Puppet, Ansible, Salt Stack and/or containers (Docker, Kubernetes, etc.)Proficiency with continuous integration and continuous delivery tooling and practicesExperience managing Infrastructure as code via tools such as Terraform or CloudFormationExperience in setting up and managing/modifying CI/CD pipelines using Jenkins.Significant experience in configuring industry leading infrastructure/application monitoring tools (Stackdriver, Kibana, Grafana, Datadog, Splunk, Dynatrace, AppDynamics etc)', 'CI/CD', 'Practice sustainable incident response as part of an on-call rotation and through blameless postmortems', 'Welcome to possible', '3+\xa0years of experience developing and/or administering software in public\xa0cloud. Hands-on 6+ months in GCP.', '• Recognition is the cornerstone of our culture', '1+ years of developing and/or administering software in\xa0public cloud\xa0(AWS, Azure or GCP)', 'Roles and Responsibilities:', 'scripting languages', 'Automate mundane tasks using scripting languages.', 'Support services prior to production via infrastructure design, software platform development, load testing, capacity planning and launch reviews', 'Automating infrastructure builds/configurations', 'Define, Implement and assign ownership for Stability/Reliability(SLIs, SLOs, Error Budgets)', 'Mindtree Equal Employment Opportunity Policy', 'Significant experience in configuring industry leading infrastructure/application monitoring tools (Stackdriver, Kibana, Grafana, Datadog, Splunk, Dynatrace, AppDynamics etc)', 'Build and manage CI/CD pipelines using Jenkins.Automate mundane tasks using scripting languages.Handle large volumes of proprietary data, introspect and provide insights - Identify, gather, transform and analyze data within and across database platforms.Influence and design cloud infrastructure, architecture, standards and methods for large-scale systemsSupport services prior to production via infrastructure design, software platform development, load testing, capacity planning and launch reviewsMaintain services during deployment and in production by measuring and monitoring key performance and service level indicators including availability, latency, and overall system healthAutomate system scalability and continually work to improve system resiliency, performance and efficiencyPractice sustainable incident response as part of an on-call rotation and through blameless postmortemsRemediate tasks within corrective action plan via sustainable, preventative, and automated measures whenever possibleProvision and manage GCP infrastructure including Deploying and implementing Google Kubernetes Engine resourcesAutomating infrastructure builds/configurationsDefine, Implement and assign ownership for Stability/Reliability(SLIs, SLOs, Error Budgets)Collaboration with tribes/dev teams on Reliability development (Fixes, Logging, Delivery Metrics)', 'Mindtree provides equal employment opportunities to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics.', 'Handle large volumes of proprietary data, introspect and provide insights - Identify, gather, transform and analyze data within and across database platforms.', 'Influence and design cloud infrastructure, architecture, standards and methods for large-scale systems', 'Experience in languages such as Python, Ruby, Bash, Java, Go, Perl, JavaScript and/or node.js', 'Role: Data Engineer (DevOps)', '3+ years of hands-on experience in setting up and managing/modifying\xa0CI/CD\xa0pipelines using Jenkins preferably.', 'Key Skillsets:', 'Company Name: Mindtree', '1+ years Hands-on experience with Hadoop infrastructure and Data Ingestion\xa0', 'Experience in monitoring infrastructure and application uptime and availability to ensure functional and performance objectives.', 'Experience managing Infrastructure as code via tools such as Terraform or CloudFormation', 'Mindtree [NSE: MINDTREE] is a global technology consulting and services company, helping enterprises marry scale with agility to achieve competitive advantage. “Born digital,” in 1999 and now a Larsen & Toubro Group Company, Mindtree applies its deep domain knowledge to 350+ enterprise client engagements to break down silos, make sense of digital complexity and bring new initiatives to market faster. We enable IT to move at the speed of business, leveraging emerging technologies and the efficiencies of Continuous Delivery to spur business innovation. Operating in more than 15 countries across the world, we’re consistently regarded as one of the best places to work, embodied every day by our winning culture made up of 21,000 entrepreneurial, collaborative and dedicated “Mindtree Minds”', 'Remediate tasks within corrective action plan via sustainable, preventative, and automated measures whenever possible']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,CapTech Consulting,"Richmond, VA",6 days ago,Be among the first 25 applicants,"['', 'Experience tuning SQL queries to ensure performance and reliability', 'CapTech is an equal opportunity employer committed to fostering a culture of equality, inclusion and fairness — each foundational to our core values. We strive to create a diverse environment where each employee is encouraged to bring their unique ideas, backgrounds and experiences to the workplace.', 'Present programming documentation and design to team members and convey complex information in a clear and concise manner.', 'Design, develop, and implement data processing pipelines at scale', 'Participate in requirements gathering sessions with business and technical staff to distill technical requirements from business requests.', 'Competitive salary with performance-based bonus opportunities', 'Team building and social activities', 'Strong SQL development skills', 'Development experience building ETL graphs using the Ab Initio GDE, EME and Co-Operating system', 'Extract data from multiple sources, integrate disparate data into a common data model, and integrate data into a target database, application, or file using efficient programming processes.', 'Some of our technologies might include: Python, Cassandra, Spark, Java, Scala, Informatica, SQL Server, SSIS, Oracle, Kafka.', 'Partner with clients to fully understand business philosophy and IT Strategy; recommend process improvements to increase efficiency and reliability in ETL development.', 'Collaborate with Quality Assurance resources to debug code and ensure the timely delivery of products.', 'Software engineering best-practices, including version control (Git, TFS, JIRA, etc.) and test-driven development', 'Qualifications', 'Communicate with all levels of stakeholders as appropriate, including executives, data modelers, application developers, business users, and customers', 'Evaluate structured and unstructured datasets utilizing statistics, data mining, and predictive analytics to gain additional business insights', 'Competitive salary with performance-based bonus opportunitiesSingle and Family Health Insurance plans, including Dental coverageShort-Term and Long-Term disabilityMatching 401(k)Competitive Paid Time OffTraining and Certification opportunities eligible for expense reimbursementTeam building and social activitiesMentor program to help you develop your career', 'Company Description', 'Single and Family Health Insurance plans, including Dental coverage', 'Minimum of 4-5 years experience designing, developing, and testing software aligned with defined requirements', 'Short-Term and Long-Term disability', 'Write and refine code to ensure performance and reliability of data extraction and processing.', 'Development experience with Unix tools and shell scripts', 'Mentor program to help you develop your career', 'Competitive Paid Time Off', 'Development experience with at least two different programming languages (Python, Java, Scala, etc.)', 'Design, develop, document, and test advanced data systems that bring together data from disparate sources, making it available to data scientists, analysts, and other users using scripting and/or programming languages (Python, Java, Scala, etc)Evaluate structured and unstructured datasets utilizing statistics, data mining, and predictive analytics to gain additional business insightsDesign, develop, and implement data processing pipelines at scalePresent programming documentation and design to team members and convey complex information in a clear and concise manner.Extract data from multiple sources, integrate disparate data into a common data model, and integrate data into a target database, application, or file using efficient programming processes.Write and refine code to ensure performance and reliability of data extraction and processing.Communicate with all levels of stakeholders as appropriate, including executives, data modelers, application developers, business users, and customersParticipate in requirements gathering sessions with business and technical staff to distill technical requirements from business requests.Partner with clients to fully understand business philosophy and IT Strategy; recommend process improvements to increase efficiency and reliability in ETL development.Collaborate with Quality Assurance resources to debug code and ensure the timely delivery of products.Some of our technologies might include: Python, Cassandra, Spark, Java, Scala, Informatica, SQL Server, SSIS, Oracle, Kafka.', ""Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experienceDevelopment experience building ETL graphs using the Ab Initio GDE, EME and Co-Operating systemStrong SQL development skillsDevelopment experience with at least two different programming languages (Python, Java, Scala, etc.)Development experience with Unix tools and shell scriptsDevelopment experience with at least two different database platforms (Teradata, Oracle, MySQL, MS SQL, etc.)Minimum of 4-5 years experience designing, developing, and testing software aligned with defined requirementsExperience tuning SQL queries to ensure performance and reliabilitySoftware engineering best-practices, including version control (Git, TFS, JIRA, etc.) and test-driven development"", ""Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience"", 'Job Description', 'Development experience with at least two different database platforms (Teradata, Oracle, MySQL, MS SQL, etc.)', 'Matching 401(k)', 'Design, develop, document, and test advanced data systems that bring together data from disparate sources, making it available to data scientists, analysts, and other users using scripting and/or programming languages (Python, Java, Scala, etc)', 'Specific Responsibilities For The Data Engineer, Analytics Position Include', 'Training and Certification opportunities eligible for expense reimbursement']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Junior Data Engineer,Invesco Ltd.,"Atlanta, GA",1 week ago,43 applicants,[''],Associate,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer - Everyday Health,Everyday Health Group,"New York, NY",7 days ago,Be among the first 25 applicants,"['', 'AWS: SQS, SNS, Lambda, DynamoDB, or similar from Google Cloud', 'You find it offensive when a system lacks a robust API', 'Exceptional communication, both with technical and non-technical stakeholders', 'Design complex queries to crunch data and produce summary and aggregate datasets', 'ETL management systems such as Apache Airflow', 'Build ETL processes, across a wide array of just about every kind of database system', 'Python, Java, JS/ECMAScript, C/C++, C#', '4-6 years of professional software developmentAt least 2-3 of those years specifically working directly with dataAt least 1 year working in the cloud (AWS, GCP, or Azure) preferredAt least 1 year working with Big Data concepts and systems, preferredSome exposure to non-relational or NoSQL database systemsKnowledge of and experience with multiple SQL dialectsExperience with more than one programming languageExperience with shell scripting and LinuxKnowledge of modern web technologies and architectureFoundational understanding of object-oriented programming conceptsExceptional communication, both with technical and non-technical stakeholdersExperienced collaboration skills, up/down/across a mid-sized organizationAbility to thrive in a landscape of rapid evolution and varied tasksAbility to see tasks thru from requirements gathering all the way to deploymentA strong tendency to be self-driven and self-motivated', 'Ability to thrive in a landscape of rapid evolution and varied tasks', 'A strong tendency to be self-driven and self-motivated', 'PostgreSQL, MySQL', 'Help maintain the cloud-based Data Warehouse central to our efforts', 'What You’ll Do', 'Build and maintain various self-service data platforms and tools for internal use', 'Knowledge of modern web technologies and architecture', 'At least 1 year working with Big Data concepts and systems, preferred', '4-6 years of professional software development', 'You wish you could automate everything', 'You daydream about data schema', 'You’re more likely to get distracted by a good chart than your social media feed', 'Knowledge of and experience with multiple SQL dialects', 'Qualifications', 'Position at Everyday Health - Pregnancy & Parenting', 'Description', 'The Team', 'Facilitate and execute the collection, processing, and analysis of virtually all business dataBuild ETL processes, across a wide array of just about every kind of database systemWrite code to interact with an untold number of internal and external APIs and systemsDesign complex queries to crunch data and produce summary and aggregate datasetsCombine and correlate large datasets from multiple data sources, and analyze for integrityBuild and maintain various self-service data platforms and tools for internal useHelp maintain the cloud-based Data Warehouse central to our effortsGenerally speaking, manipulate and move a lot of data around from one place to another', 'AWS RedShift, Google BigQuery, Snowflake', 'Facilitate and execute the collection, processing, and analysis of virtually all business data', 'Experienced collaboration skills, up/down/across a mid-sized organization', 'Experience with more than one programming language', 'At least 1 year working in the cloud (AWS, GCP, or Azure) preferred', 'You think in SQL', 'Combine and correlate large datasets from multiple data sources, and analyze for integrity', 'You don’t think there’s anything you couldn’t do, when it comes to software development', 'Key Technologies', 'Some exposure to non-relational or NoSQL database systems', 'Python, Java, JS/ECMAScript, C/C++, C#PostgreSQL, MySQLAWS: SQS, SNS, Lambda, DynamoDB, or similar from Google CloudAWS RedShift, Google BigQuery, SnowflakeETL management systems such as Apache Airflow', 'Foundational understanding of object-oriented programming concepts', 'At least 2-3 of those years specifically working directly with data', 'About You', 'Generally speaking, manipulate and move a lot of data around from one place to another', 'Write code to interact with an untold number of internal and external APIs and systems', 'You think in SQLYou daydream about data schemaYou wish you could automate everythingYou find it offensive when a system lacks a robust APIYou’re more likely to get distracted by a good chart than your social media feedYou don’t think there’s anything you couldn’t do, when it comes to software development', 'Ability to see tasks thru from requirements gathering all the way to deployment', 'Experience with shell scripting and Linux']",Entry level,Full-time,Engineering,Marketing and Advertising,2021-03-18 14:34:51
Data Engineer - Madison,Skills Pipeline,"Madison, WI",2 days ago,Be among the first 25 applicants,"['', '100% employer-funded short & long-term disability coverage', 'Collaborate with members of your team (eg, Software Developers, Customer Success) on the project goals', 'Salary paid semi-monthlyCompetitive medical, dental, and vision coverage100% employer-funded short & long-term disability coverage15 days of PTO and extra time off for holidaysRemote work throughout COVID-19 and the option to work remote two days per week once we return to the officeA pinball machine for when you really need a break', 'Experience with data analysis programming languages (i.e. Python/R, SAS, etc)', 'Build high-performance algorithms, predictive models, and prototypes', 'Integrate up-and-coming data management and software engineering technologies into existing data structures', 'Compile and provide data sets as requested by team members or clients', 'Execute customer onboarding verifying customer data feeds matches the uniformity of our data', 'Troubleshoot database-related application performance issues and recommend solutions', 'Ensure that all systems meet our business requirements as well as industry', 'standards', 'Responsibilities', 'Intellectual curiosity to find new and creative ways to solve data issues', 'Proven ability in executing and supporting strategies to ensure the health of enterprise database environments', 'Bonus: experience with LAMP stack development and RESTful APIs', 'Salary paid semi-monthly', 'Skills And Qualifications', '15 days of PTO and extra time off for holidays', 'Remote work throughout COVID-19 and the option to work remote two days per week once we return to the office', 'Design, construct, install, test, and maintain data management systems', 'Bachelors degree in an applicable discipline or equivalent experience', 'Research new uses for existing data and recommend different ways to constantly improve data reliability and quality', 'Strong knowledge of the technical aspects of database security, enterprise backup and restore, and replication strategies', 'Design, construct, install, test, and maintain data management systemsBuild high-performance algorithms, predictive models, and prototypesEnsure that all systems meet our business requirements as well as industrystandardsTroubleshoot database-related application performance issues and recommend solutionsIntegrate up-and-coming data management and software engineering technologies into existing data structuresDevelop set processes for data modeling and analysisEmploy an array of technological languages and tools to connect systems togetherCollaborate with members of your team (eg, Software Developers, Customer Success) on the project goalsCompile and provide data sets as requested by team members or clientsResearch new uses for existing data and recommend different ways to constantly improve data reliability and qualityExecute customer onboarding verifying customer data feeds matches the uniformity of our data', 'Develop set processes for data modeling and analysis', 'Bachelors degree in an applicable discipline or equivalent experienceProven ability in executing and supporting strategies to ensure the health of enterprise database environmentsStrong knowledge of the technical aspects of database security, enterprise backup and restore, and replication strategiesIntellectual curiosity to find new and creative ways to solve data issuesExcellent written and verbal communication, interpersonal and collaborative skillsExperience with data analysis programming languages (i.e. Python/R, SAS, etc)Bonus: experience with LAMP stack development and RESTful APIs', 'Competitive medical, dental, and vision coverage', 'Employ an array of technological languages and tools to connect systems together', 'Excellent written and verbal communication, interpersonal and collaborative skills', 'A pinball machine for when you really need a break', 'Benefits + perks: ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Senior Data Engineer - Data Quality/Python(KTD),Kroger,"Cincinnati, OH",4 weeks ago,Be among the first 25 applicants,"['', 'Essential Job Functions', 'Any experience with a variety of SQL, NoSQL and Big Data Platforms', '4+ years proven track record of delivering large scale, high quality operational or analytical data systems', 'Utilize enterprise standards for data domains and data solutions, focusing on simplified integration and streamlined operational and analytical usesEnsure there is clarity between ongoing projects, escalating when necessary, including direct collaboration with 84.51Leverage innovative new technologies and approaches to renovate, extend, and transform the existing core data assets, including SQL-based, NoSQL-based, and Cloud-based data platformsDefine high-level migration plans to address the gaps between the current and future stateContribute to the development of cost/benefit analysis for leadership to shape sound architectural decisionsAnalyze technology environments to detect critical deficiencies and recommend solutions for improvementPromote the reuse of data assets, including the management of the data catalog for referenceDraft architectural diagrams, interface specifications and other design documentsMust be able to perform the essential job functions of this position with or without reasonable accommodation', 'Define high-level migration plans to address the gaps between the current and future state', 'Excellent oral/written communication skills', 'Keywords', 'Shift(s):', 'Position Summary', 'FLSA Status: ', 'Any experience building solutions using elastic architectures (preferably Microsoft Azure and Google Cloud Platform)', 'Draft architectural diagrams, interface specifications and other design documents', 'Jobs at Kroger:', 'Any experience with SSAS Tabular models, Power BI, Dataflows and DAXAny experience with Azure Data Platform stack: Azure Data Lake, Data Factory and DatabricksAny experience with Python, Spark and SQLAny experience with streaming technologies like Kafka, IBM MQ and EventHubAny experience with data science solutions or platformsAny experience with a variety of SQL, NoSQL and Big Data PlatformsAny experience building solutions using elastic architectures (preferably Microsoft Azure and Google Cloud Platform)', "" Bachelor's Degree in computer science, software engineering, or related field4+ years experience in the data development and principles including end-to-end design patterns4+ years proven track record of delivering large scale, high quality operational or analytical data systems4+ years successful and applicable experience building complex data solutions that have been successfully delivered to customersAny experience in a minimum of two of the following technical disciplines: data warehousing, big data management, analytics development, data science, application programming interfaces (APIs), data integration, cloud, servers and storage, and database mgmtExcellent oral/written communication skills"", 'Required Certifications/Licenses:', 'Must be able to perform the essential job functions of this position with or without reasonable accommodation', 'Promote the reuse of data assets, including the management of the data catalog for reference', 'at https://www.kroger.com/livekt ', 'Any experience with Azure Data Platform stack: Azure Data Lake, Data Factory and Databricks', 'Company Name:', '4+ years experience in the data development and principles including end-to-end design patterns', 'Desired Previous Experience/Education', 'Any experience in a minimum of two of the following technical disciplines: data warehousing, big data management, analytics development, data science, application programming interfaces (APIs), data integration, cloud, servers and storage, and database mgmt', 'Any experience with SSAS Tabular models, Power BI, Dataflows and DAX', 'Any experience with Python, Spark and SQL', 'Line Of Business', ' See what life is like at Kroger Technology ', 'Leverage innovative new technologies and approaches to renovate, extend, and transform the existing core data assets, including SQL-based, NoSQL-based, and Cloud-based data platforms', 'Additional Technology Information', 'Company Overview: ', 'Analyze technology environments to detect critical deficiencies and recommend solutions for improvement', 'Utilize enterprise standards for data domains and data solutions, focusing on simplified integration and streamlined operational and analytical uses', 'Ensure there is clarity between ongoing projects, escalating when necessary, including direct collaboration with 84.51', 'Contribute to the development of cost/benefit analysis for leadership to shape sound architectural decisions', ' banner names ', 'Any experience with data science solutions or platforms', 'Position Type:', 'Education Level:', "" Bachelor's Degree in computer science, software engineering, or related field"", 'Any experience with streaming technologies like Kafka, IBM MQ and EventHub', '4+ years successful and applicable experience building complex data solutions that have been successfully delivered to customers', 'Minimum Position Qualifications']",Associate,Full-time,Information Technology,Food & Beverages,2021-03-18 14:34:51
Data Engineer,Yum! Brands,"Plano, TX",2 weeks ago,59 applicants,"['Dive into the data and perform Exploratory Data Analysis(EDA), statistical modelling and analytics using Python, SQL and Scala to perform Market Basket Analysis of order transactions, gather delivery process metrices and clean and control various master databases.', '1+ years hands-on coding skills in languages like Scala, Python, SQL', 'Key Responsibilities', '1+ years of experience with tools like Kafka and Spark, Cloud Datawarehouse and Hadoop', 'Assisting and working closely with business stakeholders and data/research scientists by delivering data driven solutions to leverage company data to drive business outcomes, hence driving Pizza Hut growth.', 'Working closely with data scientists to scale, integrate and production data driven machine learning solutions.', 'Creating and engineering data pipelines for end-to-end delivery of raw and trained data sets, to and from brand cloud environment using Big data frameworks like Apache Spark, Hadoop, and, Scala, databases like Snowflake. Develop and maintain global data models for delivery process optimization and fleet telematics, use cases and applications at Pizza Hut using Python, AWS EKS, AWS Cloudwatch and AWS S3 storage. Working closely with data scientists to scale, integrate and production data driven machine learning solutions. Dive into the data and perform Exploratory Data Analysis(EDA), statistical modelling and analytics using Python, SQL and Scala to perform Market Basket Analysis of order transactions, gather delivery process metrices and clean and control various master databases. Implementing and managing production support processes around data lifecycle, data quality, coding utilities, storage, reporting and other data integration points. Assisting and working closely with business stakeholders and data/research scientists by delivering data driven solutions to leverage company data to drive business outcomes, hence driving Pizza Hut growth. 1+ years of experience with tools like Kafka and Spark, Cloud Datawarehouse and Hadoop 1+ years hands-on coding skills in languages like Scala, Python, SQL Participate and lead in design sessions, demos and prototype sessions, testing and training workshops with business users and other IT associates', 'Implementing and managing production support processes around data lifecycle, data quality, coding utilities, storage, reporting and other data integration points.', 'Creating and engineering data pipelines for end-to-end delivery of raw and trained data sets, to and from brand cloud environment using Big data frameworks like Apache Spark, Hadoop, and, Scala, databases like Snowflake.', 'Participate and lead in design sessions, demos and prototype sessions, testing and training workshops with business users and other IT associates', 'Develop and maintain global data models for delivery process optimization and fleet telematics, use cases and applications at Pizza Hut using Python, AWS EKS, AWS Cloudwatch and AWS S3 storage.']",Not Applicable,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Bounteous,"Pittsburgh, PA",3 weeks ago,Be among the first 25 applicants,"['', 'College degree in Computer Science, Data Science, Analytics or related field5+ years of experience building data pipelines and extracting, transforming, and loading marketing and customer data Strong SQL skills Proficient in at least one programming language such as Python, Java, ScalaExperience working with data warehouse solutions like Amazon Redshift, Google BigQuery, Snowflake, or similarExperience working with cloud technologies such as AWS, Google Cloud, Azure, or similar Proficient in code version control systems like GitStrong understanding of customer data platformsExposure to Spark, Hadoop, and other big data technologies is a plus', 'Must be legally eligible to work in Canada.', 'Be a platform expert in leading CDP solutions like AEP, TreasureData, RedPoint or similarDevelop a deep expertise in our client’s data infrastructure and partner with the respective teams Work with the Solution Architect and extract, transform and load marketing and customer data into the platform in an automated and scalable mannerBuild the unified customer profile Work with Consultants and Data Scientists to identify and write the necessary queries needed for segmentation, reporting, analysis, and ML models', '5+ years of experience building data pipelines and extracting, transforming, and loading marketing and customer data ', 'For Employment Opportunities Based In Canada', 'Work with the Solution Architect and extract, transform and load marketing and customer data into the platform in an automated and scalable manner', 'Work with Consultants and Data Scientists to identify and write the necessary queries needed for segmentation, reporting, analysis, and ML models', 'Proficient in code version control systems like Git', 'Experience working with data warehouse solutions like Amazon Redshift, Google BigQuery, Snowflake, or similar', 'Build the unified customer profile ', 'Experience working with cloud technologies such as AWS, Google Cloud, Azure, or similar ', 'Develop a deep expertise in our client’s data infrastructure and partner with the respective teams ', 'Data Engineer ', 'Proficient in at least one programming language such as Python, Java, Scala', 'Bounteous is an equal opportunity employer. We embrace diversity and are committed to creating an inclusive workplace. In accordance with the Ontario Human Rights Code and Accessibility for Ontarians with Disabilities Act, 2005, accommodation will be provided at any point throughout the hiring process, provided the candidate makes their accommodation needs known to Bounteous. We welcome applications from all qualified candidates. ', 'Role And Responsibilities', 'Strong SQL skills ', 'Strong understanding of customer data platforms', 'Be a platform expert in leading CDP solutions like AEP, TreasureData, RedPoint or similar', 'College degree in Computer Science, Data Science, Analytics or related field', 'Preferred Qualifications', 'Exposure to Spark, Hadoop, and other big data technologies is a plus']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Data Engineer - Melbourne,NOVEL,"Melbourne, FL",5 days ago,Be among the first 25 applicants,"['', 'Knowledge of Delta Lake', 'Novel Engineering offers a competitive employment package. This includes competitive compensation, vacation, dental, vision, employee profit sharing plan, flexible schedules, and a professional yet relaxed culture that gives you the opportunity to work in a team-oriented environment, innovate with co-workers, and thrive as an individual.', 'Position is subject to pre-employment drug and random drug and alcohol testing.', 'Fluent and experienced with Linux OS (and some bash scripting knowledge)', ""Bachelor's in computer science computer engineering or other related fields3+ years’ experience in data engineering and software developmentMust Have: Strong programming skills (python) - solid fundamentals of computer science such as data structures, time/space complexity analysisMust Have: Hands-on experience with Apache Spark (Pyspark)Must Have: Hands-on experience with Apache CassandraMust Have: Hands-on experience with Apache KafkaExperience with file formats such as Parquet and AVROKnowledge of Delta LakeFluent and experienced with Linux OS (and some bash scripting knowledge)Previous experience with AWS tools such as S3, EMR, EC2, DynamoDBGood understanding about CRM data (e.g., Salesforce), the objects, and the relationships among them Position is subject to pre-employment drug and random drug and alcohol testing."", 'Job Description', 'Must Have: Strong programming skills (python) - solid fundamentals of computer science such as data structures, time/space complexity analysis', 'Experience with file formats such as Parquet and AVRO', 'Good understanding about CRM data (e.g., Salesforce), the objects, and the relationships among them ', 'Must Have: Hands-on experience with Apache Cassandra', 'Company Overview', 'Must Have: Hands-on experience with Apache Kafka', 'Must Have: Hands-on experience with Apache Spark (Pyspark)', '3+ years’ experience in data engineering and software development', 'Benefits And Perks', ""Bachelor's in computer science computer engineering or other related fields"", 'Previous experience with AWS tools such as S3, EMR, EC2, DynamoDB']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
"Software Engineer, Data Engineering - Data Platform",Foursquare,"Chicago, IL",1 day ago,Be among the first 25 applicants,"['', 'Write test automation, conduct code reviews, and take end-to-end ownership of deployments to production.', 'Build and run Big Data processing pipelines.', 'Focus on performance, throughput, and latency throughout our architecture.', 'Work with the Data Science team to bring machine learning models into production.', 'BS/BA in a technical field such as Computer Science or equivalent experience.', 'Professional experience in at least one of Python, Java, Scala, or Ruby', 'Responsibilities', 'Write, deploy, and monitor services for data access by systems across our infrastructure.', 'About Team', 'Qualifications', ' Comfort with Unix/Linux and the command line. Experience with CI/CD systems such as Jenkins, Travis, TeamCity, and CircleCI. Experience with containerization and orchestration systems like Docker and Kubernetes. Startup experience or experience at marketing or ad-tech data companies: RTB / real-time bidding. DSP / demand-side platform. ', 'Experience with containerization and orchestration systems like Docker and Kubernetes.', 'Nice to haves', 'Experience with CI/CD systems such as Jenkins, Travis, TeamCity, and CircleCI.', ' BS/BA in a technical field such as Computer Science or equivalent experience. 1-4 years of software development experience Professional experience in at least one of Python, Java, Scala, or Ruby Professional experience with at least one of Hadoop MapReduce and/or Spark data processing pipelines, analytics systems (e.g. OLAP, BI tools), or machine learning technologies. Strong algorithms and data structures knowledge. ', 'Startup experience or experience at marketing or ad-tech data companies: RTB / real-time bidding. DSP / demand-side platform.', '1-4 years of software development experience', 'Comfort with Unix/Linux and the command line.', ' Work with the Data Science team to bring machine learning models into production. Build and run Big Data processing pipelines. Write, deploy, and monitor services for data access by systems across our infrastructure. Focus on performance, throughput, and latency throughout our architecture. Write test automation, conduct code reviews, and take end-to-end ownership of deployments to production. ', 'Strong algorithms and data structures knowledge.', 'Professional experience with at least one of Hadoop MapReduce and/or Spark data processing pipelines, analytics systems (e.g. OLAP, BI tools), or machine learning technologies.']",Entry level,Full-time,Engineering,Marketing and Advertising,2021-03-18 14:34:51
Data Engineer III,Expedia Group,"Chicago, IL",5 days ago,37 applicants,"['', 'You deeply understand of SQL, all its facets, and have experience working with traditional (e.g. Teradata) and cloud-based data lakes (e.g. Redshift, Snowflake).', 'Identify performance and data challenges, suggesting code and architecture improvements.', 'Your background:', 'Develop, build scalable and high-performant data enrichment processes, using the right development patterns and continuous deployment/integration practices.Collaborate with upstream system architects, data technical specialists, developers, data product managers, and end-users to support and resolve data issues.Participate in a DevOps environment (you build it, you run it) by managing and operating real-time and batch data solutions at scale.Implement the tools and processes to handle performance, scale, availability, accuracy, and monitoring of data crucial to internal and external partners ensuring that set SLAs are met.Breakdown requirements, simplify architectures and collaborate with other business teams to deliver intelligent data solutions in line with defined objectives and results.Identify performance and data challenges, suggesting code and architecture improvements.Seek opportunities to bring sophisticated analytical techniques and solutions to new data products.Cooperate in an agile environment to lead change and keep us current with the latest technologies.', 'You are fully hands-on coding with solid experience in SQL and in one or more key software languages (e.g. Python, Scala, Java, Javascript).', 'Your Responsibilities', 'Proven background in a variety of data technologies, such as Teradata, Hadoop, Spark, HBase, Hive, Presto and/or ETL frameworks. Experience with Dremio a plus.', 'You have knowledge of dimensional modeling concepts and other efficient data representations for optimizing SQL queries.', 'You develop sophisticated SQL queries while breaking down complexity and identifying modular parts.', 'Seek opportunities to bring sophisticated analytical techniques and solutions to new data products.', 'You are fully hands-on coding with solid experience in SQL and in one or more key software languages (e.g. Python, Scala, Java, Javascript).You deeply understand of SQL, all its facets, and have experience working with traditional (e.g. Teradata) and cloud-based data lakes (e.g. Redshift, Snowflake).You know and use the best fit data technologies and approaches to addressing performance, scalability, governance challenges.You have experience with the AWS cloud ecosystem, such as EMR, Lambda, S3, EC2, Cloud Formation, VPC, etc.You have knowledge of dimensional modeling concepts and other efficient data representations for optimizing SQL queries.You develop sophisticated SQL queries while breaking down complexity and identifying modular parts.Proven background in a variety of data technologies, such as Teradata, Hadoop, Spark, HBase, Hive, Presto and/or ETL frameworks. Experience with Dremio a plus.You have knowledge of cloud infrastructures automation tools such as CloudFormation and Docker.You know about domain and business event patterns, streaming pipelines such as Kafka or Kinesis.Experience producing tested, secure, resilient and well-documented applications.You have excellent interpersonal skills and verbal and written communication skills when working with both business and technical teams.', 'Breakdown requirements, simplify architectures and collaborate with other business teams to deliver intelligent data solutions in line with defined objectives and results.', 'Develop, build scalable and high-performant data enrichment processes, using the right development patterns and continuous deployment/integration practices.', 'You have knowledge of cloud infrastructures automation tools such as CloudFormation and Docker.', 'Cooperate in an agile environment to lead change and keep us current with the latest technologies.', 'Collaborate with upstream system architects, data technical specialists, developers, data product managers, and end-users to support and resolve data issues.', 'Participate in a DevOps environment (you build it, you run it) by managing and operating real-time and batch data solutions at scale.', 'You know about domain and business event patterns, streaming pipelines such as Kafka or Kinesis.', 'About The Position', 'You have excellent interpersonal skills and verbal and written communication skills when working with both business and technical teams.', 'You have experience with the AWS cloud ecosystem, such as EMR, Lambda, S3, EC2, Cloud Formation, VPC, etc.', 'You know and use the best fit data technologies and approaches to addressing performance, scalability, governance challenges.', 'Implement the tools and processes to handle performance, scale, availability, accuracy, and monitoring of data crucial to internal and external partners ensuring that set SLAs are met.', 'Experience producing tested, secure, resilient and well-documented applications.']",Mid-Senior level,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Engineer,Cypress HCM,United States,,N/A,"['', 'Knowledge in multiple scripting languages, Python preferred', 'Experience with AWS cloud services:', 'Bachelor’s Degree in computer science3- 5 years of experience, ideally in a SaaS environmentExperience with large-scale data and query optimization techniques.Experience with ETL & data warehouse systems.Experience with AWS cloud services:EC2, RDS, Redshift, Aurora PostgresStrong in SQL, NoSQL and RDBMS.Knowledge in multiple scripting languages, Python preferredKnowledge of cloud, distributed systems, and stream-processing systems.', 'Troubleshoot and improve the infrastructure required for optimal extraction', 'Responsibilities', '3- 5 years of experience, ideally in a SaaS environment', 'Collect, parse, analyze, and visualize large sets of dataTurn data into insights.', 'Experience with ETL & data warehouse systems.', 'Requirements', 'EC2, RDS, Redshift, Aurora Postgres', 'Implement and support systems that reliably and efficiently provide interactive query performance on large amounts of multi-modal data.', 'Knowledge of cloud, distributed systems, and stream-processing systems.', '\xa0', 'Implement and support systems that reliably and efficiently provide interactive query performance on large amounts of multi-modal data.Work on and scale systems that can scale.Create data tools for analytics and data scientist team members that assist them in building and optimizing our productTroubleshoot and improve the infrastructure required for optimal extractionCollect, parse, analyze, and visualize large sets of dataTurn data into insights.', 'Bachelor’s Degree in computer science', 'Experience with large-scale data and query optimization techniques.', 'Work on and scale systems that can scale.', 'Create data tools for analytics and data scientist team members that assist them in building and optimizing our product', 'Strong in SQL, NoSQL and RDBMS.']",Mid-Senior level,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Engineer,Advantis Global,"Cupertino, CA",4 weeks ago,164 applicants,"['', ' Experience with query APIs', ' 2 years professional experience', ' Extract Transform Load (ETL) experience', ' Experience with Web or REST API development in Python', 'Opportunity For You', ' Experience with workflow scheduling / orchestration such as Kubernetes or Airflow', ' Experience in implementing data pipelines using python, familiar with Pandas and Numpy', ' Data visualization or web development', ' 2 years professional experience Experience in implementing data pipelines using python, familiar with Pandas and Numpy Experience with workflow scheduling / orchestration such as Kubernetes or Airflow Extract Transform Load (ETL) experience using Spark, Kafka, Hadoop Experience with query APIs using JSON, ProtocolBuffers, or XML Experience with Unix-based command line interface and Bash scripts Experience with Postgres database, familiar with SQL scripting Experience with Web or REST API development in Python', ' Pandas Data visualization or web development', 'Key Success Factors', ' using JSON, ProtocolBuffers, or XML', ' Experience with Postgres database, familiar with SQL scripting', ' Pandas', 'About This Opportunity', ' Experience with Unix-based command line interface and Bash scripts', ' using Spark, Kafka, Hadoop', 'Pluses']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
ML/Data Platform Data Engineer,Blue River Technology,"Sunnyvale, CA",1 day ago,Be among the first 25 applicants,"['', ' Experience developing ETL in a microservice architecture', ' Work with product, ML scientists, roboticists, and software engineers to build a data platform that supports development of Intelligent Machines', 'About Us', ' Expertise in data modeling for time series, spatial, and image data for analytic and operational use cases', ' Experienced in data mining and visualization of large data sets', "" Expertise in data modeling for time series, spatial, and image data for analytic and operational use cases Strong Python programmer Experience with a diversity of datastores such as Redshift, Dynamo, Athena, Mongo, Postgres Experience developing ETL in a microservice architecture Data lifecycle management experience Experience with infrastructure as code, such as Terraform Self-motivated, ability to work both independently and in team environments Excellent communicator Bachelor's Degree in Computer Science or related technical subject area"", ' Experience with a diversity of datastores such as Redshift, Dynamo, Athena, Mongo, Postgres', "" Bachelor's Degree in Computer Science or related technical subject area"", ' Promote standard methodologies in data modeling, storage, and processing', ' Experience working with high dimensional data: images, videos, point clouds, etc.', ' Self-motivated, ability to work both independently and in team environments', ' Experience crafting data systems to support machine learning and robotics applications', ' Help enable our users to find their data! Develop best practices for data access and queries.', ' Experience with infrastructure as code, such as Terraform', 'Preferred Skills & Experience', ' Design and build updates to our data solutions supporting robotics and machine learning development cycle', ' Develop and architect enhanced systems to enable rapid retrieval of imagery and time series data', 'Role Responsibilities', 'Required Professional Skills & Experience', ' Design and build updates to our data solutions supporting robotics and machine learning development cycle Develop and architect enhanced systems to enable rapid retrieval of imagery and time series data Promote standard methodologies in data modeling, storage, and processing Assess, benchmark and select new technologies to be added to the digital product portfolio. Help enable our users to find their data! Develop best practices for data access and queries. Work with product, ML scientists, roboticists, and software engineers to build a data platform that supports development of Intelligent Machines', ' Experience developing on Kubernetes based systems', ' Excellent communicator', ' Strong Python programmer', ' Data lifecycle management experience', ' Assess, benchmark and select new technologies to be added to the digital product portfolio.', ' Experience working with high dimensional data: images, videos, point clouds, etc. Experience crafting data systems to support machine learning and robotics applications Experienced in data mining and visualization of large data sets Experience developing on Kubernetes based systems']",Entry level,Full-time,Engineering,Information Technology and Services,2021-03-18 14:34:51
Data Solutions Engineer,Epsilon,"Chicago, IL",4 weeks ago,137 applicants,"['', 'Ability to troubleshoot production issues and solve for performance bottlenecks', 'You enjoy working with numerous programming languages, relational databases, and distributed systems. Our platform is ever evolving, but currently is a combination of Kafka, Flume, Spark, Scala, Java, Python, NoSQL (HBase, Cassandra and ScyllaDB), MPP RDBMS, Postgres, Hadoop, AWS, AirFlow, Docker, Kubernetes and Elastic', 'Build and maintain data quality services with PythonFluent SQL with ability to ingest complex use cases, refactor and ask questionsContinuous improvement of our system, tests, and data quality indicatorsInfluence our technical decisionsKeep yourself informed and up-to-date with technologiesInterface with analysts, data scientists, and engineers to enable data oriented solutionsBuild data expertise on subject matter and be able to speak to data warehouse constructs and data architectureAbility to troubleshoot production issues and solve for performance bottlenecksExpert level skills in Python or other language (JAVA or Scala)Ability to analyze data and identify business possibilities for better operational processes and business opportunitiesExcellent communication skills and ability to work with the internal analyst communityAbility to thrive in a collaborative team environmentYou enjoy working with numerous programming languages, relational databases, and distributed systems. Our platform is ever evolving, but currently is a combination of Kafka, Flume, Spark, Scala, Java, Python, NoSQL (HBase, Cassandra and ScyllaDB), MPP RDBMS, Postgres, Hadoop, AWS, AirFlow, Docker, Kubernetes and ElasticInternet/Digital Advertising ecosystem knowledge is a plusSpark and ML are a plus.', 'Continuous improvement of our system, tests, and data quality indicators', 'Influence our technical decisions', 'Build and maintain data quality services with Python', 'Spark and ML are a plus.', 'Key Duties, Tasks And Responsibilities', 'Expert level skills in Python or other language (JAVA or Scala)', 'Fluent SQL with ability to ingest complex use cases, refactor and ask questions', 'Keep yourself informed and up-to-date with technologies', 'Ability to thrive in a collaborative team environment', 'Company Description', 'Interface with analysts, data scientists, and engineers to enable data oriented solutions', 'Great People, Deserve Great Benefits', 'Build data expertise on subject matter and be able to speak to data warehouse constructs and data architecture', 'About this role', 'Internet/Digital Advertising ecosystem knowledge is a plus', 'Ability to analyze data and identify business possibilities for better operational processes and business opportunities', 'Job Description', 'Excellent communication skills and ability to work with the internal analyst community']",Not Applicable,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
AWS Data Engineer,Rivian,"Palo Alto, CA",2 weeks ago,32 applicants,"['', 'Strong understanding of all Big Data and data warehousing services offered by AWS.', 'Design, implement and support an analytical data infrastructure providing access to large datasets and computing power.', 'Strong hands-on experience in one or more programming languages like Java, Python, Scala', ' Design, implement and support an analytical data infrastructure providing access to large datasets and computing power. Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using API, SQLS, Change Data Capture Tools and AWS big data technologies. Continuous research of the latest big data and visualization technologies to provide new capabilities and increase efficiency Creation and support of real-time data pipelines built on AWS technologies including EMR, Glue, Kinesis, Redshift/Spectrum and Athena Help continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for business. Collaborate with other tech teams to implement advanced analytics algorithms that exploit our rich datasets for statistical analysis, prediction, clustering and machine learning ', 'Degree/Diploma in computer science, engineering, mathematics, or a related technical discipline preferred', 'Creation and support of real-time data pipelines built on AWS technologies including EMR, Glue, Kinesis, Redshift/Spectrum and Athena', 'Hands-on experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets using AWS S3, Glue, Kinesis, Kafka, SQS, Change data capture tools, Spark', 'Self-starter with excellent communication skills', 'Responsibilities', '7-10 years of industry experience in software development, data architecture, data engineering, business intelligence, data science with a track record of manipulating, processing, and extracting value from large datasets', 'Collaborate with other tech teams to implement advanced analytics algorithms that exploit our rich datasets for statistical analysis, prediction, clustering and machine learning', 'Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using API, SQLS, Change Data Capture Tools and AWS big data technologies.', 'Experience in a query framework for business users and data scientists using Athena, APIs and spinning data science clusters.', ' Self-starter with excellent communication skills 7-10 years of industry experience in software development, data architecture, data engineering, business intelligence, data science with a track record of manipulating, processing, and extracting value from large datasets Strong understanding of all Big Data and data warehousing services offered by AWS. Hands-on experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets using AWS S3, Glue, Kinesis, Kafka, SQS, Change data capture tools, Spark Experience in a query framework for business users and data scientists using Athena, APIs and spinning data science clusters. Strong data base experience in both Relational, Columnar, NOSQL & Timeseries database like Redshift, DynamoDB, MongoDB, SQL Server, Druid etc. Strong hands-on experience in one or more programming languages like Java, Python, Scala Demonstrated strength in data modeling, ETL development, and data warehousing Good knowledge of statistical models and data mining algorithms Experience using analytics & reporting tools like Tableau, Power BI, Qlikview etc. Understanding of business domains like Finance, Supply Chain, Manufacturing is a plus Degree/Diploma in computer science, engineering, mathematics, or a related technical discipline preferred ', 'Understanding of business domains like Finance, Supply Chain, Manufacturing is a plus', 'Experience using analytics & reporting tools like Tableau, Power BI, Qlikview etc.', 'Qualifications', 'Strong data base experience in both Relational, Columnar, NOSQL & Timeseries database like Redshift, DynamoDB, MongoDB, SQL Server, Druid etc.', 'Good knowledge of statistical models and data mining algorithms', 'Demonstrated strength in data modeling, ETL development, and data warehousing', 'Help continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for business.', 'Continuous research of the latest big data and visualization technologies to provide new capabilities and increase efficiency']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
"Data Engineer- Buffalo, NY",IBM,"Buffalo, NY",1 week ago,Be among the first 25 applicants,"['', 'Work with IBM InfoSphere MDM (RDM and custom domain hub)Integrate with source systems and downstream applicationsAnalyze and define detailed MDM processes, tasks data flows and dependencies', 'Knowledge of ERWin, MDM, and/or ETL ', 'Knowledge and experience with SQL', 'At Our Core, We Are Committed To Believing And Investing In Our Workforce Through', 'http://www.ibm.com/ibm/responsibility/initiatives.html', 'http://www.ibm.com/ibm/responsibility/initiatives.htmlhttp://www.ibm.com/ibm/responsibility/corporateservicecorps', 'CORPORATE CITIZENSHIP', 'Demonstrated leadership experience and ability to adapt, with willingness to readily take ownership of tasks and problems, which often extend beyond initial scope of responsibility', 'Thorough and analytical, with capability to apply logic to solve problems', 'About IBM', 'Initiative to actively seek new knowledge and improve skills', 'Skill development:', 'Benefits', ""Creating ERD's"", ' or be willing to relocate to, Buffalo, NY.', 'Analyze and define detailed MDM processes, tasks data flows and dependencies', 'Demonstrated ability to evaluate clients’ needs and develop solutions to address those needs', 'Ability to thrive in an ever changing, technology based consulting environmentAbility to translate business solutions into technical requirementsDemonstrated leadership experience and ability to adapt, with willingness to readily take ownership of tasks and problems, which often extend beyond initial scope of responsibilityThorough and analytical, with capability to apply logic to solve problemsAbility to handle multiple tasks concurrently and meet deadlines, while maintaining focus in an environment with conflicting demandsDrive to overcome the most challenging or difficult obstacles and look for ways to improve resultsInitiative to actively seek new knowledge and improve skillsStrong interpersonal skills with ability to collaborate and work effectively with individuals, strengthening relationships to achieve win-win solutionsAbility to communicate complex situations clearly and simply by listening actively and conveying difficult messages in a positive mannerA passion for innovative ideas, coupled with the ability to understand and assimilate different points of viewExperience in database and database structuresDemonstrated ability to evaluate clients’ needs and develop solutions to address those needs', 'CAREER GROWTH ', 'Integrate with source systems and downstream applications', 'Experience in database and database structures', 'Ability to translate business solutions into technical requirements', 'Ability to communicate complex situations clearly and simply by listening actively and conveying difficult messages in a positive manner', 'Other Qualities Of The Data Engineer Include', 'http://www.ibm.com/ibm/responsibility/corporateservicecorps', 'Work with IBM InfoSphere MDM (RDM and custom domain hub)', 'Finding the dream job at IBM:', 'Logical and physical data modeling', 'Drive to overcome the most challenging or difficult obstacles and look for ways to improve results', 'As a Data Engineer You Will Be Expected To', ""Logical and physical data modelingCreating ERD'sKnowledge and experience with SQLKnowledge of ERWin, MDM, and/or ETL Defining and analyzing data requirements to support the business processesExperience in database and database structures"", 'Diversity of people:', ""Master's Degree"", 'Ability to thrive in an ever changing, technology based consulting environment', 'Ability to handle multiple tasks concurrently and meet deadlines, while maintaining focus in an environment with conflicting demands', 'A passion for innovative ideas, coupled with the ability to understand and assimilate different points of view', 'Strong interpersonal skills with ability to collaborate and work effectively with individuals, strengthening relationships to achieve win-win solutions', 'About Business Unit', 'Preferred Technical And Professional Expertise', 'Defining and analyzing data requirements to support the business processes']",Not Applicable,Full-time,Other,Computer Hardware,2021-03-18 14:34:51
Associate Data Engineer,State Auto Insurance,"Columbus, OH",1 week ago,Be among the first 25 applicants,"['', ' Manage the source code in GitHub ', ' Expertise in using IDEs and Tools like Eclipse, GitHub, Jenkins, Maven and IntelliJ ', ' Proficient in executing Hive queries using Hive cli, Web GUI Hue and Impala to read, write and query the data ', ""We're committed to bringing passion and customer focus to the business."", ' Develop scheduling and monitoring Oozie workflows for parallel execution of jobs ', ' Strong hands-on experience in Spark, Scala, R, Python, and/or Java. ', ' Expertise in various scripting languages like Linux/Unix shell scripts and Python ', ' Worker Sub-Type ', ""It's fun to work in a company where people truly BELIEVE in what they're doing!"", ' Bachelor’s Degree/Master degree in Computer Science, Computer Engineering, Programming, Management Information Systems, or related field. Insurance industry experience is a plus.  Minimum of 2 years of prior Data engineer experience.  Strong hands-on experience in Spark, Scala, R, Python, and/or Java.  Programming experience with the Hadoop ecosystem of applications and functional understanding of distributed data processing systems architecture (Data Lake / Big Data /Hadoop/ Spark / HIVE, etc).  Amazon Big Data ecosystem (EMR, Kinesis, Aurora) experience is a plus. ', ' Harmonize, transform, and move data from a raw format to consumable and curated views ', ' Minimum of 2 years of prior Data engineer experience. ', ' Expert in Spark SQL and Spark DataFrames using Scala for Distributed Data Processing ', ' Experience in working with cloud environment AWS EMR, EC2, S3 and Athena and GCP BigQuery ', ' Programming experience with the Hadoop ecosystem of applications and functional understanding of distributed data processing systems architecture (Data Lake / Big Data /Hadoop/ Spark / HIVE, etc). ', 'Worker Sub-Type', ' Diverse experience in working with variety of Database like SQL Server, MySql, IBM DB2 etc… ', ' Model, design, develop, code, test, debug, document and deploy application to production through standard processes ', ' Bachelor’s Degree/Master degree in Computer Science, Computer Engineering, Programming, Management Information Systems, or related field. Insurance industry experience is a plus. ', ' Develop DataFrame and RDD (Resilient Distributed Datasets) to achieve unified transformations on the data load ', ' Build distributed, scalable, and reliable data pipelines that ingest and process data at scale and in real-time ', 'Full Time', 'Communication And Collaboration Skills', ' Experience in working on Apache Hadoop ecosystem components like Map-Reduce, Hive, SQOOP, Spark, and Oozie with AWS EC2 cloud computing ', ' Optimize the Spark application to improve performance and reduced time on the Hadoop cluster ', 'Written', ' Track and delivery requirements in Jira ', ' Create metrics and apply business logic using Spark, Scala, R, Python, and/or Java ', ' Amazon Big Data ecosystem (EMR, Kinesis, Aurora) experience is a plus. ', 'Problem Solving:', 'Summary & Key Responsibilities', 'Oral:', 'Full Time / Part Time', 'Key Responsibilities', ' Apply all Phases of Software Development Life Cycle (Analysis, Design, Development, Testing and Maintenance) using Waterfall and Agile methodologies ', ' / Part Time', ' Transfer data from different platform’s into AWS platform ', ""If you like wild growth and working with happy, enthusiastic over-achievers, you'll enjoy your career with us!"", ' Apply strong Data Governance principles, standards, and frameworks to promote data consistency and quality while effectively managing and protecting the integrity of corporate data ', ' Apply all Phases of Software Development Life Cycle (Analysis, Design, Development, Testing and Maintenance) using Waterfall and Agile methodologies  Experience in working on Apache Hadoop ecosystem components like Map-Reduce, Hive, SQOOP, Spark, and Oozie with AWS EC2 cloud computing  Expert in Spark SQL and Spark DataFrames using Scala for Distributed Data Processing  Develop DataFrame and RDD (Resilient Distributed Datasets) to achieve unified transformations on the data load  Expertise in various scripting languages like Linux/Unix shell scripts and Python  Develop scheduling and monitoring Oozie workflows for parallel execution of jobs  Experience in working with cloud environment AWS EMR, EC2, S3 and Athena and GCP BigQuery  Transfer data from different platform’s into AWS platform  Diverse experience in working with variety of Database like SQL Server, MySql, IBM DB2 etc…  Manage the source code in GitHub  Track and delivery requirements in Jira  Expertise in using IDEs and Tools like Eclipse, GitHub, Jenkins, Maven and IntelliJ  Optimize the Spark application to improve performance and reduced time on the Hadoop cluster  Proficient in executing Hive queries using Hive cli, Web GUI Hue and Impala to read, write and query the data  Build distributed, scalable, and reliable data pipelines that ingest and process data at scale and in real-time  Create metrics and apply business logic using Spark, Scala, R, Python, and/or Java  Model, design, develop, code, test, debug, document and deploy application to production through standard processes  Harmonize, transform, and move data from a raw format to consumable and curated views  Apply strong Data Governance principles, standards, and frameworks to promote data consistency and quality while effectively managing and protecting the integrity of corporate data ', 'Minimum Experience/Education']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer ,The Adecco Group,"Jacksonville, FL",2 weeks ago,71 applicants,"['Participating in special projects and performs other duties as assigned.', 'KNOWLEDGE, SKILLS & ABILITIES REQUIREMENTS:', 'Knowledge of building and optimizing data pipelines, architectures and data sets.', 'We are the workforce experts delivering staffing and career service solutions to organizations and individuals across all industries. Collectively we harness the power of some of the greatest talent in the world. That talent and expertise allows us to do business globally and act locally with deep knowledge in niche areas.\xa0', 'The Company will consider for employment qualified applicants with arrest and conviction records.', 'Bachelor’s degree in Computer Science, Statistics, Informatics, Information Systems or a related field plus two (2) years minimum experience in a Data Engineering or similar business intelligence, data analytics or supply chain management role required.MBA or Graduate degree in Computer Science, Statistics, Informatics, Information Systems or a related field in lieu of experience accepted.', 'Keeps our data separated and secure across national boundaries through multiple data centers and regions.', 'Ability to perform root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.', 'Implement analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.', 'Working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.', 'Creates and maintains optimal data pipeline architecture.', 'A successful history of manipulating, processing and extracting value from large disconnected datasets.', 'Strong analytic skills related to working with unstructured datasets.', 'Strong project management and organizational skills.', 'Operates infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and relevant Azure technologies like Data Factory, Databricks, and similar.', 'Responsibilities', 'Build processes supporting data transformation, data structures, metadata, dependency and workload management.', 'Working knowledge of message queuing, stream processing, and highly scalable data stores.', 'Implements internal process improvements, automates manual processes, optimizes data delivery and re-designs infrastructure for greater scalability, etc.', 'Assembles large, complex data sets that meet functional and non-functional business requirements.', 'COMPANY OVERVIEW:\xa0', 'Qualifications', 'MBA or Graduate degree in Computer Science, Statistics, Informatics, Information Systems or a related field in lieu of experience accepted.', 'The Data Engineer is responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. Experienced data pipeline builder and data wrangler that optimizes data systems and building them from the ground up. Supports our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects.', 'Builds data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.', 'Works with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.', 'ESSENTIAL DUTIES & RESPONSIBILITIES:', 'Ability to support and work with cross-functional teams in a dynamic environment.', 'Adecco Group North America, through an impressive portfolio of staffing industry leading brands including Accounting Principals, Adecco General Staffing, Adia, Ajilon, Entegee, Lee Hecht Harrison, Modis, Paladin, Parker+Lynch, Pontoon, Special Counsel and Soliant is the world’s leading provider of Human Resources solutions.\xa0', 'Works with data and analytics experts to strive for greater functionality in our data systems.', 'Every day, we have more than 100,000 associates on assignment, 30,000 colleagues working internally to support more than 10,000 clients in the United States and Canada. Ensuring our business units are prepared to deliver outstanding service to our associates and clients, the Adecco Group North America team provides a strong infrastructure through our corporate and shared services teams.\xa0', '\xa0', 'SUMMARY:', 'Working knowledge of Scala, Python or R.', 'Creates and maintains optimal data pipeline architecture.Assembles large, complex data sets that meet functional and non-functional business requirements.Implements internal process improvements, automates manual processes, optimizes data delivery and re-designs infrastructure for greater scalability, etc.Operates infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and relevant Azure technologies like Data Factory, Databricks, and similar.Implement analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.Works with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.Keeps our data separated and secure across national boundaries through multiple data centers and regions.Builds data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.Works with data and analytics experts to strive for greater functionality in our data systems.Participating in special projects and performs other duties as assigned.', 'MINIMUM EDUCATION & EXPERIENCE REQUIREMENTS:', 'Equal Opportunity Employer Minorities/Women/Veterans/Disabled', 'Bachelor’s degree in Computer Science, Statistics, Informatics, Information Systems or a related field plus two (2) years minimum experience in a Data Engineering or similar business intelligence, data analytics or supply chain management role required.', 'Working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.Working knowledge of Scala, Python or R.Knowledge of building and optimizing data pipelines, architectures and data sets.Ability to perform root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.Strong analytic skills related to working with unstructured datasets.Build processes supporting data transformation, data structures, metadata, dependency and workload management.A successful history of manipulating, processing and extracting value from large disconnected datasets.Working knowledge of message queuing, stream processing, and highly scalable data stores.Strong project management and organizational skills.Ability to support and work with cross-functional teams in a dynamic environment.']",Entry level,Full-time,Information Technology,Information Services,2021-03-18 14:34:51
Junior Data Engineer,NFI,"Chicago, IL",1 week ago,32 applicants,"['', 'Implement the domain model in Elixir services being careful to avoid unnecessary coupling', 'Data Technologies', 'Essential Duties & Responsibilities', ' 2+ years of professional software development experience (or comparable) Modern RDBMS experience mandatory Elixir and ecto_sql experience strongly preferred Familiarity with DDD principles is a huge plus R or python experience a plus Comfortable using a Mac and working in a Linux environment Experience using Git ', 'Design', 'Document the domain model as it evolves', 'Elixir and ecto_sql experience strongly preferred', 'Comfortable using a Mac and working in a Linux environment', '2+ years of professional software development experience (or comparable)', 'Requirements', ' Collaborate with legacy system stakeholders, domain experts, product managers, and other developers to manipulate existing data sources into purpose-driven models Collaborate with domain experts, product managers, and other developers to refine a data model following DDD principles Document the domain model as it evolves Implement the domain model in Elixir services being careful to avoid unnecessary coupling Implement unit tests and integration tests Integrate with 3rd party APIs Take ownership of the production release and operation of your services ', 'Product', 'Modern RDBMS experience mandatory', 'Take ownership of the production release and operation of your services', 'Experience using Git', 'Implement unit tests and integration tests', 'Familiarity with DDD principles is a huge plus', 'R or python experience a plus', 'Collaborate with legacy system stakeholders, domain experts, product managers, and other developers to manipulate existing data sources into purpose-driven models', 'Integrate with 3rd party APIs', 'Collaborate with domain experts, product managers, and other developers to refine a data model following DDD principles']",Associate,Full-time,Information Technology,Construction,2021-03-18 14:34:51
Data Engineer III,GHX,"Louisville, CO",3 days ago,Be among the first 25 applicants,"['', 'Strong demonstrable SQL and Python skills', 'RDS', 'Develop and maintain data engineering solutions for the enterprise data platformAnalyze business requirements and work with teammates to formulate supporting design and design documentationOther duties as assigned', 'GHX is a healthcare business and data automation company, empowering healthcare organizations to enable better patient care and maximize industry savings using our world class cloud-based supply chain technology exchange platform, solutions, analytics and services. We bring together healthcare providers and manufacturers and distributors in North America and Europe &mdash; who rely on smart, secure healthcare-focused technology and comprehensive data to automate their business processes and make more informed decisions.It is our passion and vision for a more operationally efficient healthcare supply chain, helping organizations reduce - not shift - the cost of doing business, paving the way to delivering patient care more effectively. Together we take more than a billion dollars out of the cost of delivering healthcare every year. GHX is privately owned, operates in the United States, Canada and Europe, and employs more than 800 people worldwide. Our corporate headquarters is in Europe, Louisville, Colorado, just outside of Denver, with additional offices in Europe, Chicago, Illinois, Atlanta, Georgia and Omaha, Nebraska.', 'Analyze business requirements and work with teammates to formulate supporting design and design documentation', 'Demonstrated organizational, prioritization, and time management skills', 'Glue', 'Ability to think strategically', '5+ years working in an agile development environment', 'Lead and contribute to backend and ETL development effort of our data platform.', 'High-level written and verbal communication skills', 'Step Functions', '5+ years of data engineering experience building business intelligence applications with exceptional SQL, PL/SQL, and/or Python skills', 'GHX expressly prohibits any form of unlawful employee harassment based on race, color, religion, gender, sexual orientation, national origin, age, disability, or veteran status. Improper interference with the ability of GHX’s employees to perform their expected job duties is absolutely not tolerated.', 'Ability to communicate technical concepts and designs to cross-functional and offshore teams who have varying levels of technical experience', 'Promote collaboration through activities including design sessions, design reviews, and pair programming, etcetera', 'Application, system or data architecture experience', 'Ability and willingness to travel nationally to remote offices and partners approximately 10% of the time', 'Lead and contribute to backend and ETL development effort of our data platform.Lead and provide hands-on new development as well as enhancement of existing data processesDesign, maintain, and tune extraction, transformation, and load (ETL) processes using PL/SQL, SQL, Python, or SparkProvide architectural guidance and development/build standards for the teamPromote collaboration through activities including design sessions, design reviews, and pair programming, etcetera', 'Other duties as assigned', 'Application, system or data architecture experienceMachine learning experience', 'Technical writing experience in relevant areas, including queries, reports, and presentations', 'Provide architectural guidance and development/build standards for the team', 'Ability to adapt to changing conditions and lead others through change', 'Lambda', 'EC2', 'Experience developing in Snowflake', 'GHX provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, gender, sexual orientation, national origin, age, disability, marital status, amnesty, or status as a covered veteran in accordance with applicable federal, state and local laws. GHX complies with applicable state and local laws governing non-discrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment, including, but not limited to, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training.', 'Aurora MySQL', 'Global Healthcare Exchange (GHX) enables better patient care and billions in savings for the healthcare community by maximizing automation, efficiency and accuracy of business processes.', 'Thorough understanding of, and support for, Agile development methodologies', 'Bachelor’s degree in Computer Science, Mathematics, or Statistics5+ years of data engineering experience building business intelligence applications with exceptional SQL, PL/SQL, and/or Python skills5+ years of experience of ETL development in a big data environment5+ years working in an agile development environmentTechnical writing experience in relevant areas, including queries, reports, and presentations', 'Ability to design, collect, and analyze large datasets', 'Design, maintain, and tune extraction, transformation, and load (ETL) processes using PL/SQL, SQL, Python, or Spark', 'Bachelor’s degree in Computer Science, Mathematics, or Statistics', 'Lead and provide hands-on new development as well as enhancement of existing data processes', 'Athena', 'Disclaimer:', '5+ years of experience of ETL development in a big data environment', 'Experience in a diverse set of Amazon Web Services data services including:EC2S3AthenaRedshiftAurora MySQLRDSLambdaStep FunctionsGlueDevelopment experience with Python, PySpark, or RExperience developing in Snowflake', 'Machine learning experience', 'Develop and maintain data engineering solutions for the enterprise data platform', 'Attention to detail', 'Proven data engineering, problem solving, and analysis skills', 'Analytical and problem-solving ability and orientation', 'Development experience with Python, PySpark, or R', '\xa0', 'S3', 'GHX: It’s the way you do business in healthcare', 'Experience in a diverse set of Amazon Web Services data services including:', 'Thorough understanding of, and support for, Agile development methodologiesAbility to design, collect, and analyze large datasetsAbility to communicate technical concepts and designs to cross-functional and offshore teams who have varying levels of technical experienceProven data engineering, problem solving, and analysis skillsStrong demonstrable SQL and Python skillsHigh-level written and verbal communication skillsAbility to think strategicallyAbility to adapt to changing conditions and lead others through changeAnalytical and problem-solving ability and orientationDemonstrated organizational, prioritization, and time management skillsAttention to detailAbility and willingness to travel nationally to remote offices and partners approximately 10% of the time', 'Redshift', 'EC2S3AthenaRedshiftAurora MySQLRDSLambdaStep FunctionsGlue', 'The Data Engineering is responsible for the development and execution of data solutions that support product and technology initiatives including general application development activities, such as unit testing, code review, code deployment and technical documentation. This role also collaborates with Product and Engineering teams to design solutions and enable new data capabilities.']",Mid-Senior level,Full-time,Information Technology,Hospital & Health Care,2021-03-18 14:34:51
Sr Data Engineer,Ledgent Technology,"Agoura Hills, CA",1 day ago,Be among the first 25 applicants,"['', 'Requirements']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
"Data Engineer, IT Applications",American Airlines,"Fort Worth, TX",1 month ago,Be among the first 25 applicants,"['', 'Supports the development of coding standards and adheres to best practices and security guidelines', 'Data Engineering ', 'Creates detailed project specifications, requirements, and estimates', 'Demonstrated initiative, flexibility, and ability to adapt to changing priorities and work environments', ' This position is a member of the Information Technology Team, within the Data Engineering & Business Analytics group of Revenue Management/Network Planning. This role is responsible for the development and delivery of data solutions to support the analytical needs driven by the Revenue Management and Network Planning communities.', 'Airline Industry experience', 'Ability to work on multiple projects simultaneously with a keen desire to learn and expand depth of knowledge', 'Additional Benefits: Other great benefits include our Employee Assistance Program, pet insurance and discounts on hotels, cars, cruises and more', 'Bigdata Technologies such as Hadoop, HDFS, Hive etc., and related Software development.', 'Troubleshoots and debugs complex issues; identifies and implements solutions', "" Master's degree in Computer Science, Computer Engineering, Technology, Information Systems (CIS/MIS), Engineering or related technical discipline, or equivalent experience/training5 years of full Software Development Life Cycle (SDLC) experience using ITIL, Agile, XP, or similar methodologiesAirline Industry experience"", 'Minimum Qualifications- Education & Prior Job Experience', 'Build/deployment tools: Maven, Git, Junit, Jenkins', '401(k) Program: Available upon hire and, depending on the workgroup, employer contributions to your 401(k) program are available after one year.', 'Data Engineering Python or Scala.Bigdata Technologies such as Hadoop, HDFS, Hive etc., and related Software development.Apache Spark and Spark SQLIBM Cloud technologies such as Watson Studio, IBM Analytics Engine, IBM Cloud Object Storage Microsoft Azure Technologies such as ADB, ADLS, ADO and ADFBuild/deployment tools: Maven, Git, Junit, JenkinsOther: Linux/Unix shell Scripting and SQL.', "" Bachelor's degree in Computer Science, Computer Engineering, Technology, Information Systems (CIS/MIS), Engineering or related technical discipline, or equivalent experience/training3 years of full Software Development Life Cycle (SDLC) experience, using ITIL, Agile, XP, or similar methodologies"", 'Travel Perks: Ready to explore the world? You, your family and your friends can reach 365 destinations on more than 6,800 daily flights across our global network.', ' Collaborates with leaders, business analysts, project managers, IT architects, technical leads and other developers, along with internal customers, to understand requirements and develop needs according to business requirements', 'Proficiency in Java and/or R is a plus', 'Responsible for leveraging cutting edge technology to solve business problems at American Airlines by participating in all phases of the development process from inception through transition, advocating the agile process and test-driven development, using object-oriented development tools to analyze, model, design, construct and test reusable objects, and making the codebase a better place to live and work.', '3 years of full Software Development Life Cycle (SDLC) experience, using ITIL, Agile, XP, or similar methodologies', 'Python or Scala.', 'Proficiency and demonstrated experience in the following technologies:Data Engineering Python or Scala.Bigdata Technologies such as Hadoop, HDFS, Hive etc., and related Software development.Apache Spark and Spark SQLIBM Cloud technologies such as Watson Studio, IBM Analytics Engine, IBM Cloud Object Storage Microsoft Azure Technologies such as ADB, ADLS, ADO and ADFBuild/deployment tools: Maven, Git, Junit, JenkinsOther: Linux/Unix shell Scripting and SQL.Proficiency in object-oriented analysis, design techniques, principles, and frameworksDemonstrated initiative, flexibility, and ability to adapt to changing priorities and work environmentsAbility to work on multiple projects simultaneously with a keen desire to learn and expand depth of knowledgeAbility to thrive in a sense-of-urgency environment and leverage best practicesAbility to analyze complex problems and implement solutions and/or workaroundsExperience in Agile project management methodologiesExperience in DevOps Toolchain methodologies, including Test Driven Development (TDD), Continuous Integration, and Continuous DeploymentProficiency in Microsoft Suite (Word, Excel, PowerPoint, and Visio)Proficiency in Java and/or R is a plus', 'Travel Perks: Ready to explore the world? You, your family and your friends can reach 365 destinations on more than 6,800 daily flights across our global network.Health Benefits: On day one, you’ll have access to your health, dental, prescription and vision benefits to help you stay well. And that’s just the start, we also offer virtual doctor visits, flexible spending accounts and more. Wellness Programs: We want you to be the best version of yourself - that’s why our wellness programs provide you with all the right tools, resources and support you need.401(k) Program: Available upon hire and, depending on the workgroup, employer contributions to your 401(k) program are available after one year.Additional Benefits: Other great benefits include our Employee Assistance Program, pet insurance and discounts on hotels, cars, cruises and more', 'Researches and implements new technologies to enhance current processes, security, and performance', 'Health Benefits: On day one, you’ll have access to your health, dental, prescription and vision benefits to help you stay well. And that’s just the start, we also offer virtual doctor visits, flexible spending accounts and more. ', "" Master's degree in Computer Science, Computer Engineering, Technology, Information Systems (CIS/MIS), Engineering or related technical discipline, or equivalent experience/training"", ' This position is a member of the Information Technology Team, within the Data Engineering & Business Analytics group of Revenue Management/Network Planning. This role is responsible for the development and delivery of data solutions to support the analytical needs driven by the Revenue Management and Network Planning communities.Responsible for leveraging cutting edge technology to solve business problems at American Airlines by participating in all phases of the development process from inception through transition, advocating the agile process and test-driven development, using object-oriented development tools to analyze, model, design, construct and test reusable objects, and making the codebase a better place to live and work.', '5 years of full Software Development Life Cycle (SDLC) experience using ITIL, Agile, XP, or similar methodologies', 'Maintains and enhances existing enterprise services, applications, and platforms using domain driven design and test-driven development', 'Feel Free to be yourself at American', ""What You'll Get"", ""All you'll need for success"", 'Intro', "" Bachelor's degree in Computer Science, Computer Engineering, Technology, Information Systems (CIS/MIS), Engineering or related technical discipline, or equivalent experience/training"", 'Experience in DevOps Toolchain methodologies, including Test Driven Development (TDD), Continuous Integration, and Continuous Deployment', 'Proficiency in object-oriented analysis, design techniques, principles, and frameworks', ""Why you'll love this job"", ' Collaborates with leaders, business analysts, project managers, IT architects, technical leads and other developers, along with internal customers, to understand requirements and develop needs according to business requirementsMaintains and enhances existing enterprise services, applications, and platforms using domain driven design and test-driven developmentTroubleshoots and debugs complex issues; identifies and implements solutionsCreates detailed project specifications, requirements, and estimatesResearches and implements new technologies to enhance current processes, security, and performanceSupports the development of coding standards and adheres to best practices and security guidelinesWorks closely with software architects and technical leads to ensure decisions meet long-term enterprise growth needsSupporting applications as necessary, including on-call support', 'Preferred Qualifications- Education & Prior Job Experience', 'Ability to thrive in a sense-of-urgency environment and leverage best practices', 'Proficiency and demonstrated experience in the following technologies:Data Engineering Python or Scala.Bigdata Technologies such as Hadoop, HDFS, Hive etc., and related Software development.Apache Spark and Spark SQLIBM Cloud technologies such as Watson Studio, IBM Analytics Engine, IBM Cloud Object Storage Microsoft Azure Technologies such as ADB, ADLS, ADO and ADFBuild/deployment tools: Maven, Git, Junit, JenkinsOther: Linux/Unix shell Scripting and SQL.', 'Microsoft Azure Technologies such as ADB, ADLS, ADO and ADF', 'Ability to analyze complex problems and implement solutions and/or workarounds', 'Other: Linux/Unix shell Scripting and SQL.', 'Skills, Licenses & Certifications', ""What You'll Do"", 'Experience in Agile project management methodologies', 'Supporting applications as necessary, including on-call support', 'Apache Spark and Spark SQL', 'Works closely with software architects and technical leads to ensure decisions meet long-term enterprise growth needs', 'IBM Cloud technologies such as Watson Studio, IBM Analytics Engine, IBM Cloud Object Storage ', 'Proficiency in Microsoft Suite (Word, Excel, PowerPoint, and Visio)', 'Wellness Programs: We want you to be the best version of yourself - that’s why our wellness programs provide you with all the right tools, resources and support you need.']",Not Applicable,Full-time,Information Technology,Airlines/Aviation,2021-03-18 14:34:51
Data Engineer,United Talent Agency,"New York, NY",3 days ago,51 applicants,"['', ' Experience in distributed systems design and architecture', 'What You’ll Get', 'The unique and exciting opportunity to work at one of the leading global entertainment companies.The opportunity to innovate and do the best work of your career as part of collaborative and cross-functional teamAccess to the tools, leadership and resources you’ll need to create and drive a center of excellenceCompetitive benefits and programs to support your well-being', ' Passionate about scaling software', ' Create, improve, and update systems to improve scalability performance and capacity', ' Be a resource and guide to less experienced staff or to those with other specialties', ' Support full software development lifecycle', 'What You’ll Do', ' Comfortable working in a fast-paced, continuous delivery environment', 'The unique and exciting opportunity to work at one of the leading global entertainment companies.', ' Minimum 3+ years of professional work experience', ' Experience participating in cross-functional development teams', ' Familiarity with Serverless Architectures', ' BS in Computer Science or related field strongly preferred', 'Access to the tools, leadership and resources you’ll need to create and drive a center of excellence', 'The opportunity to innovate and do the best work of your career as part of collaborative and cross-functional team', 'About UTA', ' Ability to articulate ideas to non-technical audience', 'What You Need', ' Good knowledge of C#, Java, or C++ and OOP principles', ' Work with team to establish system performance metrics to drive development priorities', ' 2 years relevant experience with RESTful service development', ' 2 years of Linux experience', ' Knowledge of existing cloud provider solutions (AWS or Azure).', ' Experience in troubleshooting and tuning systems', 'Competitive benefits and programs to support your well-being']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Data Engineer,W. R. Berkley Corporation,"Manassas, VA",3 weeks ago,36 applicants,"['', ' Assemble large, complex data sets that meet functional/non-functional business requirements. ', ' Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics. ', 'What We’ll Bring', ' A budget for continual improvement ', ' Knowledge of cloud based data warehousing products such as Snowflake', ' 1+ year of experience with object-oriented/object function scripting languages like Python. ', 'Responsibilities', ' Bachelor’s Degree ', ' A broad group of industry experts who work closely with us on everything we do ', ' 3+ years of experience with relational SQL databases. ', 'Qualifications', ' An engaged and supportive leadership team that will invest in you  Talented engineering teams to build products with  A broad group of industry experts who work closely with us on everything we do  A budget for continual improvement  Generous retirement plan  Excellent medical and dental insurance (and other health benefits) ', ' 3+ years of experience in a Data Engineer role ', ' Talented engineering teams to build products with ', ' Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery/ dataOps. ', ' 1+ year of experience working with or understanding formal ETL tools ', ' An engaged and supportive leadership team that will invest in you ', ' Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. ', 'What We’re Looking For', ' Excellent medical and dental insurance (and other health benefits) ', 'Company Details', ' Create and maintain optimal data pipeline architecture.  Assemble large, complex data sets that meet functional/non-functional business requirements.  Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery/ dataOps.  Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL.  Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.  Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. ', ' Create and maintain optimal data pipeline architecture. ', 'Skills You’ll Need', ' Bachelor’s Degree  3+ years of experience in a Data Engineer role  3+ years of experience with relational SQL databases.  1+ year of experience with object-oriented/object function scripting languages like Python.  1+ year of experience working with or understanding formal ETL tools  Knowledge of cloud based data warehousing products such as Snowflake', ' Generous retirement plan ', ' Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL. ']",Associate,Full-time,Information Technology,Financial Services,2021-03-18 14:34:51
Data Engineer,JPMorgan Chase & Co.,"New York, NY",3 weeks ago,76 applicants,"['', 'Experience with container technologies such as Docker and Kubernetes', 'Design best practices for data processing, data modeling and warehouse development throughout our team and group', 'Expert level skills in Python its standard library and its package', 'Develop strategy to provide proactive solutions and enable stakeholders to extract insights and value from data', 'Experience with stream processing platforms such as Kafka or Dataflow', 'Hands-on experience with data warehouse / data lake architectures based on Hadoop, Redshift or Snowflake', 'Experience with CI/CD systems e.g. Jenkins and automation / DevOps best practices', 'Familiarity with AWS ecosystem including S3, Glue, Redshift, Kinesis, EMR, EC2, SQS', 'Familiarity with data transformation and collection tools such as Pentaho, Informatica', 'Ability to work in large, collaborative teams to achieve organizational goals', 'Proficiency in one or more modern programming languages', 'BS/BA degree or equivalent experience Advanced knowledge of application, data, and infrastructure architecture disciplines Understanding of architecture and design across all systems Working proficiency in developmental toolsets Knowledge of industry-wide technology trends and best practices Ability to work in large, collaborative teams to achieve organizational goals Passionate about building an innovative culture Proficiency in one or more modern programming languages Understanding of software skills such as business analysis, development, maintenance, and software improvement Advanced level skills in SQL, data integration, data modeling and data architecture Expert level skills in Python its standard library and its package Hands-on experience building a data warehouse and data pipelines using Java, Python or Scala in a data intensive engineering role Hands-on experience with data warehouse / data lake architectures based on Hadoop, Redshift or Snowflake Experience with workflow orchestration tools such as Apache Airflow, Autosys Familiarity with data transformation and collection tools such as Pentaho, Informatica Experience with stream processing platforms such as Kafka or Dataflow Knowledge of data columnar and serialization formats such as JSON, XML, Parquet, Avro Experience with container technologies such as Docker and Kubernetes Experience with CI/CD systems e.g. Jenkins and automation / DevOps best practices Familiarity with AWS ecosystem including S3, Glue, Redshift, Kinesis, EMR, EC2, SQS Familiarity of microservices stack based on AWS Lambdas. Elastic Search, Spring Boot, NodeJS ', 'Familiarity of microservices stack based on AWS Lambdas. Elastic Search, Spring Boot, NodeJS', 'Knowledge of data columnar and serialization formats such as JSON, XML, Parquet, Avro', 'Passionate about building an innovative culture', 'Understand end to end data interactions and dependencies across complex data pipelines and data transformation and how they impact business decisions.', 'Advanced knowledge of application, data, and infrastructure architecture disciplines', 'BS/BA degree or equivalent experience', 'Hands-on experience building a data warehouse and data pipelines using Java, Python or Scala in a data intensive engineering role', 'Build large-scale batch, ETL and real-time data pipelines using cloud and on-premises data technologies, such as Redshift, Python, Spark, PySpark, and Apache Kafka', 'Experience with workflow orchestration tools such as Apache Airflow, Autosys', 'Understanding of software skills such as business analysis, development, maintenance, and software improvement', 'Working proficiency in developmental toolsets', 'Build large-scale batch, ETL and real-time data pipelines using cloud and on-premises data technologies, such as Redshift, Python, Spark, PySpark, and Apache Kafka Design best practices for data processing, data modeling and warehouse development throughout our team and group Develop strategy to provide proactive solutions and enable stakeholders to extract insights and value from data Understand end to end data interactions and dependencies across complex data pipelines and data transformation and how they impact business decisions. ', 'Advanced level skills in SQL, data integration, data modeling and data architecture', 'Understanding of architecture and design across all systems', 'Knowledge of industry-wide technology trends and best practices']",Entry level,Full-time,Information Technology,Banking,2021-03-18 14:34:51
Data Engineer,A Cloud Guru,"Austin, TX",3 weeks ago,25 applicants,"['', ' Be an essential part of designing and building ACG’s new data platform, as we evolve the existing databases into a cutting-edge solution to meet the needs of our 2021 data plans and beyond ', ' Experience with open source orchestration platforms (e.g. Airflow) ', ' Gender-neutral paid parental leave. Expanding your family? We offer 12 weeks of gender-neutral paid parental leave, and reimburse up to $10,000 for eligible adoption expenses. ', 'Remotely awesome.', 'Gender-neutral paid parental leave.', '4 weeks PTO, plus 10 sick days, and holidays. Because even when your office is your living room, we all need time to unplug. ', ' 2+ years of development experience with Python or similar scripting language ', ' Experience working with AWS services such as DynamoDB, Glue, Lambda, Step Functions, S3, CloudFormation or Redshift ', '  2+ years of Data Engineering, Data Warehousing, or related experience   2+ years of development experience with Python or similar scripting language   2+ years of SQL experience, including experience with schema design and dimensional data modelling   Experience working with AWS services such as DynamoDB, Glue, Lambda, Step Functions, S3, CloudFormation or Redshift   Experience with ETL development, metadata management, and data quality   Knowledge of software engineering best practices with experience with implementing CI/CD, monitoring & alerting for production systems   Experience with complex data structures and No-SQL databases   Experience with open source orchestration platforms (e.g. Airflow)  ', 'What’s the interview process like at ACG?', ' Experience with ETL development, metadata management, and data quality ', 'What you bring to the table', '  Be an essential part of designing and building ACG’s new data platform, as we evolve the existing databases into a cutting-edge solution to meet the needs of our 2021 data plans and beyond   Explore and contribute to discussions around technologies under consideration, such as Snowflake, Kappa/Lambda architecture, Delta Lakes and Data Vault   Develop, test and maintain existing architecture, including databases, data pipelines and large-scale processing systems   Collaborate with the Analytics team on transformation processes to populate data models   Recommend ways to improve data reliability, efficiency and quality of the data platform and optimise for performance, scalability and cost   Discover opportunities for data acquisition and explore new ways of using existing data   Identify gaps in data processes and drive improvements   Coach and mentor other team members  ', ' Knowledge of software engineering best practices with experience with implementing CI/CD, monitoring & alerting for production systems ', ' Coach and mentor other team members ', '$1,000 continuing education budget.', 'Human connection.', ' Human connection. Get to know the Gurus with good times and get-togethers inspired by our values, virtual happy hours, lunchtime trivia, or a socially distanced drive-in movie. ', ""Hello, we're A Cloud Guru"", 'to teach the world to cloud. ', 'The Data Engineer role ', 'As a Data Engineer at ACG, you’ll get to:', ' Remotely awesome. Get $500 to level up your home office, monthly snack boxes, free Headspace access, weekly lunch funds, and $50 monthly for internet. ', ' Recommend ways to improve data reliability, efficiency and quality of the data platform and optimise for performance, scalability and cost ', ' Develop, test and maintain existing architecture, including databases, data pipelines and large-scale processing systems ', ' 4 weeks PTO, plus 10 sick days, and holidays. Because even when your office is your living room, we all need time to unplug.   Remotely awesome. Get $500 to level up your home office, monthly snack boxes, free Headspace access, weekly lunch funds, and $50 monthly for internet.   Human connection. Get to know the Gurus with good times and get-togethers inspired by our values, virtual happy hours, lunchtime trivia, or a socially distanced drive-in movie.   Gender-neutral paid parental leave. Expanding your family? We offer 12 weeks of gender-neutral paid parental leave, and reimburse up to $10,000 for eligible adoption expenses.   $1,000 continuing education budget. All Gurus get $250 a quarter to spend on personal development, and 2 hours each week reserved for learning something new.  ', '4 weeks PTO, plus 10 sick days, and holidays.', ' Identify gaps in data processes and drive improvements ', 'More than a job', ' Experience with complex data structures and No-SQL databases ', ' 2+ years of Data Engineering, Data Warehousing, or related experience ', ' Collaborate with the Analytics team on transformation processes to populate data models ', ' Discover opportunities for data acquisition and explore new ways of using existing data ', 'What makes the Engineering team awesome...', ' Explore and contribute to discussions around technologies under consideration, such as Snowflake, Kappa/Lambda architecture, Delta Lakes and Data Vault ', ' 2+ years of SQL experience, including experience with schema design and dimensional data modelling ', ' $1,000 continuing education budget. All Gurus get $250 a quarter to spend on personal development, and 2 hours each week reserved for learning something new. ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Intern - Data Engineer - VESBU,VMware,"Palo Alto, CA",1 week ago,71 applicants,"['', 'Python or NodeJS API development experience is a plus', 'Required Skills', 'Build awareness, evangelize, and encourage adoption of microservice and validate success with business teams', 'Strong hands-on SQL experience', 'Knowledge of data visualization tools and techniques', 'Develop APIs to create endpoints for service integration', 'Business Summary', 'Responsibilities Include', 'This position will perform work that the U.S. government has specified can only be performed by a U.S. citizen on U.S. soil, and therefore any offer will be contingent upon verification of both of these requirements”.', 'Work with the R&D DW team to develop the data model and ensure adoption of data best practice such as optimization for performance and governance', 'Pursuing Bachelor’s in Computer Science or equivalent Strong hands-on SQL experienceKnowledge of Python or similar language suitable for data processing, mining, and transformationStrong analytical and technical skills with ability to clearly communicate data concepts and techniques to non-data users', 'Posted Date:', 'Lead microservice development through deployment to production hosting on Kubernetes platforms ', 'Participate in requirements gathering to understand the use case and validate need with business partnerWork with the R&D data warehouse (DW) to conduct necessary data engineering such as data exploration, cleansing, and preparationWork with the R&D DW team to develop the data model and ensure adoption of data best practice such as optimization for performance and governanceValidate the data set for quality and alignment to business needsDevelop APIs to create endpoints for service integrationLead microservice development through deployment to production hosting on Kubernetes platforms Build awareness, evangelize, and encourage adoption of microservice and validate success with business teams', 'Experience: ', 'Subcategory: ', 'Preferred Skills', 'Work with the R&D data warehouse (DW) to conduct necessary data engineering such as data exploration, cleansing, and preparation', 'Job Role And Responsibilities', 'Validate the data set for quality and alignment to business needs', 'Pursuing Bachelor’s in Computer Science or equivalent ', 'University Summary', 'Exposure to container technologies such as Docker or Kubernetes is a plus', 'Able to work independently', '“', 'Full Time/ Part Time: ', 'Strong analytical and technical skills with ability to clearly communicate data concepts and techniques to non-data users', 'Knowledge of Python or similar language suitable for data processing, mining, and transformation', 'Experience with a variety of database management systems such as Oracle, MySQL, PostgreSQL', 'Participate in requirements gathering to understand the use case and validate need with business partner', 'Able to work independentlyKnowledge of data visualization tools and techniquesExperience with a variety of database management systems such as Oracle, MySQL, PostgreSQLPython or NodeJS API development experience is a plusExposure to container technologies such as Docker or Kubernetes is a plus', 'Category : ']",Not Applicable,Internship,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,"Cadence Bank, N.A.","Birmingham, AL",2 weeks ago,Be among the first 25 applicants,"['', '  Advanced, working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.  Strong analytic skills related to working with unstructured datasets.  A successful history of manipulating, processing and extracting value from large disconnected datasets.  Strong teamwork and interpersonal skills  Experience in leading process improvement initiatives  Ability to motivate high performance, multi-discipline teams  Demonstrated competency in project execution  Demonstrated abilities in relationship management ', 'Professional image with ability to form good partner relationships across functions', ' Create data tools for analytics and business intelligence team members that assist them in building and optimizing our products into an innovative industry leader.', ' Conduct regular meetings with internal and external stakeholders to ensure clarification and meet specific requirements in a timely manner.', 'Cadence Bank is an affirmative action/equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, religion, color, national origin, sex, age, status as a protected veteran, among other things, or status as a qualified individual with disability.', 'Equipment/Software', 'Minimum Experience', 'TRAVEL REQUIREMENT', ' Demonstrated competency in project execution', ' Ability to motivate high performance, multi-discipline teams', ' Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.', ' Gains understanding of customer needs and adapt product strategies to meet their expectations', ""  Bachelor's Degree - Computer Science, Statistics, Informatics, Information Systems or another quantitative field "", 'Position Summary', ' Create and maintain optimal data pipeline architecture', ' Other duties as assigned or requested', 'Behavioral Traits', "" Bachelor's Degree - Computer Science, Statistics, Informatics, Information Systems or another quantitative field"", ' Strong teamwork and interpersonal skills', ' Work with data and analytics experts to strive for greater functionality in our data systems.', 'Demonstrates the capacity to manage changing priorities and ambiguity while remaining calm and controlled', ' 1 - 3 years of experience manipulating, processing and extracting value from large disconnected datasets.', ' Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.', 'Position Description', 'The above statements are intended to describe the general nature and level of the work being performed by people assigned to this work. This is not an exhaustive list of all duties and responsibilities. Cadence Management reserves the right to amend and change responsibilities to meet business and organizational needs as necessary.', ' 1 - 3 years of experience participating in developing strategic plans to realize business objectives', ' Demonstrated abilities in relationship management', ' 1 - 3 years of experience in developing, communicating and presenting concepts to varying audiences', ' Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.', 'Communicates a ""can do"" attitude and positive outlook, minimizing negative behaviors', 'Required Education', ' Experience in leading process improvement initiatives', 'Preferred Experience', ' Keep our data separated and secure across national boundaries through multiple data centers and regions.', 'Demonstrates a meticulous attention to detail', 'Essential Responsibilities', ' 3 - 5 years of experience as a Data Engineer or Sr. Data Analyst', 'Maintains direction and focus through proactive planning and organized approaches to work resulting in target deadlines that are consistently met', 'Knowledge, Skills & Abilities', ' Industry organization participation / leadership (ISACA, EDM Council, IAPP, etc.)', ' 3 - 5 years of experience manipulating, processing and extracting value from large disconnected datasets.', ' Perform root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.', ' Assemble and optimize large, complex data sets that meet functional / non-functional business requirements.', ' 5 - 7 years of experience as a Data Engineer', ' Advanced, working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.', 'Proven ability to quickly learn new applications, processes, and procedures', ' Strong analytic skills related to working with unstructured datasets.', 'Demonstrates initiative, resourcefulness, and independence', ' Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.', ' A successful history of manipulating, processing and extracting value from large disconnected datasets.', '  Conduct regular meetings with internal and external stakeholders to ensure clarification and meet specific requirements in a timely manner.  Create and maintain optimal data pipeline architecture  Assemble and optimize large, complex data sets that meet functional / non-functional business requirements.  Perform root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.  Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.  Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.  Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.  Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.  Keep our data separated and secure across national boundaries through multiple data centers and regions.  Create data tools for analytics and business intelligence team members that assist them in building and optimizing our products into an innovative industry leader.  Work with data and analytics experts to strive for greater functionality in our data systems.  Gains understanding of customer needs and adapt product strategies to meet their expectations  Other duties as assigned or requested ', ' Proven ability to quickly learn new applications, processes, and procedures Demonstrates a meticulous attention to detail Demonstrates the capacity to manage changing priorities and ambiguity while remaining calm and controlled Maintains direction and focus through proactive planning and organized approaches to work resulting in target deadlines that are consistently met Communicates a ""can do"" attitude and positive outlook, minimizing negative behaviors Professional image with ability to form good partner relationships across functions Demonstrates initiative, resourcefulness, and independence ', '  3 - 5 years of experience as a Data Engineer or Sr. Data Analyst  1 - 3 years of experience manipulating, processing and extracting value from large disconnected datasets.  1 - 3 years of experience in developing, communicating and presenting concepts to varying audiences  1 - 3 years of experience participating in developing strategic plans to realize business objectives ', '  5 - 7 years of experience as a Data Engineer  3 - 5 years of experience manipulating, processing and extracting value from large disconnected datasets.  1 - 3 years of experience in developing, communicating and presenting concepts to varying audiences  1 - 3 years of experience participating in developing strategic plans to realize business objectives  Industry organization participation / leadership (ISACA, EDM Council, IAPP, etc.) ']",Not Applicable,Full-time,Information Technology,Banking,2021-03-18 14:34:51
Data Engineer,Nielsen,"Emeryville, CA",4 weeks ago,95 applicants,"['', 'Built applications using relational databases such as MySQL, Postgres, or SQL Server', 'About Nielsen Global Media', 'Experience with MapForce', 'Minimum Bachelor’s Degree in Computer Science', 'Bonus Skills', 'Primary Location', 'Data Engineer - 78162', 'Experience developing service oriented architectures and an understanding of design for scalability, performance and reliability', ' Minimum Bachelor’s Degree in Computer Science 2+ years of software development experience Experience coding in : Java, SQL, NoSQL, Kafka, Scala, Team experience with Agile methodologies, such as Scrum and test-driven development Experience developing service oriented architectures and an understanding of design for scalability, performance and reliability Built applications using relational databases such as MySQL, Postgres, or SQL Server Ability and passion for learning new technology ', ' DevOps experience deploying and tuning the applications you’ve built Configuration management tools such as Ansible, Chef, or Docker Experience designing and deploy applications in AWS Experience with MapForce ', 'Team experience with Agile methodologies, such as Scrum and test-driven development', 'Experience designing and deploy applications in AWS', 'Gracenote', 'Experience coding in : Java, SQL, NoSQL, Kafka, Scala,', 'Data Engineer', 'DevOps experience deploying and tuning the applications you’ve built', '2+ years of software development experience', 'Configuration management tools such as Ansible, Chef, or Docker', 'FOR THIS ROLE WE WILL BE LOOKING FOR INDIVIDUALS THAT HAVE:', 'Ability and passion for learning new technology', 'Job Type:']",Not Applicable,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Engineer (Mobile & Social Games) Python/Scala,The Topps Company,"New York, NY",1 week ago,55 applicants,"['', 'Experience with modern work tracking tools', 'Working in an agile\xa0DataOps\xa0environment', 'Adopt best practices in reporting and analysis including data integrity, testing, maintainability, validation and documentation', '5-10 years of experience applying quantitative methods to understand and answer questions', 'We are seeking a data engineering expert to provide our product decision-makers with insights to make the best possible products for our fans.\xa0The Data Engineer will provide, organize, and adapt large amounts of data from internal and external sources. As part of our data science group you will develop dashboards and visualizations to impart important knowledge to the digital team.\xa0', 'BS/BA in Computer Science, Statistics, Mathematics or equivalent experience', 'EOE', 'Be a resource and advocate for data within the division through empowering others to use the data\xa0providedBuilding, maintaining and optimizing data pipelines written in Python and Scala using SparkWorking with engineering, product, and strategy groups to create and gather data requirementsIntegrating Data Warehouse with BI tools such as LookerExecute design, implementation and maintenance of ETLAdopt best practices in reporting and analysis including data integrity, testing, maintainability, validation and documentationCataloging and collecting data from sources including Postgres DBs, Kinesis Firehoses, Internal and External APIs, etc.Working in an agile\xa0DataOps\xa0environment', 'Analytical/mathematical mindset\xa0', 'Requirements:\xa0', 'Topps Digital is a world-class digital group that is delivering the highest quality mobile sports and entertainment apps to fans all over the world. We have developed a portfolio of apps that has quickly expanded to include titles across both sports and entertainment, providing users with a fun and interactive experience that marries the thrill of collecting and trading with the instant gratification of digital collectibles.', 'Looker or other similar visualization tools', 'Working with engineering, product, and strategy groups to create and gather data requirements', 'Passion for learning and exploring\xa0', 'Execute design, implementation and maintenance of ETL', 'Integrating Data Warehouse with BI tools such as Looker', 'Experience building clear, user-friendly data visualizations, tables, and dashboards', 'Topps has been wowing fans and collectors as a vital part of pop culture for over 80 years.\xa0We Headquartered in New York, NY, The Topps Company, Inc. is a leading creator and international marketer of distinctive confections and sports and entertainment products. The Global Sports and Entertainment division produces trading cards and collectibles, custom cards, memorabilia, sticker album collections and more related to iconic and pop culture brands such as Major League Baseball, Major League Soccer, Star Wars, Bundesliga, UEFA Champions League, World Wrestling Entertainment and Garbage Pail Kids. Topps’ Digital Apps division produces, develops and operates mobile applications that give you access to an exclusive digital card collection at your fingertips that are sold via the Apple and Google app stores under the brand names BUNT, KICK, NHL SKATE, Star Wars Card Trader, The Walking Dead Card Trader, Fear The Walking Dead Trader, WWE SLAM, Marvel Collect! and Disney Collect!', 'Familiarity with version control and reproducible research best practices', 'Cataloging and collecting data from sources including Postgres DBs, Kinesis Firehoses, Internal and External APIs, etc.', 'Interest in building machine learning\xa0systems\xa0', 'Spark/Databricks', 'Strong proficiency with a statistical analysis tool such as Python or Scala', 'AWSSpark/DatabricksLooker or other similar visualization tools', 'Good communication skills', 'Preferred Technologies:', 'Data Engineer', 'SQL proficiency and the ability to extract data from large data sources with differing structures', 'Passion for playing and thinking about games is a plus\xa0', 'Responsibilities:\xa0', 'Experience working with multiple business groups to meet their analytics needs', 'AWS', 'High interest in Big Data processing\xa0', '\xa0', 'Building, maintaining and optimizing data pipelines written in Python and Scala using Spark', 'Experience with Test Driven Development is a plus\xa0', 'Be a resource and advocate for data within the division through empowering others to use the data\xa0provided', 'BS/BA in Computer Science, Statistics, Mathematics or equivalent experience5-10 years of experience applying quantitative methods to understand and answer questionsExperience building clear, user-friendly data visualizations, tables, and dashboardsStrong proficiency with a statistical analysis tool such as Python or ScalaSQL proficiency and the ability to extract data from large data sources with differing structuresFamiliarity with version control and reproducible research best practicesExperience with modern work tracking toolsExperience working with multiple business groups to meet their analytics needsPassion for playing and thinking about games is a plus\xa0High interest in Big Data processing\xa0Experience with Test Driven Development is a plus\xa0Interest in building machine learning\xa0systems\xa0Analytical/mathematical mindset\xa0Good communication skillsPassion for learning and exploring\xa0']",Mid-Senior level,Full-time,Engineering,Consumer Goods,2021-03-18 14:34:51
Data Engineer - Analytics Platform,Equifax,"Alpharetta, GA",2 weeks ago,Be among the first 25 applicants,"['', 'Design and develop Ignite Internal data quality certification and monitoring\xa0', 'Hands on experience in Cloud technologies and Google Data Cloud tools, BigTable and BigQuery and other Big data technologies like Hadoop', 'Drive Ignite Internal platform monitoring, cost reduction initiatives and best practices on cloud\xa0', 'Trust', 'Who is Equifax?\xa0', 'Lead technical research, support broad base user rollout of tools, capabilities and data across US for 200+ hands on users and implement patterns for Global teams\xa0Identify, design, and implement internal process improvements, build new capabilities and automation to drives innovation and business valueDrive Ignite Internal platform monitoring, cost reduction initiatives and best practices on cloud\xa0Design and develop Ignite Internal data quality certification and monitoring\xa0Champion project migration from Cambrian on-prem Hadoop platform to Ignite Internal GCPConfigure, code and test data preparation complex plug-ins in a variety of Google Cloud tools including\xa0 Python, JAVA, Google Data FlowComply with Equifax data security, compliance and governance rules at all times', 'At Equifax, we believe knowledge drives progress. As a global data, analytics and technology company, we play an essential role in the global economy by helping employers, employees, financial institutions and government agencies make critical decisions with greater confidence.\xa0', 'The Perks of being an Equifax Employee?', 'Hands on experience in Cloud technologies and Google Data Cloud tools, BigTable and BigQuery and other Big data technologies like HadoopStrong analytical skills and attention to detail and accuracyExcellent communication and collaboration skills and ability to work independently as well as in teamsExperience in Data Fabric data pipelines, Data Catalog and Collibra for on-boarding and purposing data sources is a huge plusWork experience in regulatory and data compliant environments and Credit Industry Domain knowledge is preferred', 'Decide-Execute-Ship', 'Bravery', 'We work to help create seamless and positive experiences during life’s pivotal moments: applying for jobs or a mortgage, financing an education or buying a car. Our impact is real and to accomplish our goals we focus on nurturing our people for career advancement and their learning and development, supporting our next generation of leaders, maintaining an inclusive and diverse work environment, and regularly engaging and recognizing our employees. Regardless of location or role, the individual and collective work of our employees makes a difference and we are looking for talented team players to join us as we help people live their financial best.', 'The Career Data Engineer will lead and perform the technical activities, of all complexities, ranging from data analysis, design, testing, implementation of new tools and technologies and support users within D&A to complete Ignite Internal migration efforts and continued enhancement of the platform.\xa0 In addition to technical and programming activities, the role requires deep knowledge of Equifax Data, Data\xa0 Fabric, Google Cloud, Equifax Security and Data Protection policies and the business acumen needed to collaborate in a fast-paced\xa0 innovative environment. \xa0 The Data Engineer will work closely with the Innovation Data Scientists, Data Stewards, Data Analyst, Product\xa0 and Technology teams.\xa0\xa0', 'BS/ MS in Computer Science, Management Information Systems, or a related field5+ years in data engineering, data wrangling or related roles.\xa0\xa0Strong skills in SQL, Python, SPARK, Google Data Prep, Google Data Flow, Java or related skillsLiterate in programming languages used for statistical modeling and analysis including machine learning (like SAS, Python, R, SPARK, H2O), data warehousing solutions, Data API\xa0 and building data pipelines, as well as possess a strong foundation in software engineeringExpertise in developing a data schema and data modelsYou possess excellent written and verbal communication skills with the ability to communicate with team members at various levels, including business leaders.', 'Lead technical research, support broad base user rollout of tools, capabilities and data across US for 200+ hands on users and implement patterns for Global teams\xa0', 'Strong skills in SQL, Python, SPARK, Google Data Prep, Google Data Flow, Java or related skills', 'Experience in Data Fabric data pipelines, Data Catalog and Collibra for on-boarding and purposing data sources is a huge plus', 'Accountability', 'Comply with Equifax data security, compliance and governance rules at all times', 'Think and act differently', 'If this sounds like somewhere you want to work, don’t delay, apply today - we’re looking for you!', 'Work experience in regulatory and data compliant environments and Credit Industry Domain knowledge is preferred', '5+ years in data engineering, data wrangling or related roles.\xa0\xa0', 'Collaboration', 'Success Attributes of an Equifax employee; does this describe you?', 'Expertise in developing a data schema and data models', 'Excellent communication and collaboration skills and ability to work independently as well as in teams', 'BS/ MS in Computer Science, Management Information Systems, or a related field', 'Identify, design, and implement internal process improvements, build new capabilities and automation to drives innovation and business value', 'Literate in programming languages used for statistical modeling and analysis including machine learning (like SAS, Python, R, SPARK, H2O), data warehousing solutions, Data API\xa0 and building data pipelines, as well as possess a strong foundation in software engineering', 'Primary Location:', 'You possess excellent written and verbal communication skills with the ability to communicate with team members at various levels, including business leaders.', 'Grow at your own pace through online courses at Learning @ Equifax.', 'Ownership', 'Strong analytical skills and attention to detail and accuracy', 'Schedule:', 'Qualifications ', 'Equifax is transforming core data supply chains to the Equifax Data Fabric, and analytics platform to Ignite Internal built on Google Cloud based technologies.\xa0 Data Fabric will transform all how Equifax manages and monetizes data globally and Ignite Internal is where all the R&D work will happen. This exciting new role will be on the cutting edge of this transformation helping with adoption of Ignite Internal and sunsetting Cambrian legacy Hadoop platform.', 'We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.', 'Function:', 'Curiosity', 'Champion project migration from Cambrian on-prem Hadoop platform to Ignite Internal GCP', 'We offer excellent compensation packages with market competitive pay, comprehensive healthcare packages, 401k matching, schedule flexibility, work from home opportunities, paid time off, and organizational growth potential.Grow at your own pace through online courses at Learning @ Equifax.', 'Extra Points for any of the Following', 'AccountabilityBraveryCuriosityCollaborationThink and act differentlyTrustOwnershipDecide-Execute-Ship', 'Configure, code and test data preparation complex plug-ins in a variety of Google Cloud tools including\xa0 Python, JAVA, Google Data Flow', 'What You’ll Do ', 'We offer excellent compensation packages with market competitive pay, comprehensive healthcare packages, 401k matching, schedule flexibility, work from home opportunities, paid time off, and organizational growth potential.']",Mid-Senior level,Full-time,Analyst,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Langham Recruitment,"Eastern, WV",5 days ago,Be among the first 25 applicants,"['', 'Cloud Native Data Warehouse Experience (AWS Preferred)', 'Skills / Experience', 'Strong SQL Experience Including Tuning Skills']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,"ITCO Solutions, Inc.","Carlsbad, CA",4 weeks ago,Be among the first 25 applicants,"['', 'Advanced knowledge of SQL (CTEs, window functions, querying semi-structured data) and relational databases (Postgres, Snowflake, Redshift, BigQuery)', 'Knowledge and use of a source control system, such as Git', 'Knowledge of data warehousing best practices and data quality management', 'Provide technical guidance for design and implementation of data storage and governance systems', 'Location: ', 'Reduce complexity', '2+ years experience building and optimizing data pipelines, warehouses and data sets', 'Infrastructure, Virtualization, Cloud, and Managed services', 'Consulting Services ', 'Familiarity with AWS/GCP cloud computing tools', 'Work closely with Analytics and Data Science teams to design informative user metrics and models', 'Simplify the management and maintenance of software purchases', 'One Liner: Build Data Pipelines in Python from Postgres thru Airflow and then Snowflake.Must haves: Python, SQL, Airflow, Tensorflow, Snowflake ', ' Work as part of the data engineering team to define and develop data ingestion, validation, transformation and loading code Maintain and improve existing data pipelines Collaborate with leadership and project leads to ensure the data, reporting, analytics, and automation needs of the business are met Provide technical guidance for design and implementation of data storage and governance systems Work closely with Analytics and Data Science teams to design informative user metrics and models ', 'Maintain and improve existing data pipelines', 'Hard-dollar costs savings by as much as 30% for enterprise software licensing', ' Hard-dollar costs savings by as much as 30% for enterprise software licensing Simplify the management and maintenance of software purchases Provide an assessment of your existing licensing', 'Requirements', 'SmartSoft ', 'ITCO Overview', 'Provide an assessment of your existing licensing', 'Workforce Solutions ', 'Proficient with Python', 'Essential Roles And Responsibilities', 'Collaborate with leadership and project leads to ensure the data, reporting, analytics, and automation needs of the business are met', 'Work as part of the data engineering team to define and develop data ingestion, validation, transformation and loading code', 'Experience developing ETL applications that move data to and from various platforms including REST APIs, SQL/Cloud DBs, and Cloud File Storage (such as S3)', 'Turn-Key project delivery', ' 2+ years experience building and optimizing data pipelines, warehouses and data sets Proficient with Python Advanced knowledge of SQL (CTEs, window functions, querying semi-structured data) and relational databases (Postgres, Snowflake, Redshift, BigQuery) Experience developing ETL applications that move data to and from various platforms including REST APIs, SQL/Cloud DBs, and Cloud File Storage (such as S3) Familiarity with AWS/GCP cloud computing tools Knowledge and use of a source control system, such as Git Knowledge of data warehousing best practices and data quality management Experience with data pipeline and workflow management tools (Airflow, Luigi) a huge plus ', 'Experience with data pipeline and workflow management tools (Airflow, Luigi) a huge plus', 'Job Description', ' Turn-Key project delivery Reduce complexity Infrastructure, Virtualization, Cloud, and Managed services ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer [1914],Accelere,"San Francisco, CA",1 week ago,64 applicants,"['', 'When you submit your resume, please include the hourly contract rate you want to consider for this 100% remote role.\xa0Please also mention how long you have worked remotely!', 'Adheres to and advocates for software engineering best practices (e.g. technical design, technical design review, unit testing, monitoring & alerting, checking in code, code review, documentation),', 'Architects, designs, implements and maintains reliable and scalable data infrastructure.', 'Responsible for service reliability and following site-reliability engineering best practices: on-call rotations for services they maintain, responsible for defining and maintaining SLAs.', 'Mentors others.', 'Development experience in one or more object-oriented programming languages (e.g. Python, Go, Java, C++)', 'Responsibilities', ""Completed Bachelor's degree in Computer Science5+ years of recent professional data engineering experienceDeep and hands-on experience designing, planning, productionizing, maintaining, and documenting reliable and scalable data infrastructure and data products in complex environments.Development experience in one or more object-oriented programming languages (e.g. Python, Go, Java, C++)Advanced\xa0SQL\xa0knowledgeExperience designing and implementing large-scale distributed systemsDeep knowledge and hands-on experience in technologies across all data lifecycle stagesStrong stakeholder management and ability to lead large organizations through the influenceContinuous learning and improvement mindsetNo prior experience in the energy industry required"", 'Part of a cross-disciplinary team, working closely with other data engineers, software engineers, data scientists, data managers, and business partners.', 'Responsible for deploying secure and well-tested software that meets privacy and compliance requirements; develops, maintains, and improves CI/CD pipeline,', 'Writes, deploys, and maintains software to build, integrate, manage, maintain, and quality assure data at the company', 'Advanced\xa0SQL\xa0knowledge', 'Deep and hands-on experience designing, planning, productionizing, maintaining, and documenting reliable and scalable data infrastructure and data products in complex environments.', 'Qualifications', 'Strong stakeholder management and ability to lead large organizations through the influence', 'Continuous learning and improvement mindset', 'Deep knowledge and hands-on experience in technologies across all data lifecycle stages', ""Completed Bachelor's degree in Computer Science"", 'Part of a cross-disciplinary team, working closely with other data engineers, software engineers, data scientists, data managers, and business partners.Architects, designs, implements and maintains reliable and scalable data infrastructure.Writes, deploys, and maintains software to build, integrate, manage, maintain, and quality assure data at the companyAdheres to and advocates for software engineering best practices (e.g. technical design, technical design review, unit testing, monitoring & alerting, checking in code, code review, documentation),Responsible for deploying secure and well-tested software that meets privacy and compliance requirements; develops, maintains, and improves CI/CD pipeline,Responsible for service reliability and following site-reliability engineering best practices: on-call rotations for services they maintain, responsible for defining and maintaining SLAs.Design, build, deploy and maintain infrastructure as code. Containerizes server deployments.Actively contributes to improving developer velocity.Mentors others.', 'Design, build, deploy and maintain infrastructure as code. Containerizes server deployments.', 'Actively contributes to improving developer velocity.', 'If you are a staffing firm, you need to submit candidates with an ""all-inclusive"" hourly rate.', '5+ years of recent professional data engineering experience', 'No prior experience in the energy industry required', 'Experience designing and implementing large-scale distributed systems']",Mid-Senior level,Contract,Engineering,Information Technology and Services,2021-03-18 14:34:51
Senior Data Engineer,Luxoft,"Los Angeles, CA",2 days ago,Be among the first 25 applicants,"['', 'Skills', ' You will design data models, operate cloud-based data warehouses, and SQL/NoSQL/temporal database systems. you will be working closely with information security teams to adopt and implement security best practices for data pipelines and data servers. You will also provide insightful code reviews, receive code reviews constructively, and take ownership of outcomes (“you ship it, you own it”), working very efficiently and routinely with the team to deliver the right data for the UI through web services. ', 'Languages', 'Responsibilities', ' - ', 'Project Description', ' BS in Computer Science or related field7+ yrs of experience implementing big data processing pipelines (SQL / NoSQL) technology: Hadoop, Apache Spark, AWS Glue/Athena, Airflow, Serverless etc.Coding proficiency in at least one modern programming language (Node, Python, Java, etc.).Experience writing and optimizing advanced SQL queries in a business environment with large-scale, complex datasets.Experience in cloud-first design, preferably AWS (VPC, Serverless databases and functions, dynamic autoscaling, container orchestration, etc.).Experience in data architecture, databases (e.g., MySQL, Oracle, PostgreSQL, DynamoDB, RDS Aurora), SQL and DDD/ER/ORM design.Interest and curiosity in emerging technologies on the web like GraphQL, web assembly, Lambda functions, MLaaS etc.Knowledge of software engineering practices & best practices for the software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations. ']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,EdgeLink,"Lake Oswego, OR",1 week ago,Be among the first 25 applicants,"['', 'Expand or improve enterprise data model', 'Positive - we’ ve got a great culture and we’ re keeping it that way', ' Polyglot - work comfortably with many technologies, tools, languages. Smart - no problem is too hard Positive - we’ ve got a great culture and we’ re keeping it that way Energetic - we move fast Two to five years of experience in a similar role ', 'Polyglot - work comfortably with many technologies, tools, languages.', 'Build ETL or API-based integrations between systems to ensure clear bi-directional data synchronization.', 'Build reports and ad hoc data queries', 'Characteristics', 'Build business intelligence and data visualization dashboards for new data sets', 'Identify areas of improvement to achieve data quality', 'Two to five years of experience in a similar role', 'Projects for the Data Engineer', ' Amaze the company with insights from data Build ETL or API-based integrations between systems to ensure clear bi-directional data synchronization. Build business intelligence and data visualization dashboards for new data sets Expand or improve enterprise data model Build reports and ad hoc data queries Evaluate enterprise datasets for quality and accuracy Determine root cause for data quality errors and make recommendations for long-term solutions Identify areas of improvement to achieve data quality ', 'Evaluate enterprise datasets for quality and accuracy', 'Amaze the company with insights from data', 'Job Description', 'Smart - no problem is too hard', 'Determine root cause for data quality errors and make recommendations for long-term solutions', 'Energetic - we move fast']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data engineer,Perficient,"Philadelphia, PA",1 week ago,Be among the first 25 applicants,"['', 'Job Overview', ' Experts of the data and its application by users  Understands data landscape and environments: sources, elements, update freq, completeness, stewards/contacts, platforms  Manages ETL: use programming and tools for data ingestion, configure pipelines, apply transformations and decoding, integrate and fuse data, move and securely deliver  Translates business requirements to build repeatable, sustainable, efficient, coded processes that can be productionized by Software Engineers and readily modified by other Data Engineers  Creates POC processes in Dev/QA and works with Software Engineers to productionize and align best coding and process practices that maximize efficiency, speed, stability, system resources and capabilities  Leverages frameworks in place with big data tools: Hadoop, Spark, Python, Kafka, etc.  Experience with relational SQL and NoSQL databases  Awareness of and compliance with: data privacy, security, legal and contractual guidelines  Incorporates data quality and privacy checks/alerts to minimize bad data being consumed by end users, models and dashboards, and to protect customer data; revises checks for new data issues  Maintains feedback loop with Data Stewards on data issues, standards, fit for use (Data Stewardship is a subset of data engineering which would include responsibilities like data curation)  Validates data products and pipelines are functioning as expected following system or application upgrades, source changes, etc.  Responsible for data architecture including sources, table structures, physical models  Works closely with Architects to align systems, tools and applications being utilized with business use case and performance requirements  Communicates with end users to set expectations and ensure alignment around data accuracy, completeness, timeliness and consistency  Provides data product support and maintenance  Establishes, tracks and monitors KPIs related to specific data products and deliverables ', ' Develop open source platform components using Spark, Scala, Java, Oozie, Hive and other components ', ' Responsible for data architecture including sources, table structures, physical models ', ' Will you now, or in the future, require sponsorship for employment visa status (e.g. H-1B visa status)?', ' Troubleshoot deployment to various environments and provide test support. ', ' Manages ETL: use programming and tools for data ingestion, configure pipelines, apply transformations and decoding, integrate and fuse data, move and securely deliver ', ' Work with data engineering team to define and develop data ingestion, validation, transformation and data engineering code.  Develop open source platform components using Spark, Scala, Java, Oozie, Hive and other components  Document code artifacts and participate in developing user documentation and run books  Troubleshoot deployment to various environments and provide test support.  Participate in design sessions, demos and prototype sessions, testing and training workshops with business users and other IT associates ', ' Participate in design sessions, demos and prototype sessions, testing and training workshops with business users and other IT associates ', 'Responsibilities', ' Establishes, tracks and monitors KPIs related to specific data products and deliverables ', ' Maintains feedback loop with Data Stewards on data issues, standards, fit for use (Data Stewardship is a subset of data engineering which would include responsibilities like data curation) ', ' Translates business requirements to build repeatable, sustainable, efficient, coded processes that can be productionized by Software Engineers and readily modified by other Data Engineers ', ' Understands data landscape and environments: sources, elements, update freq, completeness, stewards/contacts, platforms ', ' Are you legally authorized to work in the United States?', ' Document code artifacts and participate in developing user documentation and run books ', ' Validates data products and pipelines are functioning as expected following system or application upgrades, source changes, etc. ', 'Qualifications', ' Creates POC processes in Dev/QA and works with Software Engineers to productionize and align best coding and process practices that maximize efficiency, speed, stability, system resources and capabilities ', ' Incorporates data quality and privacy checks/alerts to minimize bad data being consumed by end users, models and dashboards, and to protect customer data; revises checks for new data issues ', ' Experts of the data and its application by users ', ' Provides data product support and maintenance ', ' Work with data engineering team to define and develop data ingestion, validation, transformation and data engineering code. ', ' Leverages frameworks in place with big data tools: Hadoop, Spark, Python, Kafka, etc. ', 'More About Perficient', 'Preferred Skills And Education', ' Works closely with Architects to align systems, tools and applications being utilized with business use case and performance requirements ', 'Data Engineer with Spark exp.', 'Overview', ' Experience with relational SQL and NoSQL databases ', ' Awareness of and compliance with: data privacy, security, legal and contractual guidelines ', ' Communicates with end users to set expectations and ensure alignment around data accuracy, completeness, timeliness and consistency ', ' Disclaimer: ']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,HUNTER Technical - htrjobs.com,"Alpharetta, GA",2 weeks ago,93 applicants,"['', 'Experience with Data Modeling using SQL', ' Partner with engineering teams to scale data solutions to improve business intelligence and analytical platforms, such as querying, reporting and analytical tools Leverage SQL and other analytic platforms to gather, clean, and prepare data; create dashboards/reports; develop KPIs; analyze trends; provide insights Integrate new tools, software, and technology to streamline processes, improve efficiency, and improve working methods and culture within the analytics team Act as database expert within the team and organization ', 'Expert-level SQL skills', 'Partner with engineering teams to scale data solutions to improve business intelligence and analytical platforms, such as querying, reporting and analytical tools', '5+ years of relevant experience', 'Job Description', 'Proficient in either Python or Java', 'BA/BS degree in a technical or quantitative/business-oriented field or equivalent practical experience', ' Excellent communication skills Excellent analytical skills Expert-level SQL skills Experience with Data Modeling using SQL Proficient in either Python or Java Independent-working, fast-learning and problem-solving BA/BS degree in a technical or quantitative/business-oriented field or equivalent practical experience 5+ years of relevant experience Skilled in working with enterprise level systems', 'Independent-working, fast-learning and problem-solving', 'Integrate new tools, software, and technology to streamline processes, improve efficiency, and improve working methods and culture within the analytics team', 'Skilled in working with enterprise level systems', 'Leverage SQL and other analytic platforms to gather, clean, and prepare data; create dashboards/reports; develop KPIs; analyze trends; provide insights', 'Excellent communication skills', 'Act as database expert within the team and organization', 'Requirements', 'Excellent analytical skills']",Entry level,Contract,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Daugherty Business Solutions,"Columbus, Ohio Metropolitan Area",,N/A,"['', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Collaborate and work closely with team to build data platforms.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Proven ability to pick up new languages and technologies quickly.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Knowledge of cloud and distributed systems principles, including load balancing, networks, scaling, and in-memory versus disk.', 'Daugherty Business Solutions is an inclusive Equal Employment Opportunity employer that considers applicants without regard to gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law.', 'If you require accommodations or assistance to complete the online application process, please inform any recruiter you are working with (or send an email to careers@daugherty.com) and identify the type of accommodation or assistance you are requesting. Do not include any medical or health information in this email. The recruiting team will respond to your email promptly.', 'Daugherty Business Solutions, a leading advisory services and technology consulting firm will be expanding its operations to open a new, world-class Software Development Center in Columbus, Ohio to support its rapid growth and to engage the region’s diverse talent pool and thriving business community.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Recommend ways to improve data reliability, efficiency and quality.', 'Data & Analytics', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Maintain and manage Hadoop clusters in development and production environments.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Interest in Hadoop family languages including Pig and Hive.', 'SNS/SQS and QuickSight.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Intermediate level of SQL programming and query performance tuning techniques for data integration and consumption using design for optimum performance against large data assets within an OLTP, OLAP and MPP architecture.', 'Due to COVID-19, most of our employees are working remotely. We’ve implemented a virtual hiring process and continue to interview candidates by phone or video and are onboarding new hires remotely. We value the safety of each member of our community because we know we’re all in this together.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Some experience with programming Languages, such as Scala, Java, R and Python.', 'We offer members of Team Daugherty:', 'From Ron Daugherty, founder and CEO of Daugherty Business Solutions:\xa0“I am excited to announce that Daugherty is expanding to Columbus, OH.\xa0This expansion is a direct result of the hard work of our consultants during this pandemic.\xa0Daugherty teammates continue to find ways to add even more value for our clients and to increase the demand for our consulting expertise.\xa0Columbus is a growing metro area with a strong base of talent, excellent universities, and interesting perspective clients.\xa0That’s why I’m proud to add Columbus to our existing development centers across the country.”', 'Interested? Apply today to take your career to the next level!', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Implement & automate high-performance algorithms, prototypes and predictive models.', 'Daugherty is hiring experienced Data Engineers to join our Columbus-based team.\xa0The ideal employee is a problem solver with the ability to utilize insights, creativity and perspective to drive business success for our clients.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Work with team members and functional leads to understand existing data requirements and validation rules to support moving existing data warehouse workloads into a distributed data platform.', 'When you are a Daugherty employee, your job doesn’t end when a contract is up. You stay on as an indispensable member of the team with career growth opportunities tailored to your interests and talents. We want you to be eager to take on a new challenge. We are always 100% honest about what to expect, because we don’t want Daugherty to be just another job; we want Daugherty to be your dream job.\xa0', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Assemble large, complex data sets that meet functional/non-functional business requirements.', 'As a Data Engineer you will have the opportunity to:', 'In addition to existing Software Development Centers across the organization; in Minneapolis, Atlanta, Dallas, Chicago, NY and the St. Louis Headquarters, the team in Columbus will strategically support Daugherty’s growth strategy and commitment to delivering high quality software fast and effectively for its Fortune 500 clients.\xa0', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Excellent health, dental and vision insurance.', 'We are looking for motivated people with:', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience building data pipelines to connect analytics stacks, client data visualization tools and external data sources.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Life, disability and long-term care insurance.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Employ a variety of languages and tools to marry systems together.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Familiarity with high performance data libraries including Spark, NumPy and TensorFlow.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Little to no travel.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Create custom software components (e.g. specialized UDFs) and analytics applications.', '\xa0', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Interest working with AWS technologies such as Redshift, RDS, S3, EMR, ADP, Hive, Kinesis,', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Contribute to the creation and maintenance of optimal data pipeline architectures.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Interest in Python, R, sh/bash and JVM-based languages including Scala and Java.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Robust career development and training.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Understanding of DevOps and CI/CD toolset, such as Jenkins, GitLab CI, Buildbot, Drone and Bamboo.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Exposure to stream-processing and messaging, such as Storm, Spark-Streaming, Kafka and MQ.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0Revenue sharing and a 401(k) retirement savings plan.']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
AWS Data Engineer,"Adroit Resources, Inc.","San Francisco, CA",1 week ago,163 applicants,"['', 'Strong experience with Kafka', '\ufeffWhat You Know', 'Bachelor’s Degree or equivalent experience', '·Experience with relational SQL and NoSQL databases such as Cassandra.', 'What You’ll Do', 'Education', '·5+ years of experience in a Data Engineer role', '·Experience with AWS cloud services: EC2, EMR, Athena', '·Experience supporting and working with cross-functional teams in a dynamic environment', '·Experience with big data tools: Hadoop, Spark, Kafka, etc.', '·Advanced SQL knowledge and experience working with relational databases, query authoring (SQL) as well as familiarity with unstructured datasets.', '·Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.', '·Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.', '·5+ years of experience in a Data Engineer role·Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.Strong experience with Kafka·Experience with big data tools: Hadoop, Spark, Kafka, etc.·Experience with relational SQL and NoSQL databases such as Cassandra.·Experience with AWS cloud services: EC2, EMR, Athena·Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.·Advanced SQL knowledge and experience working with relational databases, query authoring (SQL) as well as familiarity with unstructured datasets.', '·Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.·Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.·Experience supporting and working with cross-functional teams in a dynamic environment', '·Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.', '·Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.']",Mid-Senior level,Contract,Information Technology,Staffing and Recruiting,2021-03-18 14:34:51
"Data Engineer, People Analytics",First Republic Bank,"San Francisco, CA",3 weeks ago,79 applicants,"['', ' Ability to learn quickly and adjust to changing business needs.', ' Develop conceptual data models, implement data strategies, and build data flows.', ' Hands-on experience in data modeling, data visualization and pipeline design and development', ' Effective communication skills, with the ability to interact with cross-functional stakeholders in spoken and written form.', ' Uses knowledge of business objectives, strategies, and needs to identify opportunities where data can be leveraged to achieve the desired business benefits.', ' As a data engineer, you will be working closely with data architects and data analysts to implement data modeling solutions in order to streamline and support people analytics data and visualizations.', ' Partner closely with HR Technology and Operations to ensure that systems and data catalogs align to the analytical and data reporting needs; Build and maintain strong partnerships across HR and the business.', ' Hands-on experience with data platforms (Snowflake, AWS) and familiarity with data visualization (Tableau, Cognos BI) technologies', ' Must be able to communicate effectively via telephone and in person.', 'Responsibilities', 'Job Demands', ' In-depth knowledge of data warehousing and relational database design', ' Must be able to review and analyze data reports and manuals; must be computer proficient.', ' Must be able to review and analyze data reports and manuals; must be computer proficient. Must be able to communicate effectively via telephone and in person.', ' Manage various analytics projects and collaborate within HR and cross-functionally across the business', 'Qualifications', ' As a data engineer, you will be working closely with data architects and data analysts to implement data modeling solutions in order to streamline and support people analytics data and visualizations. Develop conceptual data models, implement data strategies, and build data flows. Optimize and update logical and physical data models to support new and existing projects. Develop best practices for data coding to ensure high data quality and consistency within the system. Design and develop solutions to reduce data redundancy, streamline data movements, and improve enterprise information management Communicate complex information so that it is easy to understand and drive strategic use of data to help increase value of the People Analytics function. Partner closely with HR Technology and Operations to ensure that systems and data catalogs align to the analytical and data reporting needs; Build and maintain strong partnerships across HR and the business. Manage various analytics projects and collaborate within HR and cross-functionally across the business Uses knowledge of business objectives, strategies, and needs to identify opportunities where data can be leveraged to achieve the desired business benefits.', 'Description', ' Experience with HR systems as well as other technical tools', ' Demonstrated technical ability and analytical skills, with superb attention to detail and passion for data, data definition integrity, and process efficacy', ' Develop best practices for data coding to ensure high data quality and consistency within the system. Design and develop solutions to reduce data redundancy, streamline data movements, and improve enterprise information management', ' Hands-on experience in data modeling, data visualization and pipeline design and development Hands-on experience with data platforms (Snowflake, AWS) and familiarity with data visualization (Tableau, Cognos BI) technologies Strong in at least one of these programming languages: SQL, R, Python, Go In-depth knowledge of data warehousing and relational database design Expert knowledge of metadata management and related tools. Demonstrated technical ability and analytical skills, with superb attention to detail and passion for data, data definition integrity, and process efficacy Effective communication skills, with the ability to interact with cross-functional stakeholders in spoken and written form. Ability to learn quickly and adjust to changing business needs. Experience with HR systems as well as other technical tools', ' Optimize and update logical and physical data models to support new and existing projects.', ' Strong in at least one of these programming languages: SQL, R, Python, Go', ""What You'll Do As a Data Engineer, People Analytics"", ' Communicate complex information so that it is easy to understand and drive strategic use of data to help increase value of the People Analytics function.', ' Expert knowledge of metadata management and related tools.']",Not Applicable,Full-time,Information Technology,Banking,2021-03-18 14:34:51
Data Engineer,Idaho Central Credit Union,"Chubbuck, ID",2 weeks ago,Be among the first 25 applicants,"['', 'Experience with business intelligence visualization tools and business intelligence tools, such as, Microsoft Power BI', 'Do you take pride in the work you accomplish? ', 'Work closely with team members and BI Analysts to turn data into critical information and knowledge that can be used to make sound business decisions.', 'Can you handle multiple projects at the same time and smile? ', 'Responsible for the full life cycle development, implementation, support, architecture and tuning of the Enterprise Data Warehouse, including Data Marts, Data Lake and data pipelines.', 'Ability to work with and communicate with all Credit Union personnel in the various departments.', 'ELT/ETL development.', 'Experience In', 'Ability to work with other department supervisors.', 'Identify, design, and implement internal process improvements including automating manual processes, optimize data delivery and designing cloud infrastructure for greater scalability, etc..', 'Technical skill-sets needed include but not limited to SQL, ELT/ETL development, configuring and maintaining data warehousing solutions, data modeling, data lake technologies, data architecture, and BI reporting development.', 'Advocate of CI/CD methodologies and agile ways of working.', 'Willingness to work occasionally outside of normal business hours.', 'Source-code management tools such as GitHub', '2+ years’ experience working in cloud computing with Azure experience required', 'Microsoft SQL Server, Oracle, and other relational and NoSQL databases.', 'Design and development focus include but is not limited to dimensional data model design, ETL/ELT frameworks and processing, meta-data management, operational data integration, master data management and data quality, data auditing and profiling, and business intelligence reporting solutions.', 'Ability to maintain confidentiality of Credit Union and member records at all times.', 'Write technical policies, procedures, and documentation for the systems including data flow diagrams, architecture diagrams, etc.', 'Contribute to a team responsible for the design, development and implementation of mission critical business intelligence reports and applications to enterprise scale.', 'Other duties as assigned.', 'Knowing programming languages such as Java, R, Python is a plus but is not required.', 'Technologies involved but not limited to Azure Data Lake, Azure Data Factory, Azure Data-bricks, Azure Synapse, SQL Server, Power BI, Oracle database, AI/ML', 'Support the integration of enterprise application databases and real time processing into the data warehouse.', 'Ensure all data accesses, along with data at rest or in transit is secure and follows the best in class data governance standards.', 'Strong analytical and problem-solving Ability to maintain confidentiality of Credit Union and member records at all times.', 'Ensure all data sources are accurate, congruent, reliable, and secure.', 'SQL development', 'Responsible for architecting, configuring, analyzing, and maintaining all data structures used for BI analytics and reporting.', 'Define and build data integration processes to be used across the organization.', 'Excellent English oral and written communication skills.', 'Data warehousing, Data Lake, data modeling, data pipelines', 'Work closely with team members and BI Analysts to turn data into critical information and knowledge that can be used to make sound business decisions.Responsible for architecting, configuring, analyzing, and maintaining all data structures used for BI analytics and reporting.Ensure all data sources are accurate, congruent, reliable, and secure.Responsible for the full life cycle development, implementation, support, architecture and tuning of the Enterprise Data Warehouse, including Data Marts, Data Lake and data pipelines.Define and build data integration processes to be used across the organization.Build conceptual and logical data models.Identify, design, and implement internal process improvements including automating manual processes, optimize data delivery and designing cloud infrastructure for greater scalability, etc..Ensure all data accesses, along with data at rest or in transit is secure and follows the best in class data governance standards.Support the integration of enterprise application databases and real time processing into the data warehouse.Technical skill-sets needed include but not limited to SQL, ELT/ETL development, configuring and maintaining data warehousing solutions, data modeling, data lake technologies, data architecture, and BI reporting development.Technologies involved but not limited to Azure Data Lake, Azure Data Factory, Azure Data-bricks, Azure Synapse, SQL Server, Power BI, Oracle database, AI/MLThis person will work closely with external professional services and third-party vendors to have a full knowledge of systems and help establish road-map for future enhancements.Write technical policies, procedures, and documentation for the systems including data flow diagrams, architecture diagrams, etc.Contribute to a team responsible for the design, development and implementation of mission critical business intelligence reports and applications to enterprise scale.Design and development focus include but is not limited to dimensional data model design, ETL/ELT frameworks and processing, meta-data management, operational data integration, master data management and data quality, data auditing and profiling, and business intelligence reporting solutions.Other duties as assigned.', 'Microsoft SQL Server, Oracle, and other relational and NoSQL databases.SQL developmentExperience with business intelligence visualization tools and business intelligence tools, such as, Microsoft Power BIData warehousing, Data Lake, data modeling, data pipelinesELT/ETL development.Knowing programming languages such as Java, R, Python is a plus but is not required.Source-code management tools such as GitHub2+ years’ experience working in cloud computing with Azure experience requiredKnowledge of data processing, hardware platforms, and enterprise software Technical experience with enterprise systems, databases, and user support.Advocate of CI/CD methodologies and agile ways of working.Ability to maintain confidentiality of Credit Union and member records at all times.Self-motivated with the ability to prioritize,meetdeadlines,andmanagechangingWillingness to work occasionally outside of normal business hours.Excellent English oral and written communication skills.Ability to work with other department supervisors.Strong analytical and problem-solving Ability to maintain confidentiality of Credit Union and member records at all times.Ability to work with and communicate with all Credit Union personnel in the various departments.', 'Does success motivate you to want to do more and be better? ', 'Self-motivated with the ability to prioritize,meetdeadlines,andmanagechanging', 'Do you enjoy working with other people and finding solutions when everyone else only see problems? ', 'Build conceptual and logical data models.', 'This person will work closely with external professional services and third-party vendors to have a full knowledge of systems and help establish road-map for future enhancements.', 'Are you compelled to initiate action and remain proactive in getting things done? ', 'Knowledge of data processing, hardware platforms, and enterprise software Technical experience with enterprise systems, databases, and user support.', 'Do you enjoy working with other people and finding solutions when everyone else only see problems? Are you compelled to initiate action and remain proactive in getting things done? Do you take pride in the work you accomplish? Does success motivate you to want to do more and be better? Can you handle multiple projects at the same time and smile? ']",Entry level,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Data Migration Engineer,ELLKAY,"Elmwood Park, NJ",7 days ago,Be among the first 25 applicants,"['', 'Manage multiple projects simultaneously and prioritize tasks to ensure timely delivery', '2+ years Experience with data analysis and SQL', 'Bachelor’s degree Computer Science, Data Analytics or related field', 'Knowledge of healthcare data & workflow preferred', 'Coordinate deliverables with various internal departments.', 'Excellent communication and organizational skills', 'Proactively identifies issues and works on resolution plan to alleviate impact on project.', 'Assess client’s desired scope, analyze client data and design project plan.', 'EMR Data Migration Specialist', 'Assess client’s desired scope, analyze client data and design project plan.Proactively identifies issues and works on resolution plan to alleviate impact on project.Set up environment for EMR data Migration and performs ETL for desired databasesPerforms data validation and testing to ensure accuracyEvaluate and identify opportunities to drive continuous process improvementsManage multiple projects simultaneously and prioritize tasks to ensure timely deliveryCoordinate deliverables with various internal departments.Set and manage appropriate expectations for successful project execution.', 'Bachelor’s degree Computer Science, Data Analytics or related field2+ years Experience with data analysis and SQLKnowledge of healthcare data & workflow preferredStrong problem solving and analytical skillsAbility to exercise effective decision-making capabilities in a fast-paced environment.Excellent communication and organizational skills', 'Company Description', 'Set and manage appropriate expectations for successful project execution.', 'Essential Duties And Responsibilities', 'ELLKAY', 'Strong problem solving and analytical skills', 'Performs data validation and testing to ensure accuracy', 'This is an onsite position at our Elmwood Park HQ. Remote work may be available. ', 'Set up environment for EMR data Migration and performs ETL for desired databases', 'Job Description', 'Evaluate and identify opportunities to drive continuous process improvements', 'Company Culture:', 'Ability to exercise effective decision-making capabilities in a fast-paced environment.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer - USA,InterWorks,Oklahoma City Metropolitan Area,1 month ago,56 applicants,"['', 'Solve data-acquisition, integration and management problems', 'AWS / Microsoft Azure', 'Adaptability and flexibility in changing situations', 'Matillion, Fivetran, Airflow, DBT or other ETL tools', 'Experience with software engineering practicesExperience with modern data-engineering practices and frameworksExperience with integration from API sourcesMatillion, Fivetran, Airflow, DBT or other ETL toolsAWS / Microsoft AzureSnowflake / Amazon Redshift / Google BigQuery / Azure Synapse ', 'Snowflake / Amazon Redshift / Google BigQuery / Azure Synapse ', 'Excellent verbal and written communication', 'Experience with integration from API sources', 'What You’ll Do', 'What You’ll Need', 'Experience with software engineering practices', 'Strong problem-solving skills', 'What We’d Like You to Have', 'Collaborate closely with users to understand their unique needs and support them with the best solutions', 'Create ETL processes based on client needs while managing client expectations', 'Business acumen', 'Why InterWorks', 'Strong ETL proficiency using GUI-based tools or code-based patterns', 'Passion for delivering compelling solutions that exceed client expectations', 'Tackle diverse projects that range in duration from a few days to a few months for clients ranging from local businesses to the Fortune 500Work with disparate data sources (relational databases, flat files, Excel, HDFS/Big Data systems, high-performance analytical databases, etc.) to unify client dataCollaborate closely with users to understand their unique needs and support them with the best solutionsSolve data-acquisition, integration and management problemsCreate ETL processes based on client needs while managing client expectations', 'Programming (Python, Java, C#, PHP, etc.)', 'Experience with modern data-engineering practices and frameworks', 'Must-Haves', 'Excellent SQL fluency', 'Excellent SQL fluencyProgramming (Python, Java, C#, PHP, etc.)Strong ETL proficiency using GUI-based tools or code-based patternsUnderstanding of data-modeling principlesExcellent verbal and written communicationBusiness acumenStrong problem-solving skillsAdaptability and flexibility in changing situationsPassion for delivering compelling solutions that exceed client expectations', 'Work with disparate data sources (relational databases, flat files, Excel, HDFS/Big Data systems, high-performance analytical databases, etc.) to unify client data', 'Tackle diverse projects that range in duration from a few days to a few months for clients ranging from local businesses to the Fortune 500', 'Understanding of data-modeling principles']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Lead Data Engineer,BICP,"Portland, OR",2 days ago,Be among the first 25 applicants,"['', '3+ years of experience with data engineering with emphasis on data analytics and reporting', 'Experience developing with scripting languages such as Shell and Python', '3+ years of experience with data engineering with emphasis on data analytics and reportingExperience developing with scripting languages such as Shell and PythonStrong experience developing with PySpark, preferably leveraging AWS EMR managed serviceExpert experience with SQL and Relational database engineering (Oracle, SQL Server, Teradata)- expert-level SQL abilitiesExperience with agile delivery methodologies- Scrum, SAFe, Extreme ProgrammingExperience working with source-code management tools such as GitHub and JenkinsAbility to partner with business and technology team members, to understand business requirements and translate those into value-add technology solutions', 'Strong experience developing with PySpark, preferably leveraging AWS EMR managed service', 'Experience developing solutions in SnowflakeExperience with workload automation tools such as Airflow, Autosys.Kubernetes, Lambda, Spark Streaming', 'Experience with agile delivery methodologies- Scrum, SAFe, Extreme Programming', 'Ability to partner with business and technology team members, to understand business requirements and translate those into value-add technology solutions', 'Experience developing solutions in Snowflake', 'Key Skills', 'About BICP', 'Expert experience with SQL and Relational database engineering (Oracle, SQL Server, Teradata)- expert-level SQL abilities', 'Job Description', 'Kubernetes, Lambda, Spark Streaming', 'Experience working with source-code management tools such as GitHub and Jenkins', 'Experience with workload automation tools such as Airflow, Autosys.', 'Nice To Have']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Arcadia,"Washington, DC",2 weeks ago,77 applicants,"['', 'Nice-to-haves', 'Eliminating carbon footprints, eliminating carbon copies.', 'Extensive experience in managing data pipelines, schemas, and storage for multiple systems for multiple teams', 'Experience in one or more of the following languages: Python, Java, Ruby, Javascript', 'Join a mashup of energy enthusiasts and creative tech wizards who are taking the fight to climate change. Disrupt and reimagine the energy experience using modern technologies.', 'Manage data pipelines from disparate sources, standardizing and feeding them into our centralized data warehouse', 'Experience with entity resolution at scale', 'A supportive engineering culture that values diversity, empathy, teamwork, trust, and efficiency', 'Benefits', 'Paid Time Off (holidays, vacation, professional development, volunteer, parental leave)', 'Experience in predictive modeling and statistical analysis', 'What You’ll Do', 'Significant experience with and a strong understanding of languages/tools relevant to engineering & data teams’ work', '3+ years combined programming and/or DevOps experience', 'A chance to decarbonize and disrupt the energy sector', 'Undergraduate and/or graduate degree in math, statistics, engineering, computer science, or related technical field', 'Database management experience with PostgreSQL, RDS, or Redshift', 'Work with both the Engineering and Analytics & Data Science teams to optimize data flow and queries for large data sets to improve scalability', ' 3+ years combined programming and/or DevOps experience Significant experience with and a strong understanding of languages/tools relevant to engineering & data teams’ work Experience in one or more of the following languages: Python, Java, Ruby, Javascript Advanced knowledge of algorithms, data structures, and relational algebra Database management experience with PostgreSQL, RDS, or Redshift Data extraction experience with a strong understanding of thread-based and event-based paradigms Extensive experience in managing data pipelines, schemas, and storage for multiple systems for multiple teams Strong communication skills ', 'What will help you succeed:', 'Data extraction experience with a strong understanding of thread-based and event-based paradigms', 'Market-based compensation (salary + equity)', 'Sync data across internal and external systems, such as marketing and sales automation tools, to enable key stakeholders to build best-in-class experiences', ' Manage data pipelines from disparate sources, standardizing and feeding them into our centralized data warehouse Work with both the Engineering and Analytics & Data Science teams to optimize data flow and queries for large data sets to improve scalability Sync data across internal and external systems, such as marketing and sales automation tools, to enable key stakeholders to build best-in-class experiences Support ongoing efforts to establish and enforce best practices on data quality, use, and security across the company ', 'Advanced knowledge of algorithms, data structures, and relational algebra', 'Strong communication skills', 'Free clean energy', 'What We’re Looking For', 'Experience with enterprise database interfaces and messaging APIs', 'Experience with Amazon Web Services (AWS) or other cloud infrastructure platforms', 'Must-haves', 'Healthcare, dental, vision, 401(k) and commuter benefits', ' Undergraduate and/or graduate degree in math, statistics, engineering, computer science, or related technical field Experience in predictive modeling and statistical analysis Experience with enterprise database interfaces and messaging APIs Experience with Amazon Web Services (AWS) or other cloud infrastructure platforms Experience with entity resolution at scale Experience in the energy sector ', 'All-company lunches', 'Support ongoing efforts to establish and enforce best practices on data quality, use, and security across the company', 'Experience in the energy sector', 'a 100% clean energy future.', ' Market-based compensation (salary + equity) Healthcare, dental, vision, 401(k) and commuter benefits Paid Time Off (holidays, vacation, professional development, volunteer, parental leave) A supportive engineering culture that values diversity, empathy, teamwork, trust, and efficiency Professional development opportunities All-company lunches Free clean energy A chance to decarbonize and disrupt the energy sector ', 'Professional development opportunities']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Senior Data Engineer,Insticator,"Miami, FL",3 days ago,Be among the first 25 applicants,"['', ' Competitive Salary ', ' Support development of analytics tools that utilize the data pipeline to provide actionable insights into conversion, customer acquisition, operational efficiency and other key business performance metrics ', ' Experience building and optimizing ‘big data’ data pipelines, architectures and datasets. (Apache Hadoop, MapReduce, Hive, Spark, Kafka, etc.) ', ' We sponsor H1B Visas and Green Cards ', ' Strong Experience with AWS cloud services: EC2, EMR, RDS, Redshift ', ' Experience with Snowflake ', ' Excellent social skills with proven ability to overcome objections and form trusting relationships with external clients and internal stakeholders ', ' Thank You ', 'By submitting your interest in this job, you agree to receive text notifications with additional steps to complete your job application. You will receive up to 6 messages from the number ""63879"". Message & data rates may apply. Please refer to our privacy policy for more information.', ' Curious, research-minded, data-informed ', ' Entrepreneurial & adaptable; great learning skills ', ' Unconditional Empathy - Our customers are real people with real business needs, and we are here to listen and tackle accordingly. If we care and respect each other, there is no challenge we can’t overcome. ', ' Creative confidence ', ' Understand the principles of ad serving, analytics, programmatic, RTB / DSPs / SSPs / DMPs ', 'Benefits', ' Competitive Salary  Health, Dental and Vision Insurance (location dependent)  Annual Performance Bonus  Paid Time Off  Stock Options (so you have ownership in the company and benefit as it grows)  Flexible work schedule  401k (only in USA)  We sponsor H1B Visas and Green Cards ', ' Build the infrastructure required for optimal Extraction, Transformation, and Loading (ETL) of data from a wide variety of data sources with SQL-centric ETL paradigm ', ' Experience with NoSQL databases, including MongoDB and ElasticSearch. ', ' Build pipeline for data visualization tools (Looker, Tableau, Sisense, etc.) ', ' Skilled at receiving feedback, as well as providing it ', ' Paid Time Off ', ' Strong communication skills with a proven ability to discuss data, infrastructure, and analytics with technical & non-technical across the organization ', ' Health, Dental and Vision Insurance (location dependent) ', 'Apply for this Position', 'About The Role', ' Working knowledge of AdTech and audience Data Management Platform (DMP) ', ' 5+ years of hands-on experience with creating and maintaining optimal data pipeline architecture and data warehouse, assembling large, complex data sets that meet requirements in a business environment  Advanced working SQL knowledge and working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.  Experience with NoSQL databases, including MongoDB and ElasticSearch.  Experience building and optimizing ‘big data’ data pipelines, architectures and datasets. (Apache Hadoop, MapReduce, Hive, Spark, Kafka, etc.)  Build processes supporting data transformation, data structures, metadata, schema design, dependency and workload management for structured and unstructured datasets  Proven successful record of manipulating, processing and extracting value from large disconnected datasets  Hands-on experience in end-to-end data product developing & implementing cycle and working with cross-functional teams in a dynamic environment.  Build pipeline for data visualization tools (Looker, Tableau, Sisense, etc.)  Experience in building systems using Python, R, Tensorflow, Linux/Unix Bash Scripts. Code must be fault tolerant/resistant  Strong Experience with AWS cloud services: EC2, EMR, RDS, Redshift  Experience with Snowflake  Strong communication skills with a proven ability to discuss data, infrastructure, and analytics with technical & non-technical across the organization ', ' Create system standards to maintain the integrity and security of all databases. ', ' Sleeves Up - At Insticator we provide the autonomy and creativity needed to own your role, iterate where needed and drive impact on a massive scale. ', 'Qualifications', ' Proven successful record of manipulating, processing and extracting value from large disconnected datasets ', ' Monitor, track and identify data quality issues and gaps and apply the necessary action plans to remediate issues. ', ' Stock Options (so you have ownership in the company and benefit as it grows) ', ' Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. ', ' Build processes supporting data transformation, data structures, metadata, schema design, dependency and workload management for structured and unstructured datasets ', ' Experience in building systems using Python, R, Tensorflow, Linux/Unix Bash Scripts. Code must be fault tolerant/resistant ', ' Experience with data pipeline and workflow management tools: Matiliion, Azkaban, Luigi, Airflow, etc ', ' Annual Performance Bonus ', ' Collaborative mindset and great teamwork skills ', ' 401k (only in USA) ', 'Preferred Experience', ' Working knowledge of AdTech and audience Data Management Platform (DMP)  Understand the principles of ad serving, analytics, programmatic, RTB / DSPs / SSPs / DMPs  Experience with data pipeline and workflow management tools: Matiliion, Azkaban, Luigi, Airflow, etc  Experience with stream-processing systems: Storm, Spark-Streaming, etc.  Excellent social skills with proven ability to overcome objections and form trusting relationships with external clients and internal stakeholders ', ' Sleeves Up - At Insticator we provide the autonomy and creativity needed to own your role, iterate where needed and drive impact on a massive scale.  100% Viewability - Insticator is passionate about open feedback at all levels of the company. This allows us to fail fast, create in real time and build an open company culture.  Be Defiantly Great - We are defiant, that’s in our lifeblood, we accomplish what other people think are impossible. Challenging the status quo is our lifeblood.  Unconditional Empathy - Our customers are real people with real business needs, and we are here to listen and tackle accordingly. If we care and respect each other, there is no challenge we can’t overcome. ', ' 5+ years of hands-on experience with creating and maintaining optimal data pipeline architecture and data warehouse, assembling large, complex data sets that meet requirements in a business environment ', ' Flexible work schedule ', ' Advanced working SQL knowledge and working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases. ', ' Be Defiantly Great - We are defiant, that’s in our lifeblood, we accomplish what other people think are impossible. Challenging the status quo is our lifeblood. ', ' Build the infrastructure required for optimal Extraction, Transformation, and Loading (ETL) of data from a wide variety of data sources with SQL-centric ETL paradigm  Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.  Support development of analytics tools that utilize the data pipeline to provide actionable insights into conversion, customer acquisition, operational efficiency and other key business performance metrics  Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs  Create system standards to maintain the integrity and security of all databases.  Monitor, track and identify data quality issues and gaps and apply the necessary action plans to remediate issues. ', ' Creative confidence  Collaborative mindset and great teamwork skills  Skilled at receiving feedback, as well as providing it  Entrepreneurial & adaptable; great learning skills  Transparent & communicative, patient  Curious, research-minded, data-informed ', 'About The Company', ' 100% Viewability - Insticator is passionate about open feedback at all levels of the company. This allows us to fail fast, create in real time and build an open company culture. ', ' Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs ', ' Transparent & communicative, patient ', ' Experience with stream-processing systems: Storm, Spark-Streaming, etc. ', ' Hands-on experience in end-to-end data product developing & implementing cycle and working with cross-functional teams in a dynamic environment. ']",Associate,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
Data Engineer II,Niagara Bottling,"Los Angeles, CA",6 days ago,Be among the first 25 applicants,"['', 'Create data flows, dashboards, and automated reports that present data to plant staff and management.', '2 Years – Experience in Field or similar manufacturing environment2 Years – Experience managing people/projects2 Years of experience with C++ and JavaScriptPrior Cloud Solutions experience', 'Grafana', 'Knowledge of algorithms and data structures.', 'Strong exposure to Manufacturing Technologies including automated data collection, visualization, quality, efficiency in manufacturing, automated decision control, workflow, database applications, scheduling, and interface to ERP systems.Proficiency in, but not limited to:Microsoft Office Applications – Word, Excel, Access, PowerPoint, Outlook, Projects, Visio, etc.Oracle Business SystemsPythonRSQLData APIsAzureJavaScriptC++GrafanaEfficiency Improvement Analysis ApplicationsNew System Development/Enhancement & AdministrationAble to translate data into recommendable actions to senior staffStrong analytical and problem solving skillsSelf-Motivated with a proven record of taking the initiativeAble to work with minimal supervisionAble to execute tasks in a very dynamic and ever-changing environmentExercise sound judgment and ability to work effectively with a diverse workforceHadoop ecosystem (HDFS / MapReduce etc)Knowledge in relational database designETL / Streaming technologies like Apache Spark, PySpark, Kafka etcExperience with performance tuning of ETL jobs.Experience developing flexible ontologies to fit data from multiple sources and implementing the ontology in the form of database mappings / schemas.', 'Develop lasting relationships with great people. ', 'Able to work with minimal supervision', 'Recognize potential problems with existing data architecture and develop solutions with the ability to adapt to applications and data requirements.', 'Exercise sound judgment and ability to work effectively with a diverse workforce', 'Employment agencies that have fee agreements with Niagara Bottling, LLC and have been engaged on a search shall submit résumé to the designated Niagara Bottling, LLC recruiter or, upon authorization, submit résumé into this career site to be eligible for placement fees.', 'This job is intended to be performed entirely outside of Colorado', 'Work with internal stakeholders to identify their requirements and develop innovative solutions that meet and exceed these needs.', 'Python', 'New System Development/Enhancement & Administration', 'Work in an entrepreneurial and dynamic environment with a chance to make an impact. Develop lasting relationships with great people. Have the opportunity to build a satisfying career.', 'Hadoop ecosystem (HDFS / MapReduce etc)', 'Essential Functions', '2 Years of experience with C++ and JavaScript', 'Benefits', 'Consider Applying Here, If You Want To', '4 Years – Experience in Field or similar manufacturing environment ', 'Minimum Qualifications:2 Years – Experience in Field or similar manufacturing environment2 Years – Experience managing people/projects2 Years of experience with C++ and JavaScriptPrior Cloud Solutions experience', 'Data APIs', 'R', '2 Years – Experience in Field or similar manufacturing environment', 'Prior Cloud Solutions experience', 'Self-Motivated with a proven record of taking the initiative', 'Education', ""Minimum Required:Bachelor's Degree with a preference in Computer Science, Software, or Computer Engineering."", 'Strong exposure to Manufacturing Technologies including automated data collection, visualization, quality, efficiency in manufacturing, automated decision control, workflow, database applications, scheduling, and interface to ERP systems.', 'Proficiency in, but not limited to:Microsoft Office Applications – Word, Excel, Access, PowerPoint, Outlook, Projects, Visio, etc.Oracle Business SystemsPythonRSQLData APIsAzureJavaScriptC++GrafanaEfficiency Improvement Analysis ApplicationsNew System Development/Enhancement & Administration', 'Preferred Qualifications:4 Years – Experience in Field or similar manufacturing environment 4 Years – Experience managing people/projects', 'SQL', ""experience may include a combination of work experience and educationEducationMinimum Required:Bachelor's Degree with a preference in Computer Science, Software, or Computer Engineering.Preferred:Master's Degree in Computer Science, Software, or Computer Engineering.Benefitshttps://careers.niagarawater.com/us/en/benefitsThis job is intended to be performed entirely outside of ColoradoAny employment agency, person or entity that submits a résumé into this career site or to a hiring manager does so with the understanding that the applicant's résumé will become the property of Niagara Bottling, LLC. Niagara Bottling, LLC will have the right to hire that applicant at its discretion without any fee owed to the submitting employment agency, person or entity.Employment agencies that have fee agreements with Niagara Bottling, LLC and have been engaged on a search shall submit résumé to the designated Niagara Bottling, LLC recruiter or, upon authorization, submit résumé into this career site to be eligible for placement fees."", ""Bachelor's Degree with a preference in Computer Science, Software, or Computer Engineering."", 'Qualifications', 'Perform data analysis using software tools to model and develop new understanding of company opportunities.', 'C++', 'Strong analytical and problem solving skills', 'Please note this job description is not designed to contain a comprehensive list of activities, duties or responsibilities that are required of the employee for this job.', 'Build large-scale data analytics systems in our cloud solution.', 'Microsoft Office Applications – Word, Excel, Access, PowerPoint, Outlook, Projects, Visio, etc.Oracle Business SystemsPythonRSQLData APIsAzureJavaScriptC++GrafanaEfficiency Improvement Analysis ApplicationsNew System Development/Enhancement & Administration', 'Able to execute tasks in a very dynamic and ever-changing environment', ""Preferred:Master's Degree in Computer Science, Software, or Computer Engineering."", 'Knowledge in relational database design', 'Experience developing flexible ontologies to fit data from multiple sources and implementing the ontology in the form of database mappings / schemas.', ""Any employment agency, person or entity that submits a résumé into this career site or to a hiring manager does so with the understanding that the applicant's résumé will become the property of Niagara Bottling, LLC. Niagara Bottling, LLC will have the right to hire that applicant at its discretion without any fee owed to the submitting employment agency, person or entity."", ""Master's Degree in Computer Science, Software, or Computer Engineering."", ""experience may include a combination of work experience and educationPreferred Qualifications:4 Years – Experience in Field or similar manufacturing environment 4 Years – Experience managing people/projectsexperience may include a combination of work experience and educationEducationMinimum Required:Bachelor's Degree with a preference in Computer Science, Software, or Computer Engineering.Preferred:Master's Degree in Computer Science, Software, or Computer Engineering.Benefitshttps://careers.niagarawater.com/us/en/benefitsThis job is intended to be performed entirely outside of ColoradoAny employment agency, person or entity that submits a résumé into this career site or to a hiring manager does so with the understanding that the applicant's résumé will become the property of Niagara Bottling, LLC. Niagara Bottling, LLC will have the right to hire that applicant at its discretion without any fee owed to the submitting employment agency, person or entity.Employment agencies that have fee agreements with Niagara Bottling, LLC and have been engaged on a search shall submit résumé to the designated Niagara Bottling, LLC recruiter or, upon authorization, submit résumé into this career site to be eligible for placement fees."", '4 Years – Experience in Field or similar manufacturing environment 4 Years – Experience managing people/projects', 'Design, modify, and deploy data pipelines for various software’s.', 'Able to translate data into recommendable actions to senior staff', 'Competencies', 'Efficiency Improvement Analysis Applications', 'Maintain technical skills and knowledge and keep them continuously updated.', 'Azure', 'Experience with performance tuning of ETL jobs.', '2 Years – Experience managing people/projects', 'JavaScript', 'Identify limitations of current systems while identifying enhancement opportunities to provide solutions.', 'Design, modify, and deploy data pipelines for various software’s.Recognize potential problems with existing data architecture and develop solutions with the ability to adapt to applications and data requirements.Create data flows, dashboards, and automated reports that present data to plant staff and management.Perform data analysis using software tools to model and develop new understanding of company opportunities.Work with internal stakeholders to identify their requirements and develop innovative solutions that meet and exceed these needs.Knowledge of algorithms and data structures.Build large-scale data analytics systems in our cloud solution.Maintain technical skills and knowledge and keep them continuously updated.Develop data lakes and data warehouses for Niagara to meet the needs of our analytical and data science teams.Identify limitations of current systems while identifying enhancement opportunities to provide solutions.Please note this job description is not designed to contain a comprehensive list of activities, duties or responsibilities that are required of the employee for this job.', 'Develop data lakes and data warehouses for Niagara to meet the needs of our analytical and data science teams.', 'Work in an entrepreneurial and dynamic environment with a chance to make an impact. ', 'Microsoft Office Applications – Word, Excel, Access, PowerPoint, Outlook, Projects, Visio, etc.', 'Have the opportunity to build a satisfying career.', 'experience may include a combination of work experience and educationPreferred Qualifications:4 Years – Experience in Field or similar manufacturing environment 4 Years – Experience managing people/projects', 'ETL / Streaming technologies like Apache Spark, PySpark, Kafka etc', '4 Years – Experience managing people/projects', 'Oracle Business Systems']",Entry level,Full-time,Information Technology,Food & Beverages,2021-03-18 14:34:51
Data Engineer Intern,Label Insight,"St Louis, MO",3 days ago,Be among the first 25 applicants,"['', 'We Share a Common Purpose', 'Who You Are', 'About The Engineering Team', 'The Perks Of Joining Us This Summer', ""How You'll Contribute"", 'At The End Of This Internship You Will Have', 'We Live Our Values', 'At Label Insight we embrace diversity and are committed to building a team that represents a variety of backgrounds and qualifications. We’re proud to be an Equal Opportunity Employer']",Internship,Full-time,Information Technology,Computer Software,2021-03-18 14:34:51
Lead Data Engineer – Data Pipeline project,NextPhase.ai,"San Francisco, CA",23 hours ago,39 applicants,"['7. Kubernetes, EKS, API Development', 'Duration: Long Term Contract', '6. workflow management tools: Azkaban, Luigi, Airflow, etc.', '3. AWS cloud services: S3, Athena, RDS, EC2, EMR, RDS, Lambda,', 'Job Title: Senior Data Engineer - Data Pipeline', '5. Big data tools: Hadoop, Spark, Kafka, etc.', 'Location: San Francisco, CA', '1. ETL Tools such as Apache Airflow, AWS Data Pipeline, AWS Glue, Talend', '2. Data Warehouse solution - Redshift, Snowflake', '4. scripting languages: Python, Java, Scala, etc', 'Job Description:', 'Create and maintain a data pipeline for automating data ingestion from multiple data sources']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer II - Data Engineering Team,Premera Blue Cross,"Mountlake Terrace, WA",3 weeks ago,Be among the first 25 applicants,"['', ' Perform thorough peer design and code reviews ', ' Experience with software development lifecycle, relational database theory, and skills to utilize one or more programming languages ', 'Life and disability insurance', 'Generous Paid Time Off to reenergize', 'Free parking', 'What You’ll Bring', ' Strong data processing programming skills across SQL-based and Hadoop-based technologies ', 'What we offer', ' Familiarity with healthcare specific regulatory requirements for data management ', ' 2 years’ experience in data integration, design, and management (Required) ', ' Work closely with the business to create understanding of the needs, pace, and direction for our business partners, translating these needs into requirements and specifications, and maintaining contact with the customers through project completion ', ' Solve business and data engineering problems using data centric programming and scripting skills to create data models and pipelines  Work closely with the business to create understanding of the needs, pace, and direction for our business partners, translating these needs into requirements and specifications, and maintaining contact with the customers through project completion  Troubleshoot issues as they arise and solve problems independently and collaboratively  Collaborate with data scientists and analyst to further understand business problems  Develop code to complete effective solutions using applicable technology  Perform thorough peer design and code reviews  Use developing data ETL experience to develop data pipelines to support data product automation ', ' Knowledge of Tableau, SAS, R, and other analytic tools ', 'What You’ll Do', 'Data Engineer II', 'Medical, vision and dental coverageLife and disability insuranceRetirement programs (401K employer match and pension plan)Wellness incentives, onsite services, a discount program and moreTuition assistance for undergraduate and graduate degreesGenerous Paid Time Off to reenergizeFree parking', 'Wellness incentives, onsite services, a discount program and more', ' Ability to use Kimball methodology for dimensional data modeling, 3rd Normal form DW ', ' A Bachelor’s degree in computer science, computer engineering, or similar area (Required) ', 'Tuition assistance for undergraduate and graduate degrees', 'Join Our Team: Do Meaningful Work and Improve People’s Lives ', ' Develop code to complete effective solutions using applicable technology ', 'Data Engineering (Corporate Data and Analytics)', 'Retirement programs (401K employer match and pension plan)', ' Collaborate with data scientists and analyst to further understand business problems ', ' Ability to use Extract Transform Load (ETL) tools (SSIS, Data Stage, Cask) ', ' Troubleshoot issues as they arise and solve problems independently and collaboratively ', ' Use developing data ETL experience to develop data pipelines to support data product automation ', '(Required)', ' A Bachelor’s degree in computer science, computer engineering, or similar area (Required)  2 years’ experience in data integration, design, and management (Required)  Experience with software development lifecycle, relational database theory, and skills to utilize one or more programming languages  Familiarity with healthcare specific regulatory requirements for data management  Experience providing data integration services within healthcare organizations  Knowledge of Tableau, SAS, R, and other analytic tools  Strong data processing programming skills across SQL-based and Hadoop-based technologies  Ability to use Extract Transform Load (ETL) tools (SSIS, Data Stage, Cask)  Ability to use Kimball methodology for dimensional data modeling, 3rd Normal form DW ', 'Equal employment opportunity/affirmative action:', ' Solve business and data engineering problems using data centric programming and scripting skills to create data models and pipelines ', ' Experience providing data integration services within healthcare organizations ', 'Medical, vision and dental coverage']",Not Applicable,Full-time,Information Technology,Insurance,2021-03-18 14:34:51
Data Engineer,GNS Healthcare,"Somerville, MA",4 weeks ago,Be among the first 25 applicants,"['', 'Background in statistics, biostatistics, public health, research design, health economics, or other related quantitative healthcare field. Experience with backend web (API) development, Kubernetes, Docker, and Tableau Experience developing data framesExperience with data processing frameworks such as Apache Spark, Beam, Dataflow, Crunch, Scalding, Storm, Hive and BigQuery Experience extracting data from REST APIs and parallel processing large datasets', 'An expert in cloud data warehousing tools (e.g. Snowflake, Amazon RedShift, BigQuery, Microsoft SQL Server, Oracle, PostgreSQL, or equivalent) and ELT tools (e.g. Stitch, Fivetran, DBT, Glue). You thrive on building modern, cloud-native data pipelines and operations.', '3-5 years of data engineering experience with a thorough understanding of data lake architectures.An expert in cloud data warehousing tools (e.g. Snowflake, Amazon RedShift, BigQuery, Microsoft SQL Server, Oracle, PostgreSQL, or equivalent) and ELT tools (e.g. Stitch, Fivetran, DBT, Glue). You thrive on building modern, cloud-native data pipelines and operations.Fluent in SQL scripting Experience working in healthcare, life sciences, and/or with diverse healthcare data sets (e.g. medical, pharmacy claims, and lab results) Experience with industry standard measures and code sets (e.g. ICD-10 codes, ICD-9 codes, HCPCS codes, CPT codes, HEDIS metrics, ETGs, HCCs, DRGs, etc.) and publicly available sources (e.g. HCUP) You thrive on mapping and designing ingestion and transformation of data from multiple sources, creating a cohesive data asset and Common Data Model (CDM)Experience using a scripting language (Python, R, Java, Scala, C++, C# and Bash/PowerShell) to automate processes and support proprietary software. Machine learning, R, and python skills highly preferred. Experience using Git/Bitbucket and working on shared code repositoriesExperience using Tableau or other in-app data visualization platforms ', 'Equal Employment Opportunity', 'Company Culture', 'Provides complete documentation and communication of all processes, methods, and results. ', 'Experience using Tableau or other in-app data visualization platforms ', 'Assists team members with the design and development of RWD analyses and predictive models. ', 'Responsibilities', 'Background in statistics, biostatistics, public health, research design, health economics, or other related quantitative healthcare field. ', 'Nice To Have Skills', '3-5 years of data engineering experience with a thorough understanding of data lake architectures.', 'Experience with data processing frameworks such as Apache Spark, Beam, Dataflow, Crunch, Scalding, Storm, Hive and BigQuery ', 'Qualifications', 'You thrive on mapping and designing ingestion and transformation of data from multiple sources, creating a cohesive data asset and Common Data Model (CDM)', 'Experience using a scripting language (Python, R, Java, Scala, C++, C# and Bash/PowerShell) to automate processes and support proprietary software. Machine learning, R, and python skills highly preferred. ', 'Experience working in healthcare, life sciences, and/or with diverse healthcare data sets (e.g. medical, pharmacy claims, and lab results) ', 'Experience developing data frames', 'Assists team members with the design and development of RWD analyses and predictive models. Designs and builds consistent, reproducible, and testable ETL pipelines to ingest, normalize, and store data from large healthcare datasets, from clinical trials, to claims, and EHRs.Supports projects including specific epidemiology, health outcomes and other observational studies to better understand disease natural history, prevalence, comorbidities, treatment patterns, and health and safety outcomes in ‘real world’ patient populations. Functions as a healthcare data subject matter expert to support the design, development, testing, implementation, and support of clinical information and intelligence solutions. Provides complete documentation and communication of all processes, methods, and results. Supports production solutions, the ongoing updates, and maintenance of our reference data sources. ', 'Experience extracting data from REST APIs and parallel processing large datasets', 'Functions as a healthcare data subject matter expert to support the design, development, testing, implementation, and support of clinical information and intelligence solutions. ', 'Experience using Git/Bitbucket and working on shared code repositories', 'Supports production solutions, the ongoing updates, and maintenance of our reference data sources. ', 'Experience with industry standard measures and code sets (e.g. ICD-10 codes, ICD-9 codes, HCPCS codes, CPT codes, HEDIS metrics, ETGs, HCCs, DRGs, etc.) and publicly available sources (e.g. HCUP) ', 'Supports projects including specific epidemiology, health outcomes and other observational studies to better understand disease natural history, prevalence, comorbidities, treatment patterns, and health and safety outcomes in ‘real world’ patient populations. ', 'in silico ', 'Designs and builds consistent, reproducible, and testable ETL pipelines to ingest, normalize, and store data from large healthcare datasets, from clinical trials, to claims, and EHRs.', 'Company Overview', 'Fluent in SQL scripting ', 'Experience with backend web (API) development, Kubernetes, Docker, and Tableau ']",Mid-Senior level,Full-time,Information Technology,Biotechnology,2021-03-18 14:34:51
Principal Data Engineer,Neudesic,"Irvine, CA",2 weeks ago,47 applicants,"['', 'Do you love what you do and are disciplined in your approach? Are you someone who gets excited about working on innovative projects? Do you enjoy collaborating with others and pride yourself in honest work? If so, then we’d definitely like to hear from you. What sets us apart from the rest, is an amazing collection of people who live and lead with our core values – PDITI: Passion, Discipline, Innovation, Teaming, Integrity.', ""Neudesic is seeking a Principal Engineer, Business Intelligence to join our internal IT team and lead our transformation to becoming a predictive enterprise. In this role, you will be responsible for establishing clear strategic vision, providing leadership to our data and analytics team, and contributing to the implementation of our company's analytics platform. In addition to technical capabilities, the qualified individual will also need consultative expertise to collaborate with our stakeholders and solve complex business problems."", 'Visual Studio 2019', 'Define a Predictive Enterprise data platform strategy, establish a reporting architecture and provide corresponding roadmaps', 'Collaborate with business stakeholders to define information needs and develop business cases and priorities', 'Participate in data governance processes required to ensure data integrity, classification and security', 'Azure SQL Server 2019', 'Lead the reporting team to establish the ongoing development and operations of a business intelligence architecture that enables fact-based decision making and ad hoc analysis and insight', 'Neudesic is an Equal Employment Opportunity Employer', 'Power BI Development', 'Azure DevOps/Pipelines CICD', 'Core Technical Proficiencies Required:', 'OLTP & OLAP Data Models', 'Define a Predictive Enterprise data platform strategy, establish a reporting architecture and provide corresponding roadmapsCollaborate with business stakeholders to define information needs and develop business cases and prioritiesLead the reporting team to establish the ongoing development and operations of a business intelligence architecture that enables fact-based decision making and ad hoc analysis and insightCreate and communicate a clear and compelling strategy of a Business Intelligence and Predictive Analytics vision to team members and business stakeholdersParticipate in data governance processes required to ensure data integrity, classification and securityMentor and lead team members in the rigors of Data Engineering, Data Science and Business Analytics.', 'SSAS Development', 'Agile/SCRUM', 'Role Profile', 'Visual Studio 2019Azure DevOps/Pipelines CICDAzure SQL Server 2019OLTP & OLAP Data ModelsMachine LearningSSIS DevelopmentSSRS DevelopmentSSAS DevelopmentPower BI DevelopmentExcel Power Pivot DevelopmentAgile/SCRUMDatabase Administration', 'About Neudesic', 'All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.', 'SSIS Development', 'SSRS Development', 'Excel Power Pivot Development', 'Mentor and lead team members in the rigors of Data Engineering, Data Science and Business Analytics.', 'Database Administration', '\xa0', 'Responsibilities and Duties:', 'Machine Learning', 'Create and communicate a clear and compelling strategy of a Business Intelligence and Predictive Analytics vision to team members and business stakeholders']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Senior Data Engineer - Work From Home,PLUM Commercial Real Estate Lending,"San Francisco, CA",2 days ago,Be among the first 25 applicants,"['', ' Job Summary ', ' Experience in working with graph database such as TigerGraph or Neo4j', ' Autonomy, flexibility and a flat corporate structure.', ' High proficiency in Java', ' Participate heavily in architecture, design and implementation of machine learning pipelines', ' Development of data-driven products.', ' Conscientious and well organized', ' Experience with finance technology/analytics and commercial real estate', ' Hands-on Industry experience with machine learning frameworks like SageMaker, TensorFlow', ' Experience with data visualization tools like Tableau', ' Bachelor’s degree in Mathematics, Statistics, Engineering, Computer Science or related discipline 2+ years of production-level machine learning model deployment Experience in working with big data technologies Spark, MapReduce, NoSQL databases Experience with AWS infrastructure such as EC2, S3 and Glue Hands-on Industry experience with machine learning frameworks like SageMaker, TensorFlow High proficiency in Java Knowledge of both neural networks and traditional ML techniques Experience with data visualization tools like Tableau An entrepreneurial spirit as well as passion for solving difficult challenges through innovation and creativity, with a strong focus on results Conscientious and well organized Eager to produce results and drive forward progress while managing deadlines', ' Eager to produce results and drive forward progress while managing deadlines', ' Experience in working with graph database such as TigerGraph or Neo4j Have worked in a fast paced startup environment Experience with finance technology/analytics and commercial real estate', ' Benefits ', ' Develop, refine and scale data management and analytics procedures, systems, workflows, best practices and other issues.', ' Have worked in a fast paced startup environment', ' Generous health, dental and vision coverage for employees and family members, along with commuter pre-tax program.', ' Chance for your direct input to be realized and put into action.', ' Collaborate with our engineers to produce excellent products for Plum clients as well as streamline internal processes.', ' Early equity in a startup that is revolutionizing commercial real estate lending.', ' Knowledge of both neural networks and traditional ML techniques', ' An entrepreneurial spirit as well as passion for solving difficult challenges through innovation and creativity, with a strong focus on results', ' About Plum ', 'Must be authorized to work in the US. Plum does not sponsor employee visas.', ' Freedom to stretch the boundaries of your past work experience, learn skills outside of your immediate job description and grow your career.', ' Unlimited vacation policy.', ' Experience in working with big data technologies Spark, MapReduce, NoSQL databases', ' Opportunity to make a meaningful impact on the disruption of an industry and to shape the building of a company and culture.', ' 2+ years of production-level machine learning model deployment', ' Experience with AWS infrastructure such as EC2, S3 and Glue', ' Bachelor’s degree in Mathematics, Statistics, Engineering, Computer Science or related discipline', ' Generous health, dental and vision coverage for employees and family members, along with commuter pre-tax program. Unlimited vacation policy. Opportunity to make a meaningful impact on the disruption of an industry and to shape the building of a company and culture. Chance for your direct input to be realized and put into action. Freedom to stretch the boundaries of your past work experience, learn skills outside of your immediate job description and grow your career. Autonomy, flexibility and a flat corporate structure.', ' Responsibilities ', ' Required Qualifications ', ' Preferred Qualifications ', ' Participate heavily in architecture, design and implementation of machine learning pipelines Develop, refine and scale data management and analytics procedures, systems, workflows, best practices and other issues. Development of data-driven products. Visualize and communicate data clearly for use both internally and externally. Collaborate with our engineers to produce excellent products for Plum clients as well as streamline internal processes.', ' Visualize and communicate data clearly for use both internally and externally.']",Associate,Full-time,Information Technology,Banking,2021-03-18 14:34:51
AWS Data Engineer,HTC Global Services,"Bloomington, IL",7 days ago,Be among the first 25 applicants,"['', 'HTC – A brief profile', 'Experience in programming languages like Python or spark.Must have experience in GLUE/ EMR, Athena and Redshift, S3, Lambda, API Gateway, and KinesisGood to have AWS QuickSight, and DynamoDB.Need communication and requirement consumption skills with the ability to identify appropriate solutions.The team will be designing and developing event processing and data collection, translation, formatting, and enhancement activities within and outside of company boundaries.', 'Candidates should e-mail their resume to Kaveriselvan.Shankar@HTCinc.com or call at (248) 817-1095', 'EEO/M/F/V/H\xa0', 'About the Job', 'Skill Requirements:', 'HTC’s competitive package includes besides compensation Health, Dental, Vision, Disability Cover, both Short and Long term, Life Insurance, Flexible Spending, 401k and Paid Vacation', '\xa0', 'Need communication and requirement consumption skills with the ability to identify appropriate solutions.', 'The team will be designing and developing event processing and data collection, translation, formatting, and enhancement activities within and outside of company boundaries.', 'Good to have AWS QuickSight, and DynamoDB.', 'Established in 1990, HTC Inc., a CMM Level 5 company with headquarters in Troy, Michigan, is a leading global Information Technology solution and BPO provider. HTC assists clients across multiple industry verticals, offering turnkey project lifecycle in, e-business, data warehousing, embedded systems, ECM, SCM, CRM, and ERP solutions. HTC Inc. offers ConnectIT, our Global Delivery Methodology that enables seamlessly delivery of outsourced IT services. HTC has global delivery centers across the globe.', 'Benefits:', 'Be sure to reference the job number and title in the subject line. A Relevant degree or its foreign equivalent is required.', 'Experience in programming languages like Python or spark.', 'Must have experience in GLUE/ EMR, Athena and Redshift, S3, Lambda, API Gateway, and Kinesis', 'We are currently looking for an AWS Data Engineer. We will consider relocation for candidates with exceptional skills.']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer - BS/MS,Procter & Gamble,"Cincinnati, OH",1 week ago,Be among the first 25 applicants,"['', ' A BS or MS in Computer Science, Computer or Electrical Engineering, but we also consider other, similar engineering degree ', 'The Ideal Candidate', ' History of working independently and effectively multi-tasking ', ' Evaluate tools and develop pipelines to capture, integrate and clean data to support edge analytics solutions ', ' Strong data wrangling skills ', ' Strong interpersonal communication and collaboration skills  History of working independently and effectively multi-tasking  Familiarity with machine learning workflows (desirable)  Have experience with sensors and IoT cloud architecture (desirable)  Familiarity with RESTful Application Programming Interface (API), containers and microservices  Familiarity with data privacy and data governance  Experience with NoSQL databases ', ' A BS or MS in Computer Science, Computer or Electrical Engineering, but we also consider other, similar engineering degree  Overall G.P.A. of 3.0 or above on a 4.0 scale ', ' Overall G.P.A. of 3.0 or above on a 4.0 scale ', ' Deliver optimal data solution architectures, automation and technology choices starting from experimentation through proof of concept and often through delivery ', 'We Are Also Looking For Someone Who Has', ' Familiarity with data privacy and data governance ', ' Have experience with sensors and IoT cloud architecture (desirable) ', 'Qualifications', 'Description', ' Experience with NoSQL databases ', ' Strong interpersonal communication and collaboration skills ', 'In this role you will:', ' Strong problem-solving skills paired with extensive experience programming (Python, Java, C++, etc...) ', ' Familiarity with machine learning workflows (desirable) ', ' Strong problem-solving skills paired with extensive experience programming (Python, Java, C++, etc...)  Strong data wrangling skills  Hands on experience with relational databases and the use of SQL to extract and manipulate data  Experience with cloud services (AWS, Azure or GCP) ', ' Develop and maintain scalable data pipelines that will ingest, transform, and distribute numerous data streams and batches in support of key R&D initiatives  Support and collaborate with Data Scientists developing advanced machine learning and statistical models  Evaluate tools and develop pipelines to capture, integrate and clean data to support edge analytics solutions  Deliver optimal data solution architectures, automation and technology choices starting from experimentation through proof of concept and often through delivery ', ' Familiarity with RESTful Application Programming Interface (API), containers and microservices ', ' Develop and maintain scalable data pipelines that will ingest, transform, and distribute numerous data streams and batches in support of key R&D initiatives ', ' Experience with cloud services (AWS, Azure or GCP) ', ' Hands on experience with relational databases and the use of SQL to extract and manipulate data ', ' Support and collaborate with Data Scientists developing advanced machine learning and statistical models ', ' If you want to join us, you will need: ']",Not Applicable,Full-time,Research,Consumer Goods,2021-03-18 14:34:51
Data Engineer,Vivid Resourcing Ltd,Greater Boston,4 weeks ago,90 applicants,"['', 'Machine learning experience is a bonus!', 'Visa: ', '401(k)', 'We are using next-gen, cutting-edge tech to give our clients instant access to critical data which impacts decision making within a billion-dollar industry. Thanks to us, our clients are maximizing profitability using AI and computer vision.', 'Scala (1 year)', '135-140k base salary', 'Visa: Visa sponsorship is not available for this position', '(Remote until safe to return to office)', 'The Data Engineer will work in collaboration with our Chief Engineer and data scientists, to design, architect, build and maintain the data pipelines that we use to measure and analyze data.', 'Unlimited PTO', 'Experience designing and building ETL data pipelines', 'Full-time', 'Visa sponsorship is not available for this position', 'AWS (EC2, S3, EMR, Lambda, Redshift)', ""Python (4+ years)Scala (1 year)Apache Spark, Kafka, or FlinkAWS (EC2, S3, EMR, Lambda, Redshift)Relational databases such as SQL/PostgreSQLExperience designing and building ETL data pipelines3-5 years' professional experience in a data/software engineering-focused role.Machine learning experience is a bonus!"", 'Relational databases such as SQL/PostgreSQL', 'Location:\xa0Boston (Remote until safe to return to office)\xa0', 'Location:', 'If you are a Data Engineer seeking a new, exciting opportunity, please apply here, and send your most recent resume\xa0to joel.roberts@vividresourcing.com', 'Stock options + performance bonus', 'Data Engineer', 'joel.roberts@vividresourcing.com', '\xa0', 'Python (4+ years)', '135-140k base salaryStock options + performance bonus401(k)Unlimited PTO', 'Required Skills/Experience:', 'Apache Spark, Kafka, or Flink', ""3-5 years' professional experience in a data/software engineering-focused role."", 'We are offering:']",Mid-Senior level,Full-time,Information Technology,Broadcast Media,2021-03-18 14:34:51
Data Engineer,Lorien,"Palo Alto, CA",2 weeks ago,174 applicants,"['', '* Native or near-native fluency in Spanish and/or French.', 'Disqualifiers/Red Flags:', '* Native or near-native fluency in US English.', 'What makes this role interesting and why would a potential candidate choose to accept this position with your team versus another opportunity', ' ', '* Hands-on experience with language annotation, speech analysis, transcription, and other forms of data markup.', 'under applicable law.\xa0', '* Comfortable working in a fast-paced, highly collaborative, and dynamic work environment.', '* Excellent knowledge of aspects of linguistics analysis, particularly on semantics, pragmatics, and phonetics/phonology.', 'Lorien is an Equal Opportunity Employer - All qualified applicants will receive', '* Experience with language annotation and other forms of data markup.', '* 2 years of experience in computational linguistics and natural language processing.', 'age, disability, veteran status, or any other factor determined to be unlawful', 'What makes this role interesting and why would a potential candidate choose to accept this position with your team versus another opportunity?', '* Experience with machine learning, statistical language modeling.', 'The AI Data Team is responsible for delivering high-quality training data to ensure the best performance of the machine learning systems. Our goal is to produce the highest quality training data in the industry and to delight our customers by improving human language understanding and natural language processing.', '* Develop scripts and tools to crawl and process data.', '9-month contract with likely extensions / possible conversion to FTE ', 'In this role, you will:', ""* Bachelor's or Master's degree in Linguistics, Computational Linguistics, or a relevant field."", 'U.S. Citizens, Green Card Holders, and those', '* Familiarity with annotation tools, crowdsourcing platforms and workflow (e.g. MTURK, SageMaker Ground Truth)', 'REQUIRED SKILLS ', '* Manage and process large amounts of linguistic data, in both spoken and written modalities.', '• No Linguistics experience is a disqualifier. ', 'consideration without regard to race, color, religion, gender, national origin,', '* Advocate for strict adherence to transcription and annotation guidelines.', 'Disqualifiers/Red Flags', '* Collaborate with other Language Engineers and Scientists in defining metrics, guidelines, and workflows.', '* Oversee the quality of linguistic annotation and speech transcription.', '* Adopt and design quality control metrics and methodology to evaluate the quality of data.', '* Excellent communication, strong organizational skills with a keen eye for details.', '• Bachelor’s degree in a field not related to the above listed job role and requirements (i.e. Linguistics, Computational Linguistics, Speech, or a relevant field.)', 'SERVICES TO BE PERFORMED', '* Use existing language processing tools to bootstrap and process data.', 'Lorien', 'Health, Dental, Vision and 401k benefits are available to choose from', 'PREFERRED SKILLS', '• Lot of opportunity to learn and grow and influence the business. ', '* Proficient in one or more scripting languages (e.g., Python, Perl, shell) and regular expressions.', 'authorized to work in the U.S for any employer will be considered.', '* 4+ years’ experience in computational linguistics, language data processing, syntax, and semantics.', 'Language Engineer (needs to be fluent in English AND Spanish and/or French)\xa0', '* Practical knowledge of version control and agile development.', '\xa0', '•\xa0Candidate will have experience working on different processes on data using multiple languages.\xa0They are still developing new tooling and processes and this is a great area for growth.\xa0The candidate will be able to contribute to new developments and help solve problems. ', '• Absolutely no experience would be a disqualifier. ', 'Will work with spoken and written data, bootstrap and process data, develop scripts.\xa0Needs at least a bachelor’s in Linguistics/Computational Linguistics/other relevant field, 4 years in field doing analysis/semantics/phonology.\xa0Experience with crowdsourcing tools (mTurk or SageMaker).\xa0Needs scripting in Python/Perl or Shell. Top: 4 years in Comp Linguistics and Natural Language Processing, phonetics/speech/audio files, degree in the field.', 'We are looking for an experienced and motivated Language Engineer with strong analytical skills and language technology experience to help us develop language components for a variety of AWS products. The successful candidate will have strong knowledge in natural language processing, linguistics, and data analysis.', '* Willingness to support several projects at one time and to accept reprioritization as necessary.']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Platform Engineer,"DynPro, Inc.","Ontario, CA",2 days ago,Be among the first 25 applicants,"['', 'Build Enterprise data lake/hub(Raw, trusted/Curated, Data Provisioning Layer), Data Warehouse (Subject Areas, Logical and Physical Data marts, and Data Labs) for Reporting and Digital Invocation capabilities.', 'Knowledge of file standards (AVRO, Parquet, ORC, etc.), HIVE Meta stores, SPARK, Scala, Python & other process for persistent Data Frames to access data.', 'Experience with data warehouses, data mart creation, and data mart access control and data provisioning', 'All other relevant duties as assigned.', 'Experience in implementing the Enterprise Data Models, Business Data Models, Logical and Physical Data Marts Models, and Sandboxes', 'Unit testing, promote tested pipelines to UAT, & PROD environments, Code Productionization and transitioned to Prod Support Analyst teams', 'Design, build, Optimize and Maintain data pipeline.Optimize Data Ingestion Infrastructure Validation, understanding of Source Systems, Source and Target Data Model Architecture artifactsDocument Technical Data file standard in Data Lake (AVRO, ORC, Parquet, etc.), HDFS Build the pipeline to push Enterprise + Semantic data to Azure Synapse AnalyticsBuild Enterprise data lake/hub(Raw, trusted/Curated, Data Provisioning Layer), Data Warehouse (Subject Areas, Logical and Physical Data marts, and Data Labs) for Reporting and Digital Invocation capabilities.Document HIVE/ SparkSQL meta-data store standards, & Policies to access the data in Data Lake Assemble large, complex data sets that meet functional / non-functional business requirements.Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sourcesBuild analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.Create data tools for team members to assist them in building and optimizing analytics production.Work with data and analytics experts to strive for greater functionality in our data systems.Unit testing, promote tested pipelines to UAT, & PROD environments, Code Productionization and transitioned to Prod Support Analyst teamsBuild data transformation routines to flatten and denormalize data into quarriable data sets and Provide 3rd level Prod SupportImplement the directory structure and namespace for storing transformed + curated data, for both current state plus historical viewsImplement logic and data transformation scripts to convert from raw to semantic / canonical form of data and Implement Data Quality Rules using HIVE/Spark SQLImplement HIVE / Spark meta-layers and data frames and enable the query access to Data filesWorks in a safe manner collaborating as a team member to achieve all outcomes.Works in a safe manner collaborating as a team member to achieve all outcomes.Demonstrate Behaviours that exhibit our organizational Values: Collaboration, Courage, Perseverance, and Passion.Ensure personal adherence with all compliance programs including the Global Business Ethics and Compliance Program, Global Quality policies and procedures, Safety and Environment policies, and HR policies.All other relevant duties as assigned.', 'Implement HIVE / Spark meta-layers and data frames and enable the query access to Data filesWorks in a safe manner collaborating as a team member to achieve all outcomes.', '10+ years of hands on experience in working with one or more of the following: SQL, Oracle, ETL, and database diagnostic tools;', 'Work with data and analytics experts to strive for greater functionality in our data systems.', 'Document HIVE/ SparkSQL meta-data store standards, & Policies to access the data in Data Lake Assemble large, complex data sets that meet functional / non-functional business requirements.', 'Build data transformation routines to flatten and denormalize data into quarriable data sets and Provide 3rd level Prod Support', 'Works in a safe manner collaborating as a team member to achieve all outcomes.', 'Experience with ETL/ELT -type data integration tools that can be used to author HIVE QL or Spark SQL code through a visual interface (i.e., low-code / no-code techniques for authoring data transformation pipelines)', 'Create data tools for team members to assist them in building and optimizing analytics production.', 'Implement logic and data transformation scripts to convert from raw to semantic / canonical form of data and Implement Data Quality Rules using HIVE/Spark SQL', 'Document Technical Data file standard in Data Lake (AVRO, ORC, Parquet, etc.), HDFS Build the pipeline to push Enterprise + Semantic data to Azure Synapse Analytics', 'Education', 'Ensure personal adherence with all compliance programs including the Global Business Ethics and Compliance Program, Global Quality policies and procedures, Safety and Environment policies, and HR policies.', 'Knowledge in HIVE, Spark (Scala/Python), Data Lake Infrastructure tools & Techniques', 'Knowledge of modern batch and real-time file transfer protocols, e.g., Kafka, Apache Nifi, Storm, a plus.', 'Knowledge of EDP Architecture, Reference Data Model (ADRM), Hadoop, HDFS, & Linux directory & file Management concepts & code syntax', 'Experience in using variety of data stores including Azure Data Lakes, SQL Database, Azure Data Warehouse and Azure Synapse', 'Implement the directory structure and namespace for storing transformed + curated data, for both current state plus historical views', '5+ years Data Modelling experience', 'Design, build, Optimize and Maintain data pipeline.', 'Knowledge of EDP Architecture, Reference Data Model (ADRM), Hadoop, HDFS, & Linux directory & file Management concepts & code syntaxGood understanding of Azure Data storage options (ADLS, HDInsight, structure & Namespace)Knowledge of file standards (AVRO, Parquet, ORC, etc.), HIVE Meta stores, SPARK, Scala, Python & other process for persistent Data Frames to access data.Knowledge in HIVE, Spark (Scala/Python), Data Lake Infrastructure tools & TechniquesExperience with Data Quality tools that can be used to convert data quality business rule logic into HIVE or Spark query language for execution in the Data LakeExperience with ETL/ELT -type data integration tools that can be used to author HIVE QL or Spark SQL code through a visual interface (i.e., low-code / no-code techniques for authoring data transformation pipelines)Experience using Data Ingestion/ ETL / ELT or Change Data Capture (CDC) software in an HDFS environmentExperience in using variety of data stores including Azure Data Lakes, SQL Database, Azure Data Warehouse and Azure SynapseExperience with data warehouses, data mart creation, and data mart access control and data provisioningExperience in implementing the Enterprise Data Models, Business Data Models, Logical and Physical Data Marts Models, and SandboxesKnowledge of modern batch and real-time file transfer protocols, e.g., Kafka, Apache Nifi, Storm, a plus.Knowledge/Experience of SAP Data Ingestion Tools, Modern Data Lake Management, Azure', 'Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.', 'Experience with Data Quality tools that can be used to convert data quality business rule logic into HIVE or Spark query language for execution in the Data Lake', 'Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources', 'Demonstrate Behaviours that exhibit our organizational Values: Collaboration, Courage, Perseverance, and Passion.', 'Optimize Data Ingestion Infrastructure Validation, understanding of Source Systems, Source and Target Data Model Architecture artifacts', 'Good understanding of Azure Data storage options (ADLS, HDInsight, structure & Namespace)', 'Experience5+ years Data Modelling experience10+ years of hands on experience in working with one or more of the following: SQL, Oracle, ETL, and database diagnostic tools;Informatica tools sets (Enterprise Data Catalog, Data Quality, and AXON Data Governance) o  Azure Data Engineer Certification is preferred', 'Informatica tools sets (Enterprise Data Catalog, Data Quality, and AXON Data Governance) o  Azure Data Engineer Certification is preferred', 'Experience using Data Ingestion/ ETL / ELT or Change Data Capture (CDC) software in an HDFS environment', '5+ years Data Modelling experience10+ years of hands on experience in working with one or more of the following: SQL, Oracle, ETL, and database diagnostic tools;Informatica tools sets (Enterprise Data Catalog, Data Quality, and AXON Data Governance) o  Azure Data Engineer Certification is preferred', 'Knowledge/Experience of SAP Data Ingestion Tools, Modern Data Lake Management, Azure', 'Knowledge, Skills and AbilitiesKnowledge of EDP Architecture, Reference Data Model (ADRM), Hadoop, HDFS, & Linux directory & file Management concepts & code syntaxGood understanding of Azure Data storage options (ADLS, HDInsight, structure & Namespace)Knowledge of file standards (AVRO, Parquet, ORC, etc.), HIVE Meta stores, SPARK, Scala, Python & other process for persistent Data Frames to access data.Knowledge in HIVE, Spark (Scala/Python), Data Lake Infrastructure tools & TechniquesExperience with Data Quality tools that can be used to convert data quality business rule logic into HIVE or Spark query language for execution in the Data LakeExperience with ETL/ELT -type data integration tools that can be used to author HIVE QL or Spark SQL code through a visual interface (i.e., low-code / no-code techniques for authoring data transformation pipelines)Experience using Data Ingestion/ ETL / ELT or Change Data Capture (CDC) software in an HDFS environmentExperience in using variety of data stores including Azure Data Lakes, SQL Database, Azure Data Warehouse and Azure SynapseExperience with data warehouses, data mart creation, and data mart access control and data provisioningExperience in implementing the Enterprise Data Models, Business Data Models, Logical and Physical Data Marts Models, and SandboxesKnowledge of modern batch and real-time file transfer protocols, e.g., Kafka, Apache Nifi, Storm, a plus.Knowledge/Experience of SAP Data Ingestion Tools, Modern Data Lake Management, AzureExperience5+ years Data Modelling experience10+ years of hands on experience in working with one or more of the following: SQL, Oracle, ETL, and database diagnostic tools;Informatica tools sets (Enterprise Data Catalog, Data Quality, and AXON Data Governance) o  Azure Data Engineer Certification is preferred', 'Knowledge, Skills and AbilitiesKnowledge of EDP Architecture, Reference Data Model (ADRM), Hadoop, HDFS, & Linux directory & file Management concepts & code syntaxGood understanding of Azure Data storage options (ADLS, HDInsight, structure & Namespace)Knowledge of file standards (AVRO, Parquet, ORC, etc.), HIVE Meta stores, SPARK, Scala, Python & other process for persistent Data Frames to access data.Knowledge in HIVE, Spark (Scala/Python), Data Lake Infrastructure tools & TechniquesExperience with Data Quality tools that can be used to convert data quality business rule logic into HIVE or Spark query language for execution in the Data LakeExperience with ETL/ELT -type data integration tools that can be used to author HIVE QL or Spark SQL code through a visual interface (i.e., low-code / no-code techniques for authoring data transformation pipelines)Experience using Data Ingestion/ ETL / ELT or Change Data Capture (CDC) software in an HDFS environmentExperience in using variety of data stores including Azure Data Lakes, SQL Database, Azure Data Warehouse and Azure SynapseExperience with data warehouses, data mart creation, and data mart access control and data provisioningExperience in implementing the Enterprise Data Models, Business Data Models, Logical and Physical Data Marts Models, and SandboxesKnowledge of modern batch and real-time file transfer protocols, e.g., Kafka, Apache Nifi, Storm, a plus.Knowledge/Experience of SAP Data Ingestion Tools, Modern Data Lake Management, Azure', 'Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.', 'Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.']",Entry level,Full-time,Engineering,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Twistle,"Seattle, WA",1 month ago,Be among the first 25 applicants,"['', 'Experience working in an Agile/Scrum development process.', 'Participate in code reviews with languages like LookerML, Python, Django, JavaScript', 'BS/MS degree in Computer Science, Engineering, and/or related Healthcare experience.', '3 years of experience in working in multi-tenant SaaS applications and services.Experience working in an Agile/Scrum development process.', 'Build analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition.', 'Strong working knowledge of tools like JIRA, Asana, Confluence, and Tettra.', '3 years of experience in working in multi-tenant SaaS applications and services.', 'Strong knowledge of and experience with reporting software such as Looker, BusinessObjects, Power BI, Tableau, etc.', 'Maintain both test and production library of interfaces, applying appropriate methods, procedures, and safeguards to protect the integrity of interfaces, ensuring their recoverability.', 'Strong working knowledge of AWS services including Redshift, RDS, EMR and EC2.', 'Automate and monitor business critical applications and troubleshoot or escalate appropriately when issues arise.', 'Strong experience demonstrating and understanding tools like Kafka, Spark and Hadoop; relational NoSQL and SQL databases including Cassandra and PostgreSQL. ', 'Identify, design, and implement process improvements related to (data mapping, optimizing data delivery, and scalability of transformations while automating manual processes.', 'Collaborate with other engineers, implementation, customer success managers throughout the development process to release functional, performant and secure data on a regular basis.', 'Hands on experience with SQL, developing stored procedures, functions, views and triggers, while validating and analyzing data integrity. ', 'Demonstrate experience working with various payloads including (JSON, XML APIs, Web Services, ETL, and File Transfers', 'Build required infrastructure for optimal extraction, transformation and loading of data from various data sources using AWS and SQL technologies.', 'Automate and monitor business critical applications and troubleshoot or escalate appropriately when issues arise.Participate in code reviews with languages like LookerML, Python, Django, JavaScriptCollaborate with other engineers, implementation, customer success managers throughout the development process to release functional, performant and secure data on a regular basis.', 'Monitor, triage, and modify configuration for integrated healthcare messaging within Twistle Platform.', '5+ years’ experience in healthcare integrations, with at least 2+ years in Cloud applications.', 'Familiarity with one or more of the following development languages: Python, C#, Java.', ' Responsibilities ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Senior Data Engineer,U.S. Bank,"Richfield, MN",6 days ago,Be among the first 25 applicants,"['', ' Develop Procedures and Scripts for Data Migration', ' Effective interpersonal, verbal and written communication skills.', ' Familiarity with advanced indexing, normalization, triggers, how to make all coding set-based, and indexed views.', "" Bachelor's degree in a quantitative field such as econometrics, computer science, engineering or applied mathematics, or equivalent work experience"", 'Preferred Skills/Experience', ' Excellent understanding of T-SQL Programming', ' 5+ years of Experience as a SQL Developer or Similar Role', 'Benefits', ' Five to seven years of statistics or analytics experience', 'EEO is the Law', ' Able to build appropriate and useful Reporting Deliverables', ' Critical Thinker and problem-solving skills', ' 3+ years’ experience at isolating and resolving T-SQL performance issues, without using Profiler.', ' 5+ years SQL Server experience in programming T-SQL, Stored Procedures, and Functions', ' Design and implement data acquisition strategy with the right toolset (ETL, Streaming, etc.)', ' Working knowledge with modern database features such as Columnar indexing, in-memory processing.', ' 5+ years of Experience as a SQL Developer or Similar Role Excellent understanding of T-SQL Programming Design and implement data acquisition strategy with the right toolset (ETL, Streaming, etc.) Good Knowledge of Python/JSON Able to build appropriate and useful Reporting Deliverables Develop Procedures and Scripts for Data Migration Sense of Ownership and pride in your performance and its impact on company’s success Critical Thinker and problem-solving skills Effective interpersonal, verbal and written communication skills. Experience dealing with financial data preferred. 5+ years SQL Server experience in programming T-SQL, Stored Procedures, and Functions 3+ years’ experience at isolating and resolving T-SQL performance issues, without using Profiler. 3+ years’ experience in coding highly-efficient complex T-SQL, involving very complex logic, and many large tables up to 100GB each. Familiarity with advanced indexing, normalization, triggers, how to make all coding set-based, and indexed views. Working knowledge with modern database features such as Columnar indexing, in-memory processing.', ' Experience dealing with financial data preferred.', 'Basic Qualifications', ' 3+ years’ experience in coding highly-efficient complex T-SQL, involving very complex logic, and many large tables up to 100GB each.', ' Sense of Ownership and pride in your performance and its impact on company’s success', ' Good Knowledge of Python/JSON', "" Bachelor's degree in a quantitative field such as econometrics, computer science, engineering or applied mathematics, or equivalent work experience Five to seven years of statistics or analytics experience"", 'Job Description', 'E-Verify']",Associate,Full-time,Information Technology,Banking,2021-03-18 14:34:51
Data Engineer,Deloitte,"Arlington, VA",1 month ago,Be among the first 25 applicants,"['', 'Ability to decompose a technical problem into its sub-components and build a plan to rigorously tackle the analysis that is defensible and repeatable', ' Leverage automation, cognitive and science-based techniques to manage data, predict scenarios and prescribe actions', 'Work you’ll do', 'High-performing team player', ""Bachelor's degree in Computer Science, Engineering, Mathematics or other business-related field"", 'Active Government Security Clearance', 'Benefits', "" Ability to obtain and maintain a Government Security Clearance 2+ years relevant professional experience 2+ years data engineering, data managment, and transformation experience Bachelor's degree in Computer Science, Engineering, Mathematics or other business-related field Must be legally authorized to work in the United States without the need for employer sponsorship, now or at any time in the future "", '1+ years of relevant technology consulting or industry experience', ' Drive operational efficiency by maintaining their data ecosystems, sourcing analytics expertise and providing As-a-Service offerings for continuous insights and improvements', 'Qualifications', '2+ years relevant professional experience', 'Recruiter tips', 'Corporate citizenship', 'How You’ll Grow', 'Must be legally authorized to work in the United States without the need for employer sponsorship, now or at any time in the future', 'Ability to thrive in fast-paced work environment with multiple stakeholder', ' Active Government Security Clearance 1+ years of relevant technology consulting or industry experience Prior professional services or federal consulting experience Knowledge of data mining, machine learning, data visualization and statistical modeling Ability to thrive in fast-paced work environment with multiple stakeholder Ability to decompose a technical problem into its sub-components and build a plan to rigorously tackle the analysis that is defensible and repeatable High-performing team player ', 'Knowledge of data mining, machine learning, data visualization and statistical modeling', 'Ability to obtain and maintain a Government Security Clearance', 'Data Engineer', 'The team', 'Prior professional services or federal consulting experience', ' Implement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to generate insights leveraging cloud-based platforms', '2+ years data engineering, data managment, and transformation experience', 'Deloitte’s culture']",Not Applicable,Full-time,Management,Accounting,2021-03-18 14:34:51
Senior Data Engineer (Healthcare Map / Data Platform Team),Komodo Health,San Francisco Bay Area,1 week ago,Be among the first 25 applicants,"['', 'Design, develop, and implement distributed data systems using Spark or similar technologies to ingest incoming data at scale.\xa0', 'Create automation systems and tools to configure, monitor, and orchestrate the data services and data pipelines.', 'You are an experienced technologist who has a proven track record of designing and building large scale solutions using big data technologies, ideally using Spark, Airflow and Python.\xa0', 'We are looking for an experienced Data Engineer on the Healthcare Map / Data Platform team to play a key role in enabling Komodo scalability. In this role, you will have the opportunity to help solve complex challenges and be a part of a great team. Our engineering team is currently about 90 and growing rapidly. You will be able to use one of the biggest, most complete healthcare datasets to solve real world problems that deliver value to clients and positive impact on the world.\xa0Some of the tools we use are: Python, Snowflake, Airflow, AWS EMR, AWS Lambda, Spark, AWS EKS, and Docker.', 'Collaborate with data scientists to implement validation and/or data quality enhancements.', 'Pipeline scheduling and monitoring systems, like Airflow or Luigi', 'Building and deploying large-scale, complex data services and data pipelines.', 'Design, develop, and implement distributed data systems using Spark or similar technologies to ingest incoming data at scale.\xa0Create automation systems and tools to configure, monitor, and orchestrate the data services and data pipelines.Evaluate new technologies for continuous improvements in Data Engineering.Collaborate with data scientists to implement validation and/or data quality enhancements.', 'Basic knowledge (college level or hands on) of machine learning and statistics.', 'Help architect and implement new data ingestion framework', 'Nice to have:', 'Help architect and implement new data ingestion frameworkWrite code to ingest new data sources at high scaleHelp operate existing data sources', 'Our people are the center of our success. We are a smart, supportive team with diverse perspectives and a shared passion for fixing what’s broken in healthcare. It’s fun, challenging and important. Join us!', 'Evaluate new technologies for continuous improvements in Data Engineering.', 'Write code to ingest new data sources at high scale', 'At Komodo Health, our mission is to reduce the global burden of disease. Smarter use of data is essential to this mission. We combine the world’s most comprehensive view of patient encounters with innovative algorithms and decades of clinical expertise to power our Healthcare Map, the industry’s most precise view of the U.S. healthcare system. With the Healthcare Map as our foundation, we offer a suite of powerful software applications that deliver exceptional value to the industry.', 'The Opportunity at Komodo Health', 'BS/MS in Computer Science, related degrees or relevant technical experience.', 'Ability to work as part of a collaborative team in a fast-paced environment.', 'Python development', '5-7+ years implementing data engineering and distributed systems.\xa0', 'What you bring to Komodo Health:\xa0', 'Sincere interest in working at a healthcare startup and passion for healthcare data.', 'After three months, you will:\xa0', 'BS/MS in Computer Science, related degrees or relevant technical experience.5-7+ years implementing data engineering and distributed systems.\xa0Building and deploying large-scale, complex data services and data pipelines.Python developmentPipeline scheduling and monitoring systems, like Airflow or LuigiData processing platforms such as Spark, Hadoop, etc.Ability to work as part of a collaborative team in a fast-paced environment.Sincere interest in working at a healthcare startup and passion for healthcare data.', 'We Breathe Life Into Data', 'As one of the members of this business-critical team, you will:', 'Data processing platforms such as Spark, Hadoop, etc.', 'Help operate existing data sources']",Mid-Senior level,Full-time,Engineering,Computer Software,2021-03-18 14:34:51
Data Engineer Manager,Vitamin T,United States,2 weeks ago,34 applicants,"['', 'Previous Experience in agile/scrum environments', 'Microsoft SQL Server', 'Ideal Candidate will have all of the below:', 'Ensure the technical infrastructure strategy, configuration and management for products is fast, resilient and scalable by providing technical direction, guidance, and hands-on management to team members', 'Python', ""Bachelor's degree in a computer science or equivalent professional experience"", 'Previous hands-on experience with some or all of our tech stack', ' As a Manager of Data Engineering , you will lead and be part of a team of talented engineers that builds and maintains Connex which is our industry leading, multi-channel media optimization platform. Help turn high-level business missions into fully architected end-to-end solutions involving heterogeneous sources, performant ETL application, integrated schema designs, and efficient data access patterns. Are you up for the challenge of being part of a company that continues to lead and disrupt the digital marketing landscape with its technology and innovation?', 'Ability to articulate their technical design', 'Hands on Python code experience', '6+ years of relevant professional experience in a similar position', '3+ years of leadership experience in software architecture, design and development', 'AWS Cloud (EMR, Lambda, EC2, Docker, S3, API Gateway, DynamoDB)', ""Bachelor's degree in a computer science or equivalent professional experienceStrong written and verbal English communication skills are requiredPrevious Experience in agile/scrum environments6+ years of relevant professional experience in a similar position3+ years of leadership experience in software architecture, design and developmentSmart, high aptitude to learn new things and sense of urgency to get things done.Practical-minded – chooses stability/reliability/maintainability over shiny new objectsPassionate about technology – ideally you build things outside of work for funSignificant real-world experience developing real products at large scalePrevious hands-on experience with some or all of our tech stackPythonSpark using PySparkAWS Cloud (EMR, Lambda, EC2, Docker, S3, API Gateway, DynamoDB)Microsoft SQL ServerPossess a positive and professional attitude as the Innovation team continues to help Rise grow as a company and industry leader"", 'Strong written and verbal English communication skills are required', 'Possess a positive and professional attitude as the Innovation team continues to help Rise grow as a company and industry leader', 'Responsibilities:', 'Summary and Scope', 'Someone who has managed individuals, ideally offshore OR someone who has mentored/trained junior developers', 'Lead the evolution of our growing data team through training and mentoring while keeping everyone accountable for the high quality and performance standards that have been set', 'Stay abreast of new and innovative trends in software development and the web and data software product space', 'Experience working with across disciplines, primarily with the product team', 'Passionate about technology – ideally you build things outside of work for fun', 'Lead software delivery team for next generation products; including contributing to the production code base and providing ongoing maintenance and supportOversee technical product development from concept to release and ongoing maintenance/supportEnsure the technical infrastructure strategy, configuration and management for products is fast, resilient and scalable by providing technical direction, guidance, and hands-on management to team membersChampion technical discovery while collaborating with product managers, scrum masters, project managers, engineers, testers and designers throughout the product lifecycleStay abreast of new and innovative trends in software development and the web and data software product spaceLead the evolution of our growing data team through training and mentoring while keeping everyone accountable for the high quality and performance standards that have been set', 'Spark using PySpark', 'Champion technical discovery while collaborating with product managers, scrum masters, project managers, engineers, testers and designers throughout the product lifecycle', 'Lead software delivery team for next generation products; including contributing to the production code base and providing ongoing maintenance and support', 'Someone who has managed individuals, ideally offshore OR someone who has mentored/trained junior developersHands on Python code experienceAbility to articulate their technical designExperience working with across disciplines, primarily with the product teamCloud experience, AWS preferred but open to Azure or Google Cloud', 'Practical-minded – chooses stability/reliability/maintainability over shiny new objects', 'Smart, high aptitude to learn new things and sense of urgency to get things done.', 'Required Skills:', 'Cloud experience, AWS preferred but open to Azure or Google Cloud', 'Significant real-world experience developing real products at large scale', 'Oversee technical product development from concept to release and ongoing maintenance/support']",Mid-Senior level,Full-time,Design,Staffing and Recruiting,2021-03-18 14:34:51
Data Engineer,Piper Companies,"Raleigh, NC",1 week ago,Be among the first 25 applicants,"['', 'Salary Range: $135,000 - $145,000 (based on experience)Comprehensive benefits including medical, vision, dental, 401K, PTO and bonus', 'Introduce and maintain data platforms, data governance and data security capabilities and documentation including establishing roadmaps ', 'Salary Range: $135,000 - $145,000 (based on experience)', 'Track record of delivering high-quality features and services in a SaaS web applications', 'Responsibilities For The Data Engineer Include', 'Perform tasks related to data profiling, database design, data analysis, data quality, metadata management and support', 'Qualifications For The Data Engineer Include', 'Comprehensive benefits including medical, vision, dental, 401K, PTO and bonus', 'Security Clearance', 'Compensation For The Data Engineer Include', 'Hands-on experience and proficient knowledge of functional programing using languages', 'Experience working with Docker and Kubernetes', 'Drive the vision for implementing strategies around Data Lake and Warehouse architecture', 'Experience working with Data Lake and Warehouse architectureTrack record of delivering high-quality features and services in a SaaS web applicationsHands-on experience and proficient knowledge of functional programing using languagesExperience working with Docker and Kubernetes', 'Job Category', 'Experience working with Data Lake and Warehouse architecture', 'Develop and design models for complex analytical and data warehouse systems', 'Drive the vision for implementing strategies around Data Lake and Warehouse architectureDevelop and design models for complex analytical and data warehouse systemsPerform tasks related to data profiling, database design, data analysis, data quality, metadata management and supportIntroduce and maintain data platforms, data governance and data security capabilities and documentation including establishing roadmaps ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Amitech Solutions,"Creve Coeur, MO",6 days ago,Be among the first 25 applicants,"['', ' Software Engineering experience with programming languages like Go, Python and/or Java', ' Experience in Data Science and Machine Learning', 'Life insurance', 'About Us', ' Great communication skills', ' Defining re-usable processes for large-scale implementations within the Data Warehouse', ' Building automation to further simplify the development and promotion process', 'Why Amitech', ' Working with SME’s across Commercial and R&D organizations to understand their data and integrate into an Enterprise Data Model', ' Expertise with various Data Modeling approaches, including 3NF and Dimensional modeling', 'Health, Dental, and Vision insurance', '401 (k)', ' Experience building CI/CD Pipelines', '2018 - 2020 “Top Work Places” Winner', ' Expertise with Relational Databases such as Teradata, Oracle, Redshift and Postgres', ' Expertise in SQL, stored procedures and functions', ' Health, Dental, and Vision insurance Long and Short-Term Disability 401 (k) Life insurance Pet insurance Referral program 2018 - 2020 “Top Work Places” Winner ', 'Data Engineer', ' Implementing End to End solutions from source system to Business Intelligence reports', ' Experience working with cloud scheduling solutions, such as Airflow', ' Cloud experience in AWS and/or GCP', ' Experience working with container engines such as GKE / Fargate', 'Pet insurance', ' Ability to quickly learn technologies such as Terraform, Kubernetes, Apache Beam, and Kafka', ' Ability to solve complex issues with re-usable frameworks', 'What You’ll Need To Be Successful', 'Referral program', 'Long and Short-Term Disability', 'Preferred Qualifications', 'Key Responsibilities Include']",Entry level,Contract,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer I,Voltaiq,"Berkeley, CA",3 weeks ago,164 applicants,"['', 'Apache Spark', 'Our Stack', 'Excellent communication skills.', ' B.Sc. or higher in Computer Science or a related field or comparable experience. Experience in Python programming language. Experience working with SQL. Knowledge of Pandas. Desire to learn new technologies. Strong computer science fundamentals. Excellent teamwork skills Excellent communication skills. ', 'Development process and agile methodologies', 'The Role:', 'Exposure in designing, building, testing, and deploying ETL.', 'Data Engineer I', 'Understand the evolving needs of the customer-facing product and data science teams, and how these will be served by the data platform', 'Data modeling', 'Provide visibility into the structure, state and performance of the data platform', 'Voltaiq is an equal opportunity employer and is committed to achieving a diverse workforce through application of its equal opportunity and nondiscrimination policy, in all aspects of employment.', 'Unit testing and integration testing', 'Experience in Python programming language.', 'Troubleshoot emergent customer data pipeline issues', 'Help design, build, and test data access services and tools for analytics', 'B.Sc. or higher in Computer Science or a related field or comparable experience.', 'Preferred Skills & Qualifications/ Bonuses: ', 'Responsibilities:', 'Object-Oriented and Functional programming concepts', ' Apache Spark Data modeling ORM technologies Distributed systems Object-Oriented and Functional programming concepts Unit testing and integration testing Development process and agile methodologies Exposure in designing, building, testing, and deploying ETL. ', 'Required Skills & Qualifications:', 'Strong computer science fundamentals.', 'ORM technologies', 'Knowledge of Pandas.', 'Desire to learn new technologies.', 'Competitive salary plus equity and full benefits. Our office is located in Berkeley, CA.', 'Distributed systems', 'Excellent teamwork skills', 'Help design, build, and test scalable data processing pipelines and ETL processes', 'Communicate data platform architecture to other members of the Engineering team', 'Ensure data confidentiality, integrity, and availability for our customers', 'Experience working with SQL.', ' Help design, build, and test scalable data processing pipelines and ETL processes Help design, build, and test data access services and tools for analytics Write scripts to parse and transform battery data coming from databases, text files, and binary files Troubleshoot emergent customer data pipeline issues Communicate data platform architecture to other members of the Engineering team Understand the evolving needs of the customer-facing product and data science teams, and how these will be served by the data platform Provide visibility into the structure, state and performance of the data platform Ensure data confidentiality, integrity, and availability for our customers ', 'Write scripts to parse and transform battery data coming from databases, text files, and binary files']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,Avanade,"Philadelphia, PA",4 weeks ago,25 applicants,"['', ' 1,000 data engineers ', ' Do you enjoy making sure that information is accessible and easy to use? So do we. ', ' Travel as needed to various client locations ', ' 3,500 analytics professionals worldwide ', ' Implement effective metrics and monitoring ', 'About Avanade', ' Experience in preparing data for and building pipelines and architecture ', ' Mapping data and analytics ', ' Knowledge of multiple Azure data applications including Azure Databricks ', ' Transforming business needs into technical solutions ', 'Your Skills And Business Experience Include', ' Use proven methods to solve business problems using Azure Data and Analytics services in combination with building data pipelines, data streams and system integration ', 'Day-to-day, You Will', 'About The Role', ' Build the building blocks for transforming enterprise data solutions ', ' Use your sound eye for business to translate business requirements into technical solutions ', ' Assess client needs to build bespoke data design services ', ' Craft the architectures, data warehouses and databases that support access and Advanced Analytics, and bring them to life through modern visualization tools ', ' Analyze current business practices, processes and procedures to spot future opportunities ', ' 300 cognitive service experts ', ' 17 Gold Competencies ', ' 14-time winner of Microsoft Partner of the Year ', 'How We Support You', ' Give colleagues and clients the tools to find and use data for routine and non-routine analysis ', ' 14-time winner of Microsoft Partner of the Year  24,000+ certifications in Microsoft technology  90+ Microsoft partner awards  17 Gold Competencies  3,500 analytics professionals worldwide  1,000 data engineers  Implemented analytics systems for more than 550 clients  400 AI practitioners  300 cognitive service experts ', ' Design and build modern data pipelines, data streams, and data service Application Programming Interfaces (APIs) ', ' Give colleagues and clients the tools to find and use data for routine and non-routine analysis  Use your sound eye for business to translate business requirements into technical solutions  Analyze current business practices, processes and procedures to spot future opportunities  Assess client needs to build bespoke data design services  Build the building blocks for transforming enterprise data solutions  Design and build modern data pipelines, data streams, and data service Application Programming Interfaces (APIs)  Craft the architectures, data warehouses and databases that support access and Advanced Analytics, and bring them to life through modern visualization tools  Implement effective metrics and monitoring  Be comfortable to make your own decisions and guide your colleagues  Travel as needed to various client locations ', 'Why Avanade', ' Transforming business needs into technical solutions  Mapping data and analytics  Data profiling, cataloguing and mapping to enable the design and build of technical data flows  Use proven methods to solve business problems using Azure Data and Analytics services in combination with building data pipelines, data streams and system integration  Knowledge of multiple Azure data applications including Azure Databricks  Experience in preparing data for and building pipelines and architecture ', ' Implemented analytics systems for more than 550 clients ', ' Data profiling, cataloguing and mapping to enable the design and build of technical data flows ', 'Job Description', 'About You', ' 24,000+ certifications in Microsoft technology ', ' 400 AI practitioners ', ' 90+ Microsoft partner awards ', ' Be comfortable to make your own decisions and guide your colleagues ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer-Azure,Nesco Resource,"Austin, TX",1 week ago,Be among the first 25 applicants,"['Goals for the team:', 'Prior experience working with other Big Data tools such as Spark, Snowflake, and Kafka', 'Bachelors/Masters in Computer Science, MIS/Information Management, Engineering or related fieldStrong experience with NoSQL database, including PostgresPrior experience\xa0working with Azure Cloud Services: Data Factory, SQL database, Functions, Data Lake, Databricks, Logic Apps, and Azure Automation.Fluent with the following scripting languages: Python, Scala, and C#Advanced knowledge with SQL Server Database - including writing advanced SQL scripts, profiling and optimization.Working knowledge of BI Tools: MS Integration Services, Reporting Services, and Analysis Services, and Power BIPrior experience working with other Big Data tools such as Spark, Snowflake, and KafkaPrefer someone with experience working in Financial Services, Wealth Mgmt. Industry', 'Meet with the business users, assist with data-related technical issues, and support their data infrastructure needs.', ' ', 'Duties/Responsibilities:', ""This role can sit remote from anywhere in the U.S. but the person must be willing to relocate to Austin if/when the office re-opens. Client will discuss relocation reimbursement options if necessary in that case.  There's a 15% bonus paid out annually based on team & individual performance. VP wants someone who has been through a Migration to Azure from a legacy system (SSIS to Azure is fine as well). "", 'Prior experience\xa0working with Azure Cloud Services: Data Factory, SQL database, Functions, Data Lake, Databricks, Logic Apps, and Azure Automation.', ' - Migrate data processes using ETL to the new platform in the cloud which includes integrating all the applications on the existing platform ', 'Fluent with the following scripting languages: Python, Scala, and C#', 'Description: ', 'Working knowledge of BI Tools: MS Integration Services, Reporting Services, and Analysis Services, and Power BI', 'Advanced knowledge with SQL Server Database - including writing advanced SQL scripts, profiling and optimization.', ' Duties/Responsibilities: ', 'Strong experience with NoSQL database, including Postgres', 'Design/develop data pipelines to extract data from multiple data sources using Azure, Snowflake Cloud, and other cloud-native technologies', 'KNOWLEDGE/SKILLS: ', 'Description', 'Prefer someone with experience working in Financial Services, Wealth Mgmt. Industry', 'This is a permanent job, reporting to the VP of Data Engineering.', 'Bachelors/Masters in Computer Science, MIS/Information Management, Engineering or related field', 'KNOWLEDGE/SKILLS:', ' - Completely decommission their Data Center by EOY ', 'We are looking for a passionate data engineer to join our growing Data and Analytics team on our journey in modernizing our platform to the next-generation cloud platform. You will be responsible for expanding, optimizing, and improving overall data quality and set up next-generation data orchestration using modern cloud tools and technologies. ', 'Analyze existing legacy systems and data sets to help Business Analysts define the functional/non-functional requirements.', 'Enjoy working in Agile as part of a scrum team', 'Goals for the team: ', '\xa0', 'Design/develop data pipelines to extract data from multiple data sources using Azure, Snowflake Cloud, and other cloud-native technologiesBuild out a data model to gain actionable insights from data, operational efficiency, and other key business performance metrics.Design, manage, & monitor inbound and outbound data processesEnjoy working in Agile as part of a scrum teamAutomate the data testing processes and integrate them with monitoring systemsWork with and support\xa0an Application Engineering team, DBA, Infrastructure, and Project Management Office.Analyze existing legacy systems and data sets to help Business Analysts define the functional/non-functional requirements.Meet with the business users, assist with data-related technical issues, and support their data infrastructure needs.', 'Work with and support\xa0an Application Engineering team, DBA, Infrastructure, and Project Management Office.', 'Build out a data model to gain actionable insights from data, operational efficiency, and other key business performance metrics.', 'Automate the data testing processes and integrate them with monitoring systems', 'Design, manage, & monitor inbound and outbound data processes']",Mid-Senior level,Full-time,Information Technology,Financial Services,2021-03-18 14:34:51
Senior Data Engineer | NY - New York,HopHR,"New York, NY",2 days ago,Be among the first 25 applicants,"['', 'Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case', ' Background Experience ', 'Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards. Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions.', 'Writes ETL (Extract / Transform / Load) processes, designs database systems and develops tools for real-time and offline analytic processing.', 'Strong collaboration and communication skills within and across teams.5 or more years of progressively complex related experience.', 'Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs.', ' Fundamental Components ', 'Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources.', 'Experience building data transformation and processing solutions.', 'Position Summary ', 'Ability to understand complex systems and solve challenging analytical problems.', 'Collaborates with data science team to transform data and integrate algorithms and models into automated processes. Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines.', 'Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs.Writes ETL (Extract / Transform / Load) processes, designs database systems and develops tools for real-time and offline analytic processing.Collaborates with data science team to transform data and integrate algorithms and models into automated processes. Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines. Uses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems. Builds data marts and data models to support Data Science and other internal customers.Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards. Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions.Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case', 'Experience with bash shell scripts, UNIX utilities & UNIX Commands.', 'Strong problem-solving skills and critical thinking ability.', 'Has strong knowledge of large-scale search applications and building high-volume data pipelines.', 'Knowledge in Hadoop architecture, HDFS commands, and experience designing & optimizing queries against data in the HDFS environment.', 'Strong problem-solving skills and critical thinking ability.Strong collaboration and communication skills within and across teams.5 or more years of progressively complex related experience.Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources.Ability to understand complex systems and solve challenging analytical problems.Experience with bash shell scripts, UNIX utilities & UNIX Commands.Knowledge in Java, Python, Hive, Cassandra, Pig, MySQL or NoSQL or similar.Knowledge in Hadoop architecture, HDFS commands, and experience designing & optimizing queries against data in the HDFS environment.Experience building data transformation and processing solutions.Has strong knowledge of large-scale search applications and building high-volume data pipelines.', 'Knowledge in Java, Python, Hive, Cassandra, Pig, MySQL or NoSQL or similar.', ' Uses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems. Builds data marts and data models to support Data Science and other internal customers.']",Associate,Full-time,Information Technology,Information Technology and Services,2021-03-18 14:34:51
Data Engineer,MotorTrend Group,"El Segundo, CA",1 week ago,27 applicants,"['', ' Microsoft Office Suite (Outlook, Word, Excel, PowerPoint) SQL, MySQL or other relational databases Linux, Python, AWS Stack (EC2,EMR S3, Redshift) Tableau or any other data visualization tool SiteCatalyst (Omniture)/Google Analytics or any other web analytics tools experience (Nice to have) ', 'Data integration tools', 'Exposure to cloud platforms (preferably AWS)', ' Collaborate with product teams and data analysts to design and build data-forward solutions Build and deploy streaming and batch data pipelines capable of processing and storing petabytes of data quickly and reliably Integrate with a variety of data metric providers ranging from advertising, web analytics, and consumer devices Build and maintain dimensional data warehouses in support of business intelligence tools Develop data catalogs and data validations to ensure clarity and correctness of key business metrics Drive and maintain a culture of quality, innovation and experimentation Deliver strong Python and SQL development and maintenance techniques surrounding data movement to include technologies Investigate and understand different data sources and ability to connect to a wide variety of 3rd party APIs Design, enhance and implement ETL/data ingestion platform on the cloud Development of ETL source and target mapping design/specifications based on the business requirements. Create ETLs/ELTs to take data from various operational systems and create a unified/enterprise data model for analytics and reporting Develop load and transformation processes in support of the requirements, validate that they meet business and technical specifications, manage ongoing maintenance of the system and data, and make recommendations for process improvements to optimize data movement from source to target Provide production and operational support to existing ETL jobs. Monitor and manage production ETL jobs to verify execution and measure performance to assure ongoing data quality and optimization of the system to manage scalability and performance and identify improvement opportunities for key ETL processes. Strong troubleshooting and problem-solving skills in large data environment Capable of investigating, familiarizing and mastering new data sets quickly ', 'Proficiency in SQL, data modeling, and data warehousing', 'Deliver strong Python and SQL development and maintenance techniques surrounding data movement to include technologies', 'About Us', 'Knowledge of the Python data ecosystem using pandas and numpy', 'Supervisory Responsibility:', 'Experience with Big Data tools; Hadoop, Spark, Kafka, Hive etc', 'Proficiency with the AWS cloud services : EC2, EMR, RDS, S3, Redshift (spectrum)', 'Work Environment:', 'Strong background in scripting language using Python, Bash, Perl, PHP or any other language to solving data problems', 'Experience with relational SQL and NoSQL databases, including Postgres, ,Neo4j and MongoDb', 'Work is performed in an office environment that is well lit and ventilated.', 'Tableau or any other data visualization tool', 'SiteCatalyst (Omniture)/Google Analytics or any other web analytics tools experience (Nice to have)', 'Drive and maintain a culture of quality, innovation and experimentation', ""MotorTrend, Hot Rod, Automobile, Wheeler Dealers, Roadkill, Best Driver's Car, "", 'Education/Experience:', 'Knowledge, Skills, & Abilities:', 'The Role ', 'Equipment/Software Used:', 'Provide production and operational support to existing ETL jobs. Monitor and manage production ETL jobs to verify execution and measure performance to assure ongoing data quality and optimization of the system to manage scalability and performance and identify improvement opportunities for key ETL processes.', 'Integrate with a variety of data metric providers ranging from advertising, web analytics, and consumer devices', 'Job Summary & Responsibilities:', ""Bachelor's degree – Computer Science or equivalent"", 'Development of ETL source and target mapping design/specifications based on the business requirements. Create ETLs/ELTs to take data from various operational systems and create a unified/enterprise data model for analytics and reporting', 'Our People ', 'Experience with Stream Processing systems: Storm ,Spark-Streaming etc', 'Investigate and understand different data sources and ability to connect to a wide variety of 3rd party APIs', 'SQL, MySQL or other relational databases', 'Data Engineer', 'Capable of investigating, familiarizing and mastering new data sets quickly', 'Microsoft Office Suite (Outlook, Word, Excel, PowerPoint)', 'Strong troubleshooting and problem-solving skills in large data environment', 'Design, enhance and implement ETL/data ingestion platform on the cloud', ' This position will not include supervising one or more employees where applicable. ', 'Develop load and transformation processes in support of the requirements, validate that they meet business and technical specifications, manage ongoing maintenance of the system and data, and make recommendations for process improvements to optimize data movement from source to target', 'This position will not include supervising one or more employees where applicable.', 'Develop data catalogs and data validations to ensure clarity and correctness of key business metrics', 'Collaborate with product teams and data analysts to design and build data-forward solutions', 'Build and maintain dimensional data warehouses in support of business intelligence tools', 'Build and deploy streaming and batch data pipelines capable of processing and storing petabytes of data quickly and reliably', 'Proficiency with data exchange types and protocols (json, xml, soap, rest)', ' Work is performed in an office environment that is well lit and ventilated. ', 'Experience with BI tools like Tableau or any other open source BI tools etc.', 'Excellent problem solving skills', 'Linux, Python, AWS Stack (EC2,EMR S3, Redshift)', "" Bachelor's degree – Computer Science or equivalent Strong background in scripting language using Python, Bash, Perl, PHP or any other language to solving data problems Experience with relational SQL and NoSQL databases, including Postgres, ,Neo4j and MongoDb Experience with Big Data tools; Hadoop, Spark, Kafka, Hive etc Proficiency with the AWS cloud services : EC2, EMR, RDS, S3, Redshift (spectrum) Proficiency with data exchange types and protocols (json, xml, soap, rest) Experience with Stream Processing systems: Storm ,Spark-Streaming etc Experience with BI tools like Tableau or any other open source BI tools etc. "", ' Knowledge of the Python data ecosystem using pandas and numpy Data integration tools Proficiency in SQL, data modeling, and data warehousing Excellent problem solving skills Exposure to cloud platforms (preferably AWS) ']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-03-18 14:34:51
