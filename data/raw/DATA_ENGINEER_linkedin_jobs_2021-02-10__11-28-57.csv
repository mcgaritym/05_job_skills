job_title,company,location,date_posted,applicants,job_text,seniority_level,employment_type,job_function,industries,date_scraped
Data Engineer,Spotify,"New York, NY",6 hours ago,Over 200 applicants,"['', 'You know how to work with high volume heterogeneous data, preferably with distributed systems such as Hadoop, BigTable, and CassandraYou have experience with one or more higher-level JVM-based data processing frameworks such as Beam, Dataflow, Crunch, Scalding, Storm, Spark, or something we didn’t list- but not just Pig/Hive/BigQuery/other SQL-like abstractionsYou are knowledgeable about data modeling, data access, and data storage techniquesYou understand the value of teamwork within teams, are excellent communicators, and can build relationships with a diverse set of partnersMachine Learning experience is a plusExperience with data ingestion via API and/or web scraping/crawling (e.g. Selenium, BeautifulSoup) at scale preferredExperience with Google Cloud Platform', 'You are knowledgeable about data modeling, data access, and data storage techniques', 'Experience with Google Cloud Platform', 'Machine Learning experience is a plus', 'Demonstrate standard methodologies in continuous integration and delivery', 'Ingest and aggregate data from both internal and external data sources to build our extraordinary datasetsBuild large-scale batch and real-time data pipelines with data processing frameworks like Scio, Storm, or Spark on the Google Cloud PlatformDemonstrate standard methodologies in continuous integration and deliveryHelp drive optimization, testing, and tooling to improve data qualityCollaborate with other engineers, ML specialists, and partners, taking learning and leadership opportunities that will arise every dayWork in cross functional agile teams to continuously experiment, iterate, and deliver on new product objectives.Work from our offices in New York, with some travel to other Spotify office locations', 'Work in cross functional agile teams to continuously experiment, iterate, and deliver on new product objectives.', 'Work from our offices in New York, with some travel to other Spotify office locations', 'Help drive optimization, testing, and tooling to improve data quality', 'You know how to work with high volume heterogeneous data, preferably with distributed systems such as Hadoop, BigTable, and Cassandra', 'Collaborate with other engineers, ML specialists, and partners, taking learning and leadership opportunities that will arise every day', 'Who You Are', 'You have experience with one or more higher-level JVM-based data processing frameworks such as Beam, Dataflow, Crunch, Scalding, Storm, Spark, or something we didn’t list- but not just Pig/Hive/BigQuery/other SQL-like abstractions', 'What You Will Do', 'Build large-scale batch and real-time data pipelines with data processing frameworks like Scio, Storm, or Spark on the Google Cloud Platform', 'Ingest and aggregate data from both internal and external data sources to build our extraordinary datasets', 'Experience with data ingestion via API and/or web scraping/crawling (e.g. Selenium, BeautifulSoup) at scale preferred', 'You understand the value of teamwork within teams, are excellent communicators, and can build relationships with a diverse set of partners']",Not Applicable,Full-time,Information Technology,Marketing and Advertising,2021-02-10 11:25:00
Data Engineer,Adobe,"Lehi, UT",20 hours ago,53 applicants,"['', 'AWS', 'Consistent track record in Python programming language', 'Excellent communication skills (we’re a geographically distributed team)', 'Experience using Docker or Kubernetes is a plus', ""Collaborate with enterprise customer's engineering team to identify data sources, profile and quantify quality of data sources, develop tools to prepare data and build data pipelines for integrating customer data sources and third party data sources with Adobe solutions"", ""What You'll Do"", 'Our Company', 'Collaborate with Data architects, Enterprise architects, Solution consultants and Product engineering teams to gather customer data integration requirements, conceptualize solutions & build required technology stack', 'Collaborate with a Project Manager to bill and forecast time for customer solutions', ""Collaborate with Data architects, Enterprise architects, Solution consultants and Product engineering teams to gather customer data integration requirements, conceptualize solutions & build required technology stackCollaborate with enterprise customer's engineering team to identify data sources, profile and quantify quality of data sources, develop tools to prepare data and build data pipelines for integrating customer data sources and third party data sources with Adobe solutionsDevelop new features and improve existing data integrations with customer data ecosystemEncourage team to think out-of-the-box and overcome engineering obstacles while incorporating new innovative design principles.Collaborate with a Project Manager to bill and forecast time for customer solutions"", 'Encourage team to think out-of-the-box and overcome engineering obstacles while incorporating new innovative design principles.', 'What You Need To Succeed', 'BS/MS degree in Computer Science or equivalent industry experience', 'Develop new features and improve existing data integrations with customer data ecosystem', 'Ability to identify and resolve problems associated with production grade large scale data processing workflows', 'Strong capacity to manage numerous projects are a must', 'Python ', 'Proven experience in building/operating/maintaining fault tolerant and scalable data processing integrations using AWS', 'The Opportunity ', 'Software development experience working with Apache Airflow, Spark, MongoDB, MySQL', 'Experience crafting and maintaining unit tests and continuous integration.', 'Passion for creating Intelligent data pipelines that customers love to use', 'Proven experience in building/operating/maintaining fault tolerant and scalable data processing integrations using AWSConsistent track record in Python programming languageSoftware development experience working with Apache Airflow, Spark, MongoDB, MySQLExperience using Docker or Kubernetes is a plusBS/MS degree in Computer Science or equivalent industry experienceAbility to identify and resolve problems associated with production grade large scale data processing workflowsExcellent communication skills (we’re a geographically distributed team)Experience crafting and maintaining unit tests and continuous integration.Passion for creating Intelligent data pipelines that customers love to useStrong capacity to manage numerous projects are a must', 'ntelligent data pipelines']",Mid-Senior level,Full-time,Information Technology,Marketing and Advertising,2021-02-10 11:25:00
Data Engineer,OTC Markets Group,"New York, NY",11 hours ago,Be among the first 25 applicants,"['', 'Good Debugging Skills, excellent troubleshooting skills', 'Excellent verbal and written communication skills with employees both onsite and in remote locations.', 'Flexible Spending Accounts for health, transit, parking and dependent care, as well as Healthcare Savings.', 'Monday Bagels and Friday Pizza.', 'Good knowledge of SQL. The ability to recognize when you require input from the DBA team.', 'Please no calls or 3rd party recruiter submissions.', 'Spark streaming and optimization', 'Must know how to use GIT.', 'Please note, we will neither sponsor nor relocate for this position.', 'Collaborate with the business stake holders on the feature set specifications.', 'Collaborate with team members on the implementation and planning.', 'You have experience with key AWS services, such as EC2, RDS (Postgres, Aurora), Lambda, Athena', 'Office refreshments and company happy hours.', 'Code and test reliable and resilient real-time Spark Applications.', 'You have experience with continuous integration/deployment and Scala build tool.', 'Annual bonus and stock incentive program.', 'Ability to develop a detailed project plan when working on a project.', 'Experience in agile project development.', 'Create and maintain technical documentation and architecture diagrams.', 'Implement readable, maintainable, and highly performant Scala/Python code.', 'Generous vacation policy in addition to 9 annual holidays observed and Summer Fridays.Snacks and sodas and a very chic coffee bar.Annual bonus and stock incentive program.Office refreshments and company happy hours.Monday Bagels and Friday Pizza.Life and disability insurance, including paid parental leave.Health insurance plans designed to meet the various coverage needs and preferences (Medical, Dental, Vision).Flexible Spending Accounts for health, transit, parking and dependent care, as well as Healthcare Savings.Accounts for qualifying plans.', 'Data Engineering - 3-5 years’ experience with AWS, Scala, Spark, Kafka, S3, Python', 'Strong Linux knowledge.', 'Support projects through their entire lifecycle from analysis to production rollout.', 'Support production issues when required.', 'Snacks and sodas and a very chic coffee bar.', 'You have worked with large data sets.', 'Come as you are and just be you.', 'Code and test reliable and resilient real-time Spark Applications.Learn our architecture and start to contribute immediately as we value and encourage brainstorming and input from everyone on the team.Participate in all phases of the SDLC including but not limited to architecture, technical design and documentation, testing, implementation and product launch.Write unit/integration tests as part of the development initiative providing great test coverage which will enable continuous delivery of code.Implement readable, maintainable, and highly performant Scala/Python code.Use many of the available AWS services to build applications.Create and maintain technical documentation and architecture diagrams.Support projects through their entire lifecycle from analysis to production rollout.Plan and coordinate project schedules, goals, and milestones.Collaborate with the business stake holders on the feature set specifications.Collaborate with team members on the implementation and planning.', 'Must be self-motivated and willing to learn. You love to stay on top of new technology and share with others.', 'T', 'Participate in all phases of the SDLC including but not limited to architecture, technical design and documentation, testing, implementation and product launch.', 'Our successful candidate will:', 'Learn our architecture and start to contribute immediately as we value and encourage brainstorming and input from everyone on the team.', 'What OTC Markets offers its Team Members (why you should choose us):', 'Data Engineering - 3-5 years’ experience with AWS, Scala, Spark, Kafka, S3, PythonSpark streaming and optimizationYou have experience with continuous integration/deployment and Scala build tool.Good Debugging Skills, excellent troubleshooting skillsMust be self-motivated and willing to learn. You love to stay on top of new technology and share with others.Good knowledge of SQL. The ability to recognize when you require input from the DBA team.Excellent verbal and written communication skills with employees both onsite and in remote locations.Proven ability to work with individuals at all organization levels.Ability to work on concurrent projects, when required.Ability to develop a detailed project plan when working on a project.Support production issues when required.Experience in agile project development.Must know how to use GIT.Strong Linux knowledge.', 'You have experience with key AWS services, such as EC2, RDS (Postgres, Aurora), Lambda, AthenaExperience working with equity financial data (Quotes, Trades, Company Information)You have worked with large data sets.You have worked in the Financial Markets.', 'Life and disability insurance, including paid parental leave.', 'C', 'You have worked in the Financial Markets.', 'Proven ability to work with individuals at all organization levels.', 'Experience working with equity financial data (Quotes, Trades, Company Information)', 'Write unit/integration tests as part of the development initiative providing great test coverage which will enable continuous delivery of code.', 'Generous vacation policy in addition to 9 annual holidays observed and Summer Fridays.', 'Use many of the available AWS services to build applications.', 'Accounts for qualifying plans.', 'Health insurance plans designed to meet the various coverage needs and preferences (Medical, Dental, Vision).', 'O', 'Please note, due to COVID-19, OTC Markets is currently operating in a remote work environment with the voluntary option to work in-office a few days a week. This is a temporary adjustment to the start date work location with phase-in updates provided by the Human Resources team. ', 'Plan and coordinate project schedules, goals, and milestones.', 'Nice to haves:', 'Ability to work on concurrent projects, when required.', 'Requirements:']",Mid-Senior level,Temporary,Information Technology,Information Technology and Services,2021-02-10 11:25:00
Data Engineer,Airtable,"San Francisco, CA",11 hours ago,Be among the first 25 applicants,"['', 'Work closely with the rest of engineering, as well as other stakeholders from our growth, sales, marketing, and product teams, to understand the data needs of the business and produce systems that enable better product and growth decision-making.', ""You're passionate and thoughtful about building systems that enhance human understanding."", ""You have professional experience working with modern data storage and processing technologies, and you've wrangled enough data to understand how often the complex systems that produce data can go wrong."", 'Who You Are', 'You can write clear, correct code in at least one programming language, and are willing to become effective in others as needed to get your job done.', 'You may have experience administering modern large-scale data management systems such as ELK.', ""Work on the data collection pipeline across the entire stack, from client-side event logging to ETL. Systems you'll touch may include data warehousing using MPP databases (e.g. RedShift or Vertica), workflow systems (e.g. Airflow or Luigi), streaming data processors (Kinesis, Kafka, etc.), and distributed data processing systems (Spark, Hadoop, etc.)."", 'You communicate with clarity and precision in written form; experience communicating with graphs and plots, or at least understanding how to enable other people to do this, is a big bonus.', 'Help architect, build, and scale our initial data engineering platform, with an eye on security and privacy.', "" Help architect, build, and scale our initial data engineering platform, with an eye on security and privacy. Work closely with the rest of engineering, as well as other stakeholders from our growth, sales, marketing, and product teams, to understand the data needs of the business and produce systems that enable better product and growth decision-making. Work on the data collection pipeline across the entire stack, from client-side event logging to ETL. Systems you'll touch may include data warehousing using MPP databases (e.g. RedShift or Vertica), workflow systems (e.g. Airflow or Luigi), streaming data processors (Kinesis, Kafka, etc.), and distributed data processing systems (Spark, Hadoop, etc.). Ensure that our business-critical data is accurate and correct. "", ""What You'll Do"", "" You're passionate and thoughtful about building systems that enhance human understanding. You have professional experience working with modern data storage and processing technologies, and you've wrangled enough data to understand how often the complex systems that produce data can go wrong. You can write clear, correct code in at least one programming language, and are willing to become effective in others as needed to get your job done. You communicate with clarity and precision in written form; experience communicating with graphs and plots, or at least understanding how to enable other people to do this, is a big bonus. You may have experience administering modern large-scale data management systems such as ELK."", 'Ensure that our business-critical data is accurate and correct.']",Associate,Full-time,Engineering,Computer Software,2021-02-10 11:25:00
Data Engineer ,Johnson & Johnson,"Spring House, PA",18 hours ago,Be among the first 25 applicants,"['', 'Performs miscellaneous programming tasks to support in-house data tools, platforms, pipelines and workflows. ', 'Key Responsibilities', 'Proficiency in a general-purpose programming language such as Python, C, C++, Java, Scala, Go or similar is required.', ' Implements and supports data management solutions (storage and integration of molecular, clinical, and technical data and analysis outputs) and best practices in partnership with data scientists, IT partners and other key stakeholders.  Engineers infrastructure components and operational workflows for data access and integration. Performs miscellaneous programming tasks to support in-house data tools, platforms, pipelines and workflows.  Performs data curation and quality control as required. ', 'Implements and supports data management solutions (storage and integration of molecular, clinical, and technical data and analysis outputs) and best practices in partnership with data scientists, IT partners and other key stakeholders. ', 'Familiarity with standard bioinformatics workflows and approaches is preferred. ', 'Familiarity with a statistical programming language (R or SAS) is preferred. ', 'Performs data curation and quality control as required.', 'Familiarity with Amazon Web Services is required. ', 'Database management and programming (SQL, NoSQL) experience is required. ', 'Familiarity with high-performance computing environments is preferred.', 'Qualifications', 'Develops, documents, and supports standard procedures and best practices for data organization and exchange.', 'Strong oral and written communication skills are required.', ' A Ph.D. OR a Master’s degree with at least 2 years of relevant experience OR a bachelor’s degree with at least 8 years of experience in computer science, mathematics, bioinformatics, computational biology, physics, engineering or information technology is required.  Proficiency in a general-purpose programming language such as Python, C, C++, Java, Scala, Go or similar is required. Database management and programming (SQL, NoSQL) experience is required.  Familiarity with Amazon Web Services is required.  Experience with processing omics (transcriptomics, proteomics, genotypes etc.) or other high-dimensional data is required. Strong oral and written communication skills are required. Familiarity with high-performance computing environments is preferred. Demonstrated ability to work in diverse, interdisciplinary teams is required.  Familiarity with a statistical programming language (R or SAS) is preferred.  A working knowledge of web application design and programming is preferred.  Familiarity with standard bioinformatics workflows and approaches is preferred.  Familiarity with basics of biology or medicine is preferred. Prior experience in the pharmaceutical industry or a related field is preferred.', 'Engineers infrastructure components and operational workflows for data access and integration.', 'Description', 'A working knowledge of web application design and programming is preferred. ', ' Develops, documents, and supports standard procedures and best practices for data organization and exchange. ', 'Job Description', 'A Ph.D. OR a Master’s degree with at least 2 years of relevant experience OR a bachelor’s degree with at least 8 years of experience in computer science, mathematics, bioinformatics, computational biology, physics, engineering or information technology is required. ', 'Experience with processing omics (transcriptomics, proteomics, genotypes etc.) or other high-dimensional data is required.', 'Familiarity with basics of biology or medicine is preferred. Prior experience in the pharmaceutical industry or a related field is preferred.', 'Demonstrated ability to work in diverse, interdisciplinary teams is required. ']",Not Applicable,Full-time,Information Technology,Hospital & Health Care,2021-02-10 11:25:00
Data Engineer,Confidential,"Austin, Texas Metropolitan Area",16 hours ago,26 applicants,"['Benefits\xa0', 'Take total ownership results of design, solution, or ways to solve measurable problems.\xa0', 'Company is an equal opportunity employer that hires based on stellar qualifications, positive attitude, and exemplary work ethic rather than factors like age, gender identity, race, nationality, religion, or sexuality.\xa0', 'Unit Appreciation Rights: Up to 5% of yearly salary; based upon company and team performance.\xa0', 'Life at Company\xa0', ""Unit Appreciation Rights: Up to 5% of yearly salary; based upon company and team performance.\xa0Company Bonus: Up to 5% of yearly salary; based upon company and team performance.\xa0Health Insurance: Company offers two plans:\xa0o\xa0Aetna PPO: Company covers 100% of premiums for employees and 50% for your spouse and dependents included on the plan.\xa0Aetna HDHP HSA Plan: Company will contribute $500 to an HSA for an employee-only plan or $1000 for your spouse and dependents included on the plan.\xa0Dental Insurance: Company will pay half of the dental coverage for you/spouse/family plans.\xa0401k: Company partners with ForUsAll to provide the opportunity to invest in your future with pre-tax and post-tax plan options.\xa0Paid Time Off: untracked time off.\xa0Wedding Week: Enjoy an additional 5 paid days off before or after your wedding.\xa0Creating a Home: After 2 years of employment, Company will give you $5,000 when you buy a home.\xa0Year 3 Vacation: After 3 years of employment, you will be eligible for an all-inclusive vacation.\xa0Year 5 Sabbatical: After 5 years of employment, you will be eligible for a 2-week paid sabbatical.\xa0Donation Matching: Company will match your donation dollar for dollar, up to $250 a year and up to $1,000 if you've been here for 5 years.\xa0Community Involvement: Company encourages employees to take time off for volunteer opportunities throughout the year, including a semi-annual volunteer week in every community we serve.\xa0Product Discount: Enjoy a 20% discount on the products we sell.\xa0"", 'Year 3 Vacation: After 3 years of employment, you will be eligible for an all-inclusive vacation.\xa0', 'Experienced with data presentation tools (Power BI, Tableau, QlikView, etc.).\xa0', 'Working at Company is a once-in-a-lifetime opportunity to help build one of the fastest-growing ecommerce companies in history. We take on challenges that others would call impossible because we have a team of amazing, talented people who collaborate and think bigger together. At Company, you’ll create deep, personal connections and challenge yourself to achieve your most ambitious goals.\xa0', 'Company Bonus: Up to 5% of yearly salary; based upon company and team performance.\xa0', 'Year 5 Sabbatical: After 5 years of employment, you will be eligible for a 2-week paid sabbatical.\xa0', 'Develop, build, and cultivate strong relationships with all stakeholders that are built on trust and respect.\xa0', 'Company is an ecommerce company that connects brands with customers wherever they love to shop online. We delight our customers every day by putting our technology, marketing, and supply chain to work for them behind the scenes. Company has grown into offices and fulfillment centers in 8 cities across 6 states.\xa0', 'Bring energy on a daily basis.\xa0', '2+ years of experience in data warehouse modeling, design, and development.\xa0', 'Dental Insurance: Company will pay half of the dental coverage for you/spouse/family plans.\xa0', 'Product Discount: Enjoy a 20% discount on the products we sell.\xa0', 'Gain a deep understanding of Company’s data and how source system changes affect the data warehouse.\xa0Develop systems and processes to be fast, efficient, and scale to increasing business demands.\xa0Build excellent relationships with team mates and stakeholders that enable you to build the best solutions.\xa0Develop, build, and cultivate strong relationships with all stakeholders that are built on trust and respect.\xa0Collect and analyze system data to guide decisions for feature prioritization, scope, and design.\xa0Take total ownership results of design, solution, or ways to solve measurable problems.\xa0Bring energy on a daily basis.\xa0', 'Creating a Home: After 2 years of employment, Company will give you $5,000 when you buy a home.\xa0', 'Paid Time Off: untracked time off.\xa0', '2+ years of experience in ETL tools (SSIS, DataStage, Informatica, etc.) and OLAP tools.\xa0', 'Develop systems and processes to be fast, efficient, and scale to increasing business demands.\xa0', 'Gain a deep understanding of Company’s data and how source system changes affect the data warehouse.\xa0', 'As a Data Platform Engineer, you will:\xa0', 'Very strong with SQL development skills (stored procedures, views, functions, etc.).\xa0', '401k: Company partners with ForUsAll to provide the opportunity to invest in your future with pre-tax and post-tax plan options.\xa0', 'Who you are:\xa0', 'Collect and analyze system data to guide decisions for feature prioritization, scope, and design.\xa0', 'Quick to learn and adapt to new technologies.\xa0', 'Experienced with on-premises, hybrid, and cloud-based analytical solutions.\xa0', 'Community Involvement: Company encourages employees to take time off for volunteer opportunities throughout the year, including a semi-annual volunteer week in every community we serve.\xa0', 'Experienced with end-user data integration tools (Power Query, Alteryx, etc.).\xa0', '2+ years of experience in data warehouse modeling, design, and development.\xa02+ years of experience in ETL tools (SSIS, DataStage, Informatica, etc.) and OLAP tools.\xa0Very strong with SQL development skills (stored procedures, views, functions, etc.).\xa0Experienced with on-premises, hybrid, and cloud-based analytical solutions.\xa0Quick to learn and adapt to new technologies.\xa0Experienced with end-user data integration tools (Power Query, Alteryx, etc.).\xa0Experienced with data presentation tools (Power BI, Tableau, QlikView, etc.).\xa0', ""Donation Matching: Company will match your donation dollar for dollar, up to $250 a year and up to $1,000 if you've been here for 5 years.\xa0"", '\xa0', 'Aetna HDHP HSA Plan: Company will contribute $500 to an HSA for an employee-only plan or $1000 for your spouse and dependents included on the plan.\xa0', 'Health Insurance: Company offers two plans:\xa0o\xa0Aetna PPO: Company covers 100% of premiums for employees and 50% for your spouse and dependents included on the plan.\xa0', 'Wedding Week: Enjoy an additional 5 paid days off before or after your wedding.\xa0', 'Build excellent relationships with team mates and stakeholders that enable you to build the best solutions.\xa0', 'Equal Opportunity Employer\xa0']",Associate,Full-time,Engineering,Consumer Goods,2021-02-10 11:25:00
Data Engineer,SambaSafety,"Denver, CO",12 hours ago,Be among the first 25 applicants,"['', 'A chance to work with some of the brightest minds in technology', '401k match and generous Healthcare Benefits including a fully employer paid family medical plan', 'Most importantly you will be part of a team that is raising the bar when it comes to data, data movement and data stewardship. Working in a unique environment of “build it, test it, support it, own it” that makes your daily contributions something you can be proud of.', 'Deliver end to end comprehensive documentation along with code samples for other teams to leverage.', 'Strong communication skills. The ability to effectively explain technical concepts to team members, architects and team leads.', 'Some exposure to Hadoop, Hive, Spark, PrestoDB.', 'Participate in discussions around dimensional analysis and entity resolution for a complex disparate data sourced system.', 'Zoom Happy Hours', 'Experience with NoSQL, as well as relational data stores. PostgreSQL, Mysql, RedshiftDB, Redis, Cassandra, Snowflake, etc.', 'Who We Are', 'Unlimited Paid Time Off and Paid Volunteer Days401k match and generous Healthcare Benefits including a fully employer paid family medical planWellness &Tuition ReimbursementZoom Happy HoursFlexible Work From Home schedule & a Monthly Internet stipendLots of Samba swagSamba Virtual Events including our famous Samba SprintA chance to work with some of the brightest minds in technology', 'Understanding of micro-services architectures.', 'Wellness &Tuition Reimbursement', 'Flexible Work From Home schedule & a Monthly Internet stipend', 'Implement data classification of incoming data and manage access control.', 'Build an CICD automation pipeline facilitating automated deployment and automated testing.', 'What You’ll Do', 'Strong knowledge of modern software engineering principles, patterns and best-practices.', 'What You’ll Need', 'Code for and operationalize streaming pipelines to include data acquisition, staging, as well as integration of new data sources.', 'Degree in Computer Science, Software Engineering, or a related discipline.3+ years Java development experience, or an equivalent language with a desire to learn new things.Strong knowledge of modern software engineering principles, patterns and best-practices.Understanding of micro-services architectures.Experience designing and supporting high traffic, highly available systems.Strong communication skills. The ability to effectively explain technical concepts to team members, architects and team leads.Extensive experience deploying software to a cloud platform environment. AWS, GCP, Azure.Understanding of modern Devops concepts. Docker, Kubernetes, Serverless, Terraform.Experience with NoSQL, as well as relational data stores. PostgreSQL, Mysql, RedshiftDB, Redis, Cassandra, Snowflake, etc.Experience with distributed messaging and streaming technologies, RabbitMQ, Kinesis, Kafka, Spring cloud data flow, NiFi.Some exposure to Hadoop, Hive, Spark, PrestoDB.Capable of delivering on multiple competing priorities with little supervision.', 'Develop transformation processes for handling batch and streaming data.', 'Capable of delivering on multiple competing priorities with little supervision.', 'Code for and operationalize streaming pipelines to include data acquisition, staging, as well as integration of new data sources.Implement data classification of incoming data and manage access control.Develop transformation processes for handling batch and streaming data.Participate in discussions around dimensional analysis and entity resolution for a complex disparate data sourced system.Provide data usage pattern for analytics, API and other consumption patterns from target data store.Build an CICD automation pipeline facilitating automated deployment and automated testing.Deliver end to end comprehensive documentation along with code samples for other teams to leverage.Most importantly you will be part of a team that is raising the bar when it comes to data, data movement and data stewardship. Working in a unique environment of “build it, test it, support it, own it” that makes your daily contributions something you can be proud of.', '3+ years Java development experience, or an equivalent language with a desire to learn new things.', 'Benefits And Perks', 'Provide data usage pattern for analytics, API and other consumption patterns from target data store.', 'Data Engineer', 'Experience designing and supporting high traffic, highly available systems.', 'Experience with distributed messaging and streaming technologies, RabbitMQ, Kinesis, Kafka, Spring cloud data flow, NiFi.', 'Unlimited Paid Time Off and Paid Volunteer Days', 'Samba Virtual Events including our famous Samba Sprint', 'Extensive experience deploying software to a cloud platform environment. AWS, GCP, Azure.', 'Understanding of modern Devops concepts. Docker, Kubernetes, Serverless, Terraform.', 'Degree in Computer Science, Software Engineering, or a related discipline.', 'Lots of Samba swag']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-02-10 11:25:00
Data Engineer 3,Comcast,"Philadelphia, PA",11 hours ago,Be among the first 25 applicants,"['', 'Understand our Operating Principles; make them the guidelines for how you do your job.', 'Education', 'Core Responsibilities', 'Serves as a team leader within a work group or on cross-functional teams; accepts team lead stretch assignments, acts as a resource for colleagues with less experience. Presents to broader senior leadership team. Understands other business units and how they affect the team and work.', 'This information has been designed to indicate the general nature and level of work performed by employees in this role. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications.', 'Collaborates with technology and platform management partners to optimize data sourcing and processing rules to ensure appropriate data quality.', 'Creates technical documentation (internal) using understanding of internal business areas and processes as well as working knowledge of database design, data manipulation, ETL, implementation, information storage & retrieval and data flow & analysis.Preemptively recognize and resolve technical issues utilizing knowledge of policies and processes.Optimize data sources and processing rules to ensure appropriate data quality of all products utilizing trouble-shooting, design and development skills along with cross systems technical knowledge.Solves critical issues and shares knowledge such as trends, aggregate, quantity volume regarding specific data sources.Acts as a subject matter expert in one or more technical areas; such as data architecture, data engineering or data manipulation within big data systems like Hadoop and SQL.Serves as a team leader within a work group or on cross-functional teams; accepts team lead stretch assignments, acts as a resource for colleagues with less experience. Presents to broader senior leadership team. Understands other business units and how they affect the team and work.Acts as a liaison between business owners and technical associates to ensure the data collected and processed is both actionable and relevant to the end goals.Determines appropriateness of data for storage and optimum storage organization. Determines how tables relate to each other and how fields interact within the tables to develop relational models.Collaborates with technology and platform management partners to optimize data sourcing and processing rules to ensure appropriate data quality.Consistent exercise of independent judgment and discretion in matters of significance.Regular, consistent and punctual attendance. Must be able to work nights and weekends, variable schedule(s) and overtime as necessary.Other duties and responsibilities as assigned.', 'Be an active part of the Net Promoter System - a way of working that brings more employee and customer feedback into the company - by joining huddles, making call backs and helping us elevate opportunities to do better for our customers.', 'Disclaimer', 'Relevant Work Experience', 'Drive results and growth.', 'Employees At All Levels Are Expected To', 'Own the customer experience - think and act in ways that put our customers first, give them seamless digital options at every touchpoint, and make them promoters of our products and services.', 'Respect and promote inclusion & diversity.', 'Comcast is an EOE/Veterans/Disabled/LGBT employer.', 'Job Summary', 'Solves critical issues and shares knowledge such as trends, aggregate, quantity volume regarding specific data sources.', 'Acts as a liaison between business owners and technical associates to ensure the data collected and processed is both actionable and relevant to the end goals.', 'Consistent exercise of independent judgment and discretion in matters of significance.', 'Other duties and responsibilities as assigned.', ""Do what's right for each other, our customers, investors and our communities."", ""Understand our Operating Principles; make them the guidelines for how you do your job.Own the customer experience - think and act in ways that put our customers first, give them seamless digital options at every touchpoint, and make them promoters of our products and services.Know your stuff - be enthusiastic learners, users and advocates of our game-changing technology, products and services, especially our digital tools and experiences.Win as a team - make big things happen by working together and being open to new ideas.Be an active part of the Net Promoter System - a way of working that brings more employee and customer feedback into the company - by joining huddles, making call backs and helping us elevate opportunities to do better for our customers.Drive results and growth.Respect and promote inclusion & diversity.Do what's right for each other, our customers, investors and our communities."", 'Creates technical documentation (internal) using understanding of internal business areas and processes as well as working knowledge of database design, data manipulation, ETL, implementation, information storage & retrieval and data flow & analysis.', 'Optimize data sources and processing rules to ensure appropriate data quality of all products utilizing trouble-shooting, design and development skills along with cross systems technical knowledge.', 'Preemptively recognize and resolve technical issues utilizing knowledge of policies and processes.', 'Know your stuff - be enthusiastic learners, users and advocates of our game-changing technology, products and services, especially our digital tools and experiences.', 'Regular, consistent and punctual attendance. Must be able to work nights and weekends, variable schedule(s) and overtime as necessary.', 'Win as a team - make big things happen by working together and being open to new ideas.', 'Acts as a subject matter expert in one or more technical areas; such as data architecture, data engineering or data manipulation within big data systems like Hadoop and SQL.', 'Job Description', 'Determines appropriateness of data for storage and optimum storage organization. Determines how tables relate to each other and how fields interact within the tables to develop relational models.']",Not Applicable,Full-time,Information Technology,Information Technology and Services,2021-02-10 11:25:00
Senior Data Engineer,The Walt Disney Company,"Burbank, CA",7 hours ago,Be among the first 25 applicants,"['', 'Expertise in HANA data modeling and performance optimizationExpertise in SQL codingExpertise in ETL, data integration, data engineering, data design and data modelingExpertise in cloud-based data platformsExpertise with all aspects of data management: data governance, data mastering, meta-data management, data taxonomies and ontologiesExpertise in developing and delivering highly-scalable and flexible, cost effective, cloud-based enterprise data solutionsExpertise developing secure and compliant data solutionsExperience in designing security modelsExpertise in object-oriented software developmentFluency in Java, Javascript, SQL, PythonCurrent with new technologies and vendor solutions available in the marketplaceStrong collaboration skills and ability to establish and manage effective working relationships in a matrixed organizational structureStrong leadership skillsExcellent technical problem solving and data analysis skills; Strong communication and presentation skills. Ability to communicate well to both technical and business audiencesAbility to “influence without authority.” Strong consensus-building ability and negotiating skills.Strong communication and presentation skills, both oral and written. Ability to communicate well to both technical and business audiences.Innovative thinking and embraces innovation by constantly seeking new or contrary ideas and approaches to what has been done in the past. Thinks big and broad, without being constrained by existing norms or business models.Results oriented, biased toward actionKeeps fluent in new technologies and successful sharing with peers and introducing new technologiesChooses optimism and calls out cynicism and unproductive behaviorDemonstrates stamina, resilience and perseverance under trying circumstances', 'Data engineering and data modeling', 'Experience data modeling in multiple platforms including experience with HANA data modeling, SQL data warehouses', 'Chooses optimism and calls out cynicism and unproductive behavior', 'Experience in Snowflake', 'Experience in designing security models', 'In-depth experience in SAP HANA data modeling', 'Results oriented, biased toward action', 'Strong leadership skills', 'Expertise in HANA data modeling and performance optimization', 'Knowledge of S/4 HANA, CDS Views and data archiving concepts', 'Fluency in Java, Javascript, SQL, Python', 'Identify and promote best practices and patterns for data modeling and provide oversight for activities related to report migration and data consolidation.', 'Work with Business & Lead Architects to understand business processes along with reporting and analytical needs and convert the requirements into the Functional/Technical specification and runbooks', 'Experience', 'Keeping fluent in the industry and marketplace evolution--staying current with vendor product offerings and common and emerging data solutions in use across the industry; continously learning new data technologies and introducing these into the organization', 'Expertise in SQL coding', 'Basic Qualifications', 'Current with new technologies and vendor solutions available in the marketplace', 'Experience with BI tools such as Tableau, Cognos, Business Objects, SAP Analytics Cloud', 'Experience in SnowflakeExperience with BI tools such as Tableau, Cognos, Business Objects, SAP Analytics CloudExperience in data scienceKnowledge of S/4 HANA, CDS Views and data archiving concepts', 'In-depth experience in data engineering, ETL, data integration and data modelingIn-depth experience in SAP HANA data modelingExperience data modeling in multiple platforms including experience with HANA data modeling, SQL data warehousesExperience with data security and complianceExperience and with data lifecycle management, data-tiering and data archivingExperience with Finance and HCM data and business domains is a plusProven track record of execution--driving results and meeting commitments.5 years + in data engineering and data modeling', 'Design and developing data models across all of the organization’s data systems, especially in the major data platforms (data warehouses, data lakes, master data management systems).', 'Experience with data security and compliance', 'Expertise in cloud-based data platforms', 'Expertise in object-oriented software development', 'Strong collaboration skills and ability to establish and manage effective working relationships in a matrixed organizational structure', 'Job Summary', 'Expertise developing secure and compliant data solutions', 'Developing highly-scalable and flexible, cost-effective enterprise data solutions', 'Expertise with all aspects of data management: data governance, data mastering, meta-data management, data taxonomies and ontologies', 'Experience and with data lifecycle management, data-tiering and data archiving', 'Experience in data science', 'Proven track record of execution--driving results and meeting commitments.', 'Ability to “influence without authority.” Strong consensus-building ability and negotiating skills.', 'Demonstrates stamina, resilience and perseverance under trying circumstances', 'Responsibilities', 'Cross-training peers and mentoring teammates', 'Excellent technical problem solving and data analysis skills; Strong communication and presentation skills. Ability to communicate well to both technical and business audiences', 'Expertise in developing and delivering highly-scalable and flexible, cost effective, cloud-based enterprise data solutions', 'Keeps fluent in new technologies and successful sharing with peers and introducing new technologies', 'Data engineering and data modelingDesign and developing data models across all of the organization’s data systems, especially in the major data platforms (data warehouses, data lakes, master data management systems).Developing highly-scalable and flexible, cost-effective enterprise data solutionsDeveloping secure and compliant data solutionsWork with Business & Lead Architects to understand business processes along with reporting and analytical needs and convert the requirements into the Functional/Technical specification and runbooksIdentify and promote best practices and patterns for data modeling and provide oversight for activities related to report migration and data consolidation.Keeping fluent in the industry and marketplace evolution--staying current with vendor product offerings and common and emerging data solutions in use across the industry; continously learning new data technologies and introducing these into the organizationCross-training peers and mentoring teammates', 'In-depth experience in data engineering, ETL, data integration and data modeling', '5 years + in data engineering and data modeling', 'Preferred Qualifications', 'Expertise in ETL, data integration, data engineering, data design and data modeling', 'Experience with Finance and HCM data and business domains is a plus', 'Strong communication and presentation skills, both oral and written. Ability to communicate well to both technical and business audiences.', 'Developing secure and compliant data solutions', 'Innovative thinking and embraces innovation by constantly seeking new or contrary ideas and approaches to what has been done in the past. Thinks big and broad, without being constrained by existing norms or business models.']",Mid-Senior level,Full-time,Quality Assurance,Marketing and Advertising,2021-02-10 11:25:00
Data Engineer,2U,"Denver, CO",12 hours ago,Be among the first 25 applicants,"['', ' time off to volunteer for non-profit organizations parental leave after 9 months of employment holidays that include a winter break from Christmas through New Year and more! ', ' Degree in statistics, mathematics, computer science, economics, or other quantitative field required 2+ years of experience in data analysis and business intelligence 2+ years of experience in digital marketing Ability to understand business requirements and translate to data solutions Proficient in SQL and Python Experience in database design and database management Experience with data visualization and reporting tools, such as Tableau ', 'Familiarity with AWS and GCP platforms', 'Additional time off benefits include:', 'Unlimited snacks and drinks', 'Why It’s Great to Work at 2U', '2+ years of experience in digital marketing', 'Experience with data visualization and reporting tools, such as Tableau', ' Medical, dental, and vision coverage Life insurance, disability and 401(k) Unlimited snacks and drinks Tuition reimbursement program Generous paid leave policies including unlimited PTO for your vacation, personal, or sick days Additional time off benefits include:', 'Experience in database design and database management', 'time off to volunteer for non-profit organizations', 'Build, deploy, maintain, and improve applications built on REST API connections that enable marketing automation', 'Generous paid leave policies including unlimited PTO for your vacation, personal, or sick days', 'Conduct thoughtful analyses in order to provide data-driven insights through data mining and exploratory analysis', 'parental leave after 9 months of employment', 'Life insurance, disability and 401(k)', 'Medical, dental, and vision coverage', 'Degree in statistics, mathematics, computer science, economics, or other quantitative field required', 'Familiarity with REST API connections', 'Proficient in SQL and Python', 'holidays that include a winter break from Christmas through New Year and more!', 'Benefits', 'What We’re Looking For', 'Key Role And Responsibilities', ' Develop intimate understanding of internal business processes in order to build products and databases that anticipate user needs Build, deploy, maintain, and improve applications built on REST API connections that enable marketing automation Create, document, and deploy new data tables utilizing information from various disparate sources and collection methods  Utilize best practices for software development of high performance systems around design, coding, maintenance, and deployment  Conduct thoughtful analyses in order to provide data-driven insights through data mining and exploratory analysis ', 'About 2U Inc. (NASDAQ: TWOU)', '2+ years of experience in data analysis and business intelligence', 'Other Attributes That Will Help You In This Role', '2U Diversity and Inclusion Statement', 'Things That Should Be In Your Background', 'Ability to understand business requirements and translate to data solutions', 'Develop intimate understanding of internal business processes in order to build products and databases that anticipate user needs', ' Familiarity with REST API connections Familiarity with AWS and GCP platforms ', 'Utilize best practices for software development of high performance systems around design, coding, maintenance, and deployment ', 'Create, document, and deploy new data tables utilizing information from various disparate sources and collection methods ', 'Tuition reimbursement program']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-02-10 11:25:00
Data Engineer,Neudesic,"Houston, TX",1 hour ago,Be among the first 25 applicants,"['Data Analysis Knowledge: Understanding how data is collected, analyzed and utilized', 'Experience working with data API’s a plus', 'Integrity.', 'Team\xa0', 'Experience with database systems (SQL and/or NoSQL)', 'Excellent oral and written communication skills with a keen sense of customer service.', 'Solid understanding of concurrency and concurrent programming techniques', 'Disciplined\xa0', ""the core,\xa0Innovative\xa0by nature, committed to a\xa0Team\xa0and conduct themselves with\xa0Integrity.\xa0If these attributes mean something to you - we'd like to hear from you."", 'Excellent problem solving and troubleshooting skills.', 'About Neudesic', 'Required and Preferred Technical and Professional Expertise', 'Experience in\xa0Database development, Data Modeling, architecture, and storageStrong understanding of data structures and algorithmsSolid understanding of concurrency and concurrent programming techniquesKnowledge of functional programing languages and techniquesKnowledge of object-oriented programming languages and techniquesKnowledge of Agile Software Development methodologies and Azure DevOps including Git and GitHubData Analysis Knowledge: Understanding how data is collected, analyzed and utilized', 'Passion for technology drives us, but it’s innovation that defines us.\xa0From design to development and support to management, Neudesic offers decades of experience, proven frameworks and a disciplined approach to quickly deliver reliable, quality solutions that help our customers go to market faster.', 'Passionate', 'Strong understanding of data structures and algorithms', 'Bachelor’s degree (or higher) in Computer Science or related field with 3+ years’ experience required.', 'Experience with database systems (SQL and/or NoSQL)Data warehousing: experience in Microsoft traditional data warehousing and BI\xa0ETL tools: Azure Data Factory and Databricks a plusProgramming skills in Python, R, SQL and/or Scala a plusExperience working with data API’s a plusExposure to Azure Cloud services and big data processing solutions a plusExposure to Azure Synapse a plusExperience with Cloud Security a plus', 'What sets us apart from the rest, is an amazing collection of people who live and lead with our core values. We believe that everyone should be\xa0Passionate\xa0about what they do,\xa0Disciplined\xa0to', 'Please note - this position is located in Houston, Texas.', 'Able to convey information concisely and clearly with great technical documentation skills.', 'Knowledge of statistical analysis and machine learning is a plus.', 'Data warehousing: experience in Microsoft traditional data warehousing and BI\xa0', '.\xa0', 'Able to work closely and effectively with peer developers and work on several active projects simultaneously.', 'Innovative', 'Foundation Technical Skills', 'Knowledge of Agile Software Development methodologies and Azure DevOps including Git and GitHub', 'Exposure to Azure Synapse a plus', 'Desire to learn new technologies and languages required.', 'Our Predictive Enterprise Capability is comprised of some of the most respected and well-known architects as well as brilliant new developers and designers. Together, our teams of professionals have delivered some of the most innovative and leading edge implementations of Business Intelligence, Data Warehousing and Big Data solutions for the business-to-business as well as business-to-consumer space.', 'Role Profile', 'Programming skills in Python, R, SQL and/or Scala a plus', 'Knowledge Foundation', 'Bachelor’s degree (or higher) in Computer Science or related field with 3+ years’ experience required.Desire to learn new technologies and languages required.Excellent oral and written communication skills with a keen sense of customer service.Able to convey information concisely and clearly with great technical documentation skills.Excellent problem solving and troubleshooting skills.Able to work closely and effectively with peer developers and work on several active projects simultaneously.Knowledge of statistical analysis and machine learning is a plus.', 'Experience in\xa0Database development, Data Modeling, architecture, and storage', 'ETL tools: Azure Data Factory and Databricks a plus', '\xa0', 'Knowledge of object-oriented programming languages and techniques', 'Experience with Cloud Security a plus', 'Exposure to Azure Cloud services and big data processing solutions a plus', 'All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.', 'Knowledge of functional programing languages and techniques', 'Neudesic is an Equal Employment Opportunity Employer']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-02-10 11:25:00
Data Engineer,Amazon,"Seattle, WA",2 hours ago,Be among the first 25 applicants,"['', ' 2+ Years of experience developing and operating large-scale data structures for analytics', 'Company', 'Preferred Qualifications', ' Bachelor’s degree in Computer Science, Engineering, Mathematics, or a related field', ' 2+ Years Experience with at least one database technology such as Redshift, Oracle, MySQL or MS SQL', ' Master’s Degree in Computer Science, Engineering, Math, Statistics or related discipline', ' Demonstrated strength in SQL, Query performance tuning, Data modeling, ETL development, and Data warehousing.', ' Master’s Degree in Computer Science, Engineering, Math, Statistics or related discipline Experience with Big Data Technologies (Hadoop, Hive, Hbase, Pig, Spark, etc.) Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions Experience administering Tableau, Looker, Quicksight or other BI tools', 'Basic Qualifications', ' Coding proficiency in at least one modern programming language (Python, Ruby, Java, etc)', ' Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions', ' 2+ Years industry experience as a Data Engineer or related specialty (e.g., Software Engineer, Business Intelligence Engineer)', ' Experience administering Tableau, Looker, Quicksight or other BI tools', ' Experience with Big Data Technologies (Hadoop, Hive, Hbase, Pig, Spark, etc.)', ' Bachelor’s degree in Computer Science, Engineering, Mathematics, or a related field 2+ Years industry experience as a Data Engineer or related specialty (e.g., Software Engineer, Business Intelligence Engineer) 2+ Years Experience with at least one database technology such as Redshift, Oracle, MySQL or MS SQL 2+ Years of experience developing and operating large-scale data structures for analytics Demonstrated strength in SQL, Query performance tuning, Data modeling, ETL development, and Data warehousing. Coding proficiency in at least one modern programming language (Python, Ruby, Java, etc)', 'Description']",Not Applicable,Full-time,Information Technology,Computer Software,2021-02-10 11:25:00
Data Engineer,Centene Corporation,"Charlotte, NC",11 hours ago,Be among the first 25 applicants,"['', ' Contribute to the development and maintenance of real-time processing applications Contribute to the creation and maintenance of optimal data pipeline architectures Conduct maintenance and support for core infrastructure health, system upgrades, monitoring, CI/CD and logging Research streaming best practices and proper stream architecture Collaborate with team members to better understand existing data requirements and validation rules Analyze trends in data sets and contribute to the development of algorithms in order to improve upon the usefulness of raw data ', 'Contribute to the development and maintenance of real-time processing applications', 'Position Purpose', 'Contribute to the creation and maintenance of optimal data pipeline architectures', 'Conduct maintenance and support for core infrastructure health, system upgrades, monitoring, CI/CD and logging', 'Analyze trends in data sets and contribute to the development of algorithms in order to improve upon the usefulness of raw data', 'Education/Experience', 'Research streaming best practices and proper stream architecture', 'Centene is an equal opportunity employer that is committed to diversity, and values the ways in which we are different. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, veteran status, or other characteristic protected by applicable law.', 'Preferred', 'Collaborate with team members to better understand existing data requirements and validation rules']",Not Applicable,Full-time,Information Technology,Hospital & Health Care,2021-02-10 11:25:00
"Data Engineer, Users and Products",Google,"Boulder, CO",22 hours ago,Be among the first 25 applicants,"['', 'Experience writing and maintaining ETLs which operate on a variety of structured and unstructured sources.', 'Excellent communication, organizational, and analytical skills.', 'Design, develop and support data pipelines, warehouses and reporting systems to solve business operations, users and product problems.', 'Experience designing data models and data warehouses and using SQL and NoSQL database management systems.', ""Bachelor's degree in Computer Science, related technical field or equivalent practical experience."", 'Advanced degree in engineering or technical/scientific field of study. ', 'Write and review technical documents, including design, development, and revision documents.', 'Collaborate and influence Users and Products stakeholders and support engineers to ensure our data infrastructure meets constantly evolving requirements.', 'Mountain View, CA, USA; Boulder, CO, USA', 'Work closely with analysts to productionize various statistical and machine learning models using data processing pipelines.', 'Experience with Unix or GNU/Linux systems.', "" Bachelor's degree in Computer Science, related technical field or equivalent practical experience. Experience with one general purpose programming language (e.g., Java, C/C++, Python).  Experience in data processing using traditional and distributed systems (e.g., Hadoop, Spark, Dataflow, Airflow). Experience designing data models and data warehouses and using SQL and NoSQL database management systems. "", 'Experience in data processing using traditional and distributed systems (e.g., Hadoop, Spark, Dataflow, Airflow).', 'Experience designing data models and data warehouses and with non-relational data storage systems (NoSQL and distributed database management systems).', ' Advanced degree in engineering or technical/scientific field of study.  Experience designing data models and data warehouses and with non-relational data storage systems (NoSQL and distributed database management systems). Experience writing and maintaining ETLs which operate on a variety of structured and unstructured sources. Experience in large scale distributed data processing. Experience with Unix or GNU/Linux systems. Excellent communication, organizational, and analytical skills. ', 'About The Job', 'Responsibilities', ' Design, develop and support data pipelines, warehouses and reporting systems to solve business operations, users and product problems. Create extract, transform, and load (ETLs) and reporting systems for new data using a variety of traditional as well as large-scale distributed data systems. Collaborate and influence Users and Products stakeholders and support engineers to ensure our data infrastructure meets constantly evolving requirements. Work closely with analysts to productionize various statistical and machine learning models using data processing pipelines. Write and review technical documents, including design, development, and revision documents. ', 'Experience with one general purpose programming language (e.g., Java, C/C++, Python). ', 'Create extract, transform, and load (ETLs) and reporting systems for new data using a variety of traditional as well as large-scale distributed data systems.', 'Additional Information', 'Note: Disclosure as required by sb19-085 (8-5-20) of the minimum salary compensation for this role when being hired into our offices in Colorado.', 'Experience in large scale distributed data processing.']",Not Applicable,Full-time,Information Technology,Information Services,2021-02-10 11:25:00
Data Engineer,Tata Consultancy Services,"Charlotte, NC",7 hours ago,Be among the first 25 applicants,[''],Entry level,Full-time,Information Technology,Information Technology and Services,2021-02-10 11:25:00
Data Engineer,Team Cymru,"Lake Mary, FL",7 hours ago,Be among the first 25 applicants,"['', 'Working knowledge of networking protocols', 'Ability to create visualizations from resultant analytic results', 'Proficiency in designing and developing innovative data analytics software and methods.', ' ', 'Prolonged periods of sitting at a desk and working on a computer.', 'Supervisory Responsibilities:', 'Articulate in oral and written communication', 'High school diploma or equivalent requiredTypically has two to four years combined industry / education experienceSome specialized training or education beyond high school is preferred', 'Job Summary:', 'Operate independently and seeks assistance or guidance when required.', 'The Data Engineer utilizes a wide range of technologies to design, develop, and deploy innovative programming and technical solutions to data analytics and data processing. The Data Engineer is expected to demonstrate increased proficiency in newly acquired industry-related skills. This person can work independently and produce work according to clear-cut and complete specifications.', 'Required Skills/Abilities:', 'Competent with Linux', 'Typically has two to four years combined industry / education experience', 'Able to break-down complex requirements into workflows and identify key performance indicators.', 'Ability to recognize trends and patterns in the data that can be exploited into a repeatable analysis process', 'Creates quality product and support documentation', 'Education and Experience:', 'Embodies and demonstrates maturity, professionalism, and ethics', ""Familiarity with design patterns and industry best practicesExperience with one or alternative database technologies like: ElasticSearch, Apache Cassandra, Mongo DB, SparkExperience with Cloud technologies like: AWS, Google Cloud, AzureAbility to effectively create and utilize REST APIsProactively creates automated analytics solutions to push team's capabilities and increased situational awarenessKnowledge of MVC frameworksAbility to execute complex queries and design relational databases in PostgreSQL using referential integrity, views, stored procedures and proper indicesAbility to create visualizations from resultant analytic resultsExperience creating and distributing Jupyter Notebooks for repeatable data analysis"", 'Working-level knowledge of algorithms', 'Ability to effectively create and utilize REST APIs', 'Participates in regular review of individual output to ensure it conforms to department and company standards', 'High school diploma or equivalent required', ""Proficiency in designing and developing innovative data analytics software and methods.Contributes across whole project lifecycle, utilizing peers for guidance where necessary.Operate independently and seeks assistance or guidance when required.Ability to recognize trends and patterns in the data that can be exploited into a repeatable analysis processPerforms triage of product support requests, problem determination and assists with escalation when appropriateDemonstrates a complete understanding of a core-product or service offering's features, construction and operating characteristicsIncorporates effective test procedures, logging and monitoring in software with minimal oversightParticipates in regular review of individual output to ensure it conforms to department and company standardsContributes to efforts in maintaining and improving product qualityidentification and submission of product improvement when appropriateCreates quality product and support documentationIdentifies risks to projects, communicates and formulates mitigation plansActively contributes to cross-functional team effortsConducts self-assessments by comparing required skills with existing knowledge to develop, present and execute plans for improvementConsistently delivers to deadlines at the required quality standards"", 'Embodies and demonstrates maturity, professionalism, and ethicsArticulate in oral and written communicationWorking-level knowledge of algorithmsDemonstrates sound coding techniquesAble to break-down complex requirements into workflows and identify key performance indicators.Proficient in the use of databases: query and data definitionProficiency in one or more core languages: Golang, Python, SQL, Bash, Perl', 'Contributes to efforts in maintaining and improving product quality', 'Prolonged periods of sitting at a desk and working on a computer.Must be able to travel up to 5% of the time.', 'Contributes across whole project lifecycle, utilizing peers for guidance where necessary.', 'Identifies risks to projects, communicates and formulates mitigation plans', 'Familiarity with design patterns and industry best practices', 'Incorporates effective test procedures, logging and monitoring in software with minimal oversight', 'None.', 'Demonstrates sound coding techniques', 'Duties/Responsibilities:', 'Some specialized training or education beyond high school is preferred', 'Proficient in the use of databases: query and data definition', 'Competent with LinuxSolid oral and written communications skillsConsistently adheres to commitments with respect to delivery and timeframeWorking knowledge of networking protocols', 'Ability to execute complex queries and design relational databases in PostgreSQL using referential integrity, views, stored procedures and proper indices', 'Physical Requirements:', 'Experience creating and distributing Jupyter Notebooks for repeatable data analysis', 'Performs triage of product support requests, problem determination and assists with escalation when appropriate', 'Actively contributes to cross-functional team efforts', 'Knowledge of MVC frameworks', 'Data Engineer', 'Proficient in the use of industry standard tooling (i.e. the Atlassian Stack, etc.)', 'Virtual', 'Experience with Cloud technologies like: AWS, Google Cloud, Azure', ""Proactively creates automated analytics solutions to push team's capabilities and increased situational awareness"", 'Must be able to travel up to 5% of the time.', 'Consistently delivers to deadlines at the required quality standards', 'Solid oral and written communications skills', 'Proficiency in one or more core languages: Golang, Python, SQL, Bash, Perl', 'Experience with one or alternative database technologies like: ElasticSearch, Apache Cassandra, Mongo DB, Spark', 'Location:', 'Conducts self-assessments by comparing required skills with existing knowledge to develop, present and execute plans for improvement', 'identification and submission of product improvement when appropriate', 'Additional Desired Skills/Abilities', 'Consistently adheres to commitments with respect to delivery and timeframe', ""Demonstrates a complete understanding of a core-product or service offering's features, construction and operating characteristics""]",Entry level,Full-time,Engineering,Information Technology and Services,2021-02-10 11:25:00
Data Engineer,Cotiviti,"Atlanta, GA",12 hours ago,Be among the first 25 applicants,"['', 'Extensive knowledge in relational database design', 'Excellent written and verbal communication skills, with the ability to multitask and prioritize projects to meet scheduled deadlines.', 'Note: For the safety of our employees and those considering employment with Cotiviti, we are currently conducting all interviews virtually. In addition, the majority of the Cotiviti team is currently working remotely, and we are onboarding new hires remotely as well. As we monitor the pandemic, these arrangements may change and we will update accordingly.', 'Develop complex, interdependent Data Load Processes, and managing execution of those plans', 'Principal Responsibilities And Essential Duties', 'Mentor, develop and train team members on various aspects of Data Architecture Implementation', 'Manage Oracle or SQL Server databases hosting large and complex datasets in Healthcare domain', 'Develop, construct, test and maintain Data Architecture for Business Intelligence Solutions', 'Work with product and technical teams to understand the business requirements and build scalable and sustainable enterprise reporting solutions', 'Experience with RDBMS (SQL, Oracle, SQL, Vertica, etc.) and using T-SQL or other data integration/ETL tools.', 'Bachelor’s degree required. ', 'Experience with data aggregation, standardization, linking, quality check mechanisms, and reporting.', 'Requirements', 'Review & test the data to ensure accuracy & validity of the data prior to uploading the data to the warehouse', 'Work as a team member in creation and maintenance of ETL scripts, tools, queries and applications used for data management, data validation, and program validation.', ' Bring customer centric focus to our Internal Benchmarking platform  Work with product and technical teams to understand the business requirements and build scalable and sustainable enterprise reporting solutions Demonstrate through proof of concepts and present them to stakeholders. Formulate and provide recommendations. Develop, construct, test and maintain Data Architecture for Business Intelligence Solutions Develop complex, interdependent Data Load Processes, and managing execution of those plans Identify opportunities for Process Automation using ETL tools like Streamsets, SSIS, Python ETL Framework, etc. Analyze large and complex datasets using SQL queries and Business Intelligence tools like Tableau to provide actionable insights Mentor, develop and train team members on various aspects of Data Architecture Implementation Work as a team member in creation and maintenance of ETL scripts, tools, queries and applications used for data management, data validation, and program validation. Manage Oracle or SQL Server databases hosting large and complex datasets in Healthcare domain Program per data transformation specifications to convert source data to be loaded into target data warehouse tables using T-SQL and other Data Integration/ETL tools. Review & test the data to ensure accuracy & validity of the data prior to uploading the data to the warehouse ', 'Program per data transformation specifications to convert source data to be loaded into target data warehouse tables using T-SQL and other Data Integration/ETL tools.', 'Analyze large and complex datasets using SQL queries and Business Intelligence tools like Tableau to provide actionable insights', 'Familiarity with Agile methodologies preferred.', 'Demonstrate through proof of concepts and present them to stakeholders. Formulate and provide recommendations.', 'Ability to work well independently or in a team environment.', ' Experience developing product/solutions using MS SQL Server, SSIS and SSRS Knowledge of dimensional modelling and experience building reports in MicroStrategy (9.3.1 or above) or any other BI reporting tool (Tableau) Experience working with large and complex datasets Extensive knowledge in relational database design Experience with data aggregation, standardization, linking, quality check mechanisms, and reporting. Experience with RDBMS (SQL, Oracle, SQL, Vertica, etc.) and using T-SQL or other data integration/ETL tools. 3-5 years experience in the Analysis, design and development of solutions and strategies for creating extraction, transformation and loading (ETL) and real-time applications. Excellent written and verbal communication skills, with the ability to multitask and prioritize projects to meet scheduled deadlines. Ability to work well independently or in a team environment. Bachelor’s degree required.  Familiarity with Agile methodologies preferred. Critical thinker with strong analytical and problem solving skills ', '3-5 years experience in the Analysis, design and development of solutions and strategies for creating extraction, transformation and loading (ETL) and real-time applications.', 'Knowledge of dimensional modelling and experience building reports in MicroStrategy (9.3.1 or above) or any other BI reporting tool (Tableau)', 'Critical thinker with strong analytical and problem solving skills', 'Bring customer centric focus to our Internal Benchmarking platform ', 'Experience working with large and complex datasets', 'Experience developing product/solutions using MS SQL Server, SSIS and SSRS', 'Identify opportunities for Process Automation using ETL tools like Streamsets, SSIS, Python ETL Framework, etc.']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-02-10 11:25:00
Data Analytics Engineer,North Highland,"Atlanta, GA",8 hours ago,Be among the first 25 applicants,"['', ' Ability to gather and document business requirements and translate them to technical requirements ', ' Why North Highland? ', ' Actively contribute to business development proposals and the identification of new opportunities ', '  Use knowledge of data analytics to increase client impact, develop your skill-sets and grow your career   Work with new and emerging technologies   Work on advanced analytical projects for clients under supervision of lead data scientist   Effectively communicating with internal teams and external clients to understand the business problem, work through solutioning, and present analytic findings to the client   Building effective visual presentations and analytical dashboards using PowerBI, Tableau and other tools to present analytical findings   Handling large volumes of data in a variety of formats for preparation of data insights   Rapid prototyping and exploratory analytical work in the North Highland Insight Lab using commercial, public and private data   Being a part of a collaborative team working to creatively solve complex technical and business problems that in the end drive real business value   Make your mark by working directly with clients as a visible and engaged member of the team   Collaborate across disciplines to deliver creative solutions to client challenges   Actively contribute to business development proposals and the identification of new opportunities   Establish positive relationships with clients and peers that build credibility, foster your support network and empower career development   Develop a deeper understanding of our firm’s shared vision to build our clients capabilities and unleash their potential  ', ""  Master's degree in MIS or Analytics   Professional certifications in data technologies (BI tools, cloud technologies, ETL, etc.)   Familiarity with dimensional modeling techniques (star schemas, facts, dimensions, etc.)   Ability to build and maintain relationships across the organization, and influence senior management, peers and staff through an inclusive style and recognition of their abilities to achieve results  "", ' Professional certifications in data technologies (BI tools, cloud technologies, ETL, etc.) ', ' Solid SQL skills via exposure to RDBMS’s such as Amazon Redshift, Google Cloud BigQuery, SQL Server, Oracle, Teradata, postgres, etc. ', ' Willingness to travel to fulfill client requirements and project needs ', ' Familiarity with the scripting data and machine learning ecosystems - (Jupyter Notebooks, scikit-learn, SciPy, NumPy, pandas, Matplotlib, TensorFlow, etc.) ', ' Solid interpersonal and communication skills (written and verbal) to technical and non-technical audience of wide variety of levels including client-facing senior management ', ' Working knowledge of data movement ETL technologies Stored procedures, SSIS, Informatica, Alteryx ', 'The Exciting Work You Will Do', ' Establish positive relationships with clients and peers that build credibility, foster your support network and empower career development ', ' to apply ', ' Building effective visual presentations and analytical dashboards using PowerBI, Tableau and other tools to present analytical findings ', ' Experience with ""Big Data"" environments ', ' At North Highland, you’re never a number. ', ' Being a part of a collaborative team working to creatively solve complex technical and business problems that in the end drive real business value ', ' Click ', ' Handling large volumes of data in a variety of formats for preparation of data insights ', ' HERE ', ' Analytical scripting language skills, such as Python, R, SAS ', ' Data Visualization tools: Tableau, Qlikview, Power BI ', ' We started as three leaders and a kitchen table. ', 'LEAVE YOUR MARK ON A BETTER WORLD. ', ' Ability to self-direct and manage priorities; ability to handle issues that are not well-defined and/or conflict with available information; ability to successfully navigate ambiguity to resolution ', ' Effectively communicating with internal teams and external clients to understand the business problem, work through solutioning, and present analytic findings to the client ', ' Familiarity with dimensional modeling techniques (star schemas, facts, dimensions, etc.) ', ' Use knowledge of data analytics to increase client impact, develop your skill-sets and grow your career ', "" Bachelor's Degree in Information Systems, Computer Science or Computer Engineering or other related fields "", 'Qualifications', ' Collaborate across disciplines to deliver creative solutions to client challenges ', ' Work on advanced analytical projects for clients under supervision of lead data scientist ', '  At least 3-7 years of hands-on data experience   Motivated self-starter enjoying the fast-paced world of analytics consulting   Bachelor\'s Degree in Information Systems, Computer Science or Computer Engineering or other related fields   Solid interpersonal and communication skills (written and verbal) to technical and non-technical audience of wide variety of levels including client-facing senior management   Analytical scripting language skills, such as Python, R, SAS   Data Visualization tools: Tableau, Qlikview, Power BI   Familiarity with the scripting data and machine learning ecosystems - (Jupyter Notebooks, scikit-learn, SciPy, NumPy, pandas, Matplotlib, TensorFlow, etc.)   Experience with wide array of analytical approaches (correlation analysis, predictive and explanatory modeling, data mining, unsupervised clustering, analysis of unstructured data)   Solid SQL skills via exposure to RDBMS’s such as Amazon Redshift, Google Cloud BigQuery, SQL Server, Oracle, Teradata, postgres, etc.   Familiarity with cloud technologies Data warehouse (BigQuery, RedShift) / cloud storage (GS, S3) / app engine (GCP App Engine, AWS Elastic Beanstalk)   Experience with ""Big Data"" environments   Working knowledge of data movement ETL technologies Stored procedures, SSIS, Informatica, Alteryx   Ability to gather and document business requirements and translate them to technical requirements   Understanding of business functions (sales, finance, marketing, operations) and how they relate to business intelligence solutions   Has the investigative mindset and related skills that can be used with any number of tools to explore available data   Ability to self-direct and manage priorities; ability to handle issues that are not well-defined and/or conflict with available information; ability to successfully navigate ambiguity to resolution   Willingness to travel to fulfill client requirements and project needs  ', ' Applicants must be authorized to work in the United States without the need for visa sponsorship by North Highland. Work visa sponsorship will not be provided, now or in the future, for this position.  Click   HERE   to apply North Highland makes change happen for organizations who dare to be different. By melding workforce, customer and operational transformation, they are one of the world’s leading consulting groups, with 65+ offices around the globe. They break new ground today so tomorrow is easier to explore.For more information, visit northhighland.com and connect with us on LinkedIn , Twitter and Facebook .North Highland is an Equal Employment Opportunity (EEO)/Affirmative Action employer. All qualified applicants will receive fair and impartial consideration without regard to race, color, sex, gender identity, religion, national origin, age, sexual orientation, disability, veteran status, or any other characteristic protected by law.', ' Familiarity with cloud technologies Data warehouse (BigQuery, RedShift) / cloud storage (GS, S3) / app engine (GCP App Engine, AWS Elastic Beanstalk) ', ' Develop a deeper understanding of our firm’s shared vision to build our clients capabilities and unleash their potential ', ' Has the investigative mindset and related skills that can be used with any number of tools to explore available data ', ' Data Analytics Engineer, Analyst ', ' Rapid prototyping and exploratory analytical work in the North Highland Insight Lab using commercial, public and private data ', ' Make your mark by working directly with clients as a visible and engaged member of the team ', '  Applicants must be authorized to work in the United States without the need for visa sponsorship by North Highland. Work visa sponsorship will not be provided, now or in the future, for this position.  Click   HERE   to apply North Highland makes change happen for organizations who dare to be different. By melding workforce, customer and operational transformation, they are one of the world’s leading consulting groups, with 65+ offices around the globe. They break new ground today so tomorrow is easier to explore.For more information, visit northhighland.com and connect with us on LinkedIn , Twitter and Facebook .North Highland is an Equal Employment Opportunity (EEO)/Affirmative Action employer. All qualified applicants will receive fair and impartial consideration without regard to race, color, sex, gender identity, religion, national origin, age, sexual orientation, disability, veteran status, or any other characteristic protected by law.', ' Work with new and emerging technologies ', 'Preferred Skills', ' Motivated self-starter enjoying the fast-paced world of analytics consulting ', ' Ability to build and maintain relationships across the organization, and influence senior management, peers and staff through an inclusive style and recognition of their abilities to achieve results ', ' Understanding of business functions (sales, finance, marketing, operations) and how they relate to business intelligence solutions ', ' At least 3-7 years of hands-on data experience ', "" Master's degree in MIS or Analytics "", ' MAKE CHANGE HAPPEN. ', ' Experience with wide array of analytical approaches (correlation analysis, predictive and explanatory modeling, data mining, unsupervised clustering, analysis of unstructured data) ', ' COLLABORATE WITH AMAZING PEOPLE. ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-02-10 11:25:00
Data Engineer 3,PayPal,"San Jose, CA",14 hours ago,Be among the first 25 applicants,"['', 'Being flexible and being able to support all functions of product life cycle when required.', 'Working experience in any MPP systems, should have strong SQL programming skills', 'Strong analytical skills including the ability to define problems, collect data, establish facts, and draw valid conclusions', 'Advanced Shell or Perl scripting experience or proficiency in any programming language like Java, Python, Scala, C, C++', 'Be a Mentor for the junior members in the organization.', 'Familiar with data movement techniques and best practices to handle large volumes of data', 'Experience with File Systems, server architectures, and distributed systems', 'Expertise in database programming and performance tuning techniques', 'Knowledge in MPP Databases/ Distributed systemsKnowledge on Big data Environments and Cloud integration is a plusExposure to Data Quality and Profiling tools is a plus.Exposure to BI tools desired, but not required (Tableau, Micro strategy, Business Objects).', 'Exposure to Data Quality and Profiling tools is a plus.', 'Job Description:', 'Desired Skills:', 'Knowledge on Big data Environments and Cloud integration is a plus', 'Excellent written and oral communication skills', 'Participating and collaborating with cross functional teams in the organization to understand the business requirements and to deliver solutions that can scale.', 'Knowledge in MPP Databases/ Distributed systems', 'Creativity and out of the box thinking is required.', 'Ability to work in a fast-paced environment', 'Ability to deliver from coarse grained requirements', 'Experience with data warehousing architecture and data modeling best practices', 'Proactively anticipating problems and appropriately communicating to the team and management in a timely manner.', 'Working experience in an Agile methodology is highly preferred', 'Responsibilities:', 'Knowledge of Hadoop, Spark, HBase and Hive is highly preferred', 'Planning the execution of the project in an effective and efficient manner.', 'Required Skills:', '8+ years of experience in the IT industry, Data Engineering & Technology and Big Data Engineering is preferred.', 'Strong communication skills and willingness to take initiative to contribute beyond basic responsibilities', 'Exposure to BI tools desired, but not required (Tableau, Micro strategy, Business Objects).', '8+ years of experience in the IT industry, Data Engineering & Technology and Big Data Engineering is preferred.Advanced Shell or Perl scripting experience or proficiency in any programming language like Java, Python, Scala, C, C++Working experience in any MPP systems, should have strong SQL programming skillsKnowledge of data warehousing conceptsWorking knowledge on Big Data, Streaming IntegrationsExcellent written and oral communication skillsStrong analytical skills including the ability to define problems, collect data, establish facts, and draw valid conclusionsExpertise in database programming and performance tuning techniquesFamiliar with data movement techniques and best practices to handle large volumes of dataExperience with data warehousing architecture and data modeling best practicesExperience with File Systems, server architectures, and distributed systemsStrong communication skills and willingness to take initiative to contribute beyond basic responsibilitiesWorking experience in an Agile methodology is highly preferredKnowledge of Hadoop, Spark, HBase and Hive is highly preferred', 'Job Requirements', 'Knowledge of data warehousing concepts', 'Approaching the problem, taking into account all possibilities.', 'Working knowledge on Big Data, Streaming Integrations', 'Participating and collaborating with cross functional teams in the organization to understand the business requirements and to deliver solutions that can scale.Planning the execution of the project in an effective and efficient manner.Approaching the problem, taking into account all possibilities.Creativity and out of the box thinking is required.Proactively anticipating problems and appropriately communicating to the team and management in a timely manner.Being flexible and being able to support all functions of product life cycle when required.Ability to work in a fast-paced environmentAbility to deliver from coarse grained requirementsBe a Mentor for the junior members in the organization.', 'Job Description Summary:']",Not Applicable,Full-time,Information Technology,Computer Software,2021-02-10 11:25:00
Entry Level Data Engineer (AI Applications),IBM,"Littleton, MA",2 hours ago,Be among the first 25 applicants,"['', 'Basic knowledge in database technologies', 'Passionate about each client and their successBuilding new data ingestion pipelines, and adapting existing ones to changing data sources and changing client data requirementsWorking with others to design new ways of ingesting data that makes it faster and easierKnowledgeable and skilled with one or more languages like Python, Java, Scala and JuliaKnowledgeable and/or willing to learn about different database technologies, Hadoop and Cloud development and deliveryConscientious about deadlines and deliverablesExcited to continue to learn and develop new skillsContributing to the governance and curation of the data, and tooling to support these activities', 'Preferred Technical And Professional Expertise', 'Previous internship or co-op experience as a Data Engineer', 'Contributing to the governance and curation of the data, and tooling to support these activities', 'Conscientious about deadlines and deliverables', 'Knowledgeable and skilled with one or more languages like Python, Java, Scala and Julia', 'As a Team Member You Will Be', 'About Business Unit', 'Working with others to design new ways of ingesting data that makes it faster and easier', 'Basic knowledge in Hadoop', 'Knowledgeable and/or willing to learn about different database technologies, Hadoop and Cloud development and delivery', 'Passionate about each client and their success', 'About IBM', 'Excited to continue to learn and develop new skills', 'Basic knowledge in one or more of the following languages: Python, Java, Scala, Julia', 'Building new data ingestion pipelines, and adapting existing ones to changing data sources and changing client data requirements', 'Basic knowledge in one or more of the following languages: Python, Java, Scala, JuliaBasic knowledge in database technologiesBasic knowledge in Hadoop']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-02-10 11:25:00
BI Data Engineer,Cresco Labs,"Chicago, IL",11 hours ago,Be among the first 25 applicants,"['', 'Entrepreneur', 'Cresco Labs is an Equal Opportunity Employer and all applicants will be considered without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran, or disability status.', 'Must be 21 years of age or older to apply', 'Must comply with all legal or company regulations for working in the industry ', 'Required Experience, Education And Skills', ""Bachelor's degree in computer science or a similar field, a plus"", '2+ years in a strong analytical background ', 'Strong knowledge in database technologies and web technologies', ' Must be 21 years of age or older to apply Must comply with all legal or company regulations for working in the industry  ', 'Implement data parsing and data cleansing to improve overall data quality and productivity', 'Manage progress, goals, and insights with business leadership', 'Collect, manage analyze and visualize large data sets while maintaining ETL pipelines', 'Stay up to data on data science trends and developments', 'High level organization and structure', "" Bachelor's degree in computer science or a similar field, a plus 2+ years in a strong analytical background  Practical experience buildings out and implementing dashboards with Tableau/Power BI or other business intelligence and analytics software Strong knowledge in database technologies and web technologies Excellent written and verbal communication skills High level organization and structure Passion for data analytics and business intelligence  "", ' Design and build dynamic dashboards to support our unique requirements for visualization, security, data access, etc. Collect, manage analyze and visualize large data sets while maintaining ETL pipelines Implement data parsing and data cleansing to improve overall data quality and productivity Collaborate with business stakeholders and other BI Engineers in identifying opportunities to build and analyze metrics Manage progress, goals, and insights with business leadership Stay up to data on data science trends and developments Support the VP of Software Engineering through data analysis and reporting Automate and document processes to ensure efficiency ', 'Job Summary', 'Practical experience buildings out and implementing dashboards with Tableau/Power BI or other business intelligence and analytics software', 'Additional Requirements', 'Passion for data analytics and business intelligence ', 'Collaborate with business stakeholders and other BI Engineers in identifying opportunities to build and analyze metrics', 'MISSION STATEMENT', 'Automate and document processes to ensure efficiency', 'Support the VP of Software Engineering through data analysis and reporting', 'Company Overview', 'Excellent written and verbal communication skills', 'Design and build dynamic dashboards to support our unique requirements for visualization, security, data access, etc.', 'Core Job Duties']",Entry level,Full-time,Business Development,Consumer Goods,2021-02-10 11:25:00
Data Engineer,Titan Advanced Energy Solutions,"Salem, MA",20 hours ago,43 applicants,"['', '·\xa0\xa0\xa0\xa0\xa0\xa0Attention to detail, effective time management, and pride in work', 'Basic understanding of electrical systems and signals (voltage, current, etc.)', 'Passion for managing large multi-dimensional data sets', 'Core Responsibilities', 'Personal Values:', 'BS/BA in data science, computer science, engineering, statistics, physics, math, or similar', '·\xa0\xa0\xa0\xa0\xa0\xa0Dependable, trustworthy, empathetic & full of integrity', 'Expert knowledge in statistics and hypothesis testing', 'Expert programming in Python', 'Experience working with non-relational databases', 'Agile Methodologies and/or Lean Practices', 'Learn more at www.titanaes.com', 'Located in Salem, MA, Titan Advanced Energy Solutions (Titan) is developing and producing ultrasound-based battery management diagnostic systems for lithium ion batteries (LiBs) for the consumer electronics, automotive, storage, and Second Life markets. Our patented technology provides accurate and practically instantaneous measures of state of charge (SoC) and state of health (SoH). By measuring the battery with this improved accuracy, we provide significant benefits to the whole value chain, including an increase in daily usable charge capacity, longer battery life, unprecedented safety monitoring and control, and real-time cell level monitoring and control. Titan’s value to the LiB is multi-fold, providing net benefits to manufacturers, integrators, producers, and consumers.', 'Titan’s innovative strides have been recognized with numerous awards and funding from top clean energy programs and institutions, including Greentown Labs, the Massachusetts Clean Energy Center (MassCEC) and the Department of Energy. Growing and poised to continue on their positive momentum, this is an exciting time to join the Titan team!', 'Document and communicate processes to larger team', 'Required Experience & Skills', 'Use in-house software to conduct data and signal preprocessing', 'Expert programming in PythonExperience with time-domain signal processingExperience working with non-relational databasesExpert knowledge in statistics and hypothesis testingExperience with expert-level statistical software (R, or similar)Agile Methodologies and/or Lean PracticesStart-up experience a plusTechnology development and commercializationClear oral and written communication and leadership skills', 'Start-up experience a plus', '·\xa0\xa0\xa0\xa0\xa0\xa0Bias to action, self-motivated and entrepreneurial spirit', 'BS/BA in data science, computer science, engineering, statistics, physics, math, or similar2+ years of experience in data engineer / analyst role, or similarPassion for managing large multi-dimensional data setsBasic understanding of electrical systems and signals (voltage, current, etc.)Basic programming in Python (including Numpy, Pandas, Matplotlib)Basic knowledge in statistics including hypothesis testing (t-test, ANOVA, etc.)', 'Receive data from the laboratory on daily basisUse in-house software tools to assess data quality and report issuesConduct preliminary analyses of data including plotting and simple statisticsUse in-house software to conduct data and signal preprocessingMerge related data sets and validateDocument and communicate processes to larger team', 'Preferred Skills & Experience:', 'Merge related data sets and validate', 'Basic programming in Python (including Numpy, Pandas, Matplotlib)', 'Are you a Data Engineer with 2+ years of progressive experience looking to join an innovative and dynamic team and the thought of enabling higher adoption of renewable energy and zero emission mobility globally gets you excited?', 'Preferred', 'Experience with time-domain signal processing', '\xa0', 'Conduct preliminary analyses of data including plotting and simple statistics', '2+ years of experience in data engineer / analyst role, or similar', 'Reporting to the Head of Algorithm Development, the Data Engineer will work with our multi-disciplined, fast-pace team on a variety of interesting projects. We are looking for an independent thinker who is highly self-motivated, driven to achieve, goal and team-oriented, and passionate about clean technology.', 'Use in-house software tools to assess data quality and report issues', 'Clear oral and written communication and leadership skills', 'Skills & Experience:', 'Experience with expert-level statistical software (R, or similar)', 'Required Experience & Skills:', 'Receive data from the laboratory on daily basis', 'Technology development and commercialization', 'Titan welcomes applicants from every background – our diversity helps us thrive and serve our customers and each other. All employment decisions are based on qualifications, merit and business needs, without discrimination or bias. We are proud to be an equal opportunity employer. If you need assistance or an accommodation due to a disability, please let us know.', '·\xa0\xa0\xa0\xa0\xa0\xa0Strong collaborative communication skills; able to build consensus internally and externally', 'Basic knowledge in statistics including hypothesis testing (t-test, ANOVA, etc.)']",Entry level,Full-time,Information Technology,Electrical/Electronic Manufacturing,2021-02-10 11:25:00
Data Engineer,Agilarc LLC,"Pittsburgh, PA",,N/A,"['', 'Working with senior Agilarc consultants to provide data management solutions to vlients Ability to uncover and understand client business questions an interpret those requirements to architect and design various data solutions Reformulating existing frameworks to optimize their functioning. Testing such structures to ensure that they are fit for use. Preparing raw data for manipulation by business users. Remaining up-to-date with industry standards and technological advancements that will improve the quality of your outputs. ', 'Data Engineer Requirements:', ' ', ' Data Engineer Responsibilities: ', 'A curiosity for data and a desire for independent learning for continuous personal develoipment. ', 'Data Engineer Responsibilities:', 'Working with senior Agilarc consultants to provide data management solutions to vlients ', ""Bachelor's degree in Data Engineering, Big Data Analytics, Computer Engineering, Information Technology - Data Focused or related fields in Data Management Proficiency in SQL and various relational database management systems. Excellent analytical and problem-solving skills. A curiosity for data and a desire for independent learning for continuous personal develoipment. Capacity to successfully manage a pipeline of duties with minimal supervision. An Agile Mindset of Collaboration,\xa0Continual improvement, Positive response to Change and Success for the Team."", 'Testing such structures to ensure that they are fit for use. ', ' Data Engineer Requirements: ', 'Agilarc is looking for entry level data engineers that have a passion for data and a desire to solve business problems for a variety of clients across different industries.\xa0 The candidate will be teamed up with Senior Agilarc Resources as part of an agile team that works to solve data management problems accross the entire data lifecycle.\xa0 ', 'An Agile Mindset of Collaboration,\xa0Continual improvement, Positive response to Change and Success for the Team.', 'Excellent analytical and problem-solving skills. ', ""Bachelor's degree in Data Engineering, Big Data Analytics, Computer Engineering, Information Technology - Data Focused or related fields in Data Management "", 'Remaining up-to-date with industry standards and technological advancements that will improve the quality of your outputs. ', 'Proficiency in SQL and various relational database management systems. ', 'Capacity to successfully manage a pipeline of duties with minimal supervision. ', 'Ability to uncover and understand client business questions an interpret those requirements to architect and design various data solutions ', 'Reformulating existing frameworks to optimize their functioning. ', 'Preparing raw data for manipulation by business users. ']",Entry level,Full-time,Consulting,Information Technology and Services,2021-02-10 11:25:00
Data Engineer,RealREPP,"Irvine, CA",14 hours ago,Be among the first 25 applicants,"['', ""Bachelor's Degree in Engineering, Computer Science, Statistics, or any other related field"", 'Work alongside fellow data scientists in constructing high quality data', "" Bachelor's Degree in Engineering, Computer Science, Statistics, or any other related field Masters in any related quantitative field preferred  3+ years' experience with SQL, database engineering, scripting Prior experience with the full cycle of projects from start to finish Working knowledge of coding requirements and standards Ability to communicate with others well Ability to work well in fast-paced IT environment Prior experience with big data, budgeting, asset management Leadership and interpersonal skills "", 'Provide recommendations to improve efficiency of data', "" Manage the design and implementation of Business Intelligence (BI) Oversee various tools/resources such as Redshift and EMR Work alongside fellow data scientists in constructing high quality data Implement and update the reporting processes  Ensure data accuracy and reliability  Develop/test architectures and ensure they align with company's business requirements Provide recommendations to improve efficiency of data Analyze raw data and identify trends Provide expertise and knowledge of SQL database design  "", 'Prior experience with big data, budgeting, asset management', 'Oversee various tools/resources such as Redshift and EMR', 'Analyze raw data and identify trends', 'Ability to communicate with others well', ""Develop/test architectures and ensure they align with company's business requirements"", 'Implement and update the reporting processes ', 'Working knowledge of coding requirements and standards', 'Ensure data accuracy and reliability ', 'Leadership and interpersonal skills', 'Requirements', 'Masters in any related quantitative field preferred ', 'Job Responsibilities', 'Prior experience with the full cycle of projects from start to finish', ""3+ years' experience with SQL, database engineering, scripting"", 'Ability to work well in fast-paced IT environment', 'Manage the design and implementation of Business Intelligence (BI)', 'Provide expertise and knowledge of SQL database design ']",Entry level,Full-time,Information Technology,Construction,2021-02-10 11:25:00
Data Engineer,Blueprint Technologies,"Bellevue, WA",11 hours ago,Be among the first 25 applicants,"['', 'FLSA - Job Classification:', 'Preferred Qualifications:', 'Ability to architect and build out integration pipelines', 'At least 5-years of experience with SQL Development ', 'Create and review design specs for any given application ', 'Excellent design, coding, testing, problem solving, and debugging skills', 'Data Engineer ', 'At least 7-years of experience as a data engineer', 'Ability to develop simple, elegant solutions to complex problems', 'Well versed with software development and deployment best practices', ' Experience with a quick service restaurant business a plus Experience in the vendor selection process Experience with sales reporting ', 'Qualifications:', 'Excellent verbal and written communication skills, team player', 'Participate in code reviews and ensure high quality before any change is released into production ', 'Experience with a quick service restaurant business a plus', 'Why Blueprint?', ' At least 7-years of experience as a data engineer At least 5-years of experience with SQL Development  Experience building out large-scale integration landscapes for an Enterprise. Including strategy and execution on Enterprise level APIs for near real-time communication between software systems Excellent design, coding, testing, problem solving, and debugging skills Ability to architect and build out integration pipelines Well versed with software development and deployment best practices Proven experience in accessing, communicating and implementing enterprise strategy Ability to quickly learn new technologies, application domains, implementing new knowledge and adapt to changes Ability to look at solutions in unconventional ways and see opportunities to innovate Excellent critical thinking and problem-solving skills. Ability to develop simple, elegant solutions to complex problems Excellent verbal and written communication skills, team player ', 'What will I be doing?', 'Ability to look at solutions in unconventional ways and see opportunities to innovate', 'Who is Blueprint?', 'Proven experience in accessing, communicating and implementing enterprise strategy', 'Ability to quickly learn new technologies, application domains, implementing new knowledge and adapt to changes', 'Experience building out large-scale integration landscapes for an Enterprise. Including strategy and execution on Enterprise level APIs for near real-time communication between software systems', 'Responsibilities', 'Experience in the vendor selection process', 'Experience with sales reporting', 'Excellent critical thinking and problem-solving skills.', 'Manage and deliver high-quality, end-to-end technology and service engineering solutions, tools, applications, metrics, and reporting ', 'What does Blueprint do?', ' Manage and deliver high-quality, end-to-end technology and service engineering solutions, tools, applications, metrics, and reporting  Create and review design specs for any given application  Derive technical requirements from functional specifications  Participate in code reviews and ensure high quality before any change is released into production  Audit and improve/update current service and support offerings, especially with our ETL services, data streams, and visualizations ', 'Location:', 'Audit and improve/update current service and support offerings, especially with our ETL services, data streams, and visualizations', 'Derive technical requirements from functional specifications ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-02-10 11:25:00
Data Engineer,HealthVerity,"Philadelphia, PA",22 hours ago,Be among the first 25 applicants,"['', ' Required Skills And Experience ', ' 3+ years of experience with PySpark and Spark-SQL (writing, testing, debugging spark routines)', ' Leverage distributed computing and serverless architecture such as AWS EMR & AWS Lambda, to develop pipelines for transforming data', "" Work with the team to load data into HealthVerity's data warehouse"", ' Comfortable using AWS CLI and boto3', ' Innovating our proprietary de-identification and data science algorithms', ' Data driven, testing and measuring as much as you can', ' Empowering clients with highly rewarding data discovery and licensing tools Ingesting and managing billions of healthcare records from a wide variety of partners Standardizing on common data models across data types Orchestrating an industry-leading HIPAA privacy layer Innovating our proprietary de-identification and data science algorithms Building a culture that supports rapid iteration and new possibilities', ' About You ', ' Comfortable using *nix command line (shell scripting, AWK, SED)', ' Experience with MySQL and Postgres', ' Help establish procedures and best practices for transforming and storing data', ' Experience with Apache Airflow', ' Troubleshoot and resolve issues relating to data integrity', ' Lead requirements gathering around data pipeline automation improvements', ' Work with some of the most exciting open-source tools like Spark, Hadoop, Docker, Airflow, Zeppelin', ' Solve complex problems related to the real-time discovery of large data', ' Eager to both review peer code and have your code reviewed', ' Confident in SQL, you know it, write smart queries, it’s no big deal', ' Experience with healthcare data', ' Experienced in writing scalable applications on distributed architectures', ' Empowering clients with highly rewarding data discovery and licensing tools', ' Standardizing on common data models across data types', ' Research and implement new technologies with a team of developers to execute strategies and implement solutions', "" Work with the team to load data into HealthVerity's data warehouse Troubleshoot and resolve issues relating to data integrity Help establish procedures and best practices for transforming and storing data Lead requirements gathering around data pipeline automation improvements Work with some of the most exciting open-source tools like Spark, Hadoop, Docker, Airflow, Zeppelin Leverage distributed computing and serverless architecture such as AWS EMR & AWS Lambda, to develop pipelines for transforming data Marvel at the speed with which your creation makes it into production Research and implement new technologies with a team of developers to execute strategies and implement solutions Produce peer reviewed quality software Solve complex problems related to the real-time discovery of large data"", ' Experienced in writing scalable applications on distributed architectures Data driven, testing and measuring as much as you can Eager to both review peer code and have your code reviewed Comfortable on the command line and consider it an essential tool Confident in SQL, you know it, write smart queries, it’s no big deal', ' 1+ years of experience with AWS EMR, AWS S3 service.', ' Experience with Apache Zeppelin', ' 3+ years of experience with Python', ' Ingesting and managing billions of healthcare records from a wide variety of partners', ' Building a culture that supports rapid iteration and new possibilities', ' Marvel at the speed with which your creation makes it into production', ' 3+ years of work experience 3+ years of experience with Python 3+ years of experience with PySpark and Spark-SQL (writing, testing, debugging spark routines) 1+ years of experience with AWS EMR, AWS S3 service. Comfortable using AWS CLI and boto3 Comfortable using *nix command line (shell scripting, AWK, SED) Experience with MySQL and Postgres', ' Orchestrating an industry-leading HIPAA privacy layer', ' Produce peer reviewed quality software', ' 3+ years of work experience', ' Comfortable on the command line and consider it an essential tool', ' Experience with Apache Airflow Experience with Apache Zeppelin Experience with healthcare data']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-02-10 11:25:00
Data Engineer,Slickdeals,"Los Angeles, CA",14 hours ago,Be among the first 25 applicants,"['', 'Developing robust, low latency and fault tolerant pipelines to support business critical systems', 'To Be Successful You Will Be', 'Able to work in a fast-paced dynamic startup like environment', 'Excellent at multitasking who can execute multiple requests and reports under tight timelines', 'Education', 'Experience working with and designing complex data schemas', 'THE CANDIDATE:', 'Strong skills Experience with Apache Big Data Frameworks (Hadoop/EMR/Databricks, Spark, Hive)', 'Experience with Kafka and event driven architectures', 'Detail-oriented tactician who strives for perfection', 'Understand the necessity of data quality and requirement for confidence of accuracy of any reports', 'Working with cloud technologies to build and deploy your applications', 'Strong problem-solving skills', 'Experience in working with large size data sets (Billions of rows/Petabytes of data)', 'Planning, conducting and directing the analysis of complex business problems and projects', 'Design data schema, perform data transformations, enrichments, and manipulations with efficiency and reusability in mind', 'As a Data Engineer, Your Day-to-day Tasks Will Include', 'The Role', 'Help translate business requirements into specification documents to track and perform analysis of new and existing site features', 'Environment', 'Strong experience with Spark performance optimization and troubleshooting', 'Excellent reading comprehension and attention to detail.', ' Helping us leverage large-scale data stores and data infrastructure by building out data pipelines, streams, and utilities in Spark and other technologies for feedback to our business systems, partners, or users Developing robust, low latency and fault tolerant pipelines to support business critical systems Aggregating key metrics for business partners to inform key decisions Working with cloud technologies to build and deploy your applications ', ' Understand data structures and algorithms. Understanding of basic statistics (confidence intervals, statistical significance, etc) Experience in working with large size data sets (Billions of rows/Petabytes of data) Experience in working with various data sources (ODBC, flat files, etc) Experience working with and designing complex data schemas Strong skills in SQL, Java and/or Python Experience with SQL query performance optimization Strong skills Experience with Apache Big Data Frameworks (Hadoop/EMR/Databricks, Spark, Hive) Strong experience with Spark performance optimization and troubleshooting Experience with Kafka and event driven architectures Familiarity with workflow scheduling/orchestration tools (Airflow, Jenkins) Experience with AWS Experience with Tableau and or other Self Service Analytical tools. Implemented Redshift, Snowflake, Azure Data Warehouse, ADLS, S3, Kafka, Presto, EMR, Databricks, or Data Lake Architecture in one or more public clouds in a Production Large Scale environment. ', 'Experience in working with various data sources (ODBC, flat files, etc)', ' Understand data structures and algorithms. Understanding of basic statistics (confidence intervals, statistical significance, etc) Experience in working with large size data sets (Billions of rows/Petabytes of data) Experience in working with various data sources (ODBC, flat files, etc) Experience working with and designing complex data schemas Strong skills in SQL, Java and/or Python Experience with SQL query performance optimization Strong skills Experience with Apache Big Data Frameworks (Hadoop/EMR/Databricks, Spark, Hive) Strong experience with Spark performance optimization and troubleshooting Experience with Kafka and event driven architectures Familiarity with workflow scheduling/orchestration tools (Airflow, Jenkins) Experience with AWS Experience with Tableau and or other Self Service Analytical tools. Implemented Redshift, Snowflake, Azure Data Warehouse, ADLS, S3, Kafka, Presto, EMR, Databricks, or Data Lake Architecture in one or more public clouds in a Production Large Scale environment. To Be Successful You Will Be Highly motivated with a great attitude and desire to dive into raw data to understand trends in behavior to find insights Excellent at multitasking who can execute multiple requests and reports under tight timelines Inquisitive, self-starter, able to work autonomously Able to work in a fast-paced dynamic startup like environment Detail-oriented tactician who strives for perfection Strong verbal and written communication (and listening) skills Excellent reading comprehension and attention to detail. Strong problem-solving skills Strong documentation skills as you code (Jira, Confluence) As a Data Engineer, Your Day-to-day Tasks Will Include Helping us leverage large-scale data stores and data infrastructure by building out data pipelines, streams, and utilities in Spark and other technologies for feedback to our business systems, partners, or users Developing robust, low latency and fault tolerant pipelines to support business critical systems Aggregating key metrics for business partners to inform key decisions Working with cloud technologies to build and deploy your applications EnvironmentCan work effectively on a small and nimble team, no trouble context-switchingEducationB.S./M.S. in Computer Science or Computer Engineering or 3+ years of equivalent experience', 'Experience with Tableau and or other Self Service Analytical tools.', 'Experience with AWS', 'Highly motivated with a great attitude and desire to dive into raw data to understand trends in behavior to find insights', 'Helping us leverage large-scale data stores and data infrastructure by building out data pipelines, streams, and utilities in Spark and other technologies for feedback to our business systems, partners, or users', ' Highly motivated with a great attitude and desire to dive into raw data to understand trends in behavior to find insights Excellent at multitasking who can execute multiple requests and reports under tight timelines Inquisitive, self-starter, able to work autonomously Able to work in a fast-paced dynamic startup like environment Detail-oriented tactician who strives for perfection Strong verbal and written communication (and listening) skills Excellent reading comprehension and attention to detail. Strong problem-solving skills Strong documentation skills as you code (Jira, Confluence) ', 'Understand data structures and algorithms. Understanding of basic statistics (confidence intervals, statistical significance, etc)', 'Work directly with the business users to understand the reporting needs and lead business users to practical solutions', 'Develop/monitor/maintain new reports, dashboards, visualizations, procedures, data structures and databases', 'Design data pipelines and maintain data pipelines in cloud or on-premise environments', 'Strong documentation skills as you code (Jira, Confluence)', 'Familiarity with workflow scheduling/orchestration tools (Airflow, Jenkins)', 'Strong skills in SQL, Java and/or Python', ' Work directly with the business users to understand the reporting needs and lead business users to practical solutions Help translate business requirements into specification documents to track and perform analysis of new and existing site features Understand the necessity of data quality and requirement for confidence of accuracy of any reports Develop/monitor/maintain new reports, dashboards, visualizations, procedures, data structures and databases Design data pipelines and maintain data pipelines in cloud or on-premise environments Design data schema, perform data transformations, enrichments, and manipulations with efficiency and reusability in mind Planning, conducting and directing the analysis of complex business problems and projects ', 'Implemented Redshift, Snowflake, Azure Data Warehouse, ADLS, S3, Kafka, Presto, EMR, Databricks, or Data Lake Architecture in one or more public clouds in a Production Large Scale environment.', 'Strong verbal and written communication (and listening) skills', 'Inquisitive, self-starter, able to work autonomously', 'The Purpose', 'Aggregating key metrics for business partners to inform key decisions', 'Experience with SQL query performance optimization']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-02-10 11:25:00
Data/Analytics Engineer,Addepar,"Portland, OR",23 hours ago,Be among the first 25 applicants,"['', 'Exposure to data streaming tools and technologies like kafka', 'Administer and maintain data pipeline tools and data warehouse', 'Designing dimensional models and be able to cleanse and transform raw data into structured format', 'Experience working in cloud data environments(Preferred: AWS OR GCP, Azure, etc)', 'Experience working with and administering one or more columnar storage systems (Preferred: Snowflake OR Redshift, Bigquery, etc)', 'Stellar communications skills for requirements gathering', 'Build and maintain scalable ELT/ETL processes using workflow automation tools ', ' Build realtime and batch data integrations to integrate data from disparate source systems into the DW Build and maintain scalable ELT/ETL processes using workflow automation tools  Designing dimensional models and be able to cleanse and transform raw data into structured format Administer and maintain data pipeline tools and data warehouse Provide production support for data integration and transformation pipelines Partner with analytics engineer and data analysts to build tooling to be able to effectively move data in and out of the data warehouse/data pond Champion the strategy to build data security framework to store, move and access data Establish best data practices and formalizing data governance framework  Scale and tune data pipelines, databases and SQL queries Build and maintain data reconciliation processes ', 'You have built Data Lakes in S3 or Hadoop', 'Experience tuning SQL queries and data pipelines', 'Establish best data practices and formalizing data governance framework ', 'Knowledge and skills with Looker will be important, not critical ', ' You have built Data Lakes in S3 or Hadoop Experience working with CRM systems like Salesforce Exposure to BI/reporting tools ( Preferred:Looker OR Tableau, Sisense/Periscope, Cognos, etc) Building reports and dashboards using BI tools Basic knowledge of statistics  Experience working with with SaaS or Fintech companies is a plus  ', 'Exposure to BI/reporting tools ( Preferred:Looker OR Tableau, Sisense/Periscope, Cognos, etc)', 'Champion the strategy to build data security framework to store, move and access data', 'Build realtime and batch data integrations to integrate data from disparate source systems into the DW', 'Must have completed at least one full cycle of building a data lake and/or a data warehouse', 'Partner with analytics engineer and data analysts to build tooling to be able to effectively move data in and out of the data warehouse/data pond', '5+ years of experience as a data engineer and/or software engineer in a SaaS or financial services business', 'Requirements', 'Building reports and dashboards using BI tools', ' 5+ years of experience as a data engineer and/or software engineer in a SaaS or financial services business 2+ years of experience working with workflow orchestration tools( Preferred: Airflow, Prefect OR Luigi, autosys, etc) 4+ years of experience building ELT/ETL processes using one or more tools(E.g. Python, Informatica, SSIS, Pentaho, etc) Experience working with and administering one or more columnar storage systems (Preferred: Snowflake OR Redshift, Bigquery, etc) Knowledge and skills with Looker will be important, not critical  Must have completed at least one full cycle of building a data lake and/or a data warehouse Exposure to data streaming tools and technologies like kafka Experience working in cloud data environments(Preferred: AWS OR GCP, Azure, etc) Strong software development skills in Python and SQL Experience tuning SQL queries and data pipelines Ability to multi-task and change priorities Stellar communications skills for requirements gathering ', 'Responsibilities', 'Experience working with CRM systems like Salesforce', '4+ years of experience building ELT/ETL processes using one or more tools(E.g. Python, Informatica, SSIS, Pentaho, etc)', 'Provide production support for data integration and transformation pipelines', 'Strong software development skills in Python and SQL', 'Build and maintain data reconciliation processes', 'Ability to multi-task and change priorities', 'Experience working with with SaaS or Fintech companies is a plus ', 'Scale and tune data pipelines, databases and SQL queries', 'Basic knowledge of statistics ', 'Nice To Have', '2+ years of experience working with workflow orchestration tools( Preferred: Airflow, Prefect OR Luigi, autosys, etc)']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-02-10 11:25:00
Data Engineer,Infoway software,Raleigh-Durham-Chapel Hill Area,14 hours ago,Be among the first 25 applicants,"['', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Leverage native Azure tools such as Synapse, Cosmos DB, and Azure Data Factory to build ETL jobs from SQL server in order to automate data flows', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0A Bachelor’s degree in Computer Science or relevant experience', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa03-5 years’ experience working with Microsoft tools such as Windows servers, SQL server, .NET framework and .NET', 'Role :\xa0Senior Data Engineer', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Design and build data extraction solutions, transformation, and loading processes by creating data pipelines', 'Must have skill:', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa03-5 years’ experience working with healthcare concepts and coding systems like ICD-10, RxNorm, NDCs, HCPCs, LOINC, CPT, SnoMed, etc.', 'Client – Oncology Analytics', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa05-7 years’ experience in using SQL and/or NOSQL databases in a healthcare enterprise environment', 'Location:\xa0REMOTE', 'Must have skill:\xa0ETL SSIS, SSAS, Python, Azure Data Factory, ADLS, EDI, Databricks, Powershell, Synapse/Snowflake', 'Senior Data Engineer', 'Experience in tools like Azure Data Factory, Databricks, Python, Powershell, Synapse/Snowflake/EDW solutions. Client is into a complete Microsoft shop working within Azure and they need someone that has a lot of familiarly with Azure tooling. They aren’t looking for someone with just SSIS experience and will need someone that is used to working with lots of different data files in different formats whether they are consumed raw or need some pre-processing.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0You will confer with clinical staff in order to understand how EMR and claims data could be better investigated, treated, and mapped for consumption', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa05-7 years’ experience with big data platforms like Azura Data Lakes, Data Lakes Analytics, Azure Machine Learning, Snowflake, CosmosDB, Synapse, Databricks, Spark/PySpark, Kafka, Hadoop, and Cloudera', 'Job Description:-', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Outstanding written and verbal communication skills and comfortable presenting ideas to peers and across the company', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0We need you to propose improvements to current OLTP/OMOP solutions in order to continue improving our data model', '\xa0', 'What we need:', 'REMOTE', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Develop and design solutions by studying information needs; conferring with users; studying flow, data usage, and work processes; investigating problem areas; following the software development life cycle; maintain and refactor existing schema to maximize data usability and consistency across different business functions']",Mid-Senior level,Contract,Information Technology,Staffing and Recruiting,2021-02-10 11:25:00
Data Engineer,Strategic Staffing Solutions,"Detroit, MI",16 hours ago,Be among the first 25 applicants,"['', 'Hands-on experience as a Data Engineer/ETL Developer in a data warehousing environment', 'Understanding of data integration, business intelligence and data warehousing concepts', 'Experience working with cloud infrastructure such as Microsoft Azure or AWS', 'We have the following opening.\xa0If you are interested and qualified, please submit a word resume and I will contact you.', 'Must sit in Detroit', 'Ability to connect data warehouse solutions through API or similar connectivity ', 'Knowledge of one or more BI reporting tools such as PowerBI a plus', 'Experience with cloud-based integration platforms ', 'Long term contract', 'Data Engineer']",Associate,Contract,Information Technology,Information Technology and Services,2021-02-10 11:25:00
Data Engineer,Cedar,"New York, NY",11 hours ago,Be among the first 25 applicants,"['', 'An opportunity to work on a platform that is scaling very rapidly, engaging with millions of patients per year and growing at a rate of 360% YoY as of January 2021', 'Transparency across teams and interaction with multiple departments', 'Eligible to work in the United States', ""Bachelor's degree in Computer Science or a related field preferred"", 'What do we offer to the ideal candidate?', 'Experience with data warehouse or data lake solutions such as Snowflake, BigQuery, Redshift', 'Daily team lunch and unlimited healthy snacks at our NYC office', 'Experience working with relational databases, such as PostgreSQL, or MySQL', 'Partner with Analytics to systematize and scale high-integrity value-oriented analysis.', 'Solid understanding of data warehouse', 'Experience with MapReduce, Hadoop, Spark is preferred', 'Ability to thrive in an entrepreneurial environment, comfortable with ambiguity', ""Encompass Cedar's core values: mission driven, no mediocrity, use good judgement, positivity"", 'Collaborate with data scientists, analysts, product managers and business development teams to understand data needs and transform these into usable data products.', ' An opportunity to work on a platform that is scaling very rapidly, engaging with millions of patients per year and growing at a rate of 360% YoY as of January 2021 A chance to join a high-growth company at an early stage The ability to impact the growth of our company, we value all comments and suggestions Transparency across teams and interaction with multiple departments Competitive pay, employer-paid healthcare, stock options Daily team lunch and unlimited healthy snacks at our NYC office ', ' Experience with AWS data services/tools such as Redshift, S3, Athena, Glue is preferred. Experience with data warehouse or data lake solutions such as Snowflake, BigQuery, Redshift Experience with MapReduce, Hadoop, Spark is preferred ', 'Applicants must be currently authorized to work in the United States on a full-time basis. Cedar will not hire any applicants for Data Engineer who are present in the United States on an F-1 visa', "" Cedar is committed to a flexible work environment, so this as well as many of our roles are remote friendly.Responsibilities: Build and optimize the process of analytics aggregations and experimentation data pipelines Collaborate with data scientists, analysts, product managers and business development teams to understand data needs and transform these into usable data products. Partner with Analytics to systematize and scale high-integrity value-oriented analysis. Partner with other engineering teams to guide refactors of existing data infrastructure to improve data quality and features. Identify and resolve issues that may negatively impact data product timelines or quality. Troubleshoot transformations for data consistency and integrity. Skills and Experience: 2+ years experience in designing, building and maintaining data pipelines. Experience working with relational databases, such as PostgreSQL, or MySQL Experience with Python and ORM tools such as SQL Alchemy Solid understanding of data warehouse Demonstrates a passion for breaking down and understanding complex systems and data structures A great teammate with excellent communication and listening skills. Ability to thrive in an entrepreneurial environment, comfortable with ambiguity Excited about Cedar's mission of improving the healthcare financial experience Encompass Cedar's core values: mission driven, no mediocrity, use good judgement, positivity Bachelor's degree in Computer Science or a related field preferred Eligible to work in the United States Prefered Skills And Experiences Experience with AWS data services/tools such as Redshift, S3, Athena, Glue is preferred. Experience with data warehouse or data lake solutions such as Snowflake, BigQuery, Redshift Experience with MapReduce, Hadoop, Spark is preferred What do we offer to the ideal candidate? An opportunity to work on a platform that is scaling very rapidly, engaging with millions of patients per year and growing at a rate of 360% YoY as of January 2021 A chance to join a high-growth company at an early stage The ability to impact the growth of our company, we value all comments and suggestions Transparency across teams and interaction with multiple departments Competitive pay, employer-paid healthcare, stock options Daily team lunch and unlimited healthy snacks at our NYC office Applicants must be currently authorized to work in the United States on a full-time basis. Cedar will not hire any applicants for Data Engineer who are present in the United States on an F-1 visa"", "" 2+ years experience in designing, building and maintaining data pipelines. Experience working with relational databases, such as PostgreSQL, or MySQL Experience with Python and ORM tools such as SQL Alchemy Solid understanding of data warehouse Demonstrates a passion for breaking down and understanding complex systems and data structures A great teammate with excellent communication and listening skills. Ability to thrive in an entrepreneurial environment, comfortable with ambiguity Excited about Cedar's mission of improving the healthcare financial experience Encompass Cedar's core values: mission driven, no mediocrity, use good judgement, positivity Bachelor's degree in Computer Science or a related field preferred Eligible to work in the United States "", 'A chance to join a high-growth company at an early stage', ""Excited about Cedar's mission of improving the healthcare financial experience"", 'Competitive pay, employer-paid healthcare, stock options', '2+ years experience in designing, building and maintaining data pipelines.', 'A great teammate with excellent communication and listening skills.', 'Experience with AWS data services/tools such as Redshift, S3, Athena, Glue is preferred.', 'Responsibilities:', 'Data Engineer', 'The ability to impact the growth of our company, we value all comments and suggestions', 'Demonstrates a passion for breaking down and understanding complex systems and data structures', 'Prefered Skills And Experiences', 'Build and optimize the process of analytics aggregations and experimentation data pipelines', 'Identify and resolve issues that may negatively impact data product timelines or quality. Troubleshoot transformations for data consistency and integrity.', 'Partner with other engineering teams to guide refactors of existing data infrastructure to improve data quality and features.', 'Experience with Python and ORM tools such as SQL Alchemy', 'Skills and Experience:', ' Build and optimize the process of analytics aggregations and experimentation data pipelines Collaborate with data scientists, analysts, product managers and business development teams to understand data needs and transform these into usable data products. Partner with Analytics to systematize and scale high-integrity value-oriented analysis. Partner with other engineering teams to guide refactors of existing data infrastructure to improve data quality and features. Identify and resolve issues that may negatively impact data product timelines or quality. Troubleshoot transformations for data consistency and integrity. ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-02-10 11:25:00
 Data Engineer,Crystal Equation Corporation,"Menlo Park, CA",16 hours ago,Be among the first 25 applicants,"['', 'Broad range of partners equates to a broad range of projects and deliverables: ML Models, datasets, measurements, services, tools and process.', 'For Coding, should have familiarity with major concepts and a comfort level with at least one language of the candidate’s choice', '3+ years experience with Data Modeling.', 'Educate your partners: Use your data and analytics experience to ‘see what’s missing’, identifying and addressing gaps in their existing logging and processes.', 'Partner with leadership, engineers, program managers and data scientists to understand data needs.Design, build and launch extremely efficient and reliable data pipelines to move data across a number of platforms including Data Warehouse, online caches and real-time systems.Communicate, at scale, through multiple mediums: Presentations, dashboards, company-wide datasets, bots and more.Educate your partners: Use your data and analytics experience to ‘see what’s missing’, identifying and addressing gaps in their existing logging and processes.Broad range of partners equates to a broad range of projects and deliverables: ML Models, datasets, measurements, services, tools and process.Leverage data and business principles to solve large scale web, mobile and data infrastructure problems.Build data expertise and own data quality for your areas.', 'Experience with more than one coding language.Designing and implementing real-time pipelines.Experience with data quality and validation.Experience with SQL performance tuning and e2e process optimization.Experience with anomaly/outlier detection.Experience with notebook-based Data Science workflow.Experience with Airflow.Experience querying massive datasets using Spark, Presto, Hive, Impala, etc.', 'Experience with more than one coding language.', 'Design, build and launch extremely efficient and reliable data pipelines to move data across a number of platforms including Data Warehouse, online caches and real-time systems.', 'Experience with Airflow.', 'As a member of Infrastructure Strategy Data Engineering, you will belong to a centralized Data Science/Data Engineering team who partners closely with teams in the Infrastructure organization. Through the consulting-nature of our team, you will contribute to a variety of projects and technologies, depending on partner needs. Projects include analytics, ML modeling, tooling, services, and more.', 'Problem Solving is required', 'Experience with data quality and validation.', '3+ years of experience with workflow management engines (i.e. Airflow, Luigi, Prefect, Dagster, digdag.io, Google Cloud Composer, AWS Step Functions, Azure Data Factory, UC4, Control-M).', 'No H1B/No C2C/No OPT', 'Data Modeling skills are a MUST and considered a portion of the SQL signals we look for', 'San Francisco Local Candidates ONLY (Non-Local will be rejected)', 'Partnership is less of a thing here as most of the work will be directed by FTE Data Engineers, however this is a PLUS if the candidate exhibits good soft skills', 'Build data expertise and own data quality for your areas.', '\ufeff', 'Leverage data and business principles to solve large scale web, mobile and data infrastructure problems.', 'Experience analyzing data to discover opportunities and address gaps.', '5+ years of Python development experience.', 'Minimum Qualifications', '1-Year contract position', 'Experience with anomaly/outlier detection.', 'Experience with notebook-based Data Science workflow.', 'Strong SQL skills are a MUST', 'Experience working with cloud or on-prem Big Data/MPP analytics platform(i.e. Netezza, Teradata, AWS Redshift, Google BigQuery, Azure Data Warehouse, or similar).', 'Data Engineer, Infrastructure Strategy Responsibilities', 'Experience querying massive datasets using Spark, Presto, Hive, Impala, etc.', 'Communicate, at scale, through multiple mediums: Presentations, dashboards, company-wide datasets, bots and more.', 'Job Title: Data Analytics & Engineering - Data Engineer IV (28072)', 'Experience with SQL performance tuning and e2e process optimization.', '\xa0', 'Preferred Qualifications', '*Position requires export compliance*', '8+ Years of experiencesStrong SQL skills are a MUSTData Modeling skills are a MUST and considered a portion of the SQL signals we look forFor Coding, should have familiarity with major concepts and a comfort level with at least one language of the candidate’s choiceProblem Solving is requiredPartnership is less of a thing here as most of the work will be directed by FTE Data Engineers, however this is a PLUS if the candidate exhibits good soft skills5+ years of Python development experience.5+ years of SQL experience.3+ years of experience with workflow management engines (i.e. Airflow, Luigi, Prefect, Dagster, digdag.io, Google Cloud Composer, AWS Step Functions, Azure Data Factory, UC4, Control-M).3+ years experience with Data Modeling.Experience analyzing data to discover opportunities and address gaps.5+ years experience in custom ETL design, implementation and maintenance.Experience working with cloud or on-prem Big Data/MPP analytics platform(i.e. Netezza, Teradata, AWS Redshift, Google BigQuery, Azure Data Warehouse, or similar).', '8+ Years of experiences', 'Partner with leadership, engineers, program managers and data scientists to understand data needs.', '5+ years of SQL experience.', 'We are looking for a Data Engineer to not only build data pipelines but also extend the next generation of our data tools. As a Data Engineer, you will develop a clear sense of connection with our organization and leadership - as Data Engineering is the eyes through which they see the product.', 'Designing and implementing real-time pipelines.', '5+ years experience in custom ETL design, implementation and maintenance.']",Associate,Contract,Information Technology,Information Technology and Services,2021-02-10 11:25:00
Data Engineer (contract),Capgemini,"Philadelphia, PA",28 minutes ago,Be among the first 25 applicants,"['', 'Data Bricks - Disparate Data source', 'Data Engineering background 8-10 years', 'Upload the data AWS Graph DB Neptune', 'Python Development', 'Build the Delta lakes', 'Data Engineering background 8-10 yearsPython DevelopmentData Bricks - Disparate Data sourceBuild the Delta lakesUpload the data AWS Graph DB NeptuneGraphQL knowledge', 'GraphQL knowledge', 'Requirement']",Associate,Contract,Information Technology,Information Technology and Services,2021-02-10 11:25:00
Data Engineer,SPINS,"Chicago, IL",1 hour ago,Be among the first 25 applicants,"['', 'When we began working from home in 2020, we were committed to keeping the healthy and active SPINS culture for our employees. Keeping the culture is high priority to us as we have continued to hire and onboard new employees even while we worked remotely. We listen to employee feedback as we plan when and how we will return to the office in the future. While we can’t have some of our favorite in-office perks (plenty of snacks, onsite gym, and bike storage), we’ve continued to find ways to stay true to SPINS:', 'While your talent and experience matter most, we have to mention these qualifications matter to us', 'We’re even more impressed if you bring along your', '\xa0We’re even more impressed if you bring along your:', 'Exposure to Google Cloud Platform (or AWS or Azure)', 'The SPINS Way', 'For 20+ years, our mission has been to increase the presence and accessibility of natural and organic products to encourage healthier and more vibrant living. By leveraging SPINS’ industry-leading proprietary data and analytics, our technology enables deeper, more engaged relationships between Retailers, Brands and Consumers through our platform, web, and mobile products. At the core of our work lies a passion to create a culture of sustainable health & wellness.\xa0', 'SPINS builds high-octane software products which help companies reach their customers. We correlate data with precision to deliver insights and recommended actions based on those insights that no one else can provide. We are seeking a Data Engineer to join our team.', 'Virtual yoga, HIIT, and Kinstretch classes each week', 'What You Bring\ufeff', 'An active participant.\xa0You are always the first to volunteer for any task because you are driven by the challenge of completing difficult and interesting projects.', 'Creative thinking with a passion for achieving high quality results.', 'Determined', 'Exposure to XP or other Agile engineering teams', 'Stay connected with other SPINS employees for a weekly PELOTON ride together', 'Ability to understand both aggregated and disaggregated data with statistical measures', '5+ years professional experience in Quality Assurance.', 'While your talent and experience matter most, we have to mention these qualifications matter to us:', 'Who we are.', 'The SPINS Product and Technology teams include product development for internal and client-facing applications, data operations, data sourcing, master data, data science, custom reporting, design, engineering, data content quality and compliance. The position of Data Engineer reports to the Manager of Data & Software Quality.', 'Experience in organizations where data & insights are what we sell to clients', 'Passionate', 'Experience with the integration of AI and ML models into production operations', 'Exposure to XP or other Agile engineering teamsExperience with the integration of AI and ML models into production operationsExposure to Google Cloud Platform (or AWS or Azure)Experience with automated test frameworksExperience in CPG environments', 'What you will do.', 'Direct\xa0– We communicate with clarity, honesty and respect in all situations and embrace opportunities to provide solution-oriented feedback.Determined\xa0– We are committed to overcoming all obstacles to achieve results. We adapt to change, seek opportunities to learn and rapidly translate that learning into action.Passionate\xa0– We go above and beyond to help our partners achieve their goals. We challenge assumptions and are comfortable forging new paths.Collaborative\xa0– We leave our egos at the door, believing that working together we will produce an outcome that’s greater than each individual contribution.', 'Direct', 'Virtual yoga, HIIT, and Kinstretch classes each weekEach employee is allotted 8 hours to use to volunteer with an organization of their choice.Stay connected with other SPINS employees for a weekly PELOTON ride together', 'Passionate\xa0– We go above and beyond to help our partners achieve their goals. We challenge assumptions and are comfortable forging new paths.', 'Experience with automated test frameworks', 'Collaboration with team members in a dynamic environment', 'Collaborative\xa0– We leave our egos at the door, believing that working together we will produce an outcome that’s greater than each individual contribution.', 'Each employee is allotted 8 hours to use to volunteer with an organization of their choice.', 'Collaborative', 'Expertise in software engineering.\xa0You might have a preference for JAVA or C#, but there is no language you can’t learn.', 'Direct\xa0– We communicate with clarity, honesty and respect in all situations and embrace opportunities to provide solution-oriented feedback.', 'Data Engineer', 'Determined\xa0– We are committed to overcoming all obstacles to achieve results. We adapt to change, seek opportunities to learn and rapidly translate that learning into action.', 'Experience in CPG environments', '\xa0', 'Detail oriented, with a focus on continuous improvement while finding and fixing problems instead of avoiding them.An active participant.\xa0You are always the first to volunteer for any task because you are driven by the challenge of completing difficult and interesting projects.Prior experience in building new computations as well as deciphering existing onesCapable of thriving in complicated environments.\xa0You are undaunted by challenges and are able to focus where and when needed.Experience in organizations where data & insights are what we sell to clients5+ years professional experience in Quality Assurance.Ability to understand both aggregated and disaggregated data with statistical measuresExpertise in software engineering.\xa0You might have a preference for JAVA or C#, but there is no language you can’t learn.Collaboration with team members in a dynamic environmentCreative thinking with a passion for achieving high quality results.Effective verbal and written communication with co-workers', 'What SPINS Offers', 'Detail oriented, with a focus on continuous improvement while finding and fixing problems instead of avoiding them.', 'Prior experience in building new computations as well as deciphering existing ones', 'Effective verbal and written communication with co-workers', 'As a Data Engineer\xa0at SPINS, you will work to create test scripts and applications for our various data and software products from business requirements, automating as much as possible. Data is the core of our business, so you will be digging deep in the data to make sure it is accurate.\xa0You will document and evaluate test results and defects, and work with the engineering team to troubleshoot issues. Collaboration with teammates and across other departments is essential for success in this role.', 'Capable of thriving in complicated environments.\xa0You are undaunted by challenges and are able to focus where and when needed.']",Mid-Senior level,Full-time,Information Technology,Market Research,2021-02-10 11:25:00
Data Engineer,Virtusa,San Francisco Bay Area,17 hours ago,Over 200 applicants,"['', 'Build scalable data pipelines to extract, transform and load data into different systems.', 'Role: Data Engineer', 'Duration: 12 Months+', 'AWS Lambda, AWS S3, AWS Redshift ', 'SOAP/REST API ', 'GitHub; Jenkins', 'Experience with modern SaaS applications like Salesforce, Adobe, Marketo, Workday, ServiceNow.', 'Location: Pleasanton CA (Remote)', 'Build scalable data ingestion framework for new and existing applications.Build scalable data pipelines to extract, transform and load data into different systems.SQL Expert (with a good understanding of performance tuning)Apache Spark, Apache Airflow, Python AWS Lambda, AWS S3, AWS Redshift GitHub; JenkinsSOAP/REST API Any ETL Tool (Snaplogic Preferred)Strong knowledge of Data Warehouse and Data Modelling (Dimensional Data Model)Working Experience in Agile (Scrum, Pair Programming)Working Experience with collaboration tools (Jira, Confluence, ServiceNow)Experience with modern SaaS applications like Salesforce, Adobe, Marketo, Workday, ServiceNow.Self-Motivated, Accountable to Drive the work.', 'Self-Motivated, Accountable to Drive the work.', 'Location: This position allows work from home until safe to return to work.', 'SQL Expert (with a good understanding of performance tuning)', 'Any ETL Tool (Snaplogic Preferred)', 'Apache Spark, Apache Airflow, Python ', 'Working Experience with collaboration tools (Jira, Confluence, ServiceNow)', 'Note: Prefer candidates that can work on W2/ FTE. Third-party candidates cannot be considered.', 'Data Engineer', 'Duration: 6-12 months', 'Skills: Data Engineering, Programming languages, SQL, Spark SQL, Hive, AWS, Analytics', 'Working Experience in Agile (Scrum, Pair Programming)', 'Tier: 3/4', 'Build scalable data ingestion framework for new and existing applications.', 'SQL Expert', 'Strong knowledge of Data Warehouse and Data Modelling (Dimensional Data Model)']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-02-10 11:25:00
Data Engineer,"Recru, LLC",Greater Houston,20 hours ago,Be among the first 25 applicants,"['', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Design and implement an Enterprise Data Warehouse', 'Knowledgeable of Machine Learning', '**MUST\xa0BE ELIGIBLE TO WORK ON A W-2. UNABLE TO PROVIDE SPONSORSHIP. NO C2C.**', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Assists data analysts and data scientists with query optimization, performance tuning, and data processing', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Document and maintain source-to-target mappings and data lineage', 'Data modelling', '\ufeffMust Have Skills', 'Knowledgeable of Data Management such as MDM (Master Data Management), Data Catalog, and Data Governance', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Identify opportunities for data improvements and presents recommendations to management', '6-Month CONTRACT NO C2C', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Productionize mathematical models and machine learning models', 'Spark (preferred not required) ', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Implement data flows connecting operational systems, BI systems, and the big data platform', 'SQL', 'Knowledgeable of Statistics', 'Business Analysis and Requirements Gathering', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Architect end to end data solutions including data collection and storage, data modeling, and data consumption', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Develop data-intensive applications with API’s and streaming data pipelines', 'ETL', 'Python', 'We are currently seeking an experienced Data Engineer to join the Big Data and Advanced Analytics department.\xa0As part of the Data Engineering team, the Data Engineer will work closely with the Data Science team and Business functions to solve real-world oil and gas midstream problems using machine learning, data science and artificial intelligence.\xa0This individual will provide technical and thought leadership to the team to build out a data engineering practice within the organization.\xa0', 'Responsibilities:', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Automate manual data flows for repeated use and scalability', 'Software Development practices such as Testing, CI/CD, Version Control', 'PythonSpark (preferred not required) SQLData modellingETLSoftware Development practices such as Testing, CI/CD, Version ControlBusiness Analysis and Requirements GatheringKnowledgeable of Machine LearningKnowledgeable of StatisticsKnowledgeable of Data Management such as MDM (Master Data Management), Data Catalog, and Data Governance', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Work independently on data projects for multiple business functions', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Prepare and transform data into a usable state for analytics']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-02-10 11:25:00
Data Engineer / Data Scientist,Advantage Tech,United States,23 hours ago,32 applicants,"['', ""Bachelor's degree in Computer Science or related technical field, or equivalent practical experience2+ years of Data Engineering experienceExperience with the Python language; it’s design, base libraries, built-in features, and how it compares to other languagesExposure to Python data engineering libraries, including pandas and pysparkExperience with SQL and data warehouse tools such as AWS AthenaExperience with batch processing tools such as AWS Batch or AWS Glue, or other DAG systemsReal-time processing tools such as AWS Kinesis and AWS FirehoseIntermediate data modeling and architecture design"", '**No C2C applicants at this time. This position is NOT eligible for sponsorship', 'Skills Needed:', 'Exposure to Python data engineering libraries, including pandas and pyspark', 'You will also support processes to allow automated deployment of schema changes in conjunction with DevOps deployment model.\xa0You will work to maintain a self-starter mindset in terms of data process and architecture improvements, with plenty of professional development opportunities to help take you to the next level of your career.', 'Part of your responsibilities as a Data Engineer will be provisioning advanced data transformations through batch and streaming ETL systems.\xa0You will build and scale data warehouse systems while maintaining security standards and you will analyze and develop data architecture requirements, determine the feasibility of designs, and implement software solutions best suited for the client’s processes.', 'Intermediate data modeling and architecture design', 'The ideal Data Engineer is a hybrid of DevOps engineering, Software engineering, and Database engineering, focused on building advanced data warehousing and transformation systems.', 'Experience with SQL and data warehouse tools such as AWS Athena', ""Advantage Tech\xa0is hiring for a Mid or Senior Data Engineer to join their client's team based out of Kansas City, Mo. "", ':', ""Bachelor's degree in Computer Science or related technical field, or equivalent practical experience"", '2+ years of Data Engineering experience', 'Responsibilities:', 'The data engineering team is responsible for data integrity and availability for crucial systems at the client, and serves as a primary resource for data expertise.', 'Senior ', 'Responsibilities', 'Real-time processing tools such as AWS Kinesis and AWS Firehose', 'Arizona, Colorado, Delaware, Georgia, Kansas, Florida, Maryland, Missouri, North Carolina, Oregon, Pennsylvania, Texas, Utah, and Washington.', '\xa0', 'Mid', 'Experience with the Python language; it’s design, base libraries, built-in features, and how it compares to other languages', 'Experience with batch processing tools such as AWS Batch or AWS Glue, or other DAG systems', 'This position is 100% remote open to candidates working in Arizona, Colorado, Delaware, Georgia, Kansas, Florida, Maryland, Missouri, North Carolina, Oregon, Pennsylvania, Texas, Utah, and Washington.']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-02-10 11:25:00
Data Engineer,Robert Half,"South Plainfield, NJ",9 hours ago,Be among the first 25 applicants,"['', ' Excellent verbal and written communication and skills', ' Experience working with relational databases, ETL concepts and using statistical computer languages: R, Python, SLQ, etc..', ' Develop processes and tools to monitor and analyze data accuracy.', ' Coding knowledge and experience with several languages: C, C++, Python, Java, JavaScript, HTML, etc.', ' Work closely with the IT department to provide a secure and efficient infrastructure for researchers.', ' Work with stakeholders throughout research to create solutions for analyzing, transforming, and storing drug discovery project data. Develop processes and tools to monitor and analyze data accuracy. Work closely with the IT department to provide a secure and efficient infrastructure for researchers. Provide expertise and hands-on training for data visualization tools such as Spotfire. Continually strive to improve research team performance by maintaining a strong understanding of the state-of-the-art tools and software. Performs other tasks as assigned.', ' Continually strive to improve research team performance by maintaining a strong understanding of the state-of-the-art tools and software.', ' Attention to detail, team player, and a strong commitment to excellence.', ' Provide expertise and hands-on training for data visualization tools such as Spotfire.', ' Ability to work independently and collaboratively, as required, in a fast-paced environment.', ' Hands-on experience with data visualization tools (Spotfire, Plotly, etc.)', 'Requirements', "" Master's degree in computer science or related field with three to five years of hands-on experience with database management. Experience working with relational databases, ETL concepts and using statistical computer languages: R, Python, SLQ, etc.. Coding knowledge and experience with several languages: C, C++, Python, Java, JavaScript, HTML, etc. Hands-on experience with data visualization tools (Spotfire, Plotly, etc.) Attention to detail, team player, and a strong commitment to excellence. Excellent verbal and written communication and skills Ability to work independently and collaboratively, as required, in a fast-paced environment. Analytical thinker with excellent problem-solving skills Ability to quickly adapt to changing priorities and deadlines."", ' Work with stakeholders throughout research to create solutions for analyzing, transforming, and storing drug discovery project data.', ' Performs other tasks as assigned.', ' Ability to quickly adapt to changing priorities and deadlines.', 'Description', ' Analytical thinker with excellent problem-solving skills', "" Master's degree in computer science or related field with three to five years of hands-on experience with database management.""]",Entry level,Temporary,Information Technology,Staffing and Recruiting,2021-02-10 11:25:00
Data Engineer,"Paradigm Information Services, Inc.","Beaverton, OR",13 hours ago,Be among the first 25 applicants,"['', 'Demonstrated experience with Microsoft SQL Server, including SSAS, SSRS, and SSIS', 'Research and implement tooling to support the build out of intelligence tools', 'Experience analyzing and optimizing SQL performance', ' Bachelor’s degree in Computer Science or equivalent industry experience  5+ years of experience in BI architecture design, data modeling, data warehouse design, ETL, and OLAP structures Demonstrated experience with Microsoft SQL Server, including SSAS, SSRS, and SSIS 5+ years of experience with the Microsoft BI delivery stack, including SharePoint, Power BI, Power View, Power Pivot, Power App, Flow, and Excel Experience working in clout environments likes Azure or AWS Experience with continuous integration/continuous delivery (CI/CD) demonstrated experience work in Python or other object-oriented languages  Experience in multidimensional database design, and SQL Server Tabular Models (Star Schema) Experience analyzing and optimizing SQL performance Must be driven and goal oriented  Must have the ability work in a fast-paced environment with shifting priorities  The ability to communicate with both technical and non-technical stakeholders Experience in Master Data Management preferred Experience in a manufacturing environment a plus Experience with Microsoft Dynamics AX and CRM a plus ', '5+ years of experience in BI architecture design, data modeling, data warehouse design, ETL, and OLAP structures', '5+ years of experience with the Microsoft BI delivery stack, including SharePoint, Power BI, Power View, Power Pivot, Power App, Flow, and Excel', 'Write high-quality code that runs smoothly in production and sets high standard to be met across the team ', 'Experience in Master Data Management preferred', 'Must have the ability work in a fast-paced environment with shifting priorities ', 'As a Senior Data Engineer, you will', 'Type', 'Experience in a manufacturing environment a plus', 'Must be driven and goal oriented ', 'About Us, Paradigm', 'Create a production data platform that supports the business', 'Experience with Microsoft Dynamics AX and CRM a plus', 'Experience working in clout environments likes Azure or AWS', 'Bachelor’s degree in Computer Science or equivalent industry experience ', 'Our skills and experience wish list includes ', 'Location', 'Experience with continuous integration/continuous delivery (CI/CD)', 'Set technical documentation standards such as data dictionaries, business glossaries, metric definitions, and other data resources', 'Create and monitor the metadata management repository and data lineage for source data, intermediate data and target repositories including common information models', 'Requirements', 'demonstrated experience work in Python or other object-oriented languages ', 'The ability to communicate with both technical and non-technical stakeholders', 'Benefits', 'Develop and implement scalable solutions to meet the business requirements ', 'Establish data architectural standards, a process for maintaining the standards and a process of monitoring compliance to those standards across the team', 'Experience in multidimensional database design, and SQL Server Tabular Models (Star Schema)', ' Develop and implement scalable solutions to meet the business requirements  Create a production data platform that supports the business Collaborate across business departments to develop long-term strategic goals for data warehousing  Research and implement tooling to support the build out of intelligence tools Write high-quality code that runs smoothly in production and sets high standard to be met across the team  Establish data architectural standards, a process for maintaining the standards and a process of monitoring compliance to those standards across the team Set technical documentation standards such as data dictionaries, business glossaries, metric definitions, and other data resources Devise a strategy for obtaining data from diverse source systems and moving it into the data ecosystem Create and monitor the metadata management repository and data lineage for source data, intermediate data and target repositories including common information models ', 'Collaborate across business departments to develop long-term strategic goals for data warehousing ', 'Devise a strategy for obtaining data from diverse source systems and moving it into the data ecosystem', 'Pay Rate']",Mid-Senior level,Full-time,Analyst,Electrical/Electronic Manufacturing,2021-02-10 11:25:00
Data Engineer,Cricket Health,"California, United States",20 hours ago,28 applicants,"['', 'Strong programming skills in one or more languages - e.g. Python, Java, Ruby, Scala, .Net/C#', 'Benefits:', 'Competitive salary and vacation', 'Build automated quality control (QC) tests and perform pull request reviews for data analysts and engineers\xa0', 'Stock options + extended option exercise window', 'Competitive salary and vacationStock options + extended option exercise windowGenerous health, dental, vision and parental leave policiesContributions for 401k retirement savings plansCommitment to building and maintaining an inclusive team', 'Design and build the backbone for Cricket Health’s data infrastructure - including our data lake/warehouse, data modeling layer, and data visualization capabilities\xa0Work with Engineering and Data teams to shape data infrastructure investmentsCollaborate with application developers, product managers, and business analysts to understand requirements and translate them into design specifications and codeWork closely with Cricket Health team members to develop, test, deploy, and operate high quality, scalable data pipelines connecting internal and external data sourcesBuild automated quality control (QC) tests and perform pull request reviews for data analysts and engineers\xa0Own data IT strategy recommendations, database architecture development, dev processes, data quality assurance, and system administration', 'Commitment to building and maintaining an inclusive team', 'Generous health, dental, vision and parental leave policies', 'Ideal Qualifications:', 'Work with Engineering and Data teams to shape data infrastructure investments', 'Experience working in both a start-up and “established” company environmentWorking knowledge of data compliance and regulatory pitfalls in healthcareUnderstanding of healthcare data ontologies (CPT, ICD10, NDC, etc.)', 'Experience standing up and administering data visualization tools (e.g. Looker, Mode)', 'Design and build the backbone for Cricket Health’s data infrastructure - including our data lake/warehouse, data modeling layer, and data visualization capabilities\xa0', 'Strong background/experience developing mature data infrastructure', '5+ years of hands-on experience doing data engineering work', 'Strong background/experience developing mature data infrastructure5+ years of hands-on experience doing data engineering workPrior work standing up automated data pipelines from “from scratch” (e.g. soup to nuts); Airflow experience preferredStrong programming skills in one or more languages - e.g. Python, Java, Ruby, Scala, .Net/C#Experience standing up and administering data modeling and ELT tools\xa0Experience standing up and administering data visualization tools (e.g. Looker, Mode)Experience working with AWS services such as Redshift and S3, or the equivalent products in Google Cloud / Azure', 'Prior work standing up automated data pipelines from “from scratch” (e.g. soup to nuts); Airflow experience preferred', 'Experience working with AWS services such as Redshift and S3, or the equivalent products in Google Cloud / Azure', 'Qualifications:', 'Experience standing up and administering data modeling and ELT tools\xa0', 'Cricket Health is a comprehensive kidney care provider with a personalized, evidence-based approach to managing chronic kidney disease (CKD) and end-stage renal disease (ESRD).\xa0Cricket Health delivers world-class, technology-enabled multidisciplinary care both in-person and virtually to achieve the best outcomes possible for patients and the best value for partners, keeping patients healthy and out of the hospital, accelerating access to transplant, and increasing home dialysis adoption.\xa0We are committed to aligning the success of our company with those of our partners and the patients whom we serve.\xa0Learn more at www.crickethealth.com and follow us @crickethealth.', 'Experience working in both a start-up and “established” company environment', 'In this role, you’ll be designing and implementing infrastructure and tooling for our data pipelines and machine learning software. We use Redshift and PostgreSQL, Python and R Language for our data analysis software, and AWS to manage our infrastructure.', 'Collaborate with application developers, product managers, and business analysts to understand requirements and translate them into design specifications and code', ""Cricket Health is looking for a Data Engineer to join our growing Data Analytics team. We're interested in someone who is passionate about improving healthcare, wants to “do right” by patients and providers, and is excited to join a team of brilliant analysts, engineers, and healthcare data experts.\xa0"", 'Working knowledge of data compliance and regulatory pitfalls in healthcare', 'Work closely with Cricket Health team members to develop, test, deploy, and operate high quality, scalable data pipelines connecting internal and external data sources', 'Understanding of healthcare data ontologies (CPT, ICD10, NDC, etc.)', 'Contributions for 401k retirement savings plans', 'Own data IT strategy recommendations, database architecture development, dev processes, data quality assurance, and system administration', 'Primary Responsibilities:']",Not Applicable,Full-time,Information Technology,Hospital & Health Care,2021-02-10 11:25:00
Data Engineer,Collabera Inc.,United States,18 hours ago,Be among the first 25 applicants,"['Note: Only for W2 candidates', '', 'Required', ""3+ years of experience as a Data Engineer or in a similar roleExperience with data modeling, data warehousing, and building ETL pipelinesExperience in SQLBachelor's degree in computer science, engineering, or a related technical discipline5+ years of industry experience in software development, data engineering, business intelligence, data science, computational linguistics or related field with a track record of manipulating, processing, and extracting value from large datasetsExperience using big data technologies (Hadoop, Hive, Hbase, Spark, EMR, etc.)Coding proficiency in at least one modern programming language (Python, Ruby, Java, etc.)Knowledge of data management fundamentals and data storage principlesKnowledge of distributed systems as it pertains to data storage and computingKnowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operation"", 'Experience using big data technologies (Hadoop, Hive, Hbase, Spark, EMR, etc.)', '5+ years of industry experience in software development, data engineering, business intelligence, data science, computational linguistics or related field with a track record of manipulating, processing, and extracting value from large datasets', '4155981122', '3+ years of experience as a Data Engineer or in a similar role', 'Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operation', 'Location:\xa0\xa0\xa0\xa0\xa0\xa0Remote ', 'Knowledge of data management fundamentals and data storage principles', 'Experience with data modeling, data warehousing, and building ETL pipelines', 'nehal.athani@collabera.com', 'Coding proficiency in at least one modern programming language (Python, Ruby, Java, etc.)', 'Duration:\xa0\xa0\xa0\xa0\xa0\xa09 Months (potential conversion to FTE or contract might get extended)', 'Regards,', 'Knowledge of distributed systems as it pertains to data storage and computing', 'Nehal Athani', 'Title:\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Data Engineer', ""Bachelor's degree in computer science, engineering, or a related technical discipline"", 'Experience in SQL']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-02-10 11:25:00
Sr Data Engineer,PPG Industries,"Pittsburgh, PA",15 hours ago,Be among the first 25 applicants,"['', 'Continuously explore the advances big data technologies and advocate the related innovations across the digital organization', 'We run it like we own it.', ')', 'Key Responsibilities', 'Every Single Day At PPG', 'http://corporate.ppg.com/Our-Company/PPG-Vision.aspx', 'EEO is the Law', 'We trust our people every day, in every way.', 'Improve upon the data ingestion models, ETL jobs, and alarm to maintain data integrity and data availability.', 'Develop and build AI/ML model ready data ponds in Azure Data bricks Delta lake', 'Experience in modern DevOps practices (including Git, CI/CD)', 'Leverage domain knowledge, suggest optimal path to secure the data required for AI/ML model experimentation and future ML Ops', 'Knowledge of data management fundamentals and data storage principles', 'Proficiency with complex SQL development', 'Partner with AI/ML product teams proactively identify the data needs for rapid experimentation, stable production deployment and ensure the required data availability by collaborating with Product teams.', 'Qualifications', 'EEO is the Law Supplement', 'The PPG Way', 'We make it happen.', 'Any ETL experience in design, mapping and configuration in a complex environment processing large volumes of data', 'Pay Transparency poster', 'We are “One PPG” to the world.', 'We are One PPG: ', 'PPG vision: ', '5 + Years’ hands on experience in data engineering ', 'Experience with cloud-based data services, including data pipeline orchestration tooling (i.e. Azure Data Factory).', 'Strong business acumen and adaptability to partner with the AI/ML product teams on innovative solutions to constantly changing business requirements', 'We do better today than yesterday – everyday.', 'Click here to learn more about the PPG Way', 'PPG Way: (', 'We partner with customers to create mutual value.', ' Develop and build AI/ML model ready data ponds in Azure Data bricks Delta lake Improve upon the data ingestion models, ETL jobs, and alarm to maintain data integrity and data availability. Partner with AI/ML product teams proactively identify the data needs for rapid experimentation, stable production deployment and ensure the required data availability by collaborating with Product teams. Leverage domain knowledge, suggest optimal path to secure the data required for AI/ML model experimentation and future ML Ops Write complex and efficient queries to transform conventional data sources into easily accessible models  Knowledge of data management fundamentals and data storage principles Continuously explore the advances big data technologies and advocate the related innovations across the digital organization ', 'PPG: WE PROTECT AND BEAUTIFY THE WORLD™', 'Sr', 'Data Engineer', ' 5 + Years’ hands on experience in data engineering  Experience working with Databricks or Apache Spark/PySpark Experience with cloud-based data services, including data pipeline orchestration tooling (i.e. Azure Data Factory). Proficiency with complex SQL development Experience in modern DevOps practices (including Git, CI/CD) Strong business acumen and adaptability to partner with the AI/ML product teams on innovative solutions to constantly changing business requirements Any ETL experience in design, mapping and configuration in a complex environment processing large volumes of data ', 'http://one.ppg.com/', 'Experience working with Databricks or Apache Spark/PySpark', 'Write complex and efficient queries to transform conventional data sources into easily accessible models ', 'About Us']",Not Applicable,Full-time,Information Technology,Building Materials,2021-02-10 11:25:00
Data Engineer,Arkose Labs,San Francisco Bay Area,22 hours ago,Be among the first 25 applicants,"['', 'Previous work experience in cybersecurity and/or fraud company', 'You will be a part of a diverse and high performing environment that values and recognizes your contributions and unique perspectives and experience: we value what you bring to the culture.', 'Why Arkose Labs?', 'We hire the best and value integrity, excellence, respect, inclusion, and collaboration.', 'About Arkose Labs\xa0\xa0', 'What we want from you', 'Competitive salary, equity, and a robust benefits package includes top-notch medical, dental, vision, life insurance, 401k, and we cover 95% of the cost of employee benefits and 65% of the cost of dependent care coverage! We also offer flexible PTO.\xa0', '5+ years experience SQL', '5+ years experience with NoSQL technologies (Elasticsearch, Cassandra etc.)', 'Help build software that spans from the browser all the way down to analytics data pipelines -\xa0our engineers work to support a dedicated data science', '-\xa0Security. It’s the lens through which we implement our processes, procedures, and programs', 'Work on building highly-available, zero downtime, distributed software systems to help combat fraud', '1+ years experience Spark (or equivalent)', '5+ years experience with Big Data Pipelines (e.g. Kafka, Other Data Streaming Engines like Storm, Spark, etc.)', 'Must Have', '-\xa0Customer Focus. We empathize with our customers and obsess about solving their problems', '5+ years experience with Data Modeling and Data Warehousing(Snowflake, AWS, Athena, etc.)', '1+ years experience Python', 'What you’ll be doing', '-\xa0Team Work. We demonstrate respect, trust, integrity, and communicate openly with a positive can do attitude and constructively challenge one another', 'Work on high traffic SaaS services running at a global scale, serving ~500 million requests (multi-terabytes of data) per day and increasing', 'Work on high traffic SaaS services running at a global scale, serving ~500 million requests (multi-terabytes of data) per day and increasingWork on building highly-available, zero downtime, distributed software systems to help combat fraudHelp build software that spans from the browser all the way down to analytics data pipelines -\xa0our engineers work to support a dedicated data scienceWork within an agile, cross-functional team (SWE, SRE, QE) to build out and own services and projects, from requirements + specs to release and maintenanceHelp with migrating from a monolithic system and AWS lambdas to a microservices-based architecture running on KubernetesBuild and maintain new data pipelines to empower data science teams', 'Help with migrating from a monolithic system and AWS lambdas to a microservices-based architecture running on Kubernetes', 'Work within an agile, cross-functional team (SWE, SRE, QE) to build out and own services and projects, from requirements + specs to release and maintenance', 'About Arkose Labs\xa0', 'We value:', 'Learn more about Arkose Labs from our innovation sandbox presentation at RSA:\xa0https://youtu.be/Fyhr_ZuveLM', '5+ years experience with Big Data Pipelines (e.g. Kafka, Other Data Streaming Engines like Storm, Spark, etc.)5+ years experience with Data Modeling and Data Warehousing(Snowflake, AWS, Athena, etc.)5+ years experience with NoSQL technologies (Elasticsearch, Cassandra etc.)5+ years experience SQL1+ years experience Python1+ years experience Spark (or equivalent)', 'Startup Experience', 'Startup ExperiencePrevious work experience in cybersecurity and/or fraud company', 'The chance to work with some of the biggest names in the market such as Microsoft, Paypal, EA, Github, Twitch, Roblox, Twilio and Minecraft.\xa0', 'The unique opportunity to join an early stage, high growth startup with pioneering technology built by an ambitious team who are passionate about their product and love coming to work every day.', '\xa0', 'Arkose Labs is a fast-growing startup, backed by Microsoft and Paypal, that is disrupting the fraud industry with an innovative approach that undermines the economic drivers behind fraud. The Arkose Labs Fraud and Abuse Prevention Platform combines real-time intelligence, rich analytics and adaptive step-up challenges to progressively diminish the profitability of attacks while adapting to evolving attack patterns. Arkose Labs offers the only fraud solution with a 100% SLA guarantee. The world’s largest brands trust Arkose Labs to protect their customer journey while delivering unrivaled customer experience.\xa0', 'We offer a $1000.00 stipend for our employees to set up a home office.\xa0', 'Build and maintain new data pipelines to empower data science teams', '-\xa0Execution with precision, professionalism and urgency', 'Nice to have', '-\xa0People: first and foremost they are our most valuable resource. Our people are independent thinkers who make data driven decisions and take ownership and accountability in all the things they do.', 'Arkose Labs is an Equal Opportunity Employer that makes employment decisions without regard to race, color, religious creed, national origin, ancestry, sex, pregnancy, sexual orientation, gender, gender identity, gender expression, age, mental or physical disability, medical condition, military or veteran status, citizenship, marital status, genetic information, or any other characteristic protected by applicable law. In addition, Arkose Labs will provide reasonable accommodations for qualified individuals with disabilities.', 'We focus on professional growth and development and offer continuing education.']",Mid-Senior level,Full-time,Information Technology,Computer & Network Security,2021-02-10 11:25:00
"Data Engineer , Senior Level",USAA,"Plano, TX",10 hours ago,Be among the first 25 applicants,"['', 'Identifies and manages existing and emerging risks that stem from business activities and the job role.Ensures risks associated with business activities are effectively identified, measured, monitored, and controlled. Follows written risk and compliance policies and procedures for business activities.Design and implement complex technical solutions.Design, build, manage and optimize data pipelines for data structures encompassing data transformation, data models, schemas, metadata, data quality, and workload management.Participate in daily standups and lead design reviews.Breakdown business features into technical stories and approaches.Analyze data and enable machine learning.Create proof of concepts and prototypes.Implement efficient defect management, root cause analysis, and resolution processes.Assist in setting technical direction for the team.Mentor and coach junior engineers.', 'USAA Total Rewards', 'Comfortable working as part of a connected team, but self-motivated ', 'Minimum Requirements', 'About Usaa It', 'Implement efficient defect management, root cause analysis, and resolution processes.', '2 or more years’ experience with cloud-based data offerings (Amazon AWS, Redshift, Snowflake, Google GCP, Microsoft Azure)', 'Curious and excited by new ideas', 'Mentor and coach junior engineers.', 'Community-focused, dependable and committed ', 'Curious and excited by new ideasEnergized by a fast-paced environment Able to understand and translate business needs into leading-edge technology Comfortable working as part of a connected team, but self-motivated Community-focused, dependable and committed Exceptionally detail-oriented', 'Energized by a fast-paced environment ', 'Deep knowledge of a technology or product line.', 'NOT', '1 or more years of experience working with Agile Development Methodologies', '6 years of data management experience implementing data solutions demonstrating depth of technical understanding within a specific discipline(s)/technology(s)', 'Identifies and manages existing and emerging risks that stem from business activities and the job role.', ""Bachelor's degree in related field of study, OR 4 additional years of related experience beyond the minimum required6 years of data management experience implementing data solutions demonstrating depth of technical understanding within a specific discipline(s)/technology(s)Deep knowledge of a technology or product line."", 'About USAA', 'Compensation', 'Geographical Differential: Geographic pay differential is additional pay provided to eligible employees working in locations where market pay levels are above the national average.Shift premium will be addressed on an individual-basis for applicable roles that are consistently scheduled for non-core hours.BenefitsAt USAA our employees enjoy best-in-class benefits to support their physical, financial, and emotional wellness. These benefits include comprehensive medical, dental and vision plans, 401(k), pension, life insurance, parental benefits, adoption assistance, paid time off program with paid holidays plus 16 paid volunteer hours, and various wellness programs. Additionally, our career path planning and continuing education assists employees with their professional goals.Please click on the link below for more details.USAA Total RewardsRelocation assistance is NOTavailable for this position.For Internal CandidatesMust complete 12 months in current position (from date of hire or date of placement), or must have manager’s approval prior to posting.Last day for internal candidates to apply to the opening is 2/01/21 by 11:59 pm CST time.', 'Exceptionally detail-oriented', 'available', 'Analyze data and enable machine learning.Create proof of concepts and prototypes.', 'For Internal Candidates', 'Breakdown business features into technical stories and approaches.', 'Design and implement complex technical solutions.', 'Participate in daily standups and lead design reviews.', '3 or more years’ experience utilizing Linux scripting (Python, Shell, etc.)', 'Benefits', 'Design, build, manage and optimize data pipelines for data structures encompassing data transformation, data models, schemas, metadata, data quality, and workload management.', 'Ensures risks associated with business activities are effectively identified, measured, monitored, and controlled. ', 'DESIRED CHARACTERISTICS', '6+ years’ experience developing, deploying and supporting high-quality, fault-tolerant data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)', 'Last day for internal candidates to apply to the opening is 2/01/21 by 11:59 pm CST time.', 'Shift premium', 'Preferred Requirements', ""Bachelor's degree in related field of study, OR 4 additional years of related experience beyond the minimum required"", 'Able to understand and translate business needs into leading-edge technology ', 'Follows written risk and compliance policies and procedures for business activities.', '4 or more years’ experience in SQL development and/or NoSQL databases', 'Experience with ETL Tools (Informatica, DataStage, Data Build Tool, etc.)', '6+ years’ experience developing, deploying and supporting high-quality, fault-tolerant data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)3 or more years’ experience utilizing Linux scripting (Python, Shell, etc.)4 or more years’ experience in SQL development and/or NoSQL databases2 or more years’ experience with cloud-based data offerings (Amazon AWS, Redshift, Snowflake, Google GCP, Microsoft Azure)Experience with ETL Tools (Informatica, DataStage, Data Build Tool, etc.)1 or more years of experience working with Agile Development Methodologies', 'Assist in setting technical direction for the team.']",Not Applicable,Full-time,Information Technology,Financial Services,2021-02-10 11:25:00
Data Engineer,Aclara,"Solon, OH",11 hours ago,Be among the first 25 applicants,"['', 'Preferred:', '2+ years of experience with at least one of the following cloud platforms: Microsoft Azure, Amazon Web Services (AWS), Google Cloud Platform (GCP), others', '3+ years of experience in data engineering with an emphasis on data analytics and reporting', '3+ years of experience with one or more of the follow scripting languages: Python, SQL, Kafka and/or other', 'Experience with Python and Spark', ""Master's Degree in Computer Science, Engineering, Mathematics, Statistics or business-related field."", 'Experience on end to end deployment in Azure with CI/CD pipelines.', '3 years of experience in advanced data analytics, math or statistical modeling, or machine learning (R, Python, SAS, MATLAB).', 'Design and build data pipelines utilizing the cloud technologies including Azure', 'Collaborate on the strategy of new cloud data stores and the migration of existing data', "" Bachelor's Degree in Computer Science, Engineering, Mathematics, Statistics or business-related field 3+ years of experience in data engineering with an emphasis on data analytics and reporting 2+ years of experience with at least one of the following cloud platforms: Microsoft Azure, Amazon Web Services (AWS), Google Cloud Platform (GCP), others 2+ years of experience working in Azure based deployments (Batch/Real-Time) leveraging Azure SQL, Databricks, ADLS, Blob Storage, ADF, Azure Stream Analytics Experience with Python and Spark 2+ years ETL/ELT experience is preferred. 3+ years of experience with one or more of the follow scripting languages: Python, SQL, Kafka and/or other 3 years of experience in advanced data analytics, math or statistical modeling, or machine learning (R, Python, SAS, MATLAB). 2+ years of experience manipulating large datasets in cloud environment and applying the appropriate tools (Data Lake, Hadoop, Spark, Data Factory, Structured/Unstructured database). Ability to communicate complex quantitative analysis in a clear, precise, and actionable manner. "", ""Bachelor's Degree in Computer Science, Engineering, Mathematics, Statistics or business-related field"", 'Communicates findings to team, managers and senior leadership.', ' Normal office environment. Frequent sitting and standing.', ' Collaborate on the strategy of new cloud data stores and the migration of existing data Migrate on premise workloads to Azure cloud platform Design and build data pipelines utilizing the cloud technologies including Azure Build ETL/ELT pipelines using Data Factory and Databricks Build and test CI/CD deployment pipelines for data systems Contributes to the planning and development of the data architecture for 24/7/365 the automated collection, delivery, preprocessing of large distributed customer performance data sets and reporting. Mine and engineer data from large and varied data sets (structured/unstructured).  Communicates findings to team, managers and senior leadership. Provides on-going tracking and monitoring of data collection, delivery and preprocessing performance. Recommends ongoing improvements to methods and algorithms that lead to findings, including new information. ', "" Experience on end to end deployment in Azure with CI/CD pipelines. Experience in implementing data solutions in Azure including Azure SQL, Azure Synapse, Cosmos DB, Databricks, ADLS, Blob Storage, ADF, Azure Stream Analytics Master's Degree in Computer Science, Engineering, Mathematics, Statistics or business-related field. "", 'Normal office environment.', 'Essential Functions:', '2+ years ETL/ELT experience is preferred.', 'Contributes to the planning and development of the data architecture for 24/7/365 the automated collection, delivery, preprocessing of large distributed customer performance data sets and reporting.', 'Build ETL/ELT pipelines using Data Factory and Databricks', 'Migrate on premise workloads to Azure cloud platform', 'Provides on-going tracking and monitoring of data collection, delivery and preprocessing performance.', 'Recommends ongoing improvements to methods and algorithms that lead to findings, including new information.', 'Mine and engineer data from large and varied data sets (structured/unstructured). ', 'Required:', 'Physical Demands:', 'Frequent sitting and standing.', 'Experience in implementing data solutions in Azure including Azure SQL, Azure Synapse, Cosmos DB, Databricks, ADLS, Blob Storage, ADF, Azure Stream Analytics', 'Ability to communicate complex quantitative analysis in a clear, precise, and actionable manner.', '2+ years of experience working in Azure based deployments (Batch/Real-Time) leveraging Azure SQL, Databricks, ADLS, Blob Storage, ADF, Azure Stream Analytics', '2+ years of experience manipulating large datasets in cloud environment and applying the appropriate tools (Data Lake, Hadoop, Spark, Data Factory, Structured/Unstructured database).', 'Build and test CI/CD deployment pipelines for data systems']",Entry level,Full-time,Information Technology,Electrical/Electronic Manufacturing,2021-02-10 11:25:00
Data Engineer,Amazon Web Services (AWS),"Seattle, WA",23 hours ago,Be among the first 25 applicants,"['', ' Experience in working and delivering end-to-end projects independently.', ' Collaborate with Data Scientists to implement advanced analytics algorithms that exploit our rich data sets for statistical analysis, prediction, clustering and machine learning', ' Bachelor’s degree in Computer Science or related technical field, or equivalent work experience.', ' Experience with AWS services including S3, Redshift, EMR, Kinesis and RDS.', ' 5+ years of work experience with ETL, Data Modeling, and Data Architecture.', ' Experience in ETL optimization, designing, coding, and tuning big data processes using Apache Spark or similar technologies.', ' Develop and improve the current data architecture, data quality, monitoring and data availability.', ' Knowledge of distributed systems and data architecture (lambda)- design and implement batch and stream data processing pipelines, knows how to optimize the distribution, partitioning, and MPP of high-level data structures.', 'Basic Qualifications', ' Experience with Big Data Technologies (Hadoop, Hive, Hbase, Pig, Spark, etc.)', ' Help continually improve ongoing reporting and analysis processes, simplifying self-service support for customers', ' 4+ years of work experience in writing and optimizing SQL.', ' Experience in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.', ' Knowledge of distributed systems as it pertains to data storage and computing', ' Experience in ETL optimization, designing, coding, and tuning big data processes using Apache Spark or similar technologies. Experience with building data pipelines and applications to stream and process datasets at low latencies. Experience in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data. Knowledge of distributed systems and data architecture (lambda)- design and implement batch and stream data processing pipelines, knows how to optimize the distribution, partitioning, and MPP of high-level data structures. Experience with native AWS technologies for data and analytics such as Redshift Spectrum, Athena, S3, Lambda, Glue, EMR, Kinesis, SNS, CloudWatch, etc', 'Description', ' Experience with building data pipelines and applications to stream and process datasets at low latencies.', 'Preferred Qualifications', ' Keep up to date with advances in big data technologies and run pilots to design the data architecture to scale with the increased data sets of customer experience on AWS.', ' Bachelor’s degree in Computer Science or related technical field, or equivalent work experience. 5+ years of work experience with ETL, Data Modeling, and Data Architecture. 4+ years of work experience in writing and optimizing SQL. Experience with AWS services including S3, Redshift, EMR, Kinesis and RDS. Experience with Big Data Technologies (Hadoop, Hive, Hbase, Pig, Spark, etc.) Experience in working and delivering end-to-end projects independently. Knowledge of distributed systems as it pertains to data storage and computing', 'You Will', ' Experience with native AWS technologies for data and analytics such as Redshift Spectrum, Athena, S3, Lambda, Glue, EMR, Kinesis, SNS, CloudWatch, etc', ' Develop and improve the current data architecture, data quality, monitoring and data availability. Collaborate with Data Scientists to implement advanced analytics algorithms that exploit our rich data sets for statistical analysis, prediction, clustering and machine learning Partner with BAs across teams to build and verify hypothesis to improve the AWS Support business. Help continually improve ongoing reporting and analysis processes, simplifying self-service support for customers Keep up to date with advances in big data technologies and run pilots to design the data architecture to scale with the increased data sets of customer experience on AWS.', ' Partner with BAs across teams to build and verify hypothesis to improve the AWS Support business.', 'Company']",Not Applicable,Full-time,Strategy/Planning,Computer Software,2021-02-10 11:25:00
Senior Data Engineer,Procore Technologies,"Remote, OR",3 hours ago,Be among the first 25 applicants,"['', ' Help identify and propose solutions for technical and organizational gaps in our analytics pipeline by running proof of concepts and experiments working with Data Platform Engineers and Architects on implementation ', '  BS degree in Computer Science, a similar technical field of study, or equivalent practical experience is required; MS or Ph.D. degree in Computer Science or a related field is preferred   5+ years of experience in a Data Engineer role   Experience with AWS (EC2, EMR, RDS, Redshift), JAVA, Airflow, PostgreSQL, Spark, Snowflake, and Data pipeline/streaming tools (Kafka) is preferred   Experience building and optimizing data pipelines, architectures, and data sets   Successful history of building analytics pipeline moving billions of data points   Understanding of the tradeoffs between short-term and long-term goals and balancing immediate needs with strategic initiatives   Experience supporting and working with cross-functional teams in a dynamic environment   Strong oral and written communication skills  ', ' Strong oral and written communication skills ', ' Participate in the evolution of Data Engineering at Procore ', ' Create and share best practices for the development and deployment of data engineering solutions\u202f ', ' Contribute to code reviews, design reviews ', 'What You’ll Do', ' Partner with teams on modeling and analysis problems—from transforming problem statements into analysis problems, to working through data modeling and engineering, to analysis and communication of results ', ' Understanding of the tradeoffs between short-term and long-term goals and balancing immediate needs with strategic initiatives ', 'Senior Data Engineer', ' Experience supporting and working with cross-functional teams in a dynamic environment ', 'Senior Data Engineer ', ' Provide technical leadership to efforts around building a robust and scalable analytical data pipeline to support billions of events ', '  Provide technical leadership to efforts around building a robust and scalable analytical data pipeline to support billions of events   Help identify and propose solutions for technical and organizational gaps in our analytics pipeline by running proof of concepts and experiments working with Data Platform Engineers and Architects on implementation   Contribute to code reviews, design reviews   Partner with teams on modeling and analysis problems—from transforming problem statements into analysis problems, to working through data modeling and engineering, to analysis and communication of results   Participate in the evolution of Data Engineering at Procore   Create and share best practices for the development and deployment of data engineering solutions\u202f   Working alongside our Product, UX, and IT teams, you’ll leverage your experience and expertise in the analytics event space to influence our product roadmap, developing innovative solutions that add additional capabilities to our tools  ', 'What We’re Looking For', ' Working alongside our Product, UX, and IT teams, you’ll leverage your experience and expertise in the analytics event space to influence our product roadmap, developing innovative solutions that add additional capabilities to our tools ', 'Perks & Benefits', ' Experience building and optimizing data pipelines, architectures, and data sets ', ' 5+ years of experience in a Data Engineer role ', ' BS degree in Computer Science, a similar technical field of study, or equivalent practical experience is required; MS or Ph.D. degree in Computer Science or a related field is preferred ', ' Experience with AWS (EC2, EMR, RDS, Redshift), JAVA, Airflow, PostgreSQL, Spark, Snowflake, and Data pipeline/streaming tools (Kafka) is preferred ', 'About Us', ' Successful history of building analytics pipeline moving billions of data points ']",Associate,Full-time,Information Technology,Information Technology and Services,2021-02-10 11:25:00
Data Engineer (ETL/SQL) - US Based ,Next Pathway Inc.,"New York, United States",2 hours ago,Be among the first 25 applicants,"['', '.', '· Strong\xa0Data Warehouse\xa0knowledge and experience, include data modeling, data ingestion, transformation, data consumption patterns', 'Next Pathway is located in the heart of the Financial District, minutes from Union Station and the Subway.', 'The candidate needs to have extensive experience as follows:', 'Other Technologies you Might Work with Include:', 'Our work environment is based on 3 core principles:', '· Emphasize quality first, each and every time', 'Data Warehouse', '· Develop and maintain unit tests and integration tests, and test automation.', 'Other Skills:', 'Next Pathway\xa0- The Automated Cloud Migration Company', 'SQL', '· Experience with major programming languages (Java, Pcorrespondenceython, Scala, C++, Java Script)', '· Minimum of 5 years of proven experience in a core competency', '· Adoption of Agile and Scrum development methodology', '· Build a team of well qualified individuals that can share ideas and learn from each other', '· Team Player with Excellent Interpersonal and Communication Skills (Written and Verbal)', '· Microsoft Office Suite (Excel, Word, PowerPoint, Outlook)', '· Slack: Messaging Platform', '· Confluence: Collaboration Tool', 'Next Pathway is full of bright and diverse thinkers. With deep exposure to AI, Machine Learning and Robotic Process Automation, our team members have opportunities to be trailblazers in the technology space. We encourage self-starters, transparency and team connectivity. We know diverse teams make strong teams. We welcome people of diverse backgrounds, experiences, and perspectives.', '· Strong understanding and experience of relational database, include design and implementation', '· Strong Work Ethic with a Positive Attitude and a Passion for Data and Development', 'Next Pathway', '· Experience with\xa0Cloud Technologies\xa0(Azure, GCP, Snowflake, AWS, Yellowbrick)', '· Basecamp: Project Management & Team Communication', 'ETL', 'We are looking for a skilled\xa0Data Engineer (ETL/SQL) who is US Based\xa0to join our team!', '· BA/MS/PHD degree in Computer Science, Engineering, or a related subject', '· Experience in at least 3 of below RDBMS: Teradata, Netezza, SQL Server, Greenplum, Oracle, Sybase, DB2, MySQL, Redshift, SQLDW.', '· Strong\xa0SQL\xa0knowledge and experience, including developing and optimizing complex queries, creating efficient UDFs to extend the functionalities', '· Proven hands-on Software Development experience', 'Cloud Technologies', 'Listed as one of Canada’s hottest start-ups by the Globe and Mail, Next Pathway is a technology services company providing clients a pathway from existing to emerging technologies. Our automation technology helps our customers accelerate the migration of complex applications and workloads to the cloud.', 'Next Pathway rewards people for hard work, loyalty, innovation and mutual support. We aim to match people’s strengths, skills and talents to our requirements. Identifying this ideal match between attitude, skill and need, leads to success.', '· JIRA: Project Management Software', '· Excellent Time Management Skills', '· Zoom: Meetings and Video Conferencing', 'Data Engineer (ETL/SQL) who is US Based\xa0', '· Strong Analytical and Problem-Solving Skills', '· Put people in roles where they will succeed and feel challenged', '· Experience with\xa0ETL\xa0tools (Informatica, DataStage, SSIS, Talend)']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-02-10 11:25:00
Data Engineer,InCloudCounsel,"New York, NY",22 hours ago,Be among the first 25 applicants,"['', 'Fluency in SQL and python and proven ability to ship flexible, maintainable code', 'New York, ', ' Develop, implement and maintain data pipelines and data processing code to support product features Create new data sets and tooling to help ML and analytics teams uncover insights and develop NLP-based solutions to legal tech problems Support usage and maintenance of our data infrastructure (i.e., AWS stack, Snowflake, Airflow)  Champion efforts to enforce data governance, quality and security across the organization ', 'Create new data sets and tooling to help ML and analytics teams uncover insights and develop NLP-based solutions to legal tech problems', ' Medical, dental, and vision insurance 401K, pre-tax benefits Parental leave Generous vacation policy Annual professional development stipend Remote work flexibility ', ""Until further notice, all InCloudCounsel employees are working remotely from home.Responsibilities: Develop, implement and maintain data pipelines and data processing code to support product features Create new data sets and tooling to help ML and analytics teams uncover insights and develop NLP-based solutions to legal tech problems Support usage and maintenance of our data infrastructure (i.e., AWS stack, Snowflake, Airflow)  Champion efforts to enforce data governance, quality and security across the organization Requirements: 3-5 years of experience in data engineering Fluency in SQL and python and proven ability to ship flexible, maintainable code Experience developing and productionizing complex data processing pipelines Comfort using AWS tooling and experience maintaining modern data infrastructure (warehousing, job scheduling) Excellent communication and problem solving skills and ability to effectively collaborate with technical and business partners Familiarity with data governance frameworks and Agile methodology Previous experience developing ML pipelines is a plus Interest in legal tech is a plus About the team: We're all about freedom. InCloudCounsel is on a mission to free companies from outdated legal processes and corporate lawyers from outdated work models. We're a group of former Big Law lawyers, business professionals, and engineers working together to modernize the legal industry. The people behind developing our product, servicing our customers and lawyer partners, and driving our business operations are dynamic individuals who have also achieved the freedom to do what's important to them - they're musicians, dancers, photographers, sailors, surfers, world travelers, home flippers, and animal lovers - and our freedom inspires us to free others.Benefits Medical, dental, and vision insurance 401K, pre-tax benefits Parental leave Generous vacation policy Annual professional development stipend Remote work flexibility In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification form upon hire.NOTE to External Agencies: We are not accepting any blind submissions or resumes from recruitment agencies. Any resumes sent to a member of InCloudCounsel will NOT be accepted or considered as a submission."", 'About the team:', 'San Francisco, or ', 'Generous vacation policy', 'Champion efforts to enforce data governance, quality and security across the organization', 'Director of Data Infrastructure', 'Interest in legal tech is a plus', '401K, pre-tax benefits', 'Develop, implement and maintain data pipelines and data processing code to support product features', 'Previous experience developing ML pipelines is a plus', 'NOTE to External Agencies: We are not accepting any blind submissions or resumes from recruitment agencies. Any resumes sent to a member of InCloudCounsel will NOT be accepted or considered as a submission.', 'Remote work flexibility', 'Support usage and maintenance of our data infrastructure (i.e., AWS stack, Snowflake, Airflow) ', 'Benefits', 'Los Angeles.', 'Responsibilities:', 'Comfort using AWS tooling and experience maintaining modern data infrastructure (warehousing, job scheduling)', 'Data Engineer', 'Excellent communication and problem solving skills and ability to effectively collaborate with technical and business partners', 'Familiarity with data governance frameworks and Agile methodology', 'Annual professional development stipend', 'Experience developing and productionizing complex data processing pipelines', 'Parental leave', ' 3-5 years of experience in data engineering Fluency in SQL and python and proven ability to ship flexible, maintainable code Experience developing and productionizing complex data processing pipelines Comfort using AWS tooling and experience maintaining modern data infrastructure (warehousing, job scheduling) Excellent communication and problem solving skills and ability to effectively collaborate with technical and business partners Familiarity with data governance frameworks and Agile methodology Previous experience developing ML pipelines is a plus Interest in legal tech is a plus ', 'Medical, dental, and vision insurance', '3-5 years of experience in data engineering', 'Requirements:']",Entry level,Full-time,Information Technology,Computer Software,2021-02-10 11:25:00
Data Infrastructure Engineer,Benchling,"San Francisco, CA",11 hours ago,Be among the first 25 applicants,"['', ' Define and design data pipelines and data integrations to collect, clean, and store large scale, cross-functional datasets. Build dashboards / reports and empower the business to answer key questions across sales, marketing, customer experience, recruiting, and engineering. Engineer changes to the Benchling product itself to provide deeper instrumentation, unlock new datasets, and enable powerful feedback loops. Work closely with teams across sales, marketing, customer experience, recruiting, and engineering to develop data platform strategy and establish best practices. ', 'Work closely with teams across sales, marketing, customer experience, recruiting, and engineering to develop data platform strategy and establish best practices.', 'Comfortable with complexity in the short term but can build towards simplicity in the long term', 'Plus: experience with Spark, Airflow, Map Reduce, data warehouses, and other big data technologies', '5+ years of industry experience designing and building data processing systems', 'Engineer changes to the Benchling product itself to provide deeper instrumentation, unlock new datasets, and enable powerful feedback loops.', "" BS or MS degree in computer science or a related technical field 5+ years of industry experience designing and building data processing systems Driven by creating positive impact for our customers and Benchling's business, and ultimately accelerating the pace of research in the Life Sciences Strong communicator with both words and data - you understand what it takes to go from raw data to something a human understands Comfortable with complexity in the short term but can build towards simplicity in the long term Comfortable with SQL and Python (or some other scripting language) Experience with BI visualization tools such as Tableau or Looker Plus: experience with Spark, Airflow, Map Reduce, data warehouses, and other big data technologies "", 'BS or MS degree in computer science or a related technical field', 'Comfortable with SQL and Python (or some other scripting language)', 'Experience with BI visualization tools such as Tableau or Looker', 'Build dashboards / reports and empower the business to answer key questions across sales, marketing, customer experience, recruiting, and engineering.', 'Strong communicator with both words and data - you understand what it takes to go from raw data to something a human understands', 'About You', 'Define and design data pipelines and data integrations to collect, clean, and store large scale, cross-functional datasets.', 'Responsibilities', ""Driven by creating positive impact for our customers and Benchling's business, and ultimately accelerating the pace of research in the Life Sciences""]",Associate,Full-time,Engineering,Computer Software,2021-02-10 11:25:00
