job_title,company,location,date_posted,applicants,job_text,seniority_level,employment_type,job_function,industries,date_scraped
Data Engineer,Madewell,"Queens, NY",3 hours ago,Be among the first 25 applicants,"['4+ years of experience with schema design and dimensional data modeling ', 'Have experience designing and developing enterprise data warehouses ', 'Communicate consistently, with purpose and an understanding of your audience. ', 'Be nimble and comfortable with change. ', 'Identify operational data issues and recommend strategies to resolve data problems. ', 'Have a bachelor’s degree. Preferred bachelor’s degree in a Technical Field; ', 'You’ve got to…', 'We want you to…', 'Have a great fashion esthetic and be all over what’s happening in the industry. ', 'Get us – a passion for the brand that shows up in everything you do, everyday. ', 'Recommend improvements and modifications to existing data integrations and ETL/ELT ', 'Be a team player – cultivate productive relationships with cross-functional business partners. ', 'Evangelize a data-driven culture and processes using modern tools and methodologies ', 'Forge a strong partnership with our business stakeholders to ensure success of data initiatives ', ' ', '1+ years of experience with object-oriented programming languages, like Java or Python ', '\xa0 ', 'Oh, and by the way, you…', 'Get us – a passion for the brand that shows up in everything you do, everyday. Be a strategic thinker – spend time and energy on what drives the greatest results. Look under rocks, be curious, ask questions and use your smarts to think boldly and do the right thing. Be a team player – cultivate productive relationships with cross-functional business partners. Communicate consistently, with purpose and an understanding of your audience. Be a multi-task master – make quick decisions under tight timelines. Be nimble and comfortable with change. Work independently and take the lead, even when all of the pieces are not in place. Articulate your point of view and have the courage and conviction to stand up for your beliefs. Have a great fashion esthetic and be all over what’s happening in the industry. Always be on, up for anything and ready to have fun along the way. ', '(Reports to Head of Data and Analytics)\xa0 ', 'Design and develop scalable, high performing data models for a wide range of business groups, including Store Operations, Financial Planning, Allocation, Merchandising, Web and Marketing ', 'Be able to work with minimum supervision and proactively prioritize tasks  ', 'Look under rocks, be curious, ask questions and use your smarts to think boldly and do the right thing. ', 'Strive to implement data governance throughout the enterprise ', 'Translate business requirements into effective and resilient data pipelines and data models ', 'Have a bachelor’s degree. Preferred bachelor’s degree in a Technical Field; 4+ years of experience in the data warehouse space in hands-on roles 4+ years of experience in writing complex SQL and ETL/ELT processes, preferably in a retail, brand manufacturing or consumer-facing company 4+ years of experience with schema design and dimensional data modeling 1+ years of experience with object-oriented programming languages, like Java or Python \xa0Preferred experience in a retail, CPG or e-commerce company ', 'Translate business requirements into effective and resilient data pipelines and data models Implement best practices to ensure data quality throughout our data pipelines and integrations Recommend improvements and modifications to existing data integrations and ETL/ELT Handle various data formats including structured (databases) and semi-structured (XML, JSON, csv) Design and develop scalable, high performing data models for a wide range of business groups, including Store Operations, Financial Planning, Allocation, Merchandising, Web and Marketing Forge a strong partnership with our business stakeholders to ensure success of data initiatives Evangelize a data-driven culture and processes using modern tools and methodologies Administer Snowflake database and data orchestration tools Strive to implement data governance throughout the enterprise Identify operational data issues and recommend strategies to resolve data problems. Resolve critical data issues as they arise Have experience designing and developing enterprise data warehouses Be able to work with minimum supervision and proactively prioritize tasks  ', 'Data Engineer ', 'Work independently and take the lead, even when all of the pieces are not in place. ', '4+ years of experience in the data warehouse space in hands-on roles ', 'Be a strategic thinker – spend time and energy on what drives the greatest results. ', 'Always be on, up for anything and ready to have fun along the way. ', 'Implement best practices to ensure data quality throughout our data pipelines and integrations ', 'Handle various data formats including structured (databases) and semi-structured (XML, JSON, csv) ', 'Administer Snowflake database and data orchestration tools ', 'Oh, and by the way, you… ', '4+ years of experience in writing complex SQL and ETL/ELT processes, preferably in a retail, brand manufacturing or consumer-facing company ', 'Be a multi-task master – make quick decisions under tight timelines. ', 'Resolve critical data issues as they arise ', 'We want you to… ', 'You’ve got to… ', 'Articulate your point of view and have the courage and conviction to stand up for your beliefs. ', '\xa0Preferred experience in a retail, CPG or e-commerce company ', 'Data Engineer']",Mid-Senior level,Full-time,Information Technology,Apparel & Fashion,2021-02-16 15:27:39
Data Engineer,everis,"New Haven, CT",7 minutes ago,Be among the first 25 applicants,"['·\xa0\xa0\xa0\xa0\xa0\xa0Maintain production and development big data platforms along with the analytics applications', '·\xa0\xa0\xa0\xa0\xa0\xa0Identify, design and maintain data models as required.', '·\xa0\xa0\xa0\xa0\xa0\xa0+3 years of experience as Data Engineer', '\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0', '·\xa0\xa0\xa0\xa0\xa0\xa0+1 year of experience working with Cloudera/Hortonworks', 'everis USA is looking for high-achieving team players that are quickly adaptable to new challenges and entrepreneurial ventures. We are currently seeking a Data Engineer to work directly with the client- including business and IT staff- to provide true consultation and thought leadership on cutting edge technologies.\xa0', 'Being part of the NTT Data group enables\xa0everis\xa0to offer a wider range of solutions and services through increased capacity, as well as technological, geographical, and financial resources.\xa0', ' Data Engineer ', '·\xa0\xa0\xa0\xa0\xa0\xa0Data Provisioning (including ETL/ET).', '·\xa0\xa0\xa0\xa0\xa0\xa0Analyze the existing data repositories and business processes to derive data profiles and identify data sources.', '·\xa0\xa0\xa0\xa0\xa0\xa0Automate the big data jobs that runs both on batch node and in real-time.', '\xa0', '·\xa0\xa0\xa0\xa0\xa0\xa0Assist in the design of data acquisition, cleansing, and transformation into target data structures from potentially multiple sources: SQL and non SQL databases, SAP, structured and non-structured files, Hadoop.', 'Why everis?\xa0\xa0\xa0', '·\xa0\xa0\xa0\xa0\xa0\xa0Support in when and how to leverage new Big Data technologies such as, NoSQL, Hadoop and foster its usage within the organization with clear business outcomes', '·\xa0\xa0\xa0\xa0\xa0\xa0Experience with Big Data technologies such as NoSQL/Hadoop and infrastructures such as Hortonworks/MS Azure HDInsight', '·\xa0\xa0\xa0\xa0\xa0\xa0Help maintain documentation of data models and/or other reusable knowledge assets regarding data governance.', 'everis\xa0is committed to hiring and retaining a diverse workforce. We are proud to be an Equal Opportunity/Affirmative Action-Employer, making decisions without regard to race, color, religion, creed, sex, sexual orientation, gender identity, marital status, national origin, age, veteran status, disability, or any other protected class.\xa0everis is an Equal Opportunity Employer Male/Female/Disabled/Veteran and a VEVRAA Federal Contractor.\xa0', '·\xa0\xa0\xa0\xa0\xa0\xa0Design and develop programs that will run in Big Data infrastructures: On-premise cluster (Hortonworks), Cloud(MS Azure HDInsight):', '·\xa0\xa0\xa0\xa0\xa0\xa0Present to business, non-technical audience being able to create, prototypes, and demonstrations to help management better understand the work.', '·\xa0\xa0\xa0\xa0\xa0\xa0Massive data processing (smart meter, transformer and other utility assets data).Operationalize analytic models to run efficiently (analytics on Hadoop)', '100% company paid benefits package', 'Responsibilities:', 'Knowledge/Experience: ', 'Empowerment and rewards are the cornerstone of our career development model. We are a young, fast-growing company, with a highly innovative and entrepreneurial spirit, because of this professional experience and growth will be unmatched. Our talent and positive attitude allows us to transform our goals into achievements, and projects into realities. Additionally we offer our employees competitive salaries and 100% company paid benefits package that is unmatched.\xa0', 'everis\xa0is a multinational consulting firm providing business and strategy solutions, application development, maintenance, and outsourcing services. Established in 1996, everis has averaged 20% annual growth in revenues, and it\xa0became part of NTT Data in January, 2014.\xa0', '·\xa0\xa0\xa0\xa0\xa0\xa0Identify, manage and design requirements for data extractions, data transformations and data loading, to and from various technical platforms; including SAS, Oracle Exadata and Hadoop and other NoSQL ecosystems to ensure business value is delivered in the most efficient way.', 'Data Engineer']",Associate,Full-time,Information Technology,Information Technology and Services,2021-02-16 15:27:39
Data Engineer,Vaco,"Brentwood, TN",3 hours ago,Be among the first 25 applicants,"['5+ years of experience maintaining Microsoft SQL environment up to current version (at least SQL/Server 2016/2017) ', 'Responsibilties:', 'Conduct data analysis and interpret data to answer key business questions', 'Experience with cloud technologies like Azure, Google Cloud, or AWS strongly preferred.', 'Develop report deliverables and data visualizations producing valuable insights to stakeholders', '5+ years of in-depth Microsoft SQL development experience.', 'Programming experience with C#, .net core, or javascript preferred ', 'Effective communication skills, written, verbal and interpersonal.', '5+ years of in-depth Microsoft SQL development experience.5+ years of experience maintaining Microsoft SQL environment up to current version (at least SQL/Server 2016/2017) Extensive knowledge of T-SQL, ETL, Stored Procedures, SSIS, SSRS, and BI toolsExperience working in a fast paced Agile (Scrum/Kanban) environmentEffective communication skills, written, verbal and interpersonal.Programming experience with C#, .net core, or javascript preferred Experience with cloud technologies like Azure, Google Cloud, or AWS strongly preferred.', 'Extensive knowledge of T-SQL, ETL, Stored Procedures, SSIS, SSRS, and BI tools', 'Experience working in a fast paced Agile (Scrum/Kanban) environment', 'Work closely with client stakeholders and coordinate delivery of projects in scope and on time', 'We are unable to sponsor or transfer H1B Visas at this time.', 'Drive data architecture and modeling recommendations', 'Work closely with client stakeholders and coordinate delivery of projects in scope and on timeWork as part of a team to deliver a successful large scale data migration Drive data architecture and modeling recommendationsPerform data ETL processing from multiple sources Conduct data analysis and interpret data to answer key business questionsDevelop report deliverables and data visualizations producing valuable insights to stakeholders', 'Work as part of a team to deliver a successful large scale data migration ', 'Perform data ETL processing from multiple sources ', 'Requirements:']",Entry level,Full-time,Information Technology,Computer Software,2021-02-16 15:27:39
Data Engineer,Wyre,"San Francisco, CA",5 hours ago,Be among the first 25 applicants,"['', ""Bachelor's degree in Computer Science or a related technical field from an accredited institution"", 'Computer setup of your choice', 'Responsibilities', 'Skills in Shell Programming and any Object Oriented Programming language', 'Close attention to detail and strong organizational skills', 'Experience with ETL tools like Xplenty/AWS Glue/Talend/Nifi', 'Benefits', 'Advanced SQL writing and experience in data mining (SQL, ETL, data warehouse, etc.)', '401(k) plan with corporate matching', 'Equity options for all full-time employees', 'Collaborate closely with internal and external teams to understand and apply changes/modifications impacting data warehouse', 'Experience with Cloud-based data-warehouse system Snowflake, Redshift, Azure SQL Data warehouse is a huge plus', "" Bachelor's degree in Computer Science or a related technical field from an accredited institution Mastery in Business Intelligence and Data warehousing concepts and methodologies 5+ years of developing end-to-end Business Intelligence solutions Data Warehouse, data modeling, ETL, and reporting Experience with Cloud-based data-warehouse system Snowflake, Redshift, Azure SQL Data warehouse is a huge plus Experience with ETL tools like Xplenty/AWS Glue/Talend/Nifi Advanced SQL writing and experience in data mining (SQL, ETL, data warehouse, etc.) Skills in Shell Programming and any Object Oriented Programming language Should be able to write custom ETL programming from complex data processing and transformations using any programming language, such as PL/Sql, Python, Java, etc Fast-learner and ability to pivot priorities quickly Close attention to detail and strong organizational skills Exceptional ability to communicate across teams and departments Creative problem solver  "", 'Enjoy a highly fulfilling, mission-driven culture', 'Life insurance and disability benefits', 'Creative problem solver ', '5+ years of developing end-to-end Business Intelligence solutions Data Warehouse, data modeling, ETL, and reporting', 'Provide timely and accurate estimates for new functionality requirements', 'Flexible work hours', 'Fast-learner and ability to pivot priorities quickly', 'Exceptional ability to communicate across teams and departments', 'Requirements', 'Perform analysis, conceptualize, design, implement and develop solutions for critical BI components from the ground up', 'Strong verbal and written communication skills, including the ability to effectively lead and influence interactions with both business and technical teams', 'Mastery in Business Intelligence and Data warehousing concepts and methodologies', 'Plan and implement standards, define/code ', 'Fair pay, no matter where you live along with a competitive benefits package', 'Perform data-flow, system, and data analysis to develop meaningful presentations of data in BI applications', 'Health, dental, and vision benefits for you and your family', 'Should be able to write custom ETL programming from complex data processing and transformations using any programming language, such as PL/Sql, Python, Java, etc', 'An opportunity to build the future and freedom to work wherever you want', ' Perform analysis, conceptualize, design, implement and develop solutions for critical BI components from the ground up Strong verbal and written communication skills, including the ability to effectively lead and influence interactions with both business and technical teams Perform data-flow, system, and data analysis to develop meaningful presentations of data in BI applications Plan and implement standards, define/code  Collaborate closely with internal and external teams to understand and apply changes/modifications impacting data warehouse Monitor ETL processes, system audits, and performance. Proactively resolve issues as found Support the performance of BI systems Provide timely and accurate estimates for new functionality requirements ', 'Support the performance of BI systems', 'You are an owner! We offer stock options to each of our employee', 'Monitor ETL processes, system audits, and performance. Proactively resolve issues as found', ' Enjoy a highly fulfilling, mission-driven culture You are an owner! We offer stock options to each of our employee An opportunity to build the future and freedom to work wherever you want Fair pay, no matter where you live along with a competitive benefits package Health, dental, and vision benefits for you and your family Life insurance and disability benefits Equity options for all full-time employees 401(k) plan with corporate matching Computer setup of your choice Unlimited paid time off to relax and recharge Flexible work hours Opportunity to work in a growing startup', 'Unlimited paid time off to relax and recharge', 'Opportunity to work in a growing startup']",Mid-Senior level,Full-time,Engineering,Computer Software,2021-02-16 15:27:39
Data Engineer,"The Select Group, Inc.","Denver, CO",6 hours ago,Be among the first 25 applicants,"['', 'Tableau experience', 'DATA ENGINEER', 'One of the largest telecommunications companies in the U.S. is looking for a Data Engineer to join their Product Insights division in Denver, Colorado and work on a brand new project for them. To be successful in this role, the candidate must be skilled in SQL, Hive, AWS and Tableau along with coming from a strong data engineering background. MUST be able to work on a W2 basis to be considered.', '7+ years Data Engineer/ Data Analyst experience', 'MUST be able to work on a W2 basis to be considered.', 'Ability to do advanced SQL queries', 'Hive experience (SQL language)', 'DATA ENGINEER REQUIREMENTS', 'SQL experience', 'Understanding of S3 buckets, how to create lambdas ', 'AWS experience', '7+ years Data Engineer/ Data Analyst experienceSQL experienceAbility to do advanced SQL queriesHive experience (SQL language)AWS experienceUnderstanding of S3 buckets, how to create lambdas Tableau experience', 'DATA ENGINEER RESPONSIBILITIES', 'This team does reporting on anything that touches video- including set top box, applications, and anything streamed over the internet. This team looks at how many users there are, their watch time, etc. This individual will create tables for this new product so that they can do analytics on it. The role will be very hands-on, and this person will spend roughing 70% of their time in SQL, 20% in AWS, and 10% in Tableau.']",Mid-Senior level,Contract,Information Technology,Real Estate,2021-02-16 15:27:39
Data Engineer,Solu Technology Partners,"Brea, CA",4 hours ago,Be among the first 25 applicants,"['', 'Data Engineer for Pet Insurance Division', 'Location:\xa0', '• Experience in data modeling and development of new data warehouse/data marts', '*Solü Technology Partners provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, sexual orientation, national origin, age, disability or genetics.', 'Responsibilities', '• Detailed analytic, Reporting, BI outputs and implications', '• Experience working with/managing a BI tool (Tableau/Looker/Qlikview/other)', '• Experience with ETL tools such as Informatica, DataStage, SSIS others a plus', '• Relational databases: PostgreSQL/SQL/Oracle', '• Graph Databases: Neo4j', '• Data Engineer 5+ years experience', 'Job Description', '• Experience with translating business needs into practical applications and solutions and can easily make the connection between detailed analytic / reporting / BI outputs and implications for the business, turning insights into business actions.', 'Title:', '• ETL designing, building, and maintaining', '\xa0', 'Qualifications', '• Cloud Data Warehouses: AWS Redshift, Azure Synapse Analytics, Google Cloud, Snowflake', '• Data modeling, data warehouse, data marts', '• Translating business needs into practical applications and solutions', '• ETL tools such as Informatica, DataStage, SSIS', '• Creating and tuning views, stored procedures, and functions', '• In-depth understanding of and experience working with relational databases like PostgreSQL/SQL/Oracle, cloud data warehouses like AWS Redshift, Azure Synapse Analytics, Google Cloud, Snowflake, and graph databases such as Neo4j', '• Experience working with/supporting Data Analytics teams', 'Location:\xa0Brea, California', '• Expert at writing SQL code, creating and tuning views, stored procedures, and functions, verifying data integrity and accuracy', '• Strong persuasion skills and effective communicator', 'Desired Qualifications', 'Title:\xa0Data Engineer', 'Location:', '• Strong persuasion skills and effective communicator; ability to create and communicate a compelling vision internally and externally.', '• Writing SQL code', 'This opportunity is not open to C2C relationships or Visa sponsorship.', '• Familiarity with designing, building, and maintaining ETL feeds for new and existing data sources.']",Associate,Full-time,Information Technology,Information Technology and Services,2021-02-16 15:27:39
Data Engineer,ZEN3,"Seattle, WA",4 hours ago,Be among the first 25 applicants,"['', '5+ years of industry experience in data analytics', 'Familiarity with Cosmos, Kusto, Azure Client if possible.', 'Requirement:', 'Location: Redmond, WA', 'Strong proficiency in Power BI/DAX, experience building tabular models, and SQL', 'Familiarity with Programming languages (R, Python etc.) and data modeling', 'PBI/DAX/SQL/R OR Python/some good math skills', 'Strong understanding of database management principles and relationship modeling', '\xa0Degree in Engineering, Analytics, Information Technology, Management Science, Data Science, or related field', 'Knowledge in applied statistics and mathematics', 'Title: Database Engineer']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-02-16 15:27:39
Data Engineer,Habu,United States,3 hours ago,Be among the first 25 applicants,"['', 'Spark / Kafka / Airflow', 'Golang gRPC / Scala / Java / Python', 'Experience working with AWS or GCP or Azure', 'Habu is a startup that is redefining how companies do data-driven marketing in a privacy-first era. Launched by the team that wrote the\xa0book on data-driven marketing\xa0as well as built the\xa0market-leading DMP\xa0that was acquired by Salesforce,\xa0Habu\xa0is designed for the 2020’s. We future proofed our system with identity-strong and privacy-by-design foundations and have built software applications for privacy-safe data sharing, analytics and measurement.\xa0In a complex world of distributed systems, our modular technology can deliver intelligence from data wherever it may live.', 'Understand and implement data security, quality and protection standards', '3+ years of professional experience with the following:', 'AWS / GCP / Azure', 'We are looking for a highly skilled data engineer who is comfortable wrangling with petabytes of data. They will be responsible for the architecture, design, development of data pipelines, optimize and scale data systems and/or build them ground up.\xa0', 'Responsibilities', 'Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sourcesBuild analytics tools that utilize the data pipeline to provide actionable data insightsBuild data systems that interact with microservice architectureUnderstand and implement data security, quality and protection standardsMaintain quality and ensure the responsiveness of data intensive applicationsCollaborate with the rest of the engineering team to design and launch new features', 'Build analytics tools that utilize the data pipeline to provide actionable data insights', 'Maintain quality and ensure the responsiveness of data intensive applications', 'Project lifecycle and Agile/Scrum/Kanban methodology', 'Bachelor’s degree or higher in Computer science', 'Golang gRPC / Scala / Java / PythonSpark / Kafka / AirflowPostgres / ScyllaDB / Snowflake\xa0AWS / GCP / AzureDocker / Kubernetes / Terraform', 'We firmly believe in the value of company culture which holds true to our core principles of grit, innovation, and collaboration. These values ring true in the people, products, and passion on display each and every day. We understand what makes experiences transformative in our company and with our clients and we know, Habu can help.', 'Project lifecycle and Agile/Scrum/Kanban methodologyBachelor’s degree or higher in Computer scienceExperience in the early-mid stages of a fast-growing company', 'Scala/Java/Python;\xa0Golang/gRPC a Huge Plus!', 'Proficient experience using big data tools – Spark, Kafka, Hadoop, AirflowHighly experienced building and optimizing big data pipelines, architectures and datasetsHighly experienced with back-end programming languages like Scala, Java, GolangExperience with cloud APIs and frameworksStrong analytical skills related to working with structured and unstructured datasetsExperience working with AWS or GCP or AzureProficient in the use of relational databases, SQL and NoSQL technologiesKnowledge of code versioning tools such as GitCollaborate with the rest of the engineering team to design and launch new featuresExperience working in an agile environment', 'Proficient experience using big data tools – Spark, Kafka, Hadoop, Airflow', 'Build data systems that interact with microservice architecture', 'SQL and/or NoSQL databases', 'Highly experienced building and optimizing big data pipelines, architectures and datasets', 'Experience with cloud APIs and frameworks', '\xa0', 'Highly experienced with back-end programming languages like Scala, Java, Golang', '3+ years of professional experience with the following:Spark/Kafka/Hadoop/AirflowScala/Java/Python;\xa0Golang/gRPC a Huge Plus!SQL and/or NoSQL databasesContinuous Integration/Continuous Deployment (CI/CD)Strong communication/presentation skills', 'Strong analytical skills related to working with structured and unstructured datasets', 'Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources', 'Continuous Integration/Continuous Deployment (CI/CD)', 'Golang/gRPC a Huge Plus!', 'Strong communication/presentation skills', 'Experience in the early-mid stages of a fast-growing company', 'Our Tech stack', 'Requirements', 'Knowledge of code versioning tools such as Git', 'Postgres / ScyllaDB / Snowflake\xa0', 'Desired Skills', 'Docker / Kubernetes / Terraform', 'Experience working in an agile environment', 'Spark/Kafka/Hadoop/Airflow', 'Proficient in the use of relational databases, SQL and NoSQL technologies', 'Nice to Have', 'Collaborate with the rest of the engineering team to design and launch new features']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-02-16 15:27:39
Data Engineer,Carrot Health,"Minneapolis, MN",3 hours ago,Be among the first 25 applicants,"['', 'Ability to work in a fast-paced, dynamic environment and adapt new  skills and responsibilities in an entrepreneurial organization. ', 'Ability to work remotely during the COVID pandemic, and ability to  work in an office setting upon full return to work in accordance with  Carrot Health policies.', 'Demonstrated experience and robust competency with SQL. ', 'Familiarity with ETL, data transformation, database, data warehouse,  and big data technologies such as Matillion, Talend, Snowflake, AWS  Redshift, MySQL, PostgreSQL, Hadoop, or Spark. ', 'Employs a strong work ethic and high standards for his/her own work and the work of others. ', 'Leverage unit testing, regression testing, and QA (Quality  Assurance) disciplines to minimize risk and ensure high quality work  product. ', 'Leverage CI/CD and other automation tool sets to deploy and manage  changes, ensure quality, provide consistency, and improve scalability  across the data platform. ', 'Responsibilities: ', 'Ability to leverage software development lifecycle capabilities  including Git version control, unit testing, regression testing, and  CI/CD pipelines. ', 'Grok large and complex data sets in order to make effective engineering and product decisions. ', ' ', 'Excellent written and oral communication skills in the English language. ', 'Demonstrated experience and working knowledge of engineering solutions within AWS (Amazon Web Services). ', 'This is a full-time, salaried, overtime-exempt position based in Minneapolis, MN. ', 'Desire & ability to continually learn and innovate in new technologies. ', 'Bachelor’s Degree in Computer Science, Mathematics, or equivalent relevant experience. ', 'Design and engineer databases, data schemas, and data pipelines that  meet engineering best-practices and support the required product  capabilities. ', 'Demonstrated experience and competency in one or more programming languages, such as Python, R, Java, Ruby, JavaScript, or Go. ', ' Employs a strong work ethic and high standards for his/her own work and the work of others.  Desire & ability to continually learn and innovate in new technologies.  Ability to be self-directed and work independently, as well as collaboratively.  Excellent organizational skills and the ability to handle multiple  projects at once, with superior attention to detail and follow-through.  Excellent written and oral communication skills in the English language.  Flexibility and ability to adapt quickly to changes.  Ability to work remotely during the COVID pandemic, and ability to  work in an office setting upon full return to work in accordance with  Carrot Health policies.', 'Ability to work through ambiguous and nebulous problems and develop context for complex solutions. ', 'Required Skills: ', 'The Data Engineer is responsible for developing and operating a broad set of data services  and data assets as part of Carrot Health’s MarketView platform.  As a  Data Engineer at Carrot Health, you will be developing and operating  data platform services that provide extensible data components as the  backbone of the MarketView platform. As a member of the Data Engineering  team, you work collaboratively within the team as well as  cross-functionally with Product Managers, Product Engineers, and Data  Scientists while participating in the technology development lifecycle. ', 'Personal Attributes: ', 'Personal Attributes:', 'Work within an Agile development environment in order to deliver incremental value. ', 'Excellent organizational skills and the ability to handle multiple  projects at once, with superior attention to detail and follow-through. ', 'Develop and operate data services to monitor and manage data quality  throughout the data lifecycle and adhere to data governance objectives. ', 'Develop and operate data transformation and data pipeline services  that are key to achieving the strategic product capabilities and  delivering customer value. ', 'Ability to be self-directed and work independently, as well as collaboratively. ', 'Required Skills:', 'Responsibilities:', 'Develop and operate data services to acquire and curate client data, proprietary data, commercial data, and public data. ', '3+ years as a Data Engineer, Software Engineer, Backend Developer, or similar role. ', 'Cultivate and deliver continuous improvement projects within Data  Engineering to optimize, scale, and improve quality of technology assets  and processes. ', ' Develop and operate data services to acquire and curate client data, proprietary data, commercial data, and public data.  Develop and operate data transformation and data pipeline services  that are key to achieving the strategic product capabilities and  delivering customer value.  Develop and operate data services to monitor and manage data quality  throughout the data lifecycle and adhere to data governance objectives.  Design and engineer databases, data schemas, and data pipelines that  meet engineering best-practices and support the required product  capabilities.  Leverage unit testing, regression testing, and QA (Quality  Assurance) disciplines to minimize risk and ensure high quality work  product.  Grok large and complex data sets in order to make effective engineering and product decisions.  Work within an Agile development environment in order to deliver incremental value.  Leverage CI/CD and other automation tool sets to deploy and manage  changes, ensure quality, provide consistency, and improve scalability  across the data platform.  Cultivate and deliver continuous improvement projects within Data  Engineering to optimize, scale, and improve quality of technology assets  and processes.  ', 'Flexibility and ability to adapt quickly to changes. ', 'Data Engineer']",Mid-Senior level,Full-time,Information Technology,"Health, Wellness and Fitness",2021-02-16 15:27:39
Data Engineer,Pie Insurance,"Denver, CO",11 hours ago,Be among the first 25 applicants,"['', '3-5 years working in data as an engineer. Building data solutions for a company who uses data as a primary part of their business.Experience in data warehouse and/or data analytics. Qualified candidates may also come from a strong database skill-set involved in analytics architecture.Strong experience in writing complex SQL queries.Strong experience in ETL/ELT platforms is strongly preferred.Exposure to one major SQL RDBMS or analytics database. (Snowflake, Redshift, MySQL, Postgres, Oracle, SQL Server, etc.)Big Data and Business Intelligence exposure would help in the success of this role.', 'Exposure to one major SQL RDBMS or analytics database. (Snowflake, Redshift, MySQL, Postgres, Oracle, SQL Server, etc.)', 'Experience in data warehouse and/or data analytics. Qualified candidates may also come from a strong database skill-set involved in analytics architecture.', 'Why Pie? ', ""How You'll Do It"", 'Strong experience in writing complex SQL queries.', 'Strong experience in ETL/ELT platforms is strongly preferred.', 'Big Data and Business Intelligence exposure would help in the success of this role.', 'Our Achievements', 'The Right Stuff', '3-5 years working in data as an engineer. Building data solutions for a company who uses data as a primary part of their business.', 'Pie Insurance is an equal opportunity employer. We do not discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity, marital status, age, disability, national or ethnic origin, military service status, citizenship, or other protected characteristic.']",Entry level,Full-time,Information Technology,Computer Software,2021-02-16 15:27:39
Data Engineer / Analyst,DISYS,"Plano, TX",,N/A,"['Job Title: Data Engineer / Analyst', '', '3+years of experience with AWS', 'Description/Skills Needed:', '5+years of experience with SQL ', '3+years of experience with Python ', 'SQL ', 'Remote:', 'Duration: 10 months', 'AWS', 'Looking for Combination of Data Analyst and Data Engineer.', 'Tableau ', 'Job Title: ', 'Experience with Spark and Databricks', '3+years of experience with Tableau ', 'Location: Plano, TX // Remote: Temporarily due to COVID-19', 'Python ', 'Duration:']",Mid-Senior level,Contract,Information Technology,Banking,2021-02-16 15:27:39
Data Engineer,Prospect 33,"Philadelphia, PA",5 hours ago,Be among the first 25 applicants,"['', 'Ability to mentor junior data engineers', 'Experience consuming bespoke data sets with no control over the format of the data', 'Drive the data practice to continuously become more efficient', 'Demonstrated experience in large dataset design and modeling', 'Remote work until the end of the virus then location in central Philidelphia.', 'Excellent knowledge of tooling in the data space and the ability to hone in on the right stack for the problem', 'Proficiency in a diverse set of programming languages, such as C# (.NET) and Python', 'We are looking for a seasoned data engineer who can work with a leading hedge fund in Philadelphia. They are constantly receiving new data sources that need to be integrated and fed to the appropriate lines of business as soon as possible. The team is expected to turn around a new pipeline for a new source within 1-2 days and needs a leader who can help not only with the low-level work but one who can make the process more efficient.', 'Integration of alternative data with traditional data sources', 'Experience working with Redshift and SQL Server', 'Minimum Bachelor’s degree in Computer Science or Computer Engineering', 'Qualifications/Requirements', 'Define the roadmap and drive execution', 'Lead a small team responsible for providing best in class data services to multiple internal stakeholders including Investment and Quantitative Research, Investment Analysis, and Technology', 'Effectively manage expectations', 'What you will do:', 'Lead a small team responsible for providing best in class data services to multiple internal stakeholders including Investment and Quantitative Research, Investment Analysis, and TechnologyBuild large-scale batch and real-time data pipelines for alternative dataQuickly build new pipelines for new data sourcesIntegration of alternative data with traditional data sourcesDefine the roadmap and drive executionDrive the data practice to continuously become more efficientShare your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community', 'Demonstrated expertise in data engineering, including but not limited to data platforms, ETL process engineering, data quality monitoring, and machine-assisted data discovery', 'Working knowledge of Docker and Kubernetes', 'Must have strong coding skills and be capable of creating a new pipeline based on a new data source quickly (max 1-2 days).', 'Quickly build new pipelines for new data sources', 'Build large-scale batch and real-time data pipelines for alternative data', 'Well-versed in the features of popular Big Data solutions, including cloud-hosted platforms', 'Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and iterative delivery environment? Most importantly, do you love experimenting with new technologies and increasing your knowledge of the data space?', 'Experience consuming bespoke data sets with no control over the format of the dataExcellent knowledge of tooling in the data space and the ability to hone in on the right stack for the problemDemonstrated experience in large dataset design and modelingDemonstrated expertise in data engineering, including but not limited to data platforms, ETL process engineering, data quality monitoring, and machine-assisted data discoveryFamiliarity with NoSQL technologies, such as key/value store, columnar store, document store, etc.Well-versed in the features of popular Big Data solutions, including cloud-hosted platformsProficiency in a diverse set of programming languages, such as C# (.NET) and PythonExperience working with Redshift and SQL ServerWorking knowledge of Docker and KubernetesEffectively manage expectationsAbility to mentor junior data engineersMinimum Bachelor’s degree in Computer Science or Computer EngineeringMust have strong coding skills and be capable of creating a new pipeline based on a new data source quickly (max 1-2 days).', 'Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community', 'Familiarity with NoSQL technologies, such as key/value store, columnar store, document store, etc.']",Mid-Senior level,Full-time,Information Technology,Investment Banking,2021-02-16 15:27:39
Data Engineer - USA,InterWorks,Oklahoma City Metropolitan Area,15 hours ago,Be among the first 25 applicants,"['', 'What You’ll Do', 'Programming (Python, Java, C#, PHP, etc.)', 'Strong problem-solving skills', 'Tackle diverse projects that range in duration from a few days to a few months for clients ranging from local businesses to the Fortune 500', 'Experience with integration from API sources', 'What We’d Like You to Have', 'AWS / Microsoft Azure', 'Experience with modern data-engineering practices and frameworks', 'Adaptability and flexibility in changing situations', 'Strong ETL proficiency using GUI-based tools or code-based patterns', 'Understanding of data-modeling principles', 'Experience with software engineering practicesExperience with modern data-engineering practices and frameworksExperience with integration from API sourcesMatillion, Fivetran, Airflow, DBT or other ETL toolsAWS / Microsoft AzureSnowflake / Amazon Redshift / Google BigQuery / Azure Synapse ', 'Create ETL processes based on client needs while managing client expectations', 'Excellent SQL fluencyProgramming (Python, Java, C#, PHP, etc.)Strong ETL proficiency using GUI-based tools or code-based patternsUnderstanding of data-modeling principlesExcellent verbal and written communicationBusiness acumenStrong problem-solving skillsAdaptability and flexibility in changing situationsPassion for delivering compelling solutions that exceed client expectations', 'Excellent verbal and written communication', 'Experience with software engineering practices', 'Snowflake / Amazon Redshift / Google BigQuery / Azure Synapse ', 'Tackle diverse projects that range in duration from a few days to a few months for clients ranging from local businesses to the Fortune 500Work with disparate data sources (relational databases, flat files, Excel, HDFS/Big Data systems, high-performance analytical databases, etc.) to unify client dataCollaborate closely with users to understand their unique needs and support them with the best solutionsSolve data-acquisition, integration and management problemsCreate ETL processes based on client needs while managing client expectations', 'Must-Haves', 'Excellent SQL fluency', 'Passion for delivering compelling solutions that exceed client expectations', 'Solve data-acquisition, integration and management problems', 'Work with disparate data sources (relational databases, flat files, Excel, HDFS/Big Data systems, high-performance analytical databases, etc.) to unify client data', 'Business acumen', 'Matillion, Fivetran, Airflow, DBT or other ETL tools', 'Collaborate closely with users to understand their unique needs and support them with the best solutions', 'Why InterWorks', 'What You’ll Need']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-02-16 15:27:39
Data Engineer,Airtable,"San Francisco, CA",15 hours ago,Be among the first 25 applicants,"['', ""You're passionate and thoughtful about building systems that enhance human understanding."", 'You communicate with clarity and precision in written form; experience communicating with graphs and plots, or at least understanding how to enable other people to do this, is a big bonus.', 'Help architect, build, and scale our initial data engineering platform, with an eye on security and privacy.', 'Who You Are', ""What You'll Do"", ""Work on the data collection pipeline across the entire stack, from client-side event logging to ETL. Systems you'll touch may include data warehousing using MPP databases (e.g. RedShift or Vertica), workflow systems (e.g. Airflow or Luigi), streaming data processors (Kinesis, Kafka, etc.), and distributed data processing systems (Spark, Hadoop, etc.)."", "" Help architect, build, and scale our initial data engineering platform, with an eye on security and privacy. Work closely with the rest of engineering, as well as other stakeholders from our growth, sales, marketing, and product teams, to understand the data needs of the business and produce systems that enable better product and growth decision-making. Work on the data collection pipeline across the entire stack, from client-side event logging to ETL. Systems you'll touch may include data warehousing using MPP databases (e.g. RedShift or Vertica), workflow systems (e.g. Airflow or Luigi), streaming data processors (Kinesis, Kafka, etc.), and distributed data processing systems (Spark, Hadoop, etc.). Ensure that our business-critical data is accurate and correct. "", ""You have professional experience working with modern data storage and processing technologies, and you've wrangled enough data to understand how often the complex systems that produce data can go wrong."", 'You can write clear, correct code in at least one programming language, and are willing to become effective in others as needed to get your job done.', 'You may have experience administering modern large-scale data management systems such as ELK.', "" You're passionate and thoughtful about building systems that enhance human understanding. You have professional experience working with modern data storage and processing technologies, and you've wrangled enough data to understand how often the complex systems that produce data can go wrong. You can write clear, correct code in at least one programming language, and are willing to become effective in others as needed to get your job done. You communicate with clarity and precision in written form; experience communicating with graphs and plots, or at least understanding how to enable other people to do this, is a big bonus. You may have experience administering modern large-scale data management systems such as ELK."", 'Work closely with the rest of engineering, as well as other stakeholders from our growth, sales, marketing, and product teams, to understand the data needs of the business and produce systems that enable better product and growth decision-making.', 'Ensure that our business-critical data is accurate and correct.']",Associate,Full-time,Engineering,Computer Software,2021-02-16 15:27:39
Data Engineer,Amazon Web Services (AWS),"Seattle, WA",6 hours ago,Be among the first 25 applicants,"['', ' Knowledge of Data Management fundamentals and Data Storage principles.', "" Master's degree in Computer Science, Engineering, Mathematics, or a related technical discipline Experience building on AWS using S3, EC2, Redshift, DynamoDB, Lambda, etc. Experience with ETL optimization, designing, coding, and tuning big data processes using Apache Spark or similar technologies. Experience with- building data pipelines and applications to stream and process datasets at low latencies. Passion for efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data. Knowledge of Engineering and Operational Excellence using standard methodologies. Good communication skills and ability to work effectively on shared projects with other developers"", ' Knowledge of Engineering and Operational Excellence using standard methodologies.', 'Responsibilities For This Role Will Include', ' Design, implement, and operate stable, scalable, low cost solutions to flow data from production systems into the data warehouse and into end-user facing applications', ' 4+ years of industry experience in Software Development, Data Engineering, Business Intelligence, or related field with a track record of manipulating, processing, and extracting value from large datasets.', ' Coding proficiency in at least one modern programming language (Python, Perl, Java, etc)', "" Master's degree in Computer Science, Engineering, Mathematics, or a related technical discipline"", "" Bachelor's degree in Computer Science, Engineering, Mathematics, or a related technical discipline. 4+ years of industry experience in Software Development, Data Engineering, Business Intelligence, or related field with a track record of manipulating, processing, and extracting value from large datasets. Expert-level proficiency in writing complex, highly-optimized SQL queries across large datasets. Experience in Data Modeling, ETL Development, and Data Warehousing. Experience using big data technologies (Hadoop, Hive, Hbase, Spark, EMR, etc.). Coding proficiency in at least one modern programming language (Python, Perl, Java, etc) Knowledge of Data Management fundamentals and Data Storage principles."", 'Description', 'Preferred Qualifications', ' Experience in Data Modeling, ETL Development, and Data Warehousing.', "" Bachelor's degree in Computer Science, Engineering, Mathematics, or a related technical discipline."", 'Company', ' Experience with ETL optimization, designing, coding, and tuning big data processes using Apache Spark or similar technologies.', ' Experience building on AWS using S3, EC2, Redshift, DynamoDB, Lambda, etc.', ' Good communication skills and ability to work effectively on shared projects with other developers', ' Translate data into actionable insights for our stakeholders', ' Expert-level proficiency in writing complex, highly-optimized SQL queries across large datasets.', ' Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using SQL and AWS analytics services', ' Experience using big data technologies (Hadoop, Hive, Hbase, Spark, EMR, etc.).', ' Passion for efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.', ' Explore and learn the latest AWS technologies to provide new capabilities and increase efficiency', ' Experience with- building data pipelines and applications to stream and process datasets at low latencies.', ' Design, implement, and operate stable, scalable, low cost solutions to flow data from production systems into the data warehouse and into end-user facing applications Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using SQL and AWS analytics services Explore and learn the latest AWS technologies to provide new capabilities and increase efficiency Translate data into actionable insights for our stakeholders', 'Basic Qualifications']",Not Applicable,Full-time,Strategy/Planning,Computer Software,2021-02-16 15:27:39
Data Engineer,Jones Grove IT Recruiting,"Charlotte, NC",,N/A,"['', '*Strong SQL development experience', 'This is a full time, permanent role that must be located in Charlotte, NC.', 'Charlotte, NC', 'Full time role', 'Jones Grove brings over 25 years of dedicated Information Technology recruiting to the marketplace and that has led us to place virtually every function within an IT organization.', 'Our client is a large, international manufacturing company that offers an excellent team work environment with lots of growth opportunities.', 'The Data Engineer MUST have these items:', '*Object Oriented programming, Python preferred but others including Java or C# are fine.', '*Strong experience with Cloud environments like Azure or Amazon Web Services', 'About Jones Grove', 'Please reach out for a full job description.', 'We are an equal opportunity employer and value diversity at our company. We do not discriminate based on race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.', 'Data Engineer']",Mid-Senior level,Full-time,Information Technology,Staffing and Recruiting,2021-02-16 15:27:39
Data Engineer,Alcon,"Commercial, NJ",29 minutes ago,Be among the first 25 applicants,"['', 'Deliver business solution on Alcon’s analytics platform through end-to-end implementation that includes data security, governance, cataloging, preparation, automated testing, and data quality metrics', 'Enterprise Alcon Data Lake platform supporting multiple data initiatives & projects with core Data Management, Big Data Storage, Traditional Reporting and Machine Learning capabilities.Leading edge cloud technology positioning Alcon to lead in Data and Analytics.Data Lake, Data Science Tools, Visualization, ML leveraging Amazon Web Services Capabilities.Data Source for 3rd Party Data, Reference Data and Data Integrations.Common Master Data Lake, Data Catalog and Definitions.Opportunity to work in a high paced agile environment and the chance to work with people who are passionate of delivering outcomes and a culture of simplification and ownership. Opportunity to work in a high paced agile environment and the chance to work with people who are passionate of delivering outcomes and a culture of simplification and ownership. Possibility to influence the design of the solution responding to business needs to enhance customer experienceCollaboration and frequent direct interactions with business stakeholdersTruly international environment and daily interactions with colleagues from all over the world Attractive benefits package (private medical care, group insurance, lunch card, transportation allowance, pension plan, Multisport/cultural card, free Alcon products for you and your significant other Permanent employment contract, attractive base salary and annual bonus Flexible hours and remote work possibilities Brand new office in Marynarska 15, Warsaw, with a lot of facilities inside ', 'As part of the Analytics sprint team before data engineering to ingest data from various sources into data lake solution on AWS Cloud as per the roadmapDeliver business solution on Alcon’s analytics platform through end-to-end implementation that includes data security, governance, cataloging, preparation, automated testing, and data quality metricsContribute towards building high performing platform/product DevOps agile teamsAutomate, optimize, migrate and enhance existing solutionsPerform data modeling, data analysis and providing insights using various tools', 'Truly international environment and daily interactions with colleagues from all over the world ', 'Brand new office in Marynarska 15, Warsaw, with a lot of facilities inside ', 'Applicable experience in Data Warehousing and BI Solutions.Good understanding of traditional and latest data and analytics platforms architecture models', 'Familiarity with Visualization and Reporting Tools like Tableau, Salesforce Einstein Analytics and QlikSense', 'Knowledge in AWS Services like Glue, EMR, S3, SNS, SQS, Athena, Redshift, Lamda, and Step functionsExperience with perform data modeling, data analysis and providing insights using various tools', 'Opportunity to work in a high paced agile environment and the chance to work with people who are passionate of delivering outcomes and a culture of simplification and ownership. Opportunity to work in a high paced agile environment and the chance to work with people who are passionate of delivering outcomes and a culture of simplification and ownership. Possibility to influence the design of the solution responding to business needs to enhance customer experience', 'Collaboration and frequent direct interactions with business stakeholders', 'Fluency in English is necessary', 'Skills For Success', 'Experience in languages to ingest data is a must – SQL, Java, Python or Scala', 'Data Lake, Data Science Tools, Visualization, ML leveraging Amazon Web Services Capabilities.', 'Data Source for 3rd Party Data, Reference Data and Data Integrations.', 'Alcon', 'Contribute towards building high performing platform/product DevOps agile teamsAutomate, optimize, migrate and enhance existing solutions', 'Join Alcon and help the world See Brilliantly! ', 'As part of the Analytics sprint team before data engineering to ingest data from various sources into data lake solution on AWS Cloud as per the roadmap', 'Leading edge cloud technology positioning Alcon to lead in Data and Analytics.', 'Experience in data engineering which includes data ingestion, preparation, curation, provisioning, automated testing, and quality checks.Big Data cloud platforms, Data Lakes, and Data Warehouses', 'Applicable experience in Data Warehousing and BI Solutions.Good understanding of traditional and latest data and analytics platforms architecture modelsExperience in data engineering which includes data ingestion, preparation, curation, provisioning, automated testing, and quality checks.Big Data cloud platforms, Data Lakes, and Data WarehousesFamiliarity with Visualization and Reporting Tools like Tableau, Salesforce Einstein Analytics and QlikSenseExperience in languages to ingest data is a must – SQL, Java, Python or ScalaKnowledge in AWS Services like Glue, EMR, S3, SNS, SQS, Athena, Redshift, Lamda, and Step functionsExperience with perform data modeling, data analysis and providing insights using various toolsStrong collaboration skills for effective communication across multiple teams and stakeholders, both internal and externalFluency in English is necessary', 'Perform data modeling, data analysis and providing insights using various tools', 'Enterprise Alcon Data Lake platform supporting multiple data initiatives & projects with core Data Management, Big Data Storage, Traditional Reporting and Machine Learning capabilities.', 'Strong collaboration skills for effective communication across multiple teams and stakeholders, both internal and external', 'Common Master Data Lake, Data Catalog and Definitions.', 'What We Can Offer You', 'Permanent employment contract, attractive base salary and annual bonus ', 'How You Will Make a Difference', 'Flexible hours and remote work possibilities ', 'Attractive benefits package (private medical care, group insurance, lunch card, transportation allowance, pension plan, Multisport/cultural card, free Alcon products for you and your significant other ']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-02-16 15:27:39
Data Engineer,BetterUp,"San Francisco, CA",8 hours ago,Over 200 applicants,"['', ' Data Accessibility: Collected data should be accessible, or appropriately inaccessible, to all services, products, and people of the organization. Data Approachability: Data should be intuitively familiar to the observer. Data Awareness: Cultivate a community of data-conscious practitioners. ', 'Data Protector: Design and build reliable, scalable data infrastructure with leading privacy and security techniques to safe guard data.', 'Experience with creating high-quality, fast services and projects in Python. ', '5 Volunteer Days to give back', '4 BetterUp Inner Work days (https://www.betterup.co/inner-work)', 'Data Approachability: ', ' Data Unifier: Architect, assemble, assimilate, clean, and conform large, complex datasets to deliver business insights and power product experiences. Data Advocate: Weave data into decision-making and drive cross-functional data-oriented approaches and solutions. Data Protector: Design and build reliable, scalable data infrastructure with leading privacy and security techniques to safe guard data. Data Builder: Own the end-to-end data stack including event collection, data governance, data integrations, and modeling.  Data Custodian: Ensure consistency and quality through metrics, documentation, processes, data testing, and training. ', 'Responsibilities', 'Data Approachability: Data should be intuitively familiar to the observer.', 'Data Custodian: Ensure consistency and quality through metrics, documentation, processes, data testing, and training.', 'Access to BetterUp coaching; one for you and one for a friend or family member ', '13 paid holidays ', ' Access to BetterUp coaching; one for you and one for a friend or family member  A competitive compensation plan with opportunity for advancement Full coverage for medical, dental and vision insurance Employer Paid Life, AD&D, STD and LTD insurance Flexible paid time off Per year: ', 'Experience with modern business intelligence and product reporting tools (e.g. Mode, Looker, Periscope).', 'Employer Paid Life, AD&D, STD and LTD insurance', 'Data Unifier:', 'Data Accessibility: Collected data should be accessible, or appropriately inaccessible, to all services, products, and people of the organization.', 'Benefits', ' Experience with analytic databases (e.g. Snowflake). Advanced knowledge of SQL and experience with relational databases. Hands-on experience developing data pipelines. We use (e.g. dbt, Airflow, Stitch Data / Singer specs) . Hands-on experience with event streams and stream processing (e.g. Kafka, Spark, Data Bricks, Segment). Decoupling transactional or source systems from business intelligence reporting (e.g. dimensional modeling). Experience with creating high-quality, fast services and projects in Python.  Experience with modern business intelligence and product reporting tools (e.g. Mode, Looker, Periscope). ', 'Holiday charitable contribution of your choice on behalf of BetterUp', 'Hands-on experience with event streams and stream processing (e.g. Kafka, Spark, Data Bricks, Segment).', 'Data Custodian:', 'Advanced knowledge of SQL and experience with relational databases.', 'Collected data should be accessible, or appropriately inaccessible, to all services, products, and people of the organization.', 'Learning and Development stipend', 'Data Builder:', 'Hands-on experience developing data pipelines. We use (e.g. dbt, Airflow, Stitch Data / Singer specs) .', 'Decoupling transactional or source systems from business intelligence reporting (e.g. dimensional modeling).', 'Data Accessibility: ', 'Data Awareness: ', 'Data Awareness: Cultivate a community of data-conscious practitioners.', 'Experience with analytic databases (e.g. Snowflake).', 'Data Advocate:', 'Data Advocate: Weave data into decision-making and drive cross-functional data-oriented approaches and solutions.', 'Cultivate a community of data-conscious practitioners.', 'Data Protector:', 'If you have some or all of the following please apply:', '401(k) self contribution', 'Data should be intuitively familiar to the observer.', 'Flexible paid time off', 'Data Unifier: Architect, assemble, assimilate, clean, and conform large, complex datasets to deliver business insights and power product experiences.', 'Full coverage for medical, dental and vision insurance', 'Per year: ', ' 13 paid holidays  4 BetterUp Inner Work days (https://www.betterup.co/inner-work) 5 Volunteer Days to give back Learning and Development stipend ', 'Data Builder: Own the end-to-end data stack including event collection, data governance, data integrations, and modeling. ', 'A competitive compensation plan with opportunity for advancement']",Mid-Senior level,Full-time,Engineering,Professional Training & Coaching,2021-02-16 15:27:39
Data Engineer I,AbleTo Inc.,"New York, NY",11 hours ago,Be among the first 25 applicants,"['', ' and ', 'Follow AbleTo on ', 'Translate business requirements into actionable data tasks.', ' Build and maintain batch and real-time ETL pipelines in a Google Cloud Platform architecture (BigQuery, Dataproc, Firestore, etc.) Identify code quality issues and implement tests to improve future processes. Implement data integrity tests to ensure we are ingesting accurate data. Translate business requirements into actionable data tasks. Partner with business users to understand their needs, come up with end to end solutions, and communicate the results back to the users. Implement high-quality test-driven code. ', 'Implement data integrity tests to ensure we are ingesting accurate data.', 'Identify code quality issues and implement tests to improve future processes.', 'Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice. 1+ years experience coding in Python. Experience working with structured and NOSQL databases. Familiarity with structuring and writing ETLs. Experience working with Airflow and Bigquery is a big plus. Experience working with a multitude of stakeholders is a big plus. Follow AbleTo on LinkedIn and Twitter!AbleTo is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or veteran status. AbleTo is an E-Verify company.', '1+ years experience coding in Python.', 'Experience working with a multitude of stakeholders is a big plus.', 'Partner with business users to understand their needs, come up with end to end solutions, and communicate the results back to the users.', 'Familiarity with structuring and writing ETLs.', ' 1+ years experience coding in Python. Experience working with structured and NOSQL databases. Familiarity with structuring and writing ETLs. Experience working with Airflow and Bigquery is a big plus. Experience working with a multitude of stakeholders is a big plus. ', 'Experience working with Airflow and Bigquery is a big plus.', 'Twitter', 'Build and maintain batch and real-time ETL pipelines in a Google Cloud Platform architecture (BigQuery, Dataproc, Firestore, etc.)', 'Implement high-quality test-driven code.', '!', 'LinkedIn', 'Experience working with structured and NOSQL databases.']",Entry level,Full-time,Information Technology,Computer Software,2021-02-16 15:27:39
Data Engineer,Twistle,"Seattle, WA",3 hours ago,Be among the first 25 applicants,"['', 'BS/MS degree in Computer Science, Engineering, and/or related Healthcare experience.', 'Hands on experience with SQL, developing stored procedures, functions, views and triggers, while validating and analyzing data integrity. ', 'Build analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition.', 'Identify, design, and implement process improvements related to (data mapping, optimizing data delivery, and scalability of transformations while automating manual processes.', 'Strong working knowledge of tools like JIRA, Asana, Confluence, and Tettra.', 'Strong working knowledge of AWS services including Redshift, RDS, EMR and EC2.', 'Build required infrastructure for optimal extraction, transformation and loading of data from various data sources using AWS and SQL technologies.', 'Maintain both test and production library of interfaces, applying appropriate methods, procedures, and safeguards to protect the integrity of interfaces, ensuring their recoverability.', 'Automate and monitor business critical applications and troubleshoot or escalate appropriately when issues arise.Participate in code reviews with languages like LookerML, Python, Django, JavaScriptCollaborate with other engineers, implementation, customer success managers throughout the development process to release functional, performant and secure data on a regular basis.', 'Demonstrate experience working with various payloads including (JSON, XML APIs, Web Services, ETL, and File Transfers', 'Automate and monitor business critical applications and troubleshoot or escalate appropriately when issues arise.', 'Monitor, triage, and modify configuration for integrated healthcare messaging within Twistle Platform.', 'Participate in code reviews with languages like LookerML, Python, Django, JavaScript', 'Strong knowledge of and experience with reporting software such as Looker, BusinessObjects, Power BI, Tableau, etc.', 'Familiarity with one or more of the following development languages: Python, C#, Java.', '5+ years’ experience in healthcare integrations, with at least 2+ years in Cloud applications.', 'Experience working in an Agile/Scrum development process.', '3 years of experience in working in multi-tenant SaaS applications and services.Experience working in an Agile/Scrum development process.', '3 years of experience in working in multi-tenant SaaS applications and services.', ' Responsibilities ', 'Collaborate with other engineers, implementation, customer success managers throughout the development process to release functional, performant and secure data on a regular basis.', 'Strong experience demonstrating and understanding tools like Kafka, Spark and Hadoop; relational NoSQL and SQL databases including Cassandra and PostgreSQL. ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-02-16 15:27:39
Data Engineer,EPITEC,"Dearborn, MI",2 hours ago,Be among the first 25 applicants,"['', '\xa0TEST: Participate in testing.', 'DA/DBA: Data Architecture (Data Design, Index Design, Referential Integrity)', 'DA/DBA: Data Architecture (Data Design, Index Design, Referential Integrity)\xa0DA/DBA: DB Create/Modify (DB Create/Modify tables, DDL export/import, Data export/import)DA/DBA: Performance Tuning (DB-side identification of issues, SQL tuning, DB tuning)Data Manipulation - Hadoop Hive/HDFSData Manipulation - SQL', 'DATA LANDING: Perform Extract, Transform and Load (ETL) or variations of this activity.', '\xa0DESIGN: Design data stores. . IMPLEMENT DATA STORES: Implement data stores (Hadoop, relational DBs, noSQL DBs, etc.).', 'REQUIREMENTS MANAGEMENT: Identify, document, communicate and design per requirements.', 'Data Manipulation - SQL', 'REQUIREMENTS MANAGEMENT: Identify, document, communicate and design per requirements.\xa0DESIGN: Design data stores. . IMPLEMENT DATA STORES: Implement data stores (Hadoop, relational DBs, noSQL DBs, etc.).DATA LANDING: Perform Extract, Transform and Load (ETL) or variations of this activity.\xa0TEST: Participate in testing.\xa0TUNE: Tune data stores (indexes, SQL queries) to improve performance.\xa0L2 SUPPORT: Assist with customer inquiries and incidents/problems.', 'In this role, you will collaborate with multiple organizations (Business Customers, Product Owners, Data Tech, IT) to convert business goals into data storage solutions. This role will work in small, cross functional teams and embrace lean and agile practices, software best practices, software quality scanning, automated testing, and CI/CD', '\xa0DA/DBA: DB Create/Modify (DB Create/Modify tables, DDL export/import, Data export/import)', '\xa0L2 SUPPORT: Assist with customer inquiries and incidents/problems.', 'DA/DBA: Performance Tuning (DB-side identification of issues, SQL tuning, DB tuning)', 'Data Manipulation - Hadoop Hive/HDFS', '\xa0TUNE: Tune data stores (indexes, SQL queries) to improve performance.']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-02-16 15:27:39
Data Engineer,Entidad,United States,24 hours ago,36 applicants,"['Co-create and own a strong technical vision, strategy, and roadmap for impactful data warehousing and resulting data products.\xa0Build and maintain strong relationships with your collaborators, including fellow Engineering teams, the Product teams,\xa0and\xa0Data Analytics teams within\xa0client organizations.\xa0Create an engaging\xa0Data Management\xa0work environment by focusing on the most impactful work, creating growth opportunities, modeling transparency, and high-quality communication.\xa0Use JIRA and other tools to effectively identify, track, and communicate priorities both for day-to-day and long-term planning.\xa0Rapidly deliver value to our stakeholders,\xa0clients, and ultimately\xa0farm workers.\xa0Triage issues effectively and use that information to help guide the work of the team to make our data products more reliable and maintainable.\xa0Define and maintain standards for the team’s infrastructure, code, and development processes.\xa0', 'A short cover\xa0letter\xa0', '4+ years of data modeling and SQL experience.\xa0\xa0', 'Data Engineering Role:\xa0', 'Culturally Aligned\xa0', 'Technology can help us address this. To that end,\xa0Entidad\xa0has been working with leading nonprofit organizations that have decades of experience providing valuable services to farm workers. Together, we are designing digital experiences that are culturally aligned, inspire trust, and provide hard data to better inform ongoing advocacy efforts.\xa0', 'Please Provide:\xa0', 'Build and maintain strong relationships with your collaborators, including fellow Engineering teams, the Product teams,\xa0and\xa0Data Analytics teams within\xa0client organizations.\xa0', 'About Us:\xa0', 'Build Trust\xa0', 'Experience with\xa0Snowflake Data Cloud\xa0and PostgreSQL.\xa0', 'Data Engineer\xa0', '2+ years of experience\xa0building\xa0data pipelines.\xa0', 'Applicant\xa0Should\xa0Have:\xa0', 'Triage issues effectively and use that information to help guide the work of the team to make our data products more reliable and maintainable.\xa0', 'In the coming year we will be growing our team to support the growth of a services ecosystem capable of delivering social and safety net services to farm worker communities across the country. We need people who, like us, share a passion for building technology-based solutions that can bring real impactful change to these underserved populations.\xa0', 'Rapidly deliver value to our stakeholders,\xa0clients, and ultimately\xa0farm workers.\xa0', 'Our\xa0Values:', 'Build Trust\xa0Humility\xa0Culturally Aligned\xa0Farm Workers First\xa0', 'This is a 100% remote role.\xa0', 'Use JIRA and other tools to effectively identify, track, and communicate priorities both for day-to-day and long-term planning.\xa0', 'Entidad\xa0is hiring for an avid data wrangler with proven data modeling,\xa0data warehousing\xa0and data pipeline\xa0experience. We are looking for a\xa0technology practitioner to lead and grow our\xa0Data\xa0Engineering team. This exciting role is a partner with our product team in working with\xa0our collaborators to build\xa0high-impact\xa0data products to support digital services and business operations. You will work closely with the product team to create a strong technical vision, supporting strategy, and roadmap for a data warehouse and supporting\xa0data pipelines\xa0which\xa0is\xa0well defined, scalable, maintainable, well tested, and delights our stakeholders. And you will\xa0manage a\xa0team of data engineers\xa0to ensure that they have the right information, skills, and support.\xa0', '\xa0', 'A resume and/or LinkedIn profile\u202f\xa0A short cover\xa0letter\xa0', 'Applicant\xa0Should\xa0Have:', 'Entidad\xa0is a technology services provider developing innovative solutions to help improve the quality of life for America’s farm workers. Founded in 2018 as a California public benefit corporation,\xa0Entidad\xa0has been working closely with leading farm worker serving organizations, such as the UFW Foundation and United Farm Workers of America, to digitally transform and scale their impact on rural immigrant communities across the US.\xa0', 'Enjoy working with a diverse group of people with different expertise.\xa0', 'Enjoy building and improving cloud-native data pipelines and warehouses.\xa0', 'Define and maintain standards for the team’s infrastructure, code, and development processes.\xa0', 'A\xa0degree in computer science or related field.\xa0', 'About Us:', 'Entidad’s\xa0vision is to build technology around the needs and advancement of the farm worker. We are developing\xa0data-driven\xa0solutions, based on a microservices architecture, that enables the virtual delivery of safety net services to farm worker communities including immigration and pandemic relief aid.\xa0', 'Humility\xa0', 'Experience with cloud native ETL solutions such as AWS Data Pipeline and/or AWS Glue\xa0', 'Co-create and own a strong technical vision, strategy, and roadmap for impactful data warehousing and resulting data products.\xa0', 'Experience building event streams with Kafka or AWS Kinesis\xa0', 'A resume and/or LinkedIn profile\u202f\xa0', '\xa0\xa0', 'The pandemic has brought into focus our deep dependency on farm worker communities for our nation’s food security. We deem our agricultural workers as essential to our economy, yet we exclude them from federal labor laws that provide the most basic worker rights. This has resulted in an unsustainable labor market that keeps workers marginalized and our food supply vulnerable. We believe that improving the state of farm workers creates a positive effect that secures the food we eat, the industry that produces it, and the communities where they live.\xa0', 'Nice to Have, But Not Required:\xa0', 'Our\xa0Values:\xa0', 'Create an engaging\xa0Data Management\xa0work environment by focusing on the most impactful work, creating growth opportunities, modeling transparency, and high-quality communication.\xa0', '2+\xa0years of experience\xa0leading members of a data engineering team.\xa0', 'Experience with NoSQL data stores such as DynamoDB, MongoDB or Redis\xa0', 'Much work remains to understand how to digitally engage farm workers; there are a lot of unknowns. We seek individuals that are up for the challenge and will roll up their sleeves to help us uncover and unlock solutions.\xa0Ideal candidates are self-driven individuals who can excel in a dynamic environment with minimal guidance. We foster an environment that values creativity, collaboration, and the mutual learning that comes from bringing together individuals with diverse backgrounds and perspectives. If this sounds like you, please reach out.\xa0', 'What Success Looks Like:\xa0', 'You understand the lives of farm workers through personal connection or have previously worked with farm workers.\xa0\xa0\xa0', 'Please Provide:', 'A\xa0passion for doing mission-oriented work.\xa0\xa0', 'Nice to Have, But Not Required:', 'Data Engineering Role:\xa0\xa0', 'A\xa0passion for doing mission-oriented work.\xa0\xa0Enjoy working with a diverse group of people with different expertise.\xa0Enjoy building and improving cloud-native data pipelines and warehouses.\xa0A\xa0process-oriented mindset and enjoy writing and debating design documents.\xa04+ years of data modeling and SQL experience.\xa0\xa02+\xa0years of experience\xa0leading members of a data engineering team.\xa02+ years of experience\xa0building\xa0data pipelines.\xa0', 'You must love data engineering and supporting data engineers. You love improving data engineering processes to enhance confidence in data. You enjoy sharing your knowledge and approach with diverse stakeholders (some technical, some not) and act as an ambassador for data engineering work. And you have a great sense for how to balance prompt delivery with building strong, resilient, and maintainable data products.\xa0Your role is critical in helping support essential worker communities who are among the hardest hit by COVID-19 by helping create new channels and methods of delivering much needed aid.\xa0', 'What Success Looks Like:', 'Farm Workers First\xa0', 'A\xa0process-oriented mindset and enjoy writing and debating design documents.\xa0', 'Experience with\xa0Snowflake Data Cloud\xa0and PostgreSQL.\xa0Experience with NoSQL data stores such as DynamoDB, MongoDB or Redis\xa0Experience building event streams with Kafka or AWS Kinesis\xa0Experience with cloud native ETL solutions such as AWS Data Pipeline and/or AWS Glue\xa0Experience\xa0with data compliance such as PII, PCI, CCPA and GDPR\xa0You understand the lives of farm workers through personal connection or have previously worked with farm workers.\xa0\xa0\xa0A\xa0degree in computer science or related field.\xa0', 'Experience\xa0with data compliance such as PII, PCI, CCPA and GDPR\xa0', 'Data Engineer', 'Entidad\xa0values diversity as a core tenet of the work we do and the\xa0communities\xa0we serve. We are an equal opportunity employer, indiscriminate of race, religion, ethnicity, national origin, citizenship, gender, gender identity, sexual orientation, age, veteran status, disability, genetic information, or any other protected characteristic\xa0']",Entry level,Full-time,Information Technology,Internet,2021-02-16 15:27:39
Data Engineer,mParticle,United States,7 hours ago,46 applicants,"['', 'Able to design and develop quality cloud-based systems and operate them in an automated fashion', 'About mParticle', '1+ years of proven success working in backend of large-scale software developmentBS/MS in Computer Science or related fieldExpertise in SQL-like languages and toolsAbility to learn quickly and display solid analytical/engineering thinkingExperience in building scalable and distributed data pipelines for analytics processes and/or training machine learning modelsAble to design and develop quality cloud-based systems and operate them in an automated fashionDemonstrable experience in taking projects from spec to releaseWorking knowledge of Druid, Fivetran, Redshift, Looker, Spark, Luigi/Airflow, etc..', 'Experience in building scalable and distributed data pipelines for analytics processes and/or training machine learning models', 'Responsibilities', 'Working knowledge of Druid, Fivetran, Redshift, Looker, Spark, Luigi/Airflow, etc..', 'Founded in 2013, mParticle is the leading customer data platform that unlocks the full power of data for businesses. The company empowers brands to accelerate their growth strategy to keep pace with their customers by providing the most advanced data platform for web and apps across all devices in the marketplace. A trusted partner among renowned brands such as Airbnb, Foursquare, Hulu, King, and Spotify among many others, the mParticle platform has grown to manage over 1 billion mobile users each month, capturing over $5 billion in ecommerce transactions and processes over 250 billion API calls. Recognized as one of Crain’s 100 Best Places to Work in New York City and named to Gartner’s “Cool Vendors in Mobile App Development” list, mParticle has 45 employees and is headquartered in New York City with offices in San Francisco, Florida, Seattle and London.', '\ufeffDesired Experience', 'Build automated alerting to improve efficiency of our team’s operations, including but not limited to time series forecasting, anomaly detection, text classifications, etc.', 'Here at mParticle we embrace the differences that make us unique. We are dedicated to building an inclusive environment that fosters respect and celebrates an array of backgrounds and perspectives.', 'Here at mParticle, everyone is equal. We\xa0believe strongly in our values\xa0and are looking for someone who demonstrates empathy and sincerity to all roles and teammates. Our clients include marketing and engineering functions for some of the largest apps in the world and our platform processes nearly one-third of the world’s smartphone traffic.', 'Employment opportunities are available to all applicants without regard to race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.', 'BS/MS in Computer Science or related field', 'Build, maintain, and document automated ETL pipelines', 'Work with business stakeholders and product managers to define product requirements', 'Ability to learn quickly and display solid analytical/engineering thinking', ""We're looking for a talented and technically well-rounded person who loves to tackle complex problems and is passionate about building scalable and reliable data pipelines and applications, e.g., BI reporting, data transformations/integrations, machine learning, etc."", 'Architect, implement, and support scalable/reliable data pipelines and data applications in a cost efficient way', '*At this time, mParticle is unable to sponsor visas for this role, unfortunately.', 'Demonstrable experience in taking projects from spec to release', 'Continuously monitor and optimize the pipelines and data schemas', 'Work with business stakeholders and product managers to define product requirementsArchitect, implement, and support scalable/reliable data pipelines and data applications in a cost efficient wayBuild, maintain, and document automated ETL pipelinesContinuously monitor and optimize the pipelines and data schemasBuild automated alerting to improve efficiency of our team’s operations, including but not limited to time series forecasting, anomaly detection, text classifications, etc.', '1+ years of proven success working in backend of large-scale software development', 'We are seeking someone who wants to be a contributor in a small, dynamic work environment, loves a challenge, and has a strong balance of technical and people skills. As a Data Engineer, you will be a member of our backend engineering team and collaborate to help specify, design, and develop data pipelines/applications meeting company and product requirements. You will help evaluate and choose service protocols and architectures, write high quality, maintainable code in a fast-paced startup environment with tight schedules, and be fully responsible for ensuring quality and proper deployment of the written software.', 'Expertise in SQL-like languages and tools']",Mid-Senior level,Full-time,Engineering,Internet,2021-02-16 15:27:39
Data Engineer,Strathmore Technology Recruitment,United States,21 hours ago,35 applicants,"['', ""Bachelor's degree in Computer Science (or related field)5+ years of relevant work experienceExpertise in Data warehousing, ETL, DBA, Data modelingExperience working with analytics tools - Tableau or LookerStrong Python and SQLAbility to multi-task, organize, and prioritize work"", 'Ability to multi-task, organize, and prioritize work', 'Apply Now!', 'Strong Python and SQL', 'Experience working with analytics tools - Tableau or Looker', ""Bachelor's degree in Computer Science (or related field)"", '5+ years of relevant work experience', 'Expertise in Data warehousing, ETL, DBA, Data modeling', 'Qualifications', 'Strathmore is working with an ambitious start-up who are creating a complex software platform to assist the life sciences sector. The ideal candidate is a self-motivated, multi-tasker, and demonstrated team-player who can own an analytics platform that will influence the experience of millions of people around the world.']",Mid-Senior level,Full-time,Information Technology,Computer Software,2021-02-16 15:27:39
Data Engineer,General Mills,"Minneapolis, MN",5 hours ago,Be among the first 25 applicants,"['', 'Generate and implement your own ideas on how to improve the operational and strategic health of big data ecosystem', 'Ability to work in a team environment', 'Ability to research, plan, organize, lead, and implement new processes or technology', 'Database development experience using Oracle, SQL Server, SAP BW or SAP HANA', 'Bachelor’s Degree; Computer Science, MIS, or Engineering preferredMinimum 2 years of IT experience, 3+ preferredCloud data experienceStrong understanding of Hadoop fundamentalsDatabase development experience using Oracle, SQL Server, SAP BW or SAP HANAJob Scheduling experienceProcess mindset with experience creating, documenting and implementing standard processesDevelopment experience using Hive and/or SparkEffective verbal and written communication and influencing skills.Effective analytical and technical skills.Ability to work in a team environmentAbility to research, plan, organize, lead, and implement new processes or technology', 'Participate in the evaluation, implementation and deployment of emerging tools & process in the big data space.', 'Design, create, code, and support a variety of ETL solutions (potentially including but not limited to: Talend Studio, Python, Scala, Kafka, SAP Data Services, or others)', 'Job Scheduling experience', 'Familiarity with Kafka ', 'Collaboratively troubleshoot technical and performance issues in the big data ecosystem', 'Minimum 2 years of IT experience, 3+ preferredCloud data experience', 'Process mindset with experience creating, documenting and implementing standard processes', 'Overview', 'Bachelor’s Degree; Computer Science, MIS, or Engineering preferred', 'Development experience using Hive and/or Spark', 'Preferred Qualifications', 'Experience with agile techniques or methods', 'Effective analytical and technical skills.', 'Effective verbal and written communication and influencing skills.', 'Act as a key technical leader within General MillsDesign, create, code, and support a variety of ETL solutions (potentially including but not limited to: Talend Studio, Python, Scala, Kafka, SAP Data Services, or others)Generate and implement your own ideas on how to improve the operational and strategic health of big data ecosystemParticipate in the evaluation, implementation and deployment of emerging tools & process in the big data space.Partner with business analysts and solutions architects to deliver business initiatives.Collaboratively troubleshoot technical and performance issues in the big data ecosystem', 'Key Responsibilities', 'Python, Scala or Java development experienceFamiliarity with Kafka Familiarity with the Linux operating systemExperience with agile techniques or methods', 'Python, Scala or Java development experience', 'Familiarity with the Linux operating system', 'Partner with business analysts and solutions architects to deliver business initiatives.', 'Company Overview', 'Minimum Qualifications', 'Strong understanding of Hadoop fundamentals', 'Act as a key technical leader within General Mills']",Entry level,Full-time,Information Technology,Food & Beverages,2021-02-16 15:27:39
Data Engineer,"OM1, Inc.","Carolina, NC",9 hours ago,Be among the first 25 applicants,"['', 'Analyze and interpret data flows, attributes, and quality', 'Background in medical records and health insurance claims', 'Excellent SQL skills', 'Responsibilities', 'Experience with CI/CD tools such as Jenkins', 'Comfort working in an Agile/Scrum environment with continuous delivery', 'Solid understanding of software engineering principles', 'Integrate new data sources into our data asset', 'Strong programming skills, preferably Python, Scala, Java, or similar', 'Build, automate, and improve big-data pipelines', 'Automated unit and integration testing experience', 'Excellent attention to detail', 'Experience / Requirements', ' Familiarity with cloud-based platforms such as AWS, Databricks, and/or Snowflake DB Data processing experience with Python, Scala, Spark, and/or Airflow Automated unit and integration testing experience Experience with CI/CD tools such as Jenkins Background in medical records and health insurance claims Experience working on systems with strong security and privacy requirements', ' Strong programming skills, preferably Python, Scala, Java, or similar Excellent SQL skills Solid understanding of software engineering principles Comfort working in an Agile/Scrum environment with continuous delivery A strong desire to advance healthcare and improve patient outcomes Excellent attention to detail ', 'Code, test and deliver data-oriented product features', 'Experience working on systems with strong security and privacy requirements', 'Requirements', 'Craft scalable backend software and tools using modern software engineering practices', 'A strong desire to advance healthcare and improve patient outcomes', 'Familiarity with cloud-based platforms such as AWS, Databricks, and/or Snowflake DB', ' Build, automate, and improve big-data pipelines Code, test and deliver data-oriented product features Integrate new data sources into our data asset Model, enhance and enrich database schemas and underlying data Craft scalable backend software and tools using modern software engineering practices Analyze and interpret data flows, attributes, and quality ', 'Data processing experience with Python, Scala, Spark, and/or Airflow', 'Model, enhance and enrich database schemas and underlying data']",Entry level,Full-time,Information Technology,Pharmaceuticals,2021-02-16 15:27:39
Data Engineer,STAND 8 Technology Services,"Long Beach, CA",22 hours ago,46 applicants,"['', 'Job Description', ' Data Engineer', ' Knowledge of Office365 Ecosystem (Power Automate, SharePoint). PostgreSQL, preferably in an Azure-hosted environment', 'DESIRED QUALIFICATIONS/KNOWLEDGE', 'Knowledge of Office365 Ecosystem (Power Automate, SharePoint).', 'MANDATORY REQUIREMENTS ', 'Data wrangling/munging as needed.', 'Ability to develop automated data pipelines, preferably via Python.', 'DESIRED QUALIFICATIONS/KNOWLEDG', 'Building data pipeline to ingest semi-structured csv data into a single source of truth (database or flat file.)', ' Person will be working with PHI data, knowledge of appropriate data handling required.  Ability to develop automated data pipelines, preferably via Python. ', 'Person will be working with PHI data, knowledge of appropriate data handling required. ', 'PostgreSQL, preferably in an Azure-hosted environment', 'Data Engineer ', ' Building data pipeline to ingest semi-structured csv data into a single source of truth (database or flat file.) Data wrangling/munging as needed. ']",Entry level,Contract,Information Technology,Information Technology and Services,2021-02-16 15:27:39
Data Engineer,STAND 8 Technology Services,"Long Beach, CA",22 hours ago,46 applicants,[],Entry level,Contract,Information Technology,Information Technology and Services,2021-02-16 15:27:39
Data Engineer,Virtusa,San Francisco Bay Area,2 hours ago,Over 200 applicants,"['', 'Self-Motivated, Accountable to Drive the work.', 'Duration: 12 Months+', 'Build scalable data ingestion framework for new and existing applications.', 'Apache Spark, Apache Airflow, Python ', 'GitHub; Jenkins', 'Working Experience in Agile (Scrum, Pair Programming)', 'Duration: 6-12 months', 'Working Experience with collaboration tools (Jira, Confluence, ServiceNow)', 'Location: This position allows work from home until safe to return to work.', 'Build scalable data ingestion framework for new and existing applications.Build scalable data pipelines to extract, transform and load data into different systems.SQL Expert (with a good understanding of performance tuning)Apache Spark, Apache Airflow, Python AWS Lambda, AWS S3, AWS Redshift GitHub; JenkinsSOAP/REST API Any ETL Tool (Snaplogic Preferred)Strong knowledge of Data Warehouse and Data Modelling (Dimensional Data Model)Working Experience in Agile (Scrum, Pair Programming)Working Experience with collaboration tools (Jira, Confluence, ServiceNow)Experience with modern SaaS applications like Salesforce, Adobe, Marketo, Workday, ServiceNow.Self-Motivated, Accountable to Drive the work.', 'Build scalable data pipelines to extract, transform and load data into different systems.', 'Strong knowledge of Data Warehouse and Data Modelling (Dimensional Data Model)', 'SQL Expert', 'SOAP/REST API ', 'Any ETL Tool (Snaplogic Preferred)', 'Skills: Data Engineering, Programming languages, SQL, Spark SQL, Hive, AWS, Analytics', 'SQL Expert (with a good understanding of performance tuning)', 'Note: Prefer candidates that can work on W2/ FTE. Third-party candidates cannot be considered.', 'Location: Pleasanton CA (Remote)', 'Role: Data Engineer', 'Tier: 3/4', 'AWS Lambda, AWS S3, AWS Redshift ', 'Experience with modern SaaS applications like Salesforce, Adobe, Marketo, Workday, ServiceNow.', 'Data Engineer']",Mid-Senior level,Contract,Information Technology,Information Technology and Services,2021-02-16 15:27:39
Data Engineer,Baker Tilly US,"Milwaukee, WI",23 hours ago,Be among the first 25 applicants,"['', 'Have strong experience building out data warehouses.', 'Have hands on experience in Microsoft business intelligence technologies.Have at least 4 years of experience working within these technologies as well as other backend tech.Apply different data modeling techniques and functional knowledge to both your internal team and external partners.Exhibit responsibility and accountability towards quality completion of projects and consistently hitting project timelines.Strong verbal and written communication skills and are not ashamed to ask questions or raise concern on projects.Outstanding customer service skills following proper business requirements and human resources expectations.Disciplined to be able to work in a variety of business environments.Ability to travel potentially up to 50% of the time.Maintained a Bachelor’s degree in Computer Science, Engineering, Math, Information Technology, or other related discipline or 10 + years of commensurate experience.', 'You are very well versed in BI and data analytics, SQL, the MS Stack, Azure and other cloud services.You enjoy supporting a variety of industries and embedding yourself with client teams to work together to find a solution.You enjoy being face to face with clients, understand who the key stakeholders on projects are, and positively influence the business need behind the use of data.You are constantly looking to grow your education in technology and staying up to date with the latest trends.You are a team player that encourages collaboration and has an intrapreneurial mind.You enjoy sharing what you learned with the team and are willing to be a mentor to others.You love to learn and enjoy putting yourself out of your comfort zone and have done or at least entertained the idea of speaking at tech events.Enjoy building relationships with your colleagues through social activities and team outings supporting work-life balance.You have and are interested in maintaining different technical certifications.', 'Maintained a Bachelor’s degree in Computer Science, Engineering, Math, Information Technology, or other related discipline or 10 + years of commensurate experience.', 'Strong verbal and written communication skills and are not ashamed to ask questions or raise concern on projects.', 'You are very well versed in BI and data analytics, SQL, the MS Stack, Azure and other cloud services.', 'You will enjoy this role if you: ', 'Work to understand business processes and possible improvements across an array of industries.', 'Enjoy building relationships with your colleagues through social activities and team outings supporting work-life balance.', 'Baker Tilly\xa0has an incredible career opportunity for an Data Engineer to join our growing Digital Team.', 'You enjoy sharing what you learned with the team and are willing to be a mentor to others.', 'Have hands on experience in Microsoft business intelligence technologies.', 'You are constantly looking to grow your education in technology and staying up to date with the latest trends.', '\xa0', 'Successful candidates will have:', 'You enjoy supporting a variety of industries and embedding yourself with client teams to work together to find a solution.', 'Utilize your scoping talents to help identify more areas within the business that our team can successfully impact for future projects.', 'What you will do:', 'You enjoy being face to face with clients, understand who the key stakeholders on projects are, and positively influence the business need behind the use of data.', 'Apply different data modeling techniques and functional knowledge to both your internal team and external partners.', 'You will be responsible for working within an agile environment to aid in the delivery of a managed service defined by the Architect of Project Manager.', 'Outstanding customer service skills following proper business requirements and human resources expectations.', 'Ability to travel potentially up to 50% of the time.', 'Have at least 4 years of experience working within these technologies as well as other backend tech.', 'Exhibit responsibility and accountability towards quality completion of projects and consistently hitting project timelines.', 'You will be responsible for working within an agile environment to aid in the delivery of a managed service defined by the Architect of Project Manager.Have strong experience building out data warehouses.Lead or support the day to day sprint activities provided to you by your pod leader.Work to understand business processes and possible improvements across an array of industries.Utilize your scoping talents to help identify more areas within the business that our team can successfully impact for future projects.', 'Baker Tilly professionals on our Digital Team provide management consulting services to mid and large size companies. This division is focused specifically in BI and Data Analytics, Data Warehousing, Data Management, and Power BI or Tableau. All supporting the need to define the businesses strategy and bring light and understanding to the vast amounts of data that companies maintain.\xa0', 'You have and are interested in maintaining different technical certifications.', 'Disciplined to be able to work in a variety of business environments.', 'You love to learn and enjoy putting yourself out of your comfort zone and have done or at least entertained the idea of speaking at tech events.', 'You are a team player that encourages collaboration and has an intrapreneurial mind.', 'Lead or support the day to day sprint activities provided to you by your pod leader.']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-02-16 15:27:39
Data Engineer,wellio,"San Francisco, CA",15 hours ago,Be among the first 25 applicants,"['', 'About You', 'Basic Qualifications']",Associate,Full-time,Engineering,Computer Software,2021-02-16 15:27:39
Data Engineer,Shakti Solutions,United States,6 hours ago,33 applicants,"['', 'Good experience in Spark,S3.', 'Fax: (512) 727 5087', 'Phone: 503 278 8338', '\xa0', '4+ Years of experience as Data Engineer.', 'Good experience as Datadog/cloudwatch.', 'Thanks and Regards,', 'Responsibilities', 'Good experience in building ETL pipelines for our new Warehouse.', 'Good to have experience in Kafka', 'Shakti Group Inc', 'Email: asobhan@shaktisolutions.com', 'Shakti Solutions has been in the Information Technology industry since its inception in 1999, partnering with many of the fortune 1000 companies on cutting edge technologies in both public and private sectors.\xa0', 'Aju\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0', 'Good communication skills.', 'Good experience in RDBMS.']",Mid-Senior level,Full-time,Business Development,Information Technology and Services,2021-02-16 15:27:39
Data Engineer,Arcadia,United States,2 hours ago,Be among the first 25 applicants,"['', 'The expectations of the day to day of an engineer is as follows:', '\xa0-Developing a range of data pipelines with varying complexity', 'Works mostly independently on delivery w/decreasing involvement from engineering and more senior team members', 'Amazing benefits including unlimited FTO', 'SQL: 2-4 year (Preferred)', 'Team Projects:', 'Experience Level 2-5 years post-grad with relevant industry experience or graduate level Degree.', '\xa0-Set to work on new ingestion pipelines with full bandwidth available (as formal training will end)', 'Competitive compensation', 'In 12 months\xa0', 'Developing knowledge of industry data expected values such as PMPM by LOBs, MM trends, etc.', '\xa0-Pick an SME (Subject Matter Expert) path for what excites you the most', '\xa0Technical Domain Knowledge:', 'Responsible for delivery of work on expected timelines.', 'Delivery', 'Understanding of shared value contracts that our customers are in and how data is impacted by them', 'What You Get', 'Working and growing knowledge of new tech stack with less focus on finding efficiency in the technology and greater focus on understanding use of it.Developing ability to understand technical issues and communicate potential solutions to team members or engineering team', 'Developing internal and external professional communication skills including presentation of issues using appropriate industry vocabulary', 'Design and documentation of connectors / ingestion pipelines', '\xa0-Work within Data Engineering Scrum team', 'Support of our processes in partaking in peer code reviews , sprint planning , product grooming , maintaining Jira tasks and peer test reviews', 'NoSQL Databases: 1-2 years (Preferred)', '\xa0-Properly contribute to scrum ceremonies and ceremonies within the dev cycles while successfully updating status and progress in Jira\xa0', ""What You'll Be Doing"", 'In 3 months', 'Developing ability to understand technical issues and communicate potential solutions to team members or engineering team', 'SQL: 2-4 year (Preferred)Spark: 1-2 years (Preferred)NoSQL Databases: 1-2 years (Preferred)Database Architecture: 2-3 years (Preferred)Cloud Architecture: 1-2 years (Preferred)', 'This position is part of the Arcadia Data Engineering team, we are responsible for the onboarding, enhancement, and support of data feed integrations between Client Claim and Clinical data mgmt. platforms and our Healthcare Solution Platform. Our customers are top Healthcare providers and payers, and we help them integrate their internal systems with our analytic platform. The Data Engineering team is responsible for the data architecture that drives the partnership with customers and other internal organizations to drive success through adoption of cutting edge analytic solutions that leverage new age technologies and best practices. Our Data Engineers require both SQL Database knowledge and design , along with multiple programing languages .', 'Able to identify risk to project success and communicate to leadership', '-Set your own personal vision of development and career aspirations and set a working path forward with leadership to work on how we can help you attain those goals\xa0\xa0', 'Opportunity to be a part of a mission driven organization focused on helping provider organizations change the way they provide care to their patientsChance to be surrounded by a team of extremely talented and dedicated individuals driven to succeedCompetitive compensationAmazing benefits including unlimited FTO', 'Consistently deliver increasing connectors of increasing quality with ""lessons learned"" incorporated into next project', 'Cloud Architecture: 1-2 years (Preferred)', '\xa0', '\xa0-Work with Product, Engineering or Implementation to build out tools for better data integration', 'Design and documentation of connectors / ingestion pipelinesBuild and Unit testing of delivery connectors / ingestion pipelinesSupport of our processes in partaking in peer code reviews , sprint planning , product grooming , maintaining Jira tasks and peer test reviewsYou will be expected to contribute to multiple implementations simultaneously, which will include both new customer setup as well as support and enhancements for existing customers.The expectations of the day to day of an engineer is as follows:', 'Healthcare Analytics: 1-3 years (Preferred)', '\xa0-Ability to Deliver Data related Reviews to clients and other departments regarding code quality and test cases.', 'You will be expected to contribute to multiple implementations simultaneously, which will include both new customer setup as well as support and enhancements for existing customers.', 'Working and growing knowledge of new tech stack with less focus on finding efficiency in the technology and greater focus on understanding use of it.', 'Developing working knowledge of the business of healthcare data and how it interacts within the Arcadia products', 'Chance to be surrounded by a team of extremely talented and dedicated individuals driven to succeed', 'Business Domain Knowledge:', 'Developing working knowledge of the business of healthcare data and how it interacts within the Arcadia productsUnderstanding of shared value contracts that our customers are in and how data is impacted by themDeveloping knowledge of industry data expected values such as PMPM by LOBs, MM trends, etc.', 'TECH', 'Opportunity to be a part of a mission driven organization focused on helping provider organizations change the way they provide care to their patients', 'Healthcare Data: 2-4 years (Preferred)', 'Responsible for contributing to the advancement of team processes and internal', ""What You'll Have"", 'As a data engineer you will be expected to problem solve some basic data analysis issues and work the data to create analytic enhancements.', '\xa0-Work on higher level enhancement requests and ingestion pipelines', '\xa0-Working on standardized data connector development', '\xa0-Learn the different areas of the data connector life cycle, while having a working knowledge of the technical stacks , storage platforms , data models , and Dev. Cycle', 'What Success Looks Like:', 'DATA', 'Responsible for delivery of work on expected timelines.Able to identify risk to project success and communicate to leadershipWorks mostly independently on delivery w/decreasing involvement from engineering and more senior team membersConsistently deliver increasing connectors of increasing quality with ""lessons learned"" incorporated into next projectAble to apply critical thinking and problem solving skills to propose solutions for complex problems within day to day work', 'Communication Skills:', 'As a Data Engineer, you will drive the successful development of solution architecture and the completion of data pipeline connectors that automate the flow of data between client Claim and Clinical data platforms and our analytic health solution platform. Your efforts will be critical to driving the long-term partnership between Arcadia and our customers.', 'Able to apply critical thinking and problem solving skills to propose solutions for complex problems within day to day work', 'Spark: 1-2 years (Preferred)', 'Database Architecture: 2-3 years (Preferred)', 'Build and Unit testing of delivery connectors / ingestion pipelines', 'As a data engineer you will be expected to problem solve some basic coding issues and enhancements with frameworks that are built in Spark Scala, while also leveraging technical skills to partake in idea sessions on process improvement and POC design of how to carry out a solution.\xa0', 'As a data engineer you will be expected to problem solve some basic data analysis issues and work the data to create analytic enhancements.Healthcare Data: 2-4 years (Preferred)Healthcare Analytics: 1-3 years (Preferred)', 'In 6 months']",Mid-Senior level,Full-time,Engineering,Information Technology and Services,2021-02-16 15:27:39
Data Engineer,Burtch Works,New York City Metropolitan Area,23 hours ago,27 applicants,"[' - Excellent communication skills and business acumen in order to understand how decision impact other commercial organizations', 'healthcare, data engineer, AWS, python, big data, spark, ETL, data integration, data architecture, data platform, python, analytics, retail, ecommerce, statistics, machine learning, R, MapReduce, Hive, computer science, algorithms, java, scala, kafka', ' - Master’s degree in Computer Science or similar with at least 1-2 years of work experience as a data engineer or ML Engineer', '\ufeff', 'The ideal candidate needs to have:', ' - Experience working with large data sets and tools such as AWS, Hadoop, Spark, Hive, Kafka, Scala, Java, and Python', ' ', 'Master’s degree', 'at least 1-2 years of work experience as a data engineer', ' - Strong understanding of the Data Lifecycle and ability to build complex real-time data pipelines', 'Keywords', 'a leading global\xa0CPG company ', ' real-time data pipelines', 'Excellent communication skills', 'up to $120K', 'AWS, Hadoop, Spark, Hive, Kafka, Scala, Java, and Python', 'Data Engineer ', ' The ideal candidate needs to have:', 'base salary', 'Our client,\xa0a leading global\xa0CPG company is looking for a\xa0Data Engineer to join their analytics team! The right candidate will have strong knowledge and experience working with large amounts of structured and unstructured data, along with the ability to write algorithms and setup a cloud platform. This person will be working closely with data scientists along with other business units, and will be the data engineering subject matter expert. Strong communication and business acumen is a must! ', ' The base salary for this role depends on experience (up to $120K), plus annual bonus.', ' Keywords: healthcare, data engineer, AWS, python, big data, spark, ETL, data integration, data architecture, data platform, python, analytics, retail, ecommerce, statistics, machine learning, R, MapReduce, Hive, computer science, algorithms, java, scala, kafka']",Associate,Full-time,Information Technology,Retail,2021-02-16 15:27:39
Data Engineer,LatentView Analytics,"New York, United States",,N/A,"['', 'Good understanding of Data Warehouse methodologies ', 'Primary Responsibilities: ', 'Ability to build applications using Python frontend and backend for the Django based internal tooling.', 'Deep dives into specific customer data related issues and produces small reports to customer business\xa0performance', 'Scheduling and Monitoring of Hadoop and Spark jobs ', 'Ability to build applications using Python frontend and backend for the Django based internal tooling.Willing to learn Pyspark and ScalaDeep dives into specific customer data related issues and produces small reports to customer business\xa0performance', '2+ years’ experience with data ingestion through batch and streaming methodologies using open source or public cloud tools like Kafka, Airflow.', 'Additional Responsibilities: ', '3 to 8 years’ experience in developing Data Models, DB schemas, ETLs 2+ years’ experience with data ingestion through batch and streaming methodologies using open source or public cloud tools like Kafka, Airflow.2+ years’ experience working in one of three big public cloud ecosystems (i.e AWS, Azure or GCP)  Knowledge of Hadoop M/R, Pig and Hive is a strong plus Understanding of IN memory distributed computing frameworks like Spark (and/or DataBricks) and its parameter tuning, optimized query writing Scheduling and Monitoring of Hadoop and Spark jobs Good understanding of Data Warehouse methodologies Working knowledge of SQL Hands on experience in any of the programming languages (Shell scripting, Python, Scala, Java, etc)', '2+ years’ experience working in one of three big public cloud ecosystems (i.e AWS, Azure or GCP)  ', 'Working knowledge of SQL Hands on experience in any of the programming languages (Shell scripting, Python, Scala, Java, etc)', 'Understanding of IN memory distributed computing frameworks like Spark (and/or DataBricks) and its parameter tuning, optimized query writing ', 'Associate / Consultant (Data Engineering) - New York USA - Job Description', 'Willing to learn Pyspark and Scala', '3 to 8 years’ experience in developing Data Models, DB schemas, ETLs ', '\xa0', 'Knowledge of Hadoop M/R, Pig and Hive is a strong plus ']",Mid-Senior level,Full-time,Information Technology,Management Consulting,2021-02-16 15:27:39
NLP Data Engineer,Medix™,"California, United States",7 hours ago,Be among the first 25 applicants,"['', 'MUST have Masters Degree OR a Bachelor with 8-10 years experience in the medical or life sciences field\xa0', 'Our client is one of the largest cancer research organizations in the US. They currently do a lot of manual data set building, and are\xa0 looking to start automating this process through the use of AI tools like I2E Linguamatics.\xa0 This person will be doing mostly technical work including extracting and cleansing data and converting unstructured into structured form. The other part will be client-facing, including speaking directly with clinical professionals to gather requirements, ask questions, and seek clarity to understand medical and clinical data. Ultimately these conversations would be to drive improvements on the precision medicine platform.', 'Must-haves:\xa0', 'Create structured data sets to be used in predictive analytics for population health, cancer treatment, and research, targeted therapies for patients', 'Day to Day Responsibilities:\xa0', 'Work with disease registries and other clinical data to extract, scrub, and cleanse data into a structured form for the data model using I2E Linguamatics, or a similar NLP tool for the bioinformatics industry\xa0Use NLP/NLTK to\xa0 identify common key pieces of data within these data sets, to be used in applied biomedical research and predictive analytics ?\xa0Create structured data sets to be used in predictive analytics for population health, cancer treatment, and research, targeted therapies for patientsCreate data sets or data models to present to the Data Science Team', 'Work with disease registries and other clinical data to extract, scrub, and cleanse data into a structured form for the data model using I2E Linguamatics, or a similar NLP tool for the bioinformatics industry\xa0', 'Experience using Linguamatics, I2E, or other similar tools', 'About our client:', 'Use NLP/NLTK to\xa0 identify common key pieces of data within these data sets, to be used in applied biomedical research and predictive analytics ?\xa0', 'Create data sets or data models to present to the Data Science Team', 'Job Posting:\xa0', 'Professional Exposure to Life Science or Biomedical Science Field / Could include Public Health or Psychology work', 'MUST have Masters Degree OR a Bachelor with 8-10 years experience in the medical or life sciences field\xa0Professional Exposure to Life Science or Biomedical Science Field / Could include Public Health or Psychology workExperience using Linguamatics, I2E, or other similar tools', 'One of our clients is looking to add an NLP Data Engineer to their team to help with building a bioinformatics data set, working with NLP tools like I2E Linguamatics.\xa0']",Mid-Senior level,Full-time,Information Technology,Hospital & Health Care,2021-02-16 15:27:39
Data Engineer,Northwestern Mutual,"Milwaukee, WI",8 hours ago,Be among the first 25 applicants,"['Job Description', '', ""Designs and performs data extraction, assessment, translation, transformation, and load (ETL) processes with minimal guidance or from prescribed specificationsIn charge of building data equity, it's underlying code, tools, databases, and related infrastructureAdministers and maintains existing data engineering pipelines to their full extent of engineering needsAnalyzes data needs and identifies available internal and external sources.Utilizes business and analytical data modeling skills to design data integration and structure approaches.Utilizes programming skills to access and extract data from diverse sources residing on multiple platforms and implement data models by combining, synthesizing and structuring data from databases, files and spreadsheets.Assures data consistency and reliability. Performs quality checks, contributes to metadata and data dictionaries, documents repeatable extract, transform and load processes."", 'Utilizes programming skills to access and extract data from diverse sources residing on multiple platforms and implement data models by combining, synthesizing and structuring data from databases, files and spreadsheets.', ' We are an equal opportunity/affirmative action employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, gender identity or expression, sexual orientation, national origin, disability, age or status as a protected veteran, or any other characteristic protected by law.', 'Utilizes business and analytical data modeling skills to design data integration and structure approaches.', 'Administers and maintains existing data engineering pipelines to their full extent of engineering needs', 'At Northwestern Mutual, we are strong, innovative and growing. We invest in our people. We care and make a positive difference. ', 'Analyzes data needs and identifies available internal and external sources.', 'Assures data consistency and reliability. Performs quality checks, contributes to metadata and data dictionaries, documents repeatable extract, transform and load processes.', 'Required Certifications', 'Requirements', 'This job is not covered by the existing Collective Bargaining Agreement.', 'Preferred Experience', 'Designs and performs data extraction, assessment, translation, transformation, and load (ETL) processes with minimal guidance or from prescribed specifications', 'At Northwestern Mutual, we are strong, innovative and growing. We invest in our people. We care and make a positive difference.', ""In charge of building data equity, it's underlying code, tools, databases, and related infrastructure"", 'Grow your career with a best-in-class company that puts our client’s interests at the center of all we do. Get started now! ']",Entry level,Full-time,Information Technology,Information Technology and Services,2021-02-16 15:27:39
Data Engineer,Averity,United States,,N/A,"['• You will be building infrastructure and ETL using Big Data technologies.', '• $130,000 - $180,000', '• Our ultimate goal is to architect data tools the Data Team can utilize in designing and building our platform.', '• Unlimited PTO', 'What Skills Do You Need?', '• Experience using Data-Driven Insights to build SaaS products.', '• Bonus $ for taking a Vacation', '• At least 3-5yrs experience as a Software / Data Engineer', 'What is the Job?', '• Experience with Python & SQL.', 'We are hiring a Data Engineer (Remote) to advance our all-in-one platform and enhance the way we communicate virtually. You will be working extremely closely with the Senior Data Engineer to design and build systems to deliver solutions and leverage Big Data in the workforce.', '\xa0', ""What's in it for you?"", '• 100% Remote + Work/Life Balance', 'We are a startup that provides companies with a platform to communicate and deliver content with each of their employees seamlessly. We have offices in Manhattan, San Francisco, London and over 120 employees globally. We make sure our employees know and feel that they are a part of a mission and movement where every person matters.', 'Who Are We?', '• Computer Science Degree.', '• Experience with building Data Pipelines and Big Data solutions using tools like Spark and/or Kafka.', '• You will perform Data Analysis on large-scale datasets to find insights on the data.', '• Experience using Cloud environments, AWS is preferred.', '• 1 Day OFF every month', '• Medical, Dental and Vision Benefits', '• You will be designing scalable Big Data systems that can produce multi-modal data.']",Mid-Senior level,Full-time,Engineering,Computer Software,2021-02-16 15:27:39
Data Engineer,PeopleFun,"Richardson, TX",17 hours ago,Be among the first 25 applicants,"['', 'Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.', ' Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases. Comfortable working within Bash terminal environment. Applied experience with Python using libraries such as pandas, requests, various SQL connection stacks. Object oriented programming experience is a bonus. Experience building and optimizing ‘big data’ data pipelines, architectures and data sets. Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. Strong analytic skills related to working with unstructured datasets. Build processes supporting data transformation, data structures, metadata, dependency and workload management. A successful history of manipulating, processing and extracting value from large disconnected datasets. Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores. Strong project management and organizational skills. ', 'Competitive compensation package', 'Responsibilities', 'About PeopleFun', 'Flex PTO policy', 'Build processes supporting data transformation, data structures, metadata, dependency and workload management.', 'Applied experience with Python using libraries such as pandas, requests, various SQL connection stacks. Object oriented programming experience is a bonus.', 'Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.', 'Benefits', 'Happy hours, social events and more', 'Free drinks & snacks, catered lunch on Fridays', ' Create and maintain optimal data pipeline architecture. Assemble large, complex data sets that meet functional / non-functional business requirements. Identify, design, and implement internal process improvements automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and GCP ‘big data’ technologies. Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics. Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader. Work with data and analytics experts to strive for greater functionality in our data systems. ', 'Performance bonuses', 'Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.', 'Medical/Dental/Vision insurance', 'Strong analytic skills related to working with unstructured datasets.', ' Competitive compensation package Performance bonuses 401K with 3% employer matching Family friendly culture Flex PTO policy Medical/Dental/Vision insurance On-Site Gym and free Yoga classes $1,000 annual game device and IAP budget Free drinks & snacks, catered lunch on Fridays Happy hours, social events and more ', 'Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and GCP ‘big data’ technologies.', 'Work with data and analytics experts to strive for greater functionality in our data systems.', 'Requirements', 'On-Site Gym and free Yoga classes', 'A successful history of manipulating, processing and extracting value from large disconnected datasets.', '401K with 3% employer matching', '$1,000 annual game device and IAP budget', 'Family friendly culture', 'Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.', 'Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.', 'Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.', 'Create and maintain optimal data pipeline architecture.', ""Important To be considered, please provide a cover letter to explain, in your own words, your interest in this position and why you feel you'd be a great fit."", 'Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.', 'Strong project management and organizational skills.', 'Comfortable working within Bash terminal environment.', 'Assemble large, complex data sets that meet functional / non-functional business requirements.', 'Identify, design, and implement internal process improvements automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-02-16 15:27:39
Data Engineer,PLATO,"Phoenix, AZ",13 hours ago,Be among the first 25 applicants,[],Entry level,Full-time,Information Technology,Information Technology and Services,2021-02-16 15:27:39
Data Scientist,Nike,"Beaverton, OR",20 hours ago,69 applicants,"['', 'Solid skills in programming languages (particularly Python and SQL) and ability to apply them for data acquisition, preprocessing, modeling and monitoring.', 'Apply analytics methods to acquire, explore, cleanse, and fuse data from different sources.', 'What You Will Work On', 'Assist in the design of analytics solutions and the identification of the best method for a problem based on business requirements and constraints.', 'Excellent written and verbal communication skills, including ability to develop and deliver presentation.', 'Contribute to building, scoring, deploying, monitoring, updating and promoting Nike’s central forecasting platform.', 'Participate in a continuous learning environment within the advanced analytics community through persistent development of new skills and sharing of knowledge through mentorships and contributions to the open source community.', 'Demonstrated understanding of market demand and supply interactions, data structures, data science methods, and machine learning algorithms.', ' Advanced quantitative degree (Statistics, Mathematics, Economics, Computer Science or related field) and at least 3 years of relevant industry experience or Bachelor’s degree and at least 5 years of relevant professional experience. Demonstrated understanding of market demand and supply interactions, data structures, data science methods, and machine learning algorithms. Solid skills in programming languages (particularly Python and SQL) and ability to apply them for data acquisition, preprocessing, modeling and monitoring. Familiarity with common data science/analytics software tools (e.g. Jupyter Notebook, SQL consoles, Hadoop, Spark) and cloud computing platforms (e.g. Amazon Web Services). Experience in building, training, scoring, tuning and maintaining predictive models in production at enterprise scale and familiarity with mainstream packages relevant to managing all stages of Data Science/Analytics lifecycle. Familiarity with the Agile development process and Ability to work cross functionally and evaluate complex business information from multiple perspectives, including questioning assumptions and validity. Excellent written and verbal communication skills, including ability to develop and deliver presentation.', 'Facilitate the planning, scheduling and value measurement of the work to meet timeline targets and success criteria.', 'Support the adoption of analytic products through effective storytelling and collaboration with key partners.', ' Collaborate with other squad members to develop new sophisticated algorithms and improve existing approaches based on statistical/econometric methods, machine learning techniques and big data solutions to forecast consumer demand and determine optimal level of investment in various stages of the supply chain. Assist in the design of analytics solutions and the identification of the best method for a problem based on business requirements and constraints. Apply analytics methods to acquire, explore, cleanse, and fuse data from different sources. Contribute to building, scoring, deploying, monitoring, updating and promoting Nike’s central forecasting platform. Facilitate the planning, scheduling and value measurement of the work to meet timeline targets and success criteria. Support the adoption of analytic products through effective storytelling and collaboration with key partners. Participate in a continuous learning environment within the advanced analytics community through persistent development of new skills and sharing of knowledge through mentorships and contributions to the open source community. ', 'Collaborate with other squad members to develop new sophisticated algorithms and improve existing approaches based on statistical/econometric methods, machine learning techniques and big data solutions to forecast consumer demand and determine optimal level of investment in various stages of the supply chain.', 'Advanced quantitative degree (Statistics, Mathematics, Economics, Computer Science or related field) and at least 3 years of relevant industry experience or Bachelor’s degree and at least 5 years of relevant professional experience.', 'Familiarity with common data science/analytics software tools (e.g. Jupyter Notebook, SQL consoles, Hadoop, Spark) and cloud computing platforms (e.g. Amazon Web Services).', 'Experience in building, training, scoring, tuning and maintaining predictive models in production at enterprise scale and familiarity with mainstream packages relevant to managing all stages of Data Science/Analytics lifecycle.', 'Who We Are Looking For', 'Familiarity with the Agile development process and Ability to work cross functionally and evaluate complex business information from multiple perspectives, including questioning assumptions and validity.', 'What You Bring']",Entry level,Full-time,Engineering,Marketing and Advertising,2021-02-16 15:27:39
Data Engineer,Chicory,"New York, NY",21 hours ago,Over 200 applicants,"['', 'Experience building and optimizing data warehouses, preferably in BigQuery\xa0', 'Advanced SQL knowledge and experience working with relational databases, preferably PostgreSQL', 'Chicory is an NYC-based tech firm and the leading digital shopper marketing platform for CPG and grocery brands. Its signature “Get Ingredients” button can be found on over 1,500 recipe websites, including Taste of Home, Delish, Betty Crocker and thousands of influencer food blogs. Leveraging its extensive recipe network, Chicory partners with leading CPG brands like General Mills and grocery retailers like Wakefern to serve hyper-relevant ads to consumers in the moments when they’re planning their grocery purchases. As the pioneer of shoppable recipes and the expert in contextual commerce, Chicory creates the digital tools to take grocery shoppers from inspiration to checkout in a few clicks. ', 'Developing and maintaining datasets in our data warehouse (BigQuery)', 'Responsibilities', ""As an early member of our team, you'll provide significant strategic and technical guidance. You'll help us solve some of the many technical challenges that still lie ahead, have a direct impact on shaping our engineering culture, and will work alongside the rest of our team to lay out our company’s roadmap. The experience you will gain will be unique and unmatched."", 'Taste of Home, Delish, Betty Crocker', 'About the Company', 'Work with stakeholders to understand needs for data structure, availability, and accessibility', 'Advanced SQL knowledge and experience working with relational databases, preferably PostgreSQLExperience building and optimizing data warehouses, preferably in BigQuery\xa0Strong analytical skills related to working with structured and unstructured datasetsExperience building and optimizing data pipelines using Dataflow (Apache Beam)\xa0Experience with Google Cloud Services: SQL, Dataflow, ComposerExperience with object-oriented scripting language: PythonExperience working with a business intelligence tool, e.g. SuperSet (preferred), Metabase, Looker, or Tableau', 'We will continue to innovate in the shopper marketing space and we need an awesome team to do so. We believe in bringing your whole self to work. Everyone on our team brings something unique to their role but we all value empathy, collaboration, a willingness to learn, and the desire to truly have an impact on our fast paced and fast growing company.\xa0', 'Develop, extend, and maintain data pipelines using Dataflow (Apache Beam) to process structured and unstructured data in near real-time', 'Strong analytical skills related to working with structured and unstructured datasets', 'Experience with Google Cloud Services: SQL, Dataflow, Composer', 'About the Role', 'Experience with object-oriented scripting language: Python', 'Owning Chicory data - this means being involved in reporting, data exports, discrepancy investigations, and able to answer any questions related to our normal data patterns', 'Experience building and optimizing data pipelines using Dataflow (Apache Beam)\xa0', 'Collaborate with engineers on data-driven web application projects', ""As an engineer at Chicory, you’ll build new and awesome grocery shopping experiences. You'll use Chicory's analytics architecture to measure the improvement your work has on millions of users. We strongly believe in adding incremental value, so your work will reach those users in a matter of days, if not hours, using our Continuous Deployment strategies."", 'Experience working with a business intelligence tool, e.g. SuperSet (preferred), Metabase, Looker, or Tableau', 'Owning Chicory data - this means being involved in reporting, data exports, discrepancy investigations, and able to answer any questions related to our normal data patternsDevelop, extend, and maintain data pipelines using Dataflow (Apache Beam) to process structured and unstructured data in near real-timeWork with stakeholders to understand needs for data structure, availability, and accessibilityAnalyze raw data and build reports using Apache SuperSet (BI tool)\xa0Developing and maintaining datasets in our data warehouse (BigQuery)Implement methods to improve data reliability and qualityCollaborate with engineers on data-driven web application projects', 'Required Qualifications', 'Analyze raw data and build reports using Apache SuperSet (BI tool)\xa0', 'Chicory is looking for a Data Engineer to join our ranks. The ideal candidate is an expert in building and maintaining data systems, has strong analytical skills, and the ability to combine data from different sources.\xa0', 'Implement methods to improve data reliability and quality']",Entry level,Full-time,Information Technology,Marketing and Advertising,2021-02-16 15:27:39
Data Engineer,Team Cymru,"Lake Mary, FL",12 hours ago,Be among the first 25 applicants,"['', 'Working-level knowledge of algorithms', 'Knowledge of MVC frameworks', 'Some specialized training or education beyond high school is preferred', 'Prolonged periods of sitting at a desk and working on a computer.Must be able to travel up to 5% of the time.', ""Proactively creates automated analytics solutions to push team's capabilities and increased situational awareness"", 'Demonstrates sound coding techniques', 'Job Summary:', 'Contributes across whole project lifecycle, utilizing peers for guidance where necessary.', 'Embodies and demonstrates maturity, professionalism, and ethics', 'Incorporates effective test procedures, logging and monitoring in software with minimal oversight', 'Additional Desired Skills/Abilities', 'identification and submission of product improvement when appropriate', 'Articulate in oral and written communication', 'Embodies and demonstrates maturity, professionalism, and ethicsArticulate in oral and written communicationWorking-level knowledge of algorithmsDemonstrates sound coding techniquesAble to break-down complex requirements into workflows and identify key performance indicators.Proficient in the use of databases: query and data definitionProficiency in one or more core languages: Golang, Python, SQL, Bash, Perl', 'Actively contributes to cross-functional team efforts', 'Working knowledge of networking protocols', 'Physical Requirements:', 'Experience with Cloud technologies like: AWS, Google Cloud, Azure', 'Duties/Responsibilities:', 'None.', 'Creates quality product and support documentation', 'Proficient in the use of industry standard tooling (i.e. the Atlassian Stack, etc.)', 'Performs triage of product support requests, problem determination and assists with escalation when appropriate', 'Operate independently and seeks assistance or guidance when required.', 'Consistently delivers to deadlines at the required quality standards', 'Solid oral and written communications skills', 'Consistently adheres to commitments with respect to delivery and timeframe', ' ', 'Proficiency in designing and developing innovative data analytics software and methods.', 'Able to break-down complex requirements into workflows and identify key performance indicators.', 'Ability to recognize trends and patterns in the data that can be exploited into a repeatable analysis process', 'Typically has two to four years combined industry / education experience', 'Conducts self-assessments by comparing required skills with existing knowledge to develop, present and execute plans for improvement', 'Ability to effectively create and utilize REST APIs', 'Proficiency in one or more core languages: Golang, Python, SQL, Bash, Perl', 'Competent with Linux', 'Education and Experience:', 'The Data Engineer utilizes a wide range of technologies to design, develop, and deploy innovative programming and technical solutions to data analytics and data processing. The Data Engineer is expected to demonstrate increased proficiency in newly acquired industry-related skills. This person can work independently and produce work according to clear-cut and complete specifications.', 'Experience creating and distributing Jupyter Notebooks for repeatable data analysis', 'Contributes to efforts in maintaining and improving product quality', 'Virtual', 'Participates in regular review of individual output to ensure it conforms to department and company standards', ""Proficiency in designing and developing innovative data analytics software and methods.Contributes across whole project lifecycle, utilizing peers for guidance where necessary.Operate independently and seeks assistance or guidance when required.Ability to recognize trends and patterns in the data that can be exploited into a repeatable analysis processPerforms triage of product support requests, problem determination and assists with escalation when appropriateDemonstrates a complete understanding of a core-product or service offering's features, construction and operating characteristicsIncorporates effective test procedures, logging and monitoring in software with minimal oversightParticipates in regular review of individual output to ensure it conforms to department and company standardsContributes to efforts in maintaining and improving product qualityidentification and submission of product improvement when appropriateCreates quality product and support documentationIdentifies risks to projects, communicates and formulates mitigation plansActively contributes to cross-functional team effortsConducts self-assessments by comparing required skills with existing knowledge to develop, present and execute plans for improvementConsistently delivers to deadlines at the required quality standards"", 'Familiarity with design patterns and industry best practices', 'Identifies risks to projects, communicates and formulates mitigation plans', 'Must be able to travel up to 5% of the time.', ""Familiarity with design patterns and industry best practicesExperience with one or alternative database technologies like: ElasticSearch, Apache Cassandra, Mongo DB, SparkExperience with Cloud technologies like: AWS, Google Cloud, AzureAbility to effectively create and utilize REST APIsProactively creates automated analytics solutions to push team's capabilities and increased situational awarenessKnowledge of MVC frameworksAbility to execute complex queries and design relational databases in PostgreSQL using referential integrity, views, stored procedures and proper indicesAbility to create visualizations from resultant analytic resultsExperience creating and distributing Jupyter Notebooks for repeatable data analysis"", 'Ability to execute complex queries and design relational databases in PostgreSQL using referential integrity, views, stored procedures and proper indices', 'Competent with LinuxSolid oral and written communications skillsConsistently adheres to commitments with respect to delivery and timeframeWorking knowledge of networking protocols', 'Supervisory Responsibilities:', 'Location:', 'High school diploma or equivalent required', 'Ability to create visualizations from resultant analytic results', 'Proficient in the use of databases: query and data definition', ""Demonstrates a complete understanding of a core-product or service offering's features, construction and operating characteristics"", 'Required Skills/Abilities:', 'High school diploma or equivalent requiredTypically has two to four years combined industry / education experienceSome specialized training or education beyond high school is preferred', 'Prolonged periods of sitting at a desk and working on a computer.', 'Data Engineer', 'Experience with one or alternative database technologies like: ElasticSearch, Apache Cassandra, Mongo DB, Spark']",Entry level,Full-time,Engineering,Information Technology and Services,2021-02-16 15:27:39
Data Engineer / Data Scientist ,Data Recognition Corporation,"Maple Grove, MN",52 minutes ago,Be among the first 25 applicants,"['', 'Relate effectively and work respectfully with diverse work groups', 'Understanding of Deep Learning methods', 'Front to back end development spanning Web UI to Database/ETL experience ', 'Ability to review and understand business requirements', 'DRC is one of the largest educational assessment and curriculum/instruction', 'DRC retains the right to change or assign other duties to this position', 'Essential Job Requirements:', 'Working with database models', 'Experience working with cloud technologies ', 'Core Skills (required):', 'Ability to review and create detailed technical documentation', 'Data Engineer / Data Scientist (Full Stack, Senior level)', 'Familiarity with BI tools ', 'Shell scripting ', 'Web scripting experience ', 'Senior Data Engineer / Data Scientist (Full Stack)', 'Position may be remote if desired', 'ETL tools and methodologies ', 'Working knowledge of machine learning frameworks', 'SQL, data engineering and coding for various database technologies', 'Teamwork and good oral/written communication skillsFamiliarity with Microsoft Office SuiteRelate effectively and work respectfully with diverse work groups', '\xa0', 'Understanding of Deep Learning methodsUnderstanding of Natural Language Processing methods Familiarity with BI tools Experience working with cloud technologies Familiarity with Agile development methodologies, including ScrumUnderstanding of test-driven development and CI/CD deployment pipelineExperience with Open Source work management toolsExperience scripting for API integration testing ', 'No agencies or third parties, please', 'No Agencies or third parties, please', 'Experience developing software in Python', 'Understanding of test-driven development and CI/CD deployment pipeline', 'Experience with Open Source work management tools', 'Experience scripting for API integration testing ', 'Teamwork and good oral/written communication skills', 'Understanding of Natural Language Processing methods ', 'Company cannot provide sponsorship for this position', 'companies in the industry.\xa0\xa0', 'Data Recognition Corporation is an Affirmative Action', ""We are looking for a data-savvy Full Stack Data Engineer/Data Scientist with demonstrated versatility to contribute to the advancement of DRC's Enterprise Analytics and Data Science initiatives.\xa0In this role, you will collaborate closely with teammates as well as Enterprise and Department stakeholders and be responsible to help design, develop and qualify KPI analytic reporting and forecasting systems that automate DRC's business processes with improved enterprise data management and visualizations for business intelligence (BI). You will also help investigate, develop, and optimize data engineering solutions in support of the Company’s advanced analytical products and data science offerings."", 'Data Recognition Corporation, Maple Grove, MN\xa0', 'Front to back end development spanning Web UI to Database/ETL experience Experience developing software in PythonWorking knowledge of machine learning frameworksWeb scripting experience SQL, data engineering and coding for various database technologiesETL tools and methodologies Working with database modelsMicroservices/REST conceptsShell scripting Ability to review and understand business requirementsAbility to review and create detailed technical documentation', 'Familiarity with Microsoft Office Suite', 'Preferred Skills (nice-to-have, not required, or desire to learn):', 'Company cannot provide sponsorship for this position\xa0', 'Microservices/REST concepts', 'Familiarity with Agile development methodologies, including Scrum', 'companies in the industry.\xa0\xa0\xa0']",Mid-Senior level,Full-time,Information Technology,Education Management,2021-02-16 15:27:39
Data Engineer,N/A,"Bellevue, WA",20 hours ago,Be among the first 25 applicants,"['·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience building and maintaining data pipelines', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience with production BI implementations in the Cloud', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Work with Project Management, data scientists and business stakeholders to understand requirements and translate to technical requirements.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience optimizing code for hardened, efficient deployments', 'Responsibilities', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Facilitate ingestion of raw telemetry, and perform cooking, joining, and aggregation to facilitate consumption downstream.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience with data warehouse technical architectures, ETL/ELT, and reporting/analytic tools', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Exceptional problem solving, technical and data analysis skills', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Advanced hands-on experience with Azure Cloud Services (Data Factory, Data Explorer, HDInsight. Cosmos DB, SQL) or equivalent', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experiment with and recommend new technologies that improve the team’s ability to innovate.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience building Power BI, Excel, and Reporting Services dashboards and reports', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Bachelor’s degree in computer science or engineering, database systems, mathematics, or 5+ years of industry experience in a data engineering role.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Ability to work in a team environment that promotes collaboration', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Great written and verbal communication and presentation skills', '\xa0', 'Qualifications', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience with Machine Learning Model deployment', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Design, architect and support high quality data frameworks that will provide actionable information to various teams across the studio.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Be self-driven, and show ability to deliver on ambiguous projects with incomplete or dirty data', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Use best engineering practices to ensure security and privacy compliance across all development projects.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience with Data Lake infrastructures (Cosmos, Hadoop)', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Collaborate and communicate with engineers, data scientists and analysts to optimize data flows, tools, operational costs and reporting infrastructures.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Build data marts for different use cases across the business, using Microsoft and other big data technologies including Azure Synapse.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience designing and building data warehouse solutions']",Entry level,Full-time,Information Technology,N/A,2021-02-16 15:27:39
Data Engineer - REMOTE,Jobot,"Newport Beach, CA",11 hours ago,Be among the first 25 applicants,"['', ' We believe that kindness is still completely relevant.', ' Flexible Work Schedules!', 'Job Details', ' At least 3 years of experience building data pipelines in Python, Spark, or similar technologies', ' At least 3 years of experience building data pipelines in Python, Spark, or similar technologies At least 2 years of experience with a analytics dashboard, e.g. Tableau, Looker, etc. At least 1 year of web development experience Experience with AWS, specifically Redshift, is a plus', ' Extremely Fun and Passionate Culture!', 'Are you a data driven decision maker? Can you help bridge the gap between software engineering and data analysis? Come join us!', ' At least 1 year of web development experience', ' Experience with AWS, specifically Redshift, is a plus', ' Competitive Base Salary! Extremely Fun and Passionate Culture! Flexible Work Schedules! Accelerated Career Growth!', 'A Bit About Us', ' Our Get a Job Give a Job program helps increase employment across the globe.', ' We take the job very seriously but do not take ourselves seriously.', ' Accelerated Career Growth!', ' Competitive Base Salary!', ' At least 2 years of experience with a analytics dashboard, e.g. Tableau, Looker, etc.', ' We take the job very seriously but do not take ourselves seriously. We believe that kindness is still completely relevant. We believe transparency and a strong team brings the best results for everyone. Our Get a Job Give a Job program helps increase employment across the globe.', ' We believe transparency and a strong team brings the best results for everyone.', 'Why join us?']",Mid-Senior level,Full-time,Information Technology,Information Technology and Services,2021-02-16 15:27:39
Data Engineer II,Amazon,"North Reading, MA",6 hours ago,Be among the first 25 applicants,"['', 'Responsibilities', ' Work directly with customers to integrate new data types, equipment, and incorporate feedback', ' Experience with one or more big data technologies (Hadoop, Hive, EMR, Spark etc.)', ' Design and build business critical, highly available data pipelines in a stable, low cost model', ' 2+ years of experience with traditional RDBMS technologies, SQL, and data modeling', ' Help continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers', ' Master’s in Computer Science, Electrical Engineering or related field or equivalent industry experience 5+ years of Data Engineering experience Experience with mentoring junior Engineers Experience leveraging SAS, R to manipulate data and set up automated processes', ' Internalize our customer challenges and derive creative solutions which apply new and innovative technology', 'Description', ' Prototype and test concepts in technology and use case intersection to propose designs in Data Engineering, Business Intelligence and Data Science space', ' Apply SW best practices including coding standards, code reviews, source control management, agile development, build processes, and testing', ' 5+ years of Data Engineering experience', 'Preferred Qualifications', ' BS/MS in Computer Science, Math, or other algorithmic-centric discipline or equivalent experience', ' Experience in one or more scripting or programming languages', 'Company', ' Actively support and foster a culture of inclusion Internalize our customer challenges and derive creative solutions which apply new and innovative technology Design and build business critical, highly available data pipelines in a stable, low cost model Design software with quality, robustness and Amazon sized scale as top priorities Prototype and test concepts in technology and use case intersection to propose designs in Data Engineering, Business Intelligence and Data Science space Work directly with customers to integrate new data types, equipment, and incorporate feedback Help continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers Apply SW best practices including coding standards, code reviews, source control management, agile development, build processes, and testing', ' Actively support and foster a culture of inclusion', ' Experience leveraging SAS, R to manipulate data and set up automated processes', ' Effective communicator, able to work effectively with global and cross-functional stakeholders', ' Master’s in Computer Science, Electrical Engineering or related field or equivalent industry experience', ' Experience with mentoring junior Engineers', ' Effective communicator, able to work effectively with global and cross-functional stakeholders BS/MS in Computer Science, Math, or other algorithmic-centric discipline or equivalent experience 3+ years of experience building production scale data pipelines using Python or ETL tools 2+ years of experience with traditional RDBMS technologies, SQL, and data modeling Experience with one or more big data technologies (Hadoop, Hive, EMR, Spark etc.) Experience in one or more scripting or programming languages', ' Design software with quality, robustness and Amazon sized scale as top priorities', ' 3+ years of experience building production scale data pipelines using Python or ETL tools', 'Basic Qualifications']",Mid-Senior level,Full-time,Information Technology,Computer Software,2021-02-16 15:27:39
Data Scientist - Remote Position,Zillow,"Tacoma, WA",38 minutes ago,Be among the first 25 applicants,"['', ""Dive into Zillow's internal and third party data (think Hive, Presto, SQL Server, Redshift, Python, Mode Analytics, Tableau, R) to make strategic recommendations (e.g., personalized user flows, segmented marketing audiences, more accurate forecasts).Assist in the prioritization and development of new product features and business initiatives. Compile and analyze data describing the performance of those initiatives (e.g., AB tests, pilots, incrementality tests).Develop a common language and approach to analyzing and communicating information and insights across teams.Create, maintain, and document a robust set of metrics to monitor day-to-day bug detection and long-term performance tracking.Tell stories that describe analytical results and insights in meetings of all sizes with diverse audiences.Consult with business partners and provide insight to a variety of non-technical audiences to ensure that all levels of Zillow Group make data-driven decisions."", 'Assist in the prioritization and development of new product features and business initiatives. Compile and analyze data describing the performance of those initiatives (e.g., AB tests, pilots, incrementality tests).', 'Strong written, verbal, and visual communication skills to concisely communicate in a way that provides context, offers insights, and minimizes misinterpretation.', 'Zillow Group is an equal opportunity employer committed to fostering an inclusive, innovative environment with the best employees. Therefore, we provide employment opportunities without regard to age, race, color, ancestry, national origin, religion, disability, sex, gender identity or expression, sexual orientation, or any other protected status in accordance with applicable law. If there are preparations we can make to help ensure you have a comfortable and positive interview experience, please let us know.', ""An undergraduate or Master's degree in a quantitative field (e.g. science, engineering, economics, finance, statistics, or similar) or demonstrable experience within data science and analytics."", '2+ years of work experience involving quantitative data analysis and complex problem solving (preferably focused on consumer-facing internet products).', 'Create, maintain, and document a robust set of metrics to monitor day-to-day bug detection and long-term performance tracking.', 'Consult with business partners and provide insight to a variety of non-technical audiences to ensure that all levels of Zillow Group make data-driven decisions.', 'The skills to work cross-functionally and push business partners to focus on realistic goals and projects.', 'About The Role', 'About The Team', ""We're Looking For a Seasoned Data Scientist Who Has"", 'Develop a common language and approach to analyzing and communicating information and insights across teams.', 'Foundational analytical skills (proven understanding of SQL, command of Excel, and experience using R and/or Python) and a Tableau pro.', 'Experience directly querying multi-terabyte-sized data sets (e.g. with Hive and Presto) including clickstream data and raw data ingested from non-standard platforms (e.g., homegrown systems).', ""An undergraduate or Master's degree in a quantitative field (e.g. science, engineering, economics, finance, statistics, or similar) or demonstrable experience within data science and analytics.2+ years of work experience involving quantitative data analysis and complex problem solving (preferably focused on consumer-facing internet products).Foundational analytical skills (proven understanding of SQL, command of Excel, and experience using R and/or Python) and a Tableau pro.Experience directly querying multi-terabyte-sized data sets (e.g. with Hive and Presto) including clickstream data and raw data ingested from non-standard platforms (e.g., homegrown systems).An understanding of concepts, terminology, and measurement issues related to web traffic.Strong written, verbal, and visual communication skills to concisely communicate in a way that provides context, offers insights, and minimizes misinterpretation.The skills to work cross-functionally and push business partners to focus on realistic goals and projects."", 'Tell stories that describe analytical results and insights in meetings of all sizes with diverse audiences.', 'An understanding of concepts, terminology, and measurement issues related to web traffic.', ""Dive into Zillow's internal and third party data (think Hive, Presto, SQL Server, Redshift, Python, Mode Analytics, Tableau, R) to make strategic recommendations (e.g., personalized user flows, segmented marketing audiences, more accurate forecasts).""]",Entry level,Full-time,Other,Marketing and Advertising,2021-02-16 15:27:39
Data Engineer,Capital Group,"Los Angeles, CA",8 hours ago,72 applicants,"['', 'You’ve implemented big data processing technology like Hadoop, AWS Redshift, Microsoft MPP (SQL DW/Synapse), and Snowflake.', '""I can lead a full life.""', 'Complimentary Experience', ' You have experience in cloud-first design, preferably AWS or Azure and corresponding services and components (data lake, MPP databases, autoscaling, container orchestration, etc.) Your background includes 2-3 years’ experience in implementing big data solutions and automating enterprise scale data pipelines leveraging Hadoop, Apache Spark, etc. You’ve implemented big data processing technology like Hadoop, AWS Redshift, Microsoft MPP (SQL DW/Synapse), and Snowflake. You have financial services/investment management experience ', ' Enjoy generous time-away and health benefits from day one, with the opportunity for flexible work options Receive 2-for-1 matching gifts for your charitable contributions and the opportunity to secure annual grants for the organizations you love Access on-demand professional development resources that allow you to hone existing skills and learn new ones ', 'Travel required', 'Enjoy generous time-away and health benefits from day one, with the opportunity for flexible work options', 'Your background includes 2-3 years’ experience in implementing big data solutions and automating enterprise scale data pipelines leveraging Hadoop, Apache Spark, etc.', '""I can apply in less than 4 minutes.""', "" You are excited by the meaning, structure, and content of data, in addition to building the mechanisms to make it available throughout the organization You have a bachelor's degree in Computer Science, Engineering or a related technical field, and/or 3+ years relevant work experience in analytics, data engineering, complex ETL, BI or related field. You write and optimize advanced SQL queries with large-scale, complex datasets You have knowledge of ETL/ELT technologies  You have experience modeling data for data warehousing and data lakes You have coding proficiency in at least one modern language such as Python, R, Java and an understanding of the Big Data technology stack, including Hive, Hbase, Oozie, Airflow, MapReduce, and Spark. You have strengths in leadership, interpersonal, and problem solving skills with the ability to continually learn new concepts and technologies and effectively apply them  "", 'Req ID', 'Receive 2-for-1 matching gifts for your charitable contributions and the opportunity to secure annual grants for the organizations you love', 'Access on-demand professional development resources that allow you to hone existing skills and learn new ones', ""COVID-19 HIRING Our recruiting and onboarding activities are virtual during the pandemic and we've transitioned to a work-from-home environment until further notice. We are offering generous work-from-home benefits to improve our associate's ability to work remotely."", ""  You are excited by the meaning, structure, and content of data, in addition to building the mechanisms to make it available throughout the organization You have a bachelor's degree in Computer Science, Engineering or a related technical field, and/or 3+ years relevant work experience in analytics, data engineering, complex ETL, BI or related field. You write and optimize advanced SQL queries with large-scale, complex datasets You have knowledge of ETL/ELT technologies  You have experience modeling data for data warehousing and data lakes You have coding proficiency in at least one modern language such as Python, R, Java and an understanding of the Big Data technology stack, including Hive, Hbase, Oozie, Airflow, MapReduce, and Spark. You have strengths in leadership, interpersonal, and problem solving skills with the ability to continually learn new concepts and technologies and effectively apply them   "", '""I can influence my income.""', 'You have knowledge of ETL/ELT technologies ', 'You are excited by the meaning, structure, and content of data, in addition to building the mechanisms to make it available throughout the organization', 'You have experience modeling data for data warehousing and data lakes', 'Other location(s)', '""I can be myself at work.""', 'You have financial services/investment management experience', 'Relocation benefits offered', ""You have a bachelor's degree in Computer Science, Engineering or a related technical field, and/or 3+ years relevant work experience in analytics, data engineering, complex ETL, BI or related field."", 'You write and optimize advanced SQL queries with large-scale, complex datasets', 'I am the person Capital Group is looking for.”', 'You have strengths in leadership, interpersonal, and problem solving skills with the ability to continually learn new concepts and technologies and effectively apply them', 'Location', ' You have experience in cloud-first design, preferably AWS or Azure and corresponding services and components (data lake, MPP databases, autoscaling, container orchestration, etc.) Your background includes 2-3 years’ experience in implementing big data solutions and automating enterprise scale data pipelines leveraging Hadoop, Apache Spark, etc. You’ve implemented big data processing technology like Hadoop, AWS Redshift, Microsoft MPP (SQL DW/Synapse), and Snowflake. You have financial services/investment management experience  ', "" You are excited by the meaning, structure, and content of data, in addition to building the mechanisms to make it available throughout the organization You have a bachelor's degree in Computer Science, Engineering or a related technical field, and/or 3+ years relevant work experience in analytics, data engineering, complex ETL, BI or related field. You write and optimize advanced SQL queries with large-scale, complex datasets You have knowledge of ETL/ELT technologies  You have experience modeling data for data warehousing and data lakes You have coding proficiency in at least one modern language such as Python, R, Java and an understanding of the Big Data technology stack, including Hive, Hbase, Oozie, Airflow, MapReduce, and Spark. You have strengths in leadership, interpersonal, and problem solving skills with the ability to continually learn new concepts and technologies and effectively apply them "", '  You have experience in cloud-first design, preferably AWS or Azure and corresponding services and components (data lake, MPP databases, autoscaling, container orchestration, etc.) Your background includes 2-3 years’ experience in implementing big data solutions and automating enterprise scale data pipelines leveraging Hadoop, Apache Spark, etc. You’ve implemented big data processing technology like Hadoop, AWS Redshift, Microsoft MPP (SQL DW/Synapse), and Snowflake. You have financial services/investment management experience   ', '""I can learn more about Capital Group."" ', 'You have coding proficiency in at least one modern language such as Python, R, Java and an understanding of the Big Data technology stack, including Hive, Hbase, Oozie, Airflow, MapReduce, and Spark.', 'You have experience in cloud-first design, preferably AWS or Azure and corresponding services and components (data lake, MPP databases, autoscaling, container orchestration, etc.)']",Not Applicable,Full-time,Information Technology,Financial Services,2021-02-16 15:27:39
Data Engineer,Tekniforce,Raleigh-Durham-Chapel Hill Area,5 hours ago,125 applicants,"['', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Propose and roll-out improvements to culture, process, tools, technology, and architecture.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Be customer oriented, and provide data and support to business partners as necessary', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Intermediate knowledge of MS Excel', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience with ETL tools such as Pentaho, KNIME preferred', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Advanced level of DAX programming', 'Required Skills for a Data Engineer:', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Lead the development of certified Power BI Data Models and Data Sources, utilizing internal and external data (APIs)', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0High level of business acumen with clear understanding of how data and information support business objectives', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Expected to challenge teammates in a constructive and professional manner', 'Responsibilities as a Data Engineer:', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Invent ways to answer key business questions by leveraging existing data assets or assisting in creating new ones', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Intermediate knowledge of R or Python preferred', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience working with data sourced from REST and/or SOAP APIs preferred', ""We are looking for a Data Engineer. This is a full time permanent hire position in Raleigh, NC with our client. In this role as a Data Engineer, you will interact with stakeholders to analyze, explain, design, and develop new data services and capabilities supporting the enable company's business to take data driven decisions"", '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Advanced level of SQL programming', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Participate in designing and implementing RESTful services and microservices.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Work in dynamic self-organized agile teams to develop high-quality solutions using the best technology stack, design, and architecture.', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Bachelor’s degree preferably in Computer Science, Management Information Systems (MIS), or Business Field with emphasis in Information Technology', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa04+ years of experience developing with PowerBI', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Experience with Azure Data Analytics Architecture preferred', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Define and evolve company’s data movement, data access, MDM, and data visualization strategies', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Mentor and provide assistance with Power BI to partners across the organization', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Ability to both work collaboratively with teams and excels being a contributor', '·\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Assist in developing best BI and data services deployment practices such as Azure Data Factories and Azure CI/CD pipelines, including development, governance and maintenance.']",Mid-Senior level,Full-time,Information Technology,Retail,2021-02-16 15:27:39
